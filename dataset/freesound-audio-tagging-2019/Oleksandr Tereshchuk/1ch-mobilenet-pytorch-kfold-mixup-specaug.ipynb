{"cells":[{"metadata":{},"cell_type":"markdown","source":"### imports"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport pickle\nimport random\nimport time\nfrom collections import Counter, defaultdict\nfrom operator import itemgetter\nfrom functools import partial\nfrom pathlib import Path\nfrom psutil import cpu_count\n\nimport numpy as np\nimport pandas as pd\n\nimport librosa\nfrom PIL import Image, ImageOps\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim import Adam, Optimizer, RMSprop, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, _LRScheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import transforms\n\nfrom fastprogress import master_bar, progress_bar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 666\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_JOBS = cpu_count()\nos.environ['MKL_NUM_THREADS'] = str(N_JOBS)\nos.environ['OMP_NUM_THREADS'] = str(N_JOBS)\nDataLoader = partial(DataLoader, num_workers=N_JOBS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from official code https://colab.research.google.com/drive/1AgPdhSp7ttY18O3fEoHOQKlt_3HJDLi8#scrollTo=cRCaCIb9oguU\ndef _one_sample_positive_class_precisions(scores, truth):\n    \"\"\"Calculate precisions for each true class for a single sample.\n\n    Args:\n      scores: np.array of (num_classes,) giving the individual classifier scores.\n      truth: np.array of (num_classes,) bools indicating which classes are true.\n\n    Returns:\n      pos_class_indices: np.array of indices of the true classes for this sample.\n      pos_class_precisions: np.array of precisions corresponding to each of those\n        classes.\n    \"\"\"\n    num_classes = scores.shape[0]\n    pos_class_indices = np.flatnonzero(truth > 0)\n    # Only calculate precisions if there are some true classes.\n    if not len(pos_class_indices):\n        return pos_class_indices, np.zeros(0)\n    # Retrieval list of classes for this sample.\n    retrieved_classes = np.argsort(scores)[::-1]\n    # class_rankings[top_scoring_class_index] == 0 etc.\n    class_rankings = np.zeros(num_classes, dtype=np.int)\n    class_rankings[retrieved_classes] = range(num_classes)\n    # Which of these is a true label?\n    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n    retrieved_class_true[class_rankings[pos_class_indices]] = True\n    # Num hits for every truncated retrieval list.\n    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n    precision_at_hits = (\n            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n            (1 + class_rankings[pos_class_indices].astype(np.float)))\n    return pos_class_indices, precision_at_hits\n\n\ndef calculate_per_class_lwlrap(truth, scores):\n    \"\"\"Calculate label-weighted label-ranking average precision.\n\n    Arguments:\n      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n        of presence of that class in that sample.\n      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n        test's real-valued score for each class for each sample.\n\n    Returns:\n      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n        class.\n      weight_per_class: np.array of (num_classes,) giving the prior of each\n        class within the truth labels.  Then the overall unbalanced lwlrap is\n        simply np.sum(per_class_lwlrap * weight_per_class)\n    \"\"\"\n    assert truth.shape == scores.shape\n    num_samples, num_classes = scores.shape\n    # Space to store a distinct precision value for each class on each sample.\n    # Only the classes that are true for each sample will be filled in.\n    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n    for sample_num in range(num_samples):\n        pos_class_indices, precision_at_hits = (\n            _one_sample_positive_class_precisions(scores[sample_num, :],\n                                                  truth[sample_num, :]))\n        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n            precision_at_hits)\n    labels_per_class = np.sum(truth > 0, axis=0)\n    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n    # Form average of each column, i.e. all the precisions assigned to labels in\n    # a particular class.\n    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n                        np.maximum(1, labels_per_class))\n    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n    #                = np.sum(per_class_lwlrap * weight_per_class)\n    return per_class_lwlrap, weight_per_class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"os.listdir('../input/creating-1ch-fat2019-preprocessed-data/work/fat2019_prep_mels1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_dir = Path('../input/freesound-audio-tagging-2019')\npreprocessed_dir = Path('../input/creating-1ch-fat2019-preprocessed-data/work/fat2019_prep_mels1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csvs = {\n    'train_curated': dataset_dir / 'train_curated.csv',\n    'trn_curated_trimmed': preprocessed_dir / 'trn_curated_trimmed.csv',\n    'train_noisy': preprocessed_dir / 'trn_noisy_best50s.csv',\n    'sample_submission': dataset_dir / 'sample_submission.csv',\n}\n\ndataset = {\n    'train_curated': dataset_dir / 'train_curated',\n    'train_noisy': dataset_dir / 'train_noisy',\n    'test': dataset_dir / 'test',\n}\n\nmels = {\n    'train_curated': preprocessed_dir / 'mels_train_curated.pkl',\n    'train_noisy': preprocessed_dir / 'mels_trn_noisy_best50s.pkl',\n    'test': preprocessed_dir / 'mels_test.pkl',  # NOTE: this data doesn't work at 2nd stage\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(csvs['trn_curated_trimmed'])# pd.concat([pd.read_csv(csvs['trn_curated_trimmed']), pd.read_csv(csvs['train_noisy'])], sort=True, ignore_index=True)\n#train_noisy = pd.read_csv(csvs['train_noisy'])\n#train_df = pd.concat([train_curated, train_noisy], sort=True, ignore_index=True)\n#train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(csvs['sample_submission'])\n#test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = test_df.columns[1:].tolist()\n#labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = len(labels)\n#num_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.zeros((len(train_df), num_classes)).astype(int)\nfor i, row in enumerate(train_df['labels'].str.split(',')):\n    for label in row:\n        idx = labels.index(label)\n        y_train[i, idx] = 1\n\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open(mels['train_curated'], 'rb') as curated, open(mels['train_noisy'], 'rb') as noisy:\n#     x_train = pickle.load(curated)\n#     x_train.extend(pickle.load(noisy))\nwith open(mels['train_curated'], 'rb') as curated:\n    x_train = pickle.load(curated)\n\nwith open(mels['test'], 'rb') as test:\n    x_test = pickle.load(test)\n    \nlen(x_train), len(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spec_augment(spec: np.ndarray, num_mask=2, \n                 freq_masking_max_percentage=0.3, time_masking_max_percentage=0.3):\n\n    spec = spec.copy()\n    for i in range(num_mask):\n        all_frames_num, all_freqs_num = spec.shape\n        freq_percentage = random.uniform(0.0, freq_masking_max_percentage)\n        \n        num_freqs_to_mask = int(freq_percentage * all_freqs_num)\n        f0 = np.random.uniform(low=0.0, high=all_freqs_num - num_freqs_to_mask)\n        f0 = int(f0)\n        spec[:, f0:f0 + num_freqs_to_mask] = 0\n\n        time_percentage = random.uniform(0.0, time_masking_max_percentage)\n        \n        num_frames_to_mask = int(time_percentage * all_frames_num)\n        t0 = np.random.uniform(low=0.0, high=all_frames_num - num_frames_to_mask)\n        t0 = int(t0)\n        spec[t0:t0 + num_frames_to_mask, :] = 0\n    \n    return spec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FATTrainDataset(Dataset):\n    def __init__(self, mels, labels, transforms, crop=True):\n        super().__init__()\n        self.mels = mels\n        self.labels = labels\n        self.transforms = transforms\n        self.crop = crop\n        \n    def __len__(self):\n        return len(self.mels)\n    \n    def __getitem__(self, idx):\n        mel = self.mels[idx]\n        base_dim, time_dim = mel.shape\n        max_masks = max(2, time_dim // base_dim)\n        mel = spec_augment(mel, num_mask=random.randint(0, max_masks))\n        if self.crop:\n            crop = random.randint(0, time_dim - base_dim)\n            mel = mel[:, crop:crop + base_dim]\n        mel = self.transforms(mel)\n        \n        label = self.labels[idx]\n        label = torch.from_numpy(label).float()\n        \n        return mel, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FATTestDataset(Dataset):\n    def __init__(self, fnames, mels, transforms, crop=False):\n        super().__init__()\n        self.fnames = fnames\n        self.mels = mels\n        self.transforms = transforms\n        self.crop = crop\n        \n    def __len__(self):\n        return len(self.fnames)\n    \n    def __getitem__(self, idx):\n        new_idx = idx % len(self.fnames)\n        \n        mel = self.mels[new_idx]\n        base_dim, time_dim = mel.shape\n        max_masks = max(2, time_dim // base_dim)\n        mel = spec_augment(mel, num_mask=random.randint(0, max_masks))\n        if self.crop:\n            crop = random.randint(0, time_dim - base_dim)\n            mel = mel[:, crop:crop + base_dim]\n        mel = self.transforms(mel)\n\n        fname = self.fnames[new_idx]\n        \n        return mel, fname","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_dict = {\n    'train': transforms.Compose([\n        #transforms.RandomHorizontalFlip(0.5),\n        transforms.ToTensor(),\n    ]),\n    'test': transforms.Compose([\n        #transforms.RandomHorizontalFlip(0.5),\n        transforms.ToTensor(),\n    ]),\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * max(1.0, width_mult))\n        features = [ConvBNReLU(1, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n\n        # building classifier\n        self.classifier = nn.Linear(self.last_channel, num_classes)\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.mobilenet = MobileNetV2(num_classes=num_classes)\n\n    def forward(self, x):\n        return self.mobilenet(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifier(num_classes=num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_1cycle_schedule(lr_max=1e-3, n_data_points=8000, epochs=200, batch_size=40):          \n    \"\"\"\n    Creates a look-up table of learning rates for 1cycle schedule with cosine annealing\n    See @sgugger's & @jeremyhoward's code in fastai library: https://github.com/fastai/fastai/blob/master/fastai/train.py\n    Wrote this to use with my Keras and (non-fastai-)PyTorch codes.\n    Note that in Keras, the LearningRateScheduler callback (https://keras.io/callbacks/#learningratescheduler) only operates once per epoch, not per batch\n      So see below for Keras callback\n\n    Keyword arguments:\n    lr_max            chosen by user after lr_finder\n    n_data_points     data points per epoch (e.g. size of training set)\n    epochs            number of epochs\n    batch_size        batch size\n    Output:  \n    lrs               look-up table of LR's, with length equal to total # of iterations\n    Then you can use this in your PyTorch code by counting iteration number and setting\n          optimizer.param_groups[0]['lr'] = lrs[iter_count]\n    \"\"\"\n    pct_start, div_factor = 0.3, 25.        # @sgugger's parameters in fastai code\n    lr_start = lr_max/div_factor\n    lr_end = lr_start/1e4\n    n_iter = n_data_points * epochs // batch_size     # number of iterations\n    a1 = int(n_iter * pct_start)\n    a2 = n_iter - a1\n\n    # make look-up table\n    lrs_first = np.linspace(lr_start, lr_max, a1)            # linear growth\n    lrs_second = (lr_max-lr_end)*(1+np.cos(np.linspace(0,np.pi,a2)))/2 + lr_end  # cosine annealing\n    lrs = np.concatenate((lrs_first, lrs_second))\n    return lrs\n\nclass OneCycleScheduler(_LRScheduler):\n    \"\"\"My modification of Keras' Learning rate scheduler to do 1Cycle learning\n       which increments per BATCH, not per epoch\n    Keyword arguments\n        **kwargs:  keyword arguments to pass to get_1cycle_schedule()\n        Also, verbose: int. 0: quiet, 1: update messages.\n        \n    Sample usage (from my train.py):\n        lrsched = OneCycleScheduler(lr_max=1e-4, n_data_points=X_train.shape[0], epochs=epochs, batch_size=batch_size, verbose=1)\n    \"\"\"\n    def __init__(self, optimizer, **kwargs):\n        self.lrs = get_1cycle_schedule(**kwargs)\n        super(OneCycleScheduler, self).__init__(optimizer)\n    \n    def get_lr(self):\n        return [self.lrs[self.last_epoch % len(self.lrs)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CyclicLR(_LRScheduler):\n    \"\"\"Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `step` should be called after a batch has been used for training.\n\n    This class has three built-in policies, as put forth in the paper:\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        base_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for each parameter group.\n        max_lr (float or list): Upper learning rate boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size_up (int): Number of training iterations in the\n            increasing half of a cycle. Default: 2000\n        step_size_down (int): Number of training iterations in the\n            decreasing half of a cycle. If step_size_down is None,\n            it is set to step_size_up. Default: None\n        mode (str): One of {triangular, triangular2, exp_range}.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n            Default: 'triangular'\n        gamma (float): Constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n            Default: 1.0\n        scale_fn (function): Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            If specified, then 'mode' is ignored.\n            Default: None\n        scale_mode (str): {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle).\n            Default: 'cycle'\n        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n            to learning rate between 'base_momentum' and 'max_momentum'.\n            Default: True\n        base_momentum (float or list): Initial momentum which is the\n            lower boundary in the cycle for each parameter group.\n            Default: 0.8\n        max_momentum (float or list): Upper momentum boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (max_momentum - base_momentum).\n            The momentum at any cycle is the difference of max_momentum\n            and some scaling of the amplitude; therefore\n            base_momentum may not actually be reached depending on\n            scaling function. Default: 0.9\n        last_epoch (int): The index of the last batch. This parameter is used when\n            resuming a training job. Since `step()` should be invoked after each\n            batch instead of after each epoch, this number represents the total\n            number of *batches* computed, not the total number of epochs computed.\n            When last_epoch=-1, the schedule is started from the beginning.\n            Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         train_batch(...)\n        >>>         scheduler.step()\n\n\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    \"\"\"\n\n    def __init__(self,\n                 optimizer,\n                 base_lr,\n                 max_lr,\n                 step_size_up=2000,\n                 step_size_down=None,\n                 mode='triangular',\n                 gamma=1.,\n                 scale_fn=None,\n                 scale_mode='cycle',\n                 cycle_momentum=True,\n                 base_momentum=0.8,\n                 max_momentum=0.9,\n                 last_epoch=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        base_lrs = self._format_param('base_lr', optimizer, base_lr)\n        if last_epoch == -1:\n            for lr, group in zip(base_lrs, optimizer.param_groups):\n                group['lr'] = lr\n\n        self.max_lrs = self._format_param('max_lr', optimizer, max_lr)\n\n        step_size_up = float(step_size_up)\n        step_size_down = float(step_size_down) if step_size_down is not None else step_size_up\n        self.total_size = step_size_up + step_size_down\n        self.step_ratio = step_size_up / self.total_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.cycle_momentum = cycle_momentum\n        if cycle_momentum:\n            if 'momentum' not in optimizer.defaults:\n                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n\n            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n            if last_epoch == -1:\n                for momentum, group in zip(base_momentums, optimizer.param_groups):\n                    group['momentum'] = momentum\n        self.base_momentums = list(map(lambda group: group['momentum'], optimizer.param_groups))\n        self.max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n\n        super(CyclicLR, self).__init__(optimizer, last_epoch)\n\n    def _format_param(self, name, optimizer, param):\n        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n        if isinstance(param, (list, tuple)):\n            if len(param) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} values for {}, got {}\".format(\n                    len(optimizer.param_groups), name, len(param)))\n            return param\n        else:\n            return [param] * len(optimizer.param_groups)\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        \"\"\"Calculates the learning rate at batch index. This function treats\n        `self.last_epoch` as the last batch index.\n\n        If `self.cycle_momentum` is ``True``, this function has a side effect of\n        updating the optimizer's momentum.\n        \"\"\"\n        cycle = math.floor(1 + self.last_epoch / self.total_size)\n        x = 1. + self.last_epoch / self.total_size - cycle\n        if x <= self.step_ratio:\n            scale_factor = x / self.step_ratio\n        else:\n            scale_factor = (x - 1) / (self.step_ratio - 1)\n\n        lrs = []\n        for base_lr, max_lr in zip(self.base_lrs, self.max_lrs):\n            base_height = (max_lr - base_lr) * scale_factor\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_epoch)\n            lrs.append(lr)\n\n        if self.cycle_momentum:\n            momentums = []\n            for base_momentum, max_momentum in zip(self.base_momentums, self.max_momentums):\n                base_height = (max_momentum - base_momentum) * scale_factor\n                if self.scale_mode == 'cycle':\n                    momentum = max_momentum - base_height * self.scale_fn(cycle)\n                else:\n                    momentum = max_momentum - base_height * self.scale_fn(self.last_epoch)\n                momentums.append(momentum)\n            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n                param_group['momentum'] = momentum\n\n        return lrs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass AdamW(Optimizer):\n    \"\"\"Implements AdamW algorithm.\n\n    It has been proposed in `Fixing Weight Decay Regularization in Adam`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\n    .. Fixing Weight Decay Regularization in Adam:\n    https://arxiv.org/abs/1711.05101\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay)\n        super(AdamW, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('AdamW does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # according to the paper, this penalty should come after the bias correction\n                # if group['weight_decay'] != 0:\n                #     grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                if group['weight_decay'] != 0:\n                    p.data.add_(-group['weight_decay'], p.data)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mixup_data(x, y, alpha=1.0, use_cuda=True):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(x_trn, x_val, y_trn, y_val, train_transforms, model, num_epochs=100, lr=1e-3, batch_size=71, fine_tune=False, weight_file_name='weight_best.pt'):\n    test_batch_size = batch_size\n    eta_min = 1e-7\n    t_max = 20\n    mixup_alpha = 1.0\n    validation_tta = 10\n    \n    num_classes = y_train.shape[1]\n    \n    train_dataset = FATTrainDataset(x_trn, y_trn, train_transforms)\n    valid_dataset = FATTrainDataset(x_val, y_val, train_transforms)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=test_batch_size, shuffle=False)\n\n    criterion = nn.BCEWithLogitsLoss().cuda()\n    if fine_tune:\n        optimizer = AdamW(params=model.parameters(), lr=lr, weight_decay=lr/10)\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', verbose=True)\n    else:\n        #optimizer = AdamW(params=model.parameters(), lr=lr, weight_decay=lr/10)\n        optimizer = SGD(params=model.parameters(), lr=lr, weight_decay=lr/10)\n        #scheduler = OneCycleScheduler(optimizer, lr_max=lr, n_data_points=len(x_trn), epochs=num_epochs, batch_size=batch_size)\n        scheduler = CyclicLR(optimizer, base_lr=eta_min, max_lr=lr, cycle_momentum=True)\n    \n\n    best_epoch = -1\n    best_lwlrap = 0.\n    best_score = np.zeros((num_classes,))\n    best_weight = np.zeros((num_classes,))\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.\n\n        for x_batch, y_batch in train_loader:\n            x_batch = x_batch.cuda()\n            y_batch = y_batch.cuda()\n            \n            x_batch, targets_a, targets_b, lam = mixup_data(x_batch, y_batch, mixup_alpha)\n            x_batch, targets_a, targets_b = map(Variable, (x_batch, targets_a, targets_b))\n\n            preds = model(x_batch)\n            loss = mixup_criterion(criterion, preds, targets_a, targets_b, lam)\n            #loss = criterion(preds, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            if not fine_tune:\n                scheduler.step()\n\n            avg_loss += loss.item() / len(train_loader)\n\n        model.eval()\n        valid_preds = np.zeros((len(x_val), num_classes))\n        avg_val_loss = 0.\n\n        for _ in range(0, validation_tta):\n            for i, (x_batch, y_batch) in enumerate(valid_loader):\n                preds = model(x_batch.cuda()).detach()\n                loss = criterion(preds, y_batch.cuda())\n\n                preds = torch.sigmoid(preds)\n                valid_preds[i * test_batch_size: (i+1) * test_batch_size] = preds.cpu().numpy()\n\n                avg_val_loss += (loss.item() / len(valid_loader)) / validation_tta\n            \n        score, weight = calculate_per_class_lwlrap(y_val, valid_preds)\n        lwlrap = (score * weight).sum()\n        \n        log_epoch = epoch % 5 == 0\n        if lwlrap > best_lwlrap:\n            log_epoch = True\n            best_epoch = epoch + 1\n            best_lwlrap = lwlrap\n            best_score = score\n            best_weight = weight\n            torch.save(model.state_dict(), weight_file_name)\n            \n        if log_epoch:\n            elapsed = time.time() - start_time\n            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n            \n        if fine_tune:\n            scheduler.step(avg_val_loss)\n#         else:\n#             scheduler.step()\n            \n    return {\n        'best_epoch': best_epoch,\n        'best_lwlrap': best_lwlrap,\n        'best_score': best_score,\n        'best_weight': best_weight,\n        'weight_file_name' : weight_file_name,\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pick_best_result(result1, result2):\n    if result2['best_lwlrap'] > result1['best_lwlrap']:\n        return result2\n    else:\n        return result1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/fat2019models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DataLoader(FATTestDataset(test_df['fname'], x_test, transforms_dict['test'], crop=True), batch_size=256, shuffle=False)\n\nkf = KFold(n_splits=5, random_state=SEED, shuffle=True)\npredictions = []\nresults = []\nfor train_index, test_index in kf.split(np.arange(len(train_df))):\n    fold = str(len(results) + 1)\n    \n    x_trn = list(itemgetter(*train_index)(x_train))\n    y_trn = np.array(list(itemgetter(*train_index)(y_train)))\n    \n    x_val = list(itemgetter(*test_index)(x_train))\n    y_val = np.array(list(itemgetter(*test_index)(y_train)))\n    \n    print(\"Stage 1:\")\n    model = Classifier(num_classes=num_classes)\n    model.load_state_dict(torch.load('../input/fat2019models/' + fold + '_mobilenet_stage4.pt'))\n    best_result = train_model(x_trn, x_val, y_trn, y_val, transforms_dict['train'], model.cuda(), num_epochs=100, lr=5e-5, batch_size=100, weight_file_name=fold + '_mobilenet_stage0.pt')\n\n    print(\"Stage 2:\")\n    model = Classifier(num_classes=num_classes)\n    model.load_state_dict(torch.load(best_result['weight_file_name']))\n    best_result = train_model(x_trn, x_val, y_trn, y_val, transforms_dict['train'], model.cuda(), num_epochs=100, lr=1e-5, fine_tune=True, batch_size=100, weight_file_name=fold + '_mobilenet_stage4.pt')\n\n    results.append(best_result)\n    \n    print(\"Predicting using:\")\n    print(best_result)\n    model = Classifier(num_classes=num_classes)\n    model.load_state_dict(torch.load(best_result['weight_file_name']))\n    model.cuda()\n    model.eval()\n    \n    for _ in range(0, 20):\n        all_outputs = []\n        for images, _ in test_loader:\n            preds = torch.sigmoid(model(images.cuda()).detach())\n            all_outputs.append(preds.cpu().numpy())\n        fold_preds = np.concatenate(all_outputs)\n        predictions.append(fold_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best results:\")\nprint(results)\n\nfor idx, val in enumerate(results):\n    df = pd.DataFrame({'labels': labels, 'scores': results[idx]['best_score'], 'weights': results[idx]['best_weight']}, columns=['labels', 'scores', 'weights'])\n    df.to_csv(str(idx + 1) + 'scores.csv', encoding='utf-8', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[labels] = np.mean(np.array(predictions), axis=0)\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}