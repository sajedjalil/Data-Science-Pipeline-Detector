{"cells":[{"metadata":{},"cell_type":"markdown","source":"This year I transitioned from being an aspiring  Data Scientist(a student) to an actual one. When I was preparing to enter this field ,I had a lot of questions about Data Scientists. Like, What do they look like? Where do they come from? ðŸ˜‚ðŸ˜‚ Just kidding."},{"metadata":{},"cell_type":"markdown","source":"The questions I really pondered on were:-\n> * How important is it to have a Master's Degree(given the fact that I only had Bachelor's) ?\n> * What does a Data Scientist actually spends most of his time on during the job?\n> * How often do freshers get to work in big companies as Data Scientists? \n\nAnd many more like these.\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.smartdatacollective.com/wp-content/uploads/2018/01/Data-Scientists.jpg)\n*Source:- smartdatacollective.com*"},{"metadata":{},"cell_type":"markdown","source":"Kaggle is a huge platform for newbies like me and I'm sure they wonder the same.In this kernel I venture to answer those questions with the most powerful weapon in hand , **DATA**."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom plotly import tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', None)\nprint(os.listdir(\"../input\"))\npd.set_option('display.max_columns', None)\nimport operator\nimport numpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mcq=pd.read_csv('/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv')\nquestion=pd.read_csv('/kaggle/input/kaggle-survey-2019/questions_only.csv')\ntext_response=pd.read_csv('/kaggle/input/kaggle-survey-2019/other_text_responses.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"col=question.columns\nfor i in range(question.shape[1]):\n    \n    print(i,question[col[i]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thankfully, Data Scientists form the largest group of participants. So, I'll just filter them out using the job role."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"c= pd.value_counts(mcq.Q5)\nd = pd.DataFrame(data=c)\nd.columns=['count']\nd=d.iloc[:9,:].sort_values('count',ascending=True)\nd['count'] = pd.to_numeric(d['count'], errors='coerce')\n\ndata=[go.Bar(\n            x=d[\"count\"],\n            y=d.index,\n             marker=dict(\n             color=['#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#142270'],\n             line=dict(\n            color=['#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#8e7bbb','#142270'],\n            width=1),\n                        ),\n            orientation='h' )]\nlayout = go.Layout(\n    height=500,\n    autosize=True,\n    title='Job role wise contribution in the survey',\n    hovermode='closest',\n    xaxis=dict(title='', ticklen=5, zeroline=False, gridwidth=2, domain=[0.2, 1]),\n    yaxis=dict(title='', ticklen=5, gridwidth=10),\n    \n)\n\nfig = go.Figure(data=data,layout=layout)\npy.offline.iplot(fig, filename='horizontal-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df=mcq[mcq['Q5']=='Data Scientist']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The main aim of this kernel is to analyze behavior patterns of Data Scientists and the marketplace trends for Data Scientists using the 2019 Kaggle Survey Data, to somehow(hopefully) help aspiring Data Scientists achieve their goal.**\n\nThe conclusions that I draw from the data are based on the assumption that the survey data is a sample of the whole population (i.e. not entirely captured by the survey).\n\nI'll break down the analysis in 4 parts:-\n\n* **Characterisitics and Demographics of Data Scientists**\n* **Common Practices and Preferences of Data Scientists**\n* **Common Practices followed by Companies**\n* **What determines your salary as a Data Scientist?**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Characteristics/ Demographics\nLet's look at the demographics or characteristics first, like Country, Gender , age , highest education etc.\nAlthough you can't work your way through most of these, but it is crucial in understanding our data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"country_dist=df['Q3'].value_counts()\nfig = px.choropleth(country_dist.values, locations=country_dist.index,\n                    locationmode='country names',\n                    color=country_dist.values,\n                    color_continuous_scale=px.colors.sequential.OrRd)\nfig.update_layout(title=\"Countrywise Distribution of data scientists\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">*  Most of the Data Scientists in the Survey come from USA and India, like the last two years.They are \nfollowed by Brazil and Russia. \n* We have just 5 countries from the African Sub-continent, with very low participation. The other countries because of very low responses must be grouped in Others.\n* The same goes for South America,except for Brazil, there is very low participation in the survey.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"gender=df['Q2'].value_counts()\nage=['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70+']\nvalue=[218,616,1134,871,529,292,157,132,72,51,13]\ntrace0=go.Bar(x=gender.index,\n              y=gender.values,\n              name='',\n             marker=dict(\n             color='#03396c '),showlegend=False)\ntrace1=go.Scatter(x=age,\n                 y=value,\n                 name=' ',\n                   marker=dict(\n             color='#2c2f33'),showlegend=False)\ncolors = ['#1a472a', '#d62d20', '#5d5d5d','#8c564b','#008080','#800000','#808000'] \ncounts = df['Q4'].value_counts(sort=True)\nlabels = counts.index\nvalues = counts.values\n\ntrace2= go.Pie(labels=labels, values=values, marker=dict(colors=colors))\nfig = tools.make_subplots(rows=2, cols=2, specs=[[{\"type\": \"bar\"},{\"type\": \"pie\", \"rowspan\": 2}],\n           [ {\"type\": \"scatter\"},None]],\n                          subplot_titles=('Gender','Highest Education','Age'))\n\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\n\nfig['layout'].update( height=700,width=1200,margin=go.layout.Margin(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ))\npy.offline.iplot(fig)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * **Male Data Scientits are more than 5 times their female counterparts**. This leads to *bias* in machine learning. ML algorithms are representative of the data they eat and the people who build them. An interesting incident can be cited here, back in 2014 Amazon trained a ML system to hire people and it was unreasonably throwing away women's resumes. A further study in the problem found out that the Consequently, the AI concluded that men were preferable. It downgraded resumes containing the words \"women's\",and filtered out candidates who'd attended two women's only colleges.[[1]](https://www.businessinsider.in/tech/amazon-built-an-ai-to-hire-people-but-reportedly-had-to-shut-it-down-because-it-was-discriminating-against-women/articleshow/66148420.cms)\n* **Most of the population is in their late twenties**. We find fewer and fewer people in Data Science as we move away from this age group. This is probably because Data Science is a relatively new field, the older generation does not know about it , let alone be skilled at it. However, I believe this trend will shift with time.\n* Having work environments more inclusive of women and the elderly will help us to create better and more diverse models.\n* It seems like having a Master's is quite common for a Data Scientist. So ,getting a Master's seems like a safe bet. But is it really necessary to invest that amount of time and money? I think this question is better answered in [Shivan Bansal's notebook](https://www.kaggle.com/shivamb/spending-for-ms-in-data-science-worth-it)."},{"metadata":{},"cell_type":"markdown","source":"# Common Practices and Preferences of Data Scientists\nLet's start with preferences first. Getting to know which MOOC platforms or Blogs most of the Data Scientists prefer can help you steer your learning process in the right direction.\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mooc={}\nfor i in range(12):\n    value=df['Q13_Part_'+str(i+1)].value_counts().values[0]\n    text=df['Q13_Part_'+str(i+1)].value_counts().index[0]\n    mooc[text]=value\nmooc=dict( sorted(mooc.items(), key=operator.itemgetter(1)))\n    \npub={}\nfor i in range(12):\n    value=df['Q12_Part_'+str(i+1)].value_counts().values[0]\n    text=df['Q12_Part_'+str(i+1)].value_counts().index[0]\n    pub[text]=value\npub=dict( sorted(pub.items(), key=operator.itemgetter(1)))\n    \ntrace0=go.Bar(x=list(mooc.values()),\n              y=list(mooc.keys()),\n              name='',\n             marker=dict(\n             color='#03396c '),showlegend=False,orientation='h')\ntrace1=go.Bar(x=list(pub.values()),\n              y=list(pub.keys()),\n              name='',\n             marker=dict(\n             color='#03396c '),showlegend=False,orientation='h')\n\nfig = tools.make_subplots(rows=2, cols=1, specs=[[{}],[{}]],\n                          subplot_titles=('MOOC(Massive Open Online Courses)','Media Source'))\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\n\n\n\nfig['layout'].update( height=700,width=900,margin=go.layout.Margin(\n        l=50,\n        r=50,\n        b=50,\n        t=50,\n        pad=4\n    ))\npy.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">* Coursera seems like the obvious choice of Data Scientists,maybe becuase of the quality of content and assignments over there. I personally like it too. There is also an increasing trend in University Courses and it's the second most  prefered thing.\n* When it comes to  favorite Media sources for DS related topics , Medium has a clear edge followed by Kaggle.Probably because a lot of Data Scientists themselves publish there articles on Medium, it seems like a credible source. Adding to this there is a huge variety of Data Science stuff that can be found on Medium from collecting data to deploying models.\n* Kaggle is the second most popular information resource, because of it's content and probably because it's fully free. They won't ask you to make a subscription after you have reached your monthly limit.\n\nI'm not plotting the most common programming language among Data Scientits because python always comes as the clear winner. So, just go with it. There are a whole lot of Machine Learning and Deep Learning libraries in python, plus it is also easy to use.\n\nLet's move on to the next question now.\n\n### What activity makes up most of their time at work?\n\nAs aspiring Data Scientists people spend most of their time building models but this is evidently not the case in actual Data Science scenarios. Data Scientists spend most of their times analyzing and cleaning data which is confirmed by this survey."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x=[]\ny=[]\nfor i in range(8):\n   \n    value=df['Q9_Part_'+str(i+1)].value_counts().values[0]\n    text=df['Q9_Part_'+str(i+1)].value_counts().index[0]\n    x.append(text)\n    y.append(value)\nx_new=['Analyze data for business decision','Build/run data infrastructure','Build prototypes to add ML to new areas','Build/run ML service that improves product','Iterartion to improve existing model','Reserach to advance stste-of-the-art','None','Other']   \nx_new=numpy.array(x_new)\ny=numpy.array(y)\ninds = y.argsort()\nsorted_xnew = x_new[inds]\nsorted_y=y[inds]\n\ntrace0 = go.Bar(\n    x=sorted_xnew[::-1],\n    y=sorted_y[::-1],\n    marker=dict(\n        color='#420666'\n    ),\n    opacity=0.6  \n)\n\ndata = [trace0]\nlayout = go.Layout(\n    title='Activities that make up most of their time at work',\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(\n    margin=dict(l=50, r=50, t=100, b=100),height=500,width=700\n)\npy.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">* Analyzing data to find insights that drive decisions and finding places to apply ML models seems to be the most time consuming process. \n* These days every other company wants to use Data Science to generate more revenue and help their business grow. But deciding where actually Data Science can do wonders, is the job of a Data Scientist. To quote someone,pardon me because I don't remember the source *\"A good Data Scientist knows where to apply Machine Learning and a great Data Scientist knows where not to apply Machine Learning.\"*\n* It's a huge responsibility to put a Machine Learning model into production and believe me the real game actually starts once you have trained and deployed the model. When it goes live unanticipated scenarios are encountered and changes have to be made. Therefore, Data Scientists spend most of their time in improving existing models.\n* Data Scientists are least interested in improving the state of the art, that's the job of a Research Scietist.\n* **Note**:- Spending more time on finding insights and looking out for areas where Machine Learning can be applied can prepare you better for the job.\n\n**Since analayzing data seems like the most widely performed activity, let's find out how Data Scientists do it.**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"years=['<1 years','1-2 years','3-5 years','5-10 years','10-20 years','20+ years']\nvalues=[290,700,1124,741,369,154]\n\ntool=df['Q14'].value_counts().index\nvalue2=df['Q14'].value_counts().values\n\nvis=[]\nvalue3=[]\nfor i in range(12):\n    value=df['Q20_Part_'+str(i+1)].value_counts().values[0]\n    text=df['Q20_Part_'+str(i+1)].value_counts().index[0]\n    vis.append(text)\n    value3.append(value)\n\n\ntrace0 = go.Bar(\n    x = years,\n    y =values, showlegend = False,\n    marker=dict(\n        color='#008080'\n    )\n)\ntool=['Local development environments','Cloud-based data software & APIs','Basic statistical software','Advanced statistical software','Business intelligence software']\ntrace1 = go.Bar(\n    x = tool,\n    y =value2, showlegend = False,\n    marker=dict(\n        color='#006666'\n    )\n)\ntrace2 = go.Scatter(\n    x = vis,\n    y =value3,showlegend = False,\n    mode = 'lines+markers',\n  \n    line= dict( color= \"#004c4c\")\n)\n\n\nfig = tools.make_subplots(rows=2, cols=2, specs=[[{\"type\": \"bar\"},{\"type\": \"bar\"}],\n           [ {\"type\": \"scatter\",\"colspan\":2},None]],\n                          subplot_titles=('Years spent writing code to Analyze data','Tools used to analyze ','Vis library'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update( height=700,width=900,margin=go.layout.Margin(\n        l=50,\n        r=50,\n        b=100,\n        t=100,\n        pad=4\n    ))\npy.offline.iplot(fig)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * Okay, so most Data Scientists have 3-5 years of experience writing code to ananlyze data.\n* Local development environments are widely used (Jupyter and RStudio),almost  more than 4 times of the other tools.\n* No Surprises that Tableau and other BI softwares are not popular among Data Scientists, because they are primarily used by Business Analysts .\n* Regarding the Visualization Libraries, Matplolib comes as a winner and most of the other libraries(Seaborn and Plotly) are wrappers built on it's powerful internals.\n\n#### Mostly used hosted notebook products against years of experience."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ml_exp=pd.concat([pd.crosstab(df['Q17_Part_1'],df['Q23']),pd.crosstab(df['Q17_Part_2'],df['Q23']),pd.crosstab(df['Q17_Part_3'],df['Q23']),pd.crosstab(df['Q17_Part_4'],df['Q23']),pd.crosstab(df['Q17_Part_5'],df['Q23']),pd.crosstab(df['Q17_Part_6'],df['Q23']),pd.crosstab(df['Q17_Part_7'],df['Q23']),pd.crosstab(df['Q17_Part_8'],df['Q23']),pd.crosstab(df['Q17_Part_9'],df['Q23']),pd.crosstab(df['Q17_Part_10'],df['Q23']),pd.crosstab(df['Q17_Part_11'],df['Q23']),pd.crosstab(df['Q17_Part_12'],df['Q23'])])\nml_exp=ml_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\nml_exp=ml_exp.fillna(0)\ntrace1 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['< 1 years'].values,\n    name='< 1 years',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['1-2 years'].values,\n    name='1-2 years',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['2-3 years'].values,\n    name='2-3 years',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['3-4 years'].values,\n    name='3-4 years',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['4-5 years'].values,\n    name='4-5 years',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['5-10 years'].values,\n    name='5-10 years',\n    marker=dict(\n    color='#4d0e20')\n)\ntrace7 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['10-15 years'].values,\n    name='10-15 years',\n    marker=dict(\n    color='#b35a00')\n)\n\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = go.Layout(\n    barmode='group',height=600,width=1200,title=' Hosted notebook product used against years of experience',yaxis_title='Number of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * Kaggle Notebooks and Google Colab are the most widely used notebook products because of the good computation power provided by them with GPUs(all for free with a limit). Another biggest advantage over traditional environments is there is no need to download and setup libraries and I think most people would agree it's time consuming and awfully boring stuff.\n* Still a lot of people prefer to work in the traditional way on their local systems and there are more experienced people in this category.\n* Kaggle notebooks and Google Colab are mostly used by Data Scientists for competitions and independent projects and hence we have more of the younger generation using these.\n* People do not use these for company work and rather go for local systems and if the ML infrastructure of the company is huge they go for paid products like AWS and GCP, and hence we can see a lot of the older generation using these.\n\n#### ML algo used on a regular basis vs years of experience"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ml_exp=pd.concat([pd.crosstab(df['Q24_Part_1'],df['Q23']),pd.crosstab(df['Q24_Part_2'],df['Q23']),pd.crosstab(df['Q24_Part_3'],df['Q23']),pd.crosstab(df['Q24_Part_4'],df['Q23']),pd.crosstab(df['Q24_Part_5'],df['Q23']),pd.crosstab(df['Q24_Part_6'],df['Q23']),pd.crosstab(df['Q24_Part_7'],df['Q23']),pd.crosstab(df['Q24_Part_8'],df['Q23']),pd.crosstab(df['Q24_Part_9'],df['Q23']),pd.crosstab(df['Q24_Part_10'],df['Q23']),pd.crosstab(df['Q24_Part_11'],df['Q23']),pd.crosstab(df['Q24_Part_12'],df['Q23'])])\nml_exp=ml_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\nml_exp=ml_exp.fillna(0)\ntrace1 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['< 1 years'].values,\n    name='< 1 years',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['1-2 years'].values,\n    name='1-2 years',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['2-3 years'].values,\n    name='2-3 years',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['3-4 years'].values,\n    name='3-4 years',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['4-5 years'].values,\n    name='4-5 years',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['5-10 years'].values,\n    name='5-10 years',\n    marker=dict(\n    color='#4d0e20')\n)\ntrace7 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['10-15 years'].values,\n    name='10-15 years',\n    marker=dict(\n    color='#b35a00')\n)\n\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = go.Layout(\n    barmode='group',height=600,width=1200,title='ML algo used on regular basis against years of experience',yaxis_title='Number of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deductions:-\n> * Looking at the ML algorithms used on a Daily basis, I can say most of the Data Scientists work on machine learning as is evident by the number of people using Linear/Logistic Regression, Decision Trees  and Boosting methods.\n* I think most Data Scientits would agree that a lot of problems can be solved by Linear algorithms, if not by them then by using trees. Jumping on neural networks without exhausting ML algorithms for classification/regression problems is always avoided by Data scientits. More you move towards a complex model, the more assumptions it make. It might give you better performance but debugging becomes trickier. One should never underestimate the power of simple ML algorithms.\n* A smaller subset of Data Scientists deal with Deep Learning problems and most of them work on Computer Vision(image data) problems, seen through the Convolutional Neural Network bars.\n* Only a few work on NLP problems(text data) and Recurrent Neural Networks are more popular amongst the younger generation while Transformers could be equally or more popular with the older population( I can't be sure because we have unequal number of samples for each age group).\n* I know there could be an overlap between Data Scientists using ML algorithms of all the three types(ML,NLP,CV), but that would rarely happen given the question asks on a *\"regular basis\"*.\n\nLet's also look at other ML algorithms not provided as options in the survey."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"text_response['count'] = 1\ntext_response['ML_algo'] = text_response['Q24_OTHER_TEXT'].str.lower()\ntext_response.drop(0)[['ML_algo','count']].groupby('ML_algo').sum()[['count']].sort_values('count', ascending=False)\n\n# Create wordcloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nplt.figure(figsize=[15,8])\n\n# Create and generate a word cloud image:\nide_words = ' '.join(text_response['ML_algo'].drop(0).dropna().values)\nwordcloud = WordCloud(colormap=\"tab10\",\n                      width=1200,\n                      height=480,\n                      normalize_plurals=False,\n                      background_color=\"white\",\n                      random_state=5).generate(ide_words)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVM and clustering algorithms seem like popular ones. In deep learning I can see neural networks and reinforcement learning but ML algorithms dominate here too.\n\nIf you are interested in Deep Learning and would want to work in the field, let's look at some common practices in those fields exclusively.I'll start with Computer Vision first.\n\nWhile filtering people for CV or NLP, I won't consider Dense Networks for algorithms as it is rather ambigous, whether a person using them works on NLP or CV or ML(tabular data). For CV, I consider CNN and GANs and for NLP, I take RNNs and Transformers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## tagging practitioner types\ndf[\"DS_field\"] = \"ML\"\n\ndf.loc[~( df.Q24_Part_7.isna() & df.Q24_Part_8.isna()), \"DS_field\"] = \"Computer Vision\"\ndf.loc[~( df.Q24_Part_9.isna() & df.Q24_Part_10.isna()), \"DS_field\"] = \"NLP\"\ndf_cv=df[df['DS_field']=='Computer Vision']\ndf_nlp=df[df['DS_field']=='NLP']\n\ncv_name=[]\ncv_count=[]\nfor i in range(7):\n    value=df_cv['Q26_Part_'+str(i+1)].value_counts().values[0]\n    text=df_cv['Q26_Part_'+str(i+1)].value_counts().index[0]\n    cv_name.append(text)\n    cv_count.append(value)\ncv_new=['General purpose image/video tools','Image segmentation methods','Object detection methods','Image classification and other general purpose networks','Generative Networks','None','Other']\ncv_new=numpy.array(cv_new)\ncv_count=numpy.array(cv_count)\ninds = cv_count.argsort()\nsorted_cv_new = cv_new[inds]\nsorted_y=cv_count[inds]\n    \nnlp={}\nfor i in range(6):\n    value=df['Q27_Part_'+str(i+1)].value_counts().values[0]\n    text=df['Q27_Part_'+str(i+1)].value_counts().index[0]\n    nlp[text]=value\nnlp=dict( sorted(nlp.items(), key=operator.itemgetter(1)))\n    \ntrace0=go.Bar(x=sorted_y,\n              y=sorted_cv_new,\n              name='',\n             marker=dict(\n             color='#03396c '),showlegend=False,orientation='h')\ntrace1=go.Bar(x=list(nlp.values()),\n              y=list(nlp.keys()),\n              name='',\n             marker=dict(\n             color='#03396c '),showlegend=False,orientation='h')\n\nfig = tools.make_subplots(rows=2, cols=1, specs=[[{}],[{}]],\n                          subplot_titles=('Computer Vision Methods(Image data)','NLP Methods(text data)'))\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 2, 1)\n\n\n\nfig['layout'].update( height=700,width=900,margin=go.layout.Margin(\n        l=100,\n        r=50,\n        b=50,\n        t=50,\n        pad=4\n    ))\npy.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Deductions:-\n> * Image Classification methods seem like the most common group of methods used by CV people along with general purpose pretrained models like VGG16, Inception, Resnets etc that could be used in a variety of tasks.\n* General purpose Image/ video tools are used in preprocessing basically therefore are the second most widely used methods.\n* Data Scientists in the Autonomous Vehicle industry extensively use Image segmentation and Object detection.\n* Traditional word embedding techniques like GLoVe and Word2Vec remain the most popular one in the Data Science Industry(Even I work with them).\n* Encoder-decoder models are used in making chatbots and machine translation.\n* Transformer models are slowly catching wind, but I think they are more complex to use.\n* Note:- If you want to enter the CV/NLP field, having worked on an Image classification or text classification model with good understanding of it would give you an edge. It worked for me, it could work for you too.\n\n\n\n# Common practices followed by Companies\nI believe there are certain trends that companies follow in hiring Data Scientists, or deciding which cloud based platform to use or whether to automate some processes or not according to thier need and resource availability.\n\nLet's look at some of these.\n\n#### Role of years of experience in determining the size of the company (and hence the worth) that will hire you.\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"org_exp=pd.crosstab(df['Q6'],df['Q23'])\norg_exp=org_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\n\ntrace1 = go.Bar(\n    x=org_exp.columns,\n    y=org_exp[org_exp.index=='0-49 employees'].values[0],\n    name='0-49 employees',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=org_exp.columns,\n    y=org_exp[org_exp.index=='50-249 employees'].values[0],\n    name='50-249 employees',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=org_exp.columns,\n    y=org_exp[org_exp.index=='250-999 employees'].values[0],\n    name='250-999 employees',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=org_exp.columns,\n    y=org_exp[org_exp.index=='1000-9,999 employees'].values[0],\n    name='1000-9,999 employees',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=org_exp.columns,\n    y=org_exp[org_exp.index=='> 10,000 employees'].values[0],\n    name='> 10,000 employees',\n    marker=dict(\n    color='#00a3e0')\n)\n\ndata = [trace1, trace2,trace3,trace4,trace5]\nlayout = go.Layout(\n    barmode='group',height=600,width=900,title='Company size according to years of experience',yaxis_title='Number of people',xaxis_title='Years of experience'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">* There is an obvious trend here, which seems quite practical too. \n* People with less experience in ML work in a small sized company or startup, as the work experience increases more people shift towards big MNCs.\n* So , as a fresher if you land a job in a big company, consider yourself lucky.\n\nNow,let's take a look at the **Company size vs. the number of people responsible for DS workloads** to analyze what kind of Companies do Data Scientists work in."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"comp_dw=pd.crosstab(df['Q6'],df['Q7'])\ncomp_dw=comp_dw[['0','1-2','3-4','5-9','10-14','15-19','20+']]\ntrace1 = go.Bar(\n    x=comp_dw.columns,\n    y=comp_dw[comp_dw.index=='0-49 employees'].values[0],\n    name='0-49 employees',\n    marker=dict(\n    color=\"#4b3832\")\n)\ntrace2 = go.Bar(\n    x=comp_dw.columns,\n    y=comp_dw[comp_dw.index=='50-249 employees'].values[0],\n    name='50-249 employees',\n    marker=dict(\n    color='#854442')\n)\ntrace3 = go.Bar(\n    x=comp_dw.columns,\n    y=comp_dw[comp_dw.index=='250-999 employees'].values[0],\n    name='250-999 employees',\n    marker=dict(\n    color='#66545e')\n)\ntrace4 = go.Bar(\n    x=comp_dw.columns,\n    y=comp_dw[comp_dw.index=='1000-9,999 employees'].values[0],\n    name='1000-9,999 employees',\n    marker=dict(\n    color='#3c2f2f')\n)\ntrace5 = go.Bar(\n    x=comp_dw.columns,\n    y=comp_dw[comp_dw.index=='> 10,000 employees'].values[0],\n    name='> 10,000 employees',\n    marker=dict(\n    color='#be9b7b')\n)\n\ndata = [trace1, trace2,trace3,trace4,trace5]\nlayout = go.Layout(\n    barmode='stack',height=600,width=900,title='',plot_bgcolor='#fffff4',xaxis_title='No. of people responsible for DS workloads',yaxis_title='Numbe rof people'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.offline.iplot(fig, filename='stacked-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">* The first bar where there are 0 people responsible for DS workload in a company seems a bit strange to me , as a person calling himself a Data Scientist should atleast count himself.\n* In the last bar, the small companies(0-49,50-249 range) employing 20+ Data Scientists must be DS Consulting firms or companies fully based on AI like H20.ai or OpenAI.\n\n> There are two kinds of majorities here. \n* Small companies(0-49 employees) having 1-2 DS people to improve decision making or work on specific tasks like recommender systems, Sentiment Analysis, Sales Prediction etc.\n* MNCs with more than 10k employees having 20+ DS leveraging AI in full capacity like Facebook, Google etc.\n\nIt would also be useful to know which ML product companies use and you can add some experience in them to get you some edge over other contenders."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"org_exp=pd.concat([pd.crosstab(df['Q32_Part_1'],df['Q6']),pd.crosstab(df['Q32_Part_2'],df['Q6']),pd.crosstab(df['Q32_Part_3'],df['Q6']),pd.crosstab(df['Q32_Part_4'],df['Q6']),pd.crosstab(df['Q32_Part_5'],df['Q6']),pd.crosstab(df['Q32_Part_6'],df['Q6']),pd.crosstab(df['Q32_Part_7'],df['Q6']),pd.crosstab(df['Q32_Part_7'],df['Q6']),pd.crosstab(df['Q32_Part_8'],df['Q6']),pd.crosstab(df['Q32_Part_9'],df['Q6']),pd.crosstab(df['Q32_Part_10'],df['Q6']),pd.crosstab(df['Q32_Part_11'],df['Q6']),pd.crosstab(df['Q32_Part_12'],df['Q6'])])\n#org_exp=org_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\n\ntrace1 = go.Bar(\n    x=org_exp.index,\n    y=org_exp['0-49 employees'].values,\n    name='0-49 employees',\n    marker=dict(\n    color=\"#66545e\")\n)\n\ntrace2 = go.Bar(\n    x=org_exp.index,\n    y=org_exp['50-249 employees'].values,\n    name='50-249 employees',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=org_exp.index,\n    y=org_exp['250-999 employees'].values,\n    name='250-999 employees',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=org_exp.index,\n    y=org_exp['1000-9,999 employees'].values,\n    name='1000-9,999 employees',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=org_exp.index,\n    y=org_exp['> 10,000 employees'].values,\n    name='> 10,000 employees',\n    marker=dict(\n    color='#00a3e0')\n)\n\ndata = [trace1, trace2,trace3,trace4,trace5]\nlayout = go.Layout(\n    barmode='group',height=600,width=1200,title='ML product used vs Company size',yaxis_title='Number of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * Okay, so most of the companies do not use a machine learning product.\n* I have learned this from my own experience that most companies using Data Science do not want to use third party  APIs like Google Cloud Natural language or Google Cloud Translation or Google Cloud Speech-to-text or Google Cloud Vision. They prefer to have a propreitary in-house ML apllication(that's what they have hired the Data Scientists for, right?) and even when they do ,they use it as an intermediate process while building their ML applications.\n* Amazon Sagemaker is different from the above, it lets you build your own ML application on top of it's computing power and ML framework and hence it is mostly used by companies along with Google Cloud Machine Learning Engine.\n* I think, it takes a little time to get used to the quirks of Sagemaker when compared to the traditional methods. You might as well start with a free account."},{"metadata":{},"cell_type":"markdown","source":"# What determines your Salary as a Data Scientist?\n\nThere are a lot of factors on which the salary of a Data Scientist depends, the common ones being the skill set and the country of work. Take a look.\n\n### What salary can a Data Scientist expect acording to his/her highest education?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"edu_sub=df[(df['Q4']=='Masterâ€™s degree') | (df['Q4']=='Bachelorâ€™s degree') ]\nedu_comp=pd.crosstab(edu_sub['Q4'],edu_sub['Q10'])\nedu_comp=edu_comp[['$0-999','1,000-1,999','2,000-2,999','3,000-3,999','4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999','15,000-19,999','20,000-24,999','25,000-29,999','30,000-39,999','40,000-49,999','50,000-59,999','60,000-69,999','70,000-79,999','80,000-89,999','90,000-99,999','100,000-124,999','125,000-149,999','150,000-199,999','200,000-249,999','250,000-299,999','250,000-299,999','> $500,000']]\n\ntrace1 = go.Bar(\n    x=edu_comp.columns,\n    y=edu_comp[edu_comp.index=='Bachelorâ€™s degree'].values[0],\n    name='Bachelorâ€™s degree',\n    marker=dict(\n    color=\"#1a472a\")\n)\ntrace2 = go.Bar(\n    x=edu_comp.columns,\n    y=edu_comp[edu_comp.index=='Masterâ€™s degree'].values[0],\n    name='Masterâ€™s degree',\n    marker=dict(\n    color='#808080')\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='group',height=600,width=1000,title='Salary according to highest education',xaxis_title='Salary',yaxis_title='Number of people'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the mid to higher income range(let's say above 30,000 USD) we have much more people with a Master's degree.\n### How does your salary increases based on your years of experience?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"comp_exp=pd.crosstab(df['Q23'],df['Q10'])\ncomp_exp=comp_exp[['$0-999','1,000-1,999','2,000-2,999','3,000-3,999','4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999','15,000-19,999','20,000-24,999','25,000-29,999','30,000-39,999','40,000-49,999','50,000-59,999','60,000-69,999','70,000-79,999','80,000-89,999','90,000-99,999','100,000-124,999','125,000-149,999','150,000-199,999','200,000-249,999','250,000-299,999','250,000-299,999','> $500,000']]\n\ntrace0 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='< 1 years'].values[0],\n    mode = 'lines+markers',\n    name = '< 1 years',\n    line = dict(\n        color = \"#5d5d5d\")\n)\ntrace1 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='1-2 years'].values[0],\n    mode = 'lines+markers',\n    name = '1-2 years',\n    line= dict( color= \"#c9df8a\")\n)\ntrace2 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='2-3 years'].values[0],\n    mode = 'lines+markers',\n    name = '2-3 years',\n    line= dict( color= \"#77ab59\")\n)\ntrace3 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='3-4 years'].values[0],\n    mode = 'lines+markers',\n    name = '3-4 years',\n    line= dict( color= \"#36802d\")\n)\ntrace4 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='4-5 years'].values[0],\n    mode = 'lines+markers',\n    name = '4-5 years',\n    line= dict( color= \"#234d20\")\n)\ntrace5 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='5-10 years'].values[0],\n    mode = 'lines+markers',\n    name = '5-10 years',\n    line= dict( color= \"#a47c48\")\n)\ntrace6 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='10-15 years'].values[0],\n    mode = 'lines+markers',\n    name = '10-15 years',\n    line= dict( color= \"#845422\")\n)\ntrace7 = go.Scatter(\n    x = comp_exp.columns,\n    y = comp_exp[comp_exp.index=='20+ years'].values[0],\n    mode = 'lines+markers',\n    name = '20+ years',\n    line= dict( color= \"#000000\")\n)\n\n\n\ndata=[trace0,trace1,trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = dict(title = 'Salary based on years of experience',\n              xaxis = dict(title = 'Salary group'),\n              \n              margin=go.layout.Margin(\n        l=50,\n        r=50,\n        b=200,\n        t=100,\n        pad=4\n    ),height=600,width=600,paper_bgcolor ='#aaaaaa'\n              )\n\nfig = dict(data=data, layout=layout)\n\npy.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">* I think the most important factor in deciding your salary as a Data Scientist is the years of experience you have in the field.\n* The lowest salary range is dominated by people with very less experience(less than 2 years). Most of the Data Scientists having high income fall in the 5-10 years of experience range."},{"metadata":{},"cell_type":"markdown","source":"The current salary ranges make the plot look hotch-potch , so I'll add a new column with three ranges\n* Low income range( 0-25,000$)\n\n* Mid income range(25,000- 100,000$)\n\n* High income range(100,000$+)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"new_range=[]\nfor salary in df['Q10']:\n    if salary in ['$0-999','1,000-1,999','2,000-2,999','3,000-3,999','4,000-4,999','5,000-7,499','7,500-9,999','10,000-14,999','15,000-19,999','20,000-24,999']:\n        new_range.append('Low Income Range')\n    elif salary in ['25,000-29,999','30,000-39,999','40,000-49,999','50,000-59,999','60,000-69,999','70,000-79,999','80,000-89,999','90,000-99,999']:\n        new_range.append('Mid Income Range')\n    \n    else :\n        new_range.append('High Income Range')\n        \ndf['salary_range']=new_range","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Does the state of ML in the company decide the salary?\nI'm also interested to know whether the state of ML in the company has any effect on the salary (I'm pretty sure it does)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_sub=pd.crosstab(df['salary_range'],df['Q8'])\ndf_sub.columns=['do not know','do not use ML','Exploring ML methods','Well established ML methods','Recently statrted using ML methods','use ML methods for getting insights']\ntrace1 = go.Bar(\n    x=df_sub.columns,\n    y=df_sub[df_sub.index=='Low Income Range'].values[0],\n    name='Low Income Range',\n    marker=dict(\n    color=\"#00a0b0\")\n)\ntrace2 = go.Bar(\n    x=df_sub.columns,\n    y=df_sub[df_sub.index=='Mid Income Range'].values[0],\n    name='Mid Income Range',\n    marker=dict(\n    color='#4f372d')\n)\ntrace3 = go.Bar(\n    x=df_sub.columns,\n    y=df_sub[df_sub.index=='High Income Range'].values[0],\n    name='High Income Range',\n    marker=dict(\n    color='#cc2a36')\n)\ndata = [trace1, trace2,trace3]\nlayout = go.Layout(\n    barmode='group',height=600,width=700,title='Salary Range vs state of ML in the company',xaxis_title='State of DS in the Company',yaxis_title='Number of people'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Some major deductions from the plot:\n* Most of the Highly paid Data Scientists come from companies which have well established ML methods. There models are in production and they generate revenue from it , therefore they invest more in their Data Science Team.\n* Companies that do not use ML methods( I don't know why would they hire a Data Scientist then), or are exploring ML methods or use ML to just generate insights pay less to their Data Scientists.\n* Most of the mid income DS are from companies that have just started using ML methods.\n\n## Does the DS Field play a role in deciding salary?\nI want to explore what is the trend in salary for different fields, let's dive into it."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\n# df_cv=df[df['DS_field']=='NLP']\n# cv_sal_exp=pd.crosstab(df_cv['salary_range'],df['Q23'])\n# cv_sal_exp=cv_sal_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\n\ndf_ml=df[df['DS_field']=='ML']\nml_sal_exp=pd.crosstab(df_ml['salary_range'],df['Q23'])\nml_sal_exp=ml_sal_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\nsal_field=pd.crosstab(df['salary_range'],df['DS_field'])\ntrace1 = go.Bar(\n    x=sal_field.columns,\n    y=sal_field[sal_field.index=='Low Income Range'].values[0],\n    name='Low Income Range',\n    marker=dict(\n    color=\"#00a0b0\"),showlegend=False\n)\ntrace2 = go.Bar(\n    x=sal_field.columns,\n    y=sal_field[sal_field.index=='Mid Income Range'].values[0],\n    name='Mid Income Range',\n    marker=dict(\n    color='#4f372d'),showlegend=False\n)\ntrace3 = go.Bar(\n    x=sal_field.columns,\n    y=sal_field[sal_field.index=='High Income Range'].values[0],\n    name='High Income Range',\n    marker=dict(\n    color='#cc2a36'),showlegend=False\n)\n\n\ntrace4 = go.Bar(\n    x=ml_sal_exp.columns,\n    y=ml_sal_exp[ml_sal_exp.index=='Low Income Range'].values[0],\n    name='Low Income Range',\n    marker=dict(\n    color=\"#00a0b0\")\n)\ntrace5 = go.Bar(\n    x=ml_sal_exp.columns,\n    y=ml_sal_exp[ml_sal_exp.index=='Mid Income Range'].values[0],\n    name='Mid Income Range',\n    marker=dict(\n    color='#4f372d')\n)\ntrace6 = go.Bar(\n    x=ml_sal_exp.columns,\n    y=ml_sal_exp[ml_sal_exp.index=='High Income Range'].values[0],\n    name='High Income Range',\n    marker=dict(\n    color='#cc2a36')\n)\n# trace7 = go.Bar(\n#     x=cv_sal_exp.columns,\n#     y=cv_sal_exp[cv_sal_exp.index=='Low Income Range'].values[0],\n#     name='Low Income Range',\n#     marker=dict(\n#     color=\"#00a0b0\")\n# )\n# trace8 = go.Bar(\n#     x=cv_sal_exp.columns,\n#     y=cv_sal_exp[cv_sal_exp.index=='Mid Income Range'].values[0],\n#     name='Mid Income Range',\n#     marker=dict(\n#     color='#4f372d')\n# )\n# trace9 = go.Bar(\n#     x=cv_sal_exp.columns,\n#     y=cv_sal_exp[cv_sal_exp.index=='High Income Range'].values[0],\n#     name='High Income Range',\n#     marker=dict(\n#     color='#cc2a36')\n# )\n\n\nfig = tools.make_subplots(rows=2, cols=1, specs=[[{}],[{}]],\n                          subplot_titles=('Salary Range vs DS Field','Expereince vs Salary for people in ML'))\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 1)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace5, 2, 1)\nfig.append_trace(trace6, 2, 1)\n# fig.append_trace(trace7, 3, 1)\n# fig.append_trace(trace8, 3, 1)\n# fig.append_trace(trace9, 3, 1)\n\n\nfig['layout'].update( height=600,width=800,margin=go.layout.Margin(\n        l=50,\n        r=50,\n        b=50,\n        t=50,\n        pad=4\n    ))\npy.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Deductions:\n* Most of the people in ML belong to the high income Salary range and on the contrary most people in Computer Vision and NLP belong to the low and mid income range.\n* I had a hypothesis that has more to do with years of experience, since Machine learning is an older field compared to Deep Learning, so I made a plot to confirm this.\n* Seems like the hypothesis is true, most of the higher salary income people have a lot of experience.\n\n## Conclusions\n> Landing a Data Science job ain't that hard. You should know what it takes and where to look.\n* Having a university degree in Data Science is not that important, a lot of Data Scientists learn from MOOCs. However, it is very important to stay up-to-date with trends in the field.Medium Blogs and Kaggle Forums are a popular choice among Data Scientits.\n* Spend more time in cleaning and analyzing data than making models because that's what people do while solving real-world problems. Local environment with matplotlib is best for analyzing data.\n* Whenever solving a problem try to stick with easily interpretable algorithms before jumping to neural networks. If you want to enter the field of NLP or CV making an image or text classifier with good background on the working and basics of it would also work. You can also go for some not so common projects to give an edge to your resume.\n* Set realistic goals, after making a good profile( with some nice independent projects or open-source contributions) and a good understanding of what you do,apply at a start-up or a place that has more chances of hiring you.\n* The picture won't be that pretty and you will have a lot of responsibilities, but you will learn a lot here.\n* You can try your hand at learning to use a ML product.\n* Years of experience will gradually but surely add worth to you.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}