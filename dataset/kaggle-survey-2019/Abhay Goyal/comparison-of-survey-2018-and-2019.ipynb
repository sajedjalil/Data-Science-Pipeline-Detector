{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2018 = pd.read_csv('/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_2019 = pd.read_csv('/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2019.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_percentage(data,question_part):\n    \"\"\"Calculates percent of each value in a given column\"\"\"\n    total = data[question_part].count()\n    counts_df= data[question_part].value_counts().to_frame()\n    percentage_df = (counts_df*100)/total\n    return percentage_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(data_2019.columns)\nlist(zip(x,data_2019.iloc[0,:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(data_2018.columns)\nlist(zip(x,data_2018.iloc[0,:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list(data_2019.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less = data_2019[['Q1', 'Q2', 'Q3','Q4', 'Q6', 'Q9_Part_1', 'Q9_Part_2', 'Q9_Part_3', 'Q9_Part_4', 'Q9_Part_5', \n                 'Q9_Part_6', 'Q10', 'Q11', 'Q15']]\ndata_most_ds = data_2019[['Q12_Part_1', 'Q12_Part_2', 'Q12_Part_3', 'Q12_Part_4', 'Q12_Part_5', 'Q12_Part_6', 'Q12_Part_7',\n                        'Q12_Part_8', 'Q12_Part_9', 'Q12_Part_10', 'Q12_Part_11', 'Q12_Part_12']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2018.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(data_2018.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less_2018 = data_2018[['Q1', 'Q3', 'Q2','Q11_Part_1','Q11_Part_2',\n                            'Q11_Part_3','Q11_Part_4','Q11_Part_5','Q11_Part_6','Q11_Part_7', 'Q24']]\ndata_less_2018","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chart = sns.countplot(data_less['Q2'])\n# plt.xticks(rotate = 90)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chart = sns.countplot(data_less_2018['Q1'])\n# plt.xticks(rotate = 90)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen from the code that the number of respondents dereased from 2018 to 2019 but the proportion remained the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_gender_2019 = return_percentage(data_less, 'Q2').reset_index()\ndf_gender_2019.columns = ['Gender', 'Percentage']\ndf_gender_2019\n# (data_less[data_less['Q2'] == 'Female'].count() / data_less['Q2'].count())*100\n# (data_less[data_less['Q2'] == 'Male'].count() / data_less['Q2'].count())*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_gender_2018= return_percentage(data_less_2018, 'Q1').reset_index()\ndf_gender_2018.columns = ['Gender', 'Percentage']\ndf_gender_2018","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make pie chart for the male and female also\n# fig = make_subplots(rows = 1, cols = 2)\n# trace0 = px.pie(df_gender_2019, values='Percentage', names='Gender')\n# trace1 = px.pie(df_gender_2018, values='Percentage', names='Gender')\n# fig.add_trace(trace0, row = 1, col = 1)\n# fig.add_trace(trace1, row = 1, col = 2)\n# fig.update_layout(height=600, width=800, title_text=\"Comparision of Respondents by Country in 2018 and 2019\", title_x = 0.5)\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# colors2 = ['dodgerblue', 'plum', '#F0A30A','#8c564b'] \n# gender_count_2019 = data_less['Q2'].value_counts(sort=True)\n# gender_count_2018 = data_less_2018['Q1'].value_counts(sort=True)\n# fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'},{'type':'domain'}]])\n# fig.add_trace(go.Pie(labels=list(data_less['Q2'].unique()), values=gender_count_2019.values, name=\"2019\",marker=dict(colors=colors2)), 1,1)\n# fig.add_trace(go.Pie(labels=list(data_less_2018['Q1'].unique()), values=gender_count_2018.values, name=\"2018\",marker=dict(colors=colors2)), 1,2)\n# fig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less_2018.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Working on making Bar charts for 2018 and 2019 and compare as to where the respondents are mostly from \n# return_percentage(data_less, 'Q3').head(10)\ndf_countries_2018 = return_percentage(data_less_2018, 'Q3').head(10).reset_index()\ndf_countries_2018.columns = ['Country', 'Percentage']\ndf_countries_2019 = return_percentage(data_less, 'Q3').head(10).reset_index()\ndf_countries_2019.columns = ['Country', 'Percentage']\n# print(df_countries_2019)\nfig = make_subplots(rows = 1, cols = 2)\ntrace0 = go.Bar(x = df_countries_2018['Country'], y = df_countries_2018['Percentage'])\ntrace1 = go.Bar(x = df_countries_2019['Country'], y = df_countries_2019['Percentage'])\nfig.add_trace(trace0, row= 1, col = 1)\nfig.add_trace(trace1, row= 1, col = 2)\nfig.update_layout(height=600, width=800, title_text=\"Comparision of Respondents by Country in 2018 and 2019\", title_x = 0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HIghest number correspondents from India in 2019 whereas in 2018 the highest number of correspondents were from USA. This shows that the number of people who filled the surveys and also were willing to work increased over the course of 1 year","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"return_percentage(data_less_2018, 'Q2')\n# return_percentage(data_less, 'Q1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"return_percentage(data_less, 'Q1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In both cases we see, the number of people who are filling the forms are 25-29","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# age_values_2018 = data_less_2018['Q2'].value_counts()\n# age_values_2018","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# age_values_2019 = data_less['Q1'].value_counts()\n# age_values_2019","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_brackets = ['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70-79','80+']\nage_values_2018 = [3037, 5141, 6159, 3776, 2253, 1360, 858, 582, 328, 273, 53, 1]\nage_values_2019 = [2502, 3610, 4458, 3120, 2087, 1439, 949, 692, 422, 338, 50, 50]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows = 1, cols = 2)\ntrace0 = go.Scatter(x = age_brackets, y = age_values_2018)\ntrace1 = go.Scatter(x = age_brackets, y = age_values_2019)\nfig.add_trace(trace0, row= 1, col = 1)\nfig.add_trace(trace1, row= 1, col = 2)\nfig.update_layout(height=600, width=800, title_text=\"Comparision of Age in 2018 and 2019\", title_x = 0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The maximum number of people working in these areas are mostly 25-29. \n* The most probable reason for this is that most companies require a Master's degree or a certain number of years of experience in the industry\n* It is always highlighted that the number of Data Science / Data Analysts jobs present are higher than ever, but the type of people companies hire or work on some kind of data science work are not the ones fresh out of college rather those who have some amount of experience in the industry.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get all columns which have data present, get data except the NaN's and then count the number of responses of each \n\ntype_of_work_2018 = pd.DataFrame()\ntype_of_work_2019 = pd.DataFrame()\n\nfor i in range(7):\n    type_of_work_2018['Q11_Part_'+str(i+1)] = data_less_2018['Q11_Part_'+str(i+1)]\nfor i in range(6):    \n    type_of_work_2019['Q9_Part_'+str(i+1)] = data_less['Q9_Part_'+str(i+1)]\n\n# type_of_work_2019\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type_of_work_2018['Q11_Part_7'].unique())\nx_2019 = ['Analyze and understand data to influence product or business decisions', \n     'Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n     'Build prototypes to explore applying machine learning to new areas',\n     'Build and/or run a machine learning service that operationally improves my product or workflows',\n     'Experimentation and iteration to improve existing ML models',\n     'Do research that advances the state of the art of machine learning']\ny_2019 = list(type_of_work_2019.count())\n\ny_2018 = list(type_of_work_2018.count())[:-1]\nx_2018 = ['Analyze and understand data to influence product or business decisions',\n          'Build and/or run a machine learning service that operationally improves my product or workflows',\n          'Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n          'Build prototypes to explore applying machine learning to new areas',\n          'Do research that advances the state of the art of machine learning',\n          'None of these activities are an important part of my role at work']\nprint(y_2018)\n\nfig = make_subplots(rows = 1, cols = 2)\ntrace0 = go.Bar(x = x_2018, y = y_2018)\ntrace1 = go.Bar(x = x_2019, y = y_2019)\nfig.add_trace(trace0, row= 1, col = 1)\nfig.add_trace(trace1, row= 1, col = 2)\nfig.update_layout(height=600, width=900, title_text=\"Comparision of Type of work done in 2018 and 2019\", title_x = 0.5, showlegend = False)\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It looks like analysing data seems to take up most of the time in any workplace where data drives the business decisions taken\n* Analysing data is a very important part and forms the base operation for any type of Machine Learning work done\n* The type of work done did not change but we can see that the number of people doing the same job increased in almost every field present. \n![Power of Analysis of data](https://www.istnetworks.com/wp-content/uploads/2018/06/e9a735_daab65cd4a7c49b58c0fa2d65297d2damv2.png)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less['Q15'].value_counts()\nnumber_of_values = ['< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years']\ny_values_2019 = [3828, 4061, 3365, 1887, 1045, 576]\ny_values_2018 = [4542, 5359, 4023, 2145, 1102, 500]\nfig = make_subplots(rows = 1, cols = 2)\ntrace0 = go.Scatter(x = number_of_values, y = y_values_2018)\ntrace1 = go.Scatter(x = number_of_values, y = y_values_2019)\nfig.add_trace(trace0, row= 1, col = 1)\nfig.add_trace(trace1, row= 1, col = 2)\nfig.update_layout(height=600, width=900, title_text=\"Comparision of Type of work done in 2018 and 2019\", title_x = 0.5, showlegend = False)\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The first thing we see here is that the maximum number of people involved in doing some kind of analysis of data are 1-2 years of experience\n* Here, we know that the analysis of data, here means that they are doing cleaning of data and visualizing the data present.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_less_2018['Q24'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_algoriths_used = data_2018['Q20'].value_counts().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo_use = {}\nfor i in range(12):\n    print(data_2019['Q24_Part_'+str(i+1)].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2018['Q20_OTHER_TEXT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text = pd.DataFrame()\n# text['ML_algo'] = data_2019['Q24_OTHER_TEXT'].str.lower()\n# text['count'] = 1\n# text.drop(0)[['ML_algo','count']].groupby('ML_algo').sum()[['count']].sort_values('count', ascending=False)\n\n# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n# plt.figure(figsize=[15,8])\n\n# # Create and generate a word cloud image:\n# ide_words = ' '.join(text['ML_algo'].drop(0).dropna().values)\n# # print(ide_words)\n# wordcloud = WordCloud(colormap=\"tab10\",\n#                       width=1200,\n#                       height=480,\n#                       normalize_plurals=False,\n#                       background_color=\"white\",\n#                       random_state=5).generate(ide_words)\n\n# # Display the generated image:\n# plt.imshow(wordcloud, interpolation='bilinear')\n# plt.axis(\"off\")\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2018['Q4'].value_counts()\n# pd.crosstab()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2019['Q4'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type_of_work_2019['All'] = '' \ntype_of_work_2019\nfor i in range(6):\n    if type_of_work_2019['Q9_Part_'+str(i+1)] is 'NaN':\n        continue\n    type_of_work_2019['All'] += type_of_work_2019['Q9_Part_'+str(i+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Type of education and work relationship","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data_2019['Q6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type_of_work_2018","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2018_mod = pd.DataFrame()\ndata_2018_mod['Q4'] = data_2018['Q4']\nar = ['Master’s degree', 'Bachelor’s degree', 'Doctoral degree']\ndata_2018_mod['Q4'] = data_2018_mod.loc[data_2018_mod['Q4'].isin(ar)]\n\nwork_degree = pd.concat([pd.crosstab(type_of_work_2018['Q11_Part_1'], data_2018_mod['Q4']), \n                    pd.crosstab(type_of_work_2018['Q11_Part_2'], data_2018_mod['Q4']),\n                    pd.crosstab(type_of_work_2018['Q11_Part_3'], data_2018_mod['Q4']),\n                    pd.crosstab(type_of_work_2018['Q11_Part_4'], data_2018_mod['Q4']),\n                    pd.crosstab(type_of_work_2018['Q11_Part_5'], data_2018_mod['Q4']),\n                    pd.crosstab(type_of_work_2018['Q11_Part_6'], data_2018_mod['Q4']),\n                    pd.crosstab(type_of_work_2018['Q11_Part_7'], data_2018_mod['Q4'])])\n# print(ml_exp[['Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data']])\nprint(work_degree[['Bachelor’s degree', 'Doctoral degree', 'Master’s degree']])\nwork_degree=work_degree.fillna(0)\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_1']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_2']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_3']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_4']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_5']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_6']))\n# return_percentage(data_less_2018, 'Q3')\ntrace1 = go.Bar(\n    x=work_degree.index,\n    y=work_degree['Bachelor’s degree'].values/10138,\n    name='Bachelor’s degree',\n    marker=dict(\n    color=\"#66545e\")\n)\n\ntrace2 = go.Bar(\n    x=work_degree.index,\n    y=work_degree['Master’s degree'].values/17855,\n    name='Master’s degree',\n    marker=dict(\n    color=\"#4d0e20\")\n)\n\ntrace3 = go.Bar(\n    x=work_degree.index,\n    y=work_degree['Doctoral degree'].values/6231,\n    name='Doctoral degree',\n    marker=dict(\n    color=\"#b35a00\")\n)\n\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(\n    barmode='group',height=600,width=1200,title='Type of work done grouped by degree in 2018',title_x = 0.5,yaxis_title='Percentage of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2019_mod = pd.DataFrame()\ndata_2019_mod['Q4'] = data_2019['Q4']\nar = ['Master’s degree', 'Bachelor’s degree', 'Doctoral degree']\ndata_2019_mod['Q4'] = data_2019_mod.loc[data_2019_mod['Q4'].isin(ar)]\n\nwork_degree = pd.concat([pd.crosstab(type_of_work_2019['Q9_Part_1'], data_2019_mod['Q4']), \n                    pd.crosstab(type_of_work_2019['Q9_Part_2'], data_2019_mod['Q4']),\n                    pd.crosstab(type_of_work_2019['Q9_Part_3'], data_2019_mod['Q4']),\n                    pd.crosstab(type_of_work_2019['Q9_Part_4'], data_2019_mod['Q4']),\n                    pd.crosstab(type_of_work_2019['Q9_Part_5'], data_2019_mod['Q4']),\n                    pd.crosstab(type_of_work_2019['Q9_Part_6'], data_2019_mod['Q4'])])\n# print(ml_exp[['Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data']])\n# print(ml_exp[['Bachelor’s degree', 'Doctoral degree', 'Master’s degree']])\nwork_degree=work_degree.fillna(0)\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_1']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_2']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_3']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_4']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_5']))\n# print(pd.crosstab(data_2019_mod['Q4'], type_of_work_2019['Q9_Part_6']))\n# return_percentage(data_less_2018, 'Q3')\ntrace1 = go.Bar(\n    x=work_degree.index,\n    y=work_degree['Bachelor’s degree'].values/5641,\n    name='Bachelor’s degree',\n    marker=dict(\n    color=\"#66545e\")\n)\n\ntrace2 = go.Bar(\n    x=work_degree.index,\n    y=work_degree['Master’s degree'].values/11879,\n    name='Master’s degree',\n    marker=dict(\n    color=\"#4d0e20\")\n)\n\ntrace3 = go.Bar(\n    x=work_degree.index,\n    y=work_degree['Doctoral degree'].values/4853,\n    name='Doctoral degree',\n    marker=dict(\n    color=\"#b35a00\")\n)\n\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(\n    barmode='group',height=600,width=1200,title='Type of work done grouped by degree in 2019',title_x = 0.5,yaxis_title='Percentage of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It can be seen from this plot that the highest number of people who are recruited by companies to work on Data Science/ Analytics work usually are the Master's students\n* This graph shows the percentage of degree students working on the tasks shown in the x-axis\n* As is evident, as the level of research experience increase, the type of work done also increases.\n* The Bachelor's Degree students are mostly given analysis and cleaning work which will help them in giving a better understanding of what the project is about and how things need to be done\n* The Doctoral Degree candidates are higher in ratio in the \"Experimentation and Research\" aspects of the work ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* If we are to compare the 2018 and 2019 trends, we see that both are actually the same in terms of the trends and the type of work given\n* In both, we can see that the Bachelor's Degree holder people are the ones which actually mostly work on data cleaning and the Doctoral candidates work on the Reseach and Building Prototypes for the companies ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}