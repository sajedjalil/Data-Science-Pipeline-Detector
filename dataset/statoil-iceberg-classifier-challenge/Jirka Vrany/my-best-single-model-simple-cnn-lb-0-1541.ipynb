{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"**UPDATE after finish**\nThe final score of this model on the **private leaderboard was 0.1639**.  My best ensemble private score was 0.1477. However I choosed another model for the final solution and my final result is a bit dissapointment. But I learned a lot and this is why we all here :-) \n\nThis was my first competition here at Kaggle. I learned a lot from the discussions and the kernels gladly published by more experienced Kagglers. \n\nTherefore I decided to give something back to the community. Here is my single best model scoring 0.1541 on the public leaderboard. \n\nMaybe it can help someone as a part of the final stacked solution. Thanks for upvoting if so ;-)\n\nThis work was built on those two great Kernels.\n\nTheGruffalo\nKeras CNN - StatOil Iceberg LB 0.1995 (now 0.1516)\n[https://www.kaggle.com/cbryant/keras-cnn-statoil-iceberg-lb-0-1995-now-0-1516](http://https://www.kaggle.com/cbryant/keras-cnn-statoil-iceberg-lb-0-1995-now-0-1516)\n\nQuantScientist\nStatoil CSV PyTorch SENet ensemble LB 0.1520\n[https://www.kaggle.com/solomonk/statoil-csv-pytorch-senet-ensemble-lb-0-1520](http://https://www.kaggle.com/solomonk/statoil-csv-pytorch-senet-ensemble-lb-0-1520)","cell_type":"markdown","metadata":{"_uuid":"a84233f5cbe98d402d74f46e87e7d745540b2cc8","_cell_guid":"c825acee-1835-4fa0-a14a-0c40a589577a"}},{"source":"## Imports","cell_type":"markdown","metadata":{"_uuid":"fb4f08cce0aaddbda853b5c86f8b66e6189b10a4","_cell_guid":"3742280f-b7f8-4639-a1dc-fe34c5d0bbe5"}},{"outputs":[],"execution_count":null,"source":"import pandas as pd \nimport numpy as np \nimport cv2 # Used to manipulated the images \nseed = 1234\nnp.random.seed(seed) # The seed I used - pick your own or comment out for a random seed. A constant seed allows for better comparisons though\n\n# Kfold\nfrom sklearn.model_selection import StratifiedKFold\n\n# Import Keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam","cell_type":"code","metadata":{"collapsed":true,"_uuid":"a6025efc05dbb5d0a9f30e1b70e6178a52f7bb5b","_cell_guid":"06ed8576-67b0-407a-bec3-a8919432eb4b"}},{"outputs":[],"execution_count":null,"source":"def get_scaled_imgs(df):\n    \"\"\"\n    basic function for reshaping and rescaling data as images\n    \"\"\"\n    imgs = []\n    \n    for i, row in df.iterrows():\n        #make 75x75 image\n        band_1 = np.array(row['band_1']).reshape(75, 75)\n        band_2 = np.array(row['band_2']).reshape(75, 75)\n        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n        \n        # Rescale\n        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n\n        imgs.append(np.dstack((a, b, c)))\n\n    return np.array(imgs)    \n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"2782430813d6deda5a6f8aae1cc385586538f5c5","_cell_guid":"715ebc9b-0999-456e-8ed0-bc96e05e0128"}},{"source":"## Adding images for training","cell_type":"markdown","metadata":{"_uuid":"8da34b46d1f4c96d9f32239df3b61573cde9d0e1","_cell_guid":"306fe2f3-f417-4f2d-956b-2cc98c8b955b"}},{"source":"Now, the biggest improvement I had was by adding more data to train on. I did this by simply including horizontally and vertically flipped data. Using OpenCV this is easily done.","cell_type":"markdown","metadata":{"_uuid":"fba1fe217316a5ca70887c67d2c0c158cb8c02dc","_cell_guid":"49117bf4-dbdd-4ea5-95ca-af23ed35053e"}},{"outputs":[],"execution_count":null,"source":"def get_more_images(imgs):\n    \"\"\"\n    augmentation for more data\n    \"\"\"    \n\n\n    more_images = []\n    vert_flip_imgs = []\n    hori_flip_imgs = []\n      \n    for i in range(0,imgs.shape[0]):\n        a=imgs[i,:,:,0]\n        b=imgs[i,:,:,1]\n        c=imgs[i,:,:,2]\n        \n        av=cv2.flip(a,1)\n        ah=cv2.flip(a,0)\n        bv=cv2.flip(b,1)\n        bh=cv2.flip(b,0)\n        cv=cv2.flip(c,1)\n        ch=cv2.flip(c,0)\n        \n        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n      \n    v = np.array(vert_flip_imgs)\n    h = np.array(hori_flip_imgs)\n       \n    more_images = np.concatenate((imgs,v,h))\n    \n    return more_images","cell_type":"code","metadata":{"collapsed":true,"_uuid":"760fa192e912111b39d3d0ee3fadd9c7fd0eb628","_cell_guid":"f069618d-46f0-42f0-8e83-e5a4716bb965"}},{"source":"## CNN Keras Model","cell_type":"markdown","metadata":{"_uuid":"f2b22fcec3493ce4df71f0dd21ed97fd1938e3f7","_cell_guid":"54a72cfc-dbc1-4136-bb31-2ea75115e1cf"}},{"outputs":[],"execution_count":null,"source":"\ndef get_model():\n    \n    \"\"\"\n    Keras Sequential model\n\n    \"\"\"\n    \n    model=Sequential()\n    \n    # Conv block 1\n    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n   \n    # Conv block 2\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n   \n    # Conv block 3\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n   \n    #Conv block 4\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n   \n    # Flatten before dense\n    model.add(Flatten())\n\n    #Dense 1\n    model.add(Dense(1024, activation='relu'))\n    model.add(Dropout(0.4))\n\n    #Dense 2\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.2))\n\n    # Output \n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    optimizer = Adam(lr=0.0001, decay=0.0)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    return model\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"edd8ce229d5914f9f57bf147dccb4810c06dbf09","_cell_guid":"c3d1f84c-44e0-45df-9363-13f5b8abd6ef"}},{"source":"## Load the data, and train the model usign K-fold CV\n\nThe traing here at Kaggle will be very slow. Better fork the notebook, download and run the code locally. Don't forget to set the epochs.","cell_type":"markdown","metadata":{"_uuid":"3ea6bdb4b84b3a7810305165a322c29a71d8c018","_cell_guid":"a6102e0d-9059-4daa-8483-c1449b67ff4f"}},{"outputs":[],"execution_count":null,"source":"\n# Training Data\ndf_train = pd.read_json('../input/train.json') # this is a dataframe\n\n\nXtrain = get_scaled_imgs(df_train)\nYtrain = np.array(df_train['is_iceberg'])\ndf_train.inc_angle = df_train.inc_angle.replace('na',0)\nidx_tr = np.where(df_train.inc_angle>0)\n\nYtrain = Ytrain[idx_tr[0]]\nXtrain = Xtrain[idx_tr[0],...]\n\nXtr_more = get_more_images(Xtrain) \nYtr_more = np.concatenate((Ytrain,Ytrain,Ytrain))\n\n# K fold CV training\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\nfor fold_n, (train, test) in enumerate(kfold.split(Xtr_more, Ytr_more)):\n    print(\"FOLD nr: \", fold_n)\n    model = get_model()\n    \n    MODEL_FILE = 'mdl_simple_k{}_wght.hdf5'.format(fold_n)\n    batch_size = 32\n    mcp_save = ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_loss', mode='min')\n    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, verbose=1, epsilon=1e-4, mode='min')\n\n    # set the epochs to 30 before training on your GPU\n    model.fit(Xtr_more[train], Ytr_more[train],\n        batch_size=batch_size,\n        epochs=1,\n        verbose=1,\n        validation_data=(Xtr_more[test], Ytr_more[test]),\n        callbacks=[mcp_save, reduce_lr_loss])\n    \n    model.load_weights(filepath = MODEL_FILE)\n\n    score = model.evaluate(Xtr_more[test], Ytr_more[test], verbose=1)\n    print('\\n Val score:', score[0])\n    print('\\n Val accuracy:', score[1])\n\n    SUBMISSION = './result/simplenet/sub_simple_v1_{}.csv'.format(fold_n)\n\n    df_test = pd.read_json('../input/test.json')\n    df_test.inc_angle = df_test.inc_angle.replace('na',0)\n    Xtest = (get_scaled_imgs(df_test))\n    pred_test = model.predict(Xtest)\n\n    submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n    print(submission.head(10))\n\n    submission.to_csv(SUBMISSION, index=False)\n    print(\"submission saved\")\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"4c2c471152ef251ae12e2589d0c7da7b4bcce369","_cell_guid":"5dc8dcec-7ed5-4ba7-b3da-b458b26f32ca"}},{"source":"## Combine the K-fold solutions together ","cell_type":"markdown","metadata":{"_uuid":"ac5554f356765781e6d3c40439ac0c288dbf9b4d","_cell_guid":"916dd785-f600-450b-b2d7-2661d0c936be"}},{"outputs":[],"execution_count":null,"source":"wdir = './result/simplenet/'\nstacked_1 = pd.read_csv(wdir + 'sub_simple_v1_0.csv')\nstacked_2 = pd.read_csv(wdir + 'sub_simple_v1_1.csv')\nstacked_3 = pd.read_csv(wdir + 'sub_simple_v1_2.csv')\nstacked_4 = pd.read_csv(wdir + 'sub_simple_v1_3.csv')\nstacked_5 = pd.read_csv(wdir + 'sub_simple_v1_4.csv')\nstacked_6 = pd.read_csv(wdir + 'sub_simple_v1_5.csv')\nstacked_7 = pd.read_csv(wdir + 'sub_simple_v1_6.csv')\nstacked_8 = pd.read_csv(wdir + 'sub_simple_v1_7.csv')\nstacked_9 = pd.read_csv(wdir + 'sub_simple_v1_8.csv')\nstacked_10 = pd.read_csv(wdir + 'sub_simple_v1_9.csv')\nsub = pd.DataFrame()\nsub['id'] = stacked_1['id']\nsub['is_iceberg'] = np.exp(np.mean(\n    [\n        stacked_1['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_2['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_3['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_4['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_5['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_6['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_7['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_8['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_9['is_iceberg'].apply(lambda x: np.log(x)),\n        stacked_10['is_iceberg'].apply(lambda x: np.log(x)),\n        ], axis=0))\n\nsub.to_csv(wdir + 'final_ensemble.csv', index=False, float_format='%.6f')    \n\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"34350475a7d6078541692599f7f306697fda4b70","_cell_guid":"4f8f68da-bd58-4d54-9b6a-c2bb8d5f3a7e"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","name":"python","version":"3.6.4","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}}}}