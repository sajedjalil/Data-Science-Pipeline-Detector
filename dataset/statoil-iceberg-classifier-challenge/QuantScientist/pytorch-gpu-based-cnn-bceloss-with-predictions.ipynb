{"nbformat":4,"cells":[{"source":"# PyTorch GPU based CNN with predictions\n\nThis notebook is heavily based on:\nhttps://www.kaggle.com/nanigans/pytorch-starter\n\nModifications:\n1. Uses a custom data loader based on torch.utils.data.Dataset\n1. Works on a **GPU** as well as on a **CPU** out of the box\n1. Runs predictions on the **Test data set**\n1. Creates a **submission file**","metadata":{"_cell_guid":"9dd723cf-8dee-4234-94fe-eb7996388f62","_uuid":"292c4605e2c18ebc4a1bc4879c3344dabb20fcee"},"cell_type":"markdown"},{"source":"import torch\nimport sys\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom sklearn import cross_validation\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\nfrom sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n\nprint('__Python VERSION:', sys.version)\nprint('__pyTorch VERSION:', torch.__version__)\n\nimport numpy\nimport numpy as np\n\nuse_cuda = torch.cuda.is_available()\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\nTensor = FloatTensor\n\nimport pandas\nimport pandas as pd\n\nimport logging\nhandler=logging.basicConfig(level=logging.INFO)\nlgr = logging.getLogger(__name__)\n%matplotlib inline\n\n# !pip install psutil\nimport psutil\nimport os\ndef cpuStats():\n        print(sys.version)\n        print(psutil.cpu_percent())\n        print(psutil.virtual_memory())  # physical memory usage\n        pid = os.getpid()\n        py = psutil.Process(pid)\n        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n        print('memory GB:', memoryUse)\n\ncpuStats()","metadata":{"_cell_guid":"303a9986-71cd-41d0-b53a-0b26ac4007cb","_uuid":"c1cf1feee03e3784251c9ae345244d9ae5375af3","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# %%timeit\nuse_cuda = torch.cuda.is_available()\n# use_cuda = False\n\nlgr.info(\"USE CUDA=\" + str (use_cuda))\nFloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\nTensor = FloatTensor\n","metadata":{"_cell_guid":"7358dba4-24e6-41ca-8ed3-8776a1914f3e","_uuid":"98b66f1ed8ea1a6e776d038d0c445ea20f3e53b0","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# fix seed\nseed=17*19\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif use_cuda:\n    torch.cuda.manual_seed(seed)","metadata":{"_cell_guid":"7f63eb8d-84cb-44fc-8622-3fb5fa128a28","_uuid":"4713772f686ceef6a2ca8f95115467444aba6341","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Load data","metadata":{"_cell_guid":"90a5dbd6-75e9-4183-8e09-8e91c63c0b10","_uuid":"0901ccf48a660dd6a01287cf8d648e7d3a37d715"},"cell_type":"markdown"},{"source":"data = pd.read_json('../input/train.json')\n\ndata['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\ndata['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\ndata['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n\n\nband_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\nband_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\nfull_img = np.stack([band_1, band_2], axis=1)\n","metadata":{"_cell_guid":"5861c386-86d2-4177-900e-7ed30aa76795","_uuid":"db4d4d6154afcb3a5d444b0f288f04b9b15e35c0","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# From Numpy to PyTorch GPU tensors","metadata":{"_cell_guid":"f5c1b74a-631e-4bf6-9188-0eba5905e26c","_uuid":"b4d6dd93df146fb4026832aa447b4663024dd63c"},"cell_type":"markdown"},{"source":"# Convert the np arrays into the correct dimention and type\n# Note that BCEloss requires Float in X as well as in y\ndef XnumpyToTensor(x_data_np):\n    x_data_np = np.array(x_data_np, dtype=np.float32)        \n#     print(x_data_np.shape)\n#     print(type(x_data_np))\n\n    if use_cuda:\n#         lgr.info (\"Using the GPU\")    \n        X_tensor = (torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n    else:\n#         lgr.info (\"Using the CPU\")\n        X_tensor = (torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n        \n#     print((X_tensor.shape)) # torch.Size([108405, 29])\n    return X_tensor\n\n\n# Convert the np arrays into the correct dimention and type\n# Note that BCEloss requires Float in X as well as in y\ndef YnumpyToTensor(y_data_np):    \n    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n#     print(y_data_np.shape)\n#     print(type(y_data_np))\n\n    if use_cuda:\n#         lgr.info (\"Using the GPU\")            \n    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor).cuda()  # BCEloss requires Float        \n    else:\n#         lgr.info (\"Using the CPU\")        \n    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n        Y_tensor = (torch.from_numpy(y_data_np)).type(torch.FloatTensor)  # BCEloss requires Float        \n\n#     print(type(Y_tensor)) # should be 'torch.cuda.FloatTensor'\n#     print(y_data_np.shape)\n#     print(type(y_data_np))    \n    return Y_tensor","metadata":{"_cell_guid":"15bfdd0f-681b-4d76-a19e-1491e83bbb7c","_uuid":"31d151252657c896e29784513f00cdd555efa1e7","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Train / Validation / Test Split","metadata":{"_cell_guid":"d5550f4d-6871-449c-a084-901f7e266ef9","_uuid":"3b4b633950fe1623fb1a314fc3fb2d6458978e87"},"cell_type":"markdown"},{"source":"class FullTrainningDataset(torch.utils.data.Dataset):\n    def __init__(self, full_ds, offset, length):\n        self.full_ds = full_ds\n        self.offset = offset\n        self.length = length\n        assert len(full_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n        super(FullTrainningDataset, self).__init__()\n        \n    def __len__(self):        \n        return self.length\n    \n    def __getitem__(self, i):\n        return self.full_ds[i+self.offset]\n    \nvalidationRatio=0.11    \n\ndef trainTestSplit(dataset, val_share=validationRatio):\n    val_offset = int(len(dataset)*(1-val_share))\n    print (\"Offest:\" + str(val_offset))\n    return FullTrainningDataset(dataset, 0, val_offset), FullTrainningDataset(dataset, val_offset, len(dataset)-val_offset)","metadata":{"_cell_guid":"cc4ffad0-682b-4189-90ff-2ba3f7dd06d1","_uuid":"03a65e001dd2f489526d69d9050ced2c7f86b278","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Dataset and DataLoader","metadata":{"_cell_guid":"f845a93e-faa5-40c6-af89-14e813a8ba34","_uuid":"e99daa83567a13ab4e86e9451e03995c731e6c2b"},"cell_type":"markdown"},{"source":"from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n\nbatch_size=128\n\ntransformations = transforms.Compose([transforms.ToTensor()])\n\n# train_imgs = torch.from_numpy(full_img_tr).float()\ntrain_imgs=XnumpyToTensor (full_img)\ntrain_targets = YnumpyToTensor(data['is_iceberg'].values)\ndset_train = TensorDataset(train_imgs, train_targets)\n\n\ntrain_ds, val_ds = trainTestSplit(dset_train)\n\ntrain_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n                                            num_workers=1)\nval_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=1)\n\nprint (train_loader)\nprint (val_loader)","metadata":{"_cell_guid":"8c2b51ec-1eeb-4519-bb56-49a3dc525386","_uuid":"dccb943235b8fd0cae479601e70f864526a8df86","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Define simple model","metadata":{"_cell_guid":"9655dd9c-c792-4d15-bb8b-94a800eb9c0e","_uuid":"21eacbdcf5f04eecf86da49621b5aff4d3b92d8d"},"cell_type":"markdown"},{"source":"import math \ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n    \n\nclass CifarResNet(nn.Module):\n    def __init__(self, block, n_size, num_classes=1):\n        super(CifarResNet, self).__init__()\n        self.inplane = 16\n        self.conv1 = nn.Conv2d(2, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, blocks=2 * n_size, stride=2)\n        self.layer2 = self._make_layer(block, 32, blocks=2 * n_size, stride=2)\n        self.layer3 = self._make_layer(block, 64, blocks=2 * n_size, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(64, num_classes)\n        self.sig=nn.Sigmoid()   \n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride):\n\n        layers = []\n        for i in range(1, blocks):\n            layers.append(block(self.inplane, planes, stride))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.sig(x)\n\n        return x\n\nmodel = CifarResNet(CifarSEBasicBlock, 1, 1)        \nprint (model)","metadata":{"_cell_guid":"03178a5e-e8ef-4334-a97d-f1cf7c0fecf8","_uuid":"4112e0197ba91e6ab2d39c65d67ed112fdc78729","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"loss_func=torch.nn.BCELoss()\n# NN params\nLR = 0.005\nMOMENTUM= 0.9\noptimizer = torch.optim.Adam(model.parameters(), lr=LR,weight_decay=5e-5) #  L2 regularization\nif use_cuda:\n    lgr.info (\"Using the GPU\")    \n    net.cuda()\n    loss_func.cuda()\n\nlgr.info (optimizer)\nlgr.info (loss_func)","metadata":{"_cell_guid":"7025d0d8-206f-4cb0-af69-7b1a85ebd7f8","_uuid":"abad58921a6aeb5ea0603056bb0021436b73d039","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"num_epoches = 20\ncriterion=loss_func\nall_losses = []\n\n\nmodel.train()\nfor epoch in range(num_epoches):\n    print('Epoch {}'.format(epoch + 1))\n    print('*' * 5 + ':')\n    running_loss = 0.0\n    running_acc = 0.0\n    for i, data in enumerate(train_loader, 1):\n        \n        img, label = data        \n        img = Variable(img)\n        label = Variable(label)\n                        \n        out = model(img)\n        loss = criterion(out, label)\n        running_loss += loss.data[0] * label.size(0)        \n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n                                \n        if i % 10 == 0:\n            all_losses.append(running_loss / (batch_size * i))\n            print('[{}/{}] Loss: {:.6f}'.format(\n                epoch + 1, num_epoches, running_loss / (batch_size * i),\n                running_acc / (batch_size * i)))\n            \n    print('Finish {} epoch, Loss: {:.6f}'.format(epoch + 1, running_loss / (len(train_ds))))\n\ntorch.save(model.state_dict(), './cnn.pth')","metadata":{"_cell_guid":"86849fe7-7c0c-4cde-be41-b5809ec7862b","_uuid":"531c3edfa7dce5409c72f87a19160f9d4150b30b","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"from sklearn.cross_validation import train_test_split\n\ndef kFoldValidation(folds): \n    print ('K FOLD VALIDATION ...')\n    val_losses = []\n    model.eval()\n    \n    for e in range(folds):\n        print ('Fold:' + str(e))        \n        data = pd.read_json('../input/train.json')        \n        data['band_1'] = data['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\n        data['band_2'] = data['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\n        data['inc_angle'] = pd.to_numeric(data['inc_angle'], errors='coerce')\n        band_1 = np.concatenate([im for im in data['band_1']]).reshape(-1, 75, 75)\n        band_2 = np.concatenate([im for im in data['band_2']]).reshape(-1, 75, 75)\n        full_img = np.stack([band_1, band_2], axis=1)\n        \n        X_train,X_val,y_train,y_val=train_test_split(full_img,data['is_iceberg'].values,\n                                                   test_size=0.22, \n                                                   random_state=(1+e))\n        val_imgs=XnumpyToTensor (X_val)\n        val_targets = YnumpyToTensor(y_val)\n        \n        dset_val = TensorDataset(val_imgs, val_targets)        \n        val_loader = torch.utils.data.DataLoader(dset_val, batch_size=batch_size, shuffle=True, \n                                                    num_workers=1)        \n        print (val_loader)\n\n        eval_loss = 0\n        eval_acc = 0\n        for data in val_loader:\n            img, label = data\n\n            img = Variable(img, volatile=True)\n            label = Variable(label, volatile=True)\n\n            out = model(img)\n            loss = criterion(out, label)\n            eval_loss += loss.data[0] * label.size(0)\n\n        print('VALIDATION Loss: {:.6f}'.format(eval_loss / (len(dset_val))))\n        val_losses.append(eval_loss / (len(dset_val)))\n        print()\n    \ndef LeavOneOutValidation(val_loader): \n    print ('Leave one out VALIDATION ...')\n    val_losses = []\n    model.eval()\n        \n    print (val_loader)\n\n    eval_loss = 0\n    eval_acc = 0\n    for data in val_loader:\n        img, label = data\n\n        img = Variable(img, volatile=True)\n        label = Variable(label, volatile=True)\n\n        out = model(img)\n        loss = criterion(out, label)\n        eval_loss += loss.data[0] * label.size(0)\n\n    print('Leave on out VALIDATION Loss: {:.6f}'.format(eval_loss / (len(val_ds))))\n    val_losses.append(eval_loss / (len(val_ds)))\n    print()\n    print()        \n    \nLeavOneOutValidation(val_loader)    \n# kFoldValidation(10)","metadata":{"_cell_guid":"d7a94ac7-9d78-4aef-9401-0cad5840af9a","_uuid":"4ace55d09023eeac33296d70c037bda67701d45a","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Prediction on test data set","metadata":{"_cell_guid":"3f6d0cdd-ae03-4128-9d7b-7797d58bdfb5","_uuid":"151d206a8aafec78833f400179fa58b6d63bfed4"},"cell_type":"markdown"},{"source":"df_test_set = pd.read_json('../input/test.json')\n\ndf_test_set['band_1'] = df_test_set['band_1'].apply(lambda x: np.array(x).reshape(75, 75))\ndf_test_set['band_2'] = df_test_set['band_2'].apply(lambda x: np.array(x).reshape(75, 75))\ndf_test_set['inc_angle'] = pd.to_numeric(df_test_set['inc_angle'], errors='coerce')\n\ndf_test_set.head(3)","metadata":{"_cell_guid":"a5316df6-e3fa-4372-a284-30ef1d143262","_uuid":"2c73c83252ce8d9977df59a40fa000e5b10ae32d","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"print (df_test_set.shape)\ncolumns = ['id', 'is_iceberg']\ndf_pred=pd.DataFrame(data=np.zeros((0,len(columns))), columns=columns)\n# df_pred.id.astype(int)\n\nfor index, row in df_test_set.iterrows():\n    rwo_no_id=row.drop('id')    \n    band_1_test = (rwo_no_id['band_1']).reshape(-1, 75, 75)\n    band_2_test = (rwo_no_id['band_2']).reshape(-1, 75, 75)\n    full_img_test = np.stack([band_1_test, band_2_test], axis=1)\n\n    x_data_np = np.array(full_img_test, dtype=np.float32)        \n    if use_cuda:\n        X_tensor_test = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n    else:\n        X_tensor_test = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n                    \n#     X_tensor_test=X_tensor_test.view(1, trainX.shape[1]) # does not work with 1d tensors            \n    predicted_val = (model(X_tensor_test).data).float() # probabilities     \n    p_test =   predicted_val.cpu().numpy().item() # otherwise we get an array, we need a single float\n    \n    df_pred = df_pred.append({'id':row['id'], 'is_iceberg':p_test},ignore_index=True)\n#     df_pred = df_pred.append({'id':row['id'].astype(int), 'probability':p_test},ignore_index=True)\n\ndf_pred.head(5)","metadata":{"_cell_guid":"d80fe5d9-07bc-4254-bef9-d8721b9bd226","_uuid":"c486408992e2dfcf0d1a5730d3f6bb051c839c91","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Create a CSV with the ID's and the coresponding probabilities.","metadata":{"_cell_guid":"47afcb96-d2cc-4212-9b15-a33f0777c39b","_uuid":"fcd82743cc346d03f97ad6c2684cb113ab71c1ac"},"cell_type":"markdown"},{"source":"# df_pred.id=df_pred.id.astype(int)\n\ndef savePred(df_pred):\n#     csv_path = 'pred/p_{}_{}_{}.csv'.format(loss, name, (str(time.time())))\n#     csv_path = 'pred_{}_{}.csv'.format(loss, (str(time.time())))\n    csv_path='sample_submission.csv'\n    df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n    print (csv_path)\n    \nsavePred (df_pred)","metadata":{"_kg_hide-output":false,"_cell_guid":"24ebbc8b-01e9-4f8f-8682-34b54720a082","_kg_hide-input":false,"_uuid":"949e671d86d82fe3fea05bbca355aa380a043405","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"6d70809a-43c5-40e0-bcbd-2e57edfca9d7","_uuid":"a0c5d23ee263765549dcc03e80c8d9c53fbb1944","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"9ad22517-6a3b-4361-bc7a-f099034b356b","_uuid":"4c32cd8ca4e3611704a172661b9ba759f6b309ce","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"08fd0448-01b3-4a6a-95e5-9ce36ad9bcb9","_uuid":"86a735b3faa63af3b1420678e1fb812a4bb95f56","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"174e096e-6cbd-40b5-8553-2560bf28582d","_uuid":"dc443e947603c93d1eeb77bb0dfde5f9f297604b","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"c206f5a4-7b13-4845-8327-9c7f5406fe90","_uuid":"2b02d612822db2e58f39591c8e329b97d6283765","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"ba365f1e-911b-401e-b9ce-9b763c8e4f10","_uuid":"6bfa0dbab5b766e9fc38c1dc0bde2f0e1952b5d5","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"c971cf5e-2bb2-4b82-b09a-329e66a2854c","_uuid":"4ad034f7f7947e36fdaac0e2894d35de27054d2b","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"a555a4c2-ab48-4e7c-8634-b0b65811ff2c","_uuid":"bf2b39e71d690f3900b916176435974ab10c5918","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"afcb62b2-2b25-4873-a09f-0caacb5a7fb2","_uuid":"e836872cf64407e8e2ec6d76906fa6ead82893f1","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"_cell_guid":"6b154a7d-69bc-4acc-9287-4b7bb38eff90","_uuid":"b6581f6927bba3c816a4ef4f71ddf91956c0ef68","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]}],"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}}}}