{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","name":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"243fd37a844987a3423c21786089e0b7ab0e2d32","_cell_guid":"60a90787-a40b-4ebf-85e9-9ee414542bbd"},"source":"# Introduction\n\nIn this kernel I aim to outline a common flaw in data augmentation/validation that I see in primarily the keras kernels for this competition. The essence of the problem is that the train/validation split is done after a one-time deterministic data augmentation is applied to grow the size of the dataset. Probably we should just be stochastically augmenting for every batch, but if we insist on augmenting data in this way, then we should do the train/validation split before augmenting. Otherwise, we will have training examples where an augmented version of the same example is in the validation set.\n\nExample kernels:\n- https://www.kaggle.com/a45632/keras-starter-4l-added-performance-graph\n- https://www.kaggle.com/cbryant/keras-cnn-statoil-iceberg-lb-0-1995-now-0-1516\n- https://www.kaggle.com/vincento/keras-starter-4l-0-1694-lb-icebergchallenge\n- https://www.kaggle.com/henokanh/cnn-batchnormalization-0-1646\n- https://www.kaggle.com/hcc1995/keras-cnn-model\n- https://www.kaggle.com/fvzaur/iceberg-ship-classification-with-cnn-on-keras","cell_type":"markdown"},{"metadata":{"_uuid":"599d65a12778f2147bb434b706ac57893c7b1c83","_cell_guid":"1624b075-5f15-4761-b2d9-10d0d8df9c88","collapsed":true},"source":"# Let's get the imports out of the way\nimport pandas as pd\nimport numpy as np\nimport cv2\nnp.random.seed(1234) \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nimport matplotlib.pyplot as plt\n\ndf_train = pd.read_json('../input/train.json')\n\ndef get_scaled_imgs(df):\n    imgs = []\n    \n    for i, row in df.iterrows():\n        #make 75x75 image\n        band_1 = np.array(row['band_1']).reshape(75, 75)\n        band_2 = np.array(row['band_2']).reshape(75, 75)\n        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n        \n        # Rescale\n        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n\n        imgs.append(np.dstack((a, b, c)))\n\n    return np.array(imgs)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7916a459af9f19ff15dcc034dfa10e19f39d8774","_cell_guid":"bcff19cc-1afa-4914-b780-d4b4ba0526e0"},"source":"This is a common function that you see in the kernels. It takes in a set of 3-channel images and outputs a larger array of: the original images, then their vetical flips, then their horizontal flips. I'm not really a fan of this way of augmenting data, but that's not the point of the kernel. Assuming that you were going to do this...","cell_type":"markdown"},{"metadata":{"_uuid":"5de07130112cba92b36b828c1d263ac86bee0dc7","_cell_guid":"908e1d84-2120-466a-9286-b6782247cb2d","collapsed":true},"source":"def get_more_images(imgs):\n    \n    more_images = []\n    vert_flip_imgs = []\n    hori_flip_imgs = []\n      \n    for i in range(0,imgs.shape[0]):\n        a=imgs[i,:,:,0]\n        b=imgs[i,:,:,1]\n        c=imgs[i,:,:,2]\n        \n        av=cv2.flip(a,1)\n        ah=cv2.flip(a,0)\n        bv=cv2.flip(b,1)\n        bh=cv2.flip(b,0)\n        cv=cv2.flip(c,1)\n        ch=cv2.flip(c,0)\n        \n        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n      \n    v = np.array(vert_flip_imgs)\n    h = np.array(hori_flip_imgs)\n       \n    more_images = np.concatenate((imgs,v,h))\n    \n    return more_images","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5cfb422be8cd39ee1507d95e911419ce64a4836b","_cell_guid":"2f5dfeab-e209-41e7-9d57-c568d9a0c59c"},"source":"Let's use a relatively straightforward model to prove this concept.","cell_type":"markdown"},{"metadata":{"_uuid":"239408382042a58c4801d9c7fd2018099b77a422","_cell_guid":"eecaef1b-8684-4301-bb3d-37b66133460e","collapsed":true},"source":"def get_model():\n    model=Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   \n    return model","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f05ad97c5c9ea20163350bd6b260f96a80bce2ea","_cell_guid":"41e0fd02-9e6f-4e3b-8079-3b6e47fa4002"},"source":"# Typical Kernal Validation\n\nBelow we will use the get_more_images function as you see in a typical kernel and vadliate using the `validadtion_split` argument in Keras `model.fit()`","cell_type":"markdown"},{"metadata":{"_uuid":"6eef2013f60b975d51cc6a0b5a01f9b493001bd6","_cell_guid":"2bcdfdf4-7e90-4ad9-ac39-6fb5f8a0ad46","collapsed":true},"source":"Xtrain = get_scaled_imgs(df_train)\nYtrain = np.array(df_train['is_iceberg'])\n\nXtr_more = get_more_images(Xtrain) \nYtr_more = np.concatenate((Ytrain,Ytrain,Ytrain))\n\nmodel = get_model()\nhistory_1 = model.fit(Xtr_more, Ytr_more, batch_size=32, epochs=10, verbose=1, validation_split=0.25)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8d295fb30a5a461c36040ef71b70fb36521de00f","_cell_guid":"d9853f19-2544-4ba8-b641-d1e3083d5bb2"},"source":"# Better Validation\n\nHere I demonstrate what I believe to be a better way of doing validation. First we split our data, and then we use augmentation to increase the size. Maybe we wouldn't even augment the validation set, but we may as well for comparison purposes.","cell_type":"markdown"},{"metadata":{"_uuid":"18665ef391125cf842ac5feccfd01a97b619f860","_cell_guid":"1e4966cb-083b-4f86-bf86-0fcc9c9a6807","collapsed":true},"source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(Xtrain, Ytrain, test_size=0.25)\n\nX_train_more = get_more_images(X_train)\ny_train_more = np.concatenate([y_train, y_train, y_train])\nX_valid_more = get_more_images(X_valid)\ny_valid_more = np.concatenate([y_valid, y_valid, y_valid])\n\nmodel = get_model()\nhistory_2 = model.fit(X_train_more, y_train_more, batch_size=32, epochs=10, verbose=1,\n                     validation_data=(X_valid_more, y_valid_more))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d3ddcd5f5bd8b916f2d5335184e1fd0b462a73de","_cell_guid":"6524d880-56e5-4116-8c98-d4f2acc442e1"},"source":"# Comparison\n\nYou can see right away by sudying the `val_loss` for epoch 10 that these two situations give us different results. Let's just plot them to be sure.","cell_type":"markdown"},{"metadata":{"_uuid":"f9807c0c86698da16b1bfda9c541e60ee2c91f8e","_cell_guid":"b8bc2224-c3c8-4f59-9a49-00a7c87155fe","collapsed":true},"source":"plt.figure(figsize=(12,8))\nplt.plot(history_1.history['val_loss'], label='bad validation')\nplt.plot(history_2.history['val_loss'], label='good validation')\nplt.title('Validation Loss by Epch')\nplt.xlabel('Epoch')\nplt.ylabel('Validation Loss')\nplt.legend()\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ef46c2d84e85620215c079504b58955a164a4bc9","_cell_guid":"0144ded3-b0ec-4176-a5c0-f3433bf90118"},"source":"I think you can see that the \"bad validation\" is dramatically underestimating the true validatdion loss.","cell_type":"markdown"},{"metadata":{"_uuid":"577842759b178725c8439ef4e7437cb287e69bb0","_cell_guid":"db7370ce-0f6a-4157-979a-096342a28ecc"},"source":"","cell_type":"markdown"}]}