{"metadata":{"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"cells":[{"metadata":{},"source":"# Statoil - iceberg\n\n### This notebook is written by H.Namazian","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n%matplotlib inline","cell_type":"code","execution_count":1},{"metadata":{"collapsed":true,"scrolled":true},"outputs":[],"source":"data_path = './data'\ntrain = pd.read_json(data_path + '/' + 'train.json')\ntest = pd.read_json(data_path + '/' + 'test.json')\nsubmission = pd.read_csv(data_path + '/' + 'sample_submission.csv').set_index('id')","cell_type":"code","execution_count":2},{"metadata":{},"source":"## Data preparation","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"train_band_1 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in train.band_1])\ntrain_band_2 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in train.band_2])\nX_train = np.concatenate([train_band_1[:, :, :, np.newaxis], \n                          train_band_2[:, :, :, np.newaxis],\n                          ((train_band_1+train_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n\ntest_band_1 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in test.band_1])\ntest_band_2 = np.array([np.array(band).astype(np.float32).reshape((75,75)) for band in test.band_2])\nX_test = np.concatenate([test_band_1[:, :, :, np.newaxis], \n                         test_band_2[:, :, :, np.newaxis],\n                         ((test_band_1+test_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n\ny = np.array([target for target in train.is_iceberg]).reshape((-1,1))\n\nprint ('train_band_1 shape: {}'.format(train_band_1.shape))\nprint ('train_band_2 shape: {}'.format(train_band_2.shape))\nprint ('train_train shape:  {}'.format(X_train.shape))\nprint ('train label shape:  {}'.format(y.shape))\nprint ('test_band_1 shape:  {}'.format(test_band_1.shape))\nprint ('test_band_2 shape:  {}'.format(test_band_2.shape))\nprint ('test_train shape:   {}'.format(X_test.shape))","cell_type":"code","execution_count":3},{"metadata":{},"source":"## Label Encoding","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"lbl = OneHotEncoder()\nlbl.fit([[0],[1]])\ny = lbl.transform(y).toarray()","cell_type":"code","execution_count":4},{"metadata":{},"source":"## Normalization","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"X_min = np.min(X_train)\nX_max = np.max(X_train)\nX_train = (X_train - X_min)/(X_max - X_min)\nX_test = (X_test - X_min)/(X_max - X_min)","cell_type":"code","execution_count":5},{"metadata":{},"source":"## Data Visualization","cell_type":"markdown"},{"metadata":{},"outputs":[],"source":"fig, ax = plt.subplots(2,4, figsize=[12,8])\n\nax[0,0].imshow(X_train[0,:,:,0])\nax[0,1].imshow(X_train[0,:,:,2])\n\nax[0,2].imshow(X_train[2,:,:,0])\nax[0,3].imshow(X_train[2,:,:,2])\n\nax[1,0].imshow(X_train[1,:,:,0])\nax[1,1].imshow(X_train[1,:,:,2])\n\nax[1,2].imshow(X_train[6,:,:,0])\nax[1,3].imshow(X_train[6,:,:,2])","cell_type":"code","execution_count":6},{"metadata":{},"source":"## functions","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"def get_batches(x, y, batch_size=10):\n    n_batches = len(x)//batch_size\n    for ii in range(0, n_batches*batch_size, batch_size):\n        if ii != (n_batches-1)*batch_size:\n            X, Y = x[ii: ii+batch_size], y[ii: ii+batch_size] \n        else:\n            X, Y = x[ii:], y[ii:]\n        yield X, Y","cell_type":"code","execution_count":9},{"metadata":{},"source":"## CNN Model","cell_type":"markdown"},{"metadata":{"collapsed":true},"outputs":[],"source":"train_data, train_label = X_train[:1400,:,:,:], y[:1400,:]\nval_data, val_label = X_train[1400:,:,:,:], y[1400:,:]","cell_type":"code","execution_count":7},{"metadata":{"collapsed":true,"scrolled":true},"outputs":[],"source":"inputs = tf.placeholder(tf.float32, [None, 75, 75, 3])\nlabels = tf.placeholder(tf.int32)\n\nconv1 = tf.layers.conv2d(inputs=inputs, filters=8, kernel_size=(7,7), strides=(1,1), \n                         padding='SAME', activation=tf.nn.relu, use_bias=True)\npool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(2,2), strides=(2,2), padding='SAME')\n\nconv2 = tf.layers.conv2d(inputs=pool1, filters=16, kernel_size=(5,5), strides=(1,1), \n                         padding='SAME', activation=tf.nn.relu, use_bias=True)\npool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2,2), strides=(2,2), padding='SAME')\n\nconv3 = tf.layers.conv2d(inputs=pool2, filters=16, kernel_size=(3,3), strides=(1,1), \n                         padding='SAME', activation=tf.nn.relu, use_bias=True)\npool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=(2,2), strides=(2,2), padding='SAME')\n\nflat = tf.reshape(pool3, [-1,1600])\n\nfc1 = tf.layers.dense(flat, units=256, use_bias=True, activation=tf.nn.relu)\ndp1 = tf.layers.dropout(fc1, rate=0.25)\n\nfc2 = tf.layers.dense(dp1, units=64, use_bias=True, activation=tf.nn.relu)\ndp2 = tf.layers.dropout(fc2, rate=0.25)\n\nlogits = tf.layers.dense(dp2, units=2, use_bias=True)\n\nloss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=tf.cast(labels, tf.float32))\ncost = tf.reduce_mean(loss)\n\npredicted = tf.nn.softmax(logits)\ncorrect_pred = tf.equal(tf.argmax(predicted, 1), tf.argmax(labels, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\noptimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n\nn_epoches = 100\nbatch_size = 32\n\nsaver = tf.train.Saver()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(n_epoches):\n        for X_batch, y_batch in get_batches(train_data, train_label, batch_size):\n            feed_dict = {inputs:X_batch, labels:y_batch}\n            train_cost,_ = sess.run([cost, optimizer], feed_dict=feed_dict)\n            \n        feed_dict = {inputs:X_train, labels:y}\n        train_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n        feed_dict = {inputs:val_data, labels:val_label}\n        val_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n        print('epoch {}, train accuracy: {:5f}, validation accuracy: {:.5f}'.format(epoch+1, train_accuracy, val_accuracy))\n    saver.save(sess, \"checkpoints/cnn_100.ckpt\")","cell_type":"code","execution_count":54},{"metadata":{"collapsed":true},"outputs":[],"source":"test_batch_size = 128\ntest_pred_res = []\nwith tf.Session() as sess:\n    saver.restore(sess, \"checkpoints/cnn_100.ckpt\")\n    for i in tqdm(range(0, X_test.shape[0], test_batch_size)):\n        test_batch = X_test[i:i+test_batch_size,:,:,:]\n        feed_dict = {inputs: test_batch}\n        test_pred = sess.run(predicted, feed_dict=feed_dict)\n        test_pred_res.append(test_pred.tolist())\n    test_pred_res = np.concatenate(test_pred_res)","cell_type":"code","execution_count":56},{"metadata":{"collapsed":true,"scrolled":true},"outputs":[],"source":"cnn_submit = submission.copy()\ncnn_submit.is_iceberg = test_pred_res[:,1]\ncnn_submit.to_csv('./cnn_100_submit.csv')","cell_type":"code","execution_count":57},{"metadata":{"collapsed":true},"outputs":[],"source":"","cell_type":"code","execution_count":null},{"metadata":{"collapsed":true},"outputs":[],"source":"","cell_type":"code","execution_count":null}],"nbformat":4}