{"cells":[{"source":"The idea of this notebook is to run LightGBM classifier with the following simple features:\n\n**Regular aggregations per band considered as a signal:**\n* Minimum\n* Maximum\n* Mean\n* Standard deviation\n* Kurtosis\n* Skew\n\n**Other aggregrations per band considered as image:**\n* Standard deviation after Sobel filtering on x\n* Standard deviation after Sobel filtering on y\n* Standard deviation after Laplace filtering\n\n**Combined bands :**\n* Pearson correlation coefficient\n* Standard deviation of sqrt(band1 x band1 + band2 x band2)\n\nOne feature extracted from color composite image: Volume of shape inspired from this [notebook](https://www.kaggle.com/submarineering/submarineering-what-about-volume). And finally incidence_angle (all NaN dropped).\n\nLightGBM model is trained with Cross-Validation over 10 stratified folds without any normalization.\n\nResults: \n* Public LB: 0.1807\n* Private LB: 0.2088\n\nNot so bad for a simple model without CNN. Plotting features importance shows that angle of incidence is important.\n","cell_type":"markdown","metadata":{"_uuid":"48b295c654df13513ab42cef309cc897b33e69c6","_cell_guid":"369ace3b-5604-4eae-9aba-83782a8ba67f"}},{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, math, shutil\nfrom scipy.ndimage import gaussian_filter\nfrom scipy.stats import kurtosis, skew\nfrom scipy.ndimage import laplace, sobel\nfrom skimage import img_as_float\nfrom sklearn.externals import joblib\nfrom skimage.morphology import reconstruction\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import StratifiedKFold #for K-fold cross validation\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nANGLE = \"inc_angle\"\nICEBERG = \"is_iceberg\"\nBAND1 = \"band_1\"\nBAND2 = \"band_2\"\nID = \"id\"\ninitial_model_path = \"lgbm\"\n#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"7e58061f0e6bf53c0862edfb7514d13833b026a7","_cell_guid":"b30ac1f2-ce8a-465b-b7e1-a8bdd143b684","collapsed":true}},{"source":"# Load data\ntrain = pd.read_json('../input/train.json')\nprint(\"rows, cols = \" + str(train.shape))\ntrain.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"24eedf826dd599e3e83f3736b97b1c7571a83d7e","_cell_guid":"b89b77a0-9e9a-48ea-bf4d-239f6bc36c7c","collapsed":true}},{"source":"Check for missing data. 133 angles not available, too bad.","cell_type":"markdown","metadata":{"_uuid":"502391a3f443045a9fe0a660a3fe1f7a0d6795e5","_cell_guid":"12a3fbca-1175-4ffa-98ef-d09fc5129a36"}},{"source":"# Any missing data?\ntrain[ANGLE] = pd.to_numeric(train[ANGLE],errors='coerce')\nnullPD = pd.DataFrame(train.isnull().sum(), columns=['TotalNull'])\nnullPD","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"0d879160abda2721f738c9faa0b862c70afdd7e5","_cell_guid":"346ad158-f217-4c64-9f21-0b3497747dc6","collapsed":true}},{"source":"Plot distribution of angle of incidence. It looks we already have good information with it!","cell_type":"markdown","metadata":{"_uuid":"e2228c2fb47563edadc4f8ec4679f052f8771117","_cell_guid":"cfe4b0be-c9a5-4f2e-8182-c58b64854518"}},{"source":"f, ax = plt.subplots(1,2,figsize=(16,5))\nr = train.plot(kind=\"kde\", y = ANGLE, ax=ax[0], label=\"Any\", grid=True, title=\"Angle of incidence KDE\")\nr = train[train[ICEBERG] == 1].plot(kind=\"kde\", label=\"Iceberg\", grid=True, y = ANGLE, ax=ax[1], title=\"Angle of incidence KDE\")\nr = train[train[ICEBERG] == 0].plot(kind=\"kde\", label=\"Not Iceberg\", grid=True, y = ANGLE, ax=r)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"98a5d4376a931bc3ab1c1fef60b3a5f1538afaef","_cell_guid":"5cd6de2f-be9d-4fbc-8776-af0ebffbb06e","collapsed":true}},{"source":"Check for balanced data. Looks good (not imbalanced)!","cell_type":"markdown","metadata":{"_uuid":"664b8283824f460526ecc6d62c5bbf16379a2ebc","_cell_guid":"15e84da0-2626-4338-9e30-c0e04f180845"}},{"source":"P = train.groupby(ICEBERG)[ID].count().reset_index()\nP['Percentage'] = 100*P[ID]/P[ID].sum()\nP","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"3bf5625a0bcda448dadf3db7d47cb237200d3e20","_cell_guid":"d3a31589-fc3a-4abf-9e95-0af3b1dc6d31","collapsed":true}},{"source":"Now the 22 features.","cell_type":"markdown","metadata":{"_uuid":"a2ed00f665d48822ec20a8b033637c03af05ed03","_cell_guid":"a68efed1-4e6a-426e-a1df-a2b9d93f3762"}},{"source":"# Features\nMIN=\"min\"\nMAX=\"max\"\nMEAN=\"mean\"\nSTD=\"std\"\nLAPLACE=\"laplacestd\"\nSOBEL0 = \"sobelstd_x\"\nSOBEL1 = \"sobelstd_y\"\nKURTOSIS = \"kurtosis\"\nSKEW = \"skew\"\nCORR = \"pearson\"\nHYP = \"hypstd\"\n\nAGG_COLS = [\n    \"%s_%s\"%(BAND1,MAX), \"%s_%s\"%(BAND2,MAX),\n    \"%s_%s\"%(BAND1,MIN), \"%s_%s\"%(BAND2,MIN),\n    \"%s_%s\"%(BAND1,MEAN), \"%s_%s\"%(BAND2,MEAN),\n    \"%s_%s\"%(BAND1,STD), \"%s_%s\"%(BAND2,STD),\n    \"%s_%s\"%(BAND1,KURTOSIS), \"%s_%s\"%(BAND2,KURTOSIS),\n    \"%s_%s\"%(BAND1,SKEW), \"%s_%s\"%(BAND2,SKEW),\n    \"%s_%s\"%(BAND1,SOBEL0), \"%s_%s\"%(BAND2,SOBEL0),\n    \"%s_%s\"%(BAND1,SOBEL1), \"%s_%s\"%(BAND2,SOBEL1),\n    \"%s_%s\"%(BAND1,LAPLACE), \"%s_%s\"%(BAND2,LAPLACE),\n    CORR,\n    HYP,\n]\n\n# Volume\nISO = \"iso\"\nISO_COLS = [\n        \"%s_%s\"%(BAND1,ISO), \"%s_%s\"%(BAND2,ISO)\n]\nVOLUME = \"vol\"\n\n# Final features\nFEATURES = [ANGLE, VOLUME] + AGG_COLS\n\n# Isolation function.\ndef iso(arr):\n    image = img_as_float(np.reshape(np.array(arr), [75,75]))\n    image = gaussian_filter(image,2.5)\n    seed = np.copy(image)\n    seed[1:-1, 1:-1] = image.min()\n    mask = image \n    dilated = reconstruction(seed, mask, method='dilation')\n    return image-dilated\n\n# Standard deviation for sobel filter\ndef sobelstd(arr, axis=0):\n    image = img_as_float(np.reshape(np.array(arr), [75,75]))\n    sobelstd = sobel(image, axis=axis, mode='reflect', cval=0.0).ravel()\n    return [sobelstd.std(), sobelstd.max(), sobelstd.mean()]\n\n# Standard deviation for laplace filter\ndef lapacestd(arr):\n    image = img_as_float(np.reshape(np.array(arr), [75,75]))\n    lapacestd = laplace(image, mode='reflect', cval=0.0).ravel()\n    return [lapacestd.std(), lapacestd.max(), lapacestd.mean()]\n\ndef volume(arr):\n    return np.sum(arr)\n\ndef hypot(arr1, arr2):\n    hyp = np.hypot(arr1, arr2)\n    return [np.std(hyp), np.max(hyp), np.median(hyp)]\n\ndef computeAdditionalFeatures(df):\n    # Aggregation on raw signal\n    df[\"%s_%s\"%(BAND1,MAX)] = df[BAND1].apply(lambda x: np.max(x))\n    df[\"%s_%s\"%(BAND2,MAX)] = df[BAND2].apply(lambda x: np.max(x))\n    df[\"%s_%s\"%(BAND1,MIN)] = df[BAND1].apply(lambda x: np.min(x))\n    df[\"%s_%s\"%(BAND2,MIN)] = df[BAND2].apply(lambda x: np.min(x))\n    df[\"%s_%s\"%(BAND1,MEAN)] = df[BAND1].apply(lambda x: np.mean(x))\n    df[\"%s_%s\"%(BAND2,MEAN)] = df[BAND2].apply(lambda x: np.mean(x))\n    df[\"%s_%s\"%(BAND1,STD)] = df[BAND1].apply(lambda x: np.std(x))\n    df[\"%s_%s\"%(BAND2,STD)] = df[BAND2].apply(lambda x: np.std(x))\n    df[\"%s_%s\"%(BAND1,KURTOSIS)] = df[BAND1].apply(lambda x: kurtosis(x))\n    df[\"%s_%s\"%(BAND2,KURTOSIS)] = df[BAND2].apply(lambda x: kurtosis(x))    \n    df[\"%s_%s\"%(BAND1,SKEW)] = df[BAND1].apply(lambda x: skew(x))\n    df[\"%s_%s\"%(BAND2,SKEW)] = df[BAND2].apply(lambda x: skew(x))     \n    df[\"%s_%s\"%(BAND1,SOBEL0)] = df[BAND1].apply(lambda x: sobelstd(x, axis=0)[0])\n    df[\"%s_%s\"%(BAND1,SOBEL1)] = df[BAND1].apply(lambda x: sobelstd(x, axis=1)[0])    \n    df[\"%s_%s\"%(BAND2,SOBEL0)] = df[BAND2].apply(lambda x: sobelstd(x, axis=0)[0])\n    df[\"%s_%s\"%(BAND2,SOBEL1)] = df[BAND2].apply(lambda x: sobelstd(x, axis=1)[0])   \n    df[\"%s_%s\"%(BAND1,LAPLACE)] = df[BAND1].apply(lambda x: lapacestd(x)[0])\n    df[\"%s_%s\"%(BAND2,LAPLACE)] = df[BAND2].apply(lambda x: lapacestd(x)[0])    \n    df[CORR] = df.apply(lambda row: np.corrcoef(x=row[BAND1], y=row[BAND2])[1,0], axis=1)\n    df[HYP] = df.apply(lambda row: hypot(row[BAND1], row[BAND2])[0], axis=1)\n    \n    # Volume\n    df[\"%s_%s\"%(BAND1,ISO)] = df[BAND1].apply(lambda x: iso(x))\n    df[\"%s_%s\"%(BAND2,ISO)] = df[BAND2].apply(lambda x: iso(x))\n    df[VOLUME] = (df[\"%s_%s\"%(BAND1,ISO)] + df[\"%s_%s\"%(BAND2,ISO)]).apply(volume)\n\n    cleanDF = df.dropna()\n    ret = cleanDF[FEATURES].copy(deep=True)\n    ret_labels = None\n    if ICEBERG in cleanDF.columns:\n        ret_labels = cleanDF[[ICEBERG]]\n    ret_ids = cleanDF[[ID]]\n    ret_cols = ret.columns\n            \n    return ret, ret_labels, ret_ids","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"fca4a081edf5ff734eb2bc57438b6dbcb9950d70","_cell_guid":"73df076f-847c-4232-b3db-65d5ba08e9de","collapsed":true}},{"source":"def read_and_normalize_train_data(train_df):\n    featuresDF, labelsDF, idsDF = computeAdditionalFeatures(train_df.copy(deep=True))\n    train_features = featuresDF.as_matrix()\n    train_target = labelsDF.as_matrix()\n    train_id = idsDF.as_matrix()\n    print(\"Features size: %s/%s\"%(str(train_features.shape), str(train_target.shape)))\n    return train_features, train_target, train_id, featuresDF[FEATURES].columns","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"412b3d747073e5e3f5944b62aa041663c7eb4914","_cell_guid":"7333638d-ed40-4287-91c7-4ad5763cc80b","collapsed":true}},{"source":"def fit_evaluate_model_lgbm(X_train, Y_train, X_valid, Y_valid, train_data_columns, model_path, num_fold, importance=False):\n    X_trainDF = pd.DataFrame(X_train, columns=train_data_columns)\n    X_validDF = pd.DataFrame(X_valid, columns=train_data_columns)\n    train_dataset = lgb.Dataset(X_trainDF, Y_train.reshape(Y_train.shape[0]))\n    test_dataset = lgb.Dataset(X_validDF, Y_valid.reshape(Y_valid.shape[0]))\n    # Fit\n    evals_result = {}\n    params = {\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'boosting': 'gbdt',\n            'learning_rate': 0.1,\n            'num_rounds': 150,\n            'early_stopping_rounds': 100\n    }\n    gbm = lgb.train(params, train_dataset, \n                    valid_sets=test_dataset, \n                    evals_result=evals_result,\n                    verbose_eval=50)\n    # Evaluate\n    predict_y_proba_gbm = gbm.predict(X_valid, num_iteration=gbm.best_iteration) # Proba of class 1\n    predict_y_gbm = np.where(predict_y_proba_gbm.reshape((predict_y_proba_gbm.shape[0])) > 0.5, 1, 0)\n\n    score_ll = metrics.log_loss(Y_valid, predict_y_proba_gbm)\n    score_ac = metrics.accuracy_score(Y_valid, predict_y_gbm)\n    score_pr = metrics.precision_score(Y_valid, predict_y_gbm)\n    score_re = metrics.recall_score(Y_valid, predict_y_gbm)\n    score = [score_ll, score_ac, score_pr, score_re]\n    \n    if (importance == True):\n        ax = lgb.plot_importance(gbm, max_num_features=20, figsize=(16, 5))\n        plt.show()\n    \n    gbmDF = pd.DataFrame([tuple(gbm.feature_importance())], columns= gbm.feature_name())\n    gbmDF.sort_index(axis=1, inplace=True)\n\n    return score, gbmDF, gbm","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"498d88997690cf4c20b0fbf63add624fe0f4f602","_cell_guid":"f870a77c-3a01-428c-b386-7edbcd7172e4","collapsed":true}},{"source":"# With KFolder stratified CV.\ndef run_cross_validation_create_models(train_df, model_path, nfolds=4, break_fold=-1, importance=False):\n    train_data, train_target, train_id, train_data_columns = read_and_normalize_train_data(train_df)\n    train_target = train_target.ravel()\n    kf = StratifiedKFold(n_splits=nfolds, random_state=None, shuffle=True)\n    num_fold = 0\n    sum_score_ll = 0\n    sum_score_ac = 0\n    sum_score_pr = 0\n    sum_score_re = 0\n    scores = []\n    models = []\n    importanceDF = pd.DataFrame()\n    for train_index, test_index in kf.split(train_data, train_target):\n        num_fold += 1\n        print('\\n==> Start KFold number {} from {}'.format(num_fold, nfolds))\n        X_train = train_data[train_index]\n        Y_train = train_target[train_index]\n        X_valid = train_data[test_index]\n        Y_valid = train_target[test_index]\n        print(\"Train size: %s/%s\"%(str(X_train.shape), str(Y_train.shape)))\n        print(\"Valid size: %s/%s\"%(str(X_valid.shape), str(Y_valid.shape)))\n        score, impDF, m = fit_evaluate_model_lgbm(X_train,Y_train, X_valid, Y_valid, train_data_columns, \n                                   model_path, num_fold, importance=False)\n        models.append(m)\n        if len(importanceDF) == 0:\n            importanceDF = impDF\n        else:\n            importanceDF = pd.concat([importanceDF, impDF])\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n        sum_score_ll += score[0]*len(test_index)\n        sum_score_ac += score[1]*len(test_index)\n        sum_score_pr += score[2]*len(test_index)\n        sum_score_re += score[3]*len(test_index)\n        scores.append(score)\n        # Break KFold loop\n        if (break_fold > 0) & (break_fold == num_fold):\n            break\n    score_ll = sum_score_ll/len(train_data)\n    score_ac = sum_score_ac/len(train_data)\n    score_pr = sum_score_pr/len(train_data)\n    score_re = sum_score_re/len(train_data)\n    print('\\nCV average scores:')\n    print('log_loss: %s\\naccuracy: %s\\nprecision: %s\\nrecall: %s\\n'%(score_ll, score_ac, score_pr, score_re))\n    return scores, importanceDF, models","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"471ddca17624f9bef38a66d9f6d32b2c519cd605","_cell_guid":"caeca89e-d2cb-4c92-a9b8-e3f7b3d388dc","collapsed":true}},{"source":"# Training\nNFOLDS = 10\nseed = 1337\nnp.random.seed(seed)\nprint(\"-----------   Seed %d   ----------------\"%seed)\nmodel_path = \"%s.cv%d.%d\"%(initial_model_path, NFOLDS, seed)\nos.makedirs(model_path, exist_ok=True)\n\nscores, importanceDF, models = run_cross_validation_create_models(train, model_path, nfolds=NFOLDS, break_fold=-1, importance = True)\nscores = np.array(scores)\nscores_loss = scores[:,0]\nscores_other = scores[:,1:4]\n\nbox_loss = pd.DataFrame(scores_loss, columns=[\"log loss\"])\nbox_other = pd.DataFrame(scores_other*100.0, columns=[\"accuracy\", \"precision\", \"recall\"])\nf, ax = plt.subplots(1, 2, figsize=(16,3))\nbox_other.boxplot(ax=ax[0], showmeans=True)\nax[0].set_title(\"Accuracy, Precision, Recall\")\nbox_loss.boxplot(ax=ax[1], showmeans=True)\nax[1].set_title(\"Log Loss\")\nplt.show()\nprint(\"CV val Log loss: %s\"%(np.mean(scores_loss)))\n\nimportanceDF.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"99a47e603618716f29745310501fa4b6bb7cb8cc","_cell_guid":"99ac7b9c-00d4-454f-9dd3-231356c1c017","collapsed":true,"scrolled":false}},{"source":"mgbmDF = pd.DataFrame(importanceDF.mean(), columns=[\"Importance\"])\nmgbmDF = mgbmDF.apply(lambda x: 100.0 * x / float(x.sum()))\nmgbmDF.sort_values(by=[\"Importance\"], ascending=[True]).plot(kind=\"barh\", legend=False, grid=True, figsize=(16,8))\na = plt.title(\"Features Importance CV mean\")","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"8c4fc0a2b79e1c58809d2e24469fbeabe65628dd","_cell_guid":"e9c9114b-089f-455c-9eeb-bb0751bd8eae","collapsed":true}},{"source":"# Testing\ntest = pd.read_json('../input/test.json')\nprint(\"rows, cols = \" + str(test.shape))","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"41fb13b682290b268bdfda61ff337fcc251ddabc","_cell_guid":"0bf42e56-7fa9-4b74-88d9-0345f454412a","collapsed":true}},{"source":"def read_and_normalize_test_data(test_df):\n    featuresDF, labelsDF, idsDF = computeAdditionalFeatures(test_df.copy(deep=True))\n    test_features = featuresDF.as_matrix()\n    test_id = idsDF.as_matrix()\n    print(\"Features size: %s/%s\"%(str(test_features.shape), str(test_id.shape)))\n    return test_features, test_id, featuresDF[FEATURES].columns","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"8c06619c2d4aefbf879dbaa125b4d52d74572bd4","_cell_guid":"36b3ce70-9eee-45ce-a48a-f27148c9185b","collapsed":true}},{"source":"X_test, X_test_id, X_test_columns = read_and_normalize_test_data(test)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"c035096bc935568f9cefaffb0c3cd52c5c93788f","_cell_guid":"a6d3b27f-e2c5-4022-8f63-37ca54eeff59","collapsed":true}},{"source":"yfull_proba_train = []\nyfull_proba_test = []\nyfull_label_test = []\nnum_fold = 0\n# Run predictions on each fold\nfor model in models:\n    num_fold += 1\n    print('==> Start KFold number {} from {}'.format(num_fold, NFOLDS))\n    # Testing\n    predicted_test = model.predict(X_test, num_iteration=model.best_iteration) # Proba of class 1\n    predicted_test_label = np.where(predicted_test.reshape((predicted_test.shape[0])) > 0.5, 1, 0)\n    yfull_proba_test.append(predicted_test)\n    yfull_label_test.append(predicted_test_label)","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"a553c171b883a4eb5c8a207121f4e61e944de961","_cell_guid":"be3a967d-2c36-45e2-8498-ac80ed34cf84","collapsed":true}},{"source":"def merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    aPD = pd.DataFrame(a, columns = [\"%s_%d\"%(ICEBERG, 1)])\n    for i in range(1, nfolds):\n        a = a + np.array(data[i])\n        aPD[\"%s_%d\"%(ICEBERG, i+1)] = np.array(data[i])\n    a = a / nfolds*1.0\n    return a, aPD","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"4390fdfb5b71f57ddc162d1e7954e828b75c1279","_cell_guid":"5bb29905-c111-4e9b-8bbf-d3e529b3b779","collapsed":true}},{"source":"kfold_cols = [\"%s_%d\"%(ICEBERG, i) for i in range(1, NFOLDS + 1) ]\npredicted_test_mean, predicted_test_pd = merge_several_folds_mean(yfull_proba_test, NFOLDS)\npredicted_test_pd.head()","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"ba55fab5b1ed8ad0e4cdcb9b3a28fd9a049c0947","_cell_guid":"e5dcce8b-2889-4bf6-af94-bc6bc4ae3116","collapsed":true}},{"source":"# Submission file\nsubmission = pd.DataFrame()\nsubmission[ID]=test[ID]\nsubmission[ICEBERG]=predicted_test_mean\nsubmission.to_csv(\"submissionlgbmv1.csv\", index=False, sep=\",\", decimal=\".\")","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"efdf26f08473c7cf6a61c5c4b9656e47b9765890","_cell_guid":"a1c52c79-785c-4312-aba8-0121f8a516bd","_kg_hide-output":false,"collapsed":true}},{"source":"","execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_uuid":"f52786da0d5a8f60bddde80f5f05effac47997dd","_cell_guid":"6c5b4c9b-3a7d-4b6d-adf0-5a01e73caaaa","collapsed":true}}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.4","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}