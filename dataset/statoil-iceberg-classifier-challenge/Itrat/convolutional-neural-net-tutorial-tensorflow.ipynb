{"metadata":{"language_info":{"file_extension":".py","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"metadata":{"_uuid":"ce931d91e1360582fa34153541dd7bb16506c718","_cell_guid":"65d51e1a-2e3d-4a46-8ec1-4c385fbac150"},"cell_type":"markdown","source":"## **Import the required libraries**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"aef0e4375a10846ed786632417e6b3a668b23ef6","_cell_guid":"889c4901-bc13-4e7b-a145-00c215705449"},"cell_type":"code","outputs":[],"source":"# import time\nimport time\nt1 = time.time()"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"a5a1bd0306fb34c97bd6cc32415537bdba8805fd","_cell_guid":"f8f8118b-359d-4ae2-be59-72ee88394c0f"},"cell_type":"code","outputs":[],"source":"import math\nimport random\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib\nfrom matplotlib import pyplot as plt\n%matplotlib inline"},{"metadata":{"_uuid":"fef65d3b2657cbdd5f807ddf329f435b189368b9","_cell_guid":"232c6ab1-d9d9-4ad7-91fc-0e3f4505425c"},"cell_type":"markdown","source":"**Making the results reproducible (knowing the random seed of the two libraries)**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6587e64d2d239b5f3ccac05832fdc9fadf3acf4b","_cell_guid":"0769b51f-3ba7-461b-ba42-34dc3d630088"},"cell_type":"code","outputs":[],"source":"# np_rand_seed = random.randint(0,100)\n# tf_rand_seed = random.randint(0,100)\nnp_rand_seed = 97\ntf_rand_seed = 82\nnp.random.seed(np_rand_seed)"},{"metadata":{"_uuid":"234dc6007bd963e70e10376d46a35672d22cedca","_cell_guid":"ba8e27a4-4f25-40d5-90c1-df3b74e30175"},"cell_type":"markdown","source":"# **1. Load and Inspect the data**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"275ac153352eaca867bf60fe8eecc7d07f4f69c7","_cell_guid":"0a1ed763-ec52-4e6f-88e7-2a10538c2a6c"},"cell_type":"code","outputs":[],"source":"data = pd.read_json('../input/train.json')\ntest_data = pd.read_json('../input/test.json')"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"978c8bee78d52a1953b1484c162e698d4dbf4339","_cell_guid":"59fa9cc3-a66c-4003-a988-99ebc9ebf741"},"cell_type":"code","outputs":[],"source":"data.head(5)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"db9cedd6992ff0eea8d6bc56069cd90482eebcb1","_cell_guid":"72fefb0a-3087-4ab2-8043-cccc54bef7d4"},"cell_type":"code","outputs":[],"source":"test_data.head(5)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"5d1f5b7b7cc8b917742709041c3cc79754c56f31","_cell_guid":"b8d12566-2ba4-4ebc-a4c8-4f48e4075e9d"},"cell_type":"code","outputs":[],"source":"print(\"Shape of train set:\", data.shape)\nprint(\"Shape of test set:\", test_data.shape)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6768152bc325662c30599994df4dcd6e452fb93c","_cell_guid":"2ea9679c-9474-4733-9b78-93a2e6d89e8b"},"cell_type":"code","outputs":[],"source":"print(\"Shape of band 1:\",  np.shape(data.band_1.iloc[0]))\nprint(\"Shape of band 2:\",  np.shape(data.band_2.iloc[0]))"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"69f8276d8dc39b2b01f145d23fb42a98c61f4379","_cell_guid":"c425a835-5ce7-405f-ac72-f47945d117e9"},"cell_type":"code","outputs":[],"source":"print(\"Type of band 1:\",  type(data.band_1.iloc[0]))\nprint(\"Type of band 2:\",  type(data.band_2.iloc[0]))"},{"metadata":{"_uuid":"8081c747d268ee030c1e3b659944ef82954e4f90","_cell_guid":"8e75b4f3-473e-465f-8ac2-b3c4d67a0020"},"cell_type":"markdown","source":"# **2. Feature Engineering**"},{"metadata":{"_uuid":"1c4a6498d16f4b82d4c115dc482442990bb9762c","_cell_guid":"ceb662b0-8aa3-4617-89bd-f5f2329ea6fa"},"cell_type":"markdown","source":"## **2.1 Feature engineering on train set**"},{"metadata":{"_uuid":"1de83b66887477e0d31f1c33a3bb4532ceff5176","_cell_guid":"f8061808-806b-4a9d-a426-f39609522309"},"cell_type":"markdown","source":"### **2.1.1 Replacing the na in inc_anlge with mean**********"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"c73144f16395cdb9b98c384d114003f44efba614","_cell_guid":"f59f93d3-cab1-4972-bed9-7626ba490076"},"cell_type":"code","outputs":[],"source":"data[data['inc_angle']=='na'] = data[data['inc_angle']!='na']['inc_angle'].mean()"},{"metadata":{"_uuid":"0286b0e9b9b7e49a2371eadc85ae0859f5dcdeb7","_cell_guid":"d62e4721-33f2-48e8-8697-4e2b4d17223f"},"cell_type":"markdown","source":"### **2.1.2 Converting the angle from degrees to radian******"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d220ec6223601124059fb6f28913d37a3697d52f","_cell_guid":"b7ed5263-5d98-4608-8d13-a5d41bd30525"},"cell_type":"code","outputs":[],"source":"data['inc_angle'] = data['inc_angle'].apply(lambda x: math.radians(x))"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"cd3a3bd9aca879bcdac06427cfe644e50e489ad6","_cell_guid":"9f73a210-4cab-4322-968e-0b8c3e64e419"},"cell_type":"code","outputs":[],"source":"data.inc_angle.head()"},{"metadata":{"_uuid":"d432da654082ba4241baf62c5b35935718359556","_cell_guid":"ba7060dd-56f8-41ed-a07c-43720d857869"},"cell_type":"markdown","source":"### ** 2.1.3 Finding and droping points with mismatch band1 and band2 data**"},{"metadata":{"_uuid":"52cf51e1f8957ccc549a664b1fbded8c218acddd","_cell_guid":"457faa33-d2f1-441e-a2a6-88fa45b9b716"},"cell_type":"markdown","source":"**Function which return the count and the index of mismatched data**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"108fc6ac01471d76325d9adf608449a6f97a9532","_cell_guid":"17ff47b5-5ab8-4caa-8c5a-ddef625e326a"},"cell_type":"code","outputs":[],"source":"def find_missing_data(series, shape):\n    \n    '''function which return the count and the index of mismatched data'''    \n    count = 0\n    missing_list = []\n    for i,x in enumerate(series):   \n        if np.shape(series.iloc[i]) != shape:\n            missing_list.append(i)\n            count += 1\n            \n    return missing_list, count"},{"metadata":{"_uuid":"ca85acfd8187174e0d3113dddcc7fa9d36d72ad9","_cell_guid":"d0345f6b-9d38-43db-85c2-1a9622694eff"},"cell_type":"markdown","source":"**Count and list of mismatched points in band1**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ecc12a319af2e9f6215935ff1f92e787a4988fff","_cell_guid":"fa6205fe-16f0-4840-8565-1f09b98178c4"},"cell_type":"code","outputs":[],"source":"missing_list1, count1 = find_missing_data(data.band_1, (5625,))\nprint(\"count: \", count1)\nprint(\"missing data: \", missing_list1)"},{"metadata":{"_uuid":"3d27c3e17cf67f027f9a51ef658eb57d0b6181ea","_cell_guid":"2c0cf47e-da7e-4d7c-927d-05cf1dae77f1"},"cell_type":"markdown","source":"**Count and list of mismatched points in band2**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"3de25d4fa2159ef52651a58ccb16b6ef8a6ea4b5","_cell_guid":"aff2c169-e2d2-428d-abbc-462dd9e79609"},"cell_type":"code","outputs":[],"source":"missing_list2, count2 = find_missing_data(data.band_2, (5625,))\nprint(\"count: \", count1)\nprint(\"missing data: \", missing_list2)"},{"metadata":{"_uuid":"e4eb2fdc12ce78db5527af4fdf3a662c11187231","_cell_guid":"c35645ad-3710-4986-b42d-d5f8d3ab8f1a"},"cell_type":"markdown","source":"**Check if the missing points are same**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"be124d3032e55a5efac2ed60ec56e4935ac53868","_cell_guid":"4082937c-1ce6-4409-b240-1cf375129e28"},"cell_type":"code","outputs":[],"source":"missing_list1 == missing_list2"},{"metadata":{"_uuid":"00966316ba5e1384602a25fe93a941ec5a14b1d0","_cell_guid":"0edffb63-a8b7-4dcf-be24-8a99213ea394"},"cell_type":"markdown","source":"**Function to drop data by index**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6b89b6728e54c7424cf5b290248294f9a17f296c","_cell_guid":"f9e21d9b-5aa7-4045-be32-15db73520c14"},"cell_type":"code","outputs":[],"source":"def drop_data(df, index):\n    \n    '''function to drop data by index'''\n    return df.drop(df.index[index])"},{"metadata":{"_uuid":"90b67ffa52acd981973b1fbb9a52d9b25acbd076","_cell_guid":"f1c234e4-738b-44c5-b690-049b76a2221c"},"cell_type":"markdown","source":"**Drop the points with mismatched images**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"0e32bbc67ca5298a9e9ee5cbe4a809266cba0e68","_cell_guid":"7aa9a271-8ce5-4d09-9643-870f5a82cd56"},"cell_type":"code","outputs":[],"source":"data = drop_data(data, missing_list1)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"f7c91ddad5c7b348a37307dac2e03ff630392e3b","_cell_guid":"f3b24d67-fd94-421c-bcd2-8a7d9e27183c"},"cell_type":"code","outputs":[],"source":"data.shape"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"f3ac6619b44f579ad48781185d72a2588766afc4","_cell_guid":"46f737e0-2445-42bf-b7d8-33285d4ea9b4"},"cell_type":"code","outputs":[],"source":"print(\"Number of positive classes: \", len(data[data['is_iceberg'] == 1.0]))\nprint(\"Number of negative classes: \", len(data[data['is_iceberg'] == 0.0]))"},{"metadata":{"_uuid":"ce7741f7f35d38618b49733084efda42f9f13024","_cell_guid":"5a025763-89f2-4c70-9d77-a06bdcce9838"},"cell_type":"markdown","source":"### 2.1.4 Scale the image data"},{"metadata":{"_uuid":"f1cb3b6f7cb2963f7026eef6b164b4ac28cf6b90","_cell_guid":"19a77444-9f8e-4d4d-948e-3a90e9a1f6a7"},"cell_type":"markdown","source":"**3 standardization to technique we can try on**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ae053563db3757bb4dc6eb3bd49992160b05af6b","_cell_guid":"93cde433-87d7-476a-b084-9eaa82433c9d"},"cell_type":"code","outputs":[],"source":"def standardise_vector(vector):\n    '''standardise vector'''\n    standardised_vector = (np.array(vector) - np.mean(vector)) / np.std(vector)\n    return standardised_vector.tolist()"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ead913b3576c08b41c8edfbe560cc5a0b53b6aa0","_cell_guid":"bb29a86b-d6d0-44dd-9271-3c53a7154989"},"cell_type":"code","outputs":[],"source":"def mean_normalise_vector(vector):\n    '''mean normalize vector'''\n    normalised_vector = (np.array(vector) - np.mean(vector)) / (np.max(vector) - np.min(vector))\n    return normalised_vector.tolist()"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"896c3ccb0591a713b201f928a585a3a48cfc1d50","_cell_guid":"ed1b04db-4ef2-4406-9521-6f32fa901ce7"},"cell_type":"code","outputs":[],"source":"def min_max_scaler(vector, minimum = 0, maximum = 1):\n    '''minmaxscaler'''\n    X_std  = (np.array(vector) - np.min(vector)) / (np.max(vector) - np.min(vector))\n    scaled_vector = X_std * (maximum - minimum) + minimum\n    return scaled_vector.tolist()"},{"metadata":{"_uuid":"4ca25f567f80d7dc562326a8c5f50ef8fbc2afa3","_cell_guid":"03254dfd-4e0b-40bb-b6eb-4fc69ed7f483"},"cell_type":"markdown","source":"**We will use standardisation as the  normalization technique since this works well with images**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"bb8239e956b43e0ace7d20357002c6e602b0ed48","_cell_guid":"d80c077d-8d1f-4b9c-8341-02ec60cddefe"},"cell_type":"code","outputs":[],"source":"data['band_1'] = data['band_1'].apply(standardise_vector)\ndata['band_2'] = data['band_2'].apply(standardise_vector)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"723505e0b9dbaaf4ac9f6c605a7bd127f0a8c534","_cell_guid":"9bc78ba9-ece5-4c01-a75c-f54d5ecafdf3"},"cell_type":"code","outputs":[],"source":"data.head(5)"},{"metadata":{"_uuid":"83ccc60bfc074d301861eef82c0f73bc8c65c06e","_cell_guid":"7b76f88f-da1a-403c-81ad-8df9254e7f34"},"cell_type":"markdown","source":"### **2.1.5 Reshaping the band1 and band2 data into 2D image**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"2934df8b01da177eab5a0c74dbdabb5cb18681e5","_cell_guid":"cb679236-ed88-419f-9605-1adc84ab3e23"},"cell_type":"code","outputs":[],"source":"band_1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_1\"]])\nband_2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in data[\"band_2\"]])"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"f9ac3e0531f4e7990884f830421a52aabe003cc7","_cell_guid":"389a1644-5a04-4fc7-9ca5-75b3f9224fd8"},"cell_type":"code","outputs":[],"source":"print(\"Shape of band 1 image:\",band_1.shape)\nprint(\"Shape of band 2 image:\",band_2.shape)"},{"metadata":{"_uuid":"9bc360c6c90d27c127adadfe448af1a3e0d74945","_cell_guid":"f1e0c9c9-9c64-4c89-98d9-c7156d410630"},"cell_type":"markdown","source":"## **2.2 Feature engieering on test Set**"},{"metadata":{"_uuid":"0e2f0ba8f4b69b9c60ba2622dd2afcf1ae70cfd4","_cell_guid":"fb903267-cd31-47a8-9143-cef67abd06e9"},"cell_type":"markdown","source":"**We carry out the same feature engineering as carried out on train set**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"cf760c092267e45eba0aed886c08ad3ec5073153","_cell_guid":"2147cc9a-20c7-45bd-a8b5-4495c1e5fb92"},"cell_type":"code","outputs":[],"source":"test_data['inc_angle'] = test_data['inc_angle'].apply(lambda x: math.radians(x))"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"c324bd680dc6f9f62460bcdcb0f0e1c4c7e046af","_cell_guid":"f84fd6d1-e5fe-4dec-89ad-4da9c9d1e5fb"},"cell_type":"code","outputs":[],"source":"test_data.inc_angle.head()"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"74df1a531cd18eb052f8a55d29611841c4f70109","_cell_guid":"7b9ca0a8-ffba-45fa-91f8-f33158cf773e"},"cell_type":"code","outputs":[],"source":"missing_list3, count3 = find_missing_data(test_data.band_1, (5625,))\nprint(\"count: \", count3)\nprint(\"missing data: \", missing_list3)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"8d23205eef621cc8256b9091ddb731be5013e923","_cell_guid":"e7f908ce-df6e-46b3-9afa-c7152b13fd9a"},"cell_type":"code","outputs":[],"source":"missing_list4, count4 = find_missing_data(test_data.band_2, (5625,))\nprint(\"count: \", count4)\nprint(\"missing data: \", missing_list4)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d1b7a5ca054dbf81d451abded28176387ad86600","_cell_guid":"acf46313-3ce4-4c6e-86e4-961154808436"},"cell_type":"code","outputs":[],"source":"test_data['band_1'] = test_data['band_1'].apply(standardise_vector)\ntest_data['band_2'] = test_data['band_2'].apply(standardise_vector)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ff9e919cd13673bd61e5213a83abd8c8041f3c18","_cell_guid":"2c0a7e57-a905-49fd-86a2-2c99eca4a384"},"cell_type":"code","outputs":[],"source":"band_1_test = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_data[\"band_1\"]])\nband_2_test = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_data[\"band_2\"]])"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"08645230a05d6e67487a9ce7a4ce40c3e5e24761","_cell_guid":"84bb2815-ce50-4b68-9550-42fa2847c63b"},"cell_type":"code","outputs":[],"source":"print(\"Shape of test set band 1 image:\",band_1_test.shape)\nprint(\"Shape of test set band 2 image:\",band_2_test.shape)"},{"metadata":{"_uuid":"aa4007a853e9e5b2d04e525fcf00256085e813b9","_cell_guid":"b515cf3e-2f33-4456-8cea-c803b8a02284"},"cell_type":"markdown","source":"# **3. Train/test/validation split**"},{"metadata":{"_uuid":"341ac6593b7cb1bef50eb528d501ce79149c63fa","_cell_guid":"54b33859-8b8d-4020-9626-1902ca025b16"},"cell_type":"markdown","source":"**Extract the labels and angles of train set**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"eecb2f52fe1b5d7370071c5d165f164ad11d13e0","_cell_guid":"d21487a5-9d5f-4244-af13-2146b569609b"},"cell_type":"code","outputs":[],"source":"labels = data.is_iceberg.as_matrix()\nangles = data.inc_angle.as_matrix()"},{"metadata":{"_uuid":"0a1abe75df22a926d19a5ace9636b8f143b03003","_cell_guid":"0d1ba57c-c175-4138-9066-d5f409846490"},"cell_type":"markdown","source":"**Carry out splits**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6072b7af745245422c114e630a79beadf9a12a9d","_cell_guid":"d5438945-8391-4459-81fa-f472d78c5a85"},"cell_type":"code","outputs":[],"source":"# randomly choosing the train and validation indices\ntrain_indices = np.random.choice(len(labels), round(len(labels)*0.75), replace=False)\nvalidation_indices = np.array(list(set(range(len(labels))) - set(train_indices)))\n\n# extract train set\nband_1_train = band_1[train_indices]\nband_2_train = band_2[train_indices]\nangles_train = angles[train_indices]\nlabels_train = labels[train_indices]\n\n# extract validation set\nband_1_validation = band_1[validation_indices]\nband_2_validation = band_2[validation_indices]\nangles_validation = angles[validation_indices]\nlabels_validation = labels[validation_indices]\n\n# extract test set\nband_1_test = band_1_test\nband_2_test = band_2_test\nangles_test = test_data.inc_angle.as_matrix()\niD = test_data.id.as_matrix()"},{"metadata":{"_uuid":"50af9ec23f4f918c97a2cd43a209e48b33a18a3b","_cell_guid":"60149af0-ec55-413f-b16e-610a136a73d9"},"cell_type":"markdown","source":"**Covert the types of all data to float**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"dbe2f5278d4818cf31580778297009d2bcf1f0bf","_cell_guid":"9a984565-5eaa-4b40-be0d-4be3589c4d44"},"cell_type":"code","outputs":[],"source":"band_1_train = band_1_train.astype(np.float32)\nband_1_validation = band_1_validation.astype(np.float32)\nband_1_test = band_1_test.astype(np.float32)\nband_2_train = band_2_train.astype(np.float32)\nband_2_validation = band_2_validation.astype(np.float32)\nband_2_test = band_2_test.astype(np.float32)\nangles_train = angles_train.astype(np.float32)\nangles_validation = angles_validation.astype(np.float32)\nangles_test = angles_test.astype(np.float32)\nlabels_train = labels_train.astype(np.float32)\nlabels_validation = labels_validation.astype(np.float32)\niD = iD.astype(np.str)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"f2ac78b986ec9532267e0ec9cc3b081a383119ed","_cell_guid":"be173599-cd4e-4233-a3f1-185da3cfa522"},"cell_type":"code","outputs":[],"source":"# delete the unnecessary variables out of memory\ndel(data, test_data, band_1, band_2)"},{"metadata":{"_uuid":"4e96cdb17e2195e1e1572d7ff665280149d9df2a","_cell_guid":"b68af081-f7a2-45f4-bc3a-36e39dd81ed4"},"cell_type":"markdown","source":"**Examine the shape of the data**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"623e269a6e5250e02f0340b7c0fbe1f86252c16c","_cell_guid":"dd1e910a-6992-4177-82bd-6718a853fa93"},"cell_type":"code","outputs":[],"source":"print(\"Shape of band_1_train:\",band_1_train.shape)\nprint(\"Shape of band_2_train:\",band_1_train.shape)\nprint(\"Shape of angles_train:\",angles_train.shape)\nprint(\"Shape of labels_train:\",labels_train.shape)\nprint(\"Shape of band_1_validation:\",band_1_validation.shape)\nprint(\"Shape of band_2_validation:\",band_2_validation.shape)\nprint(\"Shape of angles_validation:\",angles_validation.shape)\nprint(\"Shape of labels_validation:\",labels_validation.shape)\nprint(\"Shape of band_1_test:\",band_1_test.shape)\nprint(\"Shape of band_2_test:\",band_2_test.shape)\nprint(\"Shape of angles_test:\",angles_test.shape)\nprint(\"Shape of iD:\",iD.shape)"},{"metadata":{"_uuid":"e78640113bdcc73128e5d56a25fa2e604d3ac864","_cell_guid":"86e1abbe-f6b5-43f3-9602-eef233c5f039"},"cell_type":"markdown","source":"# **4. Augmenting train set**"},{"metadata":{"_uuid":"83c14ab3ff90f386d3a7dc8dffd556d071814da5","_cell_guid":"9a9dc260-45f0-44e7-b1e1-1cbe4836e406"},"cell_type":"markdown","source":"## **4.1 Functions to carry out different augmentation technique**"},{"metadata":{"_uuid":"c7666a5de11bd07b089511a6400e17463065f06b","_cell_guid":"fd49957d-2d31-41f6-9b47-4fb12cca4253"},"cell_type":"markdown","source":"**4.1.1 Image Rotation**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"fd7746aea0c8488c5351ad1db740afe1232cb7c3","_cell_guid":"aa517240-36a6-4587-a23a-7fed50b60ead"},"cell_type":"code","outputs":[],"source":"def rotate_image(img, angle = 20):\n    \n    '''a function to rotate image by a given degree'''\n    \n    # rotate image\n    original = img.copy()\n\n    M_rotate = cv2.getRotationMatrix2D((37,37),angle,1)\n    img_new = cv2.warpAffine(img,M_rotate,(75,75))\n    \n    length_row = 0\n    length_column = 0\n    boundary_step = 5\n    \n    for i in range(len(img_new)):\n        if img_new[0,i]!=float(0.0):\n            length_row = i\n            break\n    for i in range(len(img_new)):\n        if img_new[i,0]!=float(0.0):\n            length_column = i\n            break\n    \n    # subsitute the padding from original image\n    img_new[:length_column+boundary_step,:length_row+boundary_step] = \\\n    original[:length_column+boundary_step,:length_row+boundary_step] \n    img_new[-(length_row+boundary_step):,:length_column+boundary_step] = \\\n    original[-(length_row+boundary_step):,:length_column+boundary_step]\n    img_new[:length_row+boundary_step,-(length_column+boundary_step):] = \\\n    original[:length_row+boundary_step,-(length_column+boundary_step):]\n    img_new[-(length_column+boundary_step):,-(length_row+boundary_step):] = \\\n    original[-(length_column+boundary_step):,-(length_row+boundary_step):]\n    \n    return img_new"},{"metadata":{"_uuid":"c21f91a272323162b3eb158613166bb0fd0aff65","_cell_guid":"3e60279c-b4c8-42f7-8966-8145fb08377e"},"cell_type":"markdown","source":"**4.1.2 Horizontal translation**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"53905416438c9e6e9d111f4cafb40b6b2f939324","_cell_guid":"af97ae17-8fdd-4946-8826-5ef7f6d51e1e"},"cell_type":"code","outputs":[],"source":"def translate_horizontal(image, shift_horizontal = 5):\n    \n    '''a function to translate image horizontally by a shift'''\n    \n    # horizontally shift image\n    img = image.copy()\n    \n    shift_vertical = 0; \n    if shift_horizontal<0:\n        image_slice = img[:,shift_horizontal:].copy()\n    if shift_horizontal>0:\n        image_slice = img[:,:shift_horizontal].copy()\n    M_translate = np.float32([[1,0,shift_horizontal],[0,1,shift_vertical]])\n    img_new = cv2.warpAffine(img,M_translate,(75,75))\n    \n    # subsitute the padding from original image\n    if shift_horizontal<0:\n        img_new[:,shift_horizontal:] = image_slice\n    if shift_horizontal>0:\n        img_new[:,:shift_horizontal] = image_slice\n        \n    return img_new.reshape(75,75).astype(np.float32)"},{"metadata":{"_uuid":"92955ef9fcabb5138ef4cccdfad99612bc27d9c7","_cell_guid":"321a68c5-6f8d-42ee-b43f-c2382428741f"},"cell_type":"markdown","source":"**4.1.3 Vertical translation**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"a77c02ec97ea12e072cbbdac6f6c9ddccb7c31c0","_cell_guid":"69fe4098-9461-4eb4-bbc7-495122f8ab2e"},"cell_type":"code","outputs":[],"source":"def translate_vertical(image, shift_vertical = 5):\n    \n    '''a function to translate image vertically by a shift'''\n    \n    # vertically shift image\n    img = image.copy()\n    \n    shift_horizontal = 0;\n    if shift_vertical<0:\n        image_slice = img[shift_vertical:,:].copy()\n    if shift_vertical>0:\n        image_slice = img[:shift_vertical,:].copy()\n    M_translate = np.float32([[1,0,shift_horizontal],[0,1,shift_vertical]])\n    img_new = cv2.warpAffine(img,M_translate,(75,75))\n    \n    # subsitute the padding from original image\n    if shift_vertical<0:\n        img_new[shift_vertical:,:] = image_slice\n    if shift_vertical>0:\n        img_new[:shift_vertical,:] = image_slice\n        \n    return img_new.reshape(75,75).astype(np.float32)"},{"metadata":{"_uuid":"26557ae31f21790006c18b4506191938b8275998","_cell_guid":"b0873b41-4d91-4c58-9523-0db20e2b23c1"},"cell_type":"markdown","source":"**4.1.4 Translation along positive diagonal**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"c3913e63bb74b6a3c2acde54ad51b601e39dc4e1","_cell_guid":"e645346d-d086-45b6-92cd-aeece998da23"},"cell_type":"code","outputs":[],"source":"def translate_positive_diagonal(image, shift_diagonal = 5):\n    \n    '''a function to translate image along positive diagonal'''\n    \n    # translate image along positive diagonal\n    img = image.copy()\n    \n    if shift_diagonal<0:\n        hor_slice = img[shift_diagonal:,:].copy()\n        ver_slice = img[:,shift_diagonal:].copy()\n    else:\n        hor_slice = img[:shift_diagonal,:].copy()\n        ver_slice = img[:,:shift_diagonal].copy()\n    M_translate = np.float32([[1,0,shift_diagonal],[0,1,shift_diagonal]])\n    img_new = cv2.warpAffine(img,M_translate,(75,75))\n    \n    # subsitute the padding from original image\n    if shift_diagonal<0:\n        img_new[shift_diagonal:,:] = hor_slice\n        img_new[:,shift_diagonal:] = ver_slice\n    else:\n        img_new[:shift_diagonal,:] = hor_slice\n        img_new[:,:shift_diagonal] = ver_slice\n    \n    return img_new.reshape(75,75).astype(np.float32)"},{"metadata":{"_uuid":"dea2e30274189b2854e5c51c77732ff9fea35497","_cell_guid":"ad345409-8ef3-4464-8698-a9d5ae079e80"},"cell_type":"markdown","source":"**4.1.5 Translation along negative diagonal**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"9786b54ebed361eba2f97a332abc119bb2790e03","_cell_guid":"96e91234-edd2-4276-aaab-2659233677af"},"cell_type":"code","outputs":[],"source":"def translate_negative_diagonal(image, shift_diagonal = 5):\n    \n    '''a function to translate image along negative diagonal'''\n    \n    # translate image along negative diagonal\n    img = image.copy()\n    \n    if shift_diagonal<0:\n        hor_slice = img[:-shift_diagonal,:].copy()\n        ver_slice = img[:,shift_diagonal:].copy()\n    if shift_diagonal>0:\n        hor_slice = img[-shift_diagonal:,:].copy()\n        ver_slice = img[:,:shift_diagonal].copy()\n    M_translate = np.float32([[1,0,shift_diagonal],[0,1,-shift_diagonal]])\n    img_new = cv2.warpAffine(img,M_translate,(75,75))\n    \n    # subsitute the padding from original image\n    if shift_diagonal<0:\n        img_new[:-shift_diagonal,:] = hor_slice\n        img_new[:,shift_diagonal:] = ver_slice\n    if shift_diagonal>0:\n        img_new[-shift_diagonal:,:] = hor_slice\n        img_new[:,:shift_diagonal] = ver_slice\n        \n    return img_new.reshape(75,75).astype(np.float32)"},{"metadata":{"_uuid":"6efca0ecf88472b35bcfa55eee69a83befe0af8a","_cell_guid":"ca1cffcb-0e15-419f-b28f-331f5ef3aeb1"},"cell_type":"markdown","source":"**4.1.6 Flip Image**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"55084dbbb7af094519980b15e16c7eb09c04eea2","_cell_guid":"6c5b3ded-8a55-4dc3-b8e0-a3c1bc97be69"},"cell_type":"code","outputs":[],"source":"def flip(image, direction = 0):\n    \n    '''a function to flip image'''\n    img = image.copy()\n    return cv2.flip(img,direction)"},{"metadata":{"_uuid":"0f2f83ffdbe3003381841c5ccc8b0a91595d8da0","_cell_guid":"aef107c9-71e8-4a66-8de3-535b37777341"},"cell_type":"markdown","source":"**4.1.7 Zoom image**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"421f6527974b9eefec0e26fb52f617cb93ff5b99","_cell_guid":"a7ffe735-a712-4fe2-8c42-1bd5cfd3edb0"},"cell_type":"code","outputs":[],"source":"def zoom(image, zoom_shift = 5):\n    \n    '''a function to zoom image'''\n    \n    # zoom image\n    img = image.copy()\n    \n    # zoom in \n    if zoom_shift>0:\n        # scale\n        img_new = cv2.resize(img, (75+zoom_shift*2,75+zoom_shift*2)) \n        # crop\n        img_new = img_new[zoom_shift:-zoom_shift,zoom_shift:-zoom_shift] \n    # zoom out\n    else:\n        zoom_shift *=-1\n        \n        hor_top = img[:zoom_shift,:]\n        hor_bottom =img[-zoom_shift:,:]\n        ver_left = img[:,:zoom_shift]\n        ver_right = img[:,-zoom_shift:]\n        \n        # scale\n        img_new = cv2.resize(img, (75-zoom_shift*2,75-zoom_shift*2)) \n        # zero padding\n        img_new = cv2.copyMakeBorder(img_new,zoom_shift,zoom_shift,zoom_shift,zoom_shift,\n                                     cv2.BORDER_CONSTANT,value=0.0)\n        # subsitute the padding from original image\n        img_new[:zoom_shift,:] = hor_top\n        img_new[-zoom_shift:,:] = hor_bottom\n        img_new[:,:zoom_shift] = ver_left\n        img_new[:,-zoom_shift:] = ver_right     \n        \n    return img_new.reshape(75,75).astype(np.float32)"},{"metadata":{"_uuid":"fba70b21dc0f5b30fef9f198c2f085b91c849c4e","_cell_guid":"f6df0cdd-12d5-4802-9362-adee3781099a"},"cell_type":"markdown","source":"## **4.2 Displaying augmented samples**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"29c6e9968e6b27e507840bdd68c0558b5cdad8e0","_cell_guid":"b30fd90b-ae48-482c-85ea-88a164f7abb7"},"cell_type":"code","outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (20.0, 14.0)\nimage = band_1_test[3].copy()\nplt.subplot(3, 5, 1)\nplt.title(\"Original Image\")\nplt.imshow(image)\nplt.subplot(3, 5, 2)\ngenerated_image = rotate_image(image,40)\nplt.title(\"Rotation by +ve degree\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 3)\ngenerated_image = rotate_image(image,-40)\nplt.title(\"Rotation by -ve degree\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 4)\ngenerated_image = translate_horizontal(image,10)\nplt.title(\"Horizonation translation to right\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 5)\ngenerated_image = translate_horizontal(image,-10)\nplt.title(\"Horizonation translation to left\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 6)\ngenerated_image = translate_vertical(image,10)\nplt.title(\"Vertical translation downward\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 7)\ngenerated_image = translate_vertical(image,-10)\nplt.title(\"Vertical translation upward\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 8)\ngenerated_image = translate_positive_diagonal(image,10)\nplt.title(\"SE translation\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 9)\ngenerated_image = translate_positive_diagonal(image,-10)\nplt.title(\"NW translation\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 10)\ngenerated_image = translate_negative_diagonal(image,10)\nplt.title(\"NE translation\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 11)\ngenerated_image = translate_negative_diagonal(image,-10)\nplt.title(\"SW translation\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 12)\ngenerated_image = flip(image,0)\nplt.title(\"Vertical flip\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 13)\ngenerated_image = flip(image,1)\nplt.title(\"Horizontal flip\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 14)\ngenerated_image = zoom(image,10)\nplt.title(\"Zoom in\")\nplt.imshow(generated_image)\nplt.subplot(3, 5, 15)\ngenerated_image = zoom(image,-10)\nplt.title(\"Zoom out\")\nplt.imshow(generated_image)\nplt.show()"},{"metadata":{"_uuid":"74c6d702850ddb99bbb877352a0db3214eab9b45","_cell_guid":"74f3a561-59bc-4499-a3d6-4ae38bd526f0"},"cell_type":"markdown","source":"## **4.3 Augmentation of train set**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"54f811a3ef54965a0513b30a325fe4d6a8399e07","_cell_guid":"9587eef7-a980-484f-a5a5-1028040dd7fc"},"cell_type":"code","outputs":[],"source":"def augment_data(band1, band2, angles, labels):\n    \n    '''a function to augment band1 and band2 image'''\n    \n    # list to store the generated data\n    band1_generated = []\n    band2_generated = []\n    angles_generated = []\n    labels_generated = []\n    \n    # iterate through each point in train set\n    for i in range(labels.shape[0]):\n        \n        # rotate by positive degree\n        angle = np.random.randint(5,20)\n        band1_generated.append(rotate_image(band1[i],angle)) \n        band2_generated.append(rotate_image(band2[i],angle))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # rotate by negative degree\n        angle = np.random.randint(5,20)\n        band1_generated.append(rotate_image(band1[i],-angle)) \n        band2_generated.append(rotate_image(band2[i],-angle))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # positive horizontal shift\n        shift = np.random.randint(3,7)\n        band1_generated.append(translate_horizontal(band1[i],+shift)) \n        band2_generated.append(translate_horizontal(band2[i],+shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # negative horizontal shift\n        shift = np.random.randint(3,7) \n        band1_generated.append(translate_horizontal(band1[i],-shift)) \n        band2_generated.append(translate_horizontal(band2[i],-shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # positive vertical shift\n        shift = np.random.randint(0,7)  \n        band1_generated.append(translate_vertical(band1[i],+shift)) \n        band2_generated.append(translate_vertical(band2[i],+shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # negative vertical shift\n        shift = np.random.randint(3,7) \n        band1_generated.append(translate_vertical(band1[i],-shift)) \n        band2_generated.append(translate_vertical(band2[i],-shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # translate along positive diagonal in positive direction\n        shift = np.random.randint(3,7)  \n        band1_generated.append(translate_positive_diagonal(band1[i],+shift)) \n        band2_generated.append(translate_positive_diagonal(band2[i],+shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # translate along positive diagonal in negative direction\n        shift = np.random.randint(3,7)  \n        band1_generated.append(translate_positive_diagonal(band1[i],-shift)) \n        band2_generated.append(translate_positive_diagonal(band2[i],-shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # translate along negative diagonal in positive direction\n        shift = np.random.randint(3,7)   \n        band1_generated.append(translate_negative_diagonal(band1[i],+shift)) \n        band2_generated.append(translate_negative_diagonal(band2[i],+shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # translate along negative diagonal in negative direction\n        shift = np.random.randint(3,7)   \n        band1_generated.append(translate_negative_diagonal(band1[i],-shift)) \n        band2_generated.append(translate_negative_diagonal(band2[i],-shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # vertical flip\n        band1_generated.append(flip(band1[i],0)) \n        band2_generated.append(flip(band2[i],0))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # horizontal flip\n        band1_generated.append(flip(band1[i],1)) \n        band2_generated.append(flip(band2[i],1))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # zoom in image\n        zoom_shift = np.random.randint(2,5)\n        band1_generated.append(zoom(band1[i],zoom_shift)) \n        band2_generated.append(zoom(band2[i],zoom_shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])\n        \n        # zoom out image\n        zoom_shift = np.random.randint(2,5) \n        band1_generated.append(zoom(band1[i],-zoom_shift)) \n        band2_generated.append(zoom(band2[i],-zoom_shift))\n        angles_generated.append(angles[i])\n        labels_generated.append(labels[i])        \n        \n    # convert the generated data into numpy array\n    band1_generated = np.array(band1_generated)\n    band2_generated = np.array(band2_generated)\n    angles_generated = np.array(angles_generated)\n    labels_generated = np.array(labels_generated)\n    \n    # concatenate the generated data to original train set\n    band1_augmented = np.concatenate((band1, band1_generated),axis=0)\n    band2_augmented = np.concatenate((band2, band2_generated),axis=0)\n    angles_augmented = np.concatenate((angles, angles_generated),axis=0)\n    labels_augmented = np.concatenate((labels, labels_generated),axis=0)\n    \n    return band1_augmented, band2_augmented, angles_augmented, labels_augmented"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"e7861fcbe44ec65a91a900633da546e6711c16c2","_cell_guid":"9c625022-16e7-4f65-9000-02b8afef57a4"},"cell_type":"code","outputs":[],"source":"# augment train set\nband_1_train, band_2_train, angles_train, labels_train = \\\n    augment_data(band_1_train, band_2_train, angles_train, labels_train)"},{"metadata":{"_uuid":"4b250de5e7c99ed5c514008a5bfc1666c52cd9d1","_cell_guid":"858c8b60-0fbb-482f-b83e-f5455c7cd2c0"},"cell_type":"markdown","source":"**Examine the shape of augmented data**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"bc0b27c8892a107a93a16ecc9754594fed11059c","_cell_guid":"aaf38ea0-f683-40d7-9830-72fccdf74d43"},"cell_type":"code","outputs":[],"source":"print(\"Shape of band_1_train:\",band_1_train.shape)\nprint(\"Shape of band_2_train:\",band_2_train.shape)\nprint(\"Shape of angles_train:\",angles_train.shape)\nprint(\"Shape of labels_train:\",labels_train.shape)"},{"metadata":{"_uuid":"6210cd84ac3592a3afc8c9ca2b86ed82663c82d8","_cell_guid":"00f68f8f-dd57-430d-9a37-6fcbb9d96a88"},"cell_type":"markdown","source":"# **5. Concatenate the band1 and band2 data into 3D image**"},{"metadata":{"_uuid":"35691ccb27ed0abdffb2201795ba3fe588bec8c3","_cell_guid":"149f153d-72f6-42a4-96a4-13f5ff572f14"},"cell_type":"markdown","source":"**Here we stack band_1, band_2, and average of the two to create a 3D image**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6f1bc08fbd08798e703c9d220d63b76a5869acf3","_cell_guid":"5dd8ac27-559d-4460-bba2-fe1ddd55cf2a"},"cell_type":"code","outputs":[],"source":"image_train = np.concatenate([band_1_train[:, :, :, np.newaxis],\n                             band_2_train[:, :, :, np.newaxis],\n                             ((band_1_train+band_2_train)/2)[:, :, :, np.newaxis]],\n                             axis=-1)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"49a5a584d7bd0410c6388c79ef0163cea4a10971","_cell_guid":"e5e2b891-6285-4dd7-996b-1a032d8e401d"},"cell_type":"code","outputs":[],"source":"image_validation = np.concatenate([band_1_validation[:, :, :, np.newaxis],\n                             band_2_validation[:, :, :, np.newaxis],\n                             ((band_1_validation+band_2_validation)/2)[:, :, :, np.newaxis]],\n                             axis=-1)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"3a8c2bbcc922e93ce4bde9ad1c7e3816943c143e","_cell_guid":"3a3c9324-f9bd-4caa-8eff-22fb65ada9a4"},"cell_type":"code","outputs":[],"source":"image_test = np.concatenate([band_1_test[:, :, :, np.newaxis],\n                             band_2_test[:, :, :, np.newaxis],\n                             ((band_1_test+band_2_test)/2)[:, :, :, np.newaxis]],\n                             axis=-1)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"5230a8714b07eff239b63e410f00c95be206ad48","_cell_guid":"d6e7f949-175a-414c-9cd6-633a5b97f35f"},"cell_type":"code","outputs":[],"source":"# delete the unnecessary variables out of memory\ndel(band_1_train, band_1_validation, band_1_test, band_2_train, band_2_validation, band_2_test)"},{"metadata":{"_uuid":"a8cdc9ca9c0e22fde443064e9f9158ec1fbafe3a","_cell_guid":"5f09f9a1-fb44-4177-b0d6-56228c272e72"},"cell_type":"markdown","source":"**Examine the shape of 3D images**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"45b99a5d71fd8ed6b0a5fb96b5c43860d799991b","_cell_guid":"57a9e8ed-6939-4a4d-8263-2868ea460d99"},"cell_type":"code","outputs":[],"source":"print(\"Shape of image_train:\",image_train.shape)\nprint(\"Shape of image_validation:\",image_validation.shape)\nprint(\"Shape of image_test:\",image_test.shape)"},{"metadata":{"_uuid":"384670c874a5d9a3a15b5695c272c83f1330600e","_cell_guid":"489070f8-65d8-41bd-bbe7-4f75e51fa2cf"},"cell_type":"markdown","source":"# **6. Creating Convolutional Neural Network**"},{"metadata":{"_uuid":"26e48ef04d0fdc67dbcc721a7d1221fb9b92a478","_cell_guid":"0a899373-2b7d-411a-834a-1c8e11de5160"},"cell_type":"markdown","source":"**Import tensorflow and reset default graph**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d56bcf5c484d635476c67678c0c04e39261fb8d7","_cell_guid":"e898585c-3f91-4949-8400-1629a355e787"},"cell_type":"code","outputs":[],"source":"import tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\ntf.set_random_seed(tf_rand_seed)\n# sess = tf.InteractiveSession()"},{"metadata":{"_uuid":"12a05a3dd884a6b0b226246de50a7b05cf44a826","_cell_guid":"556b305d-9d13-4262-b75d-07bad395d18b"},"cell_type":"markdown","source":"## **6.1 One hot encoding labels**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"01a34b3c6936d379e7ab5be528500953e418abdb","_cell_guid":"8482537b-d874-487e-8898-2ecbeded489b"},"cell_type":"code","outputs":[],"source":"labels_train = pd.get_dummies(labels_train).as_matrix()\nlabels_validation = pd.get_dummies(labels_validation).as_matrix()"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"96307318f83bf88ec1e55a17ecd0bb92dc052d7b","_cell_guid":"dfd4f585-6ab3-4a9d-aa9d-b9ce01a76714"},"cell_type":"code","outputs":[],"source":"print(\"Shape of labels_train:\", labels_train.shape)\nprint(\"Shape of labels_validation:\", labels_validation.shape)"},{"metadata":{"_uuid":"0f326c144fa38bd57254a30e0902116a62f2a701","_cell_guid":"97390423-6a4a-4574-94c7-9809791c41b9"},"cell_type":"markdown","source":"## **6.2 Create placeholders**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d309c64fed3e72bf8a41e929846eead2269b8149","_cell_guid":"f96e9ef5-95bc-4994-abc1-a420f380bae1"},"cell_type":"code","outputs":[],"source":"# image dimensions\nwidth = 75\nheight = 75\nnum_channels = 3\nflat = width * height\nnum_classes = 2"},{"metadata":{"_uuid":"bc1d45292d65a93ede00c9ed76515927d3a9e751","_cell_guid":"a9d96e2c-a453-4bcb-bd6c-10a48c80a25a"},"cell_type":"markdown","source":"**Create placeholder for image, labels,  dropout keep probability, and optionally angle**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"faab2559119ab1dfc8b9c2c4ee267c484b5a35d3","_cell_guid":"fbe6dbf9-4dfc-4ecb-b234-013ce2864cc1"},"cell_type":"code","outputs":[],"source":"image = tf.placeholder(tf.float32, shape=[None, height, width, num_channels])\n# angle = tf.placeholder(tf.float32, shape= [None, 1])\ny_true = tf.placeholder(tf.int32, shape=[None, num_classes])\nkeep_prob = tf.placeholder(tf.float32)"},{"metadata":{"_uuid":"14d3dbf31065d8c3be26430af935158895f23117","_cell_guid":"9581cf50-b42f-4b02-a099-e30384b9adba"},"cell_type":"markdown","source":"## **6.3 Create functions for creating deep learning layers**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"4a4494877d9c18fe2b07af4d142d8e76fa3d1abc","_cell_guid":"7f563768-ed77-4d6a-b8d8-11bb1eefbade"},"cell_type":"code","outputs":[],"source":"def create_weights(shape):\n    '''a function to create weight tensor'''\n    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n \ndef create_biases(size):\n    '''a function to create bias tensor'''\n    return tf.Variable(tf.constant(0.05, shape=[size]))"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"6238bc6006136867fbc5bd600933a7a9a05074c9","_cell_guid":"46c51707-7a6e-4cfb-bf9b-830b5d4ec204"},"cell_type":"code","outputs":[],"source":"def create_convolutional_layer(input,\n                               num_input_channels,\n                               conv_filter_size,\n                               max_pool_filter_size,\n                               num_filters):  \n    \n    '''a function to create convoutional layer'''\n    \n    # create filter for the convolutional layer\n    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n    \n    # create biases\n    biases = create_biases(num_filters)\n    \n    # create covolutional layer\n    layer = tf.nn.conv2d(input=input,\n                     filter=weights,\n                     strides=[1, 1, 1, 1],\n                     padding='SAME')\n    \n    # add the bias to the convolutional layer\n    layer += biases\n    \n    # relu activation layer fed into layer\n    layer = tf.nn.relu(layer)\n    \n    # max pooling to half the size of the image\n    layer = tf.nn.max_pool(value=layer,\n                            ksize=[1, max_pool_filter_size, max_pool_filter_size, 1],\n                            strides=[1, 2, 2, 1],\n                            padding='SAME')\n        \n    # return the output layer of the convolution\n    return layer"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"8d97660a66941ab4374cd6a6d54376f0ba4c8982","_cell_guid":"31172ffc-01bd-4c09-aefa-d8f4bb19f944"},"cell_type":"code","outputs":[],"source":"def create_flatten_layer(layer):\n    \n    '''a function for creating flattened layer from convolutional output'''\n    \n    # extract the shape of the layer\n    layer_shape = layer.get_shape()\n    # calculate the number features of the flattened layer\n    num_features = layer_shape[1:4].num_elements()\n    # create the flattened layer\n    layer = tf.reshape(layer, [-1, num_features])\n    # return the layer\n    return layer"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"8d94f5193f6fd1c610574eeb345091853b4815ae","_cell_guid":"89c03a25-c9ea-45f2-bf57-1e539e303a8a"},"cell_type":"code","outputs":[],"source":"def create_fc_layer(input,          \n                    num_inputs,    \n                    num_outputs,\n                    use_relu=True,\n                    dropout = False, \n                    keep_prob = 0.2):\n    \n    '''a function for creating fully connected layer'''\n    \n    #Let's define trainable weights and biases.\n    weights = create_weights(shape=[num_inputs, num_outputs])\n    biases = create_biases(num_outputs)\n    \n    # matrix multiplication between input and weight matrix\n    layer = tf.matmul(input, weights) + biases\n    \n    # add relu activation if wanted\n    if use_relu:\n        layer = tf.nn.relu(layer)\n        \n    # if dropout is wanted add dropout\n    if dropout:        \n        layer = tf.nn.dropout(layer, keep_prob)\n    \n    # return layer\n    return layer"},{"metadata":{"_uuid":"8757842d89257cc9e43957de412a0536179475da","_cell_guid":"733844d6-ad60-4267-9d56-dee86b4b80fb"},"cell_type":"markdown","source":"## **6.4 Create Layers of Covnet**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"be7fa26badd1f22aa94f45bf410509b8fce2515f","_cell_guid":"dc7f32ea-5387-4e76-a234-5bcbd0aeb360"},"cell_type":"code","outputs":[],"source":"# paramters for 1st convolutional layer\nconv1_features = 64\nconv1_filter_size = 3\nmax_pool_size1 = 2\n\n# paramters for 2nd convolutional layer\nconv2_features = 128\nconv2_filter_size = 3\nmax_pool_size2 = 2\n\n# paramters for 3rd convolutional layer\nconv3_features = 128\nconv3_filter_size = 3\nmax_pool_size3 = 2\n\n# paramters for 4th convolutional layer\nconv4_features = 64\nconv4_filter_size = 3\nmax_pool_size4 = 2\n\n# number of featuers of 1st fully connected layer\nfc_layer_size1 = 512\n\n# number of featuers of 2nd fully connected layer\nfc_layer_size2 = 256"},{"metadata":{"_uuid":"ec02a2a96d8b7caaaf462683ee139d59dc639afb","_cell_guid":"46215e33-9f44-4051-8f6b-2b26d97f01ec"},"cell_type":"markdown","source":"**Create convolutional layer 1**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"83240829d931db0742a5a82ae9c8cf3d1f2bd957","_cell_guid":"8395723a-7061-40ba-86c3-bc67d02dd90d"},"cell_type":"code","outputs":[],"source":"layer_conv1 = create_convolutional_layer(input=image,\n                                         num_input_channels= num_channels,\n                                         conv_filter_size = conv1_filter_size,\n                                         max_pool_filter_size = max_pool_size1,\n                                         num_filters = conv1_features)\nlayer_conv1"},{"metadata":{"_uuid":"fbb57f121c9d99c7d6d6cc6776899408b3aab52e","_cell_guid":"1d979586-de2c-4be8-b275-fbb691412a4c"},"cell_type":"markdown","source":"**Create convolutional layer 2**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"d8d51144ac3f6ff3e577243ea02367155e1d107e","_cell_guid":"c482934e-bc4c-450f-8f51-c93851819a33"},"cell_type":"code","outputs":[],"source":"layer_conv2 = create_convolutional_layer(input=layer_conv1,\n                                         num_input_channels= conv1_features,\n                                         conv_filter_size = conv2_filter_size,\n                                         max_pool_filter_size = max_pool_size2,\n                                         num_filters = conv2_features)\nlayer_conv2"},{"metadata":{"_uuid":"39a726eab1eb99ff4da8921060bb33f03a7d9335","_cell_guid":"56ebb6fe-b0c2-4e67-accd-e912ed7baeb4"},"cell_type":"markdown","source":"**Create convolutional layer 3**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"e12dfae3bcadd8ebbe8fdd7e398f85b822e55615","_cell_guid":"490ffd07-c4ca-40c0-b9d6-48364fc9a242"},"cell_type":"code","outputs":[],"source":"layer_conv3 = create_convolutional_layer(input=layer_conv2,\n                                         num_input_channels= conv2_features,\n                                         conv_filter_size = conv3_filter_size,\n                                         max_pool_filter_size = max_pool_size3,\n                                         num_filters = conv3_features)\nlayer_conv3"},{"metadata":{"_uuid":"8aaeab46a0a75569fcfe42f17ff995fe80975811","_cell_guid":"641313b1-8324-4aa4-9c7b-0992afcf7db1"},"cell_type":"markdown","source":"**Create convolutional layer 4**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"5f2940f265c34616882056afec21ed207adf1d90","_cell_guid":"245e200d-cec8-4b1a-8ff4-4bbfe9ff1134"},"cell_type":"code","outputs":[],"source":"layer_conv4 = create_convolutional_layer(input=layer_conv3,\n                                         num_input_channels= conv3_features,\n                                         conv_filter_size = conv4_filter_size,\n                                         max_pool_filter_size = max_pool_size4,\n                                         num_filters = conv4_features)\nlayer_conv4"},{"metadata":{"_uuid":"c0d1fba42eeffc8449a05226b821b78b5467304a","_cell_guid":"447be3ea-65bf-4849-90e0-d5f137f4dfae"},"cell_type":"markdown","source":"**Flatten the output of last convolutional layer**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"0029beae7c95206075edaca6476b8202abfd4e70","_cell_guid":"165e82a2-3eb2-4e4a-9894-3721f07b38fb"},"cell_type":"code","outputs":[],"source":"layer_flat = create_flatten_layer(layer_conv4)\nlayer_flat"},{"metadata":{"_uuid":"936382641cb46b416fa1daee15fc9ce3582618e1","_cell_guid":"ccc14524-2daa-4508-93f9-96248109e605"},"cell_type":"markdown","source":"**Create a connected layer for angle and concat this with the fully connected layer (OPTIONAL)**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"55d82528b489fa49df3a9f14605e2d2cbaa9cb10","_cell_guid":"1f4e97ac-cccc-4964-b455-08b04d3f5036"},"cell_type":"code","outputs":[],"source":"# layer_angle = create_fc_layer(input = angle,\n#                               num_inputs=1,\n#                               num_outputs=1,\n#                               use_relu= True)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"eae39bcff711f872e538961873a4714413a949c9","_cell_guid":"6c307554-c25d-4509-a970-de2256716b1d"},"cell_type":"code","outputs":[],"source":"# combined_layer = tf.concat((layer_flat, layer_angle), axis=1)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"bfb293a098c097e688db997f142a3ab288e44f1e","_cell_guid":"56d4b3d9-e5ba-4753-bcde-60a43e27feb6"},"cell_type":"code","outputs":[],"source":"# layer_fc1 = create_fc_layer(input=combined_layer,\n#                             num_inputs=combined_layer.get_shape()[1:4].num_elements(),\n#                             num_outputs=fc_layer_size1,\n#                             use_relu=True,\n#                             dropout =True,\n#                             keep_prob = keep_prob)"},{"metadata":{"_uuid":"71663fb30a168b87d17fe22af7540f75ab5063f1","_cell_guid":"1b13e8cd-5569-411d-8842-0e8a3f37c715"},"cell_type":"markdown","source":"**Create the first fully connected layer**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"19cd20866f5fb39458636733dde47051acf6a6c2","_cell_guid":"68bcfb64-2a79-4ae0-b5b9-ee5b4ddd10a6"},"cell_type":"code","outputs":[],"source":"layer_fc1 = create_fc_layer(input=layer_flat,\n                            num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n                            num_outputs=fc_layer_size1,\n                            use_relu=True,\n                            dropout =True,\n                            keep_prob = keep_prob)\nlayer_fc1"},{"metadata":{"_uuid":"078052726a28c5d4d5c1bfe3f6e1f771f68682d6","_cell_guid":"362493b5-55d7-4a89-b8f3-389e38f9d51e"},"cell_type":"markdown","source":"**Create the second  fully connected layer**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"51b1fe669b6d40ec320e1fe18dd75fa287580325","_cell_guid":"e739c54b-eff2-4ddd-88b0-772862b49fcb"},"cell_type":"code","outputs":[],"source":"layer_fc2 = create_fc_layer(input=layer_fc1,\n                            num_inputs=fc_layer_size1,\n                            num_outputs=fc_layer_size2,\n                            use_relu=True,\n                            dropout =True,\n                            keep_prob = keep_prob)\nlayer_fc2"},{"metadata":{"_uuid":"da257d4d2aee1d94cda2a11de7f8b4b08743bbc6","_cell_guid":"d875e8a1-456a-4db1-b4a6-59da827bfb29"},"cell_type":"markdown","source":"**Create the output layer**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"b9be66ace22a4dda6292bbed115dbdb19a2fbd44","_cell_guid":"417abe95-a10e-47d9-924d-17b8536560fe"},"cell_type":"code","outputs":[],"source":"output_layer = create_fc_layer(input=layer_fc2,\n                     num_inputs = fc_layer_size2,\n                     num_outputs = num_classes,\n                     use_relu=False)\noutput_layer"},{"metadata":{"_uuid":"399cb0b3d7ab28317b80c5223f0235dc1f043c94","_cell_guid":"65fdd93d-837d-451d-a8f1-d062a3c128f7"},"cell_type":"markdown","source":"## **6.5 Create prediction & accuracy metric**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"7867e90b3d4f84e14f21a17193d1635b7c4dc9af","_cell_guid":"bda3d034-92ec-4551-b6c6-a0bbbf0e227b"},"cell_type":"code","outputs":[],"source":"# softmax operation on the output layer\ny_pred = tf.nn.softmax(output_layer)\n# extract the vector of predicted class\ny_pred_cls = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n# extract the vector of labels\ny_true_cls = tf.argmax(y_true, axis=1, output_type=tf.int32)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"f1f266f8f5295489d75d280290d04edca7b68e48","_cell_guid":"32a17e5a-1ad6-419b-94b5-7821462281d6"},"cell_type":"code","outputs":[],"source":"# extract the vector of correct prediction\ncorrect_prediction = tf.equal(y_pred_cls, y_true_cls)\n# operation to calculate accuracy\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"},{"metadata":{"_uuid":"a231ed620c1235ecb1d9a67424050579964c053a","_cell_guid":"a839a5ce-81ac-444d-a539-27bc7b2b31e6"},"cell_type":"markdown","source":"## **6.6 Create Optimizer**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"954598ebd2b0bd919007750b2c9109b37fefadd8","_cell_guid":"95eaf536-4f2f-4e8e-b657-eb2ac917531c"},"cell_type":"code","outputs":[],"source":"# operation to calculate cross entropy\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer,\n                                                    labels=y_true)\n# mean of cross entropy to act as the loss\nloss = tf.reduce_mean(cross_entropy)"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"7cf565d4965fbe220672deb528624ba2f52b4019","_cell_guid":"cb63c693-3541-4ce9-be66-7e2cf217010d"},"cell_type":"code","outputs":[],"source":"# sess.run(tf.global_variables_initializer())\n# loss.eval(feed_dict={image: image_validation,\n#                          angle: np.transpose([angles_validation]),\n#                          y_true: labels_validation, keep_prob: 1.0})"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"ecf00f144bbb9de1643b4c0bb083e32bb54f1ade","_cell_guid":"c454b3f6-0eff-4bf6-b570-05fb943965a8"},"cell_type":"code","outputs":[],"source":"# learning rate of optimizer\nlearning_rate = (1e-3)*0.30\n# train step\ntrain_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"},{"metadata":{"_uuid":"929dd90ea76f7ab14457483f6e85caec6251fd2a","_cell_guid":"f2cf65c5-62f8-4cbf-b93f-130e883fc3bf"},"cell_type":"markdown","source":"# **7. Train Model**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"54c303483cdc7e442e07e642ccb5e14bc58f78b9","_cell_guid":"9a0d2e0d-6018-4d59-9b02-f8f0407e176d"},"cell_type":"code","outputs":[],"source":"# lists to store the train loss, validation loss, validation accuracy at each iteration\ntrain_loss = []\nvalid_loss = []\nvalid_acc = []\n\n# batch size\nbatch_size = 255\n# max iteration\nmax_iter = 700"},{"metadata":{"_uuid":"8b66b331e1e01188131ae697a9a093818f022faf","_cell_guid":"1b162bee-89fb-47a3-a293-e366fd2ce5c9"},"cell_type":"markdown","source":"**Here we train and save the model with the highest accuracy or lowest loss. But here I think it is wise to save the model with lowest loss**"},{"execution_count":null,"metadata":{"scrolled":false,"collapsed":true,"_uuid":"4209c04412d1637d8193fb7e54c5963cfc0d8391","_cell_guid":"a7c419a1-ed67-4773-86d3-5978a5699326"},"cell_type":"code","outputs":[],"source":"# create a saver object\nsaver = tf.train.Saver(max_to_keep=1)\n\n# variables to store the accuracy, loss, iteration of our best model\nbest_accuracy = 0\nbest_loss = 1000000\nbest_iteration = None\n\niteration = 0\n\n# create a graph session and optimize under it\nwith tf.Session() as sess:\n    \n    # initialize variables\n    sess.run(tf.global_variables_initializer())\n\n    # while 57 minutes have not elapsed (to finish before the kernel is killed)\n    while (time.time()-t1) < 3420:\n        \n        # break if max iteration is reached\n        if iteration >= max_iter:\n            break\n\n        # randomly choosing the indices of the batch \n        rand_index = np.random.choice(labels_train.shape[0], size=batch_size)\n\n        # extract the batch image and labels\n        image_rand = image_train[rand_index]\n#         angles_rand = angles_train[rand_index]\n        labels_rand = labels_train[rand_index]\n\n        # feed dictionary for batch\n        feed_dict_batch =  {image: image_rand,\n#                             angle: np.transpose([angles_rand]),\n                            y_true: labels_rand,\n                            keep_prob: 0.7}\n        # feed dictionary for train\n        feed_dict_train =  {image: image_rand,\n#                             angle: np.transpose([angles_rand]),\n                            y_true: labels_rand,\n                            keep_prob: 1.0}\n        # feed dictionary for validation\n        feed_dict_validation =  {image: image_validation,\n#                                  angle: np.transpose([angles_validation]),\n                                 y_true: labels_validation,\n                                 keep_prob: 1.0}\n        \n        # execute optimization step\n        sess.run(train_step, feed_dict=feed_dict_batch)\n\n        # calculate temporary train loss and append it to the designated list\n        temp_train_loss = loss.eval(session=sess, feed_dict=feed_dict_train)\n        train_loss.append(temp_train_loss)\n        # calculate temporary validation loss and append it to the designated list\n        temp_validation_loss = loss.eval(session=sess, feed_dict=feed_dict_validation)\n        valid_loss.append(temp_validation_loss)\n        # calculate temporary validation accuracy and append it to the designated list\n        temp_validation_accuracy = accuracy.eval(session=sess, feed_dict=feed_dict_validation)\n        valid_acc.append(temp_validation_accuracy)\n\n        # if the valid loss is tied with best recorded so far but valid acc is better then\n        # update the parameters of the best model and save the model\n        if (temp_validation_loss == best_loss) and (temp_validation_accuracy > best_accuracy):\n            best_accuracy = temp_validation_accuracy\n            best_loss = temp_validation_loss\n            best_iteration = iteration           \n            saver.save(sess, './my-model', global_step = best_iteration)\n        \n        # if valid accuracy is better than best recorded so far then update the best valid accuracy\n        if temp_validation_accuracy > best_accuracy:\n            best_accuracy = temp_validation_accuracy\n        \n        # if valid loss is better than best recorded so far then\n        # update the parameters of the best model and save the model\n        if temp_validation_loss < best_loss:\n            best_loss = temp_validation_loss\n            best_iteration = iteration          \n            saver.save(sess, './my-model', global_step = best_iteration)\n\n        # print metric info\n        print(\"iterations:\",iteration,\n              \"| train_loss:\", temp_train_loss,\n              \"| validation_loss:\", temp_validation_loss,\n              \"| valid_accuracy:\", temp_validation_accuracy)\n        \n        # increment iteration\n        iteration = iteration+1"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"0aac341d8057e34adb2ec031e33f83f4d3c56b18","_cell_guid":"2f5723d5-e20e-4d5f-9533-daee1bfd1423"},"cell_type":"code","outputs":[],"source":"# delete unnecessary variables out of memory\ndel(image_train, image_validation, angles_train, angles_validation, labels_train, labels_validation)"},{"metadata":{"_uuid":"2083e8d574740ef1f4cb66fcad2da3d9da8507fd","_cell_guid":"e3d6fd2a-a371-4164-860f-7694a2e0cd26"},"cell_type":"markdown","source":"# **8. Save the submission and performance metrics of our best model**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"88e061beeec475745458a7f42e3e45244f69ef4b","_cell_guid":"977cc468-abd1-4cc4-b451-595f1db8fcbb"},"cell_type":"code","outputs":[],"source":"# t5 = time.time()\n\nwith tf.Session() as sess:    \n    \n    # restore the best model\n    model_path = \"./\"+\"my-model-\"+str(best_iteration)\n    saver.restore(sess, model_path)\n    \n    # break the test set into k folds other wise kernel will be out of memory\n    n = len(iD)\n    k = 12\n    step = n//k\n    \n    # array to store the prediction\n    preds = np.array([])\n\n    # iterate through each fold\n    for i in range(k):\n\n        # start and end indices of the fold\n        start = (step*i)\n        end = (step*(i+1)) \n    \n        # feed dictionary for the fold\n        feed_dict_test =  {image: image_test[start:end],\n#                            angle: np.transpose([angles_test[start:end]]),\n                           keep_prob: 1.0}\n\n        # evaluate predictions of the fold\n        fold_preds = y_pred.eval(session=sess, feed_dict = feed_dict_test)[:,1]\n        # append the predictions of the fold to the designated array\n        preds = np.append(preds, fold_preds)\n    \n    # save the submission csv file\n    submission_path = \"./submission.csv\"\n    submission = pd.DataFrame({\"id\": iD, \"is_iceberg\": preds})\n    submission.to_csv(submission_path, header = True, index=False)\n    \n    # save the csv file containing performance metrics of the best model \n    results = pd.DataFrame([int(best_iteration),train_loss[best_iteration],\n                            valid_loss[best_iteration], valid_acc[best_iteration]],\n                           index=[\"iteration\", \"train loss\", \"valid loss\", \"accuracy\"],\n                           columns = [\"results\"])    \n    results_path = \"./results.csv\"    \n    results.to_csv(results_path, header = True, index=True)\n    \n# t6 = time.time()\n# print(\"time take for prediction: \", t6-t5)"},{"metadata":{"_uuid":"c18d3725b7448a2304d57c3374f8de627862c430","_cell_guid":"593ceda5-7e05-4cf0-abc8-154feb035219"},"cell_type":"markdown","source":"# **9. Visualization of the performance**"},{"metadata":{"_uuid":"81039db2d254d7022c9d0819e3dfcdbca097baac","_cell_guid":"589a0642-75a9-421c-9d57-f286f78c7f2f"},"cell_type":"markdown","source":"## **9.1 Plot of loss over iteration**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"904340b5b401d6fab9c7c5fbfd54fc6f47f28b0a","_cell_guid":"aba71e61-9348-49f9-ac09-0b0cd317678a"},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(16, 8), dpi= 80, facecolor='w', edgecolor='k')\niterations = list(range(1,iteration+1))\nplt.plot(iterations, train_loss, label = \"train loss\")\nplt.plot(iterations, valid_loss, label = \"valid loss\")\nplt.title(\"Loss\")\nplt.xlabel(\"iter\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.grid()\nplt.show()"},{"metadata":{"_uuid":"15a5202b6b8e18d5baa81d6e2561ff043c6e943b","_cell_guid":"ca868335-904c-42a1-b279-e1c91d6e6dc6"},"cell_type":"markdown","source":"## **9.2 Plot of training accuracy over iteration**"},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"98b6f21e7a9b8da25783bb23bf707faf898ea27d","_cell_guid":"4cbf848e-a13c-4a6a-9989-b79c6eeffefd"},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(16, 8), dpi= 80, facecolor='w', edgecolor='k')\nplt.plot(iterations, valid_acc, label = \"train loss\")\nplt.title(\"Accuracy\")\nplt.xlabel(\"iter\")\nplt.ylabel(\"accuracy\")\nplt.grid()\nplt.show()"},{"metadata":{"_uuid":"535cb132d3732b315b2dfdd71b57d933da0c7d01","_cell_guid":"ec1c6478-0ddb-4834-adf4-d92bf10a1681"},"cell_type":"markdown","source":"# **10. Advice**"},{"metadata":{"_uuid":"856a105767f9f19f7fd93237084a717dbcde2ac9","_cell_guid":"77776bbd-985f-4904-b6ab-c3f4d3fcdf54"},"cell_type":"markdown","source":"Looking at the plot it is safe to say that the loss would have gone lower if we increase the number of iterations. I would advise to train the model using higher computational power to decrease iteration time and increase kernel time, an easy way to do that is by running the script on google cloud (such as dataflow) or amazon web service instances having high memory. Another advice I would give if you take the first advise is to increase the batch size to 500 to stabalize the optimization."}],"nbformat_minor":1,"nbformat":4}