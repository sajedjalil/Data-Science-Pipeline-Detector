{"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","metadata":{"_uuid":"9341f2516086ca38bca96e06a9dbfc39f813a95a","collapsed":true,"_cell_guid":"7922e149-72fa-48cd-a545-b70bddb40d28"},"outputs":[],"execution_count":null},{"source":"####TL;DR\n**Runs on GPU** There is some compatiblity issue with CPUs\n1. Hyperparameters in Deep learning are many, tuning them will take weeks or months. Generally researchers do this tuning and publish paper when they find a nice set of architecture which performs better than other.\n\n2. Since the model is pre-trained, it converges very fast and you but still you need GPU to use this. Due to some library issues, it doesn't work on CPU.\n\n2. For our purpose, we can use those architectures, which are made available by those researchers to us.\n\n3. Using those pretrained nets, layers of which already 'knows' how to extract features, we can don't have to tune the hyperparameters. Since they are already trained of some dataset(say imagenet), their pre-trained weights provide a good initialization of weights and because of this, our Convnet converges very fast which otherwise can take days on these deep architectures. That's the idea behind **Transfer Learning**.  Examples of which are VGG16, InceptionNet, goolenet, Resnet etc.\n\n4. In this kernel we will use pretrained VGG-16 network which performs very well on small size images.\n\n5.** VGG architecture has proved to worked well on small sized images(CIFAR-10)  ** I expected it to work well for this dataset as well.\n\n6. The code also includes the data augmentation steps, thus considerably improving the performance.\n\n- **GPU is needed**\n\nHere is the link of the research paper if you are interested.\n[https://arxiv.org/pdf/1409.1556.pdf](http://)\n\nAlso here is the doc for keras library:\n[https://keras.io/applications/#vgg16](http://)\n\n\n","cell_type":"markdown","metadata":{"_uuid":"41648b358ed7c6c682ad50b85a347595733e2214","_cell_guid":"f06173b0-d354-4693-904e-a87f840ec372"}},{"source":"#Mandatory imports\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom os.path import join as opj\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport pylab\nplt.rcParams['figure.figsize'] = 10, 10\n%matplotlib inline","cell_type":"code","metadata":{"_uuid":"c73fefe3108dfd1f26cfda367a8df7283ac4c586","collapsed":true,"_cell_guid":"42618ab1-ea7d-4a8b-98ae-584774306108"},"outputs":[],"execution_count":null},{"source":"train = pd.read_json(\"../input/train.json\")\ntarget_train=train['is_iceberg']\ntest = pd.read_json(\"../input/test.json\")\n","cell_type":"code","metadata":{"_uuid":"d375d4f754ad7fb77db2142c7c075b4ad4168390","collapsed":true,"_cell_guid":"9d5839b6-f7fb-426d-b05f-cf2dba9313a8"},"outputs":[],"execution_count":null},{"source":"\n\nKeras provide the implementation of pretrained VGG, it in it's library so we don't have to build the net by ourselves.\nHere we are removing the last layer of VGG and putting our sigmoid layer for binary predictions.\n\nThe following code will NOT WORK, since on kaggle notebook, the weights of model cannot be downloaded, however, you can copy paste the code in your own notebook to make it work.","cell_type":"markdown","metadata":{"_uuid":"88e01459499ec6e3b007b800c9bae0623f3dce7f","_cell_guid":"3cfe5ed2-f8f7-462b-92a9-b2a3a5b40978"}},{"source":"\ntarget_train=train['is_iceberg']\ntest['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\ntrain['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\ntrain['inc_angle']=train['inc_angle'].fillna(method='pad')\nX_angle=train['inc_angle']\ntest['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\nX_test_angle=test['inc_angle']\n\n#Generate the training data\nX_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\nX_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\nX_band_3=(X_band_1+X_band_2)/2\n#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\nX_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n                          , X_band_2[:, :, :, np.newaxis]\n                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n\n\n\nX_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\nX_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\nX_band_test_3=(X_band_test_1+X_band_test_2)/2\n#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\nX_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n                          , X_band_test_2[:, :, :, np.newaxis]\n                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)\n\n#Import Keras.\nfrom matplotlib import pyplot\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\nfrom keras import initializers\nfrom keras.optimizers import Adam\nfrom keras.optimizers import rmsprop\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nfrom keras.optimizers import SGD\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n\nfrom keras.datasets import cifar10\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.xception import Xception\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.vgg19 import VGG19\nfrom keras.layers import Concatenate, Dense, LSTM, Input, concatenate\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\t\n\n#Data Aug for multi-input\nfrom keras.preprocessing.image import ImageDataGenerator\nbatch_size=64\n# Define the image transformations here\ngen = ImageDataGenerator(horizontal_flip = True,\n                         vertical_flip = True,\n                         width_shift_range = 0.,\n                         height_shift_range = 0.,\n                         channel_shift_range=0,\n                         zoom_range = 0.2,\n                         rotation_range = 10)\n\n# Here is the function that merges our two generators\n# We use the exact same generator with the same random seed for both the y and angle arrays\ndef gen_flow_for_two_inputs(X1, X2, y):\n    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n    while True:\n            X1i = genX1.next()\n            X2i = genX2.next()\n            #Assert arrays are equal - this was for peace of mind, but slows down training\n            #np.testing.assert_array_equal(X1i[0],X2i[0])\n            yield [X1i[0], X2i[1]], X1i[1]\n\n# Finally create generator\ndef get_callbacks(filepath, patience=2):\n   es = EarlyStopping('val_loss', patience=10, mode=\"min\")\n   msave = ModelCheckpoint(filepath, save_best_only=True)\n   return [es, msave]\n\n\ndef getVggAngleModel():\n    input_2 = Input(shape=[1], name=\"angle\")\n    angle_layer = Dense(1, )(input_2)\n    base_model = VGG16(weights='imagenet', include_top=False, \n                 input_shape=X_train.shape[1:], classes=1)\n    x = base_model.get_layer('block5_pool').output\n    \n\n    x = GlobalMaxPooling2D()(x)\n    merge_one = concatenate([x, angle_layer])\n    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n    merge_one = Dropout(0.3)(merge_one)\n    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n    merge_one = Dropout(0.3)(merge_one)\n    \n    predictions = Dense(1, activation='sigmoid')(merge_one)\n    \n    model = Model(input=[base_model.input, input_2], output=predictions)\n    \n    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=sgd,\n                  metrics=['accuracy'])\n    return model\n\n\n#Using K-fold Cross Validation with Data Augmentation.\ndef myAngleCV(X_train, X_angle, X_test):\n    K=3\n    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, target_train))\n    y_test_pred_log = 0\n    y_train_pred_log=0\n    y_valid_pred_log = 0.0*target_train\n    for j, (train_idx, test_idx) in enumerate(folds):\n        print('\\n===================FOLD=',j)\n        X_train_cv = X_train[train_idx]\n        y_train_cv = target_train[train_idx]\n        X_holdout = X_train[test_idx]\n        Y_holdout= target_train[test_idx]\n        \n        #Angle\n        X_angle_cv=X_angle[train_idx]\n        X_angle_hold=X_angle[test_idx]\n\n        #define file path and get callbacks\n        file_path = \"%s_aug_model_weights.hdf5\"%j\n        callbacks = get_callbacks(filepath=file_path, patience=5)\n        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n        galaxyModel= getVggAngleModel()\n        galaxyModel.fit_generator(\n                gen_flow,\n                steps_per_epoch=24,\n                epochs=100,\n                shuffle=True,\n                verbose=1,\n                validation_data=([X_holdout,X_angle_hold], Y_holdout),\n                callbacks=callbacks)\n\n        #Getting the Best Model\n        galaxyModel.load_weights(filepath=file_path)\n        #Getting Training Score\n        score = galaxyModel.evaluate([X_train_cv,X_angle_cv], y_train_cv, verbose=0)\n        print('Train loss:', score[0])\n        print('Train accuracy:', score[1])\n        #Getting Test Score\n        score = galaxyModel.evaluate([X_holdout,X_angle_hold], Y_holdout, verbose=0)\n        print('Test loss:', score[0])\n        print('Test accuracy:', score[1])\n\n        #Getting validation Score.\n        pred_valid=galaxyModel.predict([X_holdout,X_angle_hold])\n        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n\n        #Getting Test Scores\n        temp_test=galaxyModel.predict([X_test, X_test_angle])\n        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n\n        #Getting Train Scores\n        temp_train=galaxyModel.predict([X_train, X_angle])\n        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n\n    y_test_pred_log=y_test_pred_log/K\n    y_train_pred_log=y_train_pred_log/K\n\n    print('\\n Train Log Loss Validation= ',log_loss(target_train, y_train_pred_log))\n    print(' Test Log Loss Validation= ',log_loss(target_train, y_valid_pred_log))\n    return y_test_pred_log\n\n\n","cell_type":"code","metadata":{"_uuid":"af8be6ce23dba815bbde23fd7e196eb54ae7c4e1","collapsed":true,"_cell_guid":"067f3dd7-3dcf-4b71-857d-e00b4afbd06e"},"outputs":[],"execution_count":null},{"source":"preds=myAngleCV(X_train, X_angle, X_test)","cell_type":"code","metadata":{"_uuid":"d462c689ee61d4c1cdcee42c7ded6c7c31c9cddc","collapsed":true,"_cell_guid":"ea82458f-f41c-4abb-87aa-0dfc7a447969"},"outputs":[],"execution_count":null},{"source":"#Submission for each day.\nsubmission = pd.DataFrame()\nsubmission['id']=test['id']\nsubmission['is_iceberg']=preds\nsubmission.to_csv('sub.csv', index=False)","cell_type":"code","metadata":{"_uuid":"2e7f1db4b36211939fb9650e3b721ac8db09dda2","collapsed":true,"_cell_guid":"012fc91e-17ff-4163-a32d-79007feba4fc"},"outputs":[],"execution_count":null},{"source":"","cell_type":"code","metadata":{"_uuid":"1ec34a0eac4921cb2d6bf5f075367ec6f88005e1","collapsed":true,"_cell_guid":"f2c96b3a-e901-4687-ac8d-7b63ab760bc1"},"outputs":[],"execution_count":null}],"metadata":{"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"nbformat":4}