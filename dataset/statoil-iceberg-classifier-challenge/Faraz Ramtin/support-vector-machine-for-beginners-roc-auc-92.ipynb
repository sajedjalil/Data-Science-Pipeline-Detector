{"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.4","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"# Introduction:\nOne of the common classifier used for image-processing is Support Vector Machine (SVM). The goal of this kernel is to provide a basic guideline for beginners applying this strong tool on C-CORE Iceberg Classifier Challenge. We used [**sklearn SVM class**](http://scikit-learn.org/stable/modules/svm.html) for Python. This is a collaborative work with [Mehdi](https://www.kaggle.com/mnoori).\n\nThe codes are organized as follows:\n* Data Preparation\n* Fitting and Tuning the parameters\n* Conclusion and Visualizations","metadata":{"_uuid":"8c39f6b418a847e3a82b873c64d6024603d5815b","_cell_guid":"18752ef6-ed2c-406d-af2e-eaaba3cec0d6"}},{"cell_type":"markdown","source":"# 1- Data Preparation:","metadata":{"_uuid":"79da62d8b18f540b09dc4e38fde2b487a6433b58","_cell_guid":"6942c77b-4286-4c5b-a0a9-70ee723ce878"}},{"cell_type":"code","source":"# Importing packages:\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.metrics import accuracy_score,f1_score,log_loss,roc_auc_score\n\nfrom sklearn.model_selection import train_test_split,ShuffleSplit,cross_val_score,GridSearchCV\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\n\nfrom os.path import join as opj\nfrom matplotlib import pyplot as plt\n\nfrom matplotlib.colors import Normalize\n\n# Reading the traning data set json file to a pandas dataframe\ntrain=pd.read_json('../input/train.json')\n\n# Lets take a look at the first 5 rows of the dataset\ntrain.head(5)","execution_count":null,"metadata":{"_uuid":"b44bad2b9b5e6bcde1621429834400472ac3ac76","_cell_guid":"60ce3fcc-c1d1-4177-a66e-7584681e08e1","collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"The data frame has 3 main features: \n* band_1: flatten 75*75 horizontal radar frequency information. Please refer to [Problem Background](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge#Background).\n* band_1: flatten 75*75 Vertical radar frequency information. Please refer to [Problem Background](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge#Background).\n* inc_angle: incidence angle.\nand there is a respond binary vector called 'is_iceberg'.\n\nFirst of all for this kernel we are going to get rid of 'na's in inc_angle column. Please note that 'na' in this column is in string format which cannot be detected using [numpy.isnan](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isnan.html). So we gonna replace the 'na's with numpy.nan first, then we gonna drop those entries. We also can impute the the angle values by fiding similar enteries using other available features of the data (e.g. band_1 and band_2 data). You can refer to our work which provides a beginner guide to use [Fancy Impute Package](https://www.kaggle.com/mnoori/fancy-imputing-the-missing-inc-angles-beginner) for this feature.","metadata":{"_uuid":"a439222769977a501cb6d152a0c37f23dc2b5b88","_cell_guid":"af0fea6c-f69d-4eb4-8472-00189dd3eea5"}},{"cell_type":"code","source":"# Replace the 'na's with numpy.nan\ntrain.inc_angle.replace({'na':np.nan}, inplace=True)\n\n# Drop the rows that has NaN value for inc_angle\ntrain.drop(train[train['inc_angle'].isnull()].index,inplace=True)","execution_count":null,"metadata":{"_uuid":"5cf07efd403a94c9b15e94536ae97be60f30fdaf","_cell_guid":"1c54c3a3-9bdf-44e5-aeeb-c1a1825a93e4","collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"SVM only gets the numerical value as a numpy matrix, so we create a numpy matrix which includes all features. However, as shown in the code, we created different matrices for each feature which enables us to build different combinations of input variables for the model . For a large dataset we can also pass the sparse numpy matrix to the model [Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).","metadata":{"_uuid":"1269a306f61264d42840a5aaa9bc2f6b024b3924","_cell_guid":"6d53a626-1d2d-4b7f-8042-04a7cb679ac8"}},{"cell_type":"code","source":"X_HH_train=np.array([np.array(band).astype(np.float32) for band in train.band_1])\nX_HV_train=np.array([np.array(band).astype(np.float32) for band in train.band_2])\nX_angle_train=np.array([[np.array(angle).astype(np.float32) for angle in train.inc_angle]]).T\ny_train=train.is_iceberg.values.astype(np.float32)\nX_train=np.concatenate((X_HH_train,X_HV_train,X_angle_train), axis=1)\n# Now, we have 75*75 numerical features for band_1, 75*75 numerical features for band_2, and 1  feature for angle \nX_train.shape","execution_count":null,"metadata":{"_uuid":"dadd39ad047c6101c12ffdf0387ec1b3164a6d81","_cell_guid":"c1606960-64c8-4f34-b468-5f026b0db7c0","collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"SVM algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1.\n\nFor this problem, we will face some difficulties to scale the variance of each feature as the variance of the features are very small and close to zero. So we use [MaxAbsScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler) to scale each variable in the range of [-1,+1] which is centered by 0. This scaler is also able to keep the scaling scheme on the train set which later can be applied on test set. ","metadata":{"_uuid":"ab36d90daa041b5beb0c59b252ab55bd7ad01827","_cell_guid":"9116c77e-23ab-4d62-865f-392378b521c1"}},{"cell_type":"code","source":"scaler = MaxAbsScaler()\nX_train_maxabs = scaler.fit_transform(X_train)","execution_count":null,"metadata":{"_uuid":"8ed8fb183510b9deff9104a1541a945d96bd6840","_cell_guid":"553abd65-dc8a-4792-802f-391ed947adca","collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"# 2- Fitting and Tuning the parameters\nWe used Radial Basis Function (rbf) as the SVM kernel. \n\nWe also set the **probability** method to **False** in order to get the faster result. Please note that SVM cannot directly calculate the probability of each class and it needs to fit an additional cross-validation on the training data to get the probabilities calibrated using Platt scaling: logistic regression on the SVM’s scores. For the final submission which you need probability of each class, the **probability** method should be set to **True**.\n\nThe gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors. The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.\n ","metadata":{"_uuid":"9c713de8b73d2f83409e83e78959d58c72af4df3","_cell_guid":"3c5c52ca-5a97-42cb-b4af-e558396bd6ef"}},{"cell_type":"code","source":"# Create the SVM instance using Radial Basis Function (rbf) kernel\nclf = svm.SVC(kernel='rbf',probability=False)\n# Set the range of hyper-parameter we wanna use to tune our SVM classifier\nC_range = [0.1,1,10,50,100]\ngamma_range = [0.00001,0.0001,0.001,0.01,0.1]\nparam_grid_SVM = dict(gamma=gamma_range, C=C_range)\n# set the gridsearch using 3-fold cross validation and 'ROC Area Under the Curve' as the cross validation score. \ngrid = GridSearchCV(clf, param_grid=param_grid_SVM, cv=3,scoring='roc_auc')\ngrid.fit(X_train_maxabs, y_train)\nprint(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))","execution_count":null,"metadata":{"_uuid":"27b56a5c0a3a97926babecf55a0c136750aa471b","_cell_guid":"fc3d9b32-1ba1-4e1e-9d24-3a58784c4091","collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"# 3- Conclusion and Visualizations\nYou can see the tradeoffs among different value of C and gamma, and how it can effect the validation score of the model.\n[visualization credit](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py)","metadata":{"_uuid":"466b728a7d5a9fa5b041361808449ed4d3c1b213","_cell_guid":"e89fd405-d866-43fc-a583-d473fac4856b"}},{"cell_type":"code","source":"class MidpointNormalize(Normalize):\n\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n\n    \nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nscores = grid.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,norm=MidpointNormalize(vmin=0.5, midpoint=0.95))\nplt.xlabel('gamma')\nplt.ylabel('C')\nplt.colorbar()\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title('Validation ROC_AUC score')\nplt.show()","execution_count":null,"metadata":{"_uuid":"06f7fc27cbd848300cdea5a8e1d674f6fff88ea8","_cell_guid":"650434d1-b264-4a3b-a7c9-05810eb8bfe6","collapsed":true},"outputs":[]}]}