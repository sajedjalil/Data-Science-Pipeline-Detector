{"nbformat_minor":1,"cells":[{"cell_type":"code","metadata":{},"execution_count":null,"source":"# Required libraries\n# We will try several Machine Learning platforms\nfrom __future__ import print_function\nfrom builtins import str\nfrom builtins import range\n\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom io import BytesIO\n\nimport bson\nimport json \nimport skimage\n\nimport matplotlib.pyplot as plt\nimport keras\nimport tensorflow as tf\n\nfrom sklearn import *\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport datetime as dt\n\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import log_loss\n\n# Config the matplotlib backend as plotting inline in IPython\n%matplotlib inline\n\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nprint(\"tf.__version__ : \", tf.__version__)\nprint(\"python --version : \", sys.version)\nPyVersion = sys.version","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## 1- Data loading"},{"cell_type":"code","metadata":{"_uuid":"07a5a5782894611e9006ae1b399b0b8fb8a0f06b","_cell_guid":"52b50086-b405-4598-b11c-97887cdcce8e"},"execution_count":null,"source":"# Read data\ntrain = pd.read_json(\"../input/train.json\")\n#test = pd.read_json(\"test.json\")\ntrain.inc_angle = train.inc_angle.replace('na', 0)\ntrain.inc_angle = train.inc_angle.astype(float).fillna(0.0)\nprint(\"Total number of images :\", len(train))\ntrain.head(0)\nprint(\"done!\")\ntrain[:7]","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"# Test data\ntest = pd.read_json('../input/test.json')\ntest['inc_angle'] = pd.to_numeric(test['inc_angle'],errors='coerce')\nprint(\"Total number of images :\", len(test))\ntest.head(0)\ntest[:7]","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## 2- Data Engineering"},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# Train data\ndef get_stats(train,label=1):\n    train['max'+str(label)] = [np.max(np.array(x)) for x in train['band_'+str(label)] ]\n    train['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in train['band_'+str(label)] ]\n    train['min'+str(label)] = [np.min(np.array(x)) for x in train['band_'+str(label)] ]\n    train['minpos'+str(label)] = [np.argmin(np.array(x)) for x in train['band_'+str(label)] ]\n    train['med'+str(label)] = [np.median(np.array(x)) for x in train['band_'+str(label)] ]\n    train['std'+str(label)] = [np.std(np.array(x)) for x in train['band_'+str(label)] ]\n    train['mean'+str(label)] = [np.mean(np.array(x)) for x in train['band_'+str(label)] ]\n    train['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in train['band_'+str(label)] ]\n    train['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in train['band_'+str(label)] ]\n    train['mid50_'+str(label)] = train['p75_'+str(label)]-train['p25_'+str(label)]\n\n    return train\ntrain = get_stats(train,1)\ntrain = get_stats(train,2)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"train.head(2)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"f8f87e54cb74b57d5b8ae5b20c4087dec753d26c","_cell_guid":"96a221cb-9e07-4942-a7d0-846527fcec50","collapsed":true},"execution_count":null,"source":"col1 = ['min1','max1','std1','med1','mean1','mid50_1']\ncol2 = ['min2','max2','std2','med2','mean2','mid50_2']\ncol = [c for c in train.columns if c not in ['id','is_iceberg', 'band_1', 'band_2']]\n#col = [c for c in train.columns if c not in ['id','is_iceberg', 'inc_angle', 'band_1', 'band_2']]","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"len(col)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## 3- Data splitting"},{"cell_type":"code","metadata":{},"execution_count":null,"source":"# We could try several augmentation methods on the data to see the effect\n# Standardize values to 0 mean and unit standard deviation\nmin_max_scaler = preprocessing.MinMaxScaler()\ntrain_minmax = min_max_scaler.fit_transform(train[col])\n\n# DATA SPLITING\nX_train, X_test, y_train, y_test = train_test_split(train[col], train['is_iceberg'], test_size=0.25, random_state=42)\n#X_train, X_test, y_train, y_test = train_test_split(train_minmax, train['is_iceberg'], test_size=0.25, random_state=42)\n\nX_train = X_train.values.astype(np.float32)\nX_test = X_test.values.astype(np.float32)\ny_train = y_train.values.astype(np.int)\ny_test = y_test.values.astype(np.int)\n#xtest = test[col].values.astype(np.float32)\n\n\nn_features = X_train.shape[1]\n\nn_classes = len(np.unique(y_train))\n\nprint(\"n_features : {}\\nn_classes : {}\\nX_train.shape : {}\".format(n_features, n_classes, X_train.shape))","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"X_train.shape","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"X_train[1]","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"y_train.shape","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true},"source":"## 4- First training  &  Hyperparameter Optimization"},{"cell_type":"code","metadata":{"scrolled":false},"execution_count":null,"source":"\nprint('Start training...')\n# train\ngbm = lgb.LGBMClassifier(objective='binary',\n                        num_leaves=31,\n                        learning_rate=0.05,\n                        n_estimators=20)\ngbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric='binary_logloss',\n        early_stopping_rounds=100)\n\nprint('Start predicting...')\n# predict\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n# eval\nprint('The log_loss of prediction is:', log_loss(y_test, y_pred))\n\n# feature importances\nprint('\\nNumber of features :', len(list(gbm.feature_importances_)))\nprint('Features :', col)\nprint('Importances :', list(gbm.feature_importances_))\nprint('\\nFeature importances :', dict(zip(col,list(gbm.feature_importances_))))\n\n\n### ### Hyperparameter Optimization ##############\n# other scikit-learn modules\nestimator = lgb.LGBMClassifier(num_leaves=31)\n\n# The parameters used are in comment below, it will take too long time to run them here\nparam_grid = {\n    'learning_rate': [0.1],\n    'n_estimators': [100, 500],\n    'num_leaves': [20, 31],\n    'min_data_in_leaf': [5, 10],\n    'reg_alpha': [0],\n    'reg_lambda': [1e-6], \n    'bagging_fraction': [0.8, 0.9],\n    'min_child_samples': [10, 20],\n    'min_child_weight': [1e-6], \n    'max_bin': [256]\n}\n\ngbm = GridSearchCV(estimator, param_grid)\n\ngbm.fit(X_train, y_train)\n\nprint('\\n\\nBest parameters found by grid search are:', gbm.best_params_)\n\n'''\nparam_grid = {\n    'learning_rate': [0.01, 0.1, 0.05, 0.07, 1],\n    'n_estimators': [20, 40, 100, 500],\n    'num_leaves': [20, 31, 50, 127],\n    'min_data_in_leaf': [5, 10, 20, 50, 100],\n    'reg_alpha': [0, 1e-3, 1e-6],\n    'reg_lambda': [0, 1e-3, 1e-6], \n    'bagging_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n    'min_child_samples': [10, 20, 30],\n    'min_child_weight': [5, 1e-3, 1e-6], \n    'max_bin': [255, 256]\n}\n'''\n#","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## 5- Fine Tuning  &  Evaluation"},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# FEATURES TUNING IF NECESSARY\n## For example, we could remove least important features if required\n## And also use the best parameters provided by the grid search Cross Validation","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# Here I reuse the same previous splits instead of recreate a new one.\nX_train = pd.DataFrame(X_train, columns=col)\nX_test = pd.DataFrame(X_test, columns=col)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"X_train.head(2)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"X_test.head(2)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"# I decided here to delete the features with lower importance (7 and 8 values) to see how that could improve the result\n# Using the Feature importances Dictionary\nnew_cols = [c for c in train.columns if c not in ['id','is_iceberg', 'band_1', 'band_2', 'p75_2', 'mean2', 'minpos2']]\nlen(new_cols)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"X_train_new = X_train[new_cols]\nX_test_new = X_test[new_cols]\nX_train_new.shape","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# Come back into arrays for training\nX_train_new = X_train_new.values.astype(np.float32)\nX_test_new = X_test_new.values.astype(np.float32)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# TRAINING","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# specify your configurations as a dict\n# Use the best parameters provided by the grid search Cross Validation\nparams = {\"objective\": \"binary\",\n          #\"sigmoid\":1.0,\n          \"task\": \"train\",\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": 0.1,\n          \"num_leaves\": 20, # 31\n          \"max_bin\": 256,\n          \"min_data_in_leaf\": 5, # Problem  2000\n          \"feature_fraction\": 0.6, # 0.6\n          \"verbosity\": 0,\n          \"seed\": 0,\n          \"drop_rate\": 0.1, # 0.1\n          \"is_unbalance\": False,\n          \"max_drop\": 50,\n          \"min_child_samples\": 10,\n          \"min_child_weight\": 1e-06, # 5\n          \"min_split_gain\": 0,\n          \"colsample_bytree\": 0.6343275033,\n          \"max_depth\": 8, # 8\n          \"n_estimators\": 500, # 500\n          \"nthread\": -1,\n          \"reg_alpha\": 0,\n          \"reg_lambda\": 1e-06,# 1\n          \"silent\": True,\n          \"subsample_for_bin\": 50000, # 50000\n          \"subsample_freq\": 1, # 1\n          #\"min_data\":1,\n          #\"min_data_in_bin\":1,\n          'metric': {'binary_logloss'},\n          'bagging_fraction': 0.8,\n          'bagging_freq': 5,\n          #'num_iterations':1000,\n          \"subsample\": 0.733\n          }\n","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"y_pred[:12]","outputs":[]},{"cell_type":"code","metadata":{"scrolled":false},"execution_count":null,"source":"# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train_new, y_train)\nlgb_eval = lgb.Dataset(X_test_new, y_test, reference=lgb_train)\n\n\nprint('Start training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=2000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=100)\n\nprint('Save model...')\n# save model to file\ngbm.save_model('model.txt')\n\nprint('Start predicting...')\n# predict\ny_pred = gbm.predict(X_test_new, num_iteration=gbm.best_iteration)\n# eval\n#print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\nprint('The log_loss of prediction is:', log_loss(y_test, y_pred))","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true},"source":"## 6- SUBMISSION FILE CREATION"},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# Lets ensure first that the test set is under the same preprocessing as the train set (to reproduce the trainer performance).\ntest = get_stats(test,1)\ntest = get_stats(test,2)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"# xtest = min_max_scaler.fit_transform(test[new_cols])\nxtest = test[new_cols]\npreds = gbm.predict(xtest, num_iteration=gbm.best_iteration)","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"preds","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': preds})\nsubmission.head(10)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"submission.to_csv(\"./LightGBM_CV_submission.csv\", index=False)","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":"from IPython.display import FileLink\n#%cd $LESSON_HOME_DIR\nFileLink('LightGBM_CV_submission.csv')","outputs":[]},{"cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"source":"","outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.3","name":"python","pygments_lexer":"ipython3"}},"nbformat":4}