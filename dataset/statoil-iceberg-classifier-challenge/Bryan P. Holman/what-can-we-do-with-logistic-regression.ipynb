{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{},"source":"### TODO List\n\n* [ ] Clustering analysis on test dataset to take advantage of those observations.\n* [ ] Impute missing incidence angle information.\n\nThis specific competition seems a perfect use case for CNNs/image detection. But right now I am working on my feature engineering capabilities and am improving my understanding of logistic regression, so I want to see what I can accomplish with logistic regression alone. Let's begin.\n\n## Data Exploration"},{"cell_type":"code","metadata":{"_cell_guid":"8c2e30b7-ed21-4162-9e62-59dcf3872e59","collapsed":true,"_uuid":"07b5314de9f06e813145bc53eb81420320fa8cef"},"outputs":[],"source":"# import some packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":1},{"cell_type":"code","metadata":{},"outputs":[],"source":"# let's load in the data and look around\ndf_train = pd.read_json('../input/train.json')\ndf_train.info()","execution_count":6},{"cell_type":"markdown","metadata":{},"source":"Looks like we have 1604 images where whether or not the image contains an iceberg is labeled. The only features are the HH and HV bands, the image id, incidence angle (note: has missing values ... that's why it shows up as an object instead of a numeric), and whether or not there is an iceberg.\n\nNow let's look at the test dataset as well."},{"cell_type":"code","metadata":{},"outputs":[],"source":"df_test = pd.read_json('../input/test.json')\ndf_test.info()","execution_count":7},{"cell_type":"markdown","metadata":{},"source":"Five times as many test images! With this ratio, it would no doubt be helpful to do some unsupervised clustering on the test dataset to improve our model. I'll add that to the to do list.\n\nNo lat/lon information, time of year, etc. that might have proved useful. All the features to engineer will have to come from those radar echos.\n\nThanks to [MuonNeutrino](https://www.kaggle.com/muonneutrino/exploration-transforming-images-in-python), an excellent starting point is investigating the statistical properties of the two separate bands."},{"cell_type":"code","metadata":{},"outputs":[],"source":"# coerce incidence angle to numeric\ndf_train['inc_angle'] = pd.to_numeric(df_train['inc_angle'], errors='coerce')\n\n# combine training and test set for feature engineering\ndf_full = pd.concat([df_train, df_test], axis=0, ignore_index=True)","execution_count":11},{"cell_type":"code","metadata":{},"outputs":[],"source":"def get_stats(df, label=1):\n    df['max'+str(label)] = [np.max(np.array(x)) for x in df['band_'+str(label)] ]\n    df['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in df['band_'+str(label)] ]\n    df['min'+str(label)] = [np.min(np.array(x)) for x in df['band_'+str(label)] ]\n    df['minpos'+str(label)] = [np.argmin(np.array(x)) for x in df['band_'+str(label)] ]\n    df['med'+str(label)] = [np.median(np.array(x)) for x in df['band_'+str(label)] ]\n    df['std'+str(label)] = [np.std(np.array(x)) for x in df['band_'+str(label)] ]\n    df['mean'+str(label)] = [np.mean(np.array(x)) for x in df['band_'+str(label)] ]\n    df['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in df['band_'+str(label)] ]\n    df['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in df['band_'+str(label)] ]\n    df['mid50_'+str(label)] = df['p75_'+str(label)]-df['p25_'+str(label)]\n\n    return df\n\ndf_full = get_stats(df_full, 1)\ndf_full = get_stats(df_full, 2)","execution_count":14},{"cell_type":"code","metadata":{"scrolled":false},"outputs":[],"source":"def plot_var(name, nbins=50):\n    minval = df_full[name].min()\n    maxval = df_full[name].max()\n    plt.hist(df_full.loc[df_full.is_iceberg==1,name],range=[minval,maxval],\n             bins=nbins,color='b',alpha=0.5,label='Boat')\n    plt.hist(df_full.loc[df_full.is_iceberg==0,name],range=[minval,maxval],\n             bins=nbins,color='r',alpha=0.5,label='Iceberg')\n    plt.legend()\n    plt.xlim([minval, maxval])\n    plt.xlabel(name)\n    plt.ylabel('Number')\n    plt.show()\n    \nfor col in ['inc_angle','min1','max1','std1','med1','mean1','mid50_1', 'p25_1', 'p75_1', 'minpos1', 'maxpos1']:\n    plot_var(col)","execution_count":19},{"cell_type":"markdown","metadata":{},"source":"Looks like the minimum, maximum, median, and mean reflectivities are useful features ... particularly the maximum reflectivity! This makes intuitive sense. The icebergs are scattering back more information to the radar. Also the first and third quartile features are useful.\n\nNow let's look at the second band."},{"cell_type":"code","metadata":{},"outputs":[],"source":"for col in ['min2','max2','std2','med2','mean2','mid50_2','p25_2', 'p75_2']:\n    plot_var(col)","execution_count":20},{"cell_type":"markdown","metadata":{},"source":"Interesting. The minimum reflectivity doesn't appear useful at all for band_2. However, the maximum reflectivity is! Additionally, we can almost guarantee that if the standard deviation of the values is over 3 then the image contains an iceberg.\n\nSo far, it seems that the features most usefull are:\n* min1, max1, std1, med1, mean1, p25_1, p75_1 (from band_1)\n* max2, std2 (from band_2)\n\nNow let's look at the correlation between these features:"},{"cell_type":"code","metadata":{},"outputs":[],"source":"df_full_stats = df_full.drop(['id','is_iceberg','band_1','band_2', 'inc_angle'], axis=1)\ncorr = df_full_stats.corr()\nfig = plt.figure(1, figsize=(10,10))\nplt.imshow(corr,cmap='inferno')\nlabels = np.arange(len(df_full_stats.columns))\nplt.xticks(labels,df_full_stats.columns,rotation=90)\nplt.yticks(labels,df_full_stats.columns)\nplt.title('Correlation Matrix of Global Variables')\ncbar = plt.colorbar(shrink=0.85,pad=0.02)\nplt.show()","execution_count":21},{"cell_type":"markdown","metadata":{},"source":"Unfortunately max1 and max2 are pretty highly correlated, so we might not get much additional information including both in there. The minimums aren't too correlated with the maximums in both bands, however, so each might be useful.\n\nGoing back to the value of the maximum reflectivity, let's see if the number of pixels above some threshold could be useful."},{"cell_type":"code","metadata":{},"outputs":[],"source":"np_test = np.array(df_full.loc[1, 'band_1'])","execution_count":43},{"cell_type":"code","metadata":{},"outputs":[],"source":"len(np_test[np_test > -10])","execution_count":45},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"def get_threshold_size(df, label=1, threshold=-15):\n    df['gt_'+str(threshold)+'_'+str(label)] = [len(np.array(x)[np.array(x) > threshold]) for x in df_full['band_'+str(label)]]\n    \n    return df","execution_count":59},{"cell_type":"code","metadata":{},"outputs":[],"source":"for lbl in range(1, 3):\n    for thr in range(-15, 35, 5):\n        get_threshold_size(df_full, label=lbl, threshold=thr)","execution_count":60},{"cell_type":"code","metadata":{"scrolled":false},"outputs":[],"source":"for col in ['gt_-15_1', 'gt_-10_1', 'gt_-5_1', 'gt_0_1', 'gt_5_1', 'gt_10_1', 'gt_15_1',\n            'gt_20_1', 'gt_25_1', 'gt_30_1']:\n    plot_var(col)","execution_count":61},{"cell_type":"markdown","metadata":{},"source":"Looks like the thresholds beyond 10 aren't useful."},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"df_full.drop(['gt_15_1', 'gt_20_1', 'gt_25_1', 'gt_30_1'], axis=1, inplace=True)","execution_count":63},{"cell_type":"code","metadata":{"scrolled":false},"outputs":[],"source":"for col in ['gt_-15_2', 'gt_-10_2', 'gt_-5_2', 'gt_0_2', 'gt_5_2', 'gt_10_2', 'gt_15_2',\n            'gt_20_2', 'gt_25_2', 'gt_30_2']:\n    plot_var(col)","execution_count":62},{"cell_type":"markdown","metadata":{},"source":"For band 2, the thresholds beyond -5 aren't useful."},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"df_full.drop(['gt_0_2', 'gt_5_2', 'gt_10_2', 'gt_15_2', 'gt_20_2', 'gt_25_2', 'gt_30_2'], \n             axis=1, inplace=True)","execution_count":64},{"cell_type":"markdown","metadata":{},"source":"At this point let's remind ourselves all of the columns we have. And drop features we have deemed not useful."},{"cell_type":"code","metadata":{},"outputs":[],"source":"df_full.info()","execution_count":66},{"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"df_full.drop(['inc_angle', 'maxpos1', 'minpos1', 'maxpos2', 'minpos2'], axis=1, inplace=True)","execution_count":67},{"cell_type":"code","metadata":{},"outputs":[],"source":"df_full.info()","execution_count":68},{"cell_type":"markdown","metadata":{},"source":"## Logistic Regression\n\nNow that we have a bunch of numeric features, let's perform a regression model."},{"cell_type":"code","metadata":{},"outputs":[],"source":"X = df_full.drop(['band_1', 'band_2', 'id', 'is_iceberg'], axis=1)[:1604]\ny = df_full['is_iceberg'][:1604]","execution_count":78},{"cell_type":"code","metadata":{},"outputs":[],"source":"# sklearn imports\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import classification_report\n\nskf = StratifiedKFold(y, n_folds=3)","execution_count":93},{"cell_type":"code","metadata":{},"outputs":[],"source":"for train_index, test_index in skf:\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    logmodel = LogisticRegression()\n    logmodel.fit(X_train, y_train)\n    predictions = logmodel.predict(X_test)\n    predictions_prob = logmodel.predict_proba(X_test)[:,1]\n    print(classification_report(y_test, predictions))\n    print(log_loss(y_test, predictions_prob))","execution_count":94},{"cell_type":"markdown","metadata":{},"source":"Looks like we should expect a loss near 0.4. Let's submit it and see what happens!"},{"cell_type":"code","metadata":{},"outputs":[],"source":"X_test = df_full.drop(['band_1', 'band_2', 'id', 'is_iceberg'], axis=1)[1604:]\nlogmodel = LogisticRegression()\nlogmodel.fit(X, y)\npredictions_prob = logmodel.predict_proba(X_test)[:,1]","execution_count":96},{"cell_type":"code","metadata":{},"outputs":[],"source":"df_predictions = pd.DataFrame({'id' : df_full['id'][1604:], 'is_iceberg' : predictions_prob})\ndf_predictions.to_csv('logistic_regression_submission.csv', index=False)","execution_count":102}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}