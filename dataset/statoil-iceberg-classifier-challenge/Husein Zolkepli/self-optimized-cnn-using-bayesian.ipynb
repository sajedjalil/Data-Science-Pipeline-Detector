{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py","version":"3.6.3","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"0d99a3cf087b8bf9c32b3cf40829ea14e4174cf6","_cell_guid":"6cf9684f-771d-464c-9cf5-30bf73ea8f64"},"execution_count":null,"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"df = pd.read_json('../input/train.json')\ndf.inc_angle = df.inc_angle.replace('na', 0)\ndf.inc_angle = df.inc_angle.astype(float).fillna(0.0)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1\"]])\nx_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2\"]])\nX_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n                          , x_band2[:, :, :, np.newaxis]\n                         , ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\nX_angle_train = np.array(df.inc_angle)\ny_train = np.array(df[\"is_iceberg\"])","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# just take 100 dataset to do optimization\n# we assume this 100 able to generalize the whole dataset\n# if not enough, increase the number\nX_train = X_train[:100]\ny_train = y_train[:100]\nX_angle_train = X_angle_train[:100].reshape((-1, 1))\ny_onehot = np.zeros((y_train.shape[0], np.unique(y_train).shape[0]))\nfor i in range(y_train.shape[0]):\n    y_onehot[i, y_train[i]] = 1.0\n    \nx_train, x_test, y_train, y_test, x_train_angle, x_test_angle = train_test_split(X_train, y_onehot, X_angle_train, test_size = 0.20)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"def neural_network(fully_conn_size, # wide size of fully connected layer\n                   len_layer_conv, # each conv layer included max pooling\n                   kernel_size, # kernel size for conv process\n                   learning_rate, # learning rate\n                   pooling_size, # kernel and stride size for pooling\n                   multiply, # constant to multiply for conv output\n                   dropout_rate, # dropout\n                   beta, # l2 norm discount\n                   activation,\n                   batch_normalization, \n                   batch_size = 20):\n    \n    tf.reset_default_graph()\n    if activation == 0:\n        activation = tf.nn.sigmoid\n    elif activation == 1:\n        activation = tf.nn.tanh\n    else:\n        activation = tf.nn.relu\n    \n    def conv_layer(x, conv, out_shape):\n        w = tf.Variable(tf.truncated_normal([conv, conv, int(x.shape[3]), out_shape]))\n        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n        return tf.nn.conv2d(x, w, [1, 1, 1, 1], padding = 'SAME') + b\n    \n    def fully_connected(x, out_shape):\n        w = tf.Variable(tf.truncated_normal([int(x.shape[1]), out_shape]))\n        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n        return tf.matmul(x, w) + b\n    \n    def pooling(x, k = 2, stride = 2):\n        return tf.nn.max_pool(x, ksize = [1, k, k, 1], \n                              strides = [1, stride, stride, 1], \n                              padding = 'SAME')\n    \n    X_img = tf.placeholder(tf.float32, (None, 75, 75, 3))\n    X_angle = tf.placeholder(tf.float32, (None, 1))\n    Y = tf.placeholder(tf.float32, (None, y_onehot.shape[1]))\n    # for batch normalization\n    train = tf.placeholder(tf.bool)\n    \n    for i in range(len_layer_conv):\n        if i == 0:\n            conv = activation(conv_layer(X_img, kernel_size, \n                                         int(np.around(int(X_img.shape[3]) * multiply))))\n        else:\n            conv = activation(conv_layer(conv, kernel_size, \n                                         int(np.around(int(conv.shape[3]) * multiply))))\n        conv = pooling(conv, k = pooling_size, stride = pooling_size)\n        if batch_normalization:\n            conv = tf.layers.batch_normalization(conv, training = train)\n        conv = tf.nn.dropout(conv, dropout_rate)\n    print(conv.shape)\n    output_shape = int(conv.shape[1]) * int(conv.shape[2]) * int(conv.shape[3])\n    conv = tf.reshape(conv, [-1, output_shape])\n    conv = tf.concat([conv, X_angle], axis = 1)\n    \n    # our fully connected got 1 layer\n    # you can increase it if you want\n    for i in range(1):\n        if i == 0:\n            fc = activation(fully_connected(conv, fully_conn_size))\n        else:\n            fc = activation(fully_connected(fc, fully_conn_size))\n        fc = tf.nn.dropout(fc, dropout_rate)\n        if batch_normalization:\n            fc = tf.layers.batch_normalization(fc, training = train)\n            \n    logits = fully_connected(fc, y_onehot.shape[1])\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, \n                                                                  logits = logits))\n    cost += sum(beta * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    \n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer())\n    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n    \n    for i in range(20):\n        train_acc, train_loss = 0, 0\n        for n in range(0, (X_train.shape[0] // batch_size) * batch_size, batch_size):\n            _, loss = sess.run([optimizer, cost], \n                               feed_dict = {X_img: x_train[n: n + batch_size, :, :, :],\n                                            X_angle: x_train_angle[n: n + batch_size],\n                                            Y: y_train[n: n + batch_size, :], \n                                            train: True})\n            train_acc += sess.run(accuracy, \n                                  feed_dict = {X_img: x_train[n: n + batch_size, :, :, :],\n                                               X_angle: x_train_angle[n: n + batch_size],\n                                               Y: y_train[n: n + batch_size, :], \n                                               train: False})\n            train_loss += loss\n        results = sess.run([cost, accuracy], \n                           feed_dict = {X_img: x_test,\n                                        X_angle: x_test_angle,\n                                        Y: y_test, \n                                        train: False})\n        TEST_COST.append(results[0])\n        TEST_ACC.append(results[1])\n        train_loss /= (x_train.shape[0] // batch_size)\n        train_acc /= (x_train.shape[0] // batch_size)\n        ACC.append(train_acc)\n        COST.append(train_loss)\n    COST = np.array(COST).mean()\n    TEST_COST = np.array(TEST_COST).mean()\n    ACC = np.array(ACC).mean()\n    TEST_ACC = np.array(TEST_ACC).mean()\n    return COST, TEST_COST, ACC, TEST_ACC","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"def generate_nn(fully_conn_size, len_layer_conv, kernel_size, learning_rate, pooling_size, multiply,\n                dropout_rate, beta, activation, batch_normalization):\n    global accbest\n    param = {\n        'fully_conn_size' : int(np.around(fully_conn_size)),\n        'len_layer_conv' : int(np.around(len_layer_conv)),\n        'kernel_size': int(np.around(kernel_size)),\n        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n        'pooling_size': int(np.around(pooling_size)),\n        'multiply': multiply,\n        'dropout_rate' : max(min(dropout_rate, 0.99), 0),\n        'beta' : max(min(beta, 0.5), 0.000001),\n        'activation': int(np.around(activation)),\n        'batch_normalization' : int(np.around(batch_normalization))\n    }\n    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n    print(\"stop after 20 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n    # a very simple benchmark, just use correct accuracy\n    # if you want to change to f1 score or anything else, can\n    if (valid_acc > accbest):\n        costbest = valid_acc\n    return valid_acc","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"accbest = 0.0\nNN_BAYESIAN = BayesianOptimization(generate_nn, \n                              {'fully_conn_size': (16, 128),\n                               'len_layer_conv': (3, 5),\n                               'kernel_size': (2, 7),\n                               'learning_rate': (0.0001, 1),\n                               'pooling_size': (2, 4),\n                               'multiply': (1, 3),\n                               'dropout_rate': (0.1, 0.99),\n                               'beta': (0.000001, 0.49),\n                               'activation': (0, 2),\n                               'batch_normalization': (0, 1)\n                              })\nNN_BAYESIAN.maximize(init_points = 10, n_iter = 10, acq = 'ei', xi = 0.0)","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"print('Maximum NN accuracy value: %f' % NN_BAYESIAN.res['max']['max_val'])\nprint('Best NN parameters: ', NN_BAYESIAN.res['max']['max_params'])","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is low because our cnn model is very complex. the purpose is, bayesian still able to find the best maxima in non convex hyper-parameters function without do any derivation"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"","outputs":[]}],"nbformat":4}