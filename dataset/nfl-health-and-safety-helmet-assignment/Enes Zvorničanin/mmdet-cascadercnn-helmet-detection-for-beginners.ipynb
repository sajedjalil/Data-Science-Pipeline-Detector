{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MMDetection CascadeRCNN \n\n#### **The purpse of this notebook is to explore method for object detection, concretly helmet detection, using `MMDetection` package and CascadeRCNN model.**\n\nWe'll use labeled data set provided for training.","metadata":{}},{"cell_type":"markdown","source":"# MMDdetection\n\nMMDetection is an open source object detection toolbox based on PyTorch.\n\n## Major features\n\n* **Modular Design**\n\nThe detection framework consist of different components and one can easily construct a customized object detection framework by combining different modules.\n\n* **Support of multiple frameworks out of box**\n\nThe toolbox directly supports popular and contemporary detection frameworks, e.g. Faster RCNN, Mask RCNN, RetinaNet, etc.\n\n* **High efficiency**\n\nAll basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases, including Detectron2, maskrcnn-benchmark and SimpleDet.\n\n* **State of the art**\n\nThe toolbox stems from the codebase developed by the MMDet team, who won COCO Detection Challenge in 2018, and we keep pushing it forward.\n","metadata":{}},{"cell_type":"markdown","source":"### References\n\n* MMDetection repository https://github.com/open-mmlab/mmdetection\n* SIIM MMDetection+CascadeRCNN+Weight&Bias https://www.kaggle.com/sreevishnudamodaran/siim-mmdetection-cascadercnn-weight-bias\n* NFL Helmet Assignment - Getting Started Guide https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n\n**Plese check my related notebook**\n* MMDetection player tracking for beginners https://www.kaggle.com/eneszvo/mmdetection-player-tracking-for-beginners\n* https://zhuanlan.zhihu.com/p/385702286","metadata":{}},{"cell_type":"markdown","source":"It is recommended to install MMDetection with MIM, which automatically handle the dependencies of OpenMMLab projects, including mmcv and other python packages.","metadata":{}},{"cell_type":"code","source":"!pip install openmim\n!mim install mmdet\n!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection\n!pip install -q -e .\n%cd ..","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-21T16:01:02.924914Z","iopub.execute_input":"2021-08-21T16:01:02.925287Z","iopub.status.idle":"2021-08-21T16:02:23.989889Z","shell.execute_reply.started":"2021-08-21T16:01:02.92521Z","shell.execute_reply":"2021-08-21T16:02:23.988865Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"./mmdetection\")\nimport os\n# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check mmcv installation\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nfrom mmcv import Config\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\n# Check MMDetection installation\nfrom mmdet.apis import set_random_seed\n\n# Imports\nimport mmdet\nfrom mmdet.apis import set_random_seed\nfrom mmdet.datasets import build_dataset, build_dataloader\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector, single_gpu_test\nfrom mmdet.apis import init_detector, inference_detector, show_result_pyplot\n\nimport mmcv\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\n\nimport random\nimport numpy as np\nfrom pathlib import Path\nimport datetime\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import Video, display\nimport subprocess\nimport gc\nimport shutil\n","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:23.991784Z","iopub.execute_input":"2021-08-21T16:02:23.99216Z","iopub.status.idle":"2021-08-21T16:02:37.654804Z","shell.execute_reply.started":"2021-08-21T16:02:23.992119Z","shell.execute_reply":"2021-08-21T16:02:37.65379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seed everything\n\nglobal_seed = 0\n\ndef set_seed(seed=global_seed):\n    \"\"\"Sets the random seeds.\"\"\"\n    set_random_seed(seed, deterministic=False)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:37.656749Z","iopub.execute_input":"2021-08-21T16:02:37.657051Z","iopub.status.idle":"2021-08-21T16:02:37.66892Z","shell.execute_reply.started":"2021-08-21T16:02:37.657025Z","shell.execute_reply":"2021-08-21T16:02:37.667985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseline models\n# cfg = Config.fromfile('/kaggle/working/mmdetection/configs/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py')\n# cfg = Config.fromfile(\"/kaggle/working/mmdetection/configs/vfnet/vfnet_r50_fpn_mstrain_2x_coco.py\")\n# cfg = Config.fromfile(\"/kaggle/working/mmdetection/configs/gfl/gfl_r50_fpn_mstrain_2x_coco.py\")\n# baseline_cfg_path = \"/kaggle/working/mmdetection/configs/cascade_rcnn/cascade_rcnn_r50_fpn_20e_coco.py\"\nbaseline_cfg_path = \"/kaggle/working/mmdetection/configs/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_1x_coco.py\"\ncfg = Config.fromfile(baseline_cfg_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:37.670901Z","iopub.execute_input":"2021-08-21T16:02:37.671429Z","iopub.status.idle":"2021-08-21T16:02:37.701423Z","shell.execute_reply.started":"2021-08-21T16:02:37.671278Z","shell.execute_reply":"2021-08-21T16:02:37.700652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_name = 'vfnet_r50_fpn'\n# model_name = 'cascade_rcnn_r50_fpn'\nmodel_name = 'cascade_rcnn_x101_32x4d_fpn_1x'\nfold = 0\njob = 4\n\n# Folder to store model logs and weight files\njob_folder = f'/kaggle/working/job{job}_{model_name}_fold{fold}'\ncfg.work_dir = job_folder\n\n# Set seed thus the results are more reproducible\ncfg.seed = global_seed\n\nif not os.path.exists(job_folder):\n    os.makedirs(job_folder)\n\nprint(\"Job folder:\", job_folder)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:37.70257Z","iopub.execute_input":"2021-08-21T16:02:37.70292Z","iopub.status.idle":"2021-08-21T16:02:37.708797Z","shell.execute_reply.started":"2021-08-21T16:02:37.702885Z","shell.execute_reply":"2021-08-21T16:02:37.707963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load image level csv file\nextra_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/image_labels.csv')\nprint('Number of ground truth bounding boxes: ', len(extra_df))\n\n# Number of unique labels\nlabel_to_id = {label: i for i, label in enumerate(extra_df.label.unique())}\nprint('Unique labels: ', label_to_id)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:37.710039Z","iopub.execute_input":"2021-08-21T16:02:37.710721Z","iopub.status.idle":"2021-08-21T16:02:38.01907Z","shell.execute_reply.started":"2021-08-21T16:02:37.710665Z","shell.execute_reply":"2021-08-21T16:02:38.01815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the number of classes\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 5\n\ncfg.gpu_ids = [0]\n\n# Setting pretrained model in the init_cfg which is required \n# for transfer learning as per the latest MMdetection update\ncfg.model.backbone.init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')\ncfg.model.backbone.init_cfg=dict(type='Pretrained', checkpoint='open-mmlab://resnext101_32x4d')\ncfg.model.pop('pretrained', None)\n\n# Epochs for the runner that runs the workflow \n# Consider increase number of epochs for better performance\ncfg.runner.max_epochs = 1 \ncfg.total_epochs = 1 \n\n# Learning rate of optimizers. \n# The LR is divided by 8 since the config file is originally for 8 GPUs\ncfg.optimizer.lr = 0.02/8\n\n# Learning rate scheduler config used to register LrUpdater hook\ncfg.lr_config = dict(\n    policy='CosineAnnealing', # The policy of scheduler, also support CosineAnnealing, Cyclic, etc. Refer to details of supported LrUpdater from https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py#L9.\n    by_epoch=False,\n    warmup='linear', # The warmup policy, also support `exp` and `constant`.\n    warmup_iters=500, # The number of iterations for warmup\n    warmup_ratio=0.001, # The ratio of the starting learning rate used for warmup\n    min_lr=1e-07)\n\n# config to register logger hook\ncfg.log_config.interval = 20 # Interval to print the log\n\n# Config to set the checkpoint hook, Refer to https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/checkpoint.py for implementation.\ncfg.checkpoint_config.interval = 1 # The save interval is 1","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:38.020352Z","iopub.execute_input":"2021-08-21T16:02:38.020881Z","iopub.status.idle":"2021-08-21T16:02:38.029267Z","shell.execute_reply.started":"2021-08-21T16:02:38.020839Z","shell.execute_reply":"2021-08-21T16:02:38.028432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create COCO format data set which is the standard format in object detection.","metadata":{}},{"cell_type":"code","source":"def create_ann_file(df, category_id):\n    \n    now = datetime.datetime.now()\n\n    data = dict(\n        info=dict(\n            description='NFL-Helmet-Assignment',\n            url=None,\n            version=None,\n            year=now.year,\n            contributor=None,\n            date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n        ),\n        licenses=[dict(\n            url=None,\n            id=0,\n            name=None,\n        )],\n        images=[\n            # license, url, file_name, height, width, date_captured, id\n        ],\n        type='instances',\n        annotations=[\n            # segmentation, area, iscrowd, image_id, bbox, category_id, id\n        ],\n        categories=[\n            # supercategory, id, name\n        ],\n    )\n    \n    class_name_to_id = {}\n    labels =  [\"__ignore__\",\n                'Helmet',\n              'Helmet-Blurred', \n              'Helmet-Difficult', \n              'Helmet-Sideline',\n              'Helmet-Partial']\n\n    for i, each_label in enumerate(labels):\n        class_id = i - 1  # starts with -1\n        class_name = each_label\n        if class_id == -1:\n            assert class_name == '__ignore__'\n            continue\n        class_name_to_id[class_name] = class_id\n        data['categories'].append(dict(\n            supercategory=None,\n            id=class_id,\n            name=class_name,\n        ))\n    \n    box_id = 0\n    for i, image in tqdm(enumerate(os.listdir(TRAIN_PATH))):\n\n        img = cv2.imread(TRAIN_PATH+'/'+image)\n        height, width, _ = img.shape\n\n        data['images'].append({\n            'license':0, \n            'url': None,\n            'file_name': image,\n            'height': height,\n            'width': width,\n            'date_camputured': None,\n            'id': i\n        })\n\n        df_temp = df[df.image == image]\n        for index, row in df_temp.iterrows():\n\n            area = round(row.width*row.height, 1)\n            bbox =[row.left, row.top, row.width, row.height]\n\n            data['annotations'].append({\n                'id': box_id,\n                'image_id': i,\n                'category_id': category_id[row.label],\n                'area': area,\n                'bbox':bbox,\n                'iscrowd':0\n            })\n            box_id+=1\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:38.032071Z","iopub.execute_input":"2021-08-21T16:02:38.032458Z","iopub.status.idle":"2021-08-21T16:02:38.046115Z","shell.execute_reply.started":"2021-08-21T16:02:38.032417Z","shell.execute_reply":"2021-08-21T16:02:38.045005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '../input/nfl-health-and-safety-helmet-assignment/images'\nextra_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/image_labels.csv')\n\ncategory_id = {'Helmet':0, 'Helmet-Blurred':1,\n               'Helmet-Difficult':2, 'Helmet-Sideline':3,\n               'Helmet-Partial':4}\n\ndf_train, df_val = train_test_split(extra_df, test_size=0.2, random_state=42)\nann_file_train = create_ann_file(df_train, category_id)\nann_file_val = create_ann_file(df_val, category_id)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:02:38.048059Z","iopub.execute_input":"2021-08-21T16:02:38.048458Z","iopub.status.idle":"2021-08-21T16:14:29.858577Z","shell.execute_reply.started":"2021-08-21T16:02:38.048418Z","shell.execute_reply":"2021-08-21T16:14:29.857713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save data sets\nos.makedirs('../tmp', exist_ok=True)\n\nwith open('../tmp/ann_file_train.json', 'w') as f:\n    json.dump(ann_file_train, f, indent=4)\n        \nwith open('../tmp/ann_file_val.json', 'w') as f:\n    json.dump(ann_file_val, f, indent=4)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:14:29.859866Z","iopub.execute_input":"2021-08-21T16:14:29.860346Z","iopub.status.idle":"2021-08-21T16:14:35.039848Z","shell.execute_reply.started":"2021-08-21T16:14:29.860306Z","shell.execute_reply":"2021-08-21T16:14:35.038882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.dataset_type = 'CocoDataset' # Dataset type, this will be used to define the dataset\ncfg.classes = ('Helmet', 'Helmet-Blurred', 'Helmet-Difficult', 'Helmet-Sideline',\n               'Helmet-Partial')\n\ncfg.data.train.img_prefix = TRAIN_PATH # Prefix of image path\ncfg.data.train.classes = cfg.classes\ncfg.data.train.ann_file = '../tmp/ann_file_train.json'\ncfg.data.train.type='CocoDataset'\n\ncfg.data.val.img_prefix = TRAIN_PATH # Prefix of image path\ncfg.data.val.classes = cfg.classes\ncfg.data.val.ann_file = '../tmp/ann_file_val.json'\ncfg.data.val.type='CocoDataset'\n\ncfg.data.test.img_prefix = TRAIN_PATH # Prefix of image path\ncfg.data.test.classes = cfg.classes\ncfg.data.test.ann_file =  '../tmp/ann_file_val.json'\ncfg.data.test.type='CocoDataset'\n\ncfg.data.samples_per_gpu = 4 # Batch size of a single GPU used in testing\ncfg.data.workers_per_gpu = 2 # Worker to pre-fetch data for each single GPU","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:14:35.041181Z","iopub.execute_input":"2021-08-21T16:14:35.041568Z","iopub.status.idle":"2021-08-21T16:14:35.050712Z","shell.execute_reply.started":"2021-08-21T16:14:35.041527Z","shell.execute_reply":"2021-08-21T16:14:35.048561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The config to build the evaluation hook, refer to https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/evaluation/eval_hooks.py#L7 for more details.\ncfg.evaluation.metric = 'bbox' # Metrics used during evaluation\n\n# Set the epoch intervel to perform evaluation\ncfg.evaluation.interval = 1\n\n# Set the iou threshold of the mAP calculation during evaluation\ncfg.evaluation.iou_thrs = [0.5]\n\n# cfg.evaluation.save_best='bbox_mAP_50'","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:14:35.052173Z","iopub.execute_input":"2021-08-21T16:14:35.052541Z","iopub.status.idle":"2021-08-21T16:14:35.059856Z","shell.execute_reply.started":"2021-08-21T16:14:35.052504Z","shell.execute_reply":"2021-08-21T16:14:35.058764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# consider including other transformations for training\n\nalbu_train_transforms = [\n    dict(type='ShiftScaleRotate', shift_limit=0.0625,\n        scale_limit=0.15, rotate_limit=15, p=0.4),\n    dict(type='RandomBrightnessContrast', brightness_limit=0.2,\n         contrast_limit=0.2, p=0.5),\n#     dict(type='IAAAffine', shear=(-10.0, 10.0), p=0.4),\n# #     dict(type='MixUp', p=0.2, lambd=0.5),\n#     dict(type=\"Blur\", p=1.0, blur_limit=7),\n#     dict(type='CLAHE', p=0.5),\n#     dict(type='Equalize', mode='cv', p=0.4),\n#     dict(\n#         type=\"OneOf\",\n#         transforms=[\n#             dict(type=\"GaussianBlur\", p=1.0, blur_limit=7),\n#             dict(type=\"MedianBlur\", p=1.0, blur_limit=7),\n#         ],\n#         p=0.4,\n#     ),\n    \n#     dict(type='MixUp', p=0.2, lambd=0.5),\n#     dict(type='RandomRotate90', p=0.5),\n#     dict(type='CLAHE', p=0.5),\n#     dict(type='InvertImg', p=0.5),\n#     dict(type='Equalize', mode='cv', p=0.4),\n#     dict(type='MedianBlur', blur_limit=3, p=0.1)\n    ]\n\n\ncfg.train_pipeline = [\n    dict(type='LoadImageFromFile'), # First pipeline to load images from file path\n    dict(type='LoadAnnotations',\n         with_bbox=True,# Whether to use bounding box, True for detection\n         with_mask=True, # Whether to use instance mask, True for instance segmentation\n# Whether to convert the polygon mask to instance mask, set False for acceleration and to save memory\n         poly2mask=False), \n    dict(type='Resize', # Augmentation pipeline that resize the images and their annotations\n         img_scale=(1333, 800), # The largest scale of image\n         keep_ratio=True), # whether to keep the ratio between height and width.\n    dict(type='RandomFlip', flip_ratio=0.5), # Augmentation pipeline that flip the images and their annotations\n    dict(\n        type='Albu',\n        transforms=albu_train_transforms, # transformations defined above\n        bbox_params=dict(\n        type='BboxParams',\n        format='pascal_voc',\n        label_fields=['gt_labels'],\n        min_visibility=0.0,\n        filter_lost_elements=True),\n        keymap=dict(img='image', gt_bboxes='bboxes'),\n        update_pad_shape=False,\n        skip_img_without_anno=True),\n    dict(\n        type='Normalize', # Augmentation pipeline that normalize the input images\n        mean=[123.675, 116.28, 103.53], # These keys are the same of img_norm_cfg since the\n        std=[58.395, 57.12, 57.375], # keys of img_norm_cfg are used here as arguments\n        to_rgb=True),\n    dict(type='Pad', size_divisor=32),# Padding config, The number the padded images should be divisible\n    dict(type='DefaultFormatBundle'), # Default format bundle to gather data in the pipeline\n    dict(type='Collect',# Pipeline that decides which keys in the data should be passed to the detector\n         keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n]\ncfg.test_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(\n        type='MultiScaleFlipAug', # An encapsulation that encapsulates the testing augmentations\n        img_scale=(1333, 800), # Decides the largest scale for testing, used for the Resize pipeline\n        flip=False,\n        transforms=[\n            dict(type='Resize', keep_ratio=True),\n            dict(type='RandomFlip'), # Thought RandomFlip is added in pipeline, it is not used because flip=False\n            dict(\n                type='Normalize',\n                mean=[123.675, 116.28, 103.53],\n                std=[58.395, 57.12, 57.375],\n                to_rgb=True),\n            dict(type='Pad', size_divisor=32), # Padding config to pad images divisable by 32.\n            dict(type='ImageToTensor', keys=['img']),\n            dict(type='Collect', keys=['img']) # Collect pipeline that collect necessary keys for testing.\n        ])\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:14:35.061289Z","iopub.execute_input":"2021-08-21T16:14:35.06168Z","iopub.status.idle":"2021-08-21T16:14:35.076625Z","shell.execute_reply.started":"2021-08-21T16:14:35.061641Z","shell.execute_reply":"2021-08-21T16:14:35.075767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg_path = f'{job_folder}/job{job}_{Path(baseline_cfg_path).name}'\nprint(cfg_path)\n\n# Save config file for inference later\ncfg.dump(cfg_path)\nprint(f'Config:\\n{cfg.pretty_text}')","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:14:35.077966Z","iopub.execute_input":"2021-08-21T16:14:35.078352Z","iopub.status.idle":"2021-08-21T16:14:35.979587Z","shell.execute_reply.started":"2021-08-21T16:14:35.078313Z","shell.execute_reply":"2021-08-21T16:14:35.978728Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build Dataset and Start Training\n\nmodel = build_detector(cfg.model,\n                       train_cfg=cfg.get('train_cfg'),\n                       test_cfg=cfg.get('test_cfg'))\nmodel.init_weights()\ndatasets = [build_dataset(cfg.data.train)]","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:14:35.980907Z","iopub.execute_input":"2021-08-21T16:14:35.981489Z","iopub.status.idle":"2021-08-21T16:15:01.900173Z","shell.execute_reply.started":"2021-08-21T16:14:35.981443Z","shell.execute_reply":"2021-08-21T16:15:01.899296Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_detector(model, datasets[0], cfg, distributed=False, validate=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T16:15:01.901514Z","iopub.execute_input":"2021-08-21T16:15:01.901924Z","iopub.status.idle":"2021-08-21T18:28:50.609457Z","shell.execute_reply.started":"2021-08-21T16:15:01.901885Z","shell.execute_reply":"2021-08-21T18:28:50.608532Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best epoch number\nimport json\nfrom collections import defaultdict\n\nlog_file = f'{job_folder}/None.log.json'\n\n# Source: mmdetection/tools/analysis_tools/analyze_logs.py \ndef load_json_logs(json_logs):\n    # load and convert json_logs to log_dict, key is epoch, value is a sub dict\n    # keys of sub dict is different metrics, e.g. memory, bbox_mAP\n    # value of sub dict is a list of corresponding values of all iterations\n    log_dicts = [dict() for _ in json_logs]\n    for json_log, log_dict in zip(json_logs, log_dicts):\n        with open(json_log, 'r') as log_file:\n            for line in log_file:\n                log = json.loads(line.strip())\n                # skip lines without `epoch` field\n                if 'epoch' not in log:\n                    continue\n                epoch = log.pop('epoch')\n                if epoch not in log_dict:\n                    log_dict[epoch] = defaultdict(list)\n                for k, v in log.items():\n                    log_dict[epoch][k].append(v)\n    return log_dicts\n\nlog_dict = load_json_logs([log_file])\n# [(print(inner['bbox_mAP']) for inner in item) for item in log_dict]\n# [print(item) for item in log_dict[0]]\nbest_epoch = np.argmax([item['bbox_mAP'][0] for item in log_dict[0].values()])+1\nbest_epoch","metadata":{"execution":{"iopub.status.busy":"2021-08-21T18:28:50.611371Z","iopub.execute_input":"2021-08-21T18:28:50.611795Z","iopub.status.idle":"2021-08-21T18:28:50.629302Z","shell.execute_reply.started":"2021-08-21T18:28:50.611754Z","shell.execute_reply":"2021-08-21T18:28:50.628105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/input/nfl-health-and-safety-helmet-assignment/'\nexample_video = f'{data_dir}/test/57906_000718_Endzone.mp4'\n\nfrac = 0.65\ndisplay(Video(example_video, embed=True, height=int(720*frac), width=int(1280*frac)))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T18:36:28.713388Z","iopub.execute_input":"2021-08-21T18:36:28.713797Z","iopub.status.idle":"2021-08-21T18:36:29.250345Z","shell.execute_reply.started":"2021-08-21T18:36:28.713762Z","shell.execute_reply":"2021-08-21T18:36:29.249093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The process of converting video into frames, predicting bboxes and converting back into video is described here https://www.kaggle.com/eneszvo/mmdetection-player-tracking-for-beginners","metadata":{}},{"cell_type":"code","source":"# create frames \nimg_ext = 'png'\nimage_name = '57906_000718_Endzone'\nframe_dir = '/kaggle/tmp/mp4_img/'\nos.makedirs(frame_dir, exist_ok=True)\n\ncmd = 'ffmpeg -i \\\"{}\\\" -qscale:v 2 \\\"{}/{}_%d.{}\\\"'.format(example_video, frame_dir, image_name, img_ext)\nprint(cmd)\nsubprocess.call(cmd, shell=True)\n\nframe_bbox_dir = '/kaggle/tmp/mp4_img_bbox/'\nos.makedirs(frame_bbox_dir, exist_ok=True)\ncheckpoint = f'{job_folder}/epoch_{best_epoch}.pth'\nprint(\"Loading weights from:\", checkpoint)\ncfg = Config.fromfile(cfg_path)\n\nfor f in tqdm(os.listdir(frame_dir)):\n    img = f'{frame_dir}/{f}'\n    # the model is initialized and deleted each time because of RAM usage\n    model = init_detector(cfg, checkpoint, device='cuda:0')\n    # get results\n    result = inference_detector(model, img)\n    # save image with bboxes into out_file\n    model.show_result(img, result, out_file=os.path.join(frame_bbox_dir,f))\n    del result, model\n    gc.collect()\n    \n# make video from frames\nvideo_name = '57906_000718_Endzone_fps60.mp4'\ntmp_video_path = os.path.join('/kaggle/working/', f'tmp_{video_name}')\nvideo_path = os.path.join('/kaggle/working/', video_name)\n\nframe_rate = 60\n\nimages = [img for img in os.listdir(frame_bbox_dir)]\nimages.sort(key = lambda x: int(x.split('_')[-1][:-4]))\n\nframe = cv2.imread(os.path.join(frame_bbox_dir, images[0]))\nheight, width, layers = frame.shape\n\nvideo = cv2.VideoWriter(tmp_video_path, cv2.VideoWriter_fourcc(*'MP4V'),\n                        frame_rate, (width,height))\n\nfor f in images:\n    img = cv2.imread(os.path.join(frame_bbox_dir, f))\n    video.write(img)\n\nvideo.release()\n\n# Not all browsers support the codec, we will re-load the file at tmp_video_path\n# and convert to a codec that is more broadly readable using ffmpeg\n\nif os.path.exists(video_path):\n    os.remove(video_path)\n    \nsubprocess.run([\"ffmpeg\", \"-i\", tmp_video_path, \"-crf\", \"18\", \"-preset\", \"veryfast\",\n                \"-vcodec\",\"libx264\", video_path,])\n\nos.remove(tmp_video_path)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T20:08:09.076503Z","iopub.execute_input":"2021-08-21T20:08:09.076916Z","iopub.status.idle":"2021-08-21T20:28:22.906125Z","shell.execute_reply.started":"2021-08-21T20:08:09.076881Z","shell.execute_reply":"2021-08-21T20:28:22.904835Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm not sure how to change label above boxes, but anyway, we can see that results are good even with one epoch of training.","metadata":{}},{"cell_type":"code","source":"frac = 0.65\ndisplay(Video(video_path, embed=True, height=int(720*frac), width=int(1280*frac)))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T20:28:22.908143Z","iopub.execute_input":"2021-08-21T20:28:22.908476Z","iopub.status.idle":"2021-08-21T20:28:23.156733Z","shell.execute_reply.started":"2021-08-21T20:28:22.908446Z","shell.execute_reply":"2021-08-21T20:28:23.150461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove directories with frames (optional)\n\nfor path in [frame_dir, frame_bbox_dir]:\n    try:\n        shutil.rmtree(path)\n    except OSError as e:\n        print (\"Error: %s - %s.\" % (e.filename, e.strerror))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T20:02:02.729409Z","iopub.execute_input":"2021-08-21T20:02:02.729769Z","iopub.status.idle":"2021-08-21T20:02:04.357285Z","shell.execute_reply.started":"2021-08-21T20:02:02.729738Z","shell.execute_reply":"2021-08-21T20:02:04.356223Z"},"trusted":true},"execution_count":null,"outputs":[]}]}