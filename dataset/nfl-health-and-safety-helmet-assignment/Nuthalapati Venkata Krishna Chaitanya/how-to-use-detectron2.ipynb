{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/detectron2.git\n%cd detectron2\n!python -m pip install -e ./","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-23T18:25:01.355279Z","iopub.execute_input":"2021-10-23T18:25:01.355703Z","iopub.status.idle":"2021-10-23T18:25:09.137081Z","shell.execute_reply.started":"2021-10-23T18:25:01.355617Z","shell.execute_reply":"2021-10-23T18:25:09.136134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hack to use custom dataset with CLI\nOfficial tutorial in detectron2 use their API and need to write custom traning script, but I prefer to use default CLI interface `tools/train_net.py`.\nTo do that, we need to dig into the code where datasets are set up and register our custom dataset information.\nBelow code show how to modify `detectron2/data/datasets/builtin.py` to register `nfl2021_train` and `nfl2021_valid` dataset.","metadata":{}},{"cell_type":"code","source":"%%writefile detectron2/data/datasets/builtin.py\n\n# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\n\"\"\"\nThis file registers pre-defined datasets at hard-coded paths, and their metadata.\n\nWe hard-code metadata for common datasets. This will enable:\n1. Consistency check when loading the datasets\n2. Use models on these standard datasets directly and run demos,\n   without having to download the dataset annotations\n\nWe hard-code some paths to the dataset that's assumed to\nexist in \"./datasets/\".\n\nUsers SHOULD NOT use this file to create new dataset / metadata for new dataset.\nTo add new dataset, refer to the tutorial \"docs/DATASETS.md\".\n\"\"\"\n\nimport os\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\n\nfrom .builtin_meta import ADE20K_SEM_SEG_CATEGORIES, _get_builtin_metadata\nfrom .cityscapes import load_cityscapes_instances, load_cityscapes_semantic\nfrom .cityscapes_panoptic import register_all_cityscapes_panoptic\nfrom .coco import load_sem_seg, register_coco_instances\nfrom .coco_panoptic import register_coco_panoptic, register_coco_panoptic_separated\nfrom .lvis import get_lvis_instances_meta, register_lvis_instances\nfrom .pascal_voc import register_pascal_voc\n\n# ==== Predefined datasets and splits for COCO ==========\n\n_PREDEFINED_SPLITS_COCO = {}\n_PREDEFINED_SPLITS_COCO[\"coco\"] = {\n    \"coco_2014_train\": (\"coco/train2014\", \"coco/annotations/instances_train2014.json\"),\n    \"coco_2014_val\": (\"coco/val2014\", \"coco/annotations/instances_val2014.json\"),\n    \"coco_2014_minival\": (\"coco/val2014\", \"coco/annotations/instances_minival2014.json\"),\n    \"coco_2014_minival_100\": (\"coco/val2014\", \"coco/annotations/instances_minival2014_100.json\"),\n    \"coco_2014_valminusminival\": (\n        \"coco/val2014\",\n        \"coco/annotations/instances_valminusminival2014.json\",\n    ),\n    \"coco_2017_train\": (\"coco/train2017\", \"coco/annotations/instances_train2017.json\"),\n    \"coco_2017_val\": (\"coco/val2017\", \"coco/annotations/instances_val2017.json\"),\n    \"coco_2017_test\": (\"coco/test2017\", \"coco/annotations/image_info_test2017.json\"),\n    \"coco_2017_test-dev\": (\"coco/test2017\", \"coco/annotations/image_info_test-dev2017.json\"),\n    \"coco_2017_val_100\": (\"coco/val2017\", \"coco/annotations/instances_val2017_100.json\"),\n}\n\n_PREDEFINED_SPLITS_COCO[\"coco_person\"] = {\n    \"keypoints_coco_2014_train\": (\n        \"coco/train2014\",\n        \"coco/annotations/person_keypoints_train2014.json\",\n    ),\n    \"keypoints_coco_2014_val\": (\"coco/val2014\", \"coco/annotations/person_keypoints_val2014.json\"),\n    \"keypoints_coco_2014_minival\": (\n        \"coco/val2014\",\n        \"coco/annotations/person_keypoints_minival2014.json\",\n    ),\n    \"keypoints_coco_2014_valminusminival\": (\n        \"coco/val2014\",\n        \"coco/annotations/person_keypoints_valminusminival2014.json\",\n    ),\n    \"keypoints_coco_2014_minival_100\": (\n        \"coco/val2014\",\n        \"coco/annotations/person_keypoints_minival2014_100.json\",\n    ),\n    \"keypoints_coco_2017_train\": (\n        \"coco/train2017\",\n        \"coco/annotations/person_keypoints_train2017.json\",\n    ),\n    \"keypoints_coco_2017_val\": (\"coco/val2017\", \"coco/annotations/person_keypoints_val2017.json\"),\n    \"keypoints_coco_2017_val_100\": (\n        \"coco/val2017\",\n        \"coco/annotations/person_keypoints_val2017_100.json\",\n    ),\n}\n\n\n_PREDEFINED_SPLITS_COCO_PANOPTIC = {\n    \"coco_2017_train_panoptic\": (\n        # This is the original panoptic annotation directory\n        \"coco/panoptic_train2017\",\n        \"coco/annotations/panoptic_train2017.json\",\n        # This directory contains semantic annotations that are\n        # converted from panoptic annotations.\n        # It is used by PanopticFPN.\n        # You can use the script at detectron2/datasets/prepare_panoptic_fpn.py\n        # to create these directories.\n        \"coco/panoptic_stuff_train2017\",\n    ),\n    \"coco_2017_val_panoptic\": (\n        \"coco/panoptic_val2017\",\n        \"coco/annotations/panoptic_val2017.json\",\n        \"coco/panoptic_stuff_val2017\",\n    ),\n    \"coco_2017_val_100_panoptic\": (\n        \"coco/panoptic_val2017_100\",\n        \"coco/annotations/panoptic_val2017_100.json\",\n        \"coco/panoptic_stuff_val2017_100\",\n    ),\n}\n\n\ndef register_all_coco(root):\n    for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_COCO.items():\n        for key, (image_root, json_file) in splits_per_dataset.items():\n            # Assume pre-defined datasets live in `./datasets`.\n            register_coco_instances(\n                key,\n                _get_builtin_metadata(dataset_name),\n                os.path.join(root, json_file) if \"://\" not in json_file else json_file,\n                os.path.join(root, image_root),\n            )\n\n    for (\n        prefix,\n        (panoptic_root, panoptic_json, semantic_root),\n    ) in _PREDEFINED_SPLITS_COCO_PANOPTIC.items():\n        prefix_instances = prefix[: -len(\"_panoptic\")]\n        instances_meta = MetadataCatalog.get(prefix_instances)\n        image_root, instances_json = instances_meta.image_root, instances_meta.json_file\n        # The \"separated\" version of COCO panoptic segmentation dataset,\n        # e.g. used by Panoptic FPN\n        register_coco_panoptic_separated(\n            prefix,\n            _get_builtin_metadata(\"coco_panoptic_separated\"),\n            image_root,\n            os.path.join(root, panoptic_root),\n            os.path.join(root, panoptic_json),\n            os.path.join(root, semantic_root),\n            instances_json,\n        )\n        # The \"standard\" version of COCO panoptic segmentation dataset,\n        # e.g. used by Panoptic-DeepLab\n        register_coco_panoptic(\n            prefix,\n            _get_builtin_metadata(\"coco_panoptic_standard\"),\n            image_root,\n            os.path.join(root, panoptic_root),\n            os.path.join(root, panoptic_json),\n            instances_json,\n        )\n\n\n# ==== Predefined datasets and splits for LVIS ==========\n\n\n_PREDEFINED_SPLITS_LVIS = {\n    \"lvis_v1\": {\n        \"lvis_v1_train\": (\"coco/\", \"lvis/lvis_v1_train.json\"),\n        \"lvis_v1_val\": (\"coco/\", \"lvis/lvis_v1_val.json\"),\n        \"lvis_v1_test_dev\": (\"coco/\", \"lvis/lvis_v1_image_info_test_dev.json\"),\n        \"lvis_v1_test_challenge\": (\"coco/\", \"lvis/lvis_v1_image_info_test_challenge.json\"),\n    },\n    \"lvis_v0.5\": {\n        \"lvis_v0.5_train\": (\"coco/\", \"lvis/lvis_v0.5_train.json\"),\n        \"lvis_v0.5_val\": (\"coco/\", \"lvis/lvis_v0.5_val.json\"),\n        \"lvis_v0.5_val_rand_100\": (\"coco/\", \"lvis/lvis_v0.5_val_rand_100.json\"),\n        \"lvis_v0.5_test\": (\"coco/\", \"lvis/lvis_v0.5_image_info_test.json\"),\n    },\n    \"lvis_v0.5_cocofied\": {\n        \"lvis_v0.5_train_cocofied\": (\"coco/\", \"lvis/lvis_v0.5_train_cocofied.json\"),\n        \"lvis_v0.5_val_cocofied\": (\"coco/\", \"lvis/lvis_v0.5_val_cocofied.json\"),\n    },\n}\n\n\ndef register_all_lvis(root):\n    for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_LVIS.items():\n        for key, (image_root, json_file) in splits_per_dataset.items():\n            register_lvis_instances(\n                key,\n                get_lvis_instances_meta(dataset_name),\n                os.path.join(root, json_file) if \"://\" not in json_file else json_file,\n                os.path.join(root, image_root),\n            )\n\n\n# ==== Predefined splits for raw cityscapes images ===========\n_RAW_CITYSCAPES_SPLITS = {\n    \"cityscapes_fine_{task}_train\": (\"cityscapes/leftImg8bit/train/\", \"cityscapes/gtFine/train/\"),\n    \"cityscapes_fine_{task}_val\": (\"cityscapes/leftImg8bit/val/\", \"cityscapes/gtFine/val/\"),\n    \"cityscapes_fine_{task}_test\": (\"cityscapes/leftImg8bit/test/\", \"cityscapes/gtFine/test/\"),\n}\n\n\ndef register_all_cityscapes(root):\n    for key, (image_dir, gt_dir) in _RAW_CITYSCAPES_SPLITS.items():\n        meta = _get_builtin_metadata(\"cityscapes\")\n        image_dir = os.path.join(root, image_dir)\n        gt_dir = os.path.join(root, gt_dir)\n\n        inst_key = key.format(task=\"instance_seg\")\n        DatasetCatalog.register(\n            inst_key,\n            lambda x=image_dir, y=gt_dir: load_cityscapes_instances(\n                x, y, from_json=True, to_polygons=True\n            ),\n        )\n        MetadataCatalog.get(inst_key).set(\n            image_dir=image_dir, gt_dir=gt_dir, evaluator_type=\"cityscapes_instance\", **meta\n        )\n\n        sem_key = key.format(task=\"sem_seg\")\n        DatasetCatalog.register(\n            sem_key, lambda x=image_dir, y=gt_dir: load_cityscapes_semantic(x, y)\n        )\n        MetadataCatalog.get(sem_key).set(\n            image_dir=image_dir,\n            gt_dir=gt_dir,\n            evaluator_type=\"cityscapes_sem_seg\",\n            ignore_label=255,\n            **meta,\n        )\n\n\n# ==== Predefined splits for PASCAL VOC ===========\ndef register_all_pascal_voc(root):\n    SPLITS = [\n        (\"voc_2007_trainval\", \"VOC2007\", \"trainval\"),\n        (\"voc_2007_train\", \"VOC2007\", \"train\"),\n        (\"voc_2007_val\", \"VOC2007\", \"val\"),\n        (\"voc_2007_test\", \"VOC2007\", \"test\"),\n        (\"voc_2012_trainval\", \"VOC2012\", \"trainval\"),\n        (\"voc_2012_train\", \"VOC2012\", \"train\"),\n        (\"voc_2012_val\", \"VOC2012\", \"val\"),\n    ]\n    for name, dirname, split in SPLITS:\n        year = 2007 if \"2007\" in name else 2012\n        register_pascal_voc(name, os.path.join(root, dirname), split, year)\n        MetadataCatalog.get(name).evaluator_type = \"pascal_voc\"\n\n\ndef register_all_ade20k(root):\n    root = os.path.join(root, \"ADEChallengeData2016\")\n    for name, dirname in [(\"train\", \"training\"), (\"val\", \"validation\")]:\n        image_dir = os.path.join(root, \"images\", dirname)\n        gt_dir = os.path.join(root, \"annotations_detectron2\", dirname)\n        name = f\"ade20k_sem_seg_{name}\"\n        DatasetCatalog.register(\n            name, lambda x=image_dir, y=gt_dir: load_sem_seg(y, x, gt_ext=\"png\", image_ext=\"jpg\")\n        )\n        MetadataCatalog.get(name).set(\n            stuff_classes=ADE20K_SEM_SEG_CATEGORIES[:],\n            image_root=image_dir,\n            sem_seg_root=gt_dir,\n            evaluator_type=\"sem_seg\",\n            ignore_label=255,\n        )\n\ndef register_my_coco_datasets():\n    \"\"\"The function to register custom coco datasets.\"\"\"\n    register_coco_instances(\n        \"nfl2021_train\",\n        {},\n        \"/kaggle/input/nfl2021-coco-train-val-annotations/train.json\",\n        \"/kaggle/input/nfl2021-train-images/train_images\"\n    )\n    register_coco_instances(\n        \"nfl2021_valid\",\n        {},\n        \"/kaggle/input/nfl2021-coco-train-val-annotations/valid.json\",\n        \"/kaggle/input/nfl2021-train-images/train_images\"\n    )\n\n# True for open source;\n# Internally at fb, we register them elsewhere\nif __name__.endswith(\".builtin\"):\n    # Assume pre-defined datasets live in `./datasets`.\n    _root = os.getenv(\"DETECTRON2_DATASETS\", \"datasets\")\n    register_all_coco(_root)\n    register_all_lvis(_root)\n    register_all_cityscapes(_root)\n    register_all_cityscapes_panoptic(_root)\n    register_all_pascal_voc(_root)\n    register_all_ade20k(_root)\n    # Custom registeration happends here.\n    register_my_coco_datasets()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:25:20.344164Z","iopub.execute_input":"2021-10-23T18:25:20.34455Z","iopub.status.idle":"2021-10-23T18:25:20.3544Z","shell.execute_reply.started":"2021-10-23T18:25:20.344507Z","shell.execute_reply":"2021-10-23T18:25:20.353489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config set up\nNow you only need to modifiy config to train detector!\nBelow is the example to use FasterRCNN, but you can use any of configs in detectron2 repository.  \nhttps://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md  \nhttps://github.com/facebookresearch/detectron2/tree/master/configs  \n\nBelow looks much simpler than mmdetection configs, but it actually has a lot of hidden parameters in [default config](https://github.com/facebookresearch/detectron2/blob/master/detectron2/config/defaults.py).\n(Personally I feel it's more explicit to expose all config in yaml file.)\n\nAnyway, below are the summary of what you need to modify.\n\n### Dataset\n1. You need to specify dataset name both for train and valid. The details are already registered, so you don't need to set here.\n\n### Model\n1. You need to change num classes(`MODEL.ROI_HEADS.NUM_CLASSES`) to 1.(We only need to detect only Helmet class.)\n\n### Others\n1. You need to set pretrained model path in `MODEL.WEIGHTS` because what we want to do here is finetune, not training from scratch.\n2. Also it's better to decrease LR for finetuning.\n3. You can also change `max_epochs` as you want, here I set to 1 for just demonstration.","metadata":{}},{"cell_type":"code","source":"%%writefile configs/custom_faster_rcnn.yaml\n\nMODEL:\n  WEIGHTS: \"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_1x/137257794/model_final_b275ba.pkl\"\n  MASK_ON: False\n  RESNETS:\n    DEPTH: 50\n  META_ARCHITECTURE: \"GeneralizedRCNN\"\n  BACKBONE:\n    NAME: \"build_resnet_fpn_backbone\"\n  RESNETS:\n    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n  FPN:\n    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n  ANCHOR_GENERATOR:\n    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map\n    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)\n  RPN:\n    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n    PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level\n    PRE_NMS_TOPK_TEST: 1000  # Per FPN level\n    # Detectron1 uses 2000 proposals per-batch,\n    # (See \"modeling/rpn/rpn_outputs.py\" for details of this legacy issue)\n    # which is approximately 1000 proposals per-image since the default batch size for FPN is 2.\n    POST_NMS_TOPK_TRAIN: 1000\n    POST_NMS_TOPK_TEST: 1000\n  ROI_HEADS:\n    NAME: \"StandardROIHeads\"\n    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\"]\n    NUM_CLASSES: 1 # Added\n  ROI_BOX_HEAD:\n    NAME: \"FastRCNNConvFCHead\"\n    NUM_FC: 2\n    POOLER_RESOLUTION: 7\nDATASETS:\n  TRAIN: (\"nfl2021_train\",) # Modified\n  TEST: (\"nfl2021_valid\",) # Modified\nSOLVER:\n  IMS_PER_BATCH: 2 # Modified\n  BASE_LR: 0.002 # Modified\n  STEPS: (80000,) # If you changed MAX_ITER, it's better to change when you decrease LR too here.\n  MAX_ITER: 90000 # You may want to modify this to control how long to train\nINPUT:\n  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\nVERSION: 2\n","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:25:36.039313Z","iopub.execute_input":"2021-10-23T18:25:36.039718Z","iopub.status.idle":"2021-10-23T18:25:36.045985Z","shell.execute_reply.started":"2021-10-23T18:25:36.039684Z","shell.execute_reply":"2021-10-23T18:25:36.044996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/detectron2.git\n%cd detectron2\n!python -m pip install -e ./","metadata":{"execution":{"iopub.status.busy":"2021-10-23T07:41:30.865236Z","iopub.execute_input":"2021-10-23T07:41:30.865623Z","iopub.status.idle":"2021-10-23T07:41:37.91658Z","shell.execute_reply.started":"2021-10-23T07:41:30.865587Z","shell.execute_reply":"2021-10-23T07:41:37.915467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training is just one line. \n#!python tools/train_net.py --num-gpus 1 --config-file configs/custom_faster_rcnn.yaml","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:43:15.374499Z","iopub.execute_input":"2021-10-23T18:43:15.374904Z","iopub.status.idle":"2021-10-23T18:43:16.678469Z","shell.execute_reply.started":"2021-10-23T18:43:15.374868Z","shell.execute_reply":"2021-10-23T18:43:16.677561Z"},"trusted":true},"execution_count":null,"outputs":[]}]}