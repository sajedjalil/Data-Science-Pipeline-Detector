{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I often find inspiration about how a model can be improved by visualizing it's predictions. In this notebook I provide a function `video_with_predictions`. This function combines a model's validation predictions with the assoicated video.\n\nThe predictions I'm modeling come from the great baseline here: https://www.kaggle.com/its7171/nfl-baseline-simple-helmet-mapping\n\nNote, in the output videos:\n- If incorrect, the ground truth box helmet boxes are `black` and predictions are `white`. \n- Correct preidctions have `green` boxes\n- Impact helmet boxes are `red` if incorrect and `yellow` if correct (note these predictions are weighted 1000x compared to a non-impact helmet box prediction)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport itertools\nimport glob\nimport os\nimport cv2\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import KMeans\nimport random\n\n\ndebug = True\nCONF_THRE = 0.3\nBASE_DIR = '../input/nfl-health-and-safety-helmet-assignment'\n\nlabels = pd.read_csv(f'{BASE_DIR}/train_labels.csv')\nif debug:\n    tracking = pd.read_csv(f'{BASE_DIR}/train_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/train_baseline_helmets.csv')\nelse:\n    tracking = pd.read_csv(f'{BASE_DIR}/test_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}/test_baseline_helmets.csv')\n    \n    \n# copied from https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) / 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\ntracking = add_track_features(tracking)\n\n\nif debug:\n#     sample_keys = random.sample(list(tracking['gameKey'].unique()), 3)\n    sample_keys = [57583, 58095, 57682]\n    helmets['gameKey'] = helmets['video_frame'].str.split('_').str[0]\n    tracking = tracking[tracking['gameKey'].isin(sample_keys)]\n    helmets = helmets[helmets['gameKey'].astype(int).isin(sample_keys)]\n    labels = labels[labels['gameKey'].astype(int).isin(sample_keys)]\ntracking.shape, helmets.shape, labels.shape\n\n\ndef find_nearest(array, value):\n    value = int(value)\n    array = np.asarray(array).astype(int)\n    idx = (np.abs(array - value)).argmin()\n    return array[idx]\n\ndef norm_arr(a):\n    a = a-a.min()\n    a = a/a.max()\n    return a\n    \ndef dist(a1, a2):\n    return np.linalg.norm(a1-a2)\n\nmax_iter = 2000\ndef dist_for_different_len(a1, a2):\n    assert len(a1) >= len(a2), f'{len(a1)}, {len(a2)}'\n    len_diff = len(a1) - len(a2)\n    a2 = norm_arr(a2)\n    if len_diff == 0:\n        a1 = norm_arr(a1)\n        return dist(a1,a2), ()\n    else:\n        min_dist = 10000\n        min_detete_idx = None\n        cnt = 0\n        del_list = list(itertools.combinations(range(len(a1)),len_diff))\n        if len(del_list) > max_iter:\n            del_list = random.sample(del_list, max_iter)\n        for detete_idx in del_list:\n            this_a1 = np.delete(a1, detete_idx)\n            this_a1 = norm_arr(this_a1)\n            this_dist = dist(this_a1, a2)\n            #print(len(a1), len(a2), this_dist)\n            if min_dist > this_dist:\n                min_dist = this_dist\n                min_detete_idx = detete_idx\n                \n        return min_dist, min_detete_idx\n        \ndef rotate_arr(u, t, deg=True):\n    if deg == True:\n        t = np.deg2rad(t)\n    R = np.array([[np.cos(t), -np.sin(t)],\n                  [np.sin(t),  np.cos(t)]])\n    return  np.dot(R, u)\n\ndef dist_rot(tracking_df, a2):\n    tracking_df = tracking_df.sort_values('x')\n    x = tracking_df['x']\n    y = tracking_df['y']\n    min_dist = 10000\n    min_idx = None\n    min_x = None\n    dig_step = 3\n    dig_max = dig_step*10\n    for dig in range(-dig_max,dig_max+1,dig_step):\n        arr = rotate_arr(np.array((x,y)), dig)\n        this_dist, this_idx = dist_for_different_len(np.sort(arr[0]), a2)\n        if min_dist > this_dist:\n            min_dist = this_dist\n            min_idx = this_idx\n            min_x = arr[0]\n    tracking_df['x_rot'] = min_x\n    player_arr = tracking_df.sort_values('x_rot')['player'].values\n    players = np.delete(player_arr,min_idx)\n    return min_dist, players\n\n\ndef mapping_df(args):\n    video_frame, df = args\n    gameKey,playID,view,frame = video_frame.split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame)\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    est_frame = find_nearest(this_tracking.est_frame.values, frame)\n    this_tracking = this_tracking[this_tracking['est_frame']==est_frame]\n    len_this_tracking = len(this_tracking)\n    df['center_h_p'] = (df['left']+df['width']/2).astype(int)\n    df['center_h_m'] = (df['left']+df['width']/2).astype(int)*-1\n    df = df[df['conf']>CONF_THRE].copy()\n    if len(df) > len_this_tracking:\n        df = df.tail(len_this_tracking)\n    df_p = df.sort_values('center_h_p').copy()\n    df_m = df.sort_values('center_h_m').copy()\n    \n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n    a2_p = df_p['center_h_p'].values\n    a2_m = df_m['center_h_m'].values\n\n    min_dist_p, min_detete_idx_p = dist_rot(this_tracking ,a2_p)\n    min_dist_m, min_detete_idx_m = dist_rot(this_tracking ,a2_m)\n    if min_dist_p < min_dist_m:\n        min_dist = min_dist_p\n        min_detete_idx = min_detete_idx_p\n        tgt_df = df_p\n    else:\n        min_dist = min_dist_m\n        min_detete_idx = min_detete_idx_m\n        tgt_df = df_m\n    #print(video_frame, len(this_tracking), len(df), len(df[df['conf']>CONF_THRE]), this_tracking['x'].mean(), min_dist_p, min_dist_m, min_dist)\n    tgt_df['label'] = min_detete_idx\n    return tgt_df[['video_frame','left','width','top','height','label']]\n\np = Pool(processes=4)\nsubmission_df_list = []\ndf_list = list(helmets.groupby('video_frame'))\nwith tqdm(total=len(df_list)) as pbar:\n    for this_df in p.imap(mapping_df, df_list):\n        submission_df_list.append(this_df)\n        pbar.update(1)\np.close()\n\nsubmission_df = pd.concat(submission_df_list)\n# submission_df.to_csv('submission.csv', index=False)\n\n\n# copied from https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\nclass NFLAssignmentScorer:\n    def __init__(\n        self,\n        labels_df: pd.DataFrame = None,\n        labels_csv=\"train_labels.csv\",\n        check_constraints=True,\n        weight_col=\"isDefinitiveImpact\",\n        impact_weight=1000,\n        iou_threshold=0.35,\n        remove_sideline=True,\n    ):\n        \"\"\"\n        Helper class for grading submissions in the\n        2021 Kaggle Competition for helmet assignment.\n        Version 1.0\n        https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\n\n        Use:\n        ```\n        scorer = NFLAssignmentScorer(labels)\n        scorer.score(submission_df)\n\n        or\n\n        scorer = NFLAssignmentScorer(labels_csv='labels.csv')\n        scorer.score(submission_df)\n        ```\n\n        Args:\n            labels_df (pd.DataFrame, optional):\n                Dataframe containing theground truth label boxes.\n            labels_csv (str, optional): CSV of the ground truth label.\n            check_constraints (bool, optional): Tell the scorer if it\n                should check the submission file to meet the competition\n                constraints. Defaults to True.\n            weight_col (str, optional):\n                Column in the labels DataFrame used to applying the scoring\n                weight.\n            impact_weight (int, optional):\n                The weight applied to impacts in the scoring metrics.\n                Defaults to 1000.\n            iou_threshold (float, optional):\n                The minimum IoU allowed to correctly pair a ground truth box\n                with a label. Defaults to 0.35.\n            remove_sideline (bool, optional):\n                Remove slideline players from the labels DataFrame\n                before scoring.\n        \"\"\"\n        if labels_df is None:\n            # Read label from CSV\n            if labels_csv is None:\n                raise Exception(\"labels_df or labels_csv must be provided\")\n            else:\n                self.labels_df = pd.read_csv(labels_csv)\n        else:\n            self.labels_df = labels_df.copy()\n        if remove_sideline:\n            self.labels_df = (\n                self.labels_df.query(\"isSidelinePlayer == False\")\n                .reset_index(drop=True)\n                .copy()\n            )\n        self.impact_weight = impact_weight\n        self.check_constraints = check_constraints\n        self.weight_col = weight_col\n        self.iou_threshold = iou_threshold\n\n    def check_submission(self, sub):\n        \"\"\"\n        Checks that the submission meets all the requirements.\n\n        1. No more than 22 Boxes per frame.\n        2. Only one label prediction per video/frame\n        3. No duplicate boxes per frame.\n\n        Args:\n            sub : submission dataframe.\n\n        Returns:\n            True -> Passed the tests\n            False -> Failed the test\n        \"\"\"\n        # Maximum of 22 boxes per frame.\n        max_box_per_frame = sub.groupby([\"video_frame\"])[\"label\"].count().max()\n        if max_box_per_frame > 22:\n            print(\"Has more than 22 boxes in a single frame\")\n            return False\n        # Only one label allowed per frame.\n        has_duplicate_labels = sub[[\"video_frame\", \"label\"]].duplicated().any()\n        if has_duplicate_labels:\n            print(\"Has duplicate labels\")\n            return False\n        # Check for unique boxes\n        has_duplicate_boxes = (\n            sub[[\"video_frame\", \"left\", \"width\", \"top\", \"height\"]].duplicated().any()\n        )\n        if has_duplicate_boxes:\n            print(\"Has duplicate boxes\")\n            return False\n        return True\n\n    def add_xy(self, df):\n        \"\"\"\n        Adds `x1`, `x2`, `y1`, and `y2` columns necessary for computing IoU.\n\n        Note - for pixel math, 0,0 is the top-left corner so box orientation\n        defined as right and down (height)\n        \"\"\"\n\n        df[\"x1\"] = df[\"left\"]\n        df[\"x2\"] = df[\"left\"] + df[\"width\"]\n        df[\"y1\"] = df[\"top\"]\n        df[\"y2\"] = df[\"top\"] + df[\"height\"]\n        return df\n\n    def merge_sub_labels(self, sub, labels, weight_col=\"isDefinitiveImpact\"):\n        \"\"\"\n        Perform an outer join between submission and label.\n        Creates a `sub_label` dataframe which stores the matched label for each submission box.\n        Ground truth values are given the `_gt` suffix, submission values are given `_sub` suffix.\n        \"\"\"\n        sub = sub.copy()\n        labels = labels.copy()\n\n        sub = self.add_xy(sub)\n        labels = self.add_xy(labels)\n\n        base_columns = [\n            \"label\",\n            \"video_frame\",\n            \"x1\",\n            \"x2\",\n            \"y1\",\n            \"y2\",\n            \"left\",\n            \"width\",\n            \"top\",\n            \"height\",\n        ]\n\n        sub_labels = sub[base_columns].merge(\n            labels[base_columns + [weight_col]],\n            on=[\"video_frame\"],\n            how=\"right\",\n            suffixes=(\"_sub\", \"_gt\"),\n        )\n        return sub_labels\n\n    def get_iou_df(self, df):\n        \"\"\"\n        This function computes the IOU of submission (sub)\n        bounding boxes against the ground truth boxes (gt).\n        \"\"\"\n        df = df.copy()\n\n        # 1. get the coordinate of inters\n        df[\"ixmin\"] = df[[\"x1_sub\", \"x1_gt\"]].max(axis=1)\n        df[\"ixmax\"] = df[[\"x2_sub\", \"x2_gt\"]].min(axis=1)\n        df[\"iymin\"] = df[[\"y1_sub\", \"y1_gt\"]].max(axis=1)\n        df[\"iymax\"] = df[[\"y2_sub\", \"y2_gt\"]].min(axis=1)\n\n        df[\"iw\"] = np.maximum(df[\"ixmax\"] - df[\"ixmin\"] + 1, 0.0)\n        df[\"ih\"] = np.maximum(df[\"iymax\"] - df[\"iymin\"] + 1, 0.0)\n\n        # 2. calculate the area of inters\n        df[\"inters\"] = df[\"iw\"] * df[\"ih\"]\n\n        # 3. calculate the area of union\n        df[\"uni\"] = (\n            (df[\"x2_sub\"] - df[\"x1_sub\"] + 1) * (df[\"y2_sub\"] - df[\"y1_sub\"] + 1)\n            + (df[\"x2_gt\"] - df[\"x1_gt\"] + 1) * (df[\"y2_gt\"] - df[\"y1_gt\"] + 1)\n            - df[\"inters\"]\n        )\n        # print(uni)\n        # 4. calculate the overlaps between pred_box and gt_box\n        df[\"iou\"] = df[\"inters\"] / df[\"uni\"]\n\n        return df.drop(\n            [\"ixmin\", \"ixmax\", \"iymin\", \"iymax\", \"iw\", \"ih\", \"inters\", \"uni\"], axis=1\n        )\n\n    def filter_to_top_label_match(self, sub_labels):\n        \"\"\"\n        Ensures ground truth boxes are only linked to the box\n        in the submission file with the highest IoU.\n        \"\"\"\n        return (\n            sub_labels.sort_values(\"iou\", ascending=False)\n            .groupby([\"video_frame\", \"label_gt\"])\n            .first()\n            .reset_index()\n        )\n\n    def add_isCorrect_col(self, sub_labels):\n        \"\"\"\n        Adds True/False column if the ground truth label\n        and submission label are identical\n        \"\"\"\n        sub_labels[\"isCorrect\"] = (\n            sub_labels[\"label_gt\"] == sub_labels[\"label_sub\"]\n        ) & (sub_labels[\"iou\"] >= self.iou_threshold)\n        return sub_labels\n\n    def calculate_metric_weighted(\n        self, sub_labels, weight_col=\"isDefinitiveImpact\", weight=1000\n    ):\n        \"\"\"\n        Calculates weighted accuracy score metric.\n        \"\"\"\n        sub_labels[\"weight\"] = sub_labels.apply(\n            lambda x: weight if x[weight_col] else 1, axis=1\n        )\n        y_pred = sub_labels[\"isCorrect\"].values\n        y_true = np.ones_like(y_pred)\n        weight = sub_labels[\"weight\"]\n        return accuracy_score(y_true, y_pred, sample_weight=weight)\n\n    def score(self, sub, labels_df=None, drop_extra_cols=True):\n        \"\"\"\n        Scores the submission file against the labels.\n\n        Returns the evaluation metric score for the helmet\n        assignment kaggle competition.\n\n        If `check_constraints` is set to True, will return -999 if the\n            submission fails one of the submission constraints.\n        \"\"\"\n        if labels_df is None:\n            labels_df = self.labels_df.copy()\n\n        if self.check_constraints:\n            if not self.check_submission(sub):\n                return -999\n        sub_labels = self.merge_sub_labels(sub, labels_df, self.weight_col)\n        sub_labels = self.get_iou_df(sub_labels).copy()\n        sub_labels = self.filter_to_top_label_match(sub_labels).copy()\n        sub_labels = self.add_isCorrect_col(sub_labels)\n        score = self.calculate_metric_weighted(\n            sub_labels, self.weight_col, self.impact_weight\n        )\n        # Keep `sub_labels for review`\n        if drop_extra_cols:\n            drop_cols = [\n                \"x1_sub\",\n                \"x2_sub\",\n                \"y1_sub\",\n                \"y2_sub\",\n                \"x1_gt\",\n                \"x2_gt\",\n                \"y1_gt\",\n                \"y2_gt\",\n            ]\n            sub_labels = sub_labels.drop(drop_cols, axis=1)\n        self.sub_labels = sub_labels\n        return score\n\nif debug:\n    scorer = NFLAssignmentScorer(labels)\n    baseline_score = scorer.score(submission_df)\n    print(f\"validation score {baseline_score:0.4f}\") # this would be 0.33","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T19:25:28.29484Z","iopub.execute_input":"2021-08-16T19:25:28.295204Z","iopub.status.idle":"2021-08-16T19:25:29.918577Z","shell.execute_reply.started":"2021-08-16T19:25:28.295172Z","shell.execute_reply":"2021-08-16T19:25:29.917439Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Video with Predictions","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport subprocess\nfrom IPython.display import Video, display\nimport pandas as pd\n\nimport os\nimport cv2\nimport subprocess\nfrom IPython.display import Video, display\nimport pandas as pd\nimport numpy as np\n\ndef video_with_predictions(\n    video_path: str, sub_labels: pd.DataFrame, max_frame=9999, freeze_impacts=True,\n    verbose=True\n) -> str:\n    \"\"\"\n    Annotates a video with both the baseline model boxes and ground truth boxes.\n    \"\"\"\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)  # Black\n    \n    INCORRECT_IMPACT_COLOR = (0, 0, 255)  # Red\n    CORRECT_IMPACT_COLOR = (51, 255, 255)  # Yellow\n\n    CORRECT_COLOR = (0, 255, 0)  # Green\n    INCORRECT_COLOR = (255, 255, 255)  # White\n    WHITE = (255, 255, 255)  # White \n\n    video_name = os.path.basename(video_path).replace(\".mp4\", \"\")\n    if verbose:\n        print(f\"Running for {video_name}\")\n    sub_labels = sub_labels.copy()\n    # Add frame and video columns:\n    sub_labels['video'] = sub_labels['video_frame'].str.split('_').str[:3].str.join('_')\n    sub_labels['frame'] = sub_labels['video_frame'].str.split('_').str[-1].astype('int')\n\n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"pred_\" + video_name + \".mp4\"\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(\n        tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height)\n    )\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n\n        img_name = f\"{frame} : {video_name}\"\n        cv2.putText(\n            img,\n            img_name,\n            (5, 20),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n        \n        cv2.putText(\n            img,\n            str(frame),\n            (1230, 710),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.8,\n            WHITE,\n            thickness=1,\n        )\n        # Get stats about current state in frame\n        stats = sub_labels.query('video == @video and frame <= @frame')\n        correct_nonimp = len(stats.query('weight == 1 and isCorrect'))\n        total_nonimp = len(stats.query('weight == 1'))\n        correct_imp = len(stats.query('weight > 1 and isCorrect'))\n        total_imp = len(stats.query('weight > 1'))\n        correct_weighted = correct_nonimp + (correct_imp * 1000)\n        total_weighted = total_nonimp + (total_imp * 1000)\n        acc_imp = correct_imp/np.max([1, total_imp])\n        acc_nonimp = correct_nonimp/np.max([1, total_nonimp])\n        acc_weighted = correct_weighted/np.max([1, total_weighted])\n        cv2.putText(\n            img,\n            f'{acc_imp:0.4f} Impact Boxes Accuracy :      ({correct_imp}/{total_imp})',\n            (5, 40),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n\n        cv2.putText(\n            img,\n            f'{acc_nonimp:0.4f} Non-Impact Boxes Accuracy: ({correct_nonimp}/{total_nonimp})',\n            (5, 60),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n        \n        cv2.putText(\n            img,\n            f'{acc_weighted:0.4f} Weighted Accuracy:     ({correct_weighted}/{total_weighted})',\n            (5, 80),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            WHITE,\n            thickness=1,\n        )\n\n        \n        video_frame = f'{video_name}_{frame}' \n        boxes = sub_labels.query(\"video_frame == @video_frame\")\n        if len(boxes) == 0:\n            return\n        for box in boxes.itertuples(index=False):\n            if box.isCorrect and box.weight == 1:\n                # CORRECT\n                box_color = CORRECT_COLOR\n                gt_color = CORRECT_COLOR\n                pred_thickness = 1\n            elif box.isCorrect and box.weight > 1:\n                box_color = CORRECT_IMPACT_COLOR\n                gt_color = CORRECT_IMPACT_COLOR\n                pred_thickness = 3\n            elif (box.isCorrect == False) and (box.weight > 1):\n                box_color = INCORRECT_IMPACT_COLOR\n                gt_color = INCORRECT_IMPACT_COLOR\n                pred_thickness = 3\n            elif (box.isCorrect == False) and (box.weight == 1):                \n                box_color = INCORRECT_COLOR\n                gt_color = HELMET_COLOR\n                pred_thickness = 1\n\n            # Ground Truth Box\n            cv2.rectangle(\n                img,\n                (box.left_gt, box.top_gt),\n                (box.left_gt + box.width_gt, box.top_gt + box.height_gt),\n                gt_color,\n                thickness=1,\n            )\n            # Prediction Box\n            cv2.rectangle(\n                img,\n                (int(box.left_sub), int(box.top_sub)),\n                (int(box.left_sub + box.width_sub), int(box.top_sub + box.height_sub)),\n                box_color,\n                thickness=pred_thickness,\n            )\n\n            cv2.putText(\n                img,\n                f\"{box.label_gt}:{box.label_sub}\",\n                (max(0, box.left_gt - box.width_gt), max(0, box.top_gt - 5)),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                WHITE,\n                thickness=1,\n            )\n\n        if boxes['weight'].sum() > 22 and freeze_impacts:\n            for _ in range(60):\n                # Freeze for 60 frames on impacts\n                output_video.write(img)\n        else:\n            output_video.write(img)\n        \n        if frame >= max_frame:\n            break\n        \n    output_video.release()\n    # Not all browsers support the codec, we will re-load the file at tmp_output_path\n    # and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run(\n        [\n            \"ffmpeg\",\n            \"-i\",\n            tmp_output_path,\n            \"-crf\",\n            \"18\",\n            \"-preset\",\n            \"veryfast\",\n            \"-vcodec\",\n            \"libx264\",\n            output_path,\n        ]\n    )\n    os.remove(tmp_output_path)\n\n    return output_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process prediction videos","metadata":{}},{"cell_type":"code","source":"# Add video column to `sub_labels`\nscorer.sub_labels['video'] = scorer.sub_labels['video_frame'] \\\n    .str.split('_').str[:3].str.join('_')\n\nvideo_dir = '../input/nfl-health-and-safety-helmet-assignment/train'\n\nout_videos = []\nvideos = scorer.sub_labels['video'].unique()\nfor video in tqdm(videos):\n    video_out = video_with_predictions(f'{video_dir}/{video}.mp4',\n                           scorer.sub_labels,\n                          )\n    out_videos.append(video_out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display example videos","metadata":{}},{"cell_type":"code","source":"frac = 0.65 # scaling factor for display\ndisplay(Video(data=out_videos[2],\n              embed=True,\n              height=int(720*frac),\n              width=int(1280*frac))\n       )","metadata":{"execution":{"iopub.status.busy":"2021-08-16T20:58:42.587684Z","iopub.execute_input":"2021-08-16T20:58:42.588021Z","iopub.status.idle":"2021-08-16T20:58:43.053359Z","shell.execute_reply.started":"2021-08-16T20:58:42.587982Z","shell.execute_reply":"2021-08-16T20:58:43.051305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Video(data=out_videos[1],\n              embed=True,\n              height=int(720*frac),\n              width=int(1280*frac))\n       )","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:05:14.549967Z","iopub.execute_input":"2021-08-16T21:05:14.550349Z","iopub.status.idle":"2021-08-16T21:05:14.922714Z","shell.execute_reply.started":"2021-08-16T21:05:14.550315Z","shell.execute_reply":"2021-08-16T21:05:14.921912Z"},"trusted":true},"execution_count":null,"outputs":[]}]}