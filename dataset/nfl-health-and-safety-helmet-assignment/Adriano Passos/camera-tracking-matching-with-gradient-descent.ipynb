{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Results\n\nUsing the folowing approach I manageg to score 0.129 on public LB (it ain't much but it is honest work.. lol)\n\nHere is the link for that notebook: https://www.kaggle.com/coldfir3/image-tracking-matching-baseline\n\nMy hope by sharing this is that with some heavy fine-tuning of the algorithm we could achieve something better. I am looking forward your feedback. Also, if you like this please don't forget to upvote =)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom PIL import Image, ImageDraw\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-23T14:46:42.444486Z","iopub.execute_input":"2021-08-23T14:46:42.444867Z","iopub.status.idle":"2021-08-23T14:46:42.450281Z","shell.execute_reply.started":"2021-08-23T14:46:42.444835Z","shell.execute_reply":"2021-08-23T14:46:42.449113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_frame_from_video(frame, video):\n    frame = frame - 1\n    !ffmpeg \\\n        -hide_banner \\\n        -loglevel fatal \\\n        -nostats \\\n        -i $video -vf \"select=eq(n\\,$frame)\" -vframes 1 frame.png\n    img = Image.open('frame.png')\n    os.remove('frame.png')\n    return img\n\ndef annotate_frame(img, xc, yc, r, col = (57, 255, 20)):\n    draw = ImageDraw.Draw(img)\n    for x, y in zip(xc, yc):\n        draw.ellipse((x-r, y-r, x+r, y+r), fill=col, outline='black')\n    return img\n\n# code from: https://www.kaggle.com/robikscube/nfl-helmet-assignment-getting-started-guide\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) / 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\ndef add_video_features(videos):\n    videos['game_play'] = videos['video_frame'].apply(lambda x: '_'.join(x.split('_')[:2]))\n    videos['camera'] = videos['video_frame'].apply(lambda x: x.split('_')[2])\n    videos['frame'] = videos['video_frame'].apply(lambda x: x.split('_')[-1])\n    videos['xc'] = (videos['left'] + videos['width']/2).astype(int).values\n    videos['yc'] = (videos['top'] + videos['height']/2).astype(int).values\n    return videos\n\n\ndef annotate_field(xc, yc, player, r = 10, width = 3, col = [(27, 3, 163), (255, 7, 58)], crop = None):\n    field = Image.open('../input/nflhelmet-helper-dataset/field.png')\n    w, h = field.size\n    zero = (68,68)\n    fs = (2424,1100)\n    draw = ImageDraw.Draw(field)\n    xc, yc = xc*fs[0]/120 + zero[0], (1 - yc/53.3)*fs[1] + zero[1]\n    for x, y, p in zip(xc, yc, player):\n        c = col[0] if p[0] == 'H' else col[1]\n        draw.ellipse((x-r, y-r, x+r, y+r), fill=c, width=width, outline = 'black')\n    if isinstance(crop, float):\n#         cp = [xc.min() - crop*w, yc.min() - crop*h, xc.max() + crop*w, yc.max() + crop*h]\n        cp = [xc.min() - crop*w, 0, xc.max() + crop*2*w, h]\n        return field.crop(cp)\n    else:\n        return field\n    \n    \nclass show_play_with_tracking():\n    \n    def __init__(self, video_df = None, track_df = None):\n        if video_df is None:\n            video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n        self.video_df = add_video_features(video_df)\n        if track_df is None:\n            tracking_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_player_tracking.csv')\n            tracking_df = add_track_features(tracking_df)\n        self.tracking_df = tracking_df.query(\"est_frame > 0\")\n       \n    def __call__(self, game_play, frame, img_size = 800, video_folder = '../input/nfl-health-and-safety-helmet-assignment/train/'):\n        \n        camera = 'Sideline'\n        frame_side = get_frame_from_video(frame, video_folder + game_play + '_' + camera + '.mp4')\n        df = self.video_df.query(f\"game_play == '{game_play}' and frame == '{frame}' and camera == '{camera}'\")\n        frame_side = annotate_frame(frame_side, df.xc, df.yc, 10)\n\n        camera = 'Endzone'\n        frame_end = get_frame_from_video(frame, video_folder + game_play + '_' + camera + '.mp4')\n        df = self.video_df.query(f\"game_play == '{game_play}' and frame == '{frame}' and camera == '{camera}'\")\n        frame_end = annotate_frame(frame_end, df.xc, df.yc, 10)\n\n        frames = self.tracking_df['est_frame'].values\n        if frame not in frames:\n            index = np.absolute(frames-frame).argmin()\n            frame = frames[index]\n        df = self.tracking_df.query(f\"game_play == '{game_play}' and est_frame == {frame}\")\n        field = annotate_field(df.x, df.y, df.player, 20, crop = 0.01)\n\n\n        wf, hf = field.size\n        wc, hc = frame_side.size\n        field = field.resize((int(wf*2*hc/hf), 2*hc))\n        wf, hf = field.size\n\n        img = Image.new('RGB', (wf+wc+20, 2*hc+20))\n        img.paste(im=field, box=(5, 10))\n        img.paste(im=frame_side, box=(wf+15, 5))\n        img.paste(im=frame_end, box=(wf+15, hc+15))\n        img.thumbnail((img_size,img_size))\n        return img\n    \nspwt = show_play_with_tracking()\n\n# TODO, add interpolation of tracking_df and replace nearest\nclass get_keypoints():\n    \n    def __init__(self, video_df = None, track_df = None):\n        if video_df is None:\n            video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\n        self.video_df = add_video_features(video_df)\n        if track_df is None:\n            tracking_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_player_tracking.csv')\n            tracking_df = add_track_features(tracking_df)\n        self.tracking_df = tracking_df.query(\"est_frame > 0\")\n            \n    def __call__(self, game_play, frame, normalized = True):\n        keypoints = dict()\n        keypoints['Sideline'] = self.video_df.query(\n            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Sideline'\")[['xc', 'yc']].values\n        keypoints['Endzone'] = self.video_df.query(\n            f\"game_play == '{game_play}' and frame == '{frame}' and camera == 'Endzone'\")[['xc', 'yc']].values\n        \n        frames = self.tracking_df['est_frame'].values\n        if frame not in frames:\n            index = np.absolute(frames-frame).argmin()\n            frame = frames[index]\n        keypoints['Tracking'] = self.tracking_df.query(\n            f\"game_play == '{game_play}' and est_frame == {frame}\")[['x', 'y']].values\n    \n        if normalized:\n            for k, v in keypoints.items():\n                keypoints[k] = (v - v.min(axis = 0)) / (v.max(axis = 0) - v.min(axis = 0))\n                \n        keypoints['Sideline'][:,1] = 1-keypoints['Sideline'][:,1]\n                \n        self.keypoints = keypoints\n            \n        return keypoints\n    \n    def plot(self, add_no = False):\n        if not hasattr(self, 'keypoints'):\n            print('you must run the function first...')\n        else:\n            kp = self.keypoints\n            plt.figure(figsize=(6, 6))\n            plt.scatter(kp['Endzone'][:,0], kp['Endzone'][:,1], marker = 'x', color = 'red', label = 'Endzone')\n            plt.scatter(kp['Sideline'][:,0], kp['Sideline'][:,1], marker = '^', color = 'red', label = 'Sideline')\n            plt.scatter(kp['Tracking'][:,0], kp['Tracking'][:,1], marker = 'o', color = 'green', label = 'Tracking')  \n            plt.legend();\n    \nget_kp = get_keypoints()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-23T14:57:12.448333Z","iopub.execute_input":"2021-08-23T14:57:12.448699Z","iopub.status.idle":"2021-08-23T14:57:23.199435Z","shell.execute_reply.started":"2021-08-23T14:57:12.448671Z","shell.execute_reply":"2021-08-23T14:57:23.198227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Intro & Problem description\n\nWe have Tracking data with GT player labels that are available for both **train** and test **sets**. We also have good baseline predictions of the helmet locations on the camera views (video frames). So, one way to solve this challange would be map the 2D tracking data information to the camera views.","metadata":{}},{"cell_type":"code","source":"spwt('57583_000082', 10)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:34:57.432613Z","iopub.execute_input":"2021-08-23T14:34:57.433014Z","iopub.status.idle":"2021-08-23T14:35:00.841549Z","shell.execute_reply.started":"2021-08-23T14:34:57.432954Z","shell.execute_reply":"2021-08-23T14:35:00.835281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What I wanted to do is to map each player from tracking data to the camera data like this:\n\n<img src=\"https://media.discordapp.net/attachments/874736660103962726/878082285667237898/unknown.png?width=911&height=702\" width=800 height=800 />\n","metadata":{}},{"cell_type":"markdown","source":"The same data can be normalized and viewed on the same plot:","metadata":{}},{"cell_type":"code","source":"k = get_kp('57583_000082', 12)\nget_kp.plot(True)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:35:00.843334Z","iopub.execute_input":"2021-08-23T14:35:00.843852Z","iopub.status.idle":"2021-08-23T14:35:01.828499Z","shell.execute_reply.started":"2021-08-23T14:35:00.843814Z","shell.execute_reply":"2021-08-23T14:35:01.82732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Point Cloud Registration","metadata":{}},{"cell_type":"markdown","source":"So, a good portion of this comp comes down to point-cloud registration. There are plenty of methods that perform this taks. For example, OpenCV have `cv.findHomography`. As you can see below, this doesn't perform well. The main reason is that although the function uses Random sample consensus (RANSAC) to filter outliers, the clouds of points must be roughly alinged already.","metadata":{}},{"cell_type":"code","source":"srcPoints = k['Tracking'].astype('float32').reshape(-1,1,2)\ndstPoints = k['Sideline'].astype('float32').reshape(-1,1,2)\nM, mask = cv.findHomography(srcPoints, dstPoints, cv.RANSAC)\nprint(mask.sum())\ntfmdPoints = cv.perspectiveTransform(srcPoints,M)\nplt.scatter(srcPoints[:,0,0], srcPoints[:,0,1], marker = 'o', color = 'red', label = 'source')\nplt.scatter(dstPoints[:,0,0], dstPoints[:,0,1], marker = '^', color = 'green', label = 'target')  \nplt.scatter(tfmdPoints[:,0,0], tfmdPoints[:,0,1], marker = 'o', color = 'blue', label = 'result')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:44:29.936597Z","iopub.execute_input":"2021-08-23T14:44:29.937063Z","iopub.status.idle":"2021-08-23T14:44:30.225558Z","shell.execute_reply.started":"2021-08-23T14:44:29.937029Z","shell.execute_reply":"2021-08-23T14:44:30.224442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The alingment I mean here is not spatial, it order-wise. In other words, for the algorithm to work well you need to pass the two cloud of points in the same order (point 1 of cloud 1 must match the point 2 of cloud 2). Here is an example:","metadata":{}},{"cell_type":"code","source":"srcPoints = k['Tracking'][np.array([4, 16, 12, 14, 20, 7, 10, 3])].astype('float32').reshape(-1,1,2)\ndstPoints = k['Sideline'][np.array([13, 18, 4, 17, 14, 8, 16, 20])].astype('float32').reshape(-1,1,2)\nallSrcPoints = k['Tracking'].astype('float32').reshape(-1,1,2)\nallDstPoints = k['Sideline'].astype('float32').reshape(-1,1,2)\nM, mask = cv.findHomography(srcPoints, dstPoints, cv.RANSAC)\nprint(mask.sum())\ntfmdPoints = cv.perspectiveTransform(allSrcPoints,M)\nplt.scatter(allSrcPoints[:,0,0], allSrcPoints[:,0,1], marker = 'o', color = 'red', label = 'source')\nplt.scatter(allDstPoints[:,0,0], allDstPoints[:,0,1], marker = '^', color = 'green', label = 'target')  \nplt.scatter(tfmdPoints[:,0,0], tfmdPoints[:,0,1], marker = 'o', color = 'blue', label = 'result')\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:44:57.238687Z","iopub.execute_input":"2021-08-23T14:44:57.239102Z","iopub.status.idle":"2021-08-23T14:44:57.491078Z","shell.execute_reply.started":"2021-08-23T14:44:57.239069Z","shell.execute_reply":"2021-08-23T14:44:57.490085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But hell, this is exactly the task we are trying to achieve... we are not doing registration for the sake of it. Getting the matches between the two clouds of points is the end-goal here. So for me, this approach was a dead-end","metadata":{}},{"cell_type":"markdown","source":"# Pure Pytorch implementation (using gradient descent)\n\nSo, isntead of using open CV, I decided to do this my won using pytorch. The idea here is to solve the folowing problem:\n\n\\begin{equation}\n\\begin{bmatrix} x^{'} \\\\ y^{'} \\\\ 1 \\end{bmatrix} = H \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n\\end{equation}\n\nwhere $H$ is a transformation matrix (that I want to find) and $(x', y')$ is the transformed coorinates of a point $(x, y)$.\n\nThe optimization problem is to find $H$ such as $(x', y')$ is as close as possible to some ground truth $(x_t, y_t)$ cloud of points.","metadata":{}},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:45:18.148897Z","iopub.execute_input":"2021-08-23T14:45:18.149319Z","iopub.status.idle":"2021-08-23T14:45:19.442102Z","shell.execute_reply.started":"2021-08-23T14:45:18.149277Z","shell.execute_reply":"2021-08-23T14:45:19.440736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this I made a loss function that computes the mean square distance from all target points with the closest transformed point. I called this the `min_mse`","metadata":{}},{"cell_type":"code","source":"def min_mse(preds, targets):\n    d = torch.cdist(preds.squeeze(2), targets.squeeze(2))\n    loss = (d.min(dim = 1).values**2).mean().sqrt()\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:45:19.443562Z","iopub.execute_input":"2021-08-23T14:45:19.443854Z","iopub.status.idle":"2021-08-23T14:45:19.450708Z","shell.execute_reply.started":"2021-08-23T14:45:19.443825Z","shell.execute_reply":"2021-08-23T14:45:19.449495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each step of the optimization loop is composed by:\n1. Getting the model predictions (i.e. applying the homograpy transform on `src`)\n1. Calculating the loss between the transformed set of points and the GT cloud\n1. Calculating the gradients and updating the $H$ matrix.","metadata":{}},{"cell_type":"code","source":"def step(src, trg, m, lr = 3e-3, prt = True):\n    preds = torch.matmul(m, src) # Homography transform\n    loss = min_mse(preds, trg)   # mse between the closes pair of points\n    if prt: print(f'loss: {(loss.item()):.5f}')\n    loss.backward()\n    m.data -= lr * m.grad.data\n    m.grad = None","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:45:20.359861Z","iopub.execute_input":"2021-08-23T14:45:20.36024Z","iopub.status.idle":"2021-08-23T14:45:20.366904Z","shell.execute_reply.started":"2021-08-23T14:45:20.360208Z","shell.execute_reply":"2021-08-23T14:45:20.365748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function below simply is a training loop that initializes $H$, iterate for a few thousand epochs and plot the final results. By-the-way, a proper initialization of $H$ is a crucial step for this approach.","metadata":{}},{"cell_type":"code","source":"def fit_predict(src, trg, init_rot = 0, lr = 3e-3, n_steps = 10000, verbose = True):\n    t = np.pi * init_rot / 180\n    m = torch.tensor([[np.cos(t),-np.sin(t), 0],\n                      [np.sin(t), np.cos(t), 0],\n                      [        0,         0, 1]], dtype = torch.double)\n    m.requires_grad_()\n    for i in range(n_steps): \n        if i % (n_steps//10) and verbose:\n            step(src, trg, m, lr=lr, prt=False)\n        else:\n            step(src, trg, m, lr=lr)\n            \n    with torch.no_grad():\n        tfm = torch.matmul(m, src)\n        \n    if verbose:\n        plt.scatter(src[:,0], src[:,1], marker = 'o', color = 'red', label = 'source')\n        plt.scatter(trg[:,0], trg[:,1], marker = '^', color = 'green', label = 'target')  \n        plt.scatter(tfm[:,0], tfm[:,1], marker = 'o', color = 'blue', label = 'result')\n        plt.legend();\n        \n    return tfm","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:50:01.679717Z","iopub.execute_input":"2021-08-23T14:50:01.680265Z","iopub.status.idle":"2021-08-23T14:50:01.690666Z","shell.execute_reply.started":"2021-08-23T14:50:01.680234Z","shell.execute_reply":"2021-08-23T14:50:01.689464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Working examples\n\nHere are a couple of realy good results I had with this method","metadata":{}},{"cell_type":"code","source":"k = get_kp('57583_000082', 1, True)\nsrc = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntrg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntfm = fit_predict(src, trg)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:50:02.078097Z","iopub.execute_input":"2021-08-23T14:50:02.078525Z","iopub.status.idle":"2021-08-23T14:50:06.301004Z","shell.execute_reply.started":"2021-08-23T14:50:02.078491Z","shell.execute_reply":"2021-08-23T14:50:06.299946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So for the same clouds of points I tried with open-cv algorithm I managed to got a much better result. The same is true for the game play `57597_000658` at snap","metadata":{}},{"cell_type":"code","source":"k = get_kp('57597_000658', 1, True)\nsrc = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntrg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntfm = fit_predict(src, trg)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:50:36.823256Z","iopub.execute_input":"2021-08-23T14:50:36.823646Z","iopub.status.idle":"2021-08-23T14:50:40.963139Z","shell.execute_reply.started":"2021-08-23T14:50:36.823605Z","shell.execute_reply":"2021-08-23T14:50:40.961959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Very problematic examples\n\nBut hold your horses! apparently my code is not robust (at all). Check some bad examples here!","metadata":{}},{"cell_type":"code","source":"k = get_kp('57781_000252', 1, True)\nsrc = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntrg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntfm = fit_predict(src, trg, 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:51:28.753886Z","iopub.execute_input":"2021-08-23T14:51:28.754335Z","iopub.status.idle":"2021-08-23T14:51:34.393997Z","shell.execute_reply.started":"2021-08-23T14:51:28.754302Z","shell.execute_reply":"2021-08-23T14:51:34.393114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Woah! what a hot-mess!!! The culprit... Lots of helmets with low confidence score...","metadata":{}},{"cell_type":"code","source":"spwt('57781_000252', 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:52:19.90035Z","iopub.execute_input":"2021-08-23T14:52:19.900687Z","iopub.status.idle":"2021-08-23T14:52:22.645402Z","shell.execute_reply.started":"2021-08-23T14:52:19.90066Z","shell.execute_reply":"2021-08-23T14:52:22.64126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See how we have a whole bunch of sideline helmets... Let's filter them by confidence score and see if that helps anything","metadata":{}},{"cell_type":"code","source":"video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\nvideo_df = add_video_features(video_df)\nvideo_df.query(f\"game_play == '57781_000252' and frame == '1' and camera == 'Sideline'\").head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:56:34.065721Z","iopub.execute_input":"2021-08-23T14:56:34.066449Z","iopub.status.idle":"2021-08-23T14:56:37.591348Z","shell.execute_reply.started":"2021-08-23T14:56:34.066393Z","shell.execute_reply":"2021-08-23T14:56:37.59012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can simply fix that by passing a filtered dataframe to the constructor of `get_keypoints`","metadata":{}},{"cell_type":"code","source":"video_df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_baseline_helmets.csv')\nvideo_df = video_df.query('conf > 0.8')\nvideo_df = add_video_features(video_df)\nget_kp_highconf = get_keypoints(video_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:57:23.201033Z","iopub.execute_input":"2021-08-23T14:57:23.201319Z","iopub.status.idle":"2021-08-23T14:57:28.851993Z","shell.execute_reply.started":"2021-08-23T14:57:23.201294Z","shell.execute_reply":"2021-08-23T14:57:28.850626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = get_kp_highconf('57781_000252', 1, True)\nsrc = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntrg = torch.cat([torch.tensor(k['Sideline']), torch.ones(len(k['Sideline'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntfm = fit_predict(src, trg, 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T14:57:28.854139Z","iopub.execute_input":"2021-08-23T14:57:28.854607Z","iopub.status.idle":"2021-08-23T14:57:33.133551Z","shell.execute_reply.started":"2021-08-23T14:57:28.854554Z","shell.execute_reply":"2021-08-23T14:57:33.13243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Welp, that still sucks... sorry...\n\nBut wait, there is more...\n\nThigs get a bit worse when we try to fit the endzone data...","metadata":{}},{"cell_type":"code","source":"k = get_kp('57583_000082', 1, True)\nsrc = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntrg = torch.cat([torch.tensor(k['Endzone']), torch.ones(len(k['Endzone'])).unsqueeze(1)], axis = -1).unsqueeze(2)\nprint(src.shape, trg.shape)\ntfm = fit_predict(src, trg)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T15:00:17.647909Z","iopub.execute_input":"2021-08-23T15:00:17.648289Z","iopub.status.idle":"2021-08-23T15:00:21.757414Z","shell.execute_reply.started":"2021-08-23T15:00:17.648258Z","shell.execute_reply":"2021-08-23T15:00:21.756314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even if we initialize the cloud rotated at 90ยบ this doesn't fully fix the issue","metadata":{}},{"cell_type":"code","source":"k = get_kp('57583_000082', 1, True)\nsrc = torch.cat([torch.tensor(k['Tracking']), torch.ones(len(k['Tracking'])).unsqueeze(1)], axis = -1).unsqueeze(2)\ntrg = torch.cat([torch.tensor(k['Endzone']), torch.ones(len(k['Endzone'])).unsqueeze(1)], axis = -1).unsqueeze(2)\nprint(src.shape, trg.shape)\ntfm = fit_predict(src, trg, 90)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T15:01:04.736602Z","iopub.execute_input":"2021-08-23T15:01:04.737001Z","iopub.status.idle":"2021-08-23T15:01:08.915693Z","shell.execute_reply.started":"2021-08-23T15:01:04.736951Z","shell.execute_reply":"2021-08-23T15:01:08.914599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# That's all for today\n\nI realy hope this notebook provided some insight on Point Cloud registration and I am excited to hear your feedback!","metadata":{}}]}