{"cells":[{"metadata":{},"cell_type":"markdown","source":"![https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQwhyigJNf_xEu0-htQNZ9w5RJIeHO34WFmYQ&usqp=CAU](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQwhyigJNf_xEu0-htQNZ9w5RJIeHO34WFmYQ&usqp=CAU)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Cornell Bird Sounds Recognition**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# About Competitions:\n\nOver 10,000 bird species occur in the world,and they can be found in nearly every environment,from untouched rainforests to suburbs and even cities.Birds play an essential role in nature. They are high up in the food chain and integrate changes occuring at lower levels. Birds are excellent indicators of deteriorating habitat quality and environmentalpollution.\n\n\nThe Cornell Lab of Ornithology's Center for Conservation Bioacoustics (CCB)'s mission is to collect and interpret sounds in nature.The CCB develops innoovative conservation technologies to inspire and inform the conservation technologies to inspire and inform the conservation of wildlife and habitats globally. By parnering with the data Sience community, the CCB hopes to further its mission and improve the accuracy of soundscape analyses.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Objective\n\nIt is often easier to hear birds see them. With proper sound detection and classification,researchers ould automatically intuit facctors about an area's quality of life based on a changing bird population.The oobjetive of competition is to identify a wide variety of bird vocalizations in soundscape  recordings.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n# Map 1 library\nimport plotly.express as px\n\n# Map 2 libraries\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n# Librosa Libraries\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path=\"/kaggle/input/birdsong-recognition/\"\naudio=\"/kaggle/input/birdsong-recognition/train_audio/\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Expolring the Training Data**\nLet's explore the training data file to gather some information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(path+'train.csv')\ndisplay('shape of train',train.shape)\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv(path+'test.csv')\ndisplay('shape of test Dataset',test.shape)\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The file contain a lot of columns but we shall focus on the some of the ones which are directly related to our problems.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create some time features\ntrain['year']=train['date'].apply(lambda x:x.split('-')[0])\ntrain['month']=train['date'].apply(lambda x:x.split('-')[1])\ntrain['day_of_month']=train['date'].apply(lambda x:x.split('-')[2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of unique species in dataset\nlen(train['species'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ebird code of bird species is unique or not\ndisplay(train['ebird_code'].is_unique)\n#ebird code of bird species\ndisplay(list(train['ebird_code'].value_counts().head(15).reset_index()['index'])\n)\ndisplay(len(train['ebird_code'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recording Time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nax=sns.countplot(train['year'],palette=\"hls\")\nplt.title(\"Year of the Audio Files Registration\",fontsize=20)\nplt.xticks(rotation=90,fontsize=13)\nplt.yticks(fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=14)\nplt.xlabel(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nax=sns.countplot(train['month'],palette=\"hls\")\nplt.title(\"Month of the Audio Files Registration\",fontsize=20)\nplt.xticks(rotation=90,fontsize=13)\nplt.yticks(fontsize=15)\nplt.ylabel(\"Frequency\",fontsize=14)\nplt.xlabel(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#total number of people who provided the reordings \ntrain['recordist'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 recordist in terms of the number of recording done\ntrain['recordist'].value_counts()[:10].sort_values().iplot(kind='bar',color=\"#2750BF\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check whether playback used or not\ntrain['playback_used'].fillna('Not Defined',inplace=True)\ntrain['playback_used'].value_counts().sort_values().iplot(kind='bar',color=\"#2750BF\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Songs\nPitch is usually unspecified.This is one of the miscellaneous columns, that we need to be careful how we interpret. Most Song types are call, song or flight.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check whether pitch is unique or not\ndisplay(train['pitch'].is_unique)\n#length of pitch in the dataset\ndisplay(len(train['pitch'].value_counts()))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nax = sns.countplot(train['pitch'], palette=\"hls\", order = train['pitch'].value_counts().index)\n\n\nplt.title(\"Pitch (quality of sound - how high/low was the tone)\", fontsize=16)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check whether type of call is unique or not\ndisplay(len(train['type'].value_counts()))\ntrain['type'].value_counts().iplot(kind='bar',color=\"#2750BF\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows that type of voice is not unique.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create a new variable type by exploding all the values\nadjusted_type = train['type'].apply(lambda x: x.split(',')).reset_index().explode(\"type\")\n\n# Strip of white spaces and convert to lower chars\nadjusted_type = adjusted_type['type'].apply(lambda x: x.strip().lower()).reset_index()\nadjusted_type['type'] = adjusted_type['type'].replace('calls', 'call')\n\n# Create Top 15 list with song types\ntop_15 = list(adjusted_type['type'].value_counts().head(15).reset_index()['index'])\ndata = adjusted_type[adjusted_type['type'].isin(top_15)]\nplt.figure(figsize=(16, 6))\nax = sns.countplot(data['type'], palette=\"hls\", order = data['type'].value_counts().index)\n\n\nplt.title(\"Top 15 Song Types\", fontsize=16)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.xlabel(\"\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nax = sns.countplot(train['rating'], palette=\"hls\", order = train['rating'].value_counts().index)\n\n\nplt.title(\"Rating\", fontsize=16)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create data\ndata = train['bird_seen'].value_counts().reset_index()\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x = 'bird_seen', y = 'index', data = data, palette=\"hls\")\n\n\nplt.title(\"Song was heard, but was Bird Seen?\", fontsize=16)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.xlabel(\"\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# World View of the bird species","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['country'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['country'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top10=list(train['country'].value_counts().head(10).reset_index()['index'])\ntop10\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=train[train['country'].isin(top10)]\nplt.figure(figsize=(16, 6))\nax = sns.countplot(data['country'], palette='hls', order = data['country'].value_counts().index)\n\n\nplt.title(\"Top 10 Countries with most Recordings\", fontsize=16)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.xlabel(\"\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Map View**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import gapminder data, where we have country and iso ALPHA codes\ndf = px.data.gapminder().query(\"year==2007\")[[\"country\", \"iso_alpha\"]]\n\n# Merge the tables together (we lose a fiew rows, but not many)\ndata = pd.merge(left=train, right=df, how=\"inner\", on=\"country\")\n\n# Group by country and count how many species can be found in each\ndata = data.groupby(by=[\"country\", \"iso_alpha\"]).count()[\"species\"].reset_index()\n\nfig = px.choropleth(data, locations=\"iso_alpha\", color=\"species\", hover_name=\"country\",\n                    color_continuous_scale=px.colors.sequential.Teal,\n                    title = \"World Map: Recordings per Country\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total no of unique species in the dataset\ndisplay(len(train['species'].value_counts().index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['species'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top15_speies=list(train['country'].value_counts().head(10).reset_index()['index'])\ntop10\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution of Recorded species over the World","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# SHP file\nworld_map = gpd.read_file(\"../input/world-shapefile/world_shapefile.shp\")\n\n# Coordinate reference system\ncrs = {\"init\" : \"epsg:4326\"}\n\n# Lat and Long need to be of type float, not object\ndata = train[train[\"latitude\"] != \"Not specified\"]\ndata[\"latitude\"] = data[\"latitude\"].astype(float)\ndata[\"longitude\"] = data[\"longitude\"].astype(float)\n\n# Create geometry\ngeometry = [Point(xy) for xy in zip(data[\"longitude\"], data[\"latitude\"])]\n\n# Geo Dataframe\ngeo_df = gpd.GeoDataFrame(data, crs=crs, geometry=geometry)\n\n# Create ID for species\nspecies_id = geo_df[\"species\"].value_counts().reset_index()\nspecies_id.insert(0, 'ID', range(0, 0 + len(species_id)))\n\nspecies_id.columns = [\"ID\", \"species\", \"count\"]\n\n# Add ID to geo_df\ngeo_df = pd.merge(geo_df, species_id, how=\"left\", on=\"species\")\n\n# === PLOT ===\nfig, ax = plt.subplots(figsize = (16, 10))\nworld_map.plot(ax=ax, alpha=0.4, color=\"grey\")\n\npalette = iter(sns.hls_palette(len(species_id)))\n\nfor i in range(264):\n    geo_df[geo_df[\"ID\"] == i].plot(ax=ax, markersize=20, color=next(palette), marker=\"*\", label = \"test\");\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading an audio file ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['full_path']=audio+train['ebird_code']+'/'+train['filename']\n# Now let's sample a few audio files\n# Now let's sample a fiew audio files\nhaiwoo = train[train['ebird_code'] == \"haiwoo\"].sample(1, random_state = 33)['full_path'].values[0]\nwesmea = train[train['ebird_code'] == \"wesmea\"].sample(1, random_state = 33)['full_path'].values[0]\nwewpew = train[train['ebird_code'] == \"wewpew\"].sample(1, random_state = 33)['full_path'].values[0]\nscoori = train[train['ebird_code'] == \"scoori\"].sample(1, random_state = 33)['full_path'].values[0]\nbewwre = train[train['ebird_code'] == \"bewwre\"].sample(1, random_state = 33)['full_path'].values[0]\n\nbird_sample_list = [\"haiwoo\", \"wesmea\", \"wewpew\", \"scoori\", \"bewwre\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bewwre\nipd.Audio(bewwre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wesmea\nipd.Audio(wesmea)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wewpew\nipd.Audio(wewpew)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scoori\nipd.Audio(scoori)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bewwre\nipd.Audio(bewwre)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extracting feature from Sounds\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing 1 file\ny, sr = librosa.load(bewwre)\n\nprint('y:', y, '\\n')\nprint('y shape:', np.shape(y), '\\n')\nprint('Sample Rate (KHz):', sr, '\\n')\n\n# Verify length of the audio\nprint('Check Len of Audio:', 661794/sr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trim leading and trailing silence from an audio signal (silence before and after the actual audio)\naudio_file, _ = librosa.effects.trim(y)\n\n# the result is an numpy ndarray\nprint('Audio File:', audio_file, '\\n')\nprint('Audio File shape:', np.shape(audio_file))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the 5 files\ny_haiwoo,sr_haiwoo=librosa.load(haiwoo)\naudio_haiwoo,_=librosa.effects.trim(y_haiwoo)\ny_wesmea,sr_wesmea=librosa.load(wesmea)\naudio_wesmea,_=librosa.effects.trim(y_wesmea)\ny_wewpew,sr_wewpew=librosa.load(wewpew)\naudio_wewpew,_=librosa.effects.trim(y_wewpew)\ny_scoori,sr_scoori=librosa.load(scoori)\naudio_scoori,_=librosa.effects.trim(y_scoori)\ny_bewwre,sr_bewwre=librosa.load(bewwre)\naudio_bewwre,_=librosa.effects.trim(y_bewwre)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sound Waves**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(5, figsize = (16, 9))\nfig.suptitle('Sound Waves', fontsize=16)\n\nlibrosa.display.waveplot(y = audio_haiwoo, sr =sr_haiwoo, color = \"#A300F9\", ax=ax[0])\nlibrosa.display.waveplot(y = audio_wesmea, sr = sr_wesmea, color = \"#4300FF\", ax=ax[1])\nlibrosa.display.waveplot(y = audio_wewpew, sr = sr_wewpew, color = \"#009DFF\", ax=ax[2])\nlibrosa.display.waveplot(y = audio_scoori, sr = sr_scoori, color = \"#00FFB0\", ax=ax[3])\nlibrosa.display.waveplot(y = audio_bewwre, sr = sr_bewwre, color = \"#D9FF00\", ax=ax[4]);\n\nfor i, name in zip(range(5), bird_sample_list):\n    ax[i].set_ylabel(name, fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fourier Transform**\n\nFunction that gets a signal in the time domain as input, and outputs its decomposition into frequencies.Transform both the y-axis(frequency) to log scale, and the \"color\" axis to Deibels,which is approx.the log scale of amplitudes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Default FFT window size\nn_fft = 2048 # FFT window size\nhop_length = 512 # number audio of frames between STFT columns (looks like a good default)\n\n# Short-time Fourier transform (STFT)\nD_haiwoo = np.abs(librosa.stft(audio_haiwoo, n_fft = n_fft, hop_length = hop_length))\nD_wesmea = np.abs(librosa.stft(audio_wesmea, n_fft = n_fft, hop_length = hop_length))\nD_wewpew = np.abs(librosa.stft(audio_wewpew, n_fft = n_fft, hop_length = hop_length))\nD_scoori = np.abs(librosa.stft(audio_scoori, n_fft = n_fft, hop_length = hop_length))\nD_bewwre = np.abs(librosa.stft(audio_bewwre, n_fft = n_fft, hop_length = hop_length))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D_birds_list = [D_haiwoo, D_wesmea, D_wewpew,D_scoori, D_bewwre]\n\nfor bird,name in zip(D_birds_list,bird_sample_list):\n    print(\" shape is\",name,np.shape(bird))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Spectrogram\n\nA spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time.Spectrograms are sometimes called sonographs,voiceprints, or voicegrams. When the data is represented in 3D plot,they may be called waterfalls.In 2-dimensional arrays,the first axis is frequency while the second axis is time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert an amplitude spectrogram to Decibels-scaled spectrogram.\nDB_haiwoo = librosa.amplitude_to_db(D_haiwoo, ref = np.max)\nDB_wesmea = librosa.amplitude_to_db(D_wesmea, ref = np.max)\nDB_wewpew = librosa.amplitude_to_db(D_wewpew, ref = np.max)\nDB_scoori = librosa.amplitude_to_db(D_scoori, ref = np.max)\nDB_bewwre = librosa.amplitude_to_db(D_bewwre, ref = np.max)\n\n# === PLOT ===\nfig, ax = plt.subplots(2, 3, figsize=(16, 9))\nfig.suptitle('Spectrogram', fontsize=16)\nfig.delaxes(ax[1, 2])\n\nlibrosa.display.specshow(DB_haiwoo, sr = sr_haiwoo, hop_length = hop_length, x_axis = 'time', \n                         y_axis = 'log', cmap = 'cool', ax=ax[0, 0])\n\nlibrosa.display.specshow(DB_wesmea, sr = sr_wesmea, hop_length = hop_length, x_axis = 'time', \n                         y_axis = 'log', cmap = 'cool', ax=ax[0, 1])\n\nlibrosa.display.specshow(DB_wewpew, sr = sr_wewpew, hop_length = hop_length, x_axis = 'time', \n                         y_axis = 'log', cmap = 'cool', ax=ax[0, 2])\nlibrosa.display.specshow(DB_scoori, sr = sr_scoori, hop_length = hop_length, x_axis = 'time', \n                         y_axis = 'log', cmap = 'cool', ax=ax[1, 0])\n\nlibrosa.display.specshow(DB_bewwre, sr = sr_bewwre, hop_length = hop_length, x_axis = 'time', \n                         y_axis = 'log', cmap = 'cool', ax=ax[1, 1]);\n\n\n\nfor i, name in zip(range(0, 2*3), bird_sample_list):\n    x = i // 3\n    y = i % 3\n    ax[x, y].set_title(name, fontsize=13) \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing Audio**\n\nWe can plot the audio array using librosa.display.waveplot. Let's plot the amplitude envelope of a waveform.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# === PLOT ===\nfig, ax = plt.subplots(2, 3, figsize=(17, 5))\nfig.suptitle('Waveform', fontsize=16)\nfig.delaxes(ax[1, 2])\n\nlibrosa.display.waveplot(DB_haiwoo, sr = sr_haiwoo, x_axis = 'time', \n                          ax=ax[0, 0])\nlibrosa.display.waveplot(DB_wesmea, sr = sr_wesmea, x_axis = 'time', \n                          ax=ax[0, 1])\nlibrosa.display.waveplot(DB_wewpew, sr = sr_wewpew, x_axis = 'time', \n                          ax=ax[0, 2])\nlibrosa.display.waveplot(DB_scoori, sr = sr_scoori, x_axis = 'time', \n                          ax=ax[1, 0])\nlibrosa.display.waveplot(DB_bewwre, sr = sr_bewwre, x_axis = 'time', \n                         ax=ax[1, 1]);\nfor i, name in zip(range(0,2*3), bird_sample_list):\n    x = i // 3\n    y = i % 3\n    ax[x, y].set_title(name, fontsize=13) \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Zero Crossing Rate**\n\nThe rate at which the signal changes from positive to negative or back.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total zero_crossings in our 1 song\nzero_haiwoo = librosa.zero_crossings(audio_haiwoo, pad=False)\nzero_wesmea= librosa.zero_crossings(audio_wesmea, pad=False)\nzero_wewpew = librosa.zero_crossings(audio_wewpew, pad=False)\nzero_scoori = librosa.zero_crossings(audio_scoori, pad=False)\nzero_bewwre = librosa.zero_crossings(audio_bewwre, pad=False)\n\nzero_birds_list = [zero_haiwoo, zero_wesmea, zero_wewpew,zero_scoori, zero_bewwre]\n\nfor bird, name in zip(zero_birds_list, bird_sample_list):\n    print(\"{} change rate is {:,}\".format(name, sum(bird)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Harmonics and Perceptual**\n* Harmonics are characteristics that human ears cann't distinguish.\n* Perceptrual understanding shock wave represents that sound rhythm and emotion.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_harm_haiwoo, y_perc_haiwoo = librosa.effects.hpss(audio_haiwoo)\ny_harm_wesmea, y_perc_wesmea = librosa.effects.hpss(audio_wesmea)\ny_harm_wewpew, y_perc_wewpew = librosa.effects.hpss(audio_wewpew)\ny_harm_scoori, y_perc_scoori = librosa.effects.hpss(audio_scoori)\ny_harm_bewwre, y_perc_bewwre = librosa.effects.hpss(audio_bewwre)\n#for haiwoo\nplt.figure(figsize = (16, 6))\nplt.plot(y_perc_haiwoo, color = '#FFB100')\nplt.plot(y_harm_haiwoo, color = '#A300F9')\nplt.legend((\"Perceptrual\", \"Harmonics\"))\nplt.title(\"Harmonics and Perceptrual : Haiwoo Bird\", fontsize=16);\n#for wesmea\nplt.figure(figsize = (16, 6))\nplt.plot(y_perc_wesmea, color = '#FFB150')\nplt.plot(y_harm_wesmea, color = '#A300F9')\nplt.legend((\"Perceptrual\", \"Harmonics\"))\nplt.title(\"Harmonics and Perceptrual : wesmea Bird\", fontsize=16);\n#for wewpew\nplt.figure(figsize = (16, 6))\nplt.plot(y_perc_wewpew, color = '#FFB234')\nplt.plot(y_harm_wewpew, color = '#A300F9')\nplt.legend((\"Perceptrual\", \"Harmonics\"))\nplt.title(\"Harmonics and Perceptrual : wewpew Bird\", fontsize=16);\n#for scoori\nplt.figure(figsize = (16, 6))\nplt.plot(y_perc_scoori, color = '#FFBB10')\nplt.plot(y_harm_scoori, color = '#A300F9')\nplt.legend((\"Perceptrual\", \"Harmonics\"))\nplt.title(\"Harmonics and Perceptrual : Scoori Bird\", fontsize=16);\n#for bewwre\nplt.figure(figsize = (16, 6))\nplt.plot(y_perc_bewwre, color = '#FFB100')\nplt.plot(y_harm_bewwre, color = '#A300F9')\nplt.legend((\"Perceptrual\", \"Harmonics\"))\nplt.title(\"Harmonics and Perceptrual : bewwre Bird\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spectral Centroid**\n\nIndicates Where the \"Centre of mass\" for a sound is located and is calculated as the weighted mean of the frequencies present in the sound.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the Spectral Centroids\nspectral_centroids_scoori = librosa.feature.spectral_centroid(audio_scoori, sr=sr_scoori)[0]\nspectral_centroids_haiwoo = librosa.feature.spectral_centroid(audio_haiwoo, sr=sr_haiwoo)[0]\nspectral_centroids_wewpew = librosa.feature.spectral_centroid(audio_wewpew, sr=sr_wewpew)[0]\nspectral_centroids_wesmea = librosa.feature.spectral_centroid(audio_wesmea, sr=sr_wesmea)[0]\nspectral_centroids_bewwre = librosa.feature.spectral_centroid(audio_bewwre, sr=sr_bewwre)[0]\n\nspectral_centroids=[spectral_centroids_haiwoo,spectral_centroids_wewpew,spectral_centroids_wesmea,spectral_centroids_bewwre,spectral_centroids_scoori]\n\n\n# Shape is a vector\nfor Centroids, name in zip(spectral_centroids, bird_sample_list):\n    print(\"Centroids of \",name,'is',Centroids,'\\n')\n\n# Computing the time variable for visualization\nframes = range(len(Centroids))\n\n# Converts frame counts to time (seconds)\nt = librosa.frames_to_time(frames)\n\nprint('frames:', frames, '\\n')\nprint('t:', t)\n\n# Function that normalizes the Sound Data\ndef normalize(x, axis=0):\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Spectral Centroid along the waveform\n#for Haiwoo\nplt.figure(figsize = (19, 6))\nlibrosa.display.waveplot(audio_haiwoo, sr=sr_haiwoo, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Centroids), color='#FFB100', lw=2)\nplt.legend([\"Spectral Centroid\", \"Wave\"])\nplt.title(\"Spectral Centroid: Haiwoo Bird\", fontsize=16);\n#for Wewpew\nplt.figure(figsize=(19,6))\nlibrosa.display.waveplot(audio_wewpew, sr=sr_wewpew, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Centroids), color='#FFB100', lw=2)\nplt.legend([\"Spectral Centroid\", \"Wave\"])\nplt.title(\"Spectral Centroid:Wewpew Bird\", fontsize=16);\n#for Wesmea\nplt.figure(figsize=(19,6))\nlibrosa.display.waveplot(audio_wesmea, sr=sr_wesmea, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Centroids), color='#FFB100', lw=2)\nplt.legend([\"Spectral Centroid\", \"Wave\"])\nplt.title(\"Spectral Centroid: Wesmea Bird\", fontsize=16);\n# for bewwre\nplt.figure(figsize=(19,6))\nlibrosa.display.waveplot(audio_bewwre, sr=sr_bewwre, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Centroids), color='#FFB100', lw=2)\nplt.legend([\"Spectral Centroid\", \"Wave\"])\nplt.title(\"Spectral Centroid: Bewwre Bird\", fontsize=16);\n#for Scoori\nplt.figure(figsize=(19,6))\nlibrosa.display.waveplot(audio_scoori, sr=sr_scoori, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Centroids), color='#FFB100', lw=2)\nplt.legend([\"Spectral Centroid\", \"Wave\"])\nplt.title(\"Spectral Centroid: Scoori Bird\", fontsize=16)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Chroma Frequencies**\n\nChroma features are an interesting and powerful representation for music audio in which the entire spectrum is projected onto 12 bins representing the 12 distinct semitones of the musical octave.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Increase or decrease hop_length to change how granular you want your data to be\nhop_length = 5000\n\n# Chromogram Vesspa\nchromagram_haiwoo = librosa.feature.chroma_stft(audio_haiwoo, sr=sr_haiwoo, hop_length=hop_length)\nchromagram_wewpew = librosa.feature.chroma_stft(audio_wewpew, sr=sr_wewpew, hop_length=hop_length)\nchromagram_wesmea = librosa.feature.chroma_stft(audio_wesmea, sr=sr_wesmea, hop_length=hop_length)\nchromagram_bewwre = librosa.feature.chroma_stft(audio_bewwre, sr=sr_bewwre, hop_length=hop_length)\nchromagram_scoori = librosa.feature.chroma_stft(audio_scoori, sr=sr_scoori, hop_length=hop_length)\nchromagram=[chromagram_haiwoo,chromagram_wewpew,chromagram_wesmea,chromagram_bewwre,chromagram_scoori]\nfor bird,name in zip(chromagram,bird_sample_list):\n    print(\"Chromogram\",name,'shape',np.shape(bird))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for Haiwoo\nlibrosa.display.specshow(chromagram_haiwoo, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='twilight')\nplt.figure(figsize=(16, 6))\nplt.title(\"Chromogram Haiwoo\", fontsize=16);\n#for wewpew\nlibrosa.display.specshow(chromagram_wewpew, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='twilight')\nplt.figure(figsize=(16, 6))\nplt.title(\"Chromogram Wewpew\", fontsize=16);\n#for Wesmea\nlibrosa.display.specshow(chromagram_wesmea, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='twilight')\nplt.figure(figsize=(16, 6))\nplt.title(\"Chromogram Wesmea\", fontsize=16);\n#for Bewwre\nlibrosa.display.specshow(chromagram_bewwre, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='twilight')\nplt.figure(figsize=(16, 6))\nplt.title(\"Chromogram Bewwre\", fontsize=16);\n#for scoori\nlibrosa.display.specshow(chromagram_scoori, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='twilight')\nplt.figure(figsize=(16, 6))\nplt.title(\"Chromogram Scoori\", fontsize=16);\n#\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spectral Rolloff**\n\nIt is measure of the shape of the signal.It represents the frequency below which  specified percentage of the total spectral energy lies.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spectral RollOff Vector\nspectral_rolloff_haiwoo = librosa.feature.spectral_rolloff(audio_haiwoo, sr=sr_haiwoo)[0]\nspectral_rolloff_wewpew= librosa.feature.spectral_rolloff(audio_wewpew, sr=sr_wewpew)[0]\nspectral_rolloff_wesmea= librosa.feature.spectral_rolloff(audio_wesmea, sr=sr_wesmea)[0]\nspectral_rolloff_bewwre= librosa.feature.spectral_rolloff(audio_bewwre, sr=sr_bewwre)[0]\nspectral_rolloff_scoori= librosa.feature.spectral_rolloff(audio_scoori, sr=sr_scoori)[0]\n\nspectral_rolloff=[spectral_rolloff_haiwoo,spectral_rolloff_wewpew,spectral_rolloff_wesmea,spectral_rolloff_bewwre,spectral_rolloff_scoori]\n# Shape is a vector\nfor Rolloff, name in zip(spectral_rolloff, bird_sample_list):\n    print(\"Centroids of \",name,'is',Rolloff,'\\n')\n\n# Computing the time variable for visualization\nframes = range(len(Rolloff))\n# Converts frame counts to time (seconds)\nt = librosa.frames_to_time(frames)\nprint('frames:', frames, '\\n')\nprint('t:', t)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The plot\n#for Haiwoo\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_haiwoo, sr=sr_haiwoo, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Rolloff), color='#FFB100', lw=3)\nplt.legend([\"Spectral Rolloff\", \"Wave\"])\nplt.title(\"Spectral Rolloff: haiwoo Bird\", fontsize=16);\n#for Wesmea\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_wesmea, sr=sr_wesmea, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Rolloff), color='#FFB100', lw=3)\nplt.legend([\"Spectral Rolloff\", \"Wave\"])\nplt.title(\"Spectral Rolloff: Wesmea Bird\", fontsize=16);\\\n# for wewpew\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_wewpew, sr=sr_wewpew, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Rolloff), color='#FFB100', lw=3)\nplt.legend([\"Spectral Rolloff\", \"Wave\"])\nplt.title(\"Spectral Rolloff: Wewpew Bird\", fontsize=16);\n#for bewwri\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_bewwre, sr=sr_bewwre, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Rolloff), color='#FFB100', lw=3)\nplt.legend([\"Spectral Rolloff\", \"Wave\"])\nplt.title(\"Spectral Rolloff: Bewwre Bird\", fontsize=16);\n#for Scoori\nplt.figure(figsize = (16, 6))\nlibrosa.display.waveplot(audio_scoori, sr=sr_scoori, alpha=0.4, color = '#A300F9', lw=3)\nplt.plot(t, normalize(Rolloff), color='#FFB100', lw=3)\nplt.legend([\"Spectral Rolloff\", \"Wave\"])\nplt.title(\"Spectral Rolloff: Scoori Bird\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Please upvote me!!!**\n\n**Stay Tunned.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}