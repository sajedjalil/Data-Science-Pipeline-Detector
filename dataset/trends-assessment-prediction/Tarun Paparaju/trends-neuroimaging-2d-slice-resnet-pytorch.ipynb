{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://i.imgur.com/fs4C0r1.png\" width=\"700px\"></center>"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nHello everyone! Welcome to the <font color=\"#fa7703\">\"TReNDS Neuroimaging\"</font> competition on Kaggle! In this competition, contestants are challenged to build machine learning models that can predict certain quantities related to <font color=\"#fa7703\">normative age and brain connectivity based on fMRI scans of the patient's brain</font>. An accurate solution to this problem can open up new possibilities in the field of neuroimaging and neuroscience.\n\nIn this kernel, I will demonstrate how this problem can be <font color=\"#fa7703\">broken down into a 2D image regression problem</font> and solved using pretrained ImageNet models like ResNet. I will use PyTorch v1.5 and Kaggle's NVIDIA Tesla P100 GPU to train the model.\n\n<center><img src=\"https://i.imgur.com/Bev6ZXt.png\" width=\"260px\"></center>"},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\n\n1. [<font color=\"#fa7703\">PyTorch</font><font color=\"#5e5d5d\"> ~ by PyTorch</font>](https://pytorch.org)\n2. [<font color=\"#fa7703\">Colorama</font><font color=\"#5e5d5d\"> ~ by Jonathan Hartley</font>](https://github.com/tartley/colorama)\n2. [<font color=\"#fa7703\">Torchvision Models</font><font color=\"#5e5d5d\"> ~ by PyTorch</font>](https://pytorch.org/docs/stable/torchvision/models.html)\n3. [<font color=\"#fa7703\">Torchviz</font><font color=\"#5e5d5d\"> ~ by Sergey Zagoruyko</font>](https://github.com/szagoruyko/pytorchviz)\n4. [<font color=\"#fa7703\">Reading Matlab (.mat) Files and EDA</font><font color=\"#5e5d5d\"> ~ by Manoj</font>](https://www.kaggle.com/mks2192/reading-matlab-mat-files-and-eda)\n5. [<font color=\"#fa7703\">Brain 3D plotly visualization</font><font color=\"#5e5d5d\"> ~ by Vladislav Bakhteev</font>](https://www.kaggle.com/speedwagon/brain-3d-plotly-visualization)"},{"metadata":{},"cell_type":"markdown","source":"# Contents\n\n* [<font size=4 color=\"#fa7703\">Preparing the ground</font>](#1)\n    * [<font color=\"#5e5d5d\">Import libraries</font>](#1.1)\n    * [<font color=\"#5e5d5d\">Set hyperparameters and paths</font>](#1.2)\n    * [<font color=\"#5e5d5d\">Load .csv data</font>](#1.3)\n    * [<font color=\"#5e5d5d\">Display few 2D slices along <i>x, y, and z</i> axes</font>](#1.4)\n\n    \n* [<font size=4 color=\"#fa7703\">Modeling</font>](#2)\n    * [<font color=\"#5e5d5d\">Build PyTorch dataset</font>](#2.1)\n    * [<font color=\"#5e5d5d\">Build ResNet model</font>](#2.2)\n    * [<font color=\"#5e5d5d\">Visualize ResNet architecture</font>](#2.3)\n    * [<font color=\"#5e5d5d\">Define custom weighted absolute error loss</font>](#2.4)\n    * [<font color=\"#5e5d5d\">Define helper function for training logs</font>](#2.5)\n    * [<font color=\"#5e5d5d\">Split data into training and validation sets</font>](#2.6)\n    * [<font color=\"#5e5d5d\">Train model on GPU</font>](#2.7)\n\n\n* [<font size=4 color=\"#fa7703\">Takeaways</font>](#3)"},{"metadata":{},"cell_type":"markdown","source":"# Preparing the ground <a id=\"1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Import libraries <a id=\"1.1\"></a> <font color=\"#fa7703\" size=4>(for data loading, processing, and modeling on GPU)</font>\n\n1. <font color=\"#fa7703\">os</font> <font color=\"#5e5d5d\"> ~ for managing paths</font>\n2. <font color=\"#fa7703\">gc</font> <font color=\"#5e5d5d\"> ~ for garbage collection</font>\n3. <font color=\"#fa7703\">cv2</font> <font color=\"#5e5d5d\"> ~ to process images</font>\n4. <font color=\"#fa7703\">h5py</font> <font color=\"#5e5d5d\"> ~ to read 3D fMRI maps</font>\n5. <font color=\"#fa7703\">colored</font> <font color=\"#5e5d5d\"> ~ to print colored text</font>\n6. <font color=\"#fa7703\">numpy</font> <font color=\"#5e5d5d\"> ~ for linear algebra</font>\n7. <font color=\"#fa7703\">pandas</font> <font color=\"#5e5d5d\"> ~ to manipulate tabular data</font>\n8. <font color=\"#fa7703\">random</font> <font color=\"#5e5d5d\"> ~ to pick random slices</font>\n9. <font color=\"#fa7703\">tqdm</font> <font color=\"#5e5d5d\"> ~ to generate progress bars</font>\n10. <font color=\"#fa7703\">matplotlib</font> <font color=\"#5e5d5d\"> ~ to plot slices</font>\n11. <font color=\"#fa7703\">plotly</font> <font color=\"#5e5d5d\"> ~ to plot 3D meshes</font>\n13. <font color=\"#fa7703\">torch</font> <font color=\"#5e5d5d\"> ~ to build and train the neural network</font>\n14. <font color=\"#fa7703\">torchviz</font> <font color=\"#5e5d5d\"> ~ to visualize PyTorch dynamic graphs</font>\n15. <font color=\"#fa7703\">torchvision</font> <font color=\"#5e5d5d\"> ~ to download ResNet-18</font>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"!pip install -q colored\n!pip install -q torchviz","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport time\nimport h5py\nimport colored\nfrom colored import fg, bg, attr\n\nfrom skimage import measure\nfrom plotly.offline import iplot\nfrom plotly import figure_factory as FF\nfrom IPython.display import Markdown, display\n\nimport numpy as np\nimport pandas as pd\nfrom random import randint\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchviz import make_dot\ntorch.backends.cudnn.benchmark = True\n\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torchvision.models import resnet18, densenet121, mobilenet_v2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set hyperparameters and paths <a id=\"1.2\"></a> <font color=\"#fa7703\" size=4>(adjust these to improve LB scores :D)</font>\n\n1. Choose hyperparameters such as epochs, split percentage, learning rate, etc\n2. Set important paths and directories to load data and train the model"},{"metadata":{"trusted":false},"cell_type":"code","source":"EPOCHS = 2\nSPLIT = 0.8\nLR = (1e-4, 1e-3)\nMODEL_SAVE_PATH = \"resnet_model\"\n\nW = 64\nH = 64\nBATCH_SIZE = 32\nVAL_BATCH_SIZE = 32\nDATA_PATH = '../input/trends-assessment-prediction/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load .csv data <a id=\"1.3\"></a> <font color=\"#fa7703\" size=4>(to access 3D fMRI maps for training and validation)</font>\n\n1. Load dataframes with IDs, targets, and tabular features\n2. Extract relevant IDs for training and testing from the dataframes"},{"metadata":{"trusted":false},"cell_type":"code","source":"TEST_MAP_PATH = DATA_PATH + 'fMRI_test/'\nTRAIN_MAP_PATH = DATA_PATH + 'fMRI_train/'\n\nFEAT_PATH = DATA_PATH + 'fnc.csv'\nTARG_PATH = DATA_PATH + 'train_scores.csv'\nSAMPLE_SUB_PATH = DATA_PATH + 'sample_submission.csv'\n\nTEST_IDS = [map_id[:-4] for map_id in sorted(os.listdir(TEST_MAP_PATH))]\nTRAIN_IDS = [map_id[:-4] for map_id in sorted(os.listdir(TRAIN_MAP_PATH))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"targets = pd.read_csv(TARG_PATH)\ntargets = targets.fillna(targets.mean())\nsample_submission = pd.read_csv(SAMPLE_SUB_PATH)\n\nfeatures = pd.read_csv(FEAT_PATH)\ntest_df = features.query('Id in {}'.format(TEST_IDS)).reset_index(drop=True)\ntrain_df = features.query('Id in {}'.format(TRAIN_IDS)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"targets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display few 2D slices along x, y, z <a id=\"1.4\"></a> <font color=\"#fa7703\" size=4>(to understand the slices we are dealing with)</font>\n\n1. Calculate random slices of the fMRI map\n2. Display the calculated slices using MatPlotLib"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def display_maps(idx):\n    path = TRAIN_MAP_PATH + str(train_df.Id[idx])\n    all_maps = h5py.File(path + '.mat', 'r')['SM_feature'][()]\n    fig, ax = plt.subplots(nrows=3, ncols=5, figsize=(20, 12.5))\n\n    plt.set_cmap('gray')\n    for i in range(5):\n        idx_1, idx_2, idx_3 = randint(0, 51), randint(0, 62), randint(0, 52)\n\n        proj_1 = all_maps[:, idx_1, :, :].transpose(1, 2, 0)\n        proj_2 = all_maps[:, :, idx_2, :].transpose(1, 2, 0)\n        proj_3 = all_maps[:, :, :, idx_3].transpose(1, 2, 0)\n        ax[0, i].imshow(cv2.resize(proj_1[:, :, 0], (H, W)))\n        ax[1, i].imshow(cv2.resize(proj_2[:, :, 0], (H, W)))\n        ax[2, i].imshow(cv2.resize(proj_3[:, :, 0], (H, W)))\n        ax[0, i].set_title('Z-section {}'.format(i), fontsize=12)\n        ax[1, i].set_title('Y-section {}'.format(i), fontsize=12)\n        ax[2, i].set_title('X-section {}'.format(i), fontsize=12)\n\n    plt.suptitle('Id: {}'.format(train_df.Id[idx])); plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Map #0\n\nTarget values → {age: 57.44, domain1_var1: 30.57, domain1_var2: 62.55, domain2_var1: 55.33, domain2_var2: 51.43}"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"display_maps(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Map #1\n\nTarget values → {age: 59.58, domain1_var1: 50.97, domain1_var2: 67.47, domain2_var1: 60.65, domain2_var2: 58.31}"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"display_maps(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Map #2\n\nTarget values → {age: 71.41, domain1_var1: 53.15, domain1_var2: 58.01, domain2_var1: 52.42, domain2_var2: 62.54}"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"display_maps(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling <a id=\"2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## Build PyTorch Dataset <a id=\"2.1\"></a> <font color=\"#fa7703\" size=4>(with map slicing and resizing)</font>\n\n1. Retrieve all 53 fMRI maps for a given ID\n2. Randomly slice each map along the *x, y,* and *z* axes to get 159 2D slices\n3. Resize each 2D map to the shape (64, 64) using OpenCV-2 and concatenate all slices\n4. Get targets for given ID and stack 159 times (each slice has the same target)\n5. Return the calculated image tensors and target tensors for the given patient ID"},{"metadata":{"trusted":false},"cell_type":"code","source":"class TReNDSDataset(Dataset):\n    def __init__(self, data, targets, map_path, is_train):\n        self.data = data\n        self.is_train = is_train\n        self.map_path = map_path\n        self.map_id = self.data.Id\n        if is_train: self.targets = targets\n            \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        path = self.map_path + str(self.map_id[idx])\n        all_maps = h5py.File(path + '.mat', 'r')['SM_feature'][()]\n        cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n        \n        idx_1, idx_2, idx_3 = randint(0, 51), randint(0, 62), randint(0, 52)\n        proj_1 = cv2.resize(all_maps[:, idx_1, :, :].transpose(1, 2, 0), (H, W))\n        proj_2 = cv2.resize(all_maps[:, :, idx_2, :].transpose(1, 2, 0), (H, W))\n        proj_3 = cv2.resize(all_maps[:, :, :, idx_3].transpose(1, 2, 0), (H, W))\n        features = np.concatenate([proj_1, proj_2, proj_3], axis=2).transpose(2, 0, 1)\n        \n        if not self.is_train:\n            return torch.FloatTensor(features)\n        else:\n            i = self.map_id[idx]\n            targets = self.targets.query('Id == {}'.format(i)).values\n            targets = np.repeat(targets[:, 1:], 159, 0).reshape(-1, 5)\n            return torch.FloatTensor(features), torch.FloatTensor(targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build ResNet model <a id=\"2.2\"></a> <font color=\"#fa7703\" size=4>(with a double dense head)</font>\n\n1. Get ResNet-18 head (till AvgPool)\n2. Add two Dense layers (16 and 5 neurons) on top\n3. Reshape and tile image to match ResNet input dimensions\n4. Pass reshaped image tensor through neural network and get output"},{"metadata":{"trusted":false},"cell_type":"code","source":"class ResNetModel(nn.Module):\n    def __init__(self):\n        super(ResNetModel, self).__init__()\n        \n        self.identity = lambda x: x\n        self.dense_out = nn.Linear(16, 5)\n        self.dense_in = nn.Linear(512, 16)\n        self.resnet = resnet18(pretrained=True, progress=False)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n        \n    def forward(self, img):\n        img = img.reshape(-1, 1, H, W)\n        feat = self.resnet(img.repeat(1, 3, 1, 1))\n        \n        conc = self.dense_in(feat.squeeze())\n        return self.identity(self.dense_out(conc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize ResNet architecture<a id=\"2.3\"></a> <font color=\"#fa7703\" size=4>(with pytorchviz)</font>"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"model = ResNetModel()\nx = torch.randn(1, 3, 64, 64).requires_grad_(True)\ny = model(x)\nmake_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"del model, x, y\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define custom weighted absolute error loss<a id=\"2.4\"></a> <font color=\"#fa7703\" size=4>(for backpropagation)</font>\n\n1. Define weightage for each target\n2. Calculate weighted absolute errors and take the average"},{"metadata":{"trusted":false},"cell_type":"code","source":"def weighted_nae(inp, targ):\n    W = torch.FloatTensor([0.3, 0.175, 0.175, 0.175, 0.175])\n    return torch.mean(torch.matmul(torch.abs(inp - targ), W.cuda()/torch.mean(targ, axis=0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define helper function for training logs <a id=\"2.5\"></a> <font color=\"#fa7703\" size=4>(to check training status)</font>\n\n1. Retrieve training and validation metrics\n2. Format metrics and display them during training"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def print_metric(data, batch, epoch, start, end, metric, typ):\n    time = np.round(end - start, 1)\n    time = \"Time: %s{}%s s\".format(time)\n\n    if typ == \"Train\":\n        pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n    if typ == \"Val\":\n        pre = \"EPOCH %s\" + str(epoch+1) + \"%s  \"\n    \n    fonts = (fg(216), attr('reset'))\n    value = np.round(data.item(), 3)\n    t = typ, metric, \"%s\", value, \"%s\"\n\n    print(pre % fonts , end='')\n    print(\"{} {}: {}{}{}\".format(*t) % fonts + \"  \" + time % fonts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data into training and validation sets <a id=\"2.6\"></a> <font color=\"#fa7703\" size=4>(to validate performance properly)</font>\n\n1. Split the data into training and validation sets\n2. Define the test data loader to run inference after training"},{"metadata":{"trusted":false},"cell_type":"code","source":"val_out_shape = -1, 5\ntrain_out_shape = -1, 5\n\nsplit = int(SPLIT*len(train_df))\nval = train_df[split:].reset_index(drop=True)\ntrain = train_df[:split].reset_index(drop=True)\n\ntest_set = TReNDSDataset(test_df, None, TEST_MAP_PATH, False)\ntest_loader = DataLoader(test_set, batch_size=VAL_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model on GPU <a id=\"2.7\"></a> <font color=\"#fa7703\" size=4>(NVIDIA Tesla P100)</font>\n\n1. Define dataloders, model, optimizer, and learning rate scheduler\n2. Train the model in batches and check validation performance at the end of each epoch\n3. Save the model architecture and weights and generate testing predictions after training"},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_resnet18():\n    def cuda(tensor):\n        return tensor.cuda()\n   \n    val_set = TReNDSDataset(val, targets, TRAIN_MAP_PATH, True)\n    val_loader = DataLoader(val_set, batch_size=VAL_BATCH_SIZE)\n    train_set = TReNDSDataset(train, targets, TRAIN_MAP_PATH, True)\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n\n    network = cuda(ResNetModel())\n    optimizer = Adam([{'params': network.resnet.parameters(), 'lr': LR[0]},\n                      {'params': network.dense_in.parameters(), 'lr': LR[1]},\n                      {'params': network.dense_out.parameters(), 'lr': LR[1]}])\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5,\n                                  patience=2, verbose=True, eps=1e-6)\n    start = time.time()\n    for epoch in range(EPOCHS):\n        batch = 1\n        fonts = (fg(216), attr('reset'))\n        print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n\n        for train_batch in train_loader:\n            train_img, train_targs = train_batch\n           \n            network.train()\n            network = cuda(network)\n            train_preds = network.forward(cuda(train_img))\n            train_targs = train_targs.reshape(train_out_shape)\n            train_loss = weighted_nae(train_preds, cuda(train_targs))\n\n            optimizer.zero_grad()\n            train_loss.backward()\n\n            optimizer.step()\n            end = time.time()\n            batch = batch + 1\n            print_metric(train_loss, batch, epoch, start, end, metric=\"loss\", typ=\"Train\")\n            \n        print(\"\\n\")\n           \n        network.eval()\n        for val_batch in val_loader:\n            img, targ = val_batch\n            val_preds, val_targs = [], []\n\n            with torch.no_grad():\n                img = cuda(img)\n                network = cuda(network)\n                pred = network.forward(img)\n                val_preds.append(pred); val_targs.append(targ)\n\n        val_preds = torch.cat(val_preds, axis=0)\n        val_targs = torch.cat(val_targs, axis=0)\n        val_targs = val_targs.reshape(val_out_shape)\n        val_loss = weighted_nae(val_preds, cuda(val_targs))\n        \n        avg_preds = []\n        avg_targs = []\n        for idx in range(0, len(val_preds), 159):\n            avg_preds.append(val_preds[idx:idx+159].mean(axis=0))\n            avg_targs.append(val_targs[idx:idx+159].mean(axis=0))\n            \n        avg_preds = torch.stack(avg_preds, axis=0)\n        avg_targs = torch.stack(avg_targs, axis=0)\n        loss = weighted_nae(avg_preds, cuda(avg_targs))\n        \n        end = time.time()\n        scheduler.step(val_loss)\n        print_metric(loss, None, epoch, start, end, metric=\"loss\", typ=\"Val\")\n        \n        print(\"\\n\")\n   \n    network.eval()\n    if os.path.exists(TEST_MAP_PATH):\n\n        test_preds = []\n        for test_img in test_loader:\n            with torch.no_grad():\n                network = cuda(network)\n                test_img = cuda(test_img)\n                test_preds.append(network.forward(test_img))\n        \n        avg_preds = []\n        test_preds = torch.cat(test_preds, axis=0)\n        for idx in range(0, len(test_preds), 159):\n            avg_preds.append(test_preds[idx:idx+159].mean(axis=0))\n\n        torch.save(network.state_dict(), MODEL_SAVE_PATH + \".pt\")\n        return torch.stack(avg_preds, axis=0).detach().cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"STARTING TRAINING ...\\n\")\n\ntest_preds = train_resnet18()\n    \nprint(\"ENDING TRAINING ...\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate predictions <a id=\"2.8\"></a> <font color=\"#fa7703\" size=4>(to submit to the competition :D)</font>"},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.Predicted = test_preds.flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Takeaways <a id=\"3\"></a>\n\n1. <font color=\"#fa7703\" size=3>The maps can be sliced and resized in order to convert this into an image regression problem.</font>\n2. <font color=\"#fa7703\" size=3>Incorporating the tabular data features can probably boost the performace of this model.</font>\n3. <font color=\"#fa7703\" size=3>Using bigger models with greater representational capacity (DenseNet, EfficientNet, etc) can boost scores.</font>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}