{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TReNDS - Exploring tabular augmentation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Augmentation is broadly used for sample generation and regularization in image classification. This is an experiment to use the techniques on tabular data. I'm starting here with mixup and may add more in the future, if I feel motivated.\n\n## Credits\nMany ideas of my notebook are derived from this [notebook](https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964#MixUp) from the Bengaliai competition earlier this year. Please go there and upvote if you find this or other references usefull.\nHere are the references in detail:\n- https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964#MixUp: Multi head - metrics, - loss function, - mixup.\n- https://github.com/fastai/fastai/tree/master/fastai: mixup and many more\n- https://forums.fast.ai/t/tabulardata-mixup/52011/6: tabular mixup","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\n\nfrom fastai.tabular import * \nfrom fastai import *\n\nimport os, shutil\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Notebook Settings\n\n# np.set_printoptions(threshold=sys.maxsize)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"kaggle_path = Path('.')\nkaggle_input_path = Path('/kaggle/input/trends-assessment-prediction')\n\n#for dirname, _, filenames in os.walk(kaggle_input_path):\n#    print(dirname, filenames)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"INCLUDE_FNC_DATA = False\nIMPUTATION_STRAT = 'IGNORE_ON_TRAIN' # 'IGNORE_ON_TRAIN', 'MEAN' \nLOSS_BASE = 'MSE' # 'L1'\nLOSS_WEIGHTS = [0.4, 0.15, 0.15, 0.15, 0.15]\nBS = 128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"l_data = pd.read_csv(kaggle_input_path/'loading.csv')\n\nif INCLUDE_FNC_DATA:\n    f_data = pd.read_csv(kaggle_input_path/'fnc.csv')\n    l_data = l_data.merge(f_data, on='Id', how = 'inner')\n\ny_data = pd.read_csv(kaggle_input_path/'train_scores.csv')\n\nidx_site2 = pd.read_csv(kaggle_input_path/'reveal_ID_site2.csv')\n#submission = pd.read_csv(kaggle_input_path/'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"display(y_data.head())\ndisplay(y_data.describe())\ny_data.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"display(l_data.tail())\ndisplay(l_data.describe()),\nl_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Impute missing data","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n    ## will later ignore the value when executing the loss function\n    y_data = y_data.fillna(0)\nelse: #'MEAN'\n    y_data = y_data.fillna(mean())\n    \ny_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combine Xs and Ys","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train = l_data.merge(y_data, on='Id', how='inner').sort_values(by='Id').reset_index(drop = True)\nidx_train = train.pop('Id') \ntrain","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"test = l_data.merge(y_data, on='Id', how='outer', indicator = True)\ntest = test[test['_merge'] == 'left_only'].drop(['age',\n                                                 'domain1_var1', \n                                                 'domain1_var2',\n                                                 'domain2_var1',\n                                                 'domain2_var2',\n                                                 '_merge'], axis = 1).sort_values(by='Id').reset_index(drop = True)\nidx_test = test.pop('Id') \ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Metrics","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# variation of https://github.com/fastai/fastai/blob/master/fastai/metrics.py#L85\n\ndef norm_absolute_error(preds, targs):\n    \"Normalized absolute error between `pred` and `targ`.\"\n    sg=targs.sign()\n    y=targs*sg\n        \n    pred, targ = flatten_check(preds*sg, y)\n    return torch.abs(targ - pred).sum() / targ.sum()\n\ndef weighted_nae(preds, targs):\n    return 0.3 * norm_absolute_error(preds[:,0],targs[:,0]) + \\\n           0.175 * norm_absolute_error(preds[:,1],targs[:,1]) + \\\n           0.175 * norm_absolute_error(preds[:,2],targs[:,2]) + \\\n           0.175 * norm_absolute_error(preds[:,3],targs[:,3]) + \\\n           0.175 * norm_absolute_error(preds[:,4],targs[:,4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The customized metric callback is a variation of https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964. \nIt is used to keep track of the five single targets and the combined metric. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# variation of https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964\n\nclass Metric_idx(Callback):\n    def __init__(self, idx):\n        super().__init__()\n        self.idx = idx\n        \n    def on_epoch_begin(self, **kwargs):\n        self.targs, self.preds = Tensor([]), Tensor([])\n    \n    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n        last_output = last_output[self.idx]\n        last_target = last_target[self.idx]\n        \n        self.preds = torch.cat((self.preds, last_output.float().cpu()))\n        self.targs = torch.cat((self.targs, last_target.float().cpu()))\n        \n    def _norm_absolute_error(self):\n        return norm_absolute_error(self.preds, self.targs)\n    \n    def on_epoch_end(self, last_metrics, **kwargs): \n        return add_metrics(last_metrics, self._norm_absolute_error())\n\n    \nMetric_age = partial(Metric_idx,0)\nMetric_domain1_var1 = partial(Metric_idx,1)\nMetric_domain1_var2 = partial(Metric_idx,2)\nMetric_domain2_var1 = partial(Metric_idx,3)\nMetric_domain2_var2 = partial(Metric_idx,4)\n\nclass Metric_total(Callback):\n    def __init__(self):\n        super().__init__()\n        self.age = Metric_idx(0)\n        self.domain1_var1 = Metric_idx(1)\n        self.domain1_var2 = Metric_idx(2)\n        self.domain2_var1 = Metric_idx(3)\n        self.domain2_var2 = Metric_idx(4)\n        \n    def on_epoch_begin(self, **kwargs):\n        self.age.on_epoch_begin(**kwargs)\n        self.domain1_var1.on_epoch_begin(**kwargs)\n        self.domain1_var2.on_epoch_begin(**kwargs)\n        self.domain2_var1.on_epoch_begin(**kwargs)\n        self.domain2_var2.on_epoch_begin(**kwargs)\n        \n    \n    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n        self.age.on_batch_end(last_output, last_target, **kwargs)\n        self.domain1_var1.on_batch_end(last_output, last_target, **kwargs)\n        self.domain1_var2.on_batch_end(last_output, last_target, **kwargs)\n        self.domain2_var1.on_batch_end(last_output, last_target, **kwargs)\n        self.domain2_var2.on_batch_end(last_output, last_target, **kwargs)\n \n        \n    def on_epoch_end(self, last_metrics, **kwargs): \n        return add_metrics(last_metrics, \n                           0.3 * self.age._norm_absolute_error() +\n                           0.175*self.domain1_var1._norm_absolute_error()  +\n                           0.175*self.domain1_var2._norm_absolute_error()  +\n                           0.175*self.domain2_var1._norm_absolute_error()  +\n                           0.175*self.domain2_var2._norm_absolute_error()\n                          )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss function\nThe customized metric callback is a variation of https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964. It weights and combines the five single losses.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# variation of https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964\n\nclass Loss_combine(nn.Module):\n    def __init__(self, loss_weights = [0.4,0.15,0.15,0.15,0.15], loss_base = 'MSE'):\n        super().__init__()\n        \n        self.loss_base = loss_base\n        \n        self.loss_weights = loss_weights\n        self.fw = Tensor(LOSS_WEIGHTS).cuda()\n          \n            \n    def forward(self, input, target,reduction='mean'): #mean\n        \n        x0,x1,x2,x3,x4 = input.T\n        \n        if IMPUTATION_STRAT == 'IGNORE_ON_TRAIN':\n            sg = target.float().sign()\n            x0,x1,x2,x3,x4 = x0.float()*sg[:,0],x1.float()*sg[:,1],x2.float()*sg[:,2],x3.float()*sg[:,3],x4.float()*sg[:,4]\n        else: # 'MEAN'\n            sg = 1\n            x0,x1,x2,x3,x4 = x0.float(),x1.float(),x2.float(),x3.float(),x4.float()\n            \n        y = target.float()*sg\n        \n        if self.loss_base == 'MSE':\n            loss_func = F.mse_loss \n            reduction = 'sum'\n            return self.fw[0]*loss_func(x0,y[:,0],reduction=reduction)/sum(y[:,0]**2) + \\\n               self.fw[1]*loss_func(x1,y[:,1],reduction=reduction)/sum(y[:,1]**2) + \\\n               self.fw[2]*loss_func(x2,y[:,2],reduction=reduction)/sum(y[:,2]**2) + \\\n               self.fw[3]*loss_func(x3,y[:,3],reduction=reduction)/sum(y[:,3]**2) + \\\n               self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)/sum(y[:,4]**2)\n        else: # 'L1'\n            loss_func = F.l1_loss \n            reduction = 'mean'\n            return self.fw[0]*loss_func(x0,y[:,0],reduction=reduction) + \\\n               self.fw[1]*loss_func(x1,y[:,1],reduction=reduction) + \\\n               self.fw[2]*loss_func(x2,y[:,2],reduction=reduction) + \\\n               self.fw[3]*loss_func(x3,y[:,3],reduction=reduction) + \\\n               self.fw[4]*loss_func(x4,y[:,4],reduction=reduction)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augmentation\nAugmentation is broadly used for sample generation and regularization in image classification. This is a try to use the techniques on tabular data.\n\nThe Mixup implementation is a variation of https://github.com/fastai/fastai/blob/master/fastai/callbacks/mixup.py following this [example](https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964#MixUp) and adaptied for tabular data following this [thread](https://forums.fast.ai/t/tabulardata-mixup/52011/6).","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Vartiation https://github.com/fastai/fastai/blob/master/fastai/callbacks/mixup.py#L8\n# and https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964#MixUp\n# and https://forums.fast.ai/t/tabulardata-mixup/52011/6\n\nclass MixUpLoss(Module):\n    \"Adapt the loss function `crit` to go with mixup.\"\n    \n    def __init__(self, crit, reduction='mean'): #mean\n        super().__init__()\n        if hasattr(crit, 'reduction'): \n            self.crit = crit\n            self.old_red = crit.reduction\n            setattr(self.crit, 'reduction', 'none')\n        else: \n            self.crit = partial(crit, reduction='none')\n            self.old_crit = crit\n        self.reduction = reduction\n        \n    def forward(self, output, target):\n        if len(target.shape) == 2 and target.shape[1] == 11:\n            loss1, loss2 = self.crit(output,target[:,0:5].long()), self.crit(output,target[:,5:10].long())\n            d = loss1 * target[:,-1] + loss2 * (1-target[:,-1])\n        else:  d = self.crit(output, target)\n        \n        if self.reduction == 'mean':    return d.mean()\n        elif self.reduction == 'sum':   return d.sum()\n        return d\n    \n    def get_old(self):\n        if hasattr(self, 'old_crit'):  return self.old_crit\n        elif hasattr(self, 'old_red'): \n            setattr(self.crit, 'reduction', self.old_red)\n            return self.crit\n\nclass MixUpCallback(LearnerCallback):\n    \"Callback that creates the mixed-up input and target.\"\n    def __init__(self, learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True):\n        super().__init__(learn)\n        self.alpha,self.stack_x,self.stack_y = alpha,stack_x,stack_y\n    \n    def on_train_begin(self, **kwargs):\n        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)\n        \n    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n        \"Applies mixup to `last_input` and `last_target` if `train`.\"\n        if not train: return\n        \n        # last_input[0] ==> embedded categorical data\n        # last_input[1] ==> continous data\n        rnd = 1\n        #if .5<np.random.uniform():\n        #    rnd=np.random.uniform()/20\n        \n        l_org = last_input[1] * rnd\n        last_input = last_input[1] *rnd #0\n        \n        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))\n        lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n        \n        lambd = last_input.new(lambd)\n        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)\n        x1, y1 = last_input[shuffle], last_target[shuffle]\n        if self.stack_x:\n            new_input = [last_input, last_input[shuffle], lambd]\n        else: \n            out_shape = [lambd.size(0)] + [1 for _ in range(len(x1.shape) - 1)]\n            new_input = (last_input * lambd.view(out_shape) + x1 * (1-lambd).view(out_shape))\n        if self.stack_y:\n            new_target = torch.cat([last_target.float(), y1.float(), lambd[:,None].float()], 1)\n        else:\n            if len(last_target.shape) == 2:\n                lambd = lambd.unsqueeze(1).float()\n            new_target = last_target.float() * lambd + y1.float() * (1-lambd)\n        \n        return {'last_input': [l_org, new_input], 'last_target': new_target}  \n    \n    def on_train_end(self, **kwargs):\n        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()\n            \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 128\nvalid_idx = range(200, 400)\ndep_var = ['age','domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2']\n    \ndef prep_data(bs, valid_idx):\n    procs = [FillMissing, Categorify, Normalize]\n    cont_names = list(set(train.columns) - set(['age','domain1_var1','domain1_var2', 'domain2_var1', 'domain2_var2'])-set(dep_var))\n    cat_names = []\n\n    tlist = (TabularList.from_df(train, \n                                path=kaggle_path, \n                                cat_names=cat_names, \n                                cont_names=cont_names, \n                                procs=procs))\n\n    if valid_idx == None:\n        tlist = tlist.split_none()\n    else:\n        tlist = tlist.split_by_idx(valid_idx)\n\n    data = (tlist.label_from_df(cols=dep_var)\n                 .add_test(TabularList.from_df(test, \n                                               cat_names=cat_names,\n                                               cont_names=cont_names, \n                                               procs = procs))\n                 .databunch(path = kaggle_path, bs = bs))\n    \n    return data\n\n\ndef prep_learn(data):\n    \n    learn = tabular_learner(data, \n                        layers = [256,128,256,128,64], #[1024,1024,128,1024,128,1024,1024],#\n                        ps = 0.3,\n                        loss_func = Loss_combine(loss_weights = LOSS_WEIGHTS,  loss_base= LOSS_BASE),\n                        metrics=[Metric_age(),\n                                 Metric_domain1_var1(),\n                                 Metric_domain1_var2(),\n                                 Metric_domain2_var1(),\n                                 Metric_domain2_var2(),\n                                 Metric_total()],\n                       y_range=(Tensor([12,12,0,0,0]).cuda(),Tensor([90,90,100,100,100]).cuda())\n                       )#.to_fp16()\n\n    learn.clip_grad = 1.0\n    \n    return learn\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training experiment without augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = prep_data(bs, valid_idx)\nlearn = prep_learn(data)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"lr = 2e-2\nreduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\nlearn.fit_one_cycle(10, lr, callbacks=[reduceLR])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\nyt, yt_truth= learn.get_preds(ds_type=DatasetType.Train)\n\nprint(f'Without augmentation:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(yv,yv_truth)}')\nprint(f'Weighted normalized absolute error (Train): {weighted_nae(yt,yt_truth)}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training experiment with augmentation","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data = prep_data(bs, valid_idx)\nlearn = prep_learn(data)\n\nlr = 2e-2\nreduceLR = callbacks.ReduceLROnPlateauCallback(learn=learn, monitor = 'valid_loss', mode = 'auto', patience = 2, factor = 0.2, min_delta = 0)\nlearn.fit_one_cycle(10, lr, callbacks=[MixUpCallback(learn, alpha=0.4),reduceLR])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yv, yv_truth= learn.get_preds(ds_type=DatasetType.Valid)\nyt, yt_truth= learn.get_preds(ds_type=DatasetType.Train)\n\nprint(f'With augmentation:')\nprint(f'Weighted normalized absolute error (Valid): {weighted_nae(yv,yv_truth)}')\nprint(f'Weighted normalized absolute error (Train): {weighted_nae(yt,yt_truth)}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training conclution\n\nThe choosen augmentation technique in this configuration doesn't improve the validation score. \n\nLet's see if it generalizes better on the LB.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_submission(learn, postfix = ''):\n    preds = learn.get_preds(ds_type=DatasetType.Test)[0]\n    \n    rec = pd.DataFrame(idx_test)\n    rec['Id'] = rec['Id'].astype(str)+'_'\n    rec['Predicted'] = preds[:,1]\n    \n    submission=None\n\n    for t, tcol in enumerate(dep_var):\n        rec = pd.DataFrame(idx_test)\n        rec['Id'] = rec['Id'].astype(str)+'_'+tcol\n        rec['Predicted'] = preds[:,t]\n        if isinstance(submission, pd.DataFrame):\n            submission = submission.append(rec)\n        else:\n            submission = rec\n\n    submission = submission.sort_values('Id').reset_index(drop=True)\n    \n    display(submission.head(10))\n    submission.to_csv('submission'+postfix+'.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict without augmentation","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data = prep_data(bs, valid_idx = None)\nlearn = prep_learn(data)\n\nlr = 2e-2\nlearn.fit_one_cycle(10, lr)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"make_submission(learn, postfix = '_wo_aug')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict with augmentation","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data = prep_data(bs, valid_idx = None)\nlearn = prep_learn(data)\n\nlr = 2e-2\nlearn.fit_one_cycle(10, lr, callbacks=[MixUpCallback(learn, alpha=0.4)])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"make_submission(learn, postfix = '_with_aug')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nLB without augmentation: 0.164\n\nLB with augmentation: 0.165\n\n==> No advantage from augmentation is this particular setting. Got to try something else ... ;)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:fastai]","language":"python","name":"conda-env-fastai-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":4}