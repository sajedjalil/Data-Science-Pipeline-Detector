{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **1. Setup**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **1.1 RAPIDS Installation**\n\n[RAPIDS](https://rapids.ai/index.html) is a data science framework which includes a set of libraries for executing end-to-end data science pipelines completely in the GPU. It is very similar to NumPy, pandas and scikit-learn and it workes completely in the GPU. RAPIDS is not available in Kaggle Docker environment so it has to be installed manually.\n\nThis notebook includes codes and ideas from notebooks below. Don't forget to upvote their work as well.\n\n* [RAPIDS SVM on TReNDS Neuroimaging - Ahmet Erdem](https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging) by [@aerdem4](https://www.kaggle.com/aerdem4)\n* [RAPIDS Ensemble for TReNDS Neuroimaging - Bojan Tunguz](https://www.kaggle.com/tunguz/rapids-ensemble-for-trends-neuroimaging) by [@tunguz](https://www.kaggle.com/tunguz)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"import sys\n\n!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = ['/opt/conda/envs/rapids/lib/python3.6/site-packages'] + sys.path\nsys.path = ['/opt/conda/envs/rapids/lib/python3.6'] + sys.path\nsys.path = ['/opt/conda/envs/rapids/lib'] + sys.path\n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/\n\nimport cudf\nimport cupy as cp\nimport cuml\n\nprint(f'cuDF Version: {cudf.__version__}\\ncuPY Version: {cp.__version__}\\ncuML Version: {cuml.__version__}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge\n\nSEED = 7021991","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pickle file (`trends_tabular_data.pkl`) is loaded from [this](https://www.kaggle.com/gunesevitan/trends-neuroimaging-data-analysis-matlab-files) notebook. There are lots new features created in that notebook's *6. Feature Engineering* section and feature engineering process has a detailed data analysis and explanation.\n\nThere are additional 689 statistical features and 1590 bounding box features extracted from `fMRI_train` and `fMRI_test`. Those features have to be scaled with `StandardScaler` because they are on a different scale. `RobustScaler` yields worse results compared to `StandardScaler`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle('../input/trends-neuroimaging-data-analysis-3d-features/trends_tabular_data.pkl')\ndf_fnc = pd.read_csv('../input/trends-assessment-prediction/fnc.csv')\ndf_loading = pd.read_csv('../input/trends-assessment-prediction/loading.csv')\n\nfnc_features = [feature for feature in df_fnc.columns.tolist()[1:]]\nshifted_loading_features = ['IC_18', 'IC_20']\nloading_features = [feature for feature in df_loading.columns.tolist()[1:] if feature not in shifted_loading_features]\ntarget_features = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\nutility_features = ['is_train', 'site', 'site_predicted']\nspatial_features = [feature for feature in df.columns.tolist()[1:] if feature not in fnc_features and \n                                                                      feature not in loading_features and\n                                                                      feature not in shifted_loading_features and \n                                                                      feature not in target_features and \n                                                                      feature not in utility_features]\n\ndf.loc[:, spatial_features] = StandardScaler().fit_transform(df[spatial_features])\n\nboundingbox_features = [feature for feature in spatial_features if 'box' in feature]\nstatistical_features = [feature for feature in spatial_features if 'box' not in feature]\n\nprint(f'TReNDS Neuroimaging - Tabular Data Shape = {df.shape}')\nprint(f'TReNDS Neuroimaging - Tabular Data Memory Usage = {df.memory_usage().sum() / 1024 ** 2:.2f} MB')\n\ndel df_fnc, df_loading, shifted_loading_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **1.2 Train/Test Split and Feature/Sample Selection**\n\nTraining and test set are split by `is_train` flag and utility features are dropped. Features with large mean shifts (`IC_18` and `IC_20`) are dropped. There was one extreme outlier in `IC_02` and it is also dropped.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = df[df['is_train'] == 1].copy(deep=True).drop(columns=utility_features).reset_index(drop=True)\ndf_test = df[df['is_train'] == 0].copy(deep=True).drop(columns=(target_features + utility_features)).reset_index(drop=True)\n\ndf_train['is_filtered'] = 0\ndf_train.loc[df_train['IC_02'] < -0.02, 'is_filtered'] = 1\n\nfiltered_idx = df_train[df_train['is_filtered'] == 1].index\ndf_train.drop(filtered_idx, inplace=True)\ndf_train.drop(columns=['is_filtered'], inplace=True)\n\nprint(f'Training Data Shape = {df_train.shape}')\nprint(f'Training Data Memory Usage = {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Test Data Shape = {df_test.shape}')\nprint(f'Test Data Memory Usage = {df_test.memory_usage().sum() / 1024 ** 2:.2f} MB')\n\ndel df, filtered_idx, utility_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2. Linear Model Ensemble**\n\n`LinearModelEnsemble` is a wrapper of `sklearn.linear_model.Ridge` and `cuml.SVR`. It enables to train same models with minor changes. Those changes are random feature drops from different feature groups and using different seeds in cross-validation techniques. Those changes are done at the start of every round and they help to create more robust predictions because single models are oversensitive.\n\nIt has feature fraction functionality for every different feature groups (Loading, FNC, Statistical and Bounding Box). Those feature fraction parameters has to be implemented seperately because an imbalanced feature drop from a specific feature group hurts the models' performance drastically. The largest boost is coming from loading features so `loading_fraction` must be set to `1.0`, and other feature fraction parameters should be between `0.9` and `1.0`. If feature fractions are below `1.0`, predictions have to be created with decent amount of rounds so they can be reliable.\n\nIt creates the splits with different seed in every round, however, this functionality requires cross-validations with shuffle. Otherwise, data in the folds are going to be same in every round.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearModelEnsemble:\n    \n    def __init__(self, model, model_name, model_features, model_parameters, cv, n_rounds, loading_fraction, fnc_fraction, statistical_fraction, boundingbox_fraction, verbose):\n        \n        self.target_weights = np.array([0.3, 0.175, 0.175, 0.175, 0.175])        \n        self.target_idx = {\n            'age': df_train.loc[~df_train['age'].isnull()].index,\n            'domain1_var1': df_train.loc[~df_train['domain1_var1'].isnull()].index,\n            'domain1_var2': df_train.loc[~df_train['domain1_var2'].isnull()].index,\n            'domain2_var1': df_train.loc[~df_train['domain2_var1'].isnull()].index,\n            'domain2_var2': df_train.loc[~df_train['domain2_var2'].isnull()].index         \n        }\n        self.random_states = [1337, 5041992, 7021991, 42, 0]\n        \n        self.model = model\n        self.model_name = model_name\n        self.model_features = model_features\n        self.model_parameters = model_parameters\n        self.cv = cv        \n        self.n_rounds = n_rounds\n        self.loading_fraction = loading_fraction\n        self.fnc_fraction = fnc_fraction\n        self.statistical_fraction = statistical_fraction\n        self.boundingbox_fraction = boundingbox_fraction\n        self.verbose = verbose\n                 \n    def normalized_absolute_error(self, y_true, y_pred):\n        return np.mean(np.sum(np.abs(y_true - y_pred)) / np.sum(y_true))\n                       \n    def _ridge_train_and_predict(self, X_train, y_train, X_test, cv, r):\n        \n        self.ridge_scores = {\n            'age': [],\n            'domain1_var1': [],\n            'domain1_var2': [],\n            'domain2_var1': [],\n            'domain2_var2': [],\n        }\n        \n        self.ridge_oof_scores = []\n                \n        for i, target in enumerate(target_features):\n            \n            if self.verbose:\n                print(f'\\nFitting {target}\\n{(len(target) + 8) * \"-\"}')\n            \n            X_trn = X_train.loc[self.target_idx[target], self.model_features[target]].copy(deep=True)\n            y_trn = y_train.loc[self.target_idx[target], target].copy(deep=True)\n            train_predictions = pd.DataFrame(y_trn.copy(deep=True))\n            \n            if r == 0:\n                df_train[f'{self.model_name}_{target}'] = 0\n                df_test[f'{self.model_name}_{target}'] = 0\n                                    \n            for fold, (trn_idx, val_idx) in enumerate(cv.split(X_trn), 1):\n                \n                trn_data = X_trn.iloc[trn_idx, :]\n                trn_labels = y_trn.iloc[trn_idx]\n                val_data = X_trn.iloc[val_idx, :]\n                val_labels = y_trn.iloc[val_idx]\n                \n                ridge_regression = Ridge(**self.model_parameters[target])\n                ridge_regression.fit(trn_data, trn_labels)\n                \n                fold_train_predictions = ridge_regression.predict(val_data)\n                df_train.loc[X_trn.iloc[val_idx, :].index, f'{self.model_name}_{target}'] += (fold_train_predictions / self.n_rounds)\n                train_predictions.loc[X_trn.iloc[val_idx, :].index, target] = fold_train_predictions\n\n                fold_test_predictions = ridge_regression.predict(X_test.loc[:, self.model_features[target]])\n                df_test.loc[X_test.index, f'{self.model_name}_{target}'] += ((fold_test_predictions / cv.n_splits) / self.n_rounds)\n                                \n                ridge_score = self.normalized_absolute_error(val_labels.values, fold_train_predictions)\n                self.ridge_scores[target].append(ridge_score)\n                if self.verbose:\n                    print(f'Fold {fold} - Ridge Regression NAE {ridge_score:.6}')\n            \n            oof_nae = self.normalized_absolute_error(df_train.loc[self.target_idx[target], target], train_predictions[target])\n            self.ridge_oof_scores.append(oof_nae)\n            if self.verbose:\n                print(f'{52 * \"-\"}\\nRidge Regression ({target}) Mean NAE {np.mean(self.ridge_scores[target]):.6} [STD:{np.std(self.ridge_scores[target]):.6}]')\n            print(f'Ridge Regression ({target}) OOF NAE {oof_nae:.6}')\n            \n        weighted_nae = np.sum(np.array(self.ridge_oof_scores) * self.target_weights)\n        print(f'{52 * \"-\"}\\nRidge Regression Global Weighted NAE {weighted_nae:.6}')\n                \n    def _svr_train_and_predict(self, X_train, y_train, X_test, cv, r):          \n        \n        self.svr_scores = {\n            'age': [],\n            'domain1_var1': [],\n            'domain1_var2': [],\n            'domain2_var1': [],\n            'domain2_var2': [],\n        }\n        \n        self.svr_oof_scores = []\n                \n        for i, target in enumerate(target_features):\n            \n            if self.verbose:\n                print(f'\\nFitting {target}\\n{(len(target) + 8) * \"-\"}')\n            \n            X_trn = X_train.loc[self.target_idx[target], self.model_features[target]].copy(deep=True)\n            y_trn = y_train.loc[self.target_idx[target], target].copy(deep=True)\n            train_predictions = pd.DataFrame(y_trn.copy(deep=True))\n            \n            if r == 0:\n                df_train[f'{self.model_name}_{target}'] = 0\n                df_test[f'{self.model_name}_{target}'] = 0\n                    \n            for fold, (trn_idx, val_idx) in enumerate(cv.split(X_trn), 1):\n                \n                trn_data = X_trn.iloc[trn_idx, :]\n                trn_labels = y_trn.iloc[trn_idx]\n                val_data = X_trn.iloc[val_idx, :]\n                val_labels = y_trn.iloc[val_idx]\n                \n                svr_model = cuml.SVR(**self.model_parameters[target])\n                svr_model.fit(trn_data.values, trn_labels.values)\n                \n                fold_train_predictions = svr_model.predict(val_data.values)\n                df_train.loc[X_trn.iloc[val_idx, :].index, f'{self.model_name}_{target}'] += (cp.asnumpy(fold_train_predictions.values) / self.n_rounds)\n                train_predictions.loc[X_trn.iloc[val_idx, :].index, target] = fold_train_predictions\n\n                fold_test_predictions = svr_model.predict(X_test.loc[:, self.model_features[target]].values)\n                df_test.loc[X_test.index, f'{self.model_name}_{target}'] += ((cp.asnumpy(fold_test_predictions.values) / cv.n_splits) / self.n_rounds)\n                                                \n                svr_score = self.normalized_absolute_error(val_labels.values, cp.asnumpy(fold_train_predictions.values))\n                self.svr_scores[target].append(svr_score)  \n                if self.verbose:\n                    print(f'Fold {fold} - SVR NAE {svr_score:.6}')\n            \n            oof_nae = self.normalized_absolute_error(df_train.loc[self.target_idx[target], target], train_predictions[target])\n            self.svr_oof_scores.append(oof_nae)\n            \n            if self.verbose:\n                print(f'{52 * \"-\"}\\nSVR ({target}) Mean NAE {np.mean(self.svr_scores[target]):.6} [STD:{np.std(self.svr_scores[target]):.6}]')\n            print(f'SVR ({target}) OOF NAE {oof_nae:.6}')\n            \n        weighted_nae = np.sum(np.array(self.svr_oof_scores) * self.target_weights)\n        print(f'{52 * \"-\"}\\nSVR Global Weighted NAE {weighted_nae:.6}')\n                \n    def plot_regression(self):\n        \n        fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(25, 50))\n        \n        for i, target in enumerate(target_features):\n            \n            target_oof_predictions = df_train.loc[self.target_idx[target], f'{self.model_name}_{target}']\n            target_y_true = df_train.loc[self.target_idx[target], target]  \n            target_test_predictions = df_test.loc[:, f'{self.model_name}_{target}']\n                        \n            sns.scatterplot(target_y_true, target_oof_predictions, ax=axes[i][0])\n            sns.distplot(target_oof_predictions, label='Train Predictions', ax=axes[i][1])\n            sns.distplot(target_test_predictions, label='Test Predictions', ax=axes[i][1])\n            \n            axes[i][0].set_xlabel(f'Labels', size=18)\n            axes[i][0].set_ylabel(f'OOF Predictions', size=18)\n            axes[i][1].set_xlabel('')\n            axes[i][1].legend(prop={'size': 18})\n            for j in range(2):\n                axes[i][j].tick_params(axis='x', labelsize=15)\n                axes[i][j].tick_params(axis='y', labelsize=15)\n            axes[i][0].set_title(f'{target} Labels vs OOF Predictions', size=20, pad=20)\n            axes[i][1].set_title(f'{target} Predictions Distributions', size=20, pad=20)\n            \n        plt.show()       \n        \n    def run(self, X_train, y_train, X_test):\n        \n        if self.model == 'RidgeRegression':   \n            print(f'\\n########## Running Ridge Regression Model ##########\\n{52 * \"-\"}')\n            \n            for r in range(self.n_rounds):                  \n                self.cv.random_state = self.random_states[r]\n                print(f'\\nRound {r + 1} - Seed {self.cv.random_state}\\n{(14 + len(str(self.cv.random_state)) + len(str(r))) * \"-\"}')\n                \n                if self.loading_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    loading_feature_count = int(np.ceil(len(loading_features) * self.loading_fraction))\n                    loading_drop_features = list(set(loading_features) - set(np.random.choice(loading_features, loading_feature_count, replace=False)))\n                else:\n                    loading_drop_features = []\n                print(f'Dropped {len(loading_drop_features)}/{len(loading_features)} Random Loading Features')\n                \n                if self.fnc_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    fnc_feature_count = int(np.ceil(len(fnc_features) * self.fnc_fraction))\n                    fnc_drop_features = list(set(fnc_features) - set(np.random.choice(fnc_features, fnc_feature_count, replace=False)))\n                else:\n                    fnc_drop_features = []\n                print(f'Dropped {len(fnc_drop_features)}/{len(fnc_features)} Random FNC Features')\n                \n                if self.statistical_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    statistical_feature_count = int(np.ceil(len(statistical_features) * self.statistical_fraction))\n                    statistical_drop_features = list(set(statistical_features) - set(np.random.choice(statistical_features, statistical_feature_count, replace=False)))\n                else:\n                    statistical_drop_features = []\n                print(f'Dropped {len(statistical_drop_features)}/{len(statistical_features)} Random Statistical Features')\n                \n                if self.boundingbox_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    boundingbox_feature_count = int(np.ceil(len(boundingbox_features) * self.boundingbox_fraction))\n                    boundingbox_drop_features = list(set(boundingbox_features) - set(np.random.choice(boundingbox_features, boundingbox_feature_count, replace=False)))\n                else:\n                    boundingbox_drop_features = []\n                print(f'Dropped {len(boundingbox_drop_features)}/{len(boundingbox_features)} Random Bounding Box Features')\n                \n                drop_features = loading_drop_features + fnc_drop_features + statistical_drop_features + boundingbox_drop_features\n                self.model_features = {\n                    'age': [feature for feature in self.model_features['age'] if feature not in drop_features],\n                    'domain1_var1': [feature for feature in self.model_features['domain1_var1'] if feature not in drop_features],\n                    'domain1_var2': [feature for feature in self.model_features['domain1_var2'] if feature not in drop_features],\n                    'domain2_var1': [feature for feature in self.model_features['domain2_var1'] if feature not in drop_features],\n                    'domain2_var2': [feature for feature in self.model_features['domain2_var2'] if feature not in drop_features]\n                }               \n                \n                self._ridge_train_and_predict(X_train, y_train, X_test, cv=self.cv, r=r)              \n                \n            print(f'\\nFinal Results (Blend of {self.n_rounds} Rounds)\\n{(32 + len(str(self.n_rounds))) * \"-\"}')\n            self.blend_oof_scores = []\n            \n            for target in target_features:\n                blend_oof_nae = self.normalized_absolute_error(df_train.loc[self.target_idx[target], target], df_train.loc[self.target_idx[target], f'{self.model_name}_{target}'])\n                self.blend_oof_scores.append(blend_oof_nae)\n                print(f'Ridge Regression ({target}) Rounds OOF NAE {blend_oof_nae:.6}')\n            weighted_nae = np.sum(np.array(self.blend_oof_scores) * self.target_weights)\n            print(f'{52 * \"-\"}\\nRidge Regression Global Weighted NAE {weighted_nae:.6}')\n                \n        elif self.model == 'SVR':            \n            print(f'\\n########## Running SVR Model ##########\\n{39 * \"-\"}')\n            \n            for r in range(self.n_rounds):                \n                self.cv.random_state = self.random_states[r]\n                print(f'\\nRound {r + 1} - Seed {self.cv.random_state}\\n{(14 + len(str(self.cv.random_state)) + len(str(r))) * \"-\"}')\n                \n                if self.loading_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    loading_feature_count = int(np.ceil(len(loading_features) * self.loading_fraction))\n                    loading_drop_features = list(set(loading_features) - set(np.random.choice(loading_features, loading_feature_count, replace=False)))\n                else:\n                    loading_drop_features = []\n                print(f'Dropped {len(loading_drop_features)}/{len(loading_features)} Random Loading Features')\n                \n                if self.fnc_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    fnc_feature_count = int(np.ceil(len(fnc_features) * self.fnc_fraction))\n                    fnc_drop_features = list(set(fnc_features) - set(np.random.choice(fnc_features, fnc_feature_count, replace=False)))\n                else:\n                    fnc_drop_features = []\n                print(f'Dropped {len(fnc_drop_features)}/{len(fnc_features)} Random FNC Features')\n                \n                if self.statistical_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    statistical_feature_count = int(np.ceil(len(statistical_features) * self.statistical_fraction))\n                    statistical_drop_features = list(set(statistical_features) - set(np.random.choice(statistical_features, statistical_feature_count, replace=False)))\n                else:\n                    statistical_drop_features = []\n                print(f'Dropped {len(statistical_drop_features)}/{len(statistical_features)} Random Statistical Features')\n                \n                if self.boundingbox_fraction < 1.0:\n                    np.random.seed(self.random_states[r])\n                    boundingbox_feature_count = int(np.ceil(len(boundingbox_features) * self.boundingbox_fraction))\n                    boundingbox_drop_features = list(set(boundingbox_features) - set(np.random.choice(boundingbox_features, boundingbox_feature_count, replace=False)))\n                else:\n                    boundingbox_drop_features = []\n                print(f'Dropped {len(boundingbox_drop_features)}/{len(boundingbox_features)} Random Bounding Box Features')\n                \n                drop_features = loading_drop_features + fnc_drop_features + statistical_drop_features + boundingbox_drop_features\n                self.model_features = {\n                    'age': [feature for feature in self.model_features['age'] if feature not in drop_features],\n                    'domain1_var1': [feature for feature in self.model_features['domain1_var1'] if feature not in drop_features],\n                    'domain1_var2': [feature for feature in self.model_features['domain1_var2'] if feature not in drop_features],\n                    'domain2_var1': [feature for feature in self.model_features['domain2_var1'] if feature not in drop_features],\n                    'domain2_var2': [feature for feature in self.model_features['domain2_var2'] if feature not in drop_features]\n                }      \n                \n                self._svr_train_and_predict(X_train, y_train, X_test, cv=self.cv, r=r)\n                \n            print(f'\\nFinal Results (Blend of {self.n_rounds} Rounds)\\n{(32 + len(str(self.n_rounds))) * \"-\"}')\n            self.blend_oof_scores = []\n            \n            for target in target_features:\n                blend_oof_nae = self.normalized_absolute_error(df_train.loc[self.target_idx[target], target], df_train.loc[self.target_idx[target], f'{self.model_name}_{target}'])\n                self.blend_oof_scores.append(blend_oof_nae)\n                print(f'SVR ({target}) Rounds OOF NAE {blend_oof_nae:.6}')\n            weighted_nae = np.sum(np.array(self.blend_oof_scores) * self.target_weights)\n            print(f'{39 * \"-\"}\\nSVR Global Weighted NAE {weighted_nae:.6}')\n            \n        self.plot_regression()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3. Models Trained without FNC Features**\n\nFNC features improve `age` model's score a lot but they don't contribute much to other targets' models. Models trained without FNC features introduce diversity for the predictions of `domain1_var1`, `domain1_var2`, `domain2_var1` and `domain2_var2`. Predictions of the models without FNC Features are not used in the blend of `age`, but they make the 1% of other targets' blends.\n\nStatistical features and bounding box features are scaled by 5000, and ±0.1 random noise added to `age` for increasing cardinality. 10 split regular `KFold` with shuffle is used as the cross-validation technique. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **3.1 Ridge Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train[(loading_features + spatial_features)].copy(deep=True)\ny_train = df_train[target_features].copy(deep=True)\nX_test = df_test[(loading_features + spatial_features)].copy(deep=True)\n\nfor df in [X_train, X_test]:\n    df[statistical_features] /= 5000\n    df[boundingbox_features] /= 5000\n    \nnp.random.seed(SEED)\ny_train['age'] += np.random.normal(loc=0, scale=0.1, size=len(y_train))\n    \nmodel_features = {\n    'age': loading_features + statistical_features + boundingbox_features,\n    'domain1_var1': loading_features + statistical_features,\n    'domain1_var2': loading_features + statistical_features,\n    'domain2_var1': loading_features + statistical_features,\n    'domain2_var2': loading_features + statistical_features\n}\n\nmodel_parameters = {\n    'age': {\n        'alpha': 0.0002\n    },\n    'domain1_var1': {\n        'alpha': 0.0004\n    },\n    'domain1_var2': {\n        'alpha': 0.05\n    },\n    'domain2_var1': {\n        'alpha': 0.00075\n    },\n    'domain2_var2': {\n        'alpha': 0.001\n    }\n}\n\nlinear_model_ensemble_parameters = {\n    'model': 'RidgeRegression',\n    'model_name': 'ridge_regression_loading',\n    'model_features': model_features,\n    'model_parameters': model_parameters,\n    'cv': KFold(n_splits=10, shuffle=True, random_state=SEED),\n    'n_rounds': 5,\n    'loading_fraction': 1,\n    'fnc_fraction': 1,\n    'statistical_fraction': 1,\n    'boundingbox_fraction': 0.95,\n    'verbose': False\n}\n\nlinear_model_ensemble = LinearModelEnsemble(**linear_model_ensemble_parameters)\nlinear_model_ensemble.run(X_train, y_train, X_test)\n\ndel X_train, y_train, X_test, model_features, model_parameters, linear_model_ensemble_parameters, linear_model_ensemble","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.2 SVR**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train[(loading_features + spatial_features)].copy(deep=True)\ny_train = df_train[target_features].copy(deep=True)\nX_test = df_test[(loading_features + spatial_features)].copy(deep=True)\n\nfor df in [X_train, X_test]:\n    df[statistical_features] /= 5000\n    df[boundingbox_features] /= 5000\n    \nnp.random.seed(SEED)\ny_train['age'] += np.random.normal(loc=0, scale=0.1, size=len(y_train))\n    \nmodel_features = {\n    'age': loading_features + statistical_features + boundingbox_features,\n    'domain1_var1': loading_features + statistical_features,\n    'domain1_var2': loading_features + statistical_features,\n    'domain2_var1': loading_features + statistical_features,\n    'domain2_var2': loading_features + statistical_features\n}\n\nmodel_parameters = {\n    'age': {\n        'C': 100,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain1_var1': {\n        'C': 2,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain1_var2': {\n        'C': 1,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain2_var1': {\n        'C': 3,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain2_var2': {\n        'C': 9,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    }\n}\n\nlinear_model_ensemble_parameters = {\n    'model': 'SVR',\n    'model_name': 'svr_loading',\n    'model_features': model_features,\n    'model_parameters': model_parameters,\n    'cv': KFold(n_splits=10, shuffle=True, random_state=None),\n    'n_rounds': 5,\n    'loading_fraction': 1,\n    'fnc_fraction': 1,\n    'statistical_fraction': 1,\n    'boundingbox_fraction': 0.95,\n    'verbose': False\n}\n\nlinear_model_ensemble = LinearModelEnsemble(**linear_model_ensemble_parameters)\nlinear_model_ensemble.run(X_train, y_train, X_test)\n\ndel X_train, y_train, X_test, model_features, model_parameters, linear_model_ensemble_parameters, linear_model_ensemble","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4. Models Trained with FNC Features**\n\nModels trained with FNC features are the main models of the ensemble. They make 99% of `domain1_var1`, `domain1_var2`, `domain2_var1` and `domain2_var2` predictions, and all of `age` predictions.\n\nFNC features are scaled by 500, statistical features and bounding box features are scaled by 5000, and ±0.1 random noise added to `age` for increasing cardinality. 10 split regular `KFold` with shuffle is used as the cross-validation technique. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **4.1 Ridge Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train[(loading_features + fnc_features + spatial_features)].copy(deep=True)\ny_train = df_train[target_features].copy(deep=True)\nX_test = df_test[(loading_features + fnc_features + spatial_features)].copy(deep=True)\n\nfor df in [X_train, X_test]:\n    df[fnc_features] /= 500\n    df[statistical_features] /= 5000\n    df[boundingbox_features] /= 5000\n\nnp.random.seed(SEED)\ny_train['age'] += np.random.normal(loc=0, scale=0.1, size=len(y_train))\n    \nmodel_features = {\n    'age': loading_features + fnc_features + statistical_features + boundingbox_features,\n    'domain1_var1': loading_features + fnc_features + statistical_features,\n    'domain1_var2': loading_features + fnc_features + statistical_features,\n    'domain2_var1': loading_features + fnc_features + statistical_features,\n    'domain2_var2': loading_features + fnc_features + statistical_features \n}\n\nmodel_parameters = {\n    'age': {\n        'alpha': 0.0004\n    },\n    'domain1_var1': {\n        'alpha': 0.004\n    },\n    'domain1_var2': {\n        'alpha': 0.02\n    },\n    'domain2_var1': {\n        'alpha': 0.002\n    },\n    'domain2_var2': {\n        'alpha': 0.003\n    }\n}\n\nlinear_model_ensemble_parameters = {\n    'model': 'RidgeRegression',\n    'model_name': 'ridge_regression_all',\n    'model_features': model_features,\n    'model_parameters': model_parameters,\n    'cv': KFold(n_splits=10, shuffle=True, random_state=SEED),\n    'n_rounds': 5,\n    'loading_fraction': 1,\n    'fnc_fraction': 0.95,\n    'statistical_fraction': 1,\n    'boundingbox_fraction': 1,\n    'verbose': False\n}\n\nlinear_model_ensemble = LinearModelEnsemble(**linear_model_ensemble_parameters)\nlinear_model_ensemble.run(X_train, y_train, X_test)\n\ndel X_train, y_train, X_test, model_features, model_parameters, linear_model_ensemble_parameters, linear_model_ensemble","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.2 SVR**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train[(loading_features + fnc_features + spatial_features)].copy(deep=True)\ny_train = df_train[target_features].copy(deep=True)\nX_test = df_test[(loading_features + fnc_features + spatial_features)].copy(deep=True)\n\nfor df in [X_train, X_test]:\n    df[fnc_features] /= 500\n    df[statistical_features] /= 5000\n    df[boundingbox_features] /= 5000   \n    \nnp.random.seed(SEED)\ny_train['age'] += np.random.normal(loc=0, scale=0.1, size=len(y_train))\n    \nmodel_features = {\n    'age': loading_features + fnc_features + statistical_features + boundingbox_features,\n    'domain1_var1': loading_features + fnc_features + statistical_features,\n    'domain1_var2': loading_features + fnc_features + statistical_features,\n    'domain2_var1': loading_features + fnc_features + statistical_features,\n    'domain2_var2': loading_features + fnc_features + statistical_features\n}\n\nmodel_parameters = {\n    'age': {\n        'C': 72,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain1_var1': {\n        'C': 7,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain1_var2': {\n        'C': 1,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain2_var1': {\n        'C': 10,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    },\n    'domain2_var2': {\n        'C': 10,\n        'kernel': 'rbf',\n        'cache_size': 3000\n    }\n}\n\nlinear_model_ensemble_parameters = {\n    'model': 'SVR',\n    'model_name': 'svr_all',\n    'model_features': model_features,\n    'model_parameters': model_parameters,\n    'cv': KFold(n_splits=10, shuffle=True, random_state=SEED),\n    'n_rounds': 5,\n    'loading_fraction': 1,\n    'fnc_fraction': 1,\n    'statistical_fraction': 1,\n    'boundingbox_fraction': 1,\n    'verbose': False\n}\n\nlinear_model_ensemble = LinearModelEnsemble(**linear_model_ensemble_parameters)\nlinear_model_ensemble.run(X_train, y_train, X_test)\n\ndel X_train, y_train, X_test, model_features, model_parameters, linear_model_ensemble_parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5. Evaluation and Blending**\n\nThere are 2 (2 times `n_rounds`) models used in blend of `age`, and those models are `Ridge` and `SVR` trained with FNC features. Those models' predictions are blended equally (0.5 + 0.5).\n\nThere are 4 (4 times `n_rounds`) models used in blends of other targets, and those models are `Ridge` and `SVR` trained with and without FNC features. Models trained without FNC are blended equally (0.05 + 0.05) which makes up to 1%, and models trained with FNC are also blended equally (0.495 + 0.495) which makes up 99%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalized_absolute_error(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred)) / np.sum(y_true))\n\ntarget_weights = np.array([0.3, 0.175, 0.175, 0.175, 0.175])\nridge_loading_scores = []\nsvr_loading_scores = []\nridge_all_scores = []\nsvr_all_scores = []\nblend_scores = []\n\nprint('---------- age ----------\\n')\n\nage_labels = df_train[~df_train['age'].isnull()]['age']\n\nridge_loading_age_oof_score = normalized_absolute_error(age_labels, df_train.loc[~df_train['age'].isnull()]['ridge_regression_loading_age'])\nsvr_loading_age_oof_score = normalized_absolute_error(age_labels, df_train.loc[~df_train['age'].isnull()]['svr_loading_age'])\nridge_all_age_oof_score = normalized_absolute_error(age_labels, df_train.loc[~df_train['age'].isnull()]['ridge_regression_all_age'])\nsvr_all_age_oof_score = normalized_absolute_error(age_labels, df_train.loc[~df_train['age'].isnull()]['svr_all_age'])\n\ndf_train.loc[~df_train['age'].isnull(), 'blend_age'] = (df_train.loc[~df_train['age'].isnull(), 'ridge_regression_loading_age'] * 0) +\\\n                                                       (df_train.loc[~df_train['age'].isnull(), 'svr_loading_age'] * 0) +\\\n                                                       (df_train.loc[~df_train['age'].isnull(), 'ridge_regression_all_age'] * 0.5) +\\\n                                                       (df_train.loc[~df_train['age'].isnull(), 'svr_all_age'] * 0.5)\n\ndf_test['blend_age'] = (df_test['ridge_regression_loading_age'] * 0) +\\\n                       (df_test['svr_loading_age'] * 0) +\\\n                       (df_test['ridge_regression_all_age'] * 0.5) +\\\n                       (df_test['svr_all_age'] * 0.5)          \n\nblend_age_oof_score = normalized_absolute_error(age_labels, df_train[~df_train['age'].isnull()]['blend_age'])\n\nridge_loading_scores.append(ridge_loading_age_oof_score)\nsvr_loading_scores.append(svr_loading_age_oof_score)\nridge_all_scores.append(ridge_all_age_oof_score)\nsvr_all_scores.append(svr_all_age_oof_score)\nblend_scores.append(blend_age_oof_score)\n\nprint(f'Ridge Regression Loading (age) OOF NAE {ridge_loading_age_oof_score:.6}')\nprint(f'SVR Loading (age) OOF NAE {svr_loading_age_oof_score:.6}')\nprint(f'Ridge Regression All (age) OOF NAE {ridge_all_age_oof_score:.6}')\nprint(f'SVR (age) All OOF NAE {svr_all_age_oof_score:.6}')\nprint(f'Blend (age) OOF NAE {blend_age_oof_score:.6}')\n\nprint('\\n---------- domain1_var1 ----------\\n')\n\ndomain1_var1_labels = df_train[~df_train['domain1_var1'].isnull()]['domain1_var1']\n\nridge_loading_domain1_var1_oof_score = normalized_absolute_error(domain1_var1_labels, df_train.loc[~df_train['domain1_var1'].isnull()]['ridge_regression_loading_domain1_var1'])\nsvr_loading_domain1_var1_oof_score = normalized_absolute_error(domain1_var1_labels, df_train.loc[~df_train['domain1_var1'].isnull()]['svr_loading_domain1_var1'])\nridge_all_domain1_var1_oof_score = normalized_absolute_error(domain1_var1_labels, df_train.loc[~df_train['domain1_var1'].isnull()]['ridge_regression_all_domain1_var1'])\nsvr_all_domain1_var1_oof_score = normalized_absolute_error(domain1_var1_labels, df_train.loc[~df_train['domain1_var1'].isnull()]['svr_all_domain1_var1'])\n\ndf_train.loc[~df_train['domain1_var1'].isnull(), 'blend_domain1_var1'] = (df_train.loc[~df_train['domain1_var1'].isnull(), 'ridge_regression_loading_domain1_var1'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain1_var1'].isnull(), 'svr_loading_domain1_var1'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain1_var1'].isnull(), 'ridge_regression_all_domain1_var1'] * 0.495) +\\\n                                                                         (df_train.loc[~df_train['domain1_var1'].isnull(), 'svr_all_domain1_var1'] * 0.495)\n\ndf_test['blend_domain1_var1'] = (df_test['ridge_regression_loading_domain1_var1'] * 0.005) +\\\n                                (df_test['svr_loading_domain1_var1'] * 0.005) +\\\n                                (df_test['ridge_regression_all_domain1_var1'] * 0.495) +\\\n                                (df_test['svr_all_domain1_var1'] * 0.495)          \n\nblend_domain1_var1_oof_score = normalized_absolute_error(domain1_var1_labels, df_train[~df_train['domain1_var1'].isnull()]['blend_domain1_var1'])\n\nridge_loading_scores.append(ridge_loading_domain1_var1_oof_score)\nsvr_loading_scores.append(svr_loading_domain1_var1_oof_score)\nridge_all_scores.append(ridge_all_domain1_var1_oof_score)\nsvr_all_scores.append(svr_all_domain1_var1_oof_score)\nblend_scores.append(blend_domain1_var1_oof_score)\n\nprint(f'Ridge Regression Loading (domain1_var1) OOF NAE {ridge_loading_domain1_var1_oof_score:.6}')\nprint(f'SVR Loading (domain1_var1) OOF NAE {svr_loading_domain1_var1_oof_score:.6}')\nprint(f'Ridge Regression All (domain1_var1) OOF NAE {ridge_all_domain1_var1_oof_score:.6}')\nprint(f'SVR (domain1_var1) All OOF NAE {svr_all_domain1_var1_oof_score:.6}')\nprint(f'Blend (domain1_var1) OOF NAE {blend_domain1_var1_oof_score:.6}')\n\nprint('\\n---------- domain1_var2 ----------\\n')\n\ndomain1_var2_labels = df_train[~df_train['domain1_var2'].isnull()]['domain1_var2']\n\nridge_loading_domain1_var2_oof_score = normalized_absolute_error(domain1_var2_labels, df_train.loc[~df_train['domain1_var2'].isnull()]['ridge_regression_loading_domain1_var2'])\nsvr_loading_domain1_var2_oof_score = normalized_absolute_error(domain1_var2_labels, df_train.loc[~df_train['domain1_var2'].isnull()]['svr_loading_domain1_var2'])\nridge_all_domain1_var2_oof_score = normalized_absolute_error(domain1_var2_labels, df_train.loc[~df_train['domain1_var2'].isnull()]['ridge_regression_all_domain1_var2'])\nsvr_all_domain1_var2_oof_score = normalized_absolute_error(domain1_var2_labels, df_train.loc[~df_train['domain1_var2'].isnull()]['svr_all_domain1_var2'])\n\ndf_train.loc[~df_train['domain1_var2'].isnull(), 'blend_domain1_var2'] = (df_train.loc[~df_train['domain1_var2'].isnull(), 'ridge_regression_loading_domain1_var2'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain1_var2'].isnull(), 'svr_loading_domain1_var2'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain1_var2'].isnull(), 'ridge_regression_all_domain1_var2'] * 0.495) +\\\n                                                                         (df_train.loc[~df_train['domain1_var2'].isnull(), 'svr_all_domain1_var2'] * 0.495)\n\ndf_test['blend_domain1_var2'] = (df_test['ridge_regression_loading_domain1_var2'] * 0.005) +\\\n                                (df_test['svr_loading_domain1_var2'] * 0.005) +\\\n                                (df_test['ridge_regression_all_domain1_var2'] * 0.495) +\\\n                                (df_test['svr_all_domain1_var2'] * 0.495)          \n\nblend_domain1_var2_oof_score = normalized_absolute_error(domain1_var2_labels, df_train[~df_train['domain1_var2'].isnull()]['blend_domain1_var2'])\n\nridge_loading_scores.append(ridge_loading_domain1_var2_oof_score)\nsvr_loading_scores.append(svr_loading_domain1_var2_oof_score)\nridge_all_scores.append(ridge_all_domain1_var2_oof_score)\nsvr_all_scores.append(svr_all_domain1_var2_oof_score)\nblend_scores.append(blend_domain1_var2_oof_score)\n\nprint(f'Ridge Regression Loading (domain1_var2) OOF NAE {ridge_loading_domain1_var2_oof_score:.6}')\nprint(f'SVR Loading (domain1_var2) OOF NAE {svr_loading_domain1_var2_oof_score:.6}')\nprint(f'Ridge Regression All (domain1_var2) OOF NAE {ridge_all_domain1_var2_oof_score:.6}')\nprint(f'SVR (domain1_var2) All OOF NAE {svr_all_domain1_var2_oof_score:.6}')\nprint(f'Blend (domain1_var2) OOF NAE {blend_domain1_var2_oof_score:.6}')\n\nprint('\\n---------- domain2_var1 ----------\\n')\n\ndomain2_var1_labels = df_train[~df_train['domain2_var1'].isnull()]['domain2_var1']\n\nridge_loading_domain2_var1_oof_score = normalized_absolute_error(domain2_var1_labels, df_train.loc[~df_train['domain2_var1'].isnull()]['ridge_regression_loading_domain2_var1'])\nsvr_loading_domain2_var1_oof_score = normalized_absolute_error(domain2_var1_labels, df_train.loc[~df_train['domain2_var1'].isnull()]['svr_loading_domain2_var1'])\nridge_all_domain2_var1_oof_score = normalized_absolute_error(domain2_var1_labels, df_train.loc[~df_train['domain2_var1'].isnull()]['ridge_regression_all_domain2_var1'])\nsvr_all_domain2_var1_oof_score = normalized_absolute_error(domain2_var1_labels, df_train.loc[~df_train['domain2_var1'].isnull()]['svr_all_domain2_var1'])\n\ndf_train.loc[~df_train['domain2_var1'].isnull(), 'blend_domain2_var1'] = (df_train.loc[~df_train['domain2_var1'].isnull(), 'ridge_regression_loading_domain2_var1'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain2_var1'].isnull(), 'svr_loading_domain2_var1'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain2_var1'].isnull(), 'ridge_regression_all_domain2_var1'] * 0.495) +\\\n                                                                         (df_train.loc[~df_train['domain2_var1'].isnull(), 'svr_all_domain2_var1'] * 0.495)\n\ndf_test['blend_domain2_var1'] = (df_test['ridge_regression_loading_domain2_var1'] * 0.005) +\\\n                                (df_test['svr_loading_domain2_var1'] * 0.005) +\\\n                                (df_test['ridge_regression_all_domain2_var1'] * 0.495) +\\\n                                (df_test['svr_all_domain2_var1'] * 0.495)          \n\nblend_domain2_var1_oof_score = normalized_absolute_error(domain2_var1_labels, df_train[~df_train['domain2_var1'].isnull()]['blend_domain2_var1'])\n\nridge_loading_scores.append(ridge_loading_domain2_var1_oof_score)\nsvr_loading_scores.append(svr_loading_domain2_var1_oof_score)\nridge_all_scores.append(ridge_all_domain2_var1_oof_score)\nsvr_all_scores.append(svr_all_domain2_var1_oof_score)\nblend_scores.append(blend_domain2_var1_oof_score)\n\nprint(f'Ridge Regression Loading (domain2_var1) OOF NAE {ridge_loading_domain2_var1_oof_score:.6}')\nprint(f'SVR Loading (domain2_var1) OOF NAE {svr_loading_domain2_var1_oof_score:.6}')\nprint(f'Ridge Regression All (domain2_var1) OOF NAE {ridge_all_domain2_var1_oof_score:.6}')\nprint(f'SVR (domain2_var1) All OOF NAE {svr_all_domain2_var1_oof_score:.6}')\nprint(f'Blend (domain2_var1) OOF NAE {blend_domain2_var1_oof_score:.6}')\n\nprint('\\n---------- domain2_var2 ----------\\n')\n\ndomain2_var2_labels = df_train[~df_train['domain2_var2'].isnull()]['domain2_var2']\n\nridge_loading_domain2_var2_oof_score = normalized_absolute_error(domain2_var2_labels, df_train.loc[~df_train['domain2_var2'].isnull()]['ridge_regression_loading_domain2_var2'])\nsvr_loading_domain2_var2_oof_score = normalized_absolute_error(domain2_var2_labels, df_train.loc[~df_train['domain2_var2'].isnull()]['svr_loading_domain2_var2'])\nridge_all_domain2_var2_oof_score = normalized_absolute_error(domain2_var2_labels, df_train.loc[~df_train['domain2_var2'].isnull()]['ridge_regression_all_domain2_var2'])\nsvr_all_domain2_var2_oof_score = normalized_absolute_error(domain2_var2_labels, df_train.loc[~df_train['domain2_var2'].isnull()]['svr_all_domain2_var2'])\n\ndf_train.loc[~df_train['domain2_var2'].isnull(), 'blend_domain2_var2'] = (df_train.loc[~df_train['domain2_var2'].isnull(), 'ridge_regression_loading_domain2_var2'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain2_var2'].isnull(), 'svr_loading_domain2_var2'] * 0.005) +\\\n                                                                         (df_train.loc[~df_train['domain2_var2'].isnull(), 'ridge_regression_all_domain2_var2'] * 0.495) +\\\n                                                                         (df_train.loc[~df_train['domain2_var2'].isnull(), 'svr_all_domain2_var2'] * 0.495)\n\ndf_test['blend_domain2_var2'] = (df_test['ridge_regression_loading_domain2_var2'] * 0.005) +\\\n                                (df_test['svr_loading_domain2_var2'] * 0.005) +\\\n                                (df_test['ridge_regression_all_domain2_var2'] * 0.495) +\\\n                                (df_test['svr_all_domain2_var2'] * 0.495)          \n\nblend_domain2_var2_oof_score = normalized_absolute_error(domain2_var2_labels, df_train[~df_train['domain2_var2'].isnull()]['blend_domain2_var2'])\n\nridge_loading_scores.append(ridge_loading_domain2_var2_oof_score)\nsvr_loading_scores.append(svr_loading_domain2_var2_oof_score)\nridge_all_scores.append(ridge_all_domain2_var2_oof_score)\nsvr_all_scores.append(svr_all_domain2_var2_oof_score)\nblend_scores.append(blend_domain2_var2_oof_score)\n\nprint(f'Ridge Regression Loading (domain2_var2) OOF NAE {ridge_loading_domain2_var2_oof_score:.6}')\nprint(f'SVR Loading (domain2_var2) OOF NAE {svr_loading_domain2_var2_oof_score:.6}')\nprint(f'Ridge Regression All (domain2_var2) OOF NAE {ridge_all_domain2_var2_oof_score:.6}')\nprint(f'SVR (domain2_var2) All OOF NAE {svr_all_domain2_var2_oof_score:.6}')\nprint(f'Blend (domain2_var2) OOF NAE {blend_domain2_var2_oof_score:.6}')\n\nprint('\\n---------- Final Score ----------\\n')\n\nridge_loading_weighted_nae = np.sum(np.array(ridge_loading_scores) * target_weights)\nsvr_loading_weighted_nae = np.sum(np.array(svr_loading_scores) * target_weights)\nridge_all_weighted_nae = np.sum(np.array(ridge_all_scores) * target_weights)\nsvr_all_weighted_nae = np.sum(np.array(svr_all_scores) * target_weights)\nblend_weighted_nae = np.sum(np.array(blend_scores) * target_weights)\n\nprint(f'Ridge Regression Loading Global Weighted NAE {ridge_loading_weighted_nae:.6}')\nprint(f'SVR Loading Global Weighted NAE {svr_loading_weighted_nae:.6}')\nprint(f'Ridge Regression All Global Weighted NAE {ridge_all_weighted_nae:.6}')\nprint(f'SVR All Weighted NAE {svr_all_weighted_nae:.6}')\nprint(f'Blend Global Weighted NAE {blend_weighted_nae:.6}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(25, 50))\n\nfor i, target in enumerate(['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']):\n\n    target_oof_predictions = df_train.loc[linear_model_ensemble.target_idx[target], f'blend_{target}']\n    target_y_true = df_train.loc[linear_model_ensemble.target_idx[target], target]  \n    target_test_predictions = df_test.loc[:, f'blend_{target}']\n\n    sns.scatterplot(target_y_true, target_oof_predictions, ax=axes[i][0])\n    sns.distplot(target_oof_predictions, label=f'{target} Train Predictions', ax=axes[i][1])\n    sns.distplot(target_test_predictions, label=f'{target} Test Predictions', ax=axes[i][1])\n\n    axes[i][0].set_xlabel(f'Labels', size=18)\n    axes[i][0].set_ylabel(f'OOF Predictions', size=18)\n    axes[i][1].set_xlabel('')\n    axes[i][1].legend(prop={'size': 18})\n    for j in range(2):\n        axes[i][j].tick_params(axis='x', labelsize=15)\n        axes[i][j].tick_params(axis='y', labelsize=15)\n    axes[i][0].set_title(f'{target} Blend Labels vs OOF Predictions', size=20, pad=20)\n    axes[i][1].set_title(f'{target} Blend Predictions Distributions', size=20, pad=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that most of the error is coming from the first decile in all target features. This problem is related to both competition metric, normalized absolute error and target distributions. Values closer to target means are easier to predict, so error is smaller in those deciles, however, values closer to distribution tails are harder to predict so error is greater in first and last deciles.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score_by_deciles(target, predictions):\n    y_true = df_train[target]\n    y_pred = df_train[predictions]\n    \n    print(f'{target} NAE in Deciles\\n{(\"-\" * (15 + len(target)))}')\n    \n    deciles = np.arange(0, 1, 0.1)\n    \n    for decile in deciles:\n        decile_lower_bound = y_true.quantile(decile)\n        decile_upper_bound = y_true.quantile(decile + 0.1)\n        \n        decile_y_true = y_true.loc[(y_true >= decile_lower_bound) & (y_true < decile_upper_bound)]\n        decile_y_pred = y_pred.loc[(y_true >= decile_lower_bound) & (y_true < decile_upper_bound)]\n        \n        decile_nae = normalized_absolute_error(decile_y_true, decile_y_pred)        \n        print(f'Decile: {decile:.6}-{decile + 0.1:.6} ({decile_lower_bound:.6}-{decile_upper_bound:.6}) NAE {decile_nae:.6}')\n    global_nae = normalized_absolute_error(y_true, y_pred)   \n    print(f'{(\"-\" * (15 + len(target)))}\\n{target} Global NAE {global_nae:.6}\\n')\n\nfor target in target_features:\n    get_score_by_deciles(target, f'blend_{target}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_mapping = {'blend_age': 'age', \n                  'blend_domain1_var1': 'domain1_var1',\n                  'blend_domain1_var2': 'domain1_var2',\n                  'blend_domain2_var1': 'domain2_var1', \n                  'blend_domain2_var2': 'domain2_var2'}\n\ndf_submission = pd.melt(df_test.rename(columns=column_mapping)[['Id', 'age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']], id_vars=['Id'], value_name='Predicted')\ndf_submission['Id'] = df_submission['Id'].astype('str') + '_' +  df_submission['variable'].astype('str')\ndf_submission = df_submission.drop(columns=['variable']).sort_values('Id')\ndf_submission.to_csv('linear_model_blend_submission.csv', index=False)\n\ndf_submission.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}