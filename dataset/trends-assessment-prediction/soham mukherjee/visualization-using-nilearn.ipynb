{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!wget https://github.com/Chaogan-Yan/DPABI/raw/master/Templates/ch2better.nii","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\nimport h5py\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nmask_filename = '../input/trends-assessment-prediction/fMRI_mask.nii'\nsubject_filename = '../input/trends-assessment-prediction/fMRI_train/10004.mat'\nsmri_filename = 'ch2better.nii'\nmask_niimg = nl.image.load_img(mask_filename)\n\ndef load_subject(filename, mask_niimg):\n    \"\"\"\n    Load a subject saved in .mat format with\n        the version 7.3 flag. Return the subject\n        niimg, using a mask niimg as a template\n        for nifti headers.\n        \n    Args:\n        filename    <str>            the .mat filename for the subject data\n        mask_niimg  niimg object     the mask niimg object used for nifti headers\n    \"\"\"\n    subject_data = None\n    with h5py.File(subject_filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    return subject_niimg\nsubject_niimg = load_subject(subject_filename, mask_niimg)\nprint(\"Image shape is %s\" % (str(subject_niimg.shape)))\nnum_components = subject_niimg.shape[-1]\nprint(\"Detected {num_components} spatial maps\".format(num_components=num_components))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nlplt.plot_prob_atlas(subject_niimg, bg_img=smri_filename, view_type='filled_contours', draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"grid_size = int(np.ceil(np.sqrt(num_components)))\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*10, grid_size*10))\n[axi.set_axis_off() for axi in axes.ravel()]\nrow = -1\nfor i, cur_img in enumerate(nl.image.iter_img(subject_niimg)):\n    col = i % grid_size\n    if col == 0:\n        row += 1\n    nlplt.plot_stat_map(cur_img, bg_img=smri_filename, title=\"IC %d\" % i, axes=axes[row, col], threshold=3, colorbar=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom nilearn import datasets\nhaxby_dataset = datasets.fetch_haxby()   # load dataset\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\n# Build the mean image because we have no anatomic data\nfrom nilearn import image\nfunc_filename = haxby_dataset.func[0]\nmean_img = image.mean_img(func_filename)\n\nz_slice = -14\n\nfig = plt.figure(figsize=(4, 5.4), facecolor='k')\n\nfrom nilearn.plotting import plot_anat, show\ndisplay = plot_anat(mean_img, display_mode='z', cut_coords=[z_slice],\n                    figure=fig)\nmask_vt_filename = haxby_dataset.mask_vt[0]\nmask_house_filename = haxby_dataset.mask_house[0]\nmask_face_filename = haxby_dataset.mask_face[0]\ndisplay.add_contours(mask_vt_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['red'])\ndisplay.add_contours(mask_house_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['blue'])\ndisplay.add_contours(mask_face_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['limegreen'])\n\n# We generate a legend using the trick described on\n# http://matplotlib.sourceforge.net/users/legend_guide.httpml#using-proxy-artist\nfrom matplotlib.patches import Rectangle\np_v = Rectangle((0, 0), 1, 1, fc=\"red\")\np_h = Rectangle((0, 0), 1, 1, fc=\"blue\")\np_f = Rectangle((0, 0), 1, 1, fc=\"limegreen\")\nplt.legend([p_v, p_h, p_f], [\"vt\", \"house\", \"face\"])\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn import datasets\n\n# haxby dataset to have EPI images and masks\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# one motor contrast map from NeuroVault\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn import plotting\n\n# Visualizing t-map image on EPI template with manual\n# positioning of coordinates using cut_coords given as a list\nplotting.plot_stat_map(stat_img,\n                       threshold=3, title=\"plot_stat_map\",\n                       cut_coords=[36, -27, 66])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"view = plotting.view_img(stat_img, threshold=3)\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_glass_brain(stat_img, title='plot_glass_brain',\n                          threshold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_anat(haxby_anat_filename, title=\"plot_anat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_roi(haxby_mask_filename, bg_img=haxby_anat_filename,\n                  title=\"plot_roi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import image processing tool\nfrom nilearn import image\n\n# Compute the voxel_wise mean of functional images across time.\n# Basically reducing the functional image from 4D to 3D\nmean_haxby_img = image.mean_img(haxby_func_filename)\n\n# Visualizing mean image (3D)\nplotting.plot_epi(mean_haxby_img, title=\"plot_epi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# haxby dataset to have anatomical image, EPI images and masks\n#haxby_dataset = datasets.fetch_haxby()\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# localizer dataset to have contrast maps\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='ortho',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='ortho', cut_coords=[36, -27, 60]\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='z', cut_coords=5,\n                       title=\"display_mode='z', cut_coords=5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='x',\n                       cut_coords=[-36, 36],\n                       title=\"display_mode='x', cut_coords=[-36, 36]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='y', cut_coords=1,\n                       title=\"display_mode='y', cut_coords=1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='z',\n                       cut_coords=1, colorbar=False,\n                       title=\"display_mode='z', cut_coords=1, colorbar=False\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='xz',\n                       cut_coords=[36, 60],\n                       title=\"display_mode='xz', cut_coords=[36, 60]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='yx',\n                       cut_coords=[-27, 36],\n                       title=\"display_mode='yx', cut_coords=[-27, 36]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='yz',\n                       cut_coords=[-27, 60],\n                       title=\"display_mode='yz', cut_coords=[-27, 60]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='tiled',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='tiled'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from nilearn import image\n\n# Compute voxel-wise mean functional image across time dimension. Now we have\n# functional image in 3D assigned in mean_haxby_img\nmean_haxby_img = image.mean_img(haxby_func_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img, title=\"add_edges\")\n\n# We are now able to use add_edges method inherited in plotting object named as\n# display. First argument - anatomical image  and by default edges will be\n# displayed as red 'r', to choose different colors green 'g' and  blue 'b'.\ndisplay.add_edges(haxby_anat_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# As seen before, we call the `plot_anat` function with a background image\n# as first argument, in this case again the mean fMRI image and argument\n# `cut_coords` as list for manual cut with coordinates pointing at masked\n# brain regions\ndisplay = plotting.plot_anat(mean_haxby_img, title=\"add_contours\",\n                             cut_coords=[-34, -39, -9])\n# Now use `add_contours` in display object with the path to a mask image from\n# the Haxby dataset as first argument and argument `levels` given as list\n# of values to select particular level in the contour to display and argument\n# `colors` specified as red 'r' to see edges as red in color.\n# See help on matplotlib.pyplot.contour to use more options with this method\ndisplay.add_contours(haxby_mask_filename, levels=[0.5], colors='r')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"add_contours with filled=True\",\n                             cut_coords=[-34, -39, -9])\n\n# By default, no color fillings will be shown using `add_contours`. To see\n# contours with color fillings use argument filled=True. contour colors are\n# changed to blue 'b' with alpha=0.7 sets the transparency of color fillings.\n# See help on matplotlib.pyplot.contourf to use more options given that filled\n# should be True\ndisplay.add_contours(haxby_mask_filename, filled=True, alpha=0.7,\n                     levels=[0.5], colors='b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img, title=\"add_markers\",\n                             cut_coords=[-34, -39, -9])\n\n# Coordinates of seed regions should be specified in first argument and second\n# argument `marker_color` denotes color of the sphere in this case yellow 'y'\n# and third argument `marker_size` denotes size of the sphere\ncoords = [(-34, -39, -9)]\ndisplay.add_markers(coords, marker_color='y', marker_size=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True, scale_size=25, scale_units='mm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Fetch brain development functional datasets"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from nilearn import datasets\n\nrest_dataset = datasets.fetch_development_fmri(n_subjects=20)\nfunc_filenames = rest_dataset.func\nconfounds = rest_dataset.confounds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import dictionary learning algorithm from decomposition module and call the\n# object and fit the model to the functional datasets\nfrom nilearn.decomposition import DictLearning\n\n# Initialize DictLearning object\ndict_learn = DictLearning(n_components=8, smoothing_fwhm=6.,\n                          memory=\"nilearn_cache\", memory_level=2,\n                          random_state=0)\n# Fit to the data\ndict_learn.fit(func_filenames)\n# Resting state networks/maps in attribute `components_img_`\n# Note that this attribute is implemented from version 0.4.1.\n# For older versions, see the note section above for details.\ncomponents_img = dict_learn.components_img_\n\n# Visualization of functional networks\n# Show networks using plotting utilities\nfrom nilearn import plotting\n\nplotting.plot_prob_atlas(components_img, view_type='filled_contours',\n                         title='Dictionary Learning maps')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# Import Region Extractor algorithm from regions module\n# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n# maps, less the threshold means that more intense non-voxels will be survived.\nfrom nilearn.regions import RegionExtractor\n\nextractor = RegionExtractor(components_img, threshold=0.5,\n                            thresholding_strategy='ratio_n_voxels',\n                            extractor='local_regions',\n                            standardize=True, min_region_size=1350)\n# Just call fit() to process for regions extraction\nextractor.fit()\n# Extracted regions are stored in regions_img_\nregions_extracted_img = extractor.regions_img_\n# Each region index is stored in index_\nregions_index = extractor.index_\n# Total number of regions extracted\nn_regions_extracted = regions_extracted_img.shape[-1]\n\n# Visualization of region extraction results\ntitle = ('%d regions are extracted from %d components.'\n         '\\nEach separate color of region indicates extracted region'\n         % (n_regions_extracted, 8))\nplotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n                         title=title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# First we need to do subjects timeseries signals extraction and then estimating\n# correlation matrices on those signals.\n# To extract timeseries signals, we call transform() from RegionExtractor object\n# onto each subject functional data stored in func_filenames.\n# To estimate correlation matrices we import connectome utilities from nilearn\nfrom nilearn.connectome import ConnectivityMeasure\n\ncorrelations = []\n# Initializing ConnectivityMeasure object with kind='correlation'\nconnectome_measure = ConnectivityMeasure(kind='correlation')\nfor filename, confound in zip(func_filenames, confounds):\n    # call transform from RegionExtractor object to extract timeseries signals\n    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n    # call fit_transform from ConnectivityMeasure object\n    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n    # saving each subject correlation to correlations\n    correlations.append(correlation)\n\n# Mean of all correlations\nimport numpy as np\nmean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n                                                          n_regions_extracted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"title = 'Correlation between %d regions' % n_regions_extracted\n\n# First plot the matrix\ndisplay = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n                               colorbar=True, title=title)\n\n# Then find the center of the regions and plot a connectome\nregions_img = regions_extracted_img\ncoords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n\nplotting.plot_connectome(mean_correlations, coords_connectome,\n                         edge_threshold='90%', title=title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# First, we plot a network of index=4 without region extraction (left plot)\nfrom nilearn import image\n\nimg = image.index_img(components_img, 4)\ncoords = plotting.find_xyz_cut_coords(img)\ndisplay = plotting.plot_stat_map(img, cut_coords=coords, colorbar=False,\n                                 title='Showing one specific network')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# For this, we take the indices of the all regions extracted related to original\n# network given as 4.\nregions_indices_of_map3 = np.where(np.array(regions_index) == 4)\n\ndisplay = plotting.plot_anat(cut_coords=coords,\n                             title='Regions from this network')\n\n# Add as an overlay all the regions of index 4\ncolors = 'rgbcmyk'\nfor each_index_of_map3, color in zip(regions_indices_of_map3[0], colors):\n    display.add_overlay(image.index_img(regions_extracted_img, each_index_of_map3),\n                        cmap=plotting.cm.alpha_cmap(color))\n\nplotting.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from nilearn import datasets\n\n# By default 2nd subject will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First anatomical nifti image (3D) located is at: %s' %\n      haxby_dataset.anat[0])\nprint('First functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn.image.image import mean_img\n\n# Compute the mean EPI: we do the mean along the axis 3, which is time\nfunc_filename = haxby_dataset.func[0]\nmean_haxby = mean_img(func_filename)\n\nfrom nilearn.plotting import plot_epi, show\nplot_epi(mean_haxby)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn.masking import compute_epi_mask\nmask_img = compute_epi_mask(func_filename)\n\n# Visualize it as an ROI\nfrom nilearn.plotting import plot_roi\nplot_roi(mask_img, mean_haxby)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn.masking import apply_mask\nmasked_data = apply_mask(func_filename, mask_img)\n\n# masked_data shape is (timepoints, voxels). We can plot the first 150\n# timepoints from two voxels\n\n# And now plot a few of these\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(7, 5))\nplt.plot(masked_data[:150, :2])\nplt.xlabel('Time [TRs]', fontsize=16)\nplt.ylabel('Intensity', fontsize=16)\nplt.xlim(0, 150)\nplt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn import datasets\n\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fsaverage = datasets.fetch_surf_fsaverage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn import surface\n\ntexture = surface.vol_to_surf(stat_img, fsaverage.pial_right)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nilearn import plotting\n\nplotting.plot_surf_stat_map(fsaverage.infl_right, texture, hemi='right',\n                            title='Surface right hemisphere', colorbar=True,\n                            threshold=1., bg_map=fsaverage.sulc_right)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_glass_brain(stat_img, display_mode='r', plot_abs=False,\n                          title='Glass brain', threshold=2.)\n\nplotting.plot_stat_map(stat_img, display_mode='x', threshold=1.,\n                       cut_coords=range(0, 51, 10), title='Slices')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"big_fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\nbig_texture = surface.vol_to_surf(stat_img, big_fsaverage.pial_right)\n\nplotting.plot_surf_stat_map(big_fsaverage.infl_right,\n                            big_texture, hemi='right', colorbar=True,\n                            title='Surface right hemisphere: fine mesh',\n                            threshold=1., bg_map=big_fsaverage.sulc_right)\n\n\nplotting.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"view = plotting.view_surf(fsaverage.infl_right, texture, threshold='90%',\n                          bg_map=fsaverage.sulc_right)\n\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"view = plotting.view_img_on_surf(stat_img, threshold='90%')\n# view.open_in_browser()\n\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from nilearn import datasets\nprint('Datasets are stored in: %r' % datasets.get_data_dirs())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"motor_images = datasets.fetch_neurovault_motor_task()\nmotor_images.images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmap_filename = motor_images.images[0]\n\nfrom nilearn import plotting\nplotting.plot_stat_map(tmap_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(tmap_filename, threshold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"rsn = datasets.fetch_atlas_smith_2009()['rsn10']\nrsn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from nilearn import image\nprint(image.load_img(rsn).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"first_rsn = image.index_img(rsn, 0)\nprint(first_rsn.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plotting.plot_stat_map(first_rsn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for img in image.iter_img(rsn):\n    # img is now an in-memory 3D img\n    plotting.plot_stat_map(img, threshold=3, display_mode=\"z\", cut_coords=1,\n                           colorbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"selected_volumes = image.index_img(rsn, slice(3, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for img in image.iter_img(selected_volumes):\n    plotting.plot_stat_map(img)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Load 4D probabilistic atlases\nfrom nilearn import datasets\n\n# Harvard Oxford Atlasf\nharvard_oxford = datasets.fetch_atlas_harvard_oxford('cort-prob-2mm')\nharvard_oxford_sub = datasets.fetch_atlas_harvard_oxford('sub-prob-2mm')\n\n# Multi Subject Dictionary Learning Atlas\nmsdl = datasets.fetch_atlas_msdl()\n\n# Smith ICA Atlas and Brain Maps 2009\nsmith = datasets.fetch_atlas_smith_2009()\n\n# ICBM tissue probability\nicbm = datasets.fetch_icbm152_2009()\n\n# Allen RSN networks\nallen = datasets.fetch_atlas_allen_2011()\n\n# Pauli subcortical atlas\nsubcortex = datasets.fetch_atlas_pauli_2017()\n\n# Visualization\nfrom nilearn import plotting\n\natlas_types = {'Harvard_Oxford': harvard_oxford.maps,\n               'Harvard_Oxford sub': harvard_oxford_sub.maps,\n               'MSDL': msdl.maps, 'Smith 2009 10 RSNs': smith.rsn10,\n               'Smith2009 20 RSNs': smith.rsn20,\n               'Smith2009 70 RSNs': smith.rsn70,\n               'Smith2009 20 Brainmap': smith.bm20,\n               'Smith2009 70 Brainmap': smith.bm70,\n               'ICBM tissues': (icbm['wm'], icbm['gm'], icbm['csf']),\n               'Allen2011': allen.rsn28,\n               'Pauli2017 Subcortical Atlas': subcortex.maps,\n               }\n\nfor name, atlas in sorted(atlas_types.items()):\n    plotting.plot_prob_atlas(atlas, title=name)\n\n# An optional colorbar can be set\nplotting.plot_prob_atlas(smith.bm10, title='Smith2009 10 Brainmap (with'\n                                           ' colorbar)',\n                         colorbar=True)\nprint('ready')\nplotting.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hope you guys find this notebook useful.\n# If you like this notebook, please upvote.***üëç"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}