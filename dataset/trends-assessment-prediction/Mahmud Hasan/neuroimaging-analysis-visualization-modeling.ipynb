{"cells":[{"metadata":{},"cell_type":"markdown","source":"## About this notebook...\nA/c to me, this is one of the best competition on kaggle. It give you a chance to learn some cool new things on kaggle and broad you knowledge in Datascience.\n\nAs already written in Introduction, Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.\n\nI try my level best to give all things as simple as possible.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Understand the Problem statment!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What is NeuroImaging?\nNeuroimaging or brain imaging is the use of various techniques to either directly or indirectly image the structure, function, or pharmacology of the nervous system.It is a relatively new discipline within medicine, neuroscience, and psychology.Physicians who specialize in the performance and interpretation of neuroimaging in the clinical setting are neuroradiologists.\n\nNeuroimaging falls into two broad categories:\n\nStructural imaging, which deals with the structure of the nervous system and the diagnosis of gross (large scale) intracranial disease (such as a tumor) and injury.\nFunctional imaging, which is used to diagnose metabolic diseases and lesions on a finer scale (such as Alzheimer's disease) and also for neurological and cognitive psychology research and building brain-computer interfaces.\nHuman brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.\n\n**In this challenge, we have to predict age and assessment values from two domains using features derived from brain MRI images as inputs.**\n\n# How Brain MRI is done?\nMagnetic resonance imaging (MRI) of the head is a painless, noninvasive test that produces detailed images of your brain and brain stem. An MRI machine creates the images using a magnetic field and radio waves. This test is also known as a brain MRI or a cranial MRI. You will go to a hospital or radiology center to take a head MRI.\n\nAn MRI scan is different from a CT scan or an X-ray in that it doesn’t use radiation to produce images. An MRI scan combines images to create a 3-D picture of your internal structures, so it’s more effective than other scans at detecting abnormalities in small structures of the brain such as the pituitary gland and brain stem. Sometimes a contrast agent, or dye, can be given through an intravenous (IV) line to better visualize certain structures or abnormalities.\n\nA functional MRI (fMRI) of the brain is useful for people who might have to undergo brain surgery. An fMRI can pinpoint areas of the brain responsible for speech and language, and body movement. It does this by measuring metabolic changes that take place in your brain when you perform certain tasks. During this test, you may need to carry out small tasks, such as answering basic questions or tapping your thumb with your fingertips.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Let understand the Dataset.\n* fMRI_train - a folder containing 53 3D spatial maps for train samples in [.mat] format.\n* fMRI_test - a folder containing 53 3D spatial maps for test samples in [.mat] format.\n* fnc.csv - static FNC correlation features for both train and test samples.\n* loading.csv - sMRI SBM loadings for both train and test samples.\n* train_scores.csv - age and assessment values for train samples.\n* reveal_ID_site2.csv - a list of subject IDs whose data was collected with a different scanner than the train samples.\n* fMRI_mask.nii - a 3D binary spatial map.\n* ICN_numbers.txt - intrinsic connectivity network numbers for each fMRI spatial map; matches FNC names.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# How Features Were Obtained\nAn unbiased strategy was utilized to obtain the provided features. This means that a separate, unrelated large imaging dataset was utilized to learn feature templates. Then, these templates were \"projected\" onto the original imaging data of each subject used for this competition using spatially constrained independent component analysis (scICA) via group information guided ICA (GIG-ICA).\n\nThe first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA decomposition of gray matter concentration maps from structural MRI (sMRI) scans.\n\nThe second set are static functional network connectivity (FNC) matrices. These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI (fMRI).\n\nThe third set of features are the component spatial maps (SM). These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI (fMRI).","execution_count":null},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!wget https://github.com/Chaogan-Yan/DPABI/raw/master/Templates/ch2better.nii","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Necessary Library for this project","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport re\nimport h5py\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom scipy.stats import ks_2samp\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nimport lightgbm as lgb\n\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\n\nSEED = 1337","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fnc = pd.read_csv('../input/trends-assessment-prediction/fnc.csv')\ndf_loading = pd.read_csv('../input/trends-assessment-prediction/loading.csv')\ndf_train_scores = pd.read_csv('../input/trends-assessment-prediction/train_scores.csv')\n\nfnc_features, loading_features = list(df_fnc.columns[1:]), list(df_loading.columns[1:])\ndf_train_scores['is_train'] = 1\ndf = df_fnc.merge(df_loading, on='Id')\ndf = df.merge(df_train_scores, how='left', on='Id')\n\ndf.loc[df['is_train'].isnull(), 'is_train'] = 0\ndf['is_train'] = df['is_train'].astype(np.uint8)\ndf['Id'] = df['Id'].astype(np.uint16)\n\nprint(f'Static FNC Correlation Shape = {df_fnc.shape}')\nprint(f'Static FNC Correlation Memory Usage = {df_fnc.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'sMRI SBM Loadings Shape = {df_loading.shape}')\nprint(f'sMRI SBM Loadings Memory Usage = {df_loading.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Train Scores Shape = {df_train_scores.shape}')\nprint(f'Train Scores Memory Usage = {df_train_scores.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint('-------------------------------------')\nprint(f'Train & Test Set Shape = {df.shape}')\nprint(f'Train & Test Set Memory Usage = {df.memory_usage().sum() / 1024 ** 2:.2f} MB')\n\ndel df_fnc, df_loading, df_train_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **1. Train Scores (Targets)**\n`train_scores.csv` is the file which consists of target features. Those features are `age`, `domain1_var1`, `domain1_var2`, `domain2_var1` and `domain2_var2`. There are 5 target features to predict and submissions are scored using feature-weighted, normalized absolute errors.\n\n**score** ${= \\Large \\sum\\limits_{f} w_{f} \\Big( \\frac{\\sum_{i} \\big| y_{f,i}  - \\hat{y}_{f,i}\\big| }{\\sum_{i} \\hat{y}_{f,i}} \\Big) }$\n\nThe weights are `[.3, .175, .175, .175, .175]` corresponding to features `[age, domain1_var1, domain1_var2, domain2_var1, domain2_var2]`. This means every targets normalized absolute error is independent from each other. They can be trained and predicted with a single model or 5 different models.\n\nAnother important thing to consider is, `train_scores.csv` are not the original age and raw assessment values. They have been transformed and de-identified to help protect subject identity and minimize the risk of unethical usage of the data. Nonetheless, they are directly derived from the original assessment values and, thus, associations with the provided features is equally likely.\n\n**Before transformation, the age in the training set is rounded to nearest year for privacy reasons. However, age is not rounded to year (higher precision) in the test set. Thus, heavily overfitting to the training set age will very likely have a negative impact on your submissions.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **1.1 Targets' Distributions**\n\nEven though some of target features are slightly tailed, all of them follow a normal distribution. Their descriptive statistical summary are very similar to each other.\n\nA small percentage of values are missing in target features except `age`. Target features in the same domain have same number of missing values and they are missing in same samples.  Those are skipped in the score calculation. However, every row should be predicted in the submission file.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_target(target_feature):   \n    \n    if target_feature == 'age':\n        print(f'Target feature {target_feature} Statistical Analysis\\n{\"-\" * 39}')\n    else:\n        print(f'Target feature {target_feature} Statistical Analysis\\n{\"-\" * 48}')\n        \n    print(f'Mean: {df[target_feature].mean():.4}  -  Median: {df[target_feature].median():.4}  -  Std: {df[target_feature].std():.4}')\n    print(f'Min: {df[target_feature].min():.4}  -  25%: {df[target_feature].quantile(0.25):.4}  -  50%: {df[target_feature].quantile(0.5):.4}  -  75%: {df[target_feature].quantile(0.75):.4}  -  Max: {df[target_feature].max():.4}')\n    print(f'Skew: {df[target_feature].skew():.4}  -  Kurtosis: {df[target_feature].kurtosis():.4}')\n    missing_values_count = df[(df['is_train'] == 1) & (df[target_feature]).isnull()].shape[0]\n    training_samples_count = df[df['is_train'] == 1].shape[0]\n    print(f'Missing Values: {missing_values_count}/{training_samples_count} ({missing_values_count * 100 / training_samples_count:.4}%)')\n\n    fig = plt.subplots(figsize=(6, 4), dpi=100)\n\n    sns.distplot(df[target_feature], label=target_feature)\n\n    plt.xlabel('')\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    plt.title(f'{target_feature} Distribution in Training Set')\n    plt.show()\n    \nfor target_feature in ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']:\n    plot_target(target_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **1.2 Targets' Correlations**\n\nTarget features are not correlated with each other too much. The strongest correlations are between `age` and `domain1_var1` (**0.34**), and between  `age` and `domain2_var1` (**0.23**). There might be a relationship between  `age` and var1 features.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10), dpi=100)\n\nsns.heatmap(df[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 14}, \n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=14, rotation=45)\nplt.tick_params(axis='y', labelsize=14, rotation=0)\n    \nplt.title('Target Features Correlations', size=18, pad=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2. Source-based Morphometry Loadings**\nThe first set of features are source-based morphometry (SBM) loadings. These are subject-level weights from a group-level ICA decomposition of gray matter concentration maps from structural MRI (sMRI) scans. Those features are for both training and test samples.\n\nThere are **26** features in `loading.csv` file without `Id`. Those features are named from `IC_01` to `IC_30`, but `IC_19`, `IC_23`, `IC_25` and `IC_27` don't exist. They are parts of brain and their explanations are listed below:\n\n`\nIC_01 - Cerebellum\nIC_02 - ACC+mpfc\nIC_03 - Caudate\nIC_04 - Cerebellum\nIC_05 - Calcarine\nIC_06 - Calcarine\nIC_07 - Precuneus+PCC\nIC_08 - Frontal\nIC_09 - IPL+AG\nIC_10 - MTG\nIC_11 - Frontal\nIC_12 - SMA\nIC_13 - Temporal Pole\nIC_14 - Temporal Pole + Fusiform\nIC_15 - STG\nIC_16 - Middle Occipital?\nIC_17 - Cerebellum\nIC_18 - Cerebellum\nIC_20 - MCC\nIC_21 - Temporal Pole + Cerebellum\nIC_22 - Insula + Caudate\nIC_24 - IPL+Postcentral\nIC_26 - Inf+Mid Frontal\nIC_28 - Calcarine\nIC_29 - MTG\nIC_30 - Inf Frontal\n`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **2.1 Source-based Morphometry Loadings' Distributions**\n\nAll of the loading features follow a normal distribution. Their distributions and descriptive statistical summary in training and test samples are very similar except `IC_20`. It is an exception because the distribution of `IC_20` in test samples is shifted. This feature may require some preprocessing.\n\nNone of the loading features have a visible relationship with any of the targets. Data points of loading features are scattered around the means of `domain1_var1`, `domain1_var2`, `domain2_var1`, `domain2_var2`, however `age` has a tiny relationship with loading features. This relationship is not easy to detect but it looks like `age` is easier to predict than other target features.\n\nThere are some data points which can be classified as mild outliers in all loading features. Models used for predicting targets, should be robust to outliers. The most extreme outlier in loading features belongs to `IC_02`. It is a single data point and it's locations are very close in every target.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_loading(loading_feature):    \n    \n    print(f'Loading feature {loading_feature} Statistical Analysis\\n{\"-\" * 42}')\n        \n    print(f'Mean: {df[loading_feature].mean():.4}  -  Median: {df[loading_feature].median():.4}  -  Std: {df[loading_feature].std():.4}')\n    print(f'Min: {df[loading_feature].min():.4}  -  25%: {df[loading_feature].quantile(0.25):.4}  -  50%: {df[loading_feature].quantile(0.5):.4}  -  75%: {df[loading_feature].quantile(0.75):.4}  -  Max: {df[loading_feature].max():.4}')\n    print(f'Skew: {df[loading_feature].skew():.4}  -  Kurtosis: {df[loading_feature].kurtosis():.4}')\n    missing_values_count = df[df[loading_feature].isnull()].shape[0]\n    training_samples_count = df.shape[0]\n    print(f'Missing Values: {missing_values_count}/{training_samples_count} ({missing_values_count * 100 / training_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(25, 12), dpi=100, constrained_layout=True)\n    title_size = 18\n    label_size = 18\n\n    # Loading Feature Training and Test Set Distribution\n    sns.distplot(df[df['is_train'] == 1][loading_feature], label='Training', ax=axes[0][0])\n    sns.distplot(df[df['is_train'] == 0][loading_feature], label='Test', ax=axes[0][0])\n    axes[0][0].set_xlabel('')\n    axes[0][0].tick_params(axis='x', labelsize=label_size)\n    axes[0][0].tick_params(axis='y', labelsize=label_size)\n    axes[0][0].legend()\n    axes[0][0].set_title(f'{loading_feature} Distribution in Training and Test Set', size=title_size, pad=title_size)\n    \n    # Loading Feature vs age\n    sns.scatterplot(df[loading_feature], df['age'], ax=axes[0][1])\n    axes[0][1].set_title(f'{loading_feature} vs age', size=title_size, pad=title_size)\n    axes[0][1].set_xlabel('')\n    axes[0][1].set_ylabel('')\n    axes[0][1].tick_params(axis='x', labelsize=label_size)\n    axes[0][1].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain1_var1\n    sns.scatterplot(df[loading_feature], df['domain1_var1'], ax=axes[0][2])\n    axes[0][2].set_title(f'{loading_feature} vs domain1_var1', size=title_size, pad=title_size)\n    axes[0][2].set_xlabel('')\n    axes[0][2].set_ylabel('')\n    axes[0][2].tick_params(axis='x', labelsize=label_size)\n    axes[0][2].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain1_var2\n    sns.scatterplot(df[loading_feature], df['domain1_var2'], ax=axes[1][0])\n    axes[1][0].set_title(f'{loading_feature} vs domain1_var2', size=title_size, pad=title_size)\n    axes[1][0].set_xlabel('')\n    axes[1][0].set_ylabel('')\n    axes[1][0].tick_params(axis='x', labelsize=label_size)\n    axes[1][0].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain2_var1\n    sns.scatterplot(df[loading_feature], df['domain2_var1'], ax=axes[1][1])\n    axes[1][1].set_title(f'{loading_feature} vs domain2_var1', size=title_size, pad=title_size)\n    axes[1][1].set_xlabel('')\n    axes[1][1].set_ylabel('')\n    axes[1][1].tick_params(axis='x', labelsize=label_size)\n    axes[1][1].tick_params(axis='y', labelsize=label_size)\n    \n    # Loading Feature vs domain2_var2\n    sns.scatterplot(df[loading_feature], df['domain2_var2'], ax=axes[1][2])\n    axes[1][2].set_title(f'{loading_feature} vs domain2_var2', size=title_size, pad=title_size)\n    axes[1][2].set_xlabel('')\n    axes[1][2].set_ylabel('')\n    axes[1][2].tick_params(axis='x', labelsize=label_size)\n    axes[1][2].tick_params(axis='y', labelsize=label_size)\n    \n    plt.show()\n    \nfor loading_feature in sorted(loading_features):\n    plot_loading(loading_feature)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **2.2 Source-based Morphometry Loadings' Correlations**\n\nThere are strong correlations between `age` and some loading features that exceed **-0.4**, but none of the loading features are correlated with other targets.\n\nLoading features have decent correlations between themselves that exceed **0.5** and **-0.5**. Some of those high positive correlations belong to feature groups which are from the same part of the brain. However, all features from the same part of the brain are not necessarily correlated with each other, so there is no pattern here.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"loading_target_features = sorted(loading_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(30, 30), dpi=100)\n\nsns.heatmap(df[loading_target_features].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15}, \n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=45)\nplt.tick_params(axis='y', labelsize=18, rotation=45)\n\nplt.title('Target and Loading Features Correlations', size=25, pad=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3. Static Functional Network Connectivity**\nThe second set of features are static functional network connectivity (FNC) matrices. These are the subject-level cross-correlation values among 53 component timecourses estimated from GIG-ICA of resting state functional MRI (fMRI).\n\nThere are **1378** features in `fnc.csv` file without `Id`. Those features are named as `Network1(X)_vs_Network2(Y)` and there are **7** different networks. Network names and abbreviations are listed below:\n\n`\nSCN - Sub-cortical Network\nADN - Auditory Network\nSMN - Sensorimotor Network\nVSN - Visual Network\nCON - Cognitive-control Network    \nDMN - Default-mode Network\nCBN - Cerebellar Network\n`\n\nThose groups might be useful to analyze **1378** features part by part.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **3.1. Sub-cortical Network (SCN) Features**\n\nFirst FNC feature sub-group is SCN features. This is the smallest sub-group of fnc features and there are **10** features in it. This sub-group has cross-correlations with only itself.\n\nSCN features are strongly correlated with each other, but none of the SCN features are correlated with targets. Correlations between target and SCN features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"scn_pattern = r'SCN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nscn_features = [col for col in fnc_features if re.match(scn_pattern, col)]\nscn_target_features = sorted(scn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(25, 25), dpi=100)\n\nsns.heatmap(df[scn_target_features].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15}, \n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=75)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\n\nplt.title('Target and SCN Features Correlations', size=25, pad=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.2. Auditory Network (ADN) Features**\n\nSecond FNC feature sub-group is ADN features. This is also the second smallest sub-group of FNC features and there are **11** features in it. This sub-group has cross-correlations with SCN (10) and itself (1).\n\nADN features are strongly correlated with each other if only ADN is cross-correlated with SCN. There are 10 ADN(2) vs SCN(5) features and they have very strong correlations. There is only one ADN vs ADN (`ADN(56)_vs_ADN(21)`) feature and it doesn't have any significant correlation with anything. Correlations between target and ADN features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"adn_pattern = r'ADN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nadn_features = [col for col in fnc_features if re.match(adn_pattern, col)]\nadn_target_features = sorted(adn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(25, 25), dpi=100)\n\nsns.heatmap(df[adn_target_features].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15},\n            fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=75)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\n\nplt.title('Target and ADN Features Correlations', size=25, pad=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.3. Sensorimotor Network (SMN) Features**\n\nThird FNC feature sub-group is SMN features. This is a large sub-group compared to previous ones and there are **99** features in it. This sub-group has cross-correlations with SCN (45), ADN (18) and itself (36).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect.\n\nEvery cross-correlation of SMN have very strong correlations inside the second network groups. This explains the bright red blocks near the diagonal axis.\n\n* `SMN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `SMN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `SMN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n\nEvery cross-correlation of SMN have moderate correlations inside the first network groups. This explains the repeating orange and blue color blocks along the vertical and horizontal axis.\n\n* `SMN(X)_vs_ADN(Y)`: `Y` is moderately correlated with every different `X` value for `ADN` \n* `SMN(X)_vs_SCN(Y)`: `Y` is moderately correlated with every different `X` value for `SCN` \n* `SMN(X)_vs_SMN(Y)`: `Y` is moderately correlated with every different `X` value for `SMN`\n\nCorrelations between target and SMN features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"smn_pattern = r'SMN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nsmn_features = [col for col in fnc_features if re.match(smn_pattern, col)]\nsmn_target_features = sorted(smn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(40, 40), dpi=100)\n\nsns.heatmap(df[smn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and SMN Features Correlations', size=50, pad=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.4. Visual Network (VSN) Features** \n\nFourth FNC feature sub-group is VSN features. This is a very large sub-group compared to previous ones and there are **180** features in it. This sub-group has cross-correlations with SCN (45), ADN (18), SMN (81) and itself (36).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN features can be seen on VSN features as well.\n\nEvery cross-correlation of VSN have very strong correlations inside the second network groups. This explains the bright red blocks near the diagonal axis.\n\n* `VSN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `VSN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `VSN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `VSN(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n\nEvery cross-correlation of VSN have moderate correlations inside the first network groups. This explains the repeating orange and blue color blocks along the vertical and horizontal axis.\n\n* `VSN(X)_vs_ADN(Y)`: `Y` is moderately correlated with every different `X` value for `ADN` \n* `VSN(X)_vs_SCN(Y)`: `Y` is moderately correlated with every different `X` value for `SCN` \n* `VSN(X)_vs_SMN(Y)`: `Y` is moderately correlated with every different `X` value for `SMN`\n* `VSN(X)_vs_VSN(Y)`: `Y` is moderately correlated with every different `X` value for `VSN`\n\nCorrelations between target and VSN features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"vsn_pattern = r'VSN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\nvsn_features = [col for col in fnc_features if re.match(vsn_pattern, col)]\nvsn_target_features = sorted(vsn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(40, 40), dpi=100)\n\nsns.heatmap(df[vsn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and VSN Features Correlations', size=50, pad=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.5. Cognitive-control Network (CON) Features** \n\nFifth FNC feature sub-group is CON features. This is the largest sub-group and there are **561** features in it. This sub-group has cross-correlations with SCN (85), ADN (34), SMN (153), VSN (153) and itself (136).\n\nIt is even hard to identify correlations at block level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN and VSN features can be seen on CON features as well.\n\nEvery cross-correlation of CON have very strong correlations inside the second network groups. This explains the bright red blocks near the diagonal axis.\n\n* `CON(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `CON(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `CON(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `CON(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n* `CON(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n\nEvery cross-correlation of CON have weak correlations inside the first network groups. This explains the light orange and light blue color blocks everywhere except near of diagonal axis.\n\nCorrelations between target and CON features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"con_pattern = r'CON\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\ncon_features = [col for col in fnc_features if re.match(con_pattern, col)]\ncon_target_features = sorted(con_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(50, 50), dpi=100)\n\nsns.heatmap(df[con_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and CON Features Correlations', size=50, pad=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.6. Default-mode Network (DMN) Features** \n\nSixth FNC feature sub-group is DMN features. This is the second largest sub-group and there are **315** features in it. This sub-group has cross-correlations with SCN (35), ADN (14), SMN (63), VSN (63), CON (119) and itself (21).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN, VSN and CON features can be seen on DMN features as well.\n\nEvery cross-correlation of DMN have very strong correlations inside the second network groups. This explains the bright red and blue blocks near the diagonal axis.\n\n* `DMN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `DMN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `DMN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `DMN(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n* `DMN(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n* `DMN(X)_vs_DMN(Y)`: `X` is strongly correlated with every different `Y` value for `DMN`\n\nEvery cross-correlation of DMN have weak correlations inside the first network groups. This explains the light orange and light blue color blocks everywhere except near of diagonal axis. There are also repeating light orange and light blue diagonal lines. \n\nCorrelations between target and DMN features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dmn_pattern = r'DMN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\ndmn_features = [col for col in fnc_features if re.match(dmn_pattern, col)]\ndmn_target_features = sorted(dmn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(50, 50), dpi=100)\n\nsns.heatmap(df[dmn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and DMN Features Correlations', size=50, pad=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.7. Cerebellar Network (CBN) Features** \n\nSeventh FNC feature sub-group is CBN features. This is a large sub-group and there are **202** features in it. This sub-group has cross-correlations with SCN (20), ADN (8), SMN (36), VSN (36), CON (68), DMN (28) and itself (6).\n\nIt is hard to identify correlations at feature level on this scale but it still gives lots of information. Since the feature names are sorted by alphabetical order, the correlations of feature groups are easy to detect. The same pattern from SMN, VSN, CON and DMN features can be seen on CBN features as well.\n\nEvery cross-correlation of CBN have very strong correlations inside the second network groups. This explains the bright red and blue blocks near the diagonal axis.\n\n* `CBN(X)_vs_ADN(Y)`: `X` is strongly correlated with every different `Y` value for `ADN` \n* `CBN(X)_vs_SCN(Y)`: `X` is strongly correlated with every different `Y` value for `SCN` \n* `CBN(X)_vs_SMN(Y)`: `X` is strongly correlated with every different `Y` value for `SMN`\n* `CBN(X)_vs_VSN(Y)`: `X` is strongly correlated with every different `Y` value for `VSN`\n* `CBN(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n* `CBN(X)_vs_DMN(Y)`: `X` is strongly correlated with every different `Y` value for `DMN`\n* `CBN(X)_vs_CBN(Y)`: `X` is strongly correlated with every different `Y` value for `CBN`\n\nEvery cross-correlation of CBN have moderate correlations inside the first network groups. This explains the repeating orange and blue color blocks along the vertical and horizontal axis, and repeating bright red and orange diagonal lines.\n\n* `CBN(X)_vs_ADN(Y)`: `Y` is strongly correlated with every different `X` value for `ADN` \n* `CBN(X)_vs_SCN(Y)`: `Y` is strongly correlated with every different `X` value for `SCN` \n* `CBN(X)_vs_SMN(Y)`: `Y` is strongly correlated with every different `X` value for `SMN`\n* `CBN(X)_vs_VSN(Y)`: `Y` is strongly correlated with every different `X` value for `VSN`\n* `CBN(X)_vs_CON(Y)`: `X` is strongly correlated with every different `Y` value for `CON`\n* `CBN(X)_vs_DMN(Y)`: `X` is strongly correlated with every different `Y` value for `DMN`\n* `CBN(X)_vs_CBN(Y)`: `X` is strongly correlated with every different `Y` value for `CBN`\n\nCorrelations between target and CBN features are very weak which can be seen at the bottom and right end.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cbn_pattern = r'CBN\\(\\d+\\)_vs_[A-Z]+\\(\\d+\\)'\ncbn_features = [col for col in fnc_features if re.match(cbn_pattern, col)]\ncbn_target_features = sorted(cbn_features) + ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\nfig = plt.figure(figsize=(50, 50), dpi=100)\n\nsns.heatmap(df[cbn_target_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and CBN Features Correlations', size=50, pad=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.8. Inference**\nFNC features from the same group are more likely to be correlated with each other. Correlations from different groups doesn't seem to be very strong. Even in the same groups, there are lots of weak correlations. FNC features from the same groups must be in the same network in order to have strong correlation. FNC features in the same network don't have strong correlations if they are not in the same group.\n\nThe entire heatmap of FNC features is rendered below but it is very hard to derive any meaning from it.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(60, 60), dpi=100)\n\nsns.heatmap(df[fnc_features].corr(),\n            annot=False,\n            square=True,\n            cmap='coolwarm',\n            yticklabels=False,\n            xticklabels=False)   \n\nplt.title('Target and All FNC Features Correlations', size=60, pad=60)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4. Sites**\n\n`reveal_ID_site2.csv` a list of subject IDs whose data was collected with a different scanner than the train samples.\n\n> Models are expected to generalize on data from a different scanner/site (site 2). All subjects from site 2 were assigned to the test set, so their scores are not available. While there are fewer site 2 subjects than site 1 subjects in the test set, the total number of subjects from site 2 will not be revealed until after the end of the competition. To make it more interesting, the IDs of some site 2 subjects have been revealed below. Use this to inform your models about site effects. Site effects are a form of bias. To generalize well, models should learn features that are not related to or driven by site effects.\n\nThis paragraph is taken from the data tab. It means data is collected from two different sources, site 1 and 2. This is the reason of train and test set feature distribution discrepancies.\n\n* All of the data in training set is collected from site 1.\n* All of the data collected from site 2 are in test set but the entire test set is not collected from site 2.\n\nIn this case, there is no way to find out site 1 samples in test set perfectly, but they can be predicted with a classifier by looking at feature distributions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"site2_ids = pd.read_csv('../input/trends-assessment-prediction/reveal_ID_site2.csv').values.flatten()\n\ndf['site'] = 0\ndf.loc[df['is_train'] == 1, 'site'] = 1\ndf.loc[df['Id'].isin(site2_ids), 'site'] = 2\ndf['site'] = df['site'].astype(np.uint8)\n\ndel site2_ids","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 6), dpi=100)\nsns.barplot(x=df['site'].value_counts().index, y=df['site'].value_counts())\n\npercentages = [(count / df['site'].value_counts().sum() * 100).round(2) for count in df['site'].value_counts()]\nplt.ylabel('')\nplt.xticks(np.arange(3), [f'No Site (%{percentages[1]})', f'Site 1 (%{percentages[0]})', f'Site 2 (%{percentages[2]})'])\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title('Site Counts', size=15, pad=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.1. Loading Features Site Distributions**\n\nIt is hard to make an accurate analysis on feature distributions for different sites because sample sizes are different for site 1 and 2. There are **5877** site 1 samples (training set) and **510** verified site 2 samples. That's why train/test distribution of the feature should be plotted as well in order to measure the shift in distribution.\n\nThere are three types of shifts in the distributions ploted below:\n\n* Mild shift in site 1/2 feature distribution but it can't be seen on train/test feature distribution (Mean shift close to ±0.001). Those shifts can be observed due to small sample size of site 2 subjects, but they are suspicious.\n* Severe shift in site 1/2 feature distribution that it can also be seen on train/test feature distribution (Mean shift greater than ±0.002). Those features should be eliminated.\n* No shift in both site 1/2 and train/test feature distributions. Those features can be trusted. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_site_distribution(feature):    \n    \n    print(f'{feature} Site Distribution Analysis\\n{\"-\" * 32}')\n        \n    print(f'Site 1 Mean: {df[df[\"site\"] == 1][feature].mean():.4}  -  Median: {df[df[\"site\"] == 1][feature].median():.4}  -  Std: {df[df[\"site\"] == 1][feature].std():.4}')\n    print(f'Site 2 Mean: {df[df[\"site\"] == 2][feature].mean():.4}  -  Median: {df[df[\"site\"] == 2][feature].median():.4}  -  Std: {df[df[\"site\"] == 2][feature].std():.4}')\n    print(f'\\nSite 1 Min: {df[df[\"site\"] == 1][feature].min():.4}  -  25%: {df[df[\"site\"] == 1][feature].quantile(0.25):.4}  -  50%: {df[df[\"site\"] == 1][feature].quantile(0.5):.4}  -  75%: {df[df[\"site\"] == 1][feature].quantile(0.75):.4}  -  Max: {df[df[\"site\"] == 1][feature].max():.4}')\n    print(f'Site 2 Min: {df[df[\"site\"] == 2][feature].min():.4}  -  25%: {df[df[\"site\"] == 2][feature].quantile(0.25):.4}  -  50%: {df[df[\"site\"] == 2][feature].quantile(0.5):.4}  -  75%: {df[df[\"site\"] == 2][feature].quantile(0.75):.4}  -  Max: {df[df[\"site\"] == 2][feature].max():.4}')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(20, 6), dpi=100)\n\n    sns.distplot(df[df['site'] == 1][feature], label='Site 1', ax=axes[0])\n    sns.distplot(df[df['site'] == 2][feature], label='Site 2', ax=axes[0])\n    sns.distplot(df[df['is_train'] == 1][feature], label='Training', ax=axes[1])\n    sns.distplot(df[df['is_train'] == 0][feature], label='Test', ax=axes[1])\n    \n    for i in range(2):\n        axes[i].set_xlabel('')\n        axes[i].tick_params(axis='x', labelsize=15)\n        axes[i].tick_params(axis='y', labelsize=15)\n        axes[i].legend()\n    axes[0].set_title(f'{feature} Distribution in Site 1 and 2', size=18, pad=18)\n    axes[1].set_title(f'{feature} Distribution in Training and Test Set', size=18, pad=18)\n    \n    plt.show()\n    \nfor loading_feature in sorted(loading_features):\n    plot_site_distribution(loading_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.2. FNC Features Site Distributions**\n\nThere are 1378 FNC features so they can't be analyzed one by one. Kolmogorov–Smirnov test calculated on site 1 and site 2 feature distribution can be used for feature selection. If the KS statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same. I used **0.125** for KS statistic threshold and selected features that are exceeding the threshold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_distribution_difference(feature):\n    site1_values = df[df['site'] == 1][feature].values\n    site2_values = df[df['site'] == 2][feature].values\n    return feature, ks_2samp(site1_values, site2_values)\n\nks_threshold = 0.125\nshifted_features = [fnc_feature for fnc_feature in fnc_features if get_distribution_difference(fnc_feature)[1][0] > ks_threshold]        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same things in loading features can be observed in FNC features as well. Slightly shifted train/test feature distributions are shifted even more in site 1/2. However, none of the feature shifts are as severe as `IC_20` in loading features.\n\n**0.125** is an arbitrary number and distribution difference can be measured with other ways as well, but feature selection should definitely be based on site 1 and 2 feature distribution difference.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for feature in sorted(shifted_features):\n    plot_site_distribution(feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.3. Site Classification**\n\nEven though it is impossible to predict the sites of unlabeled samples perfectly, shifted features can be used in a classifier and predict the sites to some degree. Loading features should be used in this classifier along with shifted FNC features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"site_predictors = shifted_features + loading_features\n\nX_train = df.loc[df['site'] > 0, site_predictors]\ny_train = df.loc[df['site'] > 0, 'site']\nX_test = df.loc[df['site'] == 0, site_predictors]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Expected values closer to **1** and **2** in `site_predicted`, could possible be added to the original `site` feature since they are confident predictions.\n\n`IC_20`, `IC_18`, `IC_11`, `IC_21` and `IC_05` are at the top of feature importance in site classifier model. It looks like FNC features don't contribute site classifier model as much as loading features. `IC_20` is the most dangerous feature without any doubt but other loading features should be selected very carefully in main models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['site_predicted'] = 0\n\nK = 2\nskf = StratifiedKFold(n_splits=K, shuffle=True, random_state=SEED)\n\noof_scores = []\nfeature_importance = pd.DataFrame(np.zeros((X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=X_train.columns)\n\nsite_model_parameters = {\n    'num_iterations': 500,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 5, \n    'learning_rate': 0.05,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.9,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': SEED,\n    'feature_fraction_seed': SEED,\n    'bagging_seed': SEED,\n    'drop_seed': SEED,\n    'data_random_seed': SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,   \n}\n\nprint('Running LightGBM Site Classifier Model\\n' + ('-' * 38) + '\\n')\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx, :], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx, :], label=y_train.iloc[val_idx])                \n    site_model = lgb.train(site_model_parameters, trn_data, valid_sets=[trn_data, val_data], verbose_eval=50)\n    feature_importance.iloc[:, fold - 1] = site_model.feature_importance(importance_type='gain')\n\n    site_oof_predictions = site_model.predict(X_train.iloc[val_idx, :], num_iteration=site_model.best_iteration)\n    df.loc[X_train.iloc[val_idx, :].index, 'site_predicted'] = site_oof_predictions\n    \n    site_test_predictions = site_model.predict(X_test, num_iteration=site_model.best_iteration)\n    df.loc[X_test.index, 'site_predicted'] += site_test_predictions / K\n\n    oof_score = f1_score(y_train.iloc[val_idx], np.clip(np.round(site_oof_predictions), 1, 2))\n    oof_scores.append(oof_score)            \n    print(f'\\nFold {fold} - F1 Score {oof_score:.6}\\n')\n    \ndf['site_predicted'] = df['site_predicted'].astype(np.float32)\noof_f1_score = f1_score(df.loc[df['site'] > 0, 'site'], np.clip(np.round(df.loc[df['site'] > 0, 'site_predicted']), 1, 2))\n\nprint('-' * 38)\nprint(f'LightGBM Site Classifier Model Mean F1 Score {np.mean(oof_scores):.6} [STD:{np.std(oof_scores):.6}]')\nprint(f'LightGBM Site Classifier Model OOF F1 Score {oof_f1_score:.6}')\n\nplt.figure(figsize=(20, 20))\nfeature_importance['Mean_Importance'] = feature_importance.sum(axis=1) / K\nfeature_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=feature_importance.index, data=feature_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('LightGBM Site Classifier Model Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.4. Private Leaderboard Simulation**\nIf the expected values predicted by the site classifier are rounded, there are **32** samples classified as site 2. This operation ceils the values greater than **.5** and floors the values lesser than **.5**. It yields very few samples for models to use as a validation set.\n\nIf this threshold is set to **.2**, a 0.9 train/test split can be achieved with samples which slightly look like they are from site 2. I tried to simulate the private leaderboard by:\n\n* Training on **5342** samples which are classified as site 1 (`site_predicted` expected value < 1.2)\n* Validate on **535** samples which slightly look like site 2 (`site_predicted` expected value >= 1.2)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Site Classifier expected values are rounded with .5 threshold\\n')\nprint(df.loc[df['is_train'] == 1, 'site_predicted'].round(0).astype(np.uint8).value_counts())\n\nprint('\\nSite Classifier expected values are rounded with .2 threshold\\n')\ndf.loc[df['site_predicted'] > 1.2, 'site_predicted'] = 2\ndf['site_predicted'] = df['site_predicted'].round().astype(np.uint8)\nprint(df.loc[df['is_train'] == 1, 'site_predicted'].round(0).astype(np.uint8).value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ridge regression is used in this simulation and the results are listed below. The gap between training and validation NAE looks very realistic based on the train/test discrepancies, especially for `age`.\n\n`\nage Train NAE 0.129934 - Validation NAE 0.154733\ndomain1_var1 Train NAE 0.143931 - Validation NAE 0.147178\ndomain1_var2 Train NAE 0.150529 - Validation NAE 0.146966\ndomain2_var1 Train NAE 0.176281 - Validation NAE 0.189226\ndomain2_var2 Train NAE 0.171305 - Validation NAE 0.171517\n`\n\nEven though Ridge regression scores **0.159** on public leaderboard, it scored **0.161** on slightly different samples. This means the private leaderboard score will be even worse.\n\n`Train Weighted NAE 0.151338 - Validation Weighted NAE 0.161025`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **5. Component Spatial Maps**\n\nThe third set of features are the component spatial maps (SM). These are the subject-level 3D images of 53 spatial networks estimated from GIG-ICA of resting state functional MRI (fMRI). Those features are contained under the directories below:\n\n* `fMRI_train` - a folder containing 53 3D spatial maps for train samples in .mat format\n* `fMRI_test` - a folder containing 53 3D spatial maps for test samples in .mat format\n\nThose explanations are written under the Data tab on the competition page, but they are not clear. `fMRI_train` and `fMRI_test` are directories in which there are **5877** samples. Each sample is a `.mat` file that contains **53** 3D spatial maps.\n\nThis means there are **311481** 3D spatial maps in both `fMRI_train` and `fMRI_test`, and this is the reason why competition data is **166.59 GB**.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(f'Train fMRI Samples in fMRI_train: {len(os.listdir(\"../input/trends-assessment-prediction/fMRI_train\"))}')\nprint(f'Test fMRI Samples in fMRI_test: {len(os.listdir(\"../input/trends-assessment-prediction/fMRI_test\"))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **5.1 MATLAB Files**\n\nThe `.mat` files in this competition can be read in Python using `h5py.File`. HDF5 files work generally like standard Python file objects. They support standard modes like r/w/a, and should be closed when they are no longer in use. For an example, the first file (`10001.mat`) inside the `fMRI_train` is read in the cell below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"matlab_file = h5py.File(f'../input/trends-assessment-prediction/fMRI_train/10001.mat', 'r')\nmatlab_file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `.mat` files are using dictionary interface so data inside the file can be accessed with `keys()` and `values()` methods. `.keys()` method shows us there is only one key in the file and it is `SM_feature`. That is where the data is stored.\n\nAccessing the data using `matlab_file[\"SM_feature\"]` shows us, each `.mat` file is a 4D HDF5 dataset with the shape of `(53, 52, 63, 53)`. Those are the **53** 3D spatial maps.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'.keys() -> {matlab_file.keys()}')\nprint(f'.values() -> {matlab_file.values()}')\n\nprint('\\nmatlab_file[\"SM_feature\"] ->', matlab_file['SM_feature'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **5.2 Visualizing 3D Data on 2D Space**\n\nThose **53** 3D spatial maps can be visualized with `nilearn` package. However, 3D spatial maps have to be drawn on a reference image and the reference image is `fMRI_mask.nii` file that is shared in the competition data. Niimg-like objects (`.nii` files) can be load from filenames or list of filenames with `nilearn` as well.\n\nBefore the 3D spatial maps are loaded, it's necessary to reorient the axis, since h5py flips axis order. The axis of data is reoriented in reverse order.\n\nFinally, 3D spatial maps can be loaded as a Niimg-like object with `nilearn.image.new_img_like` by using the `fMRI_mask.nii` as a reference image.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading reference image\nfmri_mask = nl.image.load_img('../input/trends-assessment-prediction/fMRI_mask.nii')\n\n# Reorienting the axis of 3D spatial map\nspatial_maps = np.moveaxis(matlab_file['SM_feature'][()], [0, 1, 2, 3], [3, 2, 1, 0]) \n\n# Loading 3D spatial maps\nspatial_maps_niimg = nl.image.new_img_like(ref_niimg=fmri_mask,\n                                           data=spatial_maps,\n                                           affine=fmri_mask.affine,\n                                           copy_header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the 3D spatial maps are loaded as Niimg-like objects, they can be iterated with `nilearn.image.iter_img` and visualized one by one with `nilearn.plotting.plot_stat_map`. The **53** 3D spatial maps that are visualized below, belong to a single sample, `10001.mat`. There are **5877** samples in both `fMRI_train` and `fMRI_test`.\n\n`nilearn` is very useful to visualize 3D spatial maps because `plot_stat_map` plots the data from three different angles which makes it easier to see the correlated parts of brain.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=53, figsize=(30, 300))\n\nfor i, img in enumerate(list(nl.image.iter_img(spatial_maps_niimg))):\n    nlplt.plot_stat_map(stat_map_img=img,\n                        bg_img='ch2better.nii',\n                        title=f'10001.mat Spatial Map {i + 1} plot_stat_map',\n                        axes=axes[i],\n                        threshold=1,\n                        display_mode='ortho',\n                        annotate=False,\n                        draw_cross=True,\n                        colorbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are various ways to visualize 3D spatial maps other than `plot_stat_map`. 5 different plotting functions used in the visualizations below in every row. Those plotting functions are `plot_glass_brain`, `plot_roi`, `plot_anat`, `plot_epi` and `plot_img`. Some of those visualizations are more detailed and some of them are faster.\n\n`threshold` parameter is used for reducing the noise in visualizations. If `None` is given, the image is not thresholded. If a number is given, it is used to threshold the image: values below the threshold (in absolute value) are plotted as transparent.\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"img = list(nl.image.iter_img(spatial_maps_niimg))[0]\n\nfig, axes = plt.subplots(nrows=5, figsize=(30, 50))\n\nnlplt.plot_glass_brain(img,\n                       title='10001.mat Spatial Map 1 plot_glass_brain',\n                       threshold=3,\n                       black_bg=True,\n                       axes=axes[0])\nnlplt.plot_roi(img,\n               title='10001.mat Spatial Map 1 plot_roi',\n               threshold=3,\n               black_bg=True,\n               axes=axes[1])\n\nnlplt.plot_anat(img,\n                title='10001.mat Spatial Map 1 plot_anat',\n                axes=axes[2])\n\nnlplt.plot_epi(img,\n               title='10001.mat Spatial Map 1 plot_epi',\n               axes=axes[3])\n\nnlplt.plot_img(img,\n               title='10001.mat Spatial Map 1 plot_img',\n               axes=axes[4])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **5.3 Visualizing 3D Data on 3D Space**\n3D spatial maps can also be visualized on 3D space with `nilearn` package. `view_img_on_surf` used as the plotting function and it renders the 3D data onto a 3D brain figure. 3D visualizations can be thresholded with `threshold` parameter as well. ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"view = nlplt.view_img_on_surf(img,\n                              title='10001.mat Spatial Map 1 view_img_on_surf',\n                              title_fontsize=20,\n                              threshold=1,\n                              black_bg=False)\nview.open_in_browser()\nview","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns in `fnc.csv`, `loading.csv` and `train_scores.csv` are merged on `Id`. Two features `is_train` and `site` are created for labeling different datasets and sites. Data is saved in pickle format for faster save and load time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_pickle('trends_tabular_data.pkl')\n\nprint(f'TReNDS Tabular Data Shape = {df.shape}')\nprint(f'TReNDS Tabular Data Memory Usage = {df.memory_usage().sum() / 1024 ** 2:.2f} MB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# visualization using nilearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !wget https://github.com/Chaogan-Yan/DPABI/raw/master/Templates/ch2better.nii","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nmask_filename = '../input/trends-assessment-prediction/fMRI_mask.nii'\nsubject_filename = '../input/trends-assessment-prediction/fMRI_train/10004.mat'\nsmri_filename = 'ch2better.nii'\nmask_niimg = nl.image.load_img(mask_filename)\n\ndef load_subject(filename, mask_niimg):\n    \"\"\"\n    Load a subject saved in .mat format with\n        the version 7.3 flag. Return the subject\n        niimg, using a mask niimg as a template\n        for nifti headers.\n        \n    Args:\n        filename    <str>            the .mat filename for the subject data\n        mask_niimg  niimg object     the mask niimg object used for nifti headers\n    \"\"\"\n    subject_data = None\n    with h5py.File(subject_filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    return subject_niimg\nsubject_niimg = load_subject(subject_filename, mask_niimg)\nprint(\"Image shape is %s\" % (str(subject_niimg.shape)))\nnum_components = subject_niimg.shape[-1]\nprint(\"Detected {num_components} spatial maps\".format(num_components=num_components))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlplt.plot_prob_atlas(subject_niimg, bg_img=smri_filename, view_type='filled_contours', draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_size = int(np.ceil(np.sqrt(num_components)))\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*10, grid_size*10))\n[axi.set_axis_off() for axi in axes.ravel()]\nrow = -1\nfor i, cur_img in enumerate(nl.image.iter_img(subject_niimg)):\n    col = i % grid_size\n    if col == 0:\n        row += 1\n    nlplt.plot_stat_map(cur_img, bg_img=smri_filename, title=\"IC %d\" % i, axes=axes[row, col], threshold=3, colorbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\nfrom nilearn import datasets\nhaxby_dataset = datasets.fetch_haxby()   # load dataset\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\n# Build the mean image because we have no anatomic data\nfrom nilearn import image\nfunc_filename = haxby_dataset.func[0]\nmean_img = image.mean_img(func_filename)\n\nz_slice = -14\n\nfig = plt.figure(figsize=(4, 5.4), facecolor='k')\n\nfrom nilearn.plotting import plot_anat, show\ndisplay = plot_anat(mean_img, display_mode='z', cut_coords=[z_slice],\n                    figure=fig)\nmask_vt_filename = haxby_dataset.mask_vt[0]\nmask_house_filename = haxby_dataset.mask_house[0]\nmask_face_filename = haxby_dataset.mask_face[0]\ndisplay.add_contours(mask_vt_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['red'])\ndisplay.add_contours(mask_house_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['blue'])\ndisplay.add_contours(mask_face_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['limegreen'])\n\n# We generate a legend using the trick described on\n# http://matplotlib.sourceforge.net/users/legend_guide.httpml#using-proxy-artist\nfrom matplotlib.patches import Rectangle\np_v = Rectangle((0, 0), 1, 1, fc=\"red\")\np_h = Rectangle((0, 0), 1, 1, fc=\"blue\")\np_f = Rectangle((0, 0), 1, 1, fc=\"limegreen\")\nplt.legend([p_v, p_h, p_f], [\"vt\", \"house\", \"face\"])\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import datasets\n\n# haxby dataset to have EPI images and masks\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# one motor contrast map from NeuroVault\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nilearn import plotting\n\n# Visualizing t-map image on EPI template with manual\n# positioning of coordinates using cut_coords given as a list\nplotting.plot_stat_map(stat_img,\n                       threshold=3, title=\"plot_stat_map\",\n                       cut_coords=[36, -27, 66])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view = plotting.view_img(stat_img, threshold=3)\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_glass_brain(stat_img, title='plot_glass_brain',\n                          threshold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_anat(haxby_anat_filename, title=\"plot_anat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_roi(haxby_mask_filename, bg_img=haxby_anat_filename,\n                  title=\"plot_roi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import image processing tool\n# from nilearn import image\n\n# Compute the voxel_wise mean of functional images across time.\n# Basically reducing the functional image from 4D to 3D\nmean_haxby_img = image.mean_img(haxby_func_filename)\n\n# Visualizing mean image (3D)\nplotting.plot_epi(mean_haxby_img, title=\"plot_epi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# haxby dataset to have anatomical image, EPI images and masks\n#haxby_dataset = datasets.fetch_haxby()\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# localizer dataset to have contrast maps\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='ortho',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='ortho', cut_coords=[36, -27, 60]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='z', cut_coords=5,\n                       title=\"display_mode='z', cut_coords=5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='x',\n                       cut_coords=[-36, 36],\n                       title=\"display_mode='x', cut_coords=[-36, 36]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='y', cut_coords=1,\n                       title=\"display_mode='y', cut_coords=1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='z',\n                       cut_coords=1, colorbar=False,\n                       title=\"display_mode='z', cut_coords=1, colorbar=False\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='xz',\n                       cut_coords=[36, 60],\n                       title=\"display_mode='xz', cut_coords=[36, 60]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='yx',\n                       cut_coords=[-27, 36],\n                       title=\"display_mode='yx', cut_coords=[-27, 36]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='yz',\n                       cut_coords=[-27, 60],\n                       title=\"display_mode='yz', cut_coords=[-27, 60]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(stat_img, display_mode='tiled',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='tiled'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import image\n\n# Compute voxel-wise mean functional image across time dimension. Now we have\n# functional image in 3D assigned in mean_haxby_img\nmean_haxby_img = image.mean_img(haxby_func_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img, title=\"add_edges\")\n\n# We are now able to use add_edges method inherited in plotting object named as\n# display. First argument - anatomical image  and by default edges will be\n# displayed as red 'r', to choose different colors green 'g' and  blue 'b'.\ndisplay.add_edges(haxby_anat_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As seen before, we call the `plot_anat` function with a background image\n# as first argument, in this case again the mean fMRI image and argument\n# `cut_coords` as list for manual cut with coordinates pointing at masked\n# brain regions\ndisplay = plotting.plot_anat(mean_haxby_img, title=\"add_contours\",\n                             cut_coords=[-34, -39, -9])\n# Now use `add_contours` in display object with the path to a mask image from\n# the Haxby dataset as first argument and argument `levels` given as list\n# of values to select particular level in the contour to display and argument\n# `colors` specified as red 'r' to see edges as red in color.\n# See help on matplotlib.pyplot.contour to use more options with this method\ndisplay.add_contours(haxby_mask_filename, levels=[0.5], colors='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"add_contours with filled=True\",\n                             cut_coords=[-34, -39, -9])\n\n# By default, no color fillings will be shown using `add_contours`. To see\n# contours with color fillings use argument filled=True. contour colors are\n# changed to blue 'b' with alpha=0.7 sets the transparency of color fillings.\n# See help on matplotlib.pyplot.contourf to use more options given that filled\n# should be True\ndisplay.add_contours(haxby_mask_filename, filled=True, alpha=0.7,\n                     levels=[0.5], colors='b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img, title=\"add_markers\",\n                             cut_coords=[-34, -39, -9])\n\n# Coordinates of seed regions should be specified in first argument and second\n# argument `marker_color` denotes color of the sphere in this case yellow 'y'\n# and third argument `marker_size` denotes size of the sphere\ncoords = [(-34, -39, -9)]\ndisplay.add_markers(coords, marker_color='y', marker_size=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True, scale_size=25, scale_units='mm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import datasets\n\nrest_dataset = datasets.fetch_development_fmri(n_subjects=20)\nfunc_filenames = rest_dataset.func\nconfounds = rest_dataset.confounds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import dictionary learning algorithm from decomposition module and call the\n# object and fit the model to the functional datasets\nfrom nilearn.decomposition import DictLearning\n\n# Initialize DictLearning object\ndict_learn = DictLearning(n_components=8, smoothing_fwhm=6.,\n                          memory=\"nilearn_cache\", memory_level=2,\n                          random_state=0)\n# Fit to the data\ndict_learn.fit(func_filenames)\n# Resting state networks/maps in attribute `components_img_`\n# Note that this attribute is implemented from version 0.4.1.\n# For older versions, see the note section above for details.\ncomponents_img = dict_learn.components_img_\n\n# Visualization of functional networks\n# Show networks using plotting utilities\n# from nilearn import plotting\n\nplotting.plot_prob_atlas(components_img, view_type='filled_contours',\n                         title='Dictionary Learning maps')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Region Extractor algorithm from regions module\n# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n# maps, less the threshold means that more intense non-voxels will be survived.\nfrom nilearn.regions import RegionExtractor\n\nextractor = RegionExtractor(components_img, threshold=0.5,\n                            thresholding_strategy='ratio_n_voxels',\n                            extractor='local_regions',\n                            standardize=True, min_region_size=1350)\n# Just call fit() to process for regions extraction\nextractor.fit()\n# Extracted regions are stored in regions_img_\nregions_extracted_img = extractor.regions_img_\n# Each region index is stored in index_\nregions_index = extractor.index_\n# Total number of regions extracted\nn_regions_extracted = regions_extracted_img.shape[-1]\n\n# Visualization of region extraction results\ntitle = ('%d regions are extracted from %d components.'\n         '\\nEach separate color of region indicates extracted region'\n         % (n_regions_extracted, 8))\nplotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n                         title=title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we need to do subjects timeseries signals extraction and then estimating\n# correlation matrices on those signals.\n# To extract timeseries signals, we call transform() from RegionExtractor object\n# onto each subject functional data stored in func_filenames.\n# To estimate correlation matrices we import connectome utilities from nilearn\nfrom nilearn.connectome import ConnectivityMeasure\n\ncorrelations = []\n# Initializing ConnectivityMeasure object with kind='correlation'\nconnectome_measure = ConnectivityMeasure(kind='correlation')\nfor filename, confound in zip(func_filenames, confounds):\n    # call transform from RegionExtractor object to extract timeseries signals\n    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n    # call fit_transform from ConnectivityMeasure object\n    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n    # saving each subject correlation to correlations\n    correlations.append(correlation)\n\n# Mean of all correlations\n# import numpy as np\nmean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n                                                          n_regions_extracted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Correlation between %d regions' % n_regions_extracted\n\n# First plot the matrix\ndisplay = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n                               colorbar=True, title=title)\n\n# Then find the center of the regions and plot a connectome\nregions_img = regions_extracted_img\ncoords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n\nplotting.plot_connectome(mean_correlations, coords_connectome,\n                         edge_threshold='90%', title=title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we plot a network of index=4 without region extraction (left plot)\n# from nilearn import image\n\nimg = image.index_img(components_img, 4)\ncoords = plotting.find_xyz_cut_coords(img)\ndisplay = plotting.plot_stat_map(img, cut_coords=coords, colorbar=False,\n                                 title='Showing one specific network')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For this, we take the indices of the all regions extracted related to original\n# network given as 4.\nregions_indices_of_map3 = np.where(np.array(regions_index) == 4)\n\ndisplay = plotting.plot_anat(cut_coords=coords,\n                             title='Regions from this network')\n\n# Add as an overlay all the regions of index 4\ncolors = 'rgbcmyk'\nfor each_index_of_map3, color in zip(regions_indices_of_map3[0], colors):\n    display.add_overlay(image.index_img(regions_extracted_img, each_index_of_map3),\n                        cmap=plotting.cm.alpha_cmap(color))\n\nplotting.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import datasets\n\n# By default 2nd subject will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First anatomical nifti image (3D) located is at: %s' %\n      haxby_dataset.anat[0])\nprint('First functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nilearn.image.image import mean_img\n\n# Compute the mean EPI: we do the mean along the axis 3, which is time\nfunc_filename = haxby_dataset.func[0]\nmean_haxby = mean_img(func_filename)\n\nfrom nilearn.plotting import plot_epi, show\nplot_epi(mean_haxby)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nilearn.masking import compute_epi_mask\nmask_img = compute_epi_mask(func_filename)\n\n# Visualize it as an ROI\nfrom nilearn.plotting import plot_roi\nplot_roi(mask_img, mean_haxby)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nilearn.masking import apply_mask\nmasked_data = apply_mask(func_filename, mask_img)\n\n# masked_data shape is (timepoints, voxels). We can plot the first 150\n# timepoints from two voxels\n\n# And now plot a few of these\n# import matplotlib.pyplot as plt\nplt.figure(figsize=(7, 5))\nplt.plot(masked_data[:150, :2])\nplt.xlabel('Time [TRs]', fontsize=16)\nplt.ylabel('Intensity', fontsize=16)\nplt.xlim(0, 150)\nplt.subplots_adjust(bottom=.12, top=.95, right=.95, left=.12)\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import datasets\n\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fsaverage = datasets.fetch_surf_fsaverage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nilearn import surface\n\ntexture = surface.vol_to_surf(stat_img, fsaverage.pial_right)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import plotting\n\nplotting.plot_surf_stat_map(fsaverage.infl_right, texture, hemi='right',\n                            title='Surface right hemisphere', colorbar=True,\n                            threshold=1., bg_map=fsaverage.sulc_right)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_glass_brain(stat_img, display_mode='r', plot_abs=False,\n                          title='Glass brain', threshold=2.)\n\nplotting.plot_stat_map(stat_img, display_mode='x', threshold=1.,\n                       cut_coords=range(0, 51, 10), title='Slices')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_fsaverage = datasets.fetch_surf_fsaverage('fsaverage')\nbig_texture = surface.vol_to_surf(stat_img, big_fsaverage.pial_right)\n\nplotting.plot_surf_stat_map(big_fsaverage.infl_right,\n                            big_texture, hemi='right', colorbar=True,\n                            title='Surface right hemisphere: fine mesh',\n                            threshold=1., bg_map=big_fsaverage.sulc_right)\n\n\nplotting.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view = plotting.view_surf(fsaverage.infl_right, texture, threshold='90%',\n                          bg_map=fsaverage.sulc_right)\n\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"view = plotting.view_img_on_surf(stat_img, threshold='90%')\n# view.open_in_browser()\n\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import datasets\nprint('Datasets are stored in: %r' % datasets.get_data_dirs())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"motor_images = datasets.fetch_neurovault_motor_task()\nmotor_images.images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmap_filename = motor_images.images[0]\n\n# from nilearn import plotting\nplotting.plot_stat_map(tmap_filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(tmap_filename, threshold=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsn = datasets.fetch_atlas_smith_2009()['rsn10']\nrsn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nilearn import image\nprint(image.load_img(rsn).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_rsn = image.index_img(rsn, 0)\nprint(first_rsn.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotting.plot_stat_map(first_rsn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img in image.iter_img(rsn):\n    # img is now an in-memory 3D img\n    plotting.plot_stat_map(img, threshold=3, display_mode=\"z\", cut_coords=1,\n                           colorbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_volumes = image.index_img(rsn, slice(3, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img in image.iter_img(selected_volumes):\n    plotting.plot_stat_map(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load 4D probabilistic atlases\n# from nilearn import datasets\n\n# Harvard Oxford Atlasf\nharvard_oxford = datasets.fetch_atlas_harvard_oxford('cort-prob-2mm')\nharvard_oxford_sub = datasets.fetch_atlas_harvard_oxford('sub-prob-2mm')\n\n# Multi Subject Dictionary Learning Atlas\nmsdl = datasets.fetch_atlas_msdl()\n\n# Smith ICA Atlas and Brain Maps 2009\nsmith = datasets.fetch_atlas_smith_2009()\n\n# ICBM tissue probability\nicbm = datasets.fetch_icbm152_2009()\n\n# Allen RSN networks\nallen = datasets.fetch_atlas_allen_2011()\n\n# Pauli subcortical atlas\nsubcortex = datasets.fetch_atlas_pauli_2017()\n\n# Visualization\n# from nilearn import plotting\n\natlas_types = {'Harvard_Oxford': harvard_oxford.maps,\n               'Harvard_Oxford sub': harvard_oxford_sub.maps,\n               'MSDL': msdl.maps, 'Smith 2009 10 RSNs': smith.rsn10,\n               'Smith2009 20 RSNs': smith.rsn20,\n               'Smith2009 70 RSNs': smith.rsn70,\n               'Smith2009 20 Brainmap': smith.bm20,\n               'Smith2009 70 Brainmap': smith.bm70,\n               'ICBM tissues': (icbm['wm'], icbm['gm'], icbm['csf']),\n               'Allen2011': allen.rsn28,\n               'Pauli2017 Subcortical Atlas': subcortex.maps,\n               }\n\nfor name, atlas in sorted(atlas_types.items()):\n    plotting.plot_prob_atlas(atlas, title=name)\n\n# An optional colorbar can be set\nplotting.plot_prob_atlas(smith.bm10, title='Smith2009 10 Brainmap (with'\n                                           ' colorbar)',\n                         colorbar=True)\nprint('ready')\nplotting.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install joypy --progress-bar off","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\nimport random\nimport tensorflow as tf\nimport cv2\n# General packages\nimport PIL\nimport plotly.graph_objs as go\nfrom IPython.display import Image, display\nimport joypy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle/input/trends-assessment-prediction/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_PATH = '../input/trends-assessment-prediction'\n\n# image and mask directories\ntrain_data_dir = f'{BASE_PATH}/fMRI_train'\ntest_data_dir = f'{BASE_PATH}/fMRI_test'\n\n\nprint('Reading data...')\nloading_data = pd.read_csv(f'{BASE_PATH}/loading.csv')\ntrain_data = pd.read_csv(f'{BASE_PATH}/train_scores.csv')\nsample_submission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')\nprint('Reading data completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_data.head())\nprint(\"Shape of train_data :\", train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(loading_data.head())\nprint(\"Shape of loading_data :\", loading_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking missing data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = loading_data.isnull().sum().sort_values(ascending = False)\npercent = (loading_data.isnull().sum()/loading_data.isnull().count()*100).sort_values(ascending = False)\nmissing_loading_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_loading_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(df, feature, title='', show_percent = False, size=2):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    total = float(len(df))\n    sns.barplot(np.round(df[feature].value_counts().index).astype(int), df[feature].value_counts().values, alpha=0.8, palette='Set2')\n\n    plt.title(title)\n    if show_percent:\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(100*height/total),\n                    ha=\"center\", rotation=45) \n    plt.xlabel(feature, fontsize=12, )\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar(train_data, 'age', 'age count and %age plot', show_percent=True, size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_bar(df, feature, title='', show_percent = False, size=2):\n    f, ax = plt.subplots(1,1, figsize=(4*size,3*size))\n    total = float(len(df))\n    sns.barplot(np.round(df[feature].value_counts().index).astype(int), df[feature].value_counts().values, alpha=0.8, palette='Set2')\n\n    plt.title(title)\n    if show_percent:\n        for p in ax.patches:\n            height = p.get_height()\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(100*height/total),\n                    ha=\"center\", rotation=45) \n    plt.xlabel(feature, fontsize=12, )\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xticks(rotation=90)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Age count Distribution\nfor col in train_data.columns[2:]:\n    plot_bar(train_data, col, f'{col} count plot', size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_data =  train_data.drop(['Id'], axis=1)\n\nplt.figure(figsize = (12, 8))\nsns.heatmap(temp_data.corr(), annot = True, cmap=\"RdYlGn\")\nplt.yticks(rotation=0) \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_data =  loading_data.drop(['Id'], axis=1)\n\nplt.figure(figsize = (20, 20))\nsns.heatmap(temp_data.corr(), annot = True, cmap=\"RdYlGn\")\nplt.yticks(rotation=0) \n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_data =  loading_data.drop(['Id'], axis=1)\n# Create correlation matrix\ncorrel = temp_data.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = correl.where(np.triu(np.ones(correl.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.5\nto_drop = [column for column in upper.columns if any(upper[column] > 0.5)]\n\nprint('Very high correlated features: ', to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw Plot\nimport joypy\n\ntargets= loading_data.columns[1:]\n\n\nplt.figure(figsize=(16,10), dpi= 80)\nfig, axes = joypy.joyplot(loading_data, column=list(targets), ylim='own', figsize=(14,10))\n\n# Decoration\nplt.title('Distribution of features IC_01 to IC_29', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    Load and display a subject's spatial map\n\"\"\"\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nmask_filename = f'{BASE_PATH}/fMRI_mask.nii'\nsubject_filename = '../input/trends-assessment-prediction/fMRI_train/10004.mat'\nsmri_filename = 'ch2better.nii'\nmask_niimg = nl.image.load_img(mask_filename)\n\n\ndef load_subject(filename, mask_niimg):\n    \"\"\"\n    Load a subject saved in .mat format with\n        the version 7.3 flag. Return the subject\n        niimg, using a mask niimg as a template\n        for nifti headers.\n        \n    Args:\n        filename    <str>            the .mat filename for the subject data\n        mask_niimg  niimg object     the mask niimg object used for nifti headers\n    \"\"\"\n    subject_data = None\n    with h5py.File(subject_filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    return subject_niimg\nsubject_niimg = load_subject(subject_filename, mask_niimg)\nprint(\"Image shape is %s\" % (str(subject_niimg.shape)))\nnum_components = subject_niimg.shape[-1]\nprint(\"Detected {num_components} spatial maps\".format(num_components=num_components))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlplt.plot_prob_atlas(subject_niimg, bg_img=smri_filename, view_type='filled_contours', draw_cross=False,title='All %d spatial maps' % num_components, threshold='auto')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_size = int(np.ceil(np.sqrt(num_components)))\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*10, grid_size*10))\n[axi.set_axis_off() for axi in axes.ravel()]\nrow = -1\nfor i, cur_img in enumerate(nl.image.iter_img(subject_niimg)):\n    col = i % grid_size\n    if col == 0:\n        row += 1\n    nlplt.plot_stat_map(cur_img, bg_img=smri_filename, title=\"IC %d\" % i, axes=axes[row, col], threshold=3, colorbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n!pip install pycaret --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycaret.regression import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_PATH = '../input/trends-assessment-prediction'\n\nfnc_df = pd.read_csv(f\"{BASE_PATH}/fnc.csv\")\nloading_df = pd.read_csv(f\"{BASE_PATH}/loading.csv\")\nlabels_df = pd.read_csv(f\"{BASE_PATH}/train_scores.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nlabels_df[\"is_train\"] = True\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\ntest_df = test_df.drop(target_cols + ['is_train'], axis=1)\n\n# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1/500\ntest_df[fnc_features] *= FNC_SCALE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_models_dict = {\n    'age': 'age_br',\n    'domain1_var1':'domain1_var1_ridge',\n    'domain1_var2':'domain1_var2_svm',\n    'domain2_var1':'domain2_var1_ridge',\n    'domain2_var2':'domain2_var2_svm',\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.fixes import logsumexp ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor index, target in enumerate(target_cols):\n    model_name = target_models_dict[target]\n    model = load_model(f'../input/pycaret-trends-models/{model_name}', platform = None, authentication = None, verbose=True)\n\n    predictions = predict_model(model, data=test_df)\n    test_df[target] = predictions['Label'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\n\nsub_df.to_csv(\"submission1.csv\", index=False)\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.read_csv('/kaggle/input/trends-assessment-prediction/train_scores.csv')\nloading = pd.read_csv('/kaggle/input/trends-assessment-prediction/loading.csv')\nsubmission = pd.read_csv('/kaggle/input/trends-assessment-prediction/sample_submission.csv')\nfnc = pd.read_csv(\"/kaggle/input/trends-assessment-prediction/fnc.csv\")\nreveal = pd.read_csv('../input/trends-assessment-prediction/reveal_ID_site2.csv')\nnumbers = pd.read_csv('../input/trends-assessment-prediction/ICN_numbers.csv')\nfmri_mask = '../input/trends-assessment-prediction/fMRI_mask.nii'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smri = 'ch2better.nii'\nmask_img = nl.image.load_img(fmri_mask)\n\ndef load_subject(filename, mask_img):\n    subject_data = None\n    with h5py.File(filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_img = nl.image.new_img_like(mask_img, subject_data, affine=mask_img.affine, copy_header=True)\n\n    return subject_img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting 4D probabilistic atlas maps...\nProbabilistic atlasing is a research strategy whose goal is to generate anatomical templates that retain quantitative information on inter-subject variations in brain architecture (Mazziotta et al., 1995). A digital probabilistic atlas of the human brain, incorporating precise statistical information on positional variability of important functional and anatomic interfaces, may rectify many current atlasing problems, since it specifically stores information on the population variability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    nlplt.plot_prob_atlas(subject_img, bg_img=smri, view_type='filled_contours',\n                          draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting a statistical map\nStatistical parametric mapping or SPM is a statistical technique for examining differences in brain activity recorded during functional neuroimaging experiments.The measurement technique depends on the imaging technology (e.g., fMRI and PET). The scanner produces a 'map' of the area that is represented as voxels. Each voxel represents the activity of a specific volume in three-dimensional space. The exact size of a voxel varies depending on the technology. fMRI voxels typically represent a volume of 27 mm3 (a cube with 3mm length sides).\n\nParametric statistical models are assumed at each voxel, using the general linear model to describe the data variability in terms of experimental and confounding effects, with residual variability. Hypotheses expressed in terms of the model parameters are assessed at each voxel with univariate statistics.\n\nAnalyses may examine differences over time (i.e. correlations between a task variable and brain activity in a certain area) using linear convolution models of how the measured signal is caused by underlying changes in neural activity.\n\nBecause many statistical tests are conducted, adjustments have to be made to control for type I errors (false positives) potentially caused by the comparison of levels of activity over many voxels. A type I error would result in falsely assessing background brain activity as related to the task. Adjustments are made based on the number of resels in the image and the theory of continuous random fields in order to set a new criterion for statistical significance that adjusts for the problem of multiple comparisons.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_stat_map(first_rsn)\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    for img in image.iter_img(rsn):\n        # img is now an in-memory 3D img\n        plotting.plot_stat_map(img, threshold=3)\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Glass brain visualization...\nGlass Brain is a tool that maps the electrical activity of your brain in realtime.The anatomically realistic 3D brain will show realtime data from electroencephalographic (EEG) signals taken from a specially-designed EEG cap.This data is mapped to the source of that electrical activity, i.e. the specific part of the brain. The underlying brain model is generated through MRI scans so that the EEG data is accurately mapped to an individual's brain model.\n\nDifferent colours are given to the different signal frequency bands to create a beautiful interactive artwork that seems to crackle with energy, showing how information is transferred (or at least estimated to do so) between different regions of the brain.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)     \n    plotting.plot_glass_brain(first_rsn,display_mode='lyrz')\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting an EPI\nIn Echo-Planar Imaging (EPI)-based Magnetic Resonance Imaging (MRI), inter-subject registration typically uses the subject's T1-weighted (T1w) anatomical image to learn deformations of the subject's brain onto a template. The estimated deformation fields are then applied to the subject's EPI scans (functional or diffusion-weighted images) to warp the latter to a template space.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_epi(first_rsn)\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting an anatomical image\nMain Idea of this visualization technique is to provide the anatomical picture of the brain. Due to the measurement procedures, BOLD images usually have a realtively low resolution, as you want to squeeze in as many data-points along time as possible.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_anat(first_rsn)\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting ROIs, or a mask\nwhen mapping brain connectivities, ROIs provide the structural substrates for measuring connectivities within individual brains and for pooling data across populations. Thus, identification of reliable, reproducible and accurate ROIs is critically important for the success of brain connectivity mapping.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"files = random.choices(os.listdir('../input/trends-assessment-prediction/fMRI_train/'), k = 3)\nfor file in files:\n    subject = os.path.join('../input/trends-assessment-prediction/fMRI_train/', file)\n    subject_img = load_subject(subject, mask_img)\n    print(\"Image shape is %s\" % (str(subject_img.shape)))\n    num_components = subject_img.shape[-1]\n    print(\"Detected {num_components} spatial maps\".format(num_components=num_components))\n    rsn = subject_img\n    #convert to 3d image\n    first_rsn = image.index_img(rsn, 0)\n    print(first_rsn.shape)\n    plotting.plot_roi(first_rsn)\n    print(\"-\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3D Plots of statistical maps or atlases on the cortical surface","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"motor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]\nview = plotting.view_img_on_surf(stat_img, threshold='90%')\nview.open_in_browser()\nview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### WHAT DO WE DO TO MISSING VALUES?\nThere are several options for handling missing values each with its own PROS and CONS. However, the choice of what should be done is largely dependent on the nature of our data and the missing values. Below is a summary highlight of several options we have for handling missing values.\n\n#### DROP MISSING VALUES\nFILL MISSING VALUES WITH TEST STATISTIC(mean, median, mode).\nPREDICT MISSING VALUE WITH A MACHINE LEARNING ALGORITHM(knn).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features.fillna(features.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loading.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loading.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the correlation between features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(features.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(loading.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#main or, target element in problem\ntarget_col = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Visualize the Target Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 5, figsize=(20, 5))\nsns.distplot(features['age'], ax=ax[0],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('Age')\n\nsns.distplot(features['domain1_var1'], ax=ax[1],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain1_var1')\n\nsns.distplot(features['domain1_var2'], ax=ax[2],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain1_var2')\n\nsns.distplot(features['domain2_var1'], ax=ax[3],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain2_var1')\n\nsns.distplot(features['domain2_var2'], ax=ax[4],rug=True, rug_kws={\"color\": \"coral\"},\n                  kde_kws={\"color\": \"royalblue\", \"lw\": 1.5},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3,\n                            \"alpha\": 1, \"color\": \"coral\"}).set_title('domain2_var2')\n\nfig.suptitle('Target Visualization', fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\ng = sns.pairplot(data=features, hue='age', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Again, \n# Model using Rapids for 500x faster kNN on GPUs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Install Rapids for 500x faster kNN on GPUs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\n!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cudf\nimport cupy as cp\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import SVR\nfrom cuml.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import KFold\nfrom cuml.metrics import mean_absolute_error, mean_squared_error\n\n\ndef metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnc_df = cudf.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\nloading_df = cudf.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\n\n\nlabels_df = cudf.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\nlabels_df[\"is_train\"] = True\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1/600\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nNUM_FOLDS = 7\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\n\nfeatures = loading_features + fnc_features\n\noveral_score = 0\nfor target, c, w, ff in [(\"age\", 60, 0.3, 0.55), (\"domain1_var1\", 12, 0.175, 0.2), (\"domain1_var2\", 8, 0.175, 0.2), (\"domain2_var1\", 9, 0.175, 0.22), (\"domain2_var2\", 12, 0.175, 0.22)]:    \n    y_oof = np.zeros(df.shape[0])\n    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n\n    \n    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n        train_df = train_df[train_df[target].notnull()]\n\n        model_1 = SVR(C=c, cache_size=3000.0)\n        model_1.fit(train_df[features].values, train_df[target].values)\n        model_2 = Ridge(alpha = 0.0001)\n        model_2.fit(train_df[features].values, train_df[target].values)\n        val_pred_1 = model_1.predict(val_df[features])\n        val_pred_2 = model_2.predict(val_df[features])\n        \n        test_pred_1 = model_1.predict(test_df[features])\n        test_pred_2 = model_2.predict(test_df[features])\n        \n        val_pred = (1-ff)*val_pred_1+ff*val_pred_2\n        val_pred = cp.asnumpy(val_pred.values.flatten())\n        \n        test_pred = (1-ff)*test_pred_1+ff*test_pred_2\n        test_pred = cp.asnumpy(test_pred.values.flatten())\n\n        y_oof[val_ind] = val_pred\n        y_test[:, f] = test_pred\n        \n    df[\"pred_{}\".format(target)] = y_oof\n    test_df[target] = y_test.mean(axis=1)\n    \n    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    mae = mean_absolute_error(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n    rmse = np.sqrt(mean_squared_error(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values))\n    overal_score += w*score\n    print(target, np.round(score, 8))\n    print(target, np.round(score, 4))\n    print(target, 'mean absolute error:', np.round(mae, 8))\n    print(target, ' root mean square error:', np.round(rmse, 8))\n    print()\n    \nprint(\"Overal score:\", np.round(overal_score, 8))\nprint(\"Overal score:\", np.round(overal_score, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = cudf.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission_rapids_ensemble.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}