{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dual prediction of `domain2` using multi-loss neural network\n\nGiven the particular relationship between `domain2_var1` and `domain2_var2`, which I showed in [first notebook](https://www.kaggle.com/miykael/trends-exploration-of-the-targets)), I pursued an dense neural network modeling approach, with multi-loss. The idea was to find a prediction for the two `domain2` variables that is close to the original values, but where the rotated values are also close to the rotated predictions.\n\nAs a note, in this notebook I will be using the adapted targets, as well as the engineered features I've explored in my [second notebook](https://www.kaggle.com/miykael/trends-feature-exploration-engineering)).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom os.path import join as opj\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nsns.set_context('notebook')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid, KFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load targets and features\n\nFirst things first, let's load the adapted targets and prepared feature datasets from my [second notebook](https://www.kaggle.com/miykael/trends-feature-exploration-engineering).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/trends-feature-exploration-engineering/datasets'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load target values and corresponding scaler\nimport joblib\nscaler_targets = joblib.load(opj(path, 'targets_scaler.pkl'))\n\ntargets = pd.read_hdf(opj(path, 'targets.h5'))\ntargets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To load the feature matrix, let's write a function that performs the following steps:\n\n1. Load the train and test set.\n2. Select the two `domain2` targets.\n3. Centralize the corr_coef features to the median, which is about 0.689\n4. Scale the four feature datasets (IC, FNC, intra and inter correlations) according to a scaler called `scale_values`. Before this scaler is applied, all feature within a dataset are scaled to the 90% value (i.e. top 10%). No centralization was applied.\n5. Missing values are dropped.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset_and_scale(dataset_id='merge', scale_values=[1, 1, 1, 1]):\n\n    # Load dataset\n    X_tr = pd.read_hdf(opj(path, '%s_train.h5' % dataset_id))\n    X_te = pd.read_hdf(opj(path, '%s_test.h5' % dataset_id))\n\n    # Specify target\n    y = pd.read_hdf(opj(path, 'targets.h5'))\n    y_tr = y.loc[X_tr.index, 'domain2_var1':]\n\n    # Remove missing values\n    missval = y_tr.isnull().sum(axis=1)!=0\n    idx = ~missval\n    X_tr = X_tr[idx]\n    X_te = X_te\n    y_tr = y_tr[idx]\n    print('Removing %s missing values from target dataset.' % missval.sum())\n    \n    # Centralize corr_coef features\n    median_offset = X_tr.iloc[:, X_tr.columns.str.contains('corr_coef')].median().mean()\n    X_tr.iloc[:, X_tr.columns.str.contains('corr_coef')] -= median_offset\n    X_te.iloc[:, X_te.columns.str.contains('corr_coef')] -= median_offset\n\n    # Establish masks for different kinds of data\n    mask_ids = []\n    for m in ['IC_', '_vs_', 'corr_coef', '^c[0-9]+_c[0-9]+']:\n        mask_ids.append(X_tr.columns.str.contains(m))\n    mask_ids = np.array(mask_ids)\n\n    # Data scaling\n    for i, m in enumerate(mask_ids):\n\n        if m.sum()==0:\n            continue\n        \n        # Apply Scale\n        scale_value = scale_values[i]\n        unify_mask_scale = np.percentile(X_tr.iloc[:, m].abs(), 90)\n\n        X_te.iloc[:, m] /= unify_mask_scale\n        X_tr.iloc[:, m] /= unify_mask_scale\n    \n        X_te.iloc[:, m] *= scale_value\n        X_tr.iloc[:, m] *= scale_value\n\n    # Drop irrelevant measurements\n    X_tr.dropna(axis=1, inplace=True)\n    X_te.dropna(axis=1, inplace=True)\n    \n    # Drop duplicate rows\n    X_tr = X_tr.T.drop_duplicates().T\n    X_te = X_te.T.drop_duplicates().T\n    \n    print('Size of dataset (train/test): ', X_tr.shape, X_te.shape)\n    \n    X_tr = X_tr.values\n    X_te = X_te.values\n    y_tr = y_tr.values\n    \n    return X_tr, X_te, y_tr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To refresh our memories about which `domain2` relationship we want to profit from, let's visualize it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\nX_tr, X_te, y_tr = load_dataset_and_scale(dataset_id='merge', scale_values=[1, 1, 1, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Revert target transformation to original\ny_tr_orig = y_tr*scaler_targets.scale_[3:]+scaler_targets.mean_[3:]\ny_tr_orig[:, :3] = np.power(y_tr_orig[:, :3], 1./1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot domain2 relationship with and without rotation\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\nax[0].set_title('Domain2 relationship (orig)')\nax[0].scatter(y_tr_orig[:, 0], y_tr_orig[:, 1], s=1, alpha=0.5, c='b')\nax[0].axis('equal')\n\nax[1].set_title('Domain2 relationship (rotated)')\nax[1].scatter(y_tr_orig[:, 2], y_tr_orig[:, 3], s=1, alpha=0.5, c='r')\nax[1].axis('equal')\n\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Define scoring functions for `domain2`\n\nNow that the data is ready, let's prepare a simple scoring function that either focuses on `var1` or on `var2`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomLoss_focus(keras.losses.Loss):\n    def __init__(self, scale=None, mean=None, focus=0, name=\"loss\"):\n        super().__init__(name=name + '_%d' % (focus + 1))\n        self.scale = np.array(scale, dtype='float32')\n        self.mean = np.array(mean, dtype='float32')\n        self.focus = np.array(focus, dtype='int32')\n\n    @tf.function\n    def call(self, y_true, y_pred):\n\n        # Rescale values\n        y_true = tf.math.add(tf.math.multiply(self.scale, y_true), self.mean)\n        y_pred = tf.math.add(tf.math.multiply(self.scale, y_pred), self.mean)\n\n        # Revert power transformation\n        y_true = tf.transpose(tf.stack([\n            tf.math.pow(y_true[:, 0], 1./1.5),\n            tf.math.pow(y_true[:, 1], 1./1.5)]))\n        y_pred = tf.transpose(tf.stack([\n            tf.math.pow(y_pred[:, 0], 1./1.5),\n            tf.math.pow(y_pred[:, 1], 1./1.5)]))\n\n        # Appli competition loss function\n        scores = tf.math.divide(tf.math.reduce_sum(tf.math.abs(y_true - y_pred), axis=0),\n                                tf.math.reduce_sum(tf.math.abs(y_true), axis=0))\n        \n        # Focus on var1 or var2\n        score = scores[self.focus]\n\n        return score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Define function to create NN model\n\nThe neural network has a rather simple architecture. It is a dense, fully connected neural network, with the potential for `BatchNormalization` and `Dropout` after every layer. The final output layer contains two output neurons with as many activation functions as loss functions.\n\nThe loss functions can be weighted to be between var1 and var2 with a hinge function, so that the wait for loss1 and loss2 always equals 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_size=100,\n                activation='relu',\n                dropout=0.5,\n                kernel_initializer='glorot_uniform',\n                lr=1e-2,\n                use_batch=True,\n                use_dropout=True,\n                hidden=(32, 16),\n                hinge=0.5,\n               ):\n\n    # Specify layer structure\n    inputs = keras.Input(shape=input_size)\n    \n    for i, h in enumerate(hidden):\n        if i==0:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(inputs)\n        else:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(x)\n        if use_batch:\n            x = layers.BatchNormalization()(x)\n        \n        x = layers.Activation(activation)(x)\n\n        # Add Dropout layer if requested\n        if use_dropout:\n            x = layers.Dropout(dropout)(x)\n\n    out = layers.Dense(2)(x)\n    out1 = layers.Activation(None, name=\"loss1\")(out)\n    out2 = layers.Activation(None, name=\"loss2\")(out)\n\n    # Create Model\n    model = keras.Model(inputs, [out1, out2])\n\n    # Define loss function, optimizer and metrics to track during training\n    optimizer = keras.optimizers.Adagrad(\n            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n                lr, decay_steps=10000, decay_rate=0.96, staircase=True))\n    \n    l1 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=0)\n    l2 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=1)\n    \n    losses = {'loss1': l1, 'loss2': l2}\n    lossWeights = {\"loss1\": hinge, \"loss2\": 1-hinge}\n\n    model.compile(\n        optimizer = optimizer,\n        loss = losses,\n        loss_weights = lossWeights\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Define parameter grid and function to run the grid (using KFold)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_grid(input_size=0):\n    \n    # Define parameter grid\n    param_grid = [\n    { \n        'nn__input_size': [input_size],\n        'nn__lr': [0.05],\n        'nn__activation': ['sigmoid'],\n        'nn__hidden': [(128, 32),],\n        'nn__dropout': [0.25],\n        'nn__kernel_initializer': ['glorot_uniform'], #['glorot_uniform', 'normal', 'uniform'],\n        'nn__use_batch': [True],\n        'nn__use_dropout': [True],\n        'nn__hinge': [0., 0.5, 1.],\n        'batch_size': [32],\n        'epochs': [50],\n    }]\n    \n    # Create Parameter Grid and return it\n    grids = ParameterGrid(param_grid)\n    return grids\n\nlen(create_grid())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_grid(grid, X_tr, X_te, y_tr, cv=5, n=2):\n    \n    x_train = X_tr.copy()\n    x_test = X_te.copy()\n    print('Parameters:', grid)\n        \n    # Create callback object\n    callbacks = [keras.callbacks.EarlyStopping(\n        monitor=\"loss\", min_delta=1e-5, patience=5, verbose=1)]\n\n    # Build NN model\n    nn_grid ={}\n    for k in grid.keys():\n        if 'nn__' in k:\n            nn_grid[k[4:]] = grid[k]\n\n    # Run NN fit on multiple folds\n    history_runs = {'loss': [], 'val_loss': []}\n    for i in range(n):\n        history_runs['loss%d' % (i+1)] = []\n        history_runs['val_loss%d' % (i+1)] = []\n\n    kfold = KFold(n_splits=cv, shuffle=True)\n    \n    preds_cv_tr = []\n    preds_cv_te = []\n    counter = 0\n    for train_idx, val_idx in kfold.split(x_train):\n        \n        # Prepare data for split\n        x_train_k = x_train[train_idx]\n        x_val_k = x_train[val_idx]\n\n        y_train_k = y_tr[train_idx]\n        y_val_k = y_tr[val_idx]\n\n        # Prepare model\n        model = build_model(**nn_grid)\n\n        # Fit model\n        history = model.fit(\n            x_train_k,\n            y_train_k,\n            validation_data=(x_val_k, y_val_k),\n            epochs=grid['epochs'],\n            batch_size=grid['batch_size'],\n            #callbacks=callbacks,\n            shuffle=True, verbose=0\n        )\n        history_runs['loss'].append(history.history['loss'])\n        history_runs['val_loss'].append(history.history['val_loss'])\n        \n        for i in range(n):\n            history_runs['loss%d' % (i+1)].append(history.history['loss%d_loss' % (i+1)])\n            history_runs['val_loss%d' % (i+1)].append(history.history['val_loss%d_loss' % (i+1)])\n        \n        # Compute predictions\n        preds_cv_tr.append(model.predict(x_train)[0])\n        preds_cv_te.append(model.predict(x_test)[0])\n        \n        # Report counter\n        print('CV: %02d/%02d' % (counter+1, cv))\n        counter += 1\n\n    # Compute mean and standard deviation\n    for l in [''] + [str(i+1) for i in range(n)]:\n        history_runs['loss'+l+'_mean'] = pd.DataFrame(history_runs['loss'+l+'']).T.mean(axis=1).values\n        history_runs['val_loss'+l+'_mean'] = pd.DataFrame(history_runs['val_loss'+l+'']).T.mean(axis=1).values\n        history_runs['loss'+l+'_std'] = pd.DataFrame(history_runs['loss'+l+'']).T.std(axis=1).values\n        history_runs['val_loss'+l+'_std'] = pd.DataFrame(history_runs['val_loss'+l+'']).T.std(axis=1).values\n    \n    # Collect mean and median predictions\n    df_pred_mean_tr = pd.DataFrame(np.mean([e for e in preds_cv_tr if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n    df_pred_mean_te = pd.DataFrame(np.mean([e for e in preds_cv_te if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n\n    df_pred_median_tr = pd.DataFrame(np.median([e for e in preds_cv_tr if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n    df_pred_median_te = pd.DataFrame(np.median([e for e in preds_cv_te if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n\n    # Assign closest value from training set\n    for tid in range(2):\n        unique_values = np.unique(y_tr[:, tid])\n        for d in [df_pred_mean_tr, df_pred_mean_te, df_pred_median_tr, df_pred_median_te]:\n            for i in range(len(d)):\n                d.iloc[i, tid] = unique_values[np.argmin(np.abs(d.iloc[i, tid]-unique_values))]\n\n    return x_train, x_test, history_runs, model, df_pred_mean_tr, df_pred_mean_te, df_pred_median_tr, df_pred_median_te","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scores(y_tr, pred_tr, scaler=None, title=''):\n\n    scores = []\n    preds = []\n    trues = []\n    for i, tidx in enumerate([3, 4]):\n\n        # Invert scaler\n        t_true = scaler.inverse_transform(np.transpose([y_tr[:, i]] * 7))[:, tidx]\n        t_pred = scaler.inverse_transform(np.transpose([pred_tr.iloc[:, i]] * 7))[:, tidx]\n\n        # Invert power transformation\n        t_true = np.power(t_true, 1./1.5)\n        t_pred = np.power(t_pred, 1./1.5)\n\n        # Compute the score\n        score = np.mean(np.sum(np.abs(t_true - t_pred), axis=0) / np.sum(t_true, axis=0))\n        scores.append(score)\n        preds.append(t_pred)\n        trues.append(t_true)\n\n    scores = np.array(scores)\n    preds = np.array(preds)\n    trues = np.array(trues)\n\n    pred_score = list(np.round(scores, 5))\n    plt.figure(figsize=(6, 6))\n    plt.title('Score on whole train set (var1, var2): ' + title + ' - ' + str(pred_score))\n    print('Score on whole train set (var1, var2) %s: ' % title, pred_score)\n    plt.scatter(trues[0], trues[1], s=0.5, alpha=0.5);\n    plt.scatter(preds[0], preds[1], s=0.5, alpha=0.5);\n    plt.axis('equal')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Write a few supportive visualization functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history_and_predictions(history, pred_tr, y_tr, cutoff=0, n=2):\n\n    # Plot history results\n    for l in [''] + [str(i+1) for i in range(n)]:\n        fig = plt.figure(constrained_layout=True, figsize=(14, 3.5))\n        plt.plot(history['loss'+l+'_mean'][cutoff:], label='train loss'+l)\n        plt.plot(history['val_loss'+l+'_mean'][cutoff:], label='val loss'+l, linestyle='dotted')\n\n        for m in ['loss'+l, 'val_loss'+l]:\n            plt.fill_between(np.arange(len(history['%s_mean' % m][cutoff:])),\n                             history['%s_mean' % m][cutoff:]-history['%s_std' % m][cutoff:],\n                             history['%s_mean' % m][cutoff:]+history['%s_std' % m][cutoff:],\n                             alpha=0.3)\n        plt.title('Validation loss{}: {:.4f} (mean last 5)'.format(str(l),\n            np.mean(history['val_loss'+l+'_mean'][-5:])\n        ))\n        plt.xlabel('epoch')\n        plt.ylabel('loss'+l+' value')\n        plt.legend()\n        plt.show()\n\n    # Categories\n    categories = ['domain2_var1', 'domain2_var2']\n\n    # Create subplot grid\n    fig = plt.figure(constrained_layout=True, figsize=(9, 4))\n    gs = fig.add_gridspec(1, 2)\n    ax = [fig.add_subplot(gs[i]) for i in range(2)]\n\n    # Plot discrepancy on training set\n    for i, axt in enumerate(ax):\n        sns.regplot(x=pred_tr[0][i, :], y=y_tr[:, i], marker='.', ax=axt,\n                    scatter_kws={'alpha': 0.33, 's': 1})\n        sns.regplot(x=pred_tr[1][i, :], y=y_tr[:, i], marker='.', ax=axt,\n                    scatter_kws={'alpha': 0.33, 's': 1})\n        axt.set_title('Prediction X_tr: %s' % categories[i])\n        axt.set_xlim(-3, 3)\n        axt.set_ylim(-3, 3)\n        axt.legend(['Mean', 'Median'])\n        axt.set_aspect('equal')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_scores(y_tr, pred_tr, scaler=None):\n\n    scores = []\n    preds = []\n    trues = []\n    for j, tidx in enumerate([3, 4]):\n\n        # Invert scaler\n        t_true = scaler.inverse_transform(np.transpose([y_tr[:, j]] * 7))[:, tidx]\n        t_pred = scaler.inverse_transform(np.transpose([pred_tr.iloc[:, j]] * 7))[:, tidx]\n\n        # Invert power transformation\n        t_true = np.power(t_true, 1./1.5)\n        t_pred = np.power(t_pred, 1./1.5)\n\n        # Compute the score\n        score = np.mean(np.sum(np.abs(t_true - t_pred), axis=0) / np.sum(t_true, axis=0))\n        scores.append(score)\n        preds.append(t_pred)\n        trues.append(t_true)\n\n    return np.array(scores), np.array(preds), np.array(trues)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predictions_m(df_pred_mean_tr, df_pred_median_tr, y_tr, scaler=None):\n\n    # Categories\n    categories = ['mean', 'median']\n    preds_m = [df_pred_mean_tr, df_pred_median_tr]\n\n    # Create subplot grid\n    fig = plt.figure(constrained_layout=True, figsize=(11, 5))\n    gs = fig.add_gridspec(1, 2)\n    ax = [fig.add_subplot(gs[i]) for i in range(2)]\n\n    # Plot discrepancy on training set\n    for i, axt in enumerate(ax):\n        scores, preds, trues = get_scores(y_tr, preds_m[i], scaler=scaler)\n        pred_score = list(np.round(scores, 5))\n        axt.scatter(trues[0], trues[1], s=0.5, alpha=0.5);\n        axt.scatter(preds[0], preds[1], s=0.5, alpha=0.5);\n        axt.set_title('Score on whole train set (var1, var2):\\n' + categories[i] + ' - ' + str(pred_score))\n        axt.legend(['Target', 'Prediction'])\n        axt.set_aspect('equal')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_score_info(history, n=2):\n    tr_mean = np.round(np.mean(history['loss_mean'][-5:]), 6)\n    te_mean = np.round(np.mean(history['val_loss_mean'][-5:]), 6)\n\n    res = [tr_mean, te_mean]\n    \n    for i in range(n):\n        res.append(np.round(np.mean(history['loss%d_mean' % (i+1)][-5:]), 6))\n        res.append(np.round(np.mean(history['val_loss%d_mean' % (i+1)][-5:]), 6))\n\n    return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Dual `domain2` prediction based on MAE score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_prediction(dataset_id='merge', cv=5, n=2, scale_values=None):\n\n    # Extract dataset\n    X_tr, X_te, y_tr = load_dataset_and_scale(dataset_id, scale_values=scale_values)\n    \n    # Reduce target to number of outputs\n    y_tr = y_tr[:, :2]\n\n    # Create grid search object\n    grids = create_grid(input_size=X_tr.shape[1])\n    print('%0d grid points will be checked!' % len(grids))\n    print('\\n---Start_Grid_Exploration---\\n')\n \n    # Go through the grids\n    preds_tr = []\n    preds_te = []\n    df_grids = []\n    for gidx, grid in enumerate(tqdm(grids)):\n\n        # Run grid\n        print('\\nGrid point: %04d/%04d' % (gidx + 1, len(grids)))\n        x_train, x_test, history, model, df_pred_mean_tr, df_pred_mean_te, df_pred_median_tr, df_pred_median_te = run_grid(grid, X_tr, X_te, y_tr, cv=cv, n=n)\n\n        # Compute predictions\n        preds_tr.append([df_pred_mean_tr, df_pred_median_tr])\n        preds_te.append([df_pred_mean_te, df_pred_median_te])\n\n        # Plot history results and predictions\n        plot_history_and_predictions(history, [df_pred_mean_tr.T.values,df_pred_median_tr.T.values], y_tr, cutoff=0, n=n)\n        plot_predictions_m(df_pred_mean_tr, df_pred_median_tr, y_tr, scaler=scaler_targets)\n        \n        # Extract scores\n        score_means = extract_score_info(history, n=n)\n\n        # Store everything in a grid\n        df_grid = pd.DataFrame(columns=grid.keys())\n        for k in grid:\n            df_grid.loc[0, k] = str(grid[k])\n        df_grid.insert(0, 'tr_mean', score_means[0])\n        df_grid.insert(1, 'te_mean', score_means[1])\n        for i in range(n):\n            df_grid.insert(2+(i*2), 'tr_mean%d' % (i+1), score_means[2+(i*2)])\n            df_grid.insert(3+(i*2), 'te_mean%d' % (i+1), score_means[3+(i*2)])\n        df_grids.append(df_grid)\n\n    # Summarize fit in datatable\n    df_fit = pd.concat(df_grids).reset_index().drop(columns='index')\n    display(df_fit)\n    \n    return preds_tr, preds_te, df_fit.iloc[0], df_grid, y_tr, history","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Define feature set scales (~average from my second notebook)\nscale_values = [0.25, 0.025, 0.04, 0.02]\n\n# Run model fit\npreds_tr, preds_te, top_grid, df_grid, y_tr, history = run_prediction(cv=8, n=2, scale_values=scale_values)\n\n# Feedback of overall score\nprint('Best score at %s / %s' % (top_grid['te_mean'], top_grid['tr_mean']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation 1\n\nDepending on the focus (i.e. focus on `var1` or `var2`, the prediction is spreading into the direction of the component and ignoring the other. A simultaneous fitting doesn't seem to improve the prediction quality, it only helps to reduce standard deviation between the folds.\n\nAlso, the network clearly starts to overfit and the standard deviation between the different folds is rather big. No reasonable improvement in prediction score can be achieved in such a way.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 7. Dual `domain2` prediction based on distance metric\n\nThe thought here is the following. What we try to minmize is the distance between a predicted point and the target point, where x and y coordinates are the var1 and var2 values. The hope was, that by including the rotated distance metric as well, the prediction of the original values would improve. Because only if the prediction (i.e. the x and y coordinates) are close enough, the rotation makes sense.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomLoss_dist(keras.losses.Loss):\n    def __init__(self, name=\"loss\"):\n        super().__init__(name=name)\n\n    @tf.function\n    def call(self, y_true, y_pred):\n\n        # Compute distance to target\n        score = tf.math.pow(tf.reduce_mean(tf.math.pow(tf.norm(tf.math.subtract(y_true, y_pred), axis=1), 3.)), 1./3.)\n        \n        return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomLoss_dist_rot(keras.losses.Loss):\n    def __init__(self, scale=None, mean=None, name=\"loss_rot\"):\n        super().__init__(name=name)\n        self.scale = np.array(scale, dtype='float32')\n        self.mean = np.array(mean, dtype='float32')\n\n    @tf.function\n    def call(self, y_true, y_pred):\n\n        # Rescale values\n        y_true = tf.math.add(tf.math.multiply(self.scale[:2], y_true), self.mean[:2])\n        y_pred = tf.math.add(tf.math.multiply(self.scale[:2], y_pred), self.mean[:2])\n\n        # Revert power transformation\n        y_true = tf.transpose(tf.stack([\n            tf.math.pow(y_true[:, 0], 1./1.5),\n            tf.math.pow(y_true[:, 1], 1./1.5)]))\n        y_pred = tf.transpose(tf.stack([\n            tf.math.pow(y_pred[:, 0], 1./1.5),\n            tf.math.pow(y_pred[:, 1], 1./1.5)]))\n\n        # Apply rotation for true and preds\n        d2_rot = 0.90771256655\n        radians = tf.constant(d2_rot, dtype='float32')\n        yt_rot = tf.transpose(tf.stack([\n            tf.math.add(tf.math.multiply(y_true[:, 0], [tf.cos(radians)]),\n                        tf.math.multiply(y_true[:, 1], [tf.sin(radians)])),\n            tf.math.add(-tf.math.multiply(y_true[:, 0], [tf.sin(radians)]),\n                        tf.math.multiply(y_true[:, 1], [tf.cos(radians)]))\n        ]))\n        yp_rot = tf.transpose(tf.stack([\n            tf.math.add(tf.math.multiply(y_pred[:, 0], [tf.cos(radians)]),\n                        tf.math.multiply(y_pred[:, 1], [tf.sin(radians)])),\n            tf.math.add(-tf.math.multiply(y_pred[:, 0], [tf.sin(radians)]),\n                        tf.math.multiply(y_pred[:, 1], [tf.cos(radians)]))\n        ]))        \n\n        # Rescale values\n        yt_rot = tf.math.divide(tf.math.subtract(yt_rot, self.mean[2:]), self.scale[2:])\n        yp_rot = tf.math.divide(tf.math.subtract(yp_rot, self.mean[2:]), self.scale[2:])\n                \n        # Compute distance to target\n        score_dist = tf.math.pow(tf.reduce_mean(tf.math.pow(tf.norm(tf.math.subtract(yt_rot, yp_rot), axis=1), 3.)), 1./3.)\n\n        return score_dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_size=100,\n                activation='relu',\n                dropout=0.5,\n                kernel_initializer='glorot_uniform',\n                lr=1e-2,\n                use_batch=True,\n                use_dropout=True,\n                hidden=(32, 16),\n                hinge_dist=0.5,\n               ):\n\n    # Specify layer structure\n    inputs = keras.Input(shape=input_size)\n    \n    for i, h in enumerate(hidden):\n        if i==0:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(inputs)\n        else:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(x)\n        if use_batch:\n            x = layers.BatchNormalization()(x)\n        \n        x = layers.Activation(activation)(x)\n\n        # Add Dropout layer if requested\n        if use_dropout:\n            x = layers.Dropout(dropout)(x)\n\n    out = layers.Dense(2)(x)\n    out1 = layers.Activation(None, name=\"loss1\")(out)\n    out2 = layers.Activation(None, name=\"loss2\")(out)\n\n    # Create Model\n    model = keras.Model(inputs, [out1, out2])\n    \n    # Define loss function, optimizer and metrics to track during training\n    optimizer = keras.optimizers.Adagrad(\n            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n                lr, decay_steps=10000, decay_rate=0.96, staircase=True))\n    \n    l1 = CustomLoss_dist()\n    l2 = CustomLoss_dist_rot(scale=scaler_targets.scale_[3:],\n                             mean=scaler_targets.mean_[3:])\n    \n    losses = {'loss1': l1, 'loss2': l2}\n    lossWeights = {\"loss1\": hinge_dist, \"loss2\": 1-hinge_dist}\n\n    model.compile(\n        optimizer = optimizer,\n        loss = losses,\n        loss_weights = lossWeights\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_grid(input_size=0):\n    \n    # Define parameter grid\n    param_grid = [\n    { \n        'nn__input_size': [input_size],\n        'nn__lr': [0.05],\n        'nn__activation': ['sigmoid'],\n        'nn__hidden': [(128, 32),],\n        'nn__dropout': [0.25],\n        'nn__kernel_initializer': ['glorot_uniform'], #['glorot_uniform', 'normal', 'uniform'],\n        'nn__use_batch': [True],\n        'nn__use_dropout': [True],\n        'nn__hinge_dist': [0., 0.5, 1.],\n        'batch_size': [32],\n        'epochs': [50],\n    }]\n    \n    # Create Parameter Grid and return it\n    grids = ParameterGrid(param_grid)\n    return grids\n\nlen(create_grid())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Define feature set scales (~average from my second notebook)\nscale_values = [0.25, 0.025, 0.04, 0.02]\n\n# Run model fit\npreds_tr, preds_te, top_grid, df_grid, y_tr, history = run_prediction(cv=8, n=2, scale_values=scale_values)\n\n# Feedback of overall score\nprint('Best score at %s / %s' % (top_grid['te_mean'], top_grid['tr_mean']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observation 2\n\nThis approach doesn't really seem to work. While it stretches the prediction into something that is orthogonal to the orientation of `domain2_var2_rotated`, in the `hinge=0` condition, it creates only round blobs in the others.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8. Dual `domain2` prediction based on MAE and distance metric\n\nThe hope was, by combining the MAE and distance metric loss functions, we can restrict the predictions to its optimal place. However, this didn't really seem to work.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_size=100,\n                activation='relu',\n                dropout=0.5,\n                kernel_initializer='glorot_uniform',\n                lr=1e-2,\n                use_batch=True,\n                use_dropout=True,\n                hidden=(32, 16),\n                hinge=0.5,\n                hinge_dist=0.5,\n               ):\n\n    # Specify layer structure\n    inputs = keras.Input(shape=input_size)\n    \n    for i, h in enumerate(hidden):\n        if i==0:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(inputs)\n        else:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(x)\n        if use_batch:\n            x = layers.BatchNormalization()(x)\n        \n        x = layers.Activation(activation)(x)\n\n        # Add Dropout layer if requested\n        if use_dropout:\n            x = layers.Dropout(dropout)(x)\n\n    out = layers.Dense(2)(x)\n    out1 = layers.Activation(None, name=\"loss1\")(out)\n    out2 = layers.Activation(None, name=\"loss2\")(out)\n    out3 = layers.Activation(None, name=\"loss3\")(out)\n    out4 = layers.Activation(None, name=\"loss4\")(out)\n\n    # Create Model\n    model = keras.Model(inputs, [out1, out2, out3, out4])\n\n    # Define loss function, optimizer and metrics to track during training\n    optimizer = keras.optimizers.Adagrad(\n            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n                lr, decay_steps=10000, decay_rate=0.96, staircase=True))\n    \n    l1 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=0)\n    l2 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=1)\n    l3 = CustomLoss_dist()\n    l4 = CustomLoss_dist_rot(scale=scaler_targets.scale_[3:],\n                             mean=scaler_targets.mean_[3:])\n    \n    losses = {'loss1': l1, 'loss2': l2, 'loss3': l3, 'loss4': l4}\n    adjustment_factor = 1./10.\n    lossWeights = {\"loss1\": hinge,\n                   \"loss2\": 1-hinge,\n                   \"loss3\": (hinge_dist)*adjustment_factor,\n                   \"loss4\": (1-hinge_dist)*adjustment_factor}\n\n    model.compile(\n        optimizer = optimizer,\n        loss = losses,\n        loss_weights = lossWeights\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_grid(input_size=0):\n    \n    # Define parameter grid\n    param_grid = [\n    { \n        'nn__input_size': [input_size],\n        'nn__lr': [0.05],\n        'nn__activation': ['sigmoid'],\n        'nn__hidden': [(128, 32),],\n        'nn__dropout': [0.25],\n        'nn__kernel_initializer': ['glorot_uniform'], #['glorot_uniform', 'normal', 'uniform'],\n        'nn__use_batch': [True],\n        'nn__use_dropout': [True],\n        'nn__hinge': [0., 0.5, 1.],\n        'nn__hinge_dist': [0., 0.5, 1.],\n        'batch_size': [32],\n        'epochs': [50],\n    }]\n    \n    # Create Parameter Grid and return it\n    grids = ParameterGrid(param_grid)\n    return grids\n\nlen(create_grid())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Define feature set scales (~average from my second notebook)\nscale_values = [0.25, 0.025, 0.04, 0.02]\n\n# Run model fit\npreds_tr, preds_te, top_grid, df_grid, y_tr, history = run_prediction(cv=8, n=4, scale_values=scale_values)\n\n# Feedback of overall score\nprint('Best score at %s / %s' % (top_grid['te_mean'], top_grid['tr_mean']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Observation 3\n\nAlso here, while the `hinge=0.5` and `hinge_dist=0.5` approach leads to the best model fit, the predictions are not helpfull to improve the final score.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}