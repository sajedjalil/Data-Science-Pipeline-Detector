{"cells":[{"metadata":{},"cell_type":"markdown","source":"This work uses **Rapids.AI** to leverage GPUs ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%time\nimport sys\n!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to_drop_67auc=['IC_20',\n#          'IC_02',\n#          'IC_05',\n#          'IC_16',\n#          'IC_10',\n#          'IC_08',\n#          'CBN(4)_vs_CON(37)',\n#          'CBN(4)_vs_CON(38)',\n#          'SCN(99)_vs_SCN(98)',\n#          'DMN(23)_vs_CON(37)',\n#          'DMN(40)_vs_CON(48)',\n#          'DMN(17)_vs_DMN(40)',\n#          'DMN(17)_vs_CON(88)',\n#          'DMN(17)_vs_CON(33)',\n#          'CON(79)_vs_SMN(54)',\n#          'CON(55)_vs_SCN(45)',\n#          'CON(88)_vs_SMN(54)',\n#          'CON(83)_vs_CON(48)',\n#          'CON(83)_vs_CON(67)',\n#          'CON(83)_vs_CON(37)',\n#          'CON(83)_vs_CON(33)',\n#         ]\n# to_drop=['IC_20',\n#          'IC_02',\n#          'IC_05',\n#          'IC_16',\n#          'IC_10',\n#          'IC_08',\n#          'CBN(4)_vs_CON(37)',\n#          'CBN(4)_vs_CON(38)',\n#          'SCN(99)_vs_SCN(98)',\n#         ]\n\n# len(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install fastai2>/dev/null\n# !pip install fast_tabnet>/dev/null\n# from fastai2.basics import *\n# from fastai2.tabular.all import *\n# from fast_tabnet.core import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# from tqdm.notebook import tqdm\n# import gc\n\n# import lightgbm as lgb\n\n\n# fnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n# loading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n# labels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n\n# fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n# df = fnc_df.merge(loading_df, on=\"Id\")\n# labels_df[\"is_train\"] = True\n\n# df = df.merge(labels_df, on=\"Id\", how=\"left\")\n\n# targets = ['age','domain1_var1','domain1_var2','domain2_var1', 'domain2_var2']\n\n# #imputing missing values in targets\n# from sklearn.impute import KNNImputer\n# imputer = KNNImputer(n_neighbors = 5, weights=\"distance\")\n# df[targets] = pd.DataFrame(imputer.fit_transform(df[targets]), columns = targets)\n\n# test_df = df[df[\"is_train\"] != True].copy()\n# train_df = df[df[\"is_train\"] == True].copy()\n\n# train_df = train_df.drop(['is_train'], axis=1)\n# test_df = test_df.drop(targets+['is_train'], axis=1)\n\n\n# features=list(set(train_df.columns)-set(targets)-set(['Id']))\n\n\n# #train_df[loading_features]=train_df[loading_features].pow(2)\n# train_df[fnc_features]=train_df[fnc_features].mul(1/600)\n# # train_df[fnc_features]=train_df[fnc_features].pow(2)\n\n# #test_df[loading_features]=test_df[loading_features].pow(2)\n# test_df[fnc_features]=test_df[fnc_features].mul(1/600)\n# # test_df[fnc_features]=test_df[fnc_features].pow(2)\n\n\n\n\n# #-------Normalizing------------------------\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# train_df[features] = scaler.fit_transform(train_df[features],train_df[targets])\n# test_df[features] = scaler.transform(test_df[features])\n# #----------------------------------------------------\n# to_drop=['IC_20',\n#          'IC_02',\n#          'IC_05',\n#          'IC_16',\n#          'IC_10',\n#          'IC_18'\n#         ]\n# train_df = train_df.drop(to_drop, axis=1)\n# test_df = test_df.drop(to_drop, axis=1)\n# print(train_df.shape,test_df.shape)\n# print(\"Train and test dataframes contain Id column!!\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def trends_scorer_multitask_scoring(y_true,y_preds):\n#     '''\n#     custom scoring function used for evaluation in this competition\n#     '''\n\n#     y_true=torch.tensor(y_true,requires_grad=True)\n#     y_preds=torch.tensor(y_preds,requires_grad=True)\n#     inp,targ = flatten_check(y_true,y_preds)\n#     w = torch.tensor([.3, .175, .175, .175, .175],requires_grad=True)\n#     op = torch.mean(torch.matmul(torch.abs(y_true-y_preds),w/torch.mean(y_true,axis=0)),axis=0)\n#     return op\n\n# def trends_scorer_multitask_scoring_gpu(y_true,y_preds):\n#     '''\n#     custom scoring function used for evaluation in this competition\n#     '''\n#     import numpy as np\n#     y_true = y_true.cpu().detach().numpy()\n#     y_preds= y_preds.cpu().detach().numpy()\n#     w = np.array([.3, .175, .175, .175, .175])\n#     op = np.mean(np.matmul(np.abs(y_true-y_preds),w/np.mean(y_true,axis=0)),axis=0)\n#     return op","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Tweedie](https://towardsdatascience.com/insurance-risk-pricing-tweedie-approach-1d71207268fc)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# from fastai2.layers import L1LossFlat,MSELossFlat \n# from torch.nn import SmoothL1Loss\n# class SmoothMAELoss(torch.nn.Module):\n#     '''\n#     For use with GPU only\n#     '''\n#     def __init__(self,l1):\n#         super().__init__()\n#         self.l1=l1\n        \n#     def forward(self,y, y_hat):\n#         loss = (1-self.l1)*SmoothL1Loss()(y, y_hat) + self.l1*L1LossFlat()(y, y_hat)\n#         return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_tabnet_data(df,train_val_idx):\n#     targets=['age','domain1_var1','domain1_var2','domain2_var1','domain2_var2']\n#     features=list(set(df.columns)-set(targets))\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     to = TabularPandas(\n#         df=df,\n#         procs=[Normalize],\n#         cat_names=None,\n#         cont_names=features,\n#         y_names=targets,\n#         y_block=TransformBlock(),\n#         splits=train_val_idx,\n#         do_setup=True,\n#         device=device,\n#         inplace=False,\n#         reduce_memory=True,\n#     )\n#     return to,len(features),len(targets)\n# def get_model(emb_szs,dls,n_features,n_labels):\n#     model=TabNetModel(\n#         emb_szs,\n#         n_cont=n_features,\n#         out_sz=n_labels,\n#         embed_p=0.0,\n#         y_range=None,\n#         n_d=32,\n#         n_a=32,\n#         n_steps=2,#DO NOT CHANGE\n#         gamma=2.194,\n#         n_independent=0,\n#         n_shared=2,\n#         epsilon=1e-15,\n#         virtual_batch_size=128,\n#         momentum=0.25,#keep it small\n#     )\n#     return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[fit_one_policy()--CNNfitting](https://atmamani.github.io/projects/dl/fastai/fastai-1/)  \n[fit_one_policy()](https://iconof.com/1cycle-learning-rate-policy/)\nCyclical learning rates, which practically eliminates the need to experimentally find the best values and\nschedule for the global learning rates. \n\n Instead of using a fixed, or a decreasing learning rate, the CLR method allows learning rate to continuously oscillate between reasonable minimum and maximum bounds.\nIf the learning rate (LR) is too small, overfitting can occur. Large learning rates help to regularize the training but if the learning rate is too large, the training will diverge. Hence a grid search of short runs to find learning rates that converge or diverge is possible but there is an easier way. Cyclical learning rates (CLR) and the learning rate range test (LR range test) were first proposed by Smith (2015) and later updated in Smith (2017) as a recipe for choosing the learning rate.\n\nTo use CLR, one specifies minimum and maximum learning rate boundaries and a stepsize\nLR Finder trains the model with exponentially growing learning rates from start_lr to end_lr for num_it and stops in case of divergence (unless stop_div=False) then plots the losses vs the learning rates with a log scale.\n\nA good value for the learning rates is then either one tenth of the minimum before the divergence, or\nwhen the slope is the steepest\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# !rm -rf /kaggle/working/models\n# from sklearn.model_selection import KFold\n# from torch.nn import SmoothL1Loss\n# NUM_FOLDS = 7\n# kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2019)\n# all_preds = []\n# for i,(train_index, val_index) in enumerate(kf.split(train_df,train_df)):\n#     print('fold-',i+1)\n#     #get data\n#     to,n_features,n_labels = get_tabnet_data(train_df,(list(train_index), list(val_index)))\n#     dls = to.dataloaders(bs=512, path='/kaggle/working/')\n#     emb_szs = get_emb_sz(to)\n#     #get model\n#     model = get_model(emb_szs,dls,n_features,n_labels)\n#     opt_func = partial(Adam,lr=0.01,mom=0.9,sqr_mom=0.99,wd=0.01,eps=1e-5,decouple_wd=True)\n#     learn = Learner(dls, model, loss_func=SmoothMAELoss(l1=0.0), opt_func=opt_func, metrics=trends_scorer_multitask_scoring_gpu)\n\n#     learn.fit_one_cycle(\n#         100,\n#         lr_max=0.09,\n#         div=25.0,\n#         div_final=1000000,\n#         pct_start=0.25,\n#         cbs=[EarlyStoppingCallback(min_delta=0.01,patience=50),\n#                              SaveModelCallback(fname=\"model_{}\".format(i+1),min_delta=0.01)]\n#     )\n# #     learn.load(\"model_{}\".format(i+1))\n# #     learn.fit_one_cycle(\n# #         50,\n# #         lr_max=0.001,\n# #         div=5.0,\n# #         div_final=1000000,\n# #         pct_start=0.5,\n# #         cbs=[EarlyStoppingCallback(min_delta=0.01,patience=50),\n# #                              SaveModelCallback(fname=\"model_{}\".format(i+1),min_delta=0.01)]\n# #     )\n#     #predicting\n#     learn.load(\"model_{}\".format(i+1))\n#     print(\"Best model:\",learn.loss)\n#     to_tst = to.new(test_df)\n#     to_tst.process()\n#     tst_dl = dls.valid.new(to_tst)\n#     tst_preds,_ = learn.get_preds(dl=tst_dl)\n#     cb = None\n#     all_preds.append(tst_preds)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # PREDICTING......\n# p=sum(all_preds)/NUM_FOLDS\n# targets=['age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']\n# res = pd.DataFrame(np.array(p),columns=[targets])\n# ids=pd.DataFrame(test_df.Id.values,columns=['Id'])\n# a=pd.concat([ids,res],axis=1)\n# b=a.iloc[:,0:6]\n# b.columns=['Id','age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']\n# b.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.8.MSE+0.2MAE---->0.159,n_d=n_a=32, and then gradually reducing...val loss=56 gamma=2.15","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.fine_tune(\n#     100,\n#     base_lr=0.002,\n#     freeze_epochs=1,\n#     pct_start=0.3,\n#     div=5.0,\n#     div_final=100000.0,\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from hyperopt import tpe\n# from hyperopt import STATUS_OK\n# from hyperopt import Trials\n# from hyperopt import hp\n# from hyperopt import fmin\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import cross_val_score\n# from sklearn.datasets import load_breast_cancer\n# import sys\n# if not sys.warnoptions:\n#     import warnings\n#     warnings.simplefilter(\"ignore\")\n    \n# N_FOLDS = 10\n# MAX_EVALS = 200\n# def objective(params, n_folds = N_FOLDS):\n    \n#     model=TabNetModel(\n#     ems[0],\n#     n_cont=1399,\n#     out_sz=5,\n#     embed_p=0.0,\n#     y_range=None,\n#     epsilon=1e-15,\n#     virtual_batch_size=128,\n#     **params\n#     )\n#     opt_func = partial(Adam, wd=0.01, eps=1e-5)\n#     learn = Learner(data[0], model, loss_func=SmoothMAELoss(l1=0.0), opt_func=opt_func, metrics=[trends_scorer_multitask_scoring_gpu])\n#     return {'loss':learn.loss,'params': params, 'status': STATUS_OK}\n\n\n# space = {\n#     'n_d' : hp.choice('n_d', range(2,64,1)),\n#     'n_a' : hp.choice('n_a', range(2,64,1)),\n#     'n_steps':hp.choice('n_steps', range(1,10,1)),\n#     'gamma': hp.uniform('gamma', 1, 5),\n#     'n_independent':hp.choice('n_independent', range(1,10,1)),\n#     'n_shared': hp.choice('n_shared', range(1,10,1)),\n#     'momentum' : hp.uniform('momentum', 0, 1)\n# }\n# # Algorithm\n# tpe_algorithm = tpe.suggest\n\n# # Trials object to track progress\n# bayes_trials = Trials()\n\n# # Optimize\n# best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n# best","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGB","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[LGB optuna integration](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import optuna.integration.lightgbm as lgb\n\n# import numpy as np\n# import pandas as pd\n\n# from sklearn.model_selection import KFold, train_test_split\n\n# from tqdm.notebook import tqdm\n# import gc\n\n\n\n# fnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n# loading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n# labels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n\n# fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n# df = fnc_df.merge(loading_df, on=\"Id\")\n# labels_df[\"is_train\"] = True\n\n# df = df.merge(labels_df, on=\"Id\", how=\"left\")\n\n# target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\n# #imputing missing values in targets\n# from sklearn.impute import KNNImputer\n# imputer = KNNImputer(n_neighbors = 5, weights=\"distance\")\n# df[target_cols] = pd.DataFrame(imputer.fit_transform(df[target_cols]), columns = target_cols)\n\n# test_df = df[df[\"is_train\"] != True].copy()\n# train_df = df[df[\"is_train\"] == True].copy()\n\n# #y_train_df = train_df[target_cols]\n# train_df = train_df.drop(['is_train'], axis=1)\n# #train_df = train_df.drop(target_cols + ['is_train'], axis=1)\n# test_df = test_df.drop(target_cols+['is_train'], axis=1)\n\n\n# targets=['age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']\n# features=list(set(train_df.columns)-set(targets)-set(['Id']))\n              \n# train_df[features]=train_df[features].pow(2)\n# train_df[fnc_features]=train_df[fnc_features].mul(1/100)\n# train_df[fnc_features]=train_df[fnc_features].pow(2)\n\n# test_df[features]=test_df[features].pow(2)\n# test_df[fnc_features]=test_df[fnc_features].mul(1/100)\n# test_df[fnc_features]=test_df[fnc_features].pow(2)\n\n# print(train_df.shape,test_df.shape)\n# print(\"Train and test dataframes contain Id columns,too!!\")\n\n\n\n\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# train_df[features] = scaler.fit_transform(train_df[features],train_df[targets])\n# test_df[features] = scaler.transform(test_df[features])\n\n\n# X_train = train_df[features]\n# X_test = test_df[features]\n# y_train = train_df[targets]\n# print(X_train.shape,X_test.shape)\n\n# def my_metric(y_pred,train_data):\n#     y_true = train_data.get_label()\n#     print(len(y_true),len(y_pred))\n#     return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))\n\n# X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, random_state=20)\n# train_data = lgb.Dataset(X_tr, label=y_tr['age'])\n# val_data = lgb.Dataset(X_val, label=y_val['age'])\n# params = {\n#         'objective':'fair',\n#         'metric':'l1',\n#         'boosting_type':'gbdt',\n#         'learning_rate':0.001,\n#         'tree_learner':'feature_parallel',\n#         'num_threads':4,\n#         'seed':0\n#         }\n\n# best_params, tuning_history = dict(), list()\n\n# model = lgb.train(params, \n#                   train_data, \n#                   num_boost_round=1000, \n#                   early_stopping_rounds=20, \n#                   valid_sets=[train_data,val_data], \n#                   verbose_eval=100,\n#                   learning_rates=lambda it: 0.01 * (0.8 ** it),\n#                   best_params=best_params,\n#                  tuning_history=tuning_history)\n \n# print(\"Best Params\", best_params)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For age--'objective': 'gamma'  \n\n{'lambda_l1': 0.029688407904725312,\n 'lambda_l2': 4.927181117399353e-05,\n 'num_leaves': 101,\n 'feature_fraction': 0.9840000000000001,\n 'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'min_child_samples': 20,\n 'objective': 'gamma',\n 'metric': 'l1',\n 'boosting_type': 'gbdt',\n 'learning_rate': 0.001,\n 'tree_learner': 'feature_parallel',\n 'num_threads': 8,\n 'seed': 0}  \n  \n ### For Domain1_var1--'objective': 'fair'  \n {'lambda_l1': 0.0,\n 'lambda_l2': 0.0,\n 'num_leaves': 251,\n 'feature_fraction': 0.95,\n 'bagging_fraction': 0.9765733975192812,\n 'bagging_freq': 1,\n 'min_child_samples': 10,\n 'objective': 'fair',\n 'metric': 'l1',\n 'boosting_type': 'gbdt',\n 'learning_rate': 0.001,\n 'tree_learner': 'feature_parallel',\n 'num_threads': 4,\n 'seed': 0}  \n \n ### For Domain1_var2--'objective': 'huber'     \n {'lambda_l1': 7.733581684659643e-05,\n 'lambda_l2': 1.1878841440097718,\n 'num_leaves': 31,\n 'feature_fraction': 1.0,\n 'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'min_child_samples': 25,\n 'objective': 'huber',\n 'metric': 'l1',\n 'boosting_type': 'gbdt',\n 'learning_rate': 0.01,\n 'tree_learner': 'feature_parallel',\n 'num_threads': 4,\n 'seed': 0}  \n\n### For Domain2_var1--'objective': 'huber'  \n {'lambda_l1': 0.041395115988296434,\n 'lambda_l2': 0.00011959715500563623,\n 'num_leaves': 105,\n 'feature_fraction': 0.6,\n 'bagging_fraction': 0.5439884362351342,\n 'bagging_freq': 4,\n 'min_child_samples': 10,\n 'objective': 'huber',\n 'metric': 'l1',\n 'boosting_type': 'gbdt',\n 'learning_rate': 0.01,\n 'max_depth': -1,\n 'tree_learner': 'feature_parallel',\n 'num_threads': 8,\n 'seed': 0}  \n \n ### For Domain2_var1--'objective': 'gamma'  \n\n{'lambda_l1': 0.0,\n 'lambda_l2': 0.0,\n 'num_leaves': 53,\n 'feature_fraction': 0.584,\n 'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'min_child_samples': 20,\n 'objective': 'gamma',\n 'metric': 'l1',\n 'boosting_type': 'gbdt',\n 'learning_rate': 0.001,\n 'tree_learner': 'feature_parallel',\n 'num_threads': 4,\n 'seed': 0}\n### For Domain2_var2--'objective': 'huber'  \n{'lambda_l1': 9.606755708273219e-05,\n 'lambda_l2': 0.17107930638380894,\n 'num_leaves': 59,\n 'feature_fraction': 1.0,\n 'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'min_child_samples': 20,\n 'objective': 'huber',\n 'metric': 'l1',\n 'boosting_type': 'gbdt',\n 'learning_rate': 0.01,\n 'max_depth': -1,\n 'tree_learner': 'feature_parallel',\n 'num_threads': 8,\n 'seed': 0}\n \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[why tree_learner=featureparallel](https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# params={\n#         'age':{'lambda_l1': 0.029688407904725312,\n#      'lambda_l2': 4.927181117399353e-03,\n#      'num_leaves': 101,\n#      'feature_fraction': 0.90,\n#      'bagging_fraction': 1.0,\n#      'bagging_freq': 0,\n#      'min_child_samples': 20,\n#      'objective': 'fair',\n#      'metric': 'l1',\n#      'boosting_type': 'gbdt',\n#      'learning_rate': 0.001,\n#      'tree_learner': 'feature_parallel',\n#      'num_threads': 4,\n#      'seed': 0}\n#     ,\n    \n#     'domain1_var1':{'lambda_l1': 0.0,\n#  'lambda_l2': 0.0,\n#  'num_leaves': 200,\n#  'feature_fraction': 0.95,\n#  'bagging_fraction': 0.9765733975192812,\n#  'bagging_freq': 1,\n#  'min_child_samples': 10,\n#  'objective': 'fair',\n#  'metric': 'l1',\n#  'boosting_type': 'gbdt',\n#  'learning_rate': 0.001,\n#  'tree_learner': 'feature_parallel',\n#  'num_threads': 4,\n#  'seed': 0}  \n#     ,\n#     'domain1_var2':{'lambda_l1': 7.733581684659643e-05,\n#  'lambda_l2': 1.1878841440097718,\n#  'num_leaves': 31,\n#  'feature_fraction': 1.0,\n#  'bagging_fraction': 1.0,\n#  'bagging_freq': 0,\n#  'min_child_samples': 25,\n#  'objective': 'huber',\n#  'metric': 'l1',\n#  'boosting_type': 'gbdt',\n#  'learning_rate': 0.01,\n#  'tree_learner': 'feature_parallel',\n#  'num_threads': 4,\n#  'seed': 0}\n#     ,\n#     'domain2_var1':{'lambda_l1': 0.041395115988296434,\n#  'lambda_l2': 0.00011959715500563623,\n#  'num_leaves': 105,\n#  'feature_fraction': 0.6,\n#  'bagging_fraction': 0.5439884362351342,\n#  'bagging_freq': 4,\n#  'min_child_samples': 10,\n#  'objective': 'huber',\n#  'metric': 'l1',\n#  'boosting_type': 'gbdt',\n#  'learning_rate': 0.01,\n#  'max_depth': -1,\n#  'tree_learner': 'feature_parallel',\n#  'num_threads': 8,\n#  'seed': 0}\n#     ,\n#     'domain2_var2':{'lambda_l1': 9.606755708273219e-05,\n#  'lambda_l2': 0.17107930638380894,\n#  'num_leaves': 59,\n#  'feature_fraction': 1.0,\n#  'bagging_fraction': 1.0,\n#  'bagging_freq': 0,\n#  'min_child_samples': 20,\n#  'objective': 'huber',\n#  'metric': 'l1',\n#  'boosting_type': 'gbdt',\n#  'learning_rate': 0.01,\n#  'max_depth': -1,\n#  'tree_learner': 'feature_parallel',\n#  'num_threads': 8,\n#  'seed': 0}\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # LightGBM--Cross Validation implementation\n\n# from sklearn.model_selection import KFold\n\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# from tqdm.notebook import tqdm\n# import gc\n\n# import lightgbm as lgb\n\n\n# fnc_df = pd.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n# loading_df = pd.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n# labels_df = pd.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n\n# fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n# df = fnc_df.merge(loading_df, on=\"Id\")\n# labels_df[\"is_train\"] = True\n\n# df = df.merge(labels_df, on=\"Id\", how=\"left\")\n\n# target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n\n# #imputing missing values in targets\n# from sklearn.impute import KNNImputer\n# imputer = KNNImputer(n_neighbors = 5, weights=\"distance\")\n# df[target_cols] = pd.DataFrame(imputer.fit_transform(df[target_cols]), columns = target_cols)\n\n# test_df = df[df[\"is_train\"] != True].copy()\n# train_df = df[df[\"is_train\"] == True].copy()\n\n# train_df = train_df.drop(['is_train'], axis=1)\n# test_df = test_df.drop(target_cols+['is_train'], axis=1)\n\n\n# targets=['age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']\n# features=list(set(train_df.columns)-set(targets)-set(['Id']))\n\n\n# train_df[features]=train_df[features].pow(2)\n# train_df[fnc_features]=train_df[fnc_features].mul(1/500)\n# train_df[fnc_features]=train_df[fnc_features].pow(2)\n\n# test_df[features]=test_df[features].pow(2)\n# test_df[fnc_features]=test_df[fnc_features].mul(1/500)\n# test_df[fnc_features]=test_df[fnc_features].pow(2)\n\n\n\n\n# #-------Normalizing------------------------\n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# train_df[features] = scaler.fit_transform(train_df[features],train_df[targets])\n# test_df[features] = scaler.transform(test_df[features])\n# #----------------------------------------------------\n\n# print(train_df.shape,test_df.shape)\n# print(\"Train and test dataframes contain Id column!!\")\n\n\n# def my_metric(y_true, y_pred):\n#     return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))\n\n\n# NFOLDS = 5\n# from sklearn.model_selection import KFold\n# kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n\n# targets=['age','domain2_var1','domain2_var2', 'domain1_var1','domain1_var2']\n# features=list(set(train_df.columns)-set(targets)-set(['Id']))\n# overal_score = 0.0\n# for target,w in tqdm([('age',0.3),('domain1_var1',0.175),('domain1_var2',0.175),('domain2_var1',0.175),('domain2_var2',0.175)]):\n#     y_oof = np.zeros(train_df.shape[0])\n#     y_test = np.zeros((test_df.shape[0], NFOLDS))\n#     print('*'*20,target,'*'*20)\n#     for i,(train_index, valid_index) in enumerate(kf.split(train_df, train_df)):\n#         print('>'*20,'Fold-',i+1)\n#         train,val = train_df.iloc[train_index],train_df.iloc[valid_index]\n#         X_train = train[features]\n#         y_train = train[target]\n#         X_val = val[features]\n#         y_val = val[target]\n#         train_data = lgb.Dataset(X_train, label=y_train)\n#         val_data = lgb.Dataset(X_val, label=y_val)\n#         #create model\n#         model = lgb.train(params[target], \n#                           train_data, \n#                           num_boost_round=10000, \n#                           early_stopping_rounds=20, \n#                           valid_sets=[train_data,val_data], \n#                           learning_rates=lambda it: 0.01 * (0.8 ** it),\n#                           verbose_eval=100)\n    \n#         val_pred = model.predict(X_val)\n#         test_pred = model.predict(test_df[features])\n#         y_oof[valid_index] = val_pred\n#         y_test[:, i] = test_pred\n\n#     train_df[\"pred_{}\".format(target)] = y_oof\n#     test_df[target] = y_test.mean(axis=1)\n\n#     score = my_metric(train_df[train_df[target].notnull()][target].values, train_df[train_df[target].notnull()][\"pred_{}\".format(target)].values)\n#     print(\"=\"*20,target, np.round(score, 5))\n#     print(\"-\"*100)\n#     overal_score += w*score\n        \n# print(\"Overal score:\", np.round(overal_score, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = df.drop(columns=['Id','age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2', 'is_train'],axis=1)\n# X_test = test_df.drop(columns=['Id','age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2', 'is_train'],axis=1)\n# y_train = df[['age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# from sklearn.linear_model import MultiTaskElasticNetCV\n# cv_model = MultiTaskElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],cv=5,verbose=1,n_jobs=10)\n# cv_model.fit(train_df[3634:],y_train_df[3634:][['domain2_var1','domain2_var2']])\n# #fitting multitask net with hyperparameters obtained from cross validation above\n# from sklearn.linear_model import MultiTaskElasticNet\n# model_d2 = MultiTaskElasticNet(alpha=cv_model.alpha_,l1_ratio=cv_model.l1_ratio_,random_state=0)\n# model_d2.fit(train_df[:3634],y_train_df[:3634][['domain2_var1','domain2_var2']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NN_model = Sequential()\n\n# # The Input Layer :\n# NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n\n# # The Hidden Layers :\n# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n# NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# # The Output Layer :\n# NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# # Compile the network :\n# NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n# NN_model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"clf.alpha_,clf.l1_ratio_ = (3.944212390476824e-05, 0.99)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**MultiOutputRegressor** consists of fitting one estimator per target.   \nThis is a simple strategy for extending regressors that do not natively support multi-target regression, like SVR().","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# def trends_scorer_multitask_scoring(estimator,X,y_true):\n#     '''\n#     custom scoring function used for evaluation in this competition\n#     '''\n#     import numpy as np\n#     y_true = np.array(y_true)\n#     y_preds=estimator.predict(X)\n#     y_preds = np.array(y_preds)\n#     w = np.array([.3, .175, .175, .175, .175])\n#     op = np.mean(np.matmul(np.abs(y_true-y_preds),w/np.mean(y_true,axis=0)),axis=0)\n#     print(op)\n#     return op   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# from sklearn.datasets import make_regression\n# from sklearn.multioutput import MultiOutputRegressor\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.ensemble import GradientBoostingRegressor\n# m = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,random_state=0))\n# m.fit(X_train,y_train)\n# preds = m.predict(X_test)\n# test_df[['age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']] = preds\n# test_df.drop(columns=[\"is_train\"],inplace=True)\n# test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from statsmodels.tsa.stattools import grangercausalitytests\n# grangercausalitytests(X_train, maxlag=20, verbose=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### Matrix factorisation using SVD\n# X_train = np.asarray(X_train)#.to_gpu_matrix(),dtype=np.float32)\n# mean = np.mean(X_train, axis = 1)\n# sd = np.std(X_train, axis = 1)\n# X_train_norm = (X_train - mean.reshape(-1, 1))/sd.reshape(-1, 1)\n# from scipy.sparse.linalg import svds\n# latent_factors = 50\n# U, sigma, V_T = svds(X_train_norm, k = latent_factors)\n# sigma = np.diag(sigma)\n# U.shape,sigma.shape,V_T.shape\n#--------------------------------------------------------------------------------------------------------\n# from sklearn.decomposition import FactorAnalysis\n# transformer = FactorAnalysis(n_components=50, random_state=0)\n# X_transformed = transformer.fit_transform(X_train)\n# X_transformed.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport cudf\nimport gc\ndef get_train_test(fnc_file,loadings_file,lablels_file):\n    '''\n    function to get training and test data sets\n    Works with Rapids.ai ONLY\n    \n    '''\n    path = \"../input/trends-assessment-prediction/\"\n    fnc_df = pd.read_csv(os.path.join(path,fnc_file))\n    loading_df = pd.read_csv(os.path.join(path,loadings_file))\n    fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n    df = fnc_df.merge(loading_df, on=\"Id\")\n    labels_df = pd.read_csv(os.path.join(path,lablels_file))\n    labels_df[\"is_train\"] = True\n    df = df.merge(labels_df, on=\"Id\", how=\"left\")\n    test_df = df[df[\"is_train\"] != True].copy()\n    train_df = df[df[\"is_train\"] == True].copy()\n    train_df = train_df.drop(['is_train'], axis=1)\n    target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n    test_df = test_df.drop(target_cols + ['is_train'], axis=1)\n    features = loading_features + fnc_features \n    #-----------------Normalizing------------------------\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    train_df[features] = scaler.fit_transform(train_df[features],train_df[target_cols])\n    test_df[features] = scaler.transform(test_df[features])\n    #----------------------------------------------------\n    # Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\n    train_df[fnc_features] = train_df[fnc_features].mul(1/800)\n    test_df[fnc_features]  = test_df[fnc_features].mul(1/800) \n    #imputing missing values in targets\n    from sklearn.impute import KNNImputer\n    imputer = KNNImputer(n_neighbors = 5, weights=\"distance\")\n    train_df = cudf.from_pandas(pd.DataFrame(imputer.fit_transform(train_df), columns = list(train_df.columns)))\n    test_df = cudf.from_pandas(test_df)#necessary for casting to gpu matrix\n    del df\n    gc.collect()\n    return train_df,test_df,features,target_cols\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom cuml import SVR\nfrom cuml import RandomForestRegressor\nfrom cuml import NearestNeighbors,KMeans,UMAP,Ridge,ElasticNet\nimport cupy as cp\nfrom sklearn.model_selection import KFold\n\n\ndef my_metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))\n\ndef cv_train_predict(df,test_df,features):\n    '''\n    training with k-fold cross-validation\n    '''\n    weights={}#Weights & other hyperparameters\n    #[target,score_weight,SVR_penalty(C),ElasticNet_l1ratio,Blend_ElNet_weight,Blend_RandForest_weight]\n    weights['age']         =[\"age\",         0.3,   40, 0.8, 0.5, 0.3]\n    weights['domain1_var1']=[\"domain1_var1\",0.175,  8, 0.5, 0.6, 0.4]\n    weights['domain1_var2']=[\"domain1_var2\",0.175,  8, 0.5, 0.6, 0.4]\n    weights['domain2_var1']=[\"domain2_var1\",0.175, 10, 0.8, 0.8, 0.5]\n    weights['domain2_var2']=[\"domain2_var2\",0.175, 10, 0.5, 0.6, 0.5]\n    NUM_FOLDS = 7\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n    overal_score = 0\n    for target,w,c,l1_ratio,el,rf in [weights['age'], weights['domain1_var1'], weights['domain1_var2'],weights['domain2_var1'],weights['domain2_var2']]:    \n        y_oof = np.zeros(df.shape[0])\n        y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n\n        for f, (train_ind, val_ind) in enumerate(kf.split(df,df)):\n            train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n          \n            #-------training,val,test data preparation for RandomForestRegressor since it operates on float32\n            X_train = np.array(train_df[features].to_gpu_matrix(),dtype=np.float32)\n            y_train = np.array(train_df[[target]].to_gpu_matrix(),dtype=np.float32)\n            X_val = np.array(val_df[features].to_gpu_matrix(),dtype=np.float32)\n            y_val = np.array(val_df[[target]].to_gpu_matrix(),dtype=np.float32)\n            X_test = np.array(test_df[features].to_gpu_matrix(),dtype=np.float32)\n            #---------------------------------------------------------------------------------------\n            \n            \n            model = RandomForestRegressor(n_estimators=200,split_criterion=2,accuracy_metric=my_metric,bootstrap=True,seed=0)\n            model.fit(X_train,y_train)\n            model_1 = SVR(C=c, cache_size=3000.0)\n            model_1.fit(train_df[features].values, train_df[target].values)\n        \n            model_2 = ElasticNet(alpha = 1,l1_ratio=l1_ratio)\n            model_2.fit(train_df[features].values, train_df[target].values)\n            \n            val_pred_rf=model.predict(X_val)\n            val_pred_1 = model_1.predict(val_df[features])\n            val_pred_2 = model_2.predict(val_df[features])\n\n            test_pred_rf=model.predict(X_test)\n            test_pred_1 = model_1.predict(test_df[features])\n            test_pred_2 = model_2.predict(test_df[features])\n            #pred    = Blended prediction(RandomForest + Blended prediction(ElasticNet & SVR))\n            \n            \n            val_pred = rf*val_pred_rf + cp.asnumpy((1-rf)*((1-el)*val_pred_1+el*val_pred_2))\n            #val_pred = cp.asnumpy(val_pred.values.flatten())\n            \n            test_pred = rf*test_pred_rf + cp.asnumpy((1-rf)*((1-el)*test_pred_1+el*test_pred_2))\n            #test_pred = cp.asnumpy(test_pred.values.flatten())\n\n            y_oof[val_ind] = val_pred\n            y_test[:, f] = test_pred\n\n        df[\"pred_{}\".format(target)] = y_oof\n        test_df[target] = y_test.mean(axis=1)\n\n        score = my_metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n        print(target, np.round(score, 5))\n        print()\n        overal_score += w*score\n        \n    print(\"Overal score:\", np.round(overal_score, 5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf,test_df,features, targets = get_train_test(\"fnc.csv\",\"loading.csv\",\"train_scores.csv\")\nprint(\"training shape={0} | testing shape={1}\".format(df.shape, test_df.shape))\nprint(type(df),type(test_df),'Id' in features,'Id' in df.columns,'Id' in test_df.columns)\ntargets = ['age','domain1_var1','domain1_var2', 'domain2_var1','domain2_var2']\ndf[targets].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nto_drop=['IC_20','IC_02','IC_05','IC_16','IC_10','IC_18']\nfeatures = list(set(features)-set(to_drop))#Id is not present in features\nprint(\"After excluding features and Id, training shape={0} | testing shape={1}\".format(df[features].shape, test_df[features].shape))\ncv_train_predict(df,test_df,features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"age-0.15866 for el=0.4 c=10, l1_ratio=0.8  \nage 0.16002 for el=0.6 c=10, l1_ratio=0.8  \nage 0.16216 for el=0.6 c=100, l1_ratio=0.8  \nage 0.16444 for el=0.4 c=100, l1_ratio=0.8  \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# NUM_FOLDS = 7\n# kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n# for f,(train_ind, val_ind) in enumerate(kf.split(df,df)):\n#     #model = SVR(C=12, cache_size=3000.0,verbose=True)\n#     train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n    \n#     X_train = np.array(train_df[features].to_gpu_matrix(),dtype=np.float32)\n#     y_train = np.array(train_df[['domain2_var2']].to_gpu_matrix(),dtype=np.float32)\n#     X_val = np.array(val_df[features].to_gpu_matrix(),dtype=np.float32)\n#     y_val = np.array(val_df[['domain2_var2']].to_gpu_matrix(),dtype=np.float32)\n#     model = RandomForestRegressor(n_estimators=100,split_criterion=2,accuracy_metric=my_metric,bootstrap=True,seed=0)\n#     model.fit(X_train,y_train)\n#     print(my_metric(y_val,model.predict(X_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### domain2_var1   \n0.18293 for 7-fold w=0.175,  c=5, l1-ratio=0.2, el=0.5, rf=0  \n0.18301 for 7-fold w=0.175,  c=5, l1-ratio=0.2, el=0.7, rf=0  \n0.18457 for 7-fold w=0.175,  c=5, l1-ratio=0.2, el=0  , rf=0  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"model_1 = SVR(C=c, cache_size=3000.0)  \nmodel_2 = ElasticNet(alpha = 1,l1_ratio=0.2)  \n\n[(\"age\", 100, weights['age'], 0.15), (\"domain1_var1\", 12, weights['domain1_var1'], 0.2), (\"domain1_var2\", 8, weights['domain1_var2'], 0.2), (\"domain2_var1\", 10, weights['domain2_var1'], 0.3), (\"domain2_var2\", 12, weights['domain2_var2'], 0.22)]  \n\nage 0.14606\n\ndomain1_var1 0.14598\n\ndomain1_var2 0.14519\n\ndomain2_var1 0.18153\n\ndomain2_var2 0.17543\n\nOveral score: 0.15724   \nLB: 0.1604","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 510 Ids of site2 are known in test data, there are more...510 are not all of site2 Ids. \n# training data does not have site2 Ids\n# site2_df = cudf.read_csv(\"../input/trends-assessment-prediction/reveal_ID_site2.csv\")\n# testdf_site2 = test_df[test_df['Id'].isin(list(site2_df['Id']))]\n# testdf_site2.shape\n# np.array(train_df[features].fillna(0).to_gpu_matrix(),dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Competition metric:-  \n$\\text{score} = \\sum_{j} w_j \\left( \\frac{\\sum_i \\lvert y_{j,i} - \\hat{y}_{j,i} \\rvert}{\\sum_i \\hat{y}_{j,i}} \\right)$   \n\nSubmissions are scored using   \n- feature-weighted,    \n- normalized,    \n- absolute errors.  \nj--->age/domain1_var1...  \ni--->data instance  \n$y_{j,i}$ is ith observation of jth feauture....  \nweights are  [.3, .175, .175, .175, .175]  \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## RandomForest split_criterion:   \n0 for GINI,   \n1 for ENTROPY,  \n2 for MSE,   \nor 3 for MAE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #SCRATCH-PAD\n# def metric(y_true, y_pred):#-----------------------------C-----H----A---N---G----E----S------\n#     import numpy as np\n#     return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))#CHANGED y_true--->y_pred\n\n# features = loading_features + fnc_features\n# X = np.array(df[features].to_gpu_matrix(),dtype=np.float32)[:5000]\n# y = np.array(df[['domain2_var1']].to_gpu_matrix(),dtype=np.float32)[:5000]\n# #model = RandomForestRegressor(n_estimators=100,split_criterion=3,accuracy_metric=metric,seed=0,bootstrap=True)\n# model=MBSGDRegressor(loss='squared_loss',penalty='elasticnet',learning_rate='adaptive',n_iter_no_change=5,verbose=True)\n# model.fit(X,y)\n# X_test = np.array(df[features].to_gpu_matrix(),dtype=np.float32)[5000:]\n# print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manual tryouts to select hyperparams for RandomForestRegressor...  \nMSE(split_criterion=2) for age:bootstrap=true,accuracy_metric=mse, metric=0.244 | bootstrap=false, metric=0.3185 \nMAE(split_criterion=3) for age:bootstrap=true,accuracy_metric=mse, metric=0.243 | bootstrap=false, metric=0.3066 MAE(split_criterion=3) for age:bootstrap=true,accuracy_metric=mean_ae, metric=0.244  \nMAE(split_criterion=3) for age:bootstrap=true,accuracy_metric=custom metric, metric=0.244\n\n-------------------------------------------------------------------------------\nMSE for domain1_var1:bootstrap=true,accuracy_metric=mse, metric=0.1628758 | bootstrap=false, metric=0.2282  \nMAE for domain1_var1:bootstrap=true,accuracy_metric=mse, metric=0.1616656 | bootstrap=false, metric=0.2212   \n\n-------------------------------------------------------------------------------\nMSE for domain1_var2:bootstrap=true, metric=0.1628758 | bootstrap=false, metric=0.2282  \nMAE for domain1_var2:bootstrap=true-->accuracy_metric=custom metric, metric=0.153| accuracy_metric=\"mean_ae\", metric=0.152  \n\n-------------------------------------------------------------------------------\nMAE for domain2_var1:bootstrap=true-->accuracy_metric=custom metric, metric=0.18972| accuracy_metric=\"mean_ae\", metric=0.18971  \nMAE for domain2_var2:bootstrap=true-->accuracy_metric=custom metric, metric=0.18| accuracy_metric=\"mean_ae\", metric=0.18  \nMSE for domain2_var1,domain2_var2: not done\n\n### Conclusion:bootstrap=True, accuracy_metric=mean_ae","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Submitting......","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"output = test_df #\nsub_df = cudf.melt(output[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}