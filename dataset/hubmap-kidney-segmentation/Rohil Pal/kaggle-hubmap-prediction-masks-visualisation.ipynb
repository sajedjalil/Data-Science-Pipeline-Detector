{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!mkdir -p /tmp/pip/cache/\n!cp ../input/segmentationmodelspytorch/segmentation_models/efficientnet_pytorch-0.6.3.xyz /tmp/pip/cache/efficientnet_pytorch-0.6.3.tar.gz\n!cp ../input/segmentationmodelspytorch/segmentation_models/pretrainedmodels-0.7.4.xyz /tmp/pip/cache/pretrainedmodels-0.7.4.tar.gz\n!cp ../input/segmentationmodelspytorch/segmentation_models/segmentation-models-pytorch-0.1.2.xyz /tmp/pip/cache/segmentation_models_pytorch-0.1.2.tar.gz\n!cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.1.20-py3-none-any.whl /tmp/pip/cache/\n!cp ../input/segmentationmodelspytorch/segmentation_models/timm-0.2.1-py3-none-any.whl /tmp/pip/cache/\n!pip install --no-index --find-links /tmp/pip/cache/ efficientnet-pytorch\n!pip install --no-index --find-links /tmp/pip/cache/ segmentation-models-pytorch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TODO\n1. How to read the data?\n2. How to crop the patches for testing?"},{"metadata":{},"cell_type":"markdown","source":"# Necessary imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\n\nfrom skimage.color import label2rgb\n\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSEED = 421","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_PATH = \"../input/kaggle-hubmap-segmentation-pytorch-training/unet-se_resnext50-cosineanneal-RES-256-best-FOLD-0-model.pth\"\n\nIMG_SIZE = 256\nTRAIN_IMGS = '../input/hubmap-256x256/train/'\nMASKS = '../input/hubmap-256x256/masks'\nLABELS = '../input/hubmap-kidney-segmentation/train.csv'\nNUM_WORKERS = 4\n\nMODEL = 'unet-se_resnext50-cosineanneal'\nENCODER = 'se_resnext50_32x4d'\nFOLD = 0\nNFOLDS = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/iafoss/256x256-images\nMEAN = np.array([0.65459856,0.48386562,0.69428385])\nSTD = np.array([0.15167958,0.23584107,0.13146145])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def img2tensor(img,dtype:np.dtype=np.float32):\n    # convert numpy image to Pytorch tensor image\n    if img.ndim==2 : img = np.expand_dims(img,2)\n    img = np.transpose(img,(2,0,1))\n    return torch.from_numpy(img.astype(dtype, copy=False))\n\nclass HuBMAPDataset(Dataset):\n    def __init__(self, fold=FOLD, train=True, preprocess_input=None, transforms=None):\n        ids = pd.read_csv(LABELS).id.values\n        kf = KFold(n_splits=NFOLDS,random_state=SEED,shuffle=True)\n        ids = set(ids[list(kf.split(ids))[fold][0 if train else 1]])\n        self.fnames = [fname for fname in os.listdir(TRAIN_IMGS) if fname.split('_')[0] in ids]\n        self.train = train\n        self.preprocess_input = preprocess_input\n        self.transforms = transforms\n        \n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        img = cv2.cvtColor(cv2.imread(os.path.join(TRAIN_IMGS, fname)), cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(os.path.join(MASKS,fname),cv2.IMREAD_GRAYSCALE)\n        \n        if self.transforms:\n            augmented = self.transforms(image=img,mask=mask)\n            img,mask = augmented['image'],augmented['mask']\n        \n        if self.preprocess_input:\n            # Normalizing the image with the given mean and std corresponding to each channel\n            img = self.preprocess_input(image=img)['image']\n        \n        return img2tensor(img),img2tensor(mask)\n    \n    def __len__(self):\n        return len(self.fnames)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter#Data\ndef get_preprocess_fn(encoder_name, pretrained='imagenet'):\n    return A.Lambda(image = get_preprocessing_fn(encoder_name = encoder_name, pretrained = pretrained))\n\n\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.RandomRotate90(),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n                         border_mode=cv2.BORDER_REFLECT),\n        A.OneOf([\n            A.OpticalDistortion(p=0.3),\n            A.GridDistortion(p=.1),\n            A.IAAPiecewiseAffine(p=0.3),\n        ], p=0.3),\n        A.OneOf([\n            A.HueSaturationValue(10,15,10),\n            A.CLAHE(clip_limit=2),\n            A.RandomBrightnessContrast(),            \n        ], p=0.3)\n    ],p=1.)\n\n# def get_val_transform():\n#     return A.Compose([\n#         A.Resize(IMG_SIZE, IMG_SIZE,always_apply=True),\n#     ],p=1.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\nPut your model definition and loading weights here"},{"metadata":{"trusted":true},"cell_type":"code","source":"class HuBMAP(nn.Module):\n    def __init__(self):\n        super(HuBMAP, self).__init__()\n        # since this is a binary segmentation problem FTU or non-FTU so classes = 1\n#         self.cnn_model = smp.Unet(encoder_name='se_resnext50_32x4d', encoder_weights='imagenet', classes=1, activation=None)\n#         self.cnn_model = smp.FPN(encoder_name='se_resnext50_32x4d', encoder_weights='imagenet', classes=1, activation=None)\n\n        self.cnn_model = smp.Unet(encoder_name=ENCODER, encoder_weights=None, classes=1, activation=None)\n#         self.cnn_model = smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', classes=1, activation=None)\n        \n    def forward(self, imgs):\n        img_segs = self.cnn_model(imgs)\n        return img_segs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualise_predictions(image_patches, pred_masks, gt_masks, figsize=(25, 25)):\n    assert image_patches.shape == pred_masks.shape and pred_masks.shape == gt_masks.shape, \"image patches and masks should be of the same shape\"\n    num_patches = image_patches.shape[0]\n    grid_dim = int(sqrt(num_patches))\n    \n    fig, axs = plt.subplots(grid_dim, grid_dim, figsize=figsize)\n    \n    for i, (img_patch, pred_mask, gt_mask) in enumerate(zip(image_patches, pred_masks, gt_masks)):\n        x = i // ndim\n        y = i % ndim\n        \n        # plot the GT mask on the image\n        img_data = label2rgb(image=img_patch, label=gt_mask, bg_label=0, alpha=0.2)\n        \n        # plot the predicted mask on the image\n        img_data = label2rgb(image=img_data, label=pred_mask, bg_label=0, alpha=0.2)\n        \n        axs[x, y].imshow(img_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make the prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataloader\n\nds = HuBMAPDataset(preprocess_input=get_preprocess_fn(encoder_name=ENCODER), transforms=None)\ndl = DataLoader(ds,batch_size=25,shuffle=False,num_workers=NUM_WORKERS)\nimgs,masks = next(iter(dl))\n\n# instantiate model and load weights\nmodel = HuBMAP()\nmodel.load_state_dict(torch.load(MODEL_PATH))\nmodel.to(DEVICE)\nmodel.eval()\n\n\npredictions = []\n# select a random batch and make predictions on it\nfor i, img in tqdm(enumerate(imgs), total=imgs.shape[0]):\n    with torch.no_grad():\n        pred_mask = model(img.unsqueeze(0).to(DEVICE))\n    \n    pred_mask = pred_mask.cpu()[0]\n    \n    binary_mask = (pred_mask > 0).int()\n    predictions.append(binary_mask)\n    \n    \npredictions = torch.stack(predictions, dim=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize the predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,16))\nfor i,(img, pred_mask, gt_mask) in enumerate(zip(imgs, predictions, masks)):\n    img = img.permute(1, 2, 0) * STD + MEAN\n    img = (img*255.0).numpy().astype(np.uint8)\n    gt_mask = gt_mask.squeeze().numpy().astype(np.uint8)\n    pred_mask = pred_mask.squeeze().numpy().astype(np.uint8)\n    pred_mask[pred_mask == 1] = 2\n    \n    full_mask = gt_mask + pred_mask\n    # plot the GT mask on the image\n    \n    # red: GT mask, blue: Predicted mask, green: overlap between GT and predicted\n    img_data = label2rgb(image=img, label=full_mask, bg_label=0, alpha=0.2, colors=['red', 'blue', 'green'])\n        \n    # plot the predicted mask on the image\n#     img_data = label2rgb(image=img_data, label=pred_mask, bg_label=0, colors=['blue'], alpha=0.2)\n        \n    plt.subplot(5,5,i+1)\n    plt.imshow(img_data)\n#     plt.imshow(mask.squeeze().numpy(), alpha=0.2)\n    plt.axis('off')\n    plt.subplots_adjust(wspace=None, hspace=None)\n    \n# del ds,dl,imgs,masks","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}