{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TIF to JPG and TFRecords in multiple resolutions\nIn this notebook we will transform the huge train images into both discrete jpeg+png files and sharded TFRecords format.\nWhen creating TFRecords of a dataset it is most efficient for the processing pipeline to have TFRecords of a certain size, generally >10MBytes, to benefit from I/O prefetching. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport glob\nfrom PIL import Image\nimport hashlib\nfrom io import BytesIO\nfrom skimage import io\nimport contextlib2\nimport json\nimport cv2\nimport os, shutil  \nfrom functools import partial\n%matplotlib inline  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\nThis notebook uses/modifies some code snippets from these notebooks:\n* [Global Wheat to TFRecords](https://www.kaggle.com/mistag/global-wheat-to-tfrecords) (own work)\n* [HuBMap: Read data and build TFRecords](https://www.kaggle.com/marcosnovaes/hubmap-read-data-and-build-tfrecords)   \n\nIn addition some sample code from Keras documentation."},{"metadata":{},"cell_type":"markdown","source":"# TFRecords creation using patches\nThe super-resolution cell scan images need to be split into patches (or tiles if you like).   \n\nHere we go for 1024x1024 as the primary tile (patch) size. Adjust the IMG_SIZE variable further below to suit needs. We also create downscaled versions of 512x512, 256x256 and 128x128, both in TFRecord format and as separate jpeg files (png for masks).   \nThis dataset also includes an anatomy mask. We will use this mask to split the TFRecords into two sets: One that includes the Cortex mask (containing the glomeruli) and one for everything else. This allow us to easily select how much \"other stuff\" to include during training."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A few helper functions for reading in image and masks (from json)\n\ndef read_tif_file(fname):\n    img = io.imread(fname)\n    img = np.squeeze(img)\n    if img.shape[0] == 3: # swap axes as required\n        img = img.swapaxes(0,1)\n        img = img.swapaxes(1,2)\n    return img\n\ndef read_mask_file(fname, mshape):\n    with open(fname) as f:\n        mdata = json.load(f)\n        polys = []\n        for index in range(mdata.__len__()):\n            if (mdata[index]['properties']['classification']['name'] == 'Cortex') or (mdata[index]['properties']['classification']['name'] == 'glomerulus'):\n                geom = np.array(mdata[index]['geometry']['coordinates'])\n                if geom.shape[0] == 1:\n                    polys.append(geom)\n                else:\n                    for j in range(geom.shape[0]):\n                        polys.append(np.array([geom[j][0]]).astype(int))\n        mask = np.zeros(mshape, dtype=np.int8)\n        cv2.fillPoly(mask, polys, 1)\n        mask = mask.astype(bool, copy=False)\n    return mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below creates a TFRecord from a single patch. The image is stored as JPEG, while the mask is stored as PNG (lossless). We also put in some extra metadata. The anatomy mask is used to determine if the patch is part of Cortex or not. Adjust the threshold (number of pixles) as required."},{"metadata":{"trusted":true},"cell_type":"code","source":"# patches are stored as jpeg, masks as PNG\ndef create_tf_example(patch, m_patch, fid, x, y, size):\n    filename = fid+'.tiff'\n    height = size # Image height\n    width = size # Image width\n    buf= BytesIO()\n    im = Image.fromarray(np.uint8(patch))\n    im.save(buf, format= 'JPEG') # encode to jpeg in memory\n    encoded_image_data= buf.getvalue()\n    image_format = b'jpeg'\n    source_id = fid+'-'+str(x)+'-'+str(y) # must be unique\n    # A hash of the image is used in some frameworks\n    key = hashlib.sha256(encoded_image_data).hexdigest()\n    # Mask encoding\n    buf= BytesIO()\n    mim = Image.fromarray(np.uint8(m_patch))\n    mim.save(buf, format= 'PNG') # encode to png in memory\n    encoded_mask_data= buf.getvalue()\n    mask_format = b'png'\n    \n    tf_record = tf.train.Example(features=tf.train.Features(feature={\n        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode()])),\n        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[source_id.encode()])),\n        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image_data])),\n        'image/key/sha256': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode()])),\n        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n        'image/patch-x': tf.train.Feature(int64_list=tf.train.Int64List(value=[x])),\n        'image/patch-y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),\n        'mask/patch-x': tf.train.Feature(int64_list=tf.train.Int64List(value=[x])),\n        'mask/patch-y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])),\n        'mask/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_mask_data])),\n        'mask/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[mask_format])),\n    }))\n    \n    return tf_record","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create shards + discrete images\nWe create two sharded datasets here, 20 shards will give a granularity of 5% for train/validate split. Notice that we rename all .jpg and .png file extensions to .jpg1 and .png1 to prevent Kaggle to produce thousands of thumbnails from all these images."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nPATH = '/kaggle/input/hubmap-kidney-segmentation/train/'\nIMG_SIZE = 1024 # adjust according to desired tile size\nSCALES = 3 # number of times to downscale each patch (generate SCALES+1 sets of images)\nSCALE_FACTOR = 2 # scale factor to use for each set, adjust according to needs\nimg_sizes = np.zeros(SCALES+1, dtype=int)\nimg_sizes[0] = IMG_SIZE\nfor i in range(SCALES):\n    img_sizes[i+1] = int(img_sizes[i]/SCALE_FACTOR)\n# create directories for jpeg/png images\nfor i in range(len(img_sizes)):\n    os.makedirs('./'+str(img_sizes[i]))\n    os.makedirs('./'+str(img_sizes[i])+'/Tissue')\n    os.makedirs('./'+str(img_sizes[i])+'/Bkgnd')\nfilelist = glob.glob(PATH+'*.tiff')\n\ndef open_sharded_tfrecords(exit_stack, base_path, size, num_shards):\n    tf_record_output_filenames = [\n        '{}-{}-{:04d}-of-{:04d}.tfrecord'.format(base_path, size, idx, num_shards)\n        for idx in range(num_shards)\n        ]\n    tfrecords = [\n        exit_stack.enter_context(tf.io.TFRecordWriter(file_name))\n        for file_name in tf_record_output_filenames\n    ]\n    return tfrecords\n\ndef wr_images(fname, image, mask):\n    cv2.imwrite(fname, image)\n    os.rename(fname, fname.replace('.jpg', '.jpg1'))\n    cv2.imwrite(fname.replace('jpg', 'png'), mask.astype(int)) \n    os.rename(fname.replace('jpg', 'png'), fname.replace('.jpg', '.png1'))\n\ns_th = 90  # saturation threshold\np_th = IMG_SIZE*IMG_SIZE//16 # pixle count threshold\nnum_shards=20\noutput_filebase1='./Tissue' # dataset covering Tissue\noutput_filebase2='./Bkgnd' # dataset covering everything else\ngcnt, ncnt = np.zeros(num_shards, dtype=int), np.zeros(num_shards, dtype=int)\n\n# A context2.ExitStack is used to automatically close all the TFRecords created \nwith contextlib2.ExitStack() as tf_record_close_stack:\n    output_tfrecords1 = []\n    output_tfrecords2 = []\n    for scnt in range(SCALES+1):\n        output_tfrecords1.append(open_sharded_tfrecords(tf_record_close_stack, output_filebase1, img_sizes[scnt], num_shards))\n        output_tfrecords2.append(open_sharded_tfrecords(tf_record_close_stack, output_filebase2, img_sizes[scnt], num_shards))\n    for file in filelist:\n        print(file)\n        fid = file.replace('\\\\','.').replace('/','.').split('.')[-2]\n        img, mask = np.zeros(10), np.zeros(10) \n        img = read_tif_file(file)\n        dims = np.array(img.shape[:2])\n        mask = read_mask_file(file.split('.')[0]+'.json', dims)\n        for x in range(img.shape[0]//IMG_SIZE):\n            for y in range(img.shape[1]//IMG_SIZE):\n                # Extract patch\n                patch = img[x*IMG_SIZE:(x+1)*IMG_SIZE, y*IMG_SIZE:(y+1)*IMG_SIZE]\n                m_patch = mask[x*IMG_SIZE:(x+1)*IMG_SIZE, y*IMG_SIZE:(y+1)*IMG_SIZE]*255\n                # separate tissue from bakground\n                IsTissue = False\n                _, s, _ = cv2.split(cv2.cvtColor(patch, cv2.COLOR_BGR2HSV))\n                if (s>s_th).sum() > p_th:\n                    IsTissue=True\n                tf_record = create_tf_example(patch, m_patch, fid, x, y, size=img_sizes[0])\n                if IsTissue:\n                    output_shard_index1 = (x+y) % num_shards\n                    output_tfrecords1[0][output_shard_index1].write(tf_record.SerializeToString())\n                    gcnt[output_shard_index1] += 1\n                    wr_images('./'+str(img_sizes[0])+'/Tissue/'+fid+'-'+str(x)+'-'+str(y)+'.jpg', patch, m_patch)\n                else:\n                    output_shard_index2 = (x+y) % num_shards\n                    output_tfrecords2[0][output_shard_index2].write(tf_record.SerializeToString())\n                    ncnt[output_shard_index2] += 1\n                    wr_images('./'+str(img_sizes[0])+'/Bkgnd/'+fid+'-'+str(x)+'-'+str(y)+'.jpg', patch, m_patch)\n                # create downscaled images\n                for s in range(SCALES): \n                    spatch = cv2.resize(patch, dsize=(img_sizes[s+1], img_sizes[s+1]), interpolation = cv2.INTER_AREA)\n                    sm_patch = cv2.resize(m_patch.astype(int), dsize=(img_sizes[s+1], img_sizes[s+1]), interpolation = cv2.INTER_NEAREST)\n                    tf_record = create_tf_example(spatch, sm_patch, fid, x, y, size=img_sizes[s+1])\n                    if IsTissue:\n                        output_tfrecords1[s+1][output_shard_index1].write(tf_record.SerializeToString())\n                        wr_images('./'+str(img_sizes[1+s])+'/Tissue/'+fid+'-'+str(x)+'-'+str(y)+'.jpg', spatch, sm_patch)\n                    else:\n                        output_tfrecords2[s+1][output_shard_index2].write(tf_record.SerializeToString())\n                        wr_images('./'+str(img_sizes[1+s])+'/Bkgnd/'+fid+'-'+str(x)+'-'+str(y)+'.jpg', spatch, sm_patch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a .json file with a few useful parameters we might need during training/inference:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dparams = {\n    \"IMG_SIZE\": IMG_SIZE,\n    \"SCALE_FACTOR\": SCALE_FACTOR,\n    \"SAT_THRES\": s_th,\n    \"PIXEL_THRES\": p_th}\nwith open(\"dparams.json\", \"w\") as json_file:\n    json_file.write(json.dumps(dparams, indent = 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Export a table with number of images per TFRecord."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nrecsizes = []\nfor i in range(num_shards):\n    for j in range(len(img_sizes)):\n        recsizes.append([output_filebase1.split('/')[-1]+'-{}-{:04d}-of-{:04d}.tfrecord'.format(img_sizes[j],i, num_shards), gcnt[i]])\nfor i in range(num_shards):\n    for j in range(len(img_sizes)):\n        recsizes.append([output_filebase2.split('/')[-1]+'-{}-{:04d}-of-{:04d}.tfrecord'.format(img_sizes[j], i, num_shards), ncnt[i]])\ndf = pd.DataFrame(recsizes, columns=['File', 'ImgCount'])\ndf.to_pickle('./record_stats.pkl')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check the output files\nThe final check is to load and plot a couple of TFRecords and verify that everything loads OK."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_imgs(dataset):\n    fig = plt.figure(figsize=(18,18))\n    idx=1\n    for raw_record in dataset.take(36):\n        axes = fig.add_subplot(6, 6, idx)\n        example = tf.train.Example()\n        example.ParseFromString(raw_record.numpy())\n        img_encoded=example.features.feature['image/encoded'].bytes_list.value[0]\n        img = Image.open(BytesIO(img_encoded))\n        mask_encoded=example.features.feature['mask/encoded'].bytes_list.value[0]\n        mask = Image.open(BytesIO(mask_encoded))\n        plt.setp(axes, xticks=[], yticks=[])\n        plt.imshow(img)\n        plt.imshow(mask, alpha=0.25)\n        idx=idx+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fname='./Tissue-1024-0004-of-0020.tfrecord'\ndataset = tf.data.TFRecordDataset(fname)\nplot_imgs(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The downsampled dataset should be the same images:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fname='./Tissue-128-0004-of-0020.tfrecord'\ndataset = tf.data.TFRecordDataset(fname)\nplot_imgs(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reconstruct Tissue\nAs a final test, let us reconstruct the Tissue of one of the train images from discrete images."},{"metadata":{"trusted":true},"cell_type":"code","source":"filelist = glob.glob('./256/Tissue/095bf7a1f*.jpg1')\n\ndef get_index(fname):\n    x=fname.split('.')[-2].split('-')[-2]\n    y=fname.split('.')[-2].split('-')[-1]\n    return int(x),int(y)\n\nx, y = np.zeros(len(filelist), dtype=int), np.zeros(len(filelist), dtype=int)\nidx = 0\nfor f in filelist:\n    x[idx], y[idx] = get_index(f)\n    idx += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cortex_img = np.zeros([(x.max()-x.min()+1)*256, (y.max()-y.min()+1)*256, 3], dtype=int)\ncortex_mask = np.zeros([(x.max()-x.min()+1)*256, (y.max()-y.min()+1)*256], dtype=int)\n\nfor f in filelist:\n    img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n    xi, yi = get_index(f)\n    cortex_img[(xi-x.min())*256:(xi-x.min()+1)*256,(yi-y.min())*256:(yi-y.min()+1)*256,:] = img\n    img = cv2.imread(f.replace('jpg', 'png'), cv2.IMREAD_UNCHANGED)\n    xi, yi = get_index(f)\n    cortex_mask[(xi-x.min())*256:(xi-x.min()+1)*256,(yi-y.min())*256:(yi-y.min()+1)*256] = img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,9))\naxes = fig.add_subplot(1, 2, 1)\nplt.setp(axes, xticks=[], yticks=[])\nplt.imshow(cortex_img)\naxes = fig.add_subplot(1, 2, 2)\nplt.setp(axes, xticks=[], yticks=[])\nplt.imshow(cortex_mask);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yup, that looks OK!   \nHere is the image directory structure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"base = './'\nfor root, dirs, files in os.walk(base):\n    level = root.replace(base, '').count(os.sep)\n    indent = ' ' * 4 * (level)\n    print('{}{}/'.format(indent, os.path.basename(root)))\n    subindent = ' ' * 4 * (level + 1)\n    if level > 0:\n        for f in files[:2]:\n            print('{}{}'.format(subindent, f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}