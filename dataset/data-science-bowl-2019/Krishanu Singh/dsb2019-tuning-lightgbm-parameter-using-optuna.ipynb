{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>DataSciencBowl2019:Tuning LightGBM Parameter Using OPTUNA</h1>\n<h4>\nIn this kernel, I will be emphasizing upon how to tune LightGBM Parameters using OPTUNA. Hyperparameter tuning is a very tedious & time consuming task.There are multiple strategies available to find optimal parameters using grid-search & random-search. OPTUNA can be used to get a good choice of Hyperparameters in a smarter way. OPTUNA is a hyperparametes optimzation framework based on Bayesian methods & to know more about it, please visit https://optuna.org/. I used OPTUNA to tune parameters for my LightGBM model for DataScienceBowl2019 Competition. To know more about DataScienceBowl2019, please visit https://datasciencebowl.com/ </h4>"},{"metadata":{},"cell_type":"markdown","source":"**<h2>1. Importing Libraries & Loading the Data</h2>**   <h4>This notebook was used as a submission to DataScienceBowl 2019 Competition. Since this is a kernel emphasizing upon Hyperparameter tuning using OPTUNA, I will be skipping the data processing & feature engineering steps. </h4>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Any results you write to the current directory are saved as output.\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport optuna\npd.set_option('display.max_columns', 1000)\n\ndef read_data():\n    sample_submission = pd.read_csv(\"../input/data-science-bowl-2019/sample_submission.csv\")\n    specs = pd.read_csv(\"../input/data-science-bowl-2019/specs.csv\")\n    test = pd.read_csv(\"../input/data-science-bowl-2019/test.csv\")\n    train = pd.read_csv(\"../input/data-science-bowl-2019/train.csv\")\n    train_labels = pd.read_csv(\"../input/data-science-bowl-2019/train_labels.csv\") \n    return train, test, train_labels, specs, sample_submission\n\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    \n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n                    \n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(last_accuracy_title.copy())\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(session[col])\n                for k in num_of_session_count.keys():\n                    x = k\n                    if col == 'title':\n                        x = activities_labels[k]\n                    counter[x] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type \n                        \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\ndef get_train_and_test(train, test):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += get_data(user_sample)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n        test_data = get_data(user_sample, test_set = True)\n        compiled_test.append(test_data)\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print('********Reading & Processing Data**********')\n# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n# tranform function to get the train and test set\nreduce_train, reduce_test, categoricals = get_train_and_test(train, test)\n\nreduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\nreduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]\n\nfeatures = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\nfeatures = [x for x in features if x not in ['accuracy_group', 'installation_id']]\n\ntarget = 'accuracy_group'\nxTrain, xTest = reduce_train[features],reduce_test[features]\nyTrain, yTest = reduce_train[target],reduce_test[target]\ncategoryCols = ['session_title']\nprint('******** Finished Reading & Preparing the Data**********')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<h1>2.OPTUNA</h1>** <h4>Optuna uses Bayesian methods to figure out an optimal set of hyperparameters. For more information on Bayesian methods for searching optimal parameters, check out this wonderful article : https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f .</h4>\n\n<h2> 2.1 Objective Function</h2> \n<h4>First important step is to define an objective function.Let's understand this with an example from OPTUNA docs.</h4>\n\n<h4><mark>def objective(trial):</mark></h4>\n<h4><mark>x = trial.suggest_uniform('x', -10, 10)</mark></h4>\n<h4><mark>return (x - 2) ** 2</mark></h4>\n\n<h4>This function returns the value of (ùë• ‚àí 2)^2.Our goal is to find the value of x that minimizes the output of the objective function.This is the optimization.During the optimization, Optuna repeatedly calls and evaluates the objective function with different values of x.A Trial object corresponds to a single execution of the objective function and is internally instantiated upon each invocation of the function.</h4>  <h4>In our case, we will be training lightGBM Model and using the cross validation score for evaluation.We will be returing this score from our objective function. For Data Science Bowl 2019 competition, the metric supposed to be used was cohen_kappa_score. Even though I will be computing cohen_kappa_score, but I will be using cross validation score for evaluation of the objective function for the sake of simplicity.</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from optuna import Trial\n\ndef objective(trial:Trial,fastCheck=True,targetMeter=0,returnInfo=False):\n    folds = 5\n    seed  = 666\n    shuffle = False\n    kf = KFold(n_splits=folds,shuffle=False,random_state=seed)\n    yValidPredTotal = np.zeros(xTrain.shape[0])\n    gc.collect()\n    catFeatures=[xTrain.columns.get_loc(catCol) for catCol in categoryCols]\n    models=[]\n    validScore=0\n    for trainIdx,validIdx in kf.split(xTrain,yTrain):\n        trainData=xTrain.iloc[trainIdx,:],yTrain[trainIdx]\n        validData=xTrain.iloc[validIdx,:],yTrain[validIdx]\n        model,yPredValid,log = fitLGBM(trial,trainData,validData,catFeatures=categoryCols,numRounds=1000)\n        yValidPredTotal[validIdx]=yPredValid\n        models.append(model)\n        gc.collect()\n        validScore+=log[\"validRMSE\"]\n    validScore/=len(models)\n    return validScore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>2.2 Defining Parameter Space for OPTUNA </h2>\n<h4>OPTUNA supports five kind of parameters</h4>\n\n<ul>\n<h4>def objective(trial):</h4>\n<h4><mark>#Categorical parameter : A categorical distribution</mark></h4>\n<h4>optimizer = trial.suggest_categorical('optimizer', ['MomentumSGD', 'Adam'])</h4>\n<h4><mark>#Int parameter : A uniform distribution on integers.</mark></h4>\n<h4>num_layers = trial.suggest_int('num_layers', 1, 3)</h4>\n<h4><mark>#Uniform Parameter : A uniform distribution in the linear domain.</mark></h4>\n<h4>dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 1.0)</h4>\n<h4><mark>#Loguniform Parameter : A uniform distribution in the log domain.</mark></h4>\n<h4>learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)</h4>\n<h4><mark>#Discrete-uniform parameter : A discretized uniform distribution in the linear domain.</mark></h4>\n<h4>drop_path_rate = trial.suggest_discrete_uniform('drop_path_rate', 0.0, 1.0, 0.1)</h4>\n</ul>\n<h4>In the below function, we will be setting up the parameter space for LightGBM. Please refer to LightGBM Documentation for exploring more on the LightGBM parameters https://lightgbm.readthedocs.io/en/latest/Parameters.html. Below parameter space may not be exhaustive one and can be modified as per the requirement</h4>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fitLGBM(trial,train,val,catFeatures=None,numRounds=1500):\n    xTrainLGBM,yTrainLGBM = train\n    xValidLGBM,yValidLGBM = val\n    boosting_list = ['gbdt','goss']\n    objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n    objective_list_class = ['binary', 'cross_entropy']\n    params={\n      'boosting':trial.suggest_categorical('boosting',boosting_list),\n      'num_leaves':trial.suggest_int('num_leaves', 2, 2**11),\n      'max_depth':trial.suggest_int('max_depth', 2, 25),\n      'max_bin': trial.suggest_int('max_bin', 32, 255),      \n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 256),\n      'min_data_in_bin': trial.suggest_int('min_data_in_bin', 1, 256),\n      'min_gain_to_split' : trial.suggest_discrete_uniform('min_gain_to_split', 0.1, 5, 0.01),      \n      'lambda_l1':trial.suggest_loguniform('lamda_l1',1e-8,10),\n      'lambda_l2':trial.suggest_loguniform('lamda_l2',1e-8,10),\n      'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n      'metric':trial.suggest_categorical('metric', ['RMSE']),\n      'objective':trial.suggest_categorical('objective',objective_list_reg),\n      'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.5, 1, 0.01),\n      'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.5, 1, 0.01),\n    }\n    earlyStop=20\n    verboseEval=0\n    dTrain = lgb.Dataset(xTrainLGBM,label=yTrainLGBM,categorical_feature=catFeatures)\n    dValid = lgb.Dataset(xValidLGBM,label=yValidLGBM,categorical_feature=catFeatures)\n    watchlist = [dTrain,dValid]\n\n    # Callback for pruning.\n    lgbmPruningCallback = optuna.integration.LightGBMPruningCallback(trial, 'rmse', valid_name='valid_1')\n\n    model = lgb.train(params,train_set=dTrain,num_boost_round=numRounds,valid_sets=watchlist,verbose_eval=verboseEval,early_stopping_rounds=earlyStop,callbacks=[lgbmPruningCallback])\n\n    #predictions\n    pred_val=model.predict(xValidLGBM,num_iteration=model.best_iteration)\n    pred_val[pred_val <= 1.12232214] = 0\n    pred_val[np.where(np.logical_and(pred_val > 1.12232214, pred_val <= 1.73925866))] = 1\n    pred_val[np.where(np.logical_and(pred_val > 1.73925866, pred_val <= 2.22506454))] = 2\n    pred_val[pred_val > 2.22506454] = 3\n    oofPred = pred_val.astype(int)        \n    score=cohen_kappa_score(oofPred,yValidLGBM,weights='quadratic')\n    print('***********************choen_kappa_score :',score)\n    log={'trainRMSE':model.best_score['training']['rmse'],\n       'validRMSE':model.best_score['valid_1']['rmse']}\n    return model,pred_val,log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>2.3 Optimization & STUDY Object </h2>\n**Below are some important terminologies mentioned in the OPTUNA docs, understanding which will make our jobs easier:**\n* **Trial**: **A single call of the objective function**\n* **Study**: **An optimization session, which is a set of trials**\n* **Parameter**: **A variable whose value is to be optimized, such as x in the above example**  \n\n<h4>\nI have already explained about Trial & Parameter in the above sections. In Optuna, we use the study object to manage optimization. Method create_study() returns a study object. A study object has useful properties for analyzing the optimization outcome.Once we create the study object, we can call the Optimize() and let the show begin!\n</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective,n_trials=10)#For the sake of simplicity, I have kept n_trials as less, but this can be altered for better results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>2.4 Pruning Uncompromising Trials in OPTUNA </h2>\n<h4>\nUnpromising trials can be automotically detected at the early stages of the training & thus can be preempted(a.k.a., automated early-stopping).Optuna provides interfaces to concisely implement the pruning mechanism in iterative training algorithms.To implement pruning mechanism in much simpler forms, Optuna provides integration modules for the following libraries\n</h4>\n* **XGBoost**: optuna.integration.XGBoostPruningCallback\n* **LightGBM**: optuna.integration.LightGBMPruningCallback\n* **Chainer**: optuna.integration.ChainerPruningExtension\n* **Keras**: optuna.integration.KerasPruningCallback\n* **TensorFlow** optuna.integration.TensorFlowPruningHook\n* **tf.keras** optuna.integration.TFKerasPruningCallback\n* **MXNet** optuna.integration.MXNetPruningCallback\n* **PyTorch** Ignite optuna.integration.PyTorchIgnitePruningHandler\n* **PyTorch** Lightning optuna.integration.PyTorchLightningPruningCallback\n* **FastAI** optuna.integration.FastAIPruningCallback\n\n**In the function fitLGBM defined above, I have added a Pruning Callback function:  \nlgbmPruningCallback = optuna.integration.LightGBMPruningCallback(trial, 'rmse', valid_name='valid_1')**"},{"metadata":{},"cell_type":"markdown","source":"<h2>2.5 OPTUNA Study History : Analysis & Visualization </h2>\n<h3>We can get the entire history of all the trials by calling *study.trials_dataframe()* which returns a pandas.DataFrame</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = study.trials_dataframe()\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>We can obtain best trial parameters by calling study.best_trial</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>optuna.visualization.plot_optimization_history(study) : This function plots optimization history of all trials in a study</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>optuna.visualization.plot_slice(study, params=None) : This function plots the parameter relationship as slice plot in a study</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_slice(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>optuna.visualization.plot_parallel_coordinate(study, params=None) : This function plots the high-dimentional parameter relationships in a study.</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_parallel_coordinate(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>\nThat's all from my side. Hope you find this kernel useful and please don't forget to UPVOTE this kernel. CHEERS!!\n</h2>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}