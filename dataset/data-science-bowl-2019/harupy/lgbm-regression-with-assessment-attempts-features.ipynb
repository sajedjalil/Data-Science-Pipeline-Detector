{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LGBM Regression with assessment attempt features\n\nThis kernel is based on https://www.kaggle.com/artgor/quick-and-dirty-regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"%ls -lh ../input/data-science-bowl-2019","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport IPython\nimport gc\n\n\npd.set_option('display.max_columns', None)\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display(*dfs, head=True):\n    \"\"\"\n    Display multiple dataframes\n    \"\"\"\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numeric_dtypes = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    mem_before = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        dtype = df[col].dtypes\n\n        if dtype in numeric_dtypes:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(dtype)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float132)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    mem_after = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(mem_after, 100 * (mem_before - mem_after) / mem_before))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/data-science-bowl-2019'\ntrain = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\nlabels = pd.read_csv(os.path.join(DATA_DIR, 'train_labels.csv'))\ntrain = train[train['installation_id'].isin(labels['installation_id'].unique())]\n\ntest = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\ntitle_pred = test.groupby('installation_id').last().reset_index()[['installation_id', 'title']]\nsbm = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))\n\nreduce_mem_usage(train)\nreduce_mem_usage(test)\n\ndisplay(\n    train,\n    labels,\n    test,\n    labels,\n    title_pred,\n    sbm,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert title_pred['installation_id'].equals(sbm['installation_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filter assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_assessments(df):\n    is_assessment = (\n        (df['title'].eq('Bird Measurer (Assessment)') & df['event_code'].eq(4110)) |\n        (~df['title'].eq('Bird Measurer (Assessment)') & df['event_code'].eq(4100)) &\n        df['type'].eq('Assessment')\n    )\n    return df[is_assessment].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract attempt results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_attempt(df):\n    \"\"\"\n    Extract attempt result as boolean (true: correct, false: incorrect)\n    \"\"\"\n    correct = df['event_data'].str.extract(r'\"correct\":([^,]+)', expand=False).eq('true').astype(int)\n    return df.assign(correct=correct)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate attempt stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"def with_name(func, name):\n    func.__name__ = name\n    return func\n\n\ndef accuracy_group(acc):\n    if acc == 0:\n        return 0\n    elif acc == 1:\n        return 3\n    elif acc == 0.5:\n        return 2\n    else:\n        return 1\n\n\ndef calc_attempt_stats(df):\n    aggs = {\n        'correct': [\n            with_name(lambda s: (s == 1).sum(), 'num_correct'),\n            with_name(lambda s: (s == 0).sum(), 'num_incorrect'),\n            with_name(lambda s: s.size, 'attempts'),\n            with_name(lambda s: s.mean(), 'accuracy'),\n        ],\n\n        'timestamp': [\n            with_name(lambda s: s.iloc[-1], 'timestamp'),\n        ],\n    }\n    \n    # apply aggregation\n    by = ['installation_id', 'title', 'game_session']\n    stats = df.groupby(by, sort=False).agg(aggs).reset_index()\n\n    # flatten multi-level columns\n    stats.columns = [col[1] if (col[1] != '') else col[0] for col in stats.columns]\n\n    # add accuracy group\n    stats = stats.assign(accuracy_group=stats['accuracy'].map(accuracy_group).astype(np.int8))\n\n    return stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Expand attempt stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import reduce\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ndef add_prefix(l, prefix, sep='_'):\n    \"\"\"\n    Add prefix to list of strings\n    \"\"\"\n    return [prefix + sep + x for x in l]\n\n\ndef concat_dataframes(dfs, axis):\n    \"\"\"\n    Concat arbitrary number of dataframes\n    \"\"\"\n    return reduce(lambda l, r: pd.concat([l, r], axis=axis), dfs)\n\n\ndef expand_stats(df):\n    \"\"\"\n    Calculate product of assessment stats and one-hot encoded title vector\n    \n    Input DataFrame:\n            num_correct\n    game_A            1 \n    game_B            2\n    \n    Output DataFrame:\n            game_A_num_correct  game_B_num_correct\n    game_A                   1                   0                   \n    game_B                   0                   2   \n    \"\"\"\n    col = 'title'\n    enc = OneHotEncoder().fit(df[[col]])\n    enc_cols = enc.categories_[0]\n    one_hot = enc.transform(df[[col]]).toarray().astype(np.int8) \n\n    dfs = [df]\n    cols = ['num_correct', 'num_incorrect', 'attempts', 'accuracy', 'accuracy_group']\n\n    for col in cols:\n        prod = pd.DataFrame(df[[col]].values * one_hot,\n                            columns=add_prefix(enc_cols, col))\n        dfs.append(prod)\n    \n    return concat_dataframes(dfs, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculate cumulative features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n\ndef filter_cols_startswith(cols, s):\n    return [c for c in cols if c.startswith(s)]\n\n\ndef calc_cum(df, is_test=False):\n\n    def process_gdf(df):\n        funcs = {\n            'cumsum': ['num_correct', 'num_incorrect', 'attempts'],\n            'cummean': ['accuracy'],  # note that this contains accuracy_group\n        }\n\n        dfs = []\n        drop_cols = []\n        for func, patterns in funcs.items():\n            for pat in patterns:\n                cols = filter_cols_startswith(df.columns, pat)\n                drop_cols += cols\n\n                # for test, it's not necessary to shift rows\n                periods = int(not is_test)\n\n                if func == 'cumsum':\n                    cum = df[cols].cumsum().shift(periods)\n                elif func == 'cummean':\n                    cum = df[cols].expanding().mean().shift(periods)\n\n                cum.columns = add_prefix(cols, func)\n                dfs.append(cum)\n\n        # keep accuracy_group for training\n        drop_cols.remove('accuracy_group')\n\n        return concat_dataframes([df.drop(drop_cols, axis=1)] + dfs, axis=1)\n    \n    return df.groupby('installation_id', sort=False).apply(process_gdf)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assessment game stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"game_stats = labels.groupby('title').agg({\n    'num_correct': with_name(lambda s: s.sum(), 'num_correct_sum'),\n    'num_incorrect': with_name(lambda s: s.sum(), 'num_incorrect_sum'),\n    'accuracy': with_name(lambda s: s.mean(), 'avg_acc'),\n    'accuracy_group': [\n        with_name(lambda s: s.mean(), 'avg_acg'),\n        with_name(lambda s: s.value_counts().index[0], 'most_freq_acg'),\n    ]\n}).reset_index()\n\ngame_stats.columns = [col[1] if (col[1] != '') else col[0] for col in game_stats.columns]\ngame_stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_funcs(df, funcs, debug=True):\n    applied = df\n    for func in funcs:\n        applied = func(applied)\n        \n        if debug:\n            print(func.__name__, applied.shape)\n            display(applied)\n\n    return applied","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions to apply on both train and test\nfuncs_common = [\n    filter_assessments,\n    extract_attempt,\n    calc_attempt_stats,\n    expand_stats,\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"funcs_train = [\n    lambda df: calc_cum(df, is_test=False),\n    lambda df: pd.merge(df, game_stats, on='title', how='left'),\n]\n\nfinal_train = apply_funcs(train, funcs_common + funcs_train)\n\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"funcs_test = [\n    lambda df: calc_cum(df, is_test=True),\n    lambda df: df.groupby('installation_id', sort=False).last().reset_index(),\n    lambda df: pd.merge(title_pred, df.drop('title', axis=1), on='installation_id', how='left'),\n    lambda df: pd.merge(df, game_stats, on='title', how='left'),\n]\n\nfinal_test = apply_funcs(test, funcs_common + funcs_test)\n\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\nins_ids_train = final_train['installation_id']  # keep installation_id for group k fold\nX_train = final_train.select_dtypes('number').drop('accuracy_group', axis=1)\ny_train = final_train['accuracy_group']\n\n# test\nins_ids_test = final_test['installation_id']\nX_test = final_test.select_dtypes('number').drop('accuracy_group', axis=1)\n\n\ndel final_train, final_test\ngc.collect()\n\nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_test:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert X_train.columns.tolist() == X_test.columns.tolist()\nassert sbm['installation_id'].equals(ins_ids_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bst_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'subsample': 0.75,\n    'subsample_freq': 1,\n    'learning_rate': 0.04,\n    'feature_fraction': 0.9,\n    'max_depth': 15,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'random_state': 42,\n}\n\nfit_params = {\n    'num_boost_round': 10000,\n    'verbose_eval': 100,\n    'early_stopping_rounds': 100,\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numba import jit\nfrom functools import partial\nimport scipy as sp\n\n\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n    def coefficients(self):\n        return self.coef_['x']\n\n\ndef div_by_sum(x):\n    return x / x.sum()\n\n\ndef print_divider(text):\n    print('\\n---------- {} ----------\\n'.format(text))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfold = GroupKFold(n_splits=5)\n\nfi_split = np.zeros(X_train.shape[1])\nfi_gain = np.zeros(X_train.shape[1])\noof_pred = np.zeros(len(X_train))\npred_test = np.zeros(len(X_test))\ncoff_avg = np.zeros(3)\n\n\nfor fold_idx, (idx_trn, idx_val) in enumerate(fold.split(X_train, y_train, ins_ids_train)):\n    print_divider(f'Fold: {fold_idx}')\n    X_trn, X_val = X_train.iloc[idx_trn], X_train.iloc[idx_val]\n    y_trn, y_val = y_train[idx_trn], y_train[idx_val]\n\n    d_trn = lgb.Dataset(X_trn, y_trn)\n    d_val = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(bst_params, d_trn,\n                      valid_sets=[d_trn, d_val],\n                      **fit_params)\n\n    fi_split += div_by_sum(model.feature_importance(importance_type='split')) / fold.n_splits\n    fi_gain += div_by_sum(model.feature_importance(importance_type='gain')) / fold.n_splits\n\n    pred_val = model.predict(X_val)\n    pred_train = model.predict(X_trn)\n    oof_pred[idx_val] = pred_val\n    pred_test += model.predict(X_test) / fold.n_splits\n\n    optr = OptimizedRounder()\n    optr.fit(pred_train, y_trn)\n    coff_avg += optr.coefficients() / fold.n_splits\n    print('\\nround coefficients:', optr.coefficients())\n    \n    del X_trn, y_trn, X_val, y_val\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coff_avg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(features, fi, fi_type, limit=30):\n    fig, ax = plt.subplots(figsize=(6, 6))\n    idxs = np.argsort(fi)[-limit:]\n    y = np.arange(len(idxs))\n    ax.barh(y, fi[idxs], align='center', height=0.5)\n    ax.set_yticks(y)\n    ax.set_yticklabels(features[idxs])\n    ax.set_xlabel('Importance')\n    ax.set_ylabel('Feature')\n    ax.set_title(f'Feature Importance: {fi_type}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = np.array(model.feature_name())\nplot_feature_importance(features, fi_split, 'split')\nplot_feature_importance(features, fi_gain, 'gain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Round prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_pred_round = optr.predict(oof_pred, coff_avg)\nqwk(y_train, oof_pred_round)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_round = optr.predict(pred_test, coff_avg)\npred_round[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sbm['accuracy_group'] = pred_round.astype(int)\nsbm['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert sbm['accuracy_group'].notnull().all()\nassert sbm['accuracy_group'].isin([0, 1, 2, 3]).all()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sbm.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:machine-learning]","language":"python","name":"conda-env-machine-learning-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}