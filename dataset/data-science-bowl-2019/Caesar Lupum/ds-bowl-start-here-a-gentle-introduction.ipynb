{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 2019 Data Science Bowl\n#### Uncover the factors to help measure how young children learn\n[CrislÃ¢nio MacÃªdo](https://www.linkedin.com/in/crislanio/) -  January, 02th, 2020\n\n[Github](https://github.com/crislanio)  ________________[Sapere Aude Tech](https://medium.com/sapere-aude-tech) ________________[AboutMe](https://crislanio.wordpress.com/about)________________[Twitter](https://twitter.com/crs_macedo)________________[Ensina.AI](https://medium.com/ensina-ai/an%C3%A1lise-dos-dados-abertos-do-governo-federal-ba65af8c421c)________________[Quora](https://www.quora.com/profile/Crislanio)________________[Hackerrank](https://www.hackerrank.com/crislanio_ufc)\n\nðŸ“’ EDA: [ðŸ“’ðŸ‘¦ðŸ‘§ DS Bowl - Start here: A GENTLE Introduction](https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction)\n\n----------\n----------","metadata":{}},{"cell_type":"markdown","source":"![](http://www.gpb.org/sites/www.gpb.org/files/styles/hero_image/public/blogs/images/2018/08/07/maxresdefault.jpg?itok=gN6ErLyU)\n## [ðŸ“’ðŸ‘¦ðŸ‘§ DS Bowl - Start here: A GENTLE Introduction](https://www.kaggle.com/caesarlupum/ds-bowl-start-here-a-gentle-introduction)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"# Content","metadata":{}},{"cell_type":"markdown","source":"### ðŸ“’In this kernel I want to show a Exhaustive Analysis of  DS Bowl, with:\n\n- <a href='#1'>1. Where does the data for the competition come from?</a>\n- <a href='#2'>2. Data Description</a>\n- <a href='#3'>3. Read in Data </a>\n- <a href='#4'>4. Glimpse of Data </a>\n- <a href='#5'>5. Insights </a>\n- <a href='#6'>6. Exploratory Data Analysis </a>\n- <a href='#7'>7. Column Types </a>\n    - <a href='#7-1'>7.1 Number of each type of column - Train </a>\n    - <a href='#7-2'>7.2 Number of unique classes in each object column  - Train </a>\n    - <a href='#7-3'>7.3 Number of each type of column - Train_labels </a>\n    - <a href='#7-4'>7.4 Number of each type of column - Train_labels </a>\n    - <a href='#7-5'>7.5 Number of unique classes in each object column  - Train_labels </a>\n    - <a href='#7-6'>7.6 Number of each type of column - Specs </a>\n    - <a href='#7-7'>7.7 Number of unique classes in each object column  - Specs </a>\n- <a href='#8'>8. Examine Missing Values </a>\n- <a href='#9'>9. Correlations </a>\n- <a href='#10'>10. Ploting </a>\n- <a href='#11'>11. Simple Baseline </a>\n- <a href='#12'>12. Simple LGBM aggregated data with CV </a>\n- <a href='#13'>13. Submission </a>\n- <a href='#14'>14. Evaluation </a>\n- <a href='#15'>15. Links for the Game </a>\n- <a href='#16'>General findings </a>\n","metadata":{}},{"cell_type":"markdown","source":"### Illuminate Learning. Ignite Possibilities.\nUncover new insights in early childhood education and how media can support learning outcomes. Participate in our fifth annual Data Science Bowl, presented by Booz Allen Hamilton and Kaggle.\n\nPBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, youâ€™ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.\n\n##### Data Science Bowl is the worldâ€™s largest data science competition focused on social good. Each year, this competition gives Kagglers a chance to use their passion to change the world. Over the last four years, more than 50,000+ Kagglers have submitted over 114,000+ submissions, to improve everything from lung cancer and heart disease detection to ocean health.\n\n> For more information on the Data Science Bowl, please visit [DataScienceBowl.com](DataScienceBowl.com)\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nfrom IPython.display import HTML\nHTML('<iframe width=\"1100\" height=\"619\" src=\"https://www.youtube.com/embed/45Da3eqQKXQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:33.166114Z","iopub.execute_input":"2022-01-23T16:19:33.166693Z","iopub.status.idle":"2022-01-23T16:19:33.177494Z","shell.execute_reply.started":"2022-01-23T16:19:33.166644Z","shell.execute_reply":"2022-01-23T16:19:33.176469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='1'>Where does the data for the competition come from?</a>\n<a href= '#1'> Top</a>\n\nThe data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a userâ€™s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.\n\nPBS KIDS is committed to creating a safe and secure environment that family members of all ages can enjoy. The PBS KIDS Measure Up! app does not collect any personally identifying information, such as name or location. All of the data used in the competition is anonymous. To view the full PBS KIDS privacy policy, please visit: [pbskids.org/privacy](pbskids.org/privacy).\n\nNo one will be able to download the entire data set and the participants do not have access to any personally identifiable information about individual users. The Data Science Bowl and the use of data for this yearâ€™s competition has been reviewed to ensure that it meets requirements of applicable child privacy regulations by PRIVO, a leading global industry expert in childrenâ€™s online privacy.\n\n### What is the PBS KIDS Measure Up! app?\nIn the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play. To learn more about PBS KIDS Measure Up!, please click here.\n\nPBS KIDS and the PBS KIDS Logo are registered trademarks of PBS. Used with permission. The contents of PBS KIDS Measure Up! were developed under a grant from the Department of Education. However, those contents do not necessarily represent the policy of the Department of Education, and you should not assume endorsement by the Federal Government. The app is funded by a Ready To Learn grant (PR/AWARD No. U295A150003, CFDA No. 84.295A) provided by the Department of Education to the Corporation for Public Broadcasting.\n![image](http://www.wics.jp/data/editor/1902/thumb-962c8c816b10ed5f6586ece6e7a5f63d_1550554761_2866_600x400.jpg)\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='2'>Data Description</a>\n<a href= '#1'> Top</a>\n\nIn this dataset, you are provided with game analytics for the PBS KIDS Measure Up! app. In this app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments. Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter.\n\nThe intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt). Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices. In the training set, you are provided the full history of gameplay data. In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\n#####  The outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n- 3: the assessment was solved on the first attempt\n- 2: the assessment was solved on the second attempt\n- 1: the assessment was solved after 3 or more attempts\n- 0: the assessment was never solved\n> The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\n> Note that this is a synchronous rerun code competition and the private test set has approximately 8MM rows. You should be mindful of memory in your notebooks to avoid submission errors.\n\n### Files\n\n- train.csv & test.csv\n\nThese are the main data files which contain the gameplay events.\n\n- event_id - Randomly generated unique identifier for the event type. Maps to event_id column in specs table.\n- game_session - Randomly generated unique identifier grouping events within a single game or video play session.\n- timestamp - Client-generated datetime\n- event_data - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and - - game_time; otherwise fields are determined by the event type.\n- installation_id - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- event_count - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n- event_code - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always - - - - identifies the 'Start Game' event for all games. Extracted from event_data.\n- game_time - Time in milliseconds since the start of the game session. Extracted from event_data.\n- title - Title of the game or video.\n- type - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- world - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length/Height), 'MAGMAPEAK' (Capacity/Displacement), 'CRYSTALCAVES' (Weight).\n- specs.csv\n\n##### This file gives the specification of the various event types.\n\n- event_id - Global unique identifier for the event type. Joins to event_id column in events table.\n- info - Description of the event.\n- args - JSON formatted string of event arguments. Each argument contains:\n- name - Argument name.\n- type - Type of the argument (string, int, number, object, array).\n- info - Description of the argument.\n- train_labels.csv\n\n###### This file demonstrates how to compute the ground truth for the assessments in the training set.\n\n- sample_submission.csv\nA sample submission in the correct format.","metadata":{}},{"cell_type":"markdown","source":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Purple\">If you find this kernel useful or interesting, please don't forget to upvote the kernel =)\n\n</body>\n</html>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Imports\n\n> We are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. \n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 100)\n\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\n\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nfrom tqdm import tqdm_notebook\n\nfrom IPython.display import HTML\n\n%matplotlib inline\nplt.rc('figure', figsize=(15.0, 8.0))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:33.17912Z","iopub.execute_input":"2022-01-23T16:19:33.179534Z","iopub.status.idle":"2022-01-23T16:19:35.728861Z","shell.execute_reply.started":"2022-01-23T16:19:33.179476Z","shell.execute_reply":"2022-01-23T16:19:35.728004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='3'>3. Read in Data</a>\n<a href= '#1'> Top</a>\n\nFirst, we can list all the available data files. There are a total of 6 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 4 other files containing additional information about energy types based on historic usage rates and observed weather. . ","metadata":{}},{"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/data-science-bowl-2019/\"))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.730756Z","iopub.execute_input":"2022-01-23T16:19:35.731131Z","iopub.status.idle":"2022-01-23T16:19:35.939476Z","shell.execute_reply.started":"2022-01-23T16:19:35.731064Z","shell.execute_reply":"2022-01-23T16:19:35.935829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nroot = '../input/data-science-bowl-2019/'\n\n# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\ntrain = pd.read_csv(root + 'train.csv',usecols=keep_cols)\ntest = pd.read_csv(root + 'test.csv', usecols=keep_cols)\n\ntrain_labels = pd.read_csv(root + 'train_labels.csv')\nspecs = pd.read_csv(root + 'specs.csv')\nsample_submission = pd.read_csv(root + 'sample_submission.csv')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.940343Z","iopub.status.idle":"2022-01-23T16:19:35.940752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='4'>4. Glimpse of Data</a>\n<a href= '#1'> Top</a>\n","metadata":{}},{"cell_type":"code","source":"print('Size of train data', train.shape)\nprint('Size of train_labels data', train_labels.shape)\nprint('Size of specs data', specs.shape)\nprint('Size of test data', test.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.94209Z","iopub.status.idle":"2022-01-23T16:19:35.942617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='5'>5. Insigths</a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.943876Z","iopub.status.idle":"2022-01-23T16:19:35.944512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train_labels**","metadata":{}},{"cell_type":"code","source":"train_labels.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.946503Z","iopub.status.idle":"2022-01-23T16:19:35.947178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Specs**","metadata":{}},{"cell_type":"code","source":"specs.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.948552Z","iopub.status.idle":"2022-01-23T16:19:35.949181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='6'>6. Exploratory Data Analysis</a>\n<a href= '#1'> Top</a>\n\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, patterns, or relationships within the data. ","metadata":{}},{"cell_type":"markdown","source":"> Still in progress","metadata":{}},{"cell_type":"markdown","source":"# <a id='7'>7. Column Types</a>\n<a href= '#1'> Top</a>\n\n\nLet's look at the number of columns of each data type. `int64` and `float64` are numeric variables ([which can be either discrete or continuous](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data)). `object` columns contain strings and are  [categorical features.](http://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/what-are-categorical-discrete-and-continuous-variables/) . ","metadata":{}},{"cell_type":"markdown","source":"### <a id='7-1'>7.1 Number of each type of column - Train</a>\n<a href= '#1'> Top</a>\n","metadata":{}},{"cell_type":"markdown","source":"train.dtypes","metadata":{}},{"cell_type":"code","source":"train.dtypes.value_counts()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.950465Z","iopub.status.idle":"2022-01-23T16:19:35.951135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='7-2'>7.2 Number of unique classes in each object column  - Train</a>\n<a href= '#1'> Top</a>\n","metadata":{}},{"cell_type":"code","source":"train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.952451Z","iopub.status.idle":"2022-01-23T16:19:35.953004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='7-3'>7.3 Number of each type of column - Train_labels</a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"train_labels.dtypes","metadata":{}},{"cell_type":"code","source":"train_labels.dtypes.value_counts()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.954284Z","iopub.status.idle":"2022-01-23T16:19:35.95494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='7-4'>7.4 Number of unique classes in each object column  - Train_labels</a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"code","source":"train_labels.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.956227Z","iopub.status.idle":"2022-01-23T16:19:35.956766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='7-5'>7.5 Number of each type of column - Specs</a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"specs.dtypes","metadata":{}},{"cell_type":"code","source":"specs.dtypes.value_counts()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.958137Z","iopub.status.idle":"2022-01-23T16:19:35.958645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='7-6'>7.6 Number of unique classes in each object column  - Specs</a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"code","source":"specs.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.960046Z","iopub.status.idle":"2022-01-23T16:19:35.960657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='8'>8. Examine Missing Values</a>\n<a href= '#1'> Top</a>\n\n\n\nNext we can look at the number and percentage of missing values in each column. ","metadata":{}},{"cell_type":"markdown","source":"### checking missing data for train","metadata":{}},{"cell_type":"code","source":"total = train.isnull().sum().sort_values(ascending = False)\npercent = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending = False)\nmissing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing__train_data.head(10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.96191Z","iopub.status.idle":"2022-01-23T16:19:35.962473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### checking missing data for Train_labels","metadata":{}},{"cell_type":"code","source":"total = train_labels.isnull().sum().sort_values(ascending = False)\npercent = (train_labels.isnull().sum()/train_labels.isnull().count()*100).sort_values(ascending = False)\nmissing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing__train_data.head(10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.963893Z","iopub.status.idle":"2022-01-23T16:19:35.964384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### checking missing data for Specs","metadata":{}},{"cell_type":"code","source":"total = specs.isnull().sum().sort_values(ascending = False)\npercent = (specs.isnull().sum()/specs.isnull().count()*100).sort_values(ascending = False)\nmissing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing__train_data.head(10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.965576Z","iopub.status.idle":"2022-01-23T16:19:35.966363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='9'>9. Correlations</a>\n<a href= '#1'> Top</a>\n\n\n\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the `.corr` dataframe method.\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some [general interpretations of the absolute value of the correlation coefficent](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) are:\n\n\n* .00-.19 â€œvery weakâ€\n*  .20-.39 â€œweakâ€\n*  .40-.59 â€œmoderateâ€\n*  .60-.79 â€œstrongâ€\n* .80-1.0 â€œvery strongâ€\n","metadata":{}},{"cell_type":"markdown","source":"> ### Train Correlation","metadata":{}},{"cell_type":"code","source":"corrs = train.corr()\ncorrs","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.967945Z","iopub.status.idle":"2022-01-23T16:19:35.968465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20, 8))\n\n# Heatmap of correlations\nsns.heatmap(corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.969496Z","iopub.status.idle":"2022-01-23T16:19:35.970076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Train_labels Correlation","metadata":{}},{"cell_type":"code","source":"corrs2 = train_labels.corr()\ncorrs2","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.971156Z","iopub.status.idle":"2022-01-23T16:19:35.971529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20, 8))\n\n# Heatmap of correlations\nsns.heatmap(corrs2, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.972506Z","iopub.status.idle":"2022-01-23T16:19:35.973103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='10'>10. Ploting </a>\n<a href= '#1'> Top</a>\n","metadata":{}},{"cell_type":"markdown","source":">  ### accuracy_group","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"accuracy_group\",data=train_labels, order = train_labels['accuracy_group'].value_counts().index)\nplt.title('Accuracy Group Count Column')\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.974098Z","iopub.status.idle":"2022-01-23T16:19:35.974676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.groupby('accuracy_group')['game_session'].count() \\\n    .plot(kind='barh', figsize=(15, 5), title='Target (accuracy group)')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.976225Z","iopub.status.idle":"2022-01-23T16:19:35.976658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.977617Z","iopub.status.idle":"2022-01-23T16:19:35.978117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Log(Count) of Observations by installation_id**","metadata":{}},{"cell_type":"code","source":"\npalete = sns.color_palette(n_colors=10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.97921Z","iopub.status.idle":"2022-01-23T16:19:35.979624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('installation_id') \\\n    .count()['event_id'] \\\n    .apply(np.log1p) \\\n    .plot(kind='hist',\n          bins=40,\n          color=palete[1],\n         figsize=(15, 5),\n         title='Log(Count) of Observations by installation_id')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.980766Z","iopub.status.idle":"2022-01-23T16:19:35.981309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Count of Observation by Game/Video title","metadata":{}},{"cell_type":"code","source":"train.groupby('title')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='barh',\n          title='Count of Observation by Game/Video title',\n         color=palete[1],\n         figsize=(15, 15))\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.982232Z","iopub.status.idle":"2022-01-23T16:19:35.982889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Count by World","metadata":{}},{"cell_type":"code","source":"train.groupby('world')['event_id'] \\\n    .count() \\\n    .sort_values() \\\n    .plot(kind='bar',\n          figsize=(15, 4),\n          title='Count by World',\n          color=palete[1])\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.983773Z","iopub.status.idle":"2022-01-23T16:19:35.984231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='11'>11. Simple Baseline </a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"See this noteboks [simple lgbm](https://www.kaggle.com/xhlulu/ds-bowl-2019-simple-lgbm-using-aggregated-data),[simple lgbm CV](https://kaggle.com/tanreinama/ds-bowl-2019-simple-lgbm-aggregated-data-with-cv\n).(Simple and Helpful)\n","metadata":{}},{"cell_type":"markdown","source":"*     group1 and group2 are intermediary \"game session\" groups,\n*     which are reduced to one record by game session. group1 takes\n*     the max value of game_time (final game time in a session) and \n*     of event_count (total number of events happened in the session).\n*     group2 takes the total number of event_code of each type","metadata":{}},{"cell_type":"code","source":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.985423Z","iopub.status.idle":"2022-01-23T16:19:35.985915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_small = group_and_reduce(train)\ntest_small = group_and_reduce(test)\n\nprint(train_small.shape)\ntrain_small.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.987123Z","iopub.status.idle":"2022-01-23T16:19:35.987551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='12'>12. Simple LGBM aggregated data with CV </a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"markdown","source":"[Kfold](https://machinelearningmastery.com/k-fold-cross-validation/)\n\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n\n![](https://datavedas.com/wp-content/uploads/2018/04/image001-1.jpg)","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import KFold\nsmall_labels = train_labels[['installation_id', 'accuracy_group']].set_index('installation_id')\ntrain_joined = train_small.join(small_labels).dropna()\nkf = KFold(n_splits=5, random_state=2019)\nX = train_joined.drop(columns='accuracy_group').values\ny = train_joined['accuracy_group'].values.astype(np.int32)\ny_pred = np.zeros((len(test_small), 4))\nfor train, test in kf.split(X):\n    x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n    train_set = lgb.Dataset(x_train, y_train)\n    val_set = lgb.Dataset(x_val, y_val)\n\n    params = {\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.9,\n        'feature_fraction': 0.9,\n        'num_leaves': 50,\n        'lambda_l1': 0.1,\n        'lambda_l2': 1,\n        'metric': 'multiclass',\n        'objective': 'multiclass',\n        'num_classes': 4,\n        'random_state': 2019\n    }\n\n    model = lgb.train(params, train_set, num_boost_round=5000, early_stopping_rounds=50, valid_sets=[train_set, val_set], verbose_eval=50)\n    y_pred += model.predict(test_small)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.988786Z","iopub.status.idle":"2022-01-23T16:19:35.989266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='13'>13. Submission </a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"code","source":"%%time\ny_pred = y_pred.argmax(axis=1)\ntest_small['accuracy_group'] = y_pred\ntest_small[['accuracy_group']].to_csv('submission.csv')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.99027Z","iopub.status.idle":"2022-01-23T16:19:35.990859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='14'>14. Evaluation </a>\n<a href= '#1'> Top</a>\n\n","metadata":{}},{"cell_type":"code","source":"%%time\nval_pred = model.predict(x_val).argmax(axis=1)\nprint(classification_report(y_val, val_pred))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.99223Z","iopub.status.idle":"2022-01-23T16:19:35.992621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id='15'>15. Links for the Game </a>\n<a href= '#1'> Top</a>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"**Mushroom Sorter**","metadata":{}},{"cell_type":"code","source":"\nHTML('<iframe width=\"1106\" height=\"622\" src=\"https://www.youtube.com/embed/1ejHigxuR2Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T16:19:35.993489Z","iopub.status.idle":"2022-01-23T16:19:35.9938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  >  âš¡ Please, you can use parts of this notebook in your own scripts or kernels, no problem, but please give credit (for example link back to this, see this...)","metadata":{}},{"cell_type":"markdown","source":"# <a id='16'>General findings</a>\n<a href='#1'>Top</a>\n","metadata":{}},{"cell_type":"markdown","source":"- **QWK Computation**\n\n    https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-670484\n\n    https://www.kaggle.com/c/data-science-bowl-2019/discussion/114138#latest-667441\n\n    https://www.kaggle.com/c/data-science-bowl-2019/discussion/114135#latest-656785\n\n    https://www.kaggle.com/c/data-science-bowl-2019/discussion/114472#latest-658804\n\n- **Baseline**\n   \n   https://www.kaggle.com/c/data-science-bowl-2019/discussion/114376#latest-659424\n   \n   https://www.kaggle.com/c/data-science-bowl-2019/discussion/114783#latest-671345\n","metadata":{}},{"cell_type":"markdown","source":"# <a id='17'>Top Notebooks</a>\n<a href='#1'>Top</a>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### XGBoost & Feature Selection DSBowl ðŸ¥£ ðŸ¥£ by [@shahules](https://www.kaggle.com/shahules/xgboost-feature-selection-dsbowl)\n* You will learn\n    - Data Preparation and Evaluation\n    - Using XGBoost with StratifiedKFold\n    - Interpreting a ML model with Confidence using [SHAP](https://shap.readthedocs.io/en/latest/)\n\nSouce: https://www.kaggle.com/shahules/xgboost-feature-selection-dsbowl \n### ðŸ“’ðŸ‘¦ðŸ‘§ DS Bowl - Start here: A GENTLE Introduction ðŸ¥£ ðŸ¥£ by [@caesarlupum](https://www.kaggle.com/caesarlupum/ds-bowl-start-here-a-gentle-introduction)\n* You will learn\n    - Where does the data for the competition come from?\n    - Data Description\n    - Exploratory Data Analysis\n    - Simple Baseline\n    - Simple LGBM aggregated data with CV\n\nSouce:  https://www.kaggle.com/caesarlupum/ds-bowl-start-here-a-gentle-introduction\n\n     \n### OOP approach to FE and models by [@artgor ](https://www.kaggle.com/artgor/oop-approach-to-fe-and-models)    \n* You will learn\n    - Data Preparation and learn/use a class for generating features\n    - Training xgb / Training catboost\n\nSource: https://www.kaggle.com/artgor/oop-approach-to-fe-and-models\n\n### Data Science Bowl 2019 EDA and Baseline by [@erikbruin ](https://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline)    \n* You will learn\n    - Understanding the train data\n    - Understanding the test set\n    - Understanding and visualizing the train labels\n    - Feature engineering\n\nSource: https://www.kaggle.com/erikbruin/data-science-bowl-2019-eda-and-baseline\n\n### A new baseline for DSB 2019 - Catboost model by [@mhviraf ](https://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model)    \n* You will learn\n    - Data Preparation \n    - Baseline model with Catboost \n\nSource: https://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model\n\n### ðŸš¸ 2019 Data Science Bowl - An Introduction by [@robikscube ](https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction)    \n* You will learn\n    - Data Preparation\n    - Understanding the target\n    - Data Visualization  \n    - Baseline model \nSource: https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction\n\n### A baseline for DSB 2019 by [@mhviraf ](https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019)    \n* You will learn  \n    - Baseline model \nSource: https://www.kaggle.com/mhviraf/a-baseline-for-dsb-2019\n\n### 2019 Data Science Bowl EDA by [@gpreda ](https://www.kaggle.com/gpreda/2019-data-science-bowl-eda)    \n* You will learn  \n    - Prepare the data analysis\n    -Data exploration\n    -Glimpse the data\n    -Missing data\n    -Unique values\n    -Most frequent values\n    -Values distribution\n    -Extract features from train/event_data\n    -Extract features from specs/args\n    -Merged data distribution\nSource: https://www.kaggle.com/gpreda/2019-data-science-bowl-eda\n\n### Convert to Regression by [@braquino ](https://www.kaggle.com/braquino/convert-to-regression)    \n* You will learn  \n    - Baseline model including a feature selection part\n    - Cohen cappa score of 0.456 (lb) with a local cv score of 0.529\n    - Add/remove features to improve local cv\nSource: https://www.kaggle.com/braquino/convert-to-regression\n\n### Quick and dirty regression by [@artgor ](https://www.kaggle.com/artgor/quick-and-dirty-regression)    \n* You will learn  \n    - Baseline model including a feature selection part\n    - Add/remove features to improve local cv\n    - Finding optimal coefficients for thresholds\nSource: https://www.kaggle.com/artgor/quick-and-dirty-regression\n\n\n","metadata":{}},{"cell_type":"markdown","source":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Blue\">Remember the upvote button is next to the fork button, and it's free too! ;)</font></p>\n<p><font size=\"4\" color=\"Purple\">Don't hesitate to give your suggestions in the comment section</font></p>\n\n</body>\n</html>\n","metadata":{}},{"cell_type":"markdown","source":"## Final","metadata":{}}]}