{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nimport gc\npd.set_option('display.max_columns', 1000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Objective\n* In the last notebook we make a feature selection function which increase our cohen kappa score\n* In this notebook we will create more features and check if they are usefull\n* In this same notebook i will try more and more features to increase the score, stay tunned.\n\nLink for the past notebook is here: https://www.kaggle.com/ragnar123/lgbm-feature-selection\n\nLink for the baseline is here: https://www.kaggle.com/ragnar123/simple-exploratory-data-analysis-and-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature engineering functions\ndef get_time(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n\ndef get_object_columns(df, columns):\n    df = df.groupby(['installation_id', columns])['event_id'].count().reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [columns], values = 'event_id')\n    df.columns = list(df.columns)\n    df.fillna(0, inplace = True)\n    return df\n\ndef get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'std']})\n    df.fillna(0, inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_std']\n    return df\n\ndef get_numeric_columns_2(df, agg_column, column):\n    df = df.groupby(['installation_id', agg_column]).agg({f'{column}': ['mean', 'sum', 'std']}).reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [agg_column], values = [col for col in df.columns if col not in ['installation_id', 'type']])\n    df.fillna(0, inplace = True)\n    df.columns = list(df.columns)\n    return df\n\ndef get_correct_incorrect(df):\n    df = df.groupby(['title'])['num_correct', 'num_incorrect'].agg({'num_correct': ['mean', 'std'], 'num_incorrect': ['mean', 'std']}).reset_index()\n    df.columns = ['title', 'num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std']\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(train, test, train_labels):\n    # columns for feature engineering\n    numerical_columns = ['game_time', 'event_count']\n    categorical_columns = ['type', 'world']\n    numerical_columns_single = ['hour', 'dayofweek', 'month', 'event_id_count', 'event_code_count']\n\n    reduce_train = pd.DataFrame({'installation_id': train['installation_id'].unique()})\n    reduce_train.set_index('installation_id', inplace = True)\n    reduce_test = pd.DataFrame({'installation_id': test['installation_id'].unique()})\n    reduce_test.set_index('installation_id', inplace = True)\n    \n    # get time features\n    train = get_time(train)\n    test = get_time(test)\n    \n    def count_segments(train, test, cols):\n        for col in cols:\n            for df in [train, test]:\n                df[f'{col}_count'] = df.groupby([col])['timestamp'].transform('count')\n        return train, test\n    \n    count_segments(train, test, ['event_id', 'event_code'])\n    \n    \n    \n    for i in numerical_columns:\n        reduce_train = reduce_train.merge(get_numeric_columns(train, i), left_index = True, right_index = True)\n        reduce_test = reduce_test.merge(get_numeric_columns(test, i), left_index = True, right_index = True)\n        \n    for i in categorical_columns:\n        reduce_train = reduce_train.merge(get_object_columns(train, i), left_index = True, right_index = True)\n        reduce_test = reduce_test.merge(get_object_columns(test, i), left_index = True, right_index = True)\n            \n    for i in categorical_columns:\n        for j in numerical_columns:\n            reduce_train = reduce_train.merge(get_numeric_columns_2(train, i, j), left_index = True, right_index = True)\n            reduce_test = reduce_test.merge(get_numeric_columns_2(test, i, j), left_index = True, right_index = True)\n            \n    for i in numerical_columns_single:\n        reduce_train = reduce_train.merge(get_numeric_columns(train, i), left_index = True, right_index = True)\n        reduce_test = reduce_test.merge(get_numeric_columns(test, i), left_index = True, right_index = True)\n            \n    reduce_train.reset_index(inplace = True)\n    reduce_test.reset_index(inplace = True)\n    \n    print('Our training set have {} rows and {} columns'.format(reduce_train.shape[0], reduce_train.shape[1]))\n    \n    # get the mode of the title\n    labels_map = dict(train_labels.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n    # merge target\n    labels = train_labels[['installation_id', 'title', 'accuracy_group']]\n    # merge with correct incorrect\n    corr_inc = get_correct_incorrect(train_labels)\n    labels = labels.merge(corr_inc, how = 'left', on = 'title')\n    # replace title with the mode\n    labels['title'] = labels['title'].map(labels_map)\n    # get title from the test set\n    reduce_test['title'] = test.groupby('installation_id').last()['title'].reset_index(drop = True)\n    # merge with correct incorrect\n    reduce_test = reduce_test.merge(corr_inc, how = 'left', on = 'title')\n    # map title\n    reduce_test['title'] = reduce_test['title'].map(labels_map)\n    # join train with labels\n    reduce_train = labels.merge(reduce_train, on = 'installation_id', how = 'left')\n    print('We have {} training rows'.format(reduce_train.shape[0]))\n    # align datasets\n    categoricals = ['title']\n    reduce_train = reduce_train[[col for col in reduce_test.columns] + ['accuracy_group']]\n    return reduce_train, reduce_test, categoricals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best features extracted from run_feature_selection function\nusefull_features = ['num_correct_mean', 'num_correct_std', 'num_incorrect_mean', 'num_incorrect_std', 'game_time_mean', 'game_time_sum', 'Activity', 'Clip', 'Game', 'CRYSTALCAVES', 'NONE', 'TREETOPCITY', ('game_time', 'mean', 'Clip'), \n                    ('game_time', 'mean', 'Game'), ('game_time', 'std', 'Assessment'), ('game_time', 'std', 'Clip'), ('game_time', 'std', 'Game'), ('game_time', 'sum', 'Activity'), ('game_time', 'sum', 'Clip'), ('game_time', 'sum', 'Game'), \n                    ('game_time', 'mean', 'NONE'), ('game_time', 'mean', 'TREETOPCITY'), ('game_time', 'std', 'CRYSTALCAVES'), ('game_time', 'std', 'MAGMAPEAK'), ('game_time', 'std', 'NONE'), ('game_time', 'std', 'TREETOPCITY'), \n                    ('game_time', 'sum', 'CRYSTALCAVES'), ('game_time', 'sum', 'MAGMAPEAK'), ('game_time', 'sum', 'NONE'), 'title']\nnew_features = ['event_count_mean','event_count_sum', 'event_count_std', ('event_count', 'mean', 'Activity'), ('event_count', 'mean', 'Assessment'), ('event_count', 'mean', 'Clip'), ('event_count', 'mean', 'Game'), ('event_count', 'std', 'Activity'), \n                ('event_count', 'std', 'Assessment'), ('event_count', 'std', 'Clip'), ('event_count', 'std', 'Game'), ('event_count', 'sum', 'Activity'), ('event_count', 'sum', 'Assessment'), ('event_count', 'sum', 'Clip'),\n                ('event_count', 'sum', 'Game'),  ('event_count', 'mean', 'CRYSTALCAVES'), ('event_count', 'mean', 'MAGMAPEAK'), ('event_count', 'mean', 'NONE'), ('event_count', 'mean', 'TREETOPCITY'), ('event_count', 'std', 'CRYSTALCAVES'),\n                ('event_count', 'std', 'MAGMAPEAK'), ('event_count', 'std', 'NONE'), ('event_count', 'std', 'TREETOPCITY'), ('event_count', 'sum', 'CRYSTALCAVES'), ('event_count', 'sum', 'MAGMAPEAK'),\n                ('event_count', 'sum', 'NONE'), ('event_count', 'sum', 'TREETOPCITY'), 'hour_mean', 'hour_sum', 'hour_std', 'dayofweek_mean', 'dayofweek_sum', 'dayofweek_std', 'event_id_count_mean', 'event_id_count_sum', 'event_id_count_std', \n                'event_code_count_mean', 'event_code_count_sum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature selection function that detects if a feature increase cohen kappa score, if it does we add it to the features pool of our model\ndef run_feature_selection(reduce_train, reduce_test, categoricals, usefull_features, new_features):\n    kf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 42)\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, reduce_train[target])):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[usefull_features].iloc[tr_ind], reduce_train[usefull_features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature = categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature = categoricals)\n\n        params = {\n                'learning_rate': 0.01,\n                'metric': 'multi_error',\n                'objective': 'multiclass',\n                'num_classes': 4,\n                'feature_fraction': 0.75,\n                'subsample': 0.75,\n                'n_jobs': -1,\n                'seed': 50,\n                'max_depth': 10\n            }\n\n        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        oof_pred[val_ind] = model.predict(x_val)\n    # using cohen_kappa because it's the evaluation metric of the competition\n    loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n    score = loss_score\n    usefull_new_features = []\n    for i in new_features:\n        print('Our best cohen kappa score is : ', score)\n        oof_pred = np.zeros((len(reduce_train), 4))\n        features = usefull_features + usefull_new_features + [i]\n        print('Evaluating {} column'.format(i))\n        for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, reduce_train[target])):\n            print('Fold {}'.format(fold + 1))\n            x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n            y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature = categoricals)\n            val_set = lgb.Dataset(x_val, y_val, categorical_feature = categoricals)\n\n            model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100,\n                              valid_sets=[train_set, val_set], verbose_eval = 500)\n            oof_pred[val_ind] = model.predict(x_val)\n        loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n        print('Our new kohen cappa score is : ', loss_score)\n        if loss_score > score:\n            print('Feature {} is usefull'.format(i))\n            usefull_new_features.append(i)\n            score = loss_score\n        else:\n            print('Feature {} is useless'.format(i))\n        gc.collect()\n    print('The best features are: ', usefull_features + usefull_new_features)\n\n    return usefull_features + usefull_new_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# preprocess \nreduce_train, reduce_test, categoricals = preprocess(train, test, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if new features are usefull, in that case add them to our feature pool\nusefull_features = run_feature_selection(reduce_train, reduce_test, categoricals, usefull_features, new_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"usefull_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(reduce_train, reduce_test, usefull_features):\n    kf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 42)\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train), 4))\n    y_pred = np.zeros((len(reduce_test), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, reduce_train[target])):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[usefull_features].iloc[tr_ind], reduce_train[usefull_features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n\n        params = {\n            'learning_rate': 0.01,\n            'metric': 'multi_error',\n            'objective': 'multiclass',\n            'num_classes': 4,\n            'feature_fraction': 0.75,\n            'subsample': 0.75,\n            'n_jobs': -1,\n            'seed': 50,\n            'max_depth': 10\n        }\n\n        model = lgb.train(params, train_set, num_boost_round = 1000000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        oof_pred[val_ind] = model.predict(x_val)\n        y_pred += model.predict(reduce_test[usefull_features]) / 10\n    loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n    print('Our oof cohen kappa score is: ', loss_score)\n    return y_pred\n\ndef predict(reduce_test, sample_submission, y_pred):\n    reduce_test = reduce_test.reset_index()\n    reduce_test = reduce_test[['installation_id']]\n    reduce_test['accuracy_group'] = y_pred.argmax(axis = 1)\n    sample_submission.drop('accuracy_group', inplace = True, axis = 1)\n    sample_submission = sample_submission.merge(reduce_test, on = 'installation_id')\n    sample_submission.to_csv('submission.csv', index = False)\n    print(sample_submission['accuracy_group'].value_counts(normalize = True))\ny_pred = run_lgb(reduce_train, reduce_test, usefull_features)\npredict(reduce_test, sample_submission, y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}