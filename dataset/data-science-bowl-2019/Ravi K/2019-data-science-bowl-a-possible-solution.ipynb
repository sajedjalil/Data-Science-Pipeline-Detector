{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2019 Data Science Bowl\n## A Possible Solution\n\n*In this challenge, youâ€™ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. *"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# All the needed libraries\nimport numpy as np\nimport pandas as pd\nimport random\nfrom random import choice\nfrom collections import Counter\n\nimport lightgbm as lgb\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score, cohen_kappa_score, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm_notebook as tqdm\n\nrandom.seed(42)\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis\n### 1. Taking stock of Data:\nLets see what kind of data is presented to us. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls -alh ../input/data-science-bowl-2019/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the above output that we have 5 files in the dataset. Lets read in the above files into respective Pandas dataframes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the data CSV files\ntraining_data = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntesting_data = pd.read_csv('../input/data-science-bowl-2019/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\nsample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Raw Data Characteristics\n\nLets look at the structure of the provided dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train data has {} rows and the following {} columns.\\n{}\\n\" \\\n      .format(training_data.shape[0],training_data.shape[1],training_data.columns.to_list()))\nprint(\"Test data has {} rows and the following {} columns.\\n{}\\n\" \\\n      .format(testing_data.shape[0],testing_data.shape[1],testing_data.columns.to_list()))\nprint(\"The specs data has {} rows and the following {} columns.\\n{}\\n\" \\\n      .format(specs.shape[0], specs.shape[1], specs.columns.to_list()))\nprint(\"The labels data has {} rows and the following {} columns.\\n{}\\n\" \\\n      .format(train_labels.shape[0], train_labels.shape[1],train_labels.columns.to_list()))\nprint(\"The sample submission has {} rows and the following {} columns.\\n{}\\n\" \\\n      .format(sample_submission.shape[0], sample_submission.shape[1],sample_submission.columns.to_list()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_training_data = training_data[training_data['installation_id'] \\\n                                      .isin(train_labels['installation_id'])]\nprint(\"{} ids out of {} in the training set are also in the labels.\" \\\n      .format(reduced_training_data.shape[0],training_data.shape[0]))\nprint(\"There are a total of {} unique ids in the reduced_training_data.\" \\\n      .format(len(reduced_training_data['installation_id'].unique())))\nprint(\"There are a total of {} unique ids in the train_labels.\" \\\n      .format(len(train_labels['installation_id'].unique())))\nprint(\"The number of common ids in the reduced set vs the labels is {}\" \\\n     .format(len(list(set(reduced_training_data['installation_id'].unique()) \\\n                      .intersection(set(train_labels['installation_id'].unique()))))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that there are 3614 unique installation ids in the training data which have their correct target variable in the train_labels dataset. We need to train a model on the training data and then validate against the ground truth provided in the labels.\n\nLets take a quick look at the labels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_labels.shape)\ntrain_labels.head()\nprint(\"There are a total of {} unique game_sessions in the train_labels out of total of {}\" \\\n      .format(len(train_labels['game_session'].unique()), train_labels.shape[0]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that the target variable 'accuracy_group is provided for each unique combination of an installation_id+game_session. It helps to know this when we are compiling data from the training_data. \n\nNow lets compile some data from the training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_data.columns.to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get some additional data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a list with all the unique categoricals from the training_data and testing_data set\nlist_of_all_titles = list(set(training_data['title'].unique()).union(set(testing_data['title'].unique())))\nlist_of_all_event_codes = list(set(training_data['event_code'].unique()).union(set(testing_data['event_code'].unique())))\nlist_of_event_id = list(set(training_data['event_id'].unique()).union(set(testing_data['event_id'].unique())))\nlist_of_worlds = list(set(training_data['world'].unique()).union(set(testing_data['world'].unique())))\n\n# create a dictionary enumerating the titles\n# enumerated_titles = dict(zip(sorted(list_of_all_titles), [\"t_\" + str(x) for x in range(len(list_of_all_titles))]))\nenumerated_titles = dict(zip(sorted(list_of_all_titles), np.arange(len(list_of_all_titles))))\nactivities_labels = {value:key for key, value in enumerated_titles.items()}\nenumerated_worlds = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\nassessment_titles = list(set(training_data[training_data['type'] == 'Assessment']['title'].value_counts().index).union(set(testing_data[testing_data['type'] == 'Assessment']['title'].value_counts().index)))\n\n# Map the replace the text titles with the number titles from the dict\ntraining_data['title'] = training_data['title'].map(enumerated_titles)\ntesting_data['title'] = testing_data['title'].map(enumerated_titles)\ntraining_data['world'] = training_data['world'].map(enumerated_worlds)\ntesting_data['world'] = testing_data['world'].map(enumerated_worlds)\ntrain_labels['title'] = train_labels['title'].map(enumerated_titles)\n\ncorrect_answer_code = dict(zip(enumerated_titles.values(), (4100*np.ones(len(enumerated_titles))).astype('int')))\ncorrect_answer_code[enumerated_titles['Bird Measurer (Assessment)']] = 4110\n\ntraining_data['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), training_data['title'], training_data['event_code']))\ntesting_data['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), testing_data['title'], testing_data['event_code']))\nall_title_event_code = list(set(training_data[\"title_event_code\"].unique()).union(testing_data[\"title_event_code\"].unique()))\n\n# convert text into datetime\ntraining_data['timestamp'] = pd.to_datetime(training_data['timestamp'])\ntesting_data['timestamp'] = pd.to_datetime(testing_data['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_qwk_score(y_true, y_pred):\n    dist = Counter(reduce_train['accuracy_group'])\n    for k in dist:\n        dist[k] /= len(reduce_train)\n    reduce_train['accuracy_group'].hist()\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n\ndef build_features(installation_group, test_set=False):\n    # Constants and parameters declaration\n    last_activity = 0\n    count_of_tasks = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    last_session_time_sec = 0\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    cumulative_true = 0 \n    cumulative_false = 0\n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(installation_group['timestamp'].values[0])\n    durations = []\n    last_accuracy_title = {'acc_' + title: -1 for title in assessment_titles}\n    event_code_count = {x: 0 for x in list_of_all_event_codes}\n    event_id_count = {y: 0 for y in list_of_event_id}\n    title_count = {z: 0 for z in enumerated_titles.values()} \n    title_event_code_count = {w: 0 for w in all_title_event_code}\n\n\n    # iterates through each game_session of one instalation_id\n    for i, game_session in installation_group.groupby('game_session', sort=False):\n        game_session_type = game_session['type'].iloc[0]\n        session_title = game_session['title'].iloc[0]\n        session_title_text = activities_labels[session_title]\n        features = []     \n        # Collect additional information for each 'Assessment' type\n        if (game_session_type == 'Assessment') & (test_set or len(game_session)>1):\n            all_assessment_attempts = game_session.query(f'event_code == {correct_answer_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            result_true = all_assessment_attempts['event_data'].str.contains('true').sum()\n            result_false = all_assessment_attempts['event_data'].str.contains('false').sum()\n\n            # Start building the features dictionary\n            features = count_of_tasks.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_count.copy())\n            features.update(title_event_code_count.copy())\n            features['installation_id'] = game_session['installation_id'].iloc[-1]\n            # features['game_session_title'] = session_title_text\n            features['session_title'] = session_title\n            \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['cumulative_true'] = cumulative_true\n            features['cumulative_false'] = cumulative_false\n            cumulative_true += result_true \n            cumulative_false += result_false\n\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((game_session.iloc[-1, 2] - game_session.iloc[0, 2] ).seconds)\n\n            # the accuracy is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n\n            accuracy = result_true/(result_true+result_false) if (result_true+result_false) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a testing_data set, all sessions belong to the final dataset\n            # it it's a training_data, needs to be passed throught this clausule: game_session.query(f'event_code == {correct_answer_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif result_true+result_false > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n                num_of_session_count = Counter(game_session[col])\n                for k in num_of_session_count.keys():\n                    counter[k] += num_of_session_count[k]\n                return counter\n            \n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(game_session)\n        if last_activity != game_session_type:\n            count_of_tasks[game_session_type] += 1\n            last_activitiy = game_session_type \n\n    \n    # if it is the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[:-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\ndef get_train_and_test(training_data, testing_data):\n    compiled_train = []\n    compiled_test = []\n    for i, (ins_id, installation_group) in tqdm(enumerate(training_data.groupby('installation_id', sort = False)), total = 17000):\n        compiled_train += build_features(installation_group)\n    for ins_id, installation_group in tqdm(testing_data.groupby('installation_id', sort = False), total = 1000):\n        compiled_test += build_features(installation_group, test_set = True)\n\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n\n    # categoricals = ['game_session_title']\n    categoricals = ['session_title']\n    return reduce_train, reduce_test, categoricals\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train, reduce_test, categoricals = get_train_and_test(training_data, testing_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n    n_bins = 10\n    train_data = train[feature]\n    test_data = test[feature]\n    if adjust:\n        test_data *= train_data.mean() / test_data.mean()\n    perc_90 = np.percentile(train_data, 95)\n    train_data = np.clip(train_data, 0, perc_90)\n    test_data = np.clip(test_data, 0, perc_90)\n    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n    msre = mean_squared_error(train_hist, test_hist)\n    if plot:\n        print(msre)\n        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n        plt.show()\n    return msre\n\n# stract_hists('Magma Peak - Level 1_2000', adjust=False, plot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\nLets extract the features and perform any scaling, selection that is applicable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract features from the training data. \nall_labels = reduce_train.columns\nprint(\"There a total of {} columns in the reduced training dataset\".format(len(all_labels)))\n\n# Remove the columns that appear in the sample_submission csv. \nfeatures = [x for x in all_labels if x not in sample_submission.columns]\nprint(\"There a total of {} columns in the feature set\".format(len(features)))\n\n# Remove columns with only '0' in the values\nfeatures = reduce_train.loc[:,(reduce_train.sum(axis=0) != 0)].columns # delete useless columns\nprint(\"{} columns were removed that contained only '0' in the cell. {} remaining\" \\\n      .format(len(reduce_train.columns) - len(features), len(features)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The dtypes in the reduced training dataframe is {}\".format(reduce_train.dtypes.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The features now have {} columns\".format(len(features)))\n\n# counter = 0\nto_remove = []\n# for feat_a in features:\n#     for feat_b in features:\n#         if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n#             c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n#             if c > 0.995:\n#                 counter += 1\n#                 to_remove.append(feat_b)\n#                 print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n\nto_exclude = [] \nadjusted_test = reduce_test.copy()\nfor feature in adjusted_test.columns:\n    #print(feature)\n    if feature not in ['accuracy_group', 'installation_id', 'session_title']:\n        data = reduce_train[feature]\n        train_mean = data.mean()\n        data = adjusted_test[feature] \n        test_mean = data.mean()\n        try:\n            error = stract_hists(feature, adjust=True)\n            ajust_factor = train_mean / test_mean\n            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n                to_exclude.append(feature)\n                print(feature, train_mean, test_mean, error)\n            else:\n                adjusted_test[feature] *= ajust_factor\n        except:\n            to_exclude.append(feature)\n            #print(\"Feature: {}, Train Mean: {}, Test Mean: {}.\".format(feature, train_mean, test_mean))\n\nfeatures = [x for x in features if x not in to_exclude]\nprint(\"The features now have {} columns\".format(len(features)))\nfeatures = [x for x in features if x not in to_remove]\nprint(\"The features now have {} columns\".format(len(features)))\n\nfeatures = [x for x in features if x not in ['accuracy_group', 'installation_id']]\nprint(\"The features now have {} columns\".format(len(features)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Base Class that will be extended for benchmarck and the actual attempted solution. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class Base_Model(object):\n    \n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.features = features\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = 'accuracy_group'\n        self.cv = self.get_cv()\n        self.verbose = verbose\n        self.params = self.get_params()\n        self.y_pred, self.score, self.model = self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        return cv.split(self.train_df, self.train_df[self.target])\n    \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        oof_pred = np.zeros((len(reduce_train), ))\n        y_pred = np.zeros((len(reduce_test), ))\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            conv_x_val = self.convert_x(x_val)\n            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n            x_test = self.convert_x(self.test_df[self.features])\n            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n            print('Partial score of fold {} is: {}'.format(fold, get_qwk_score(y_val, oof_pred[val_idx])[1]))\n        _, loss_score, _ = get_qwk_score(self.train_df[self.target], oof_pred)\n        if self.verbose:\n            print('Our oof cohen kappa score is: ', loss_score)\n        return y_pred, loss_score, model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass LogisticRegression_Model(Base_Model):\n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        lr = LogisticRegression()\n        lr.set_params(**self.params)\n        return lr.fit(train_set, val_set)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = pd.concat([x_train, x_val])\n        val_set = pd.concat([y_train, y_val])\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {  'penalty': 'l2',\n                    'solver': 'lbfgs',\n                    'class_weight': 'balanced',\n                    'random_state': 42,\n                    'max_iter': 200,\n                    'verbose': 100\n                }\n        return params\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = LogisticRegression_Model(reduce_train, reduce_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Lgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'n_estimators':5000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.75,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.9,\n                    'max_depth': 15,\n                    'lambda_l1': 1,  \n                    'lambda_l2': 1,\n                    'early_stopping_rounds': 100\n                    }\n        return params\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model = Lgb_Model(reduce_train, reduce_test, features, categoricals=categoricals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_pred = (lgb_model.y_pred)\nprint(\"The length of final_pred is {}\".format(len(final_pred)))\n\ndist = Counter(reduce_train['accuracy_group'])\nfor k in dist:\n    dist[k] /= len(reduce_train)\nreduce_train['accuracy_group'].hist()\n\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(final_pred, acum * 100)\n# print(bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\n    \nfinal_pred = np.array(list(map(classify, final_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The final_pred is {}\".format(final_pred))\nprint(\"The length of final_pred is {}\".format(len(final_pred)))\nsample_submission['accuracy_group'] = final_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}