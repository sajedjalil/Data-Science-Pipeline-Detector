{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Data Science Bowl 2019**\n**Introduction**\nPBS KIDS, a trusted name in early childhood education for decades, aims to gain insights into how media can help children learn important skills for success in school and life. In this challenge, you’ll use anonymous gameplay data, including knowledge of videos watched and games played, from the PBS KIDS Measure Up! app, a game-based learning tool developed as a part of the CPB-PBS Ready To Learn Initiative with funding from the U.S. Department of Education. Competitors will be challenged to predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Your solutions will aid in discovering important relationships between engagement with high-quality educational media and learning processes.\n\nWhere does the data for the competition come from? The data used in this competition is anonymous, tabular data of interactions with the PBS KIDS Measure Up! app. Select data, such as a user’s in-app assessment score or their path through the game, is collected by the PBS KIDS Measure Up! app, a game-based learning tool.\n\nWhat is the PBS KIDS Measure Up! app? In the PBS KIDS Measure Up! app, children ages 3 to 5 learn early STEM concepts focused on length, width, capacity, and weight while going on an adventure through Treetop City, Magma Peak, and Crystal Caves. Joined by their favorite PBS KIDS characters, children can also collect rewards and unlock digital toys as they play.\n\nBesides the info provided above by Kaggle, I found the following additional info on the website of the app:\n\nSpecific features of Measure Up! include:\n\n19 unique measuring games.\n10 measurement-focused video clips.\nSticker books featuring favorite PBS KIDS characters.\nRewards for completion of tasks.\nEmbedded challenges and reports to help parents and caregivers monitor kids’ progress.\nAbility to track your child's progress using the PBS KIDS Super Vision companion app.\nEvaluation Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\nFor each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.\n\nNote that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\nTable of contents\n1. Understanding the train data\n2. Understanding the test set\n3. Understanding and visualizing the train labels\n4. Feature engineering"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root = \"/kaggle/input/data-science-bowl-2019/\"\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\ntrain = pd.read_csv(root + 'train.csv',usecols=keep_cols)\ntest = pd.read_csv(root + 'test.csv', usecols=keep_cols)\n\ntrain_labels = pd.read_csv(root + 'train_labels.csv')\nspecs = pd.read_csv(root + 'specs.csv')\nsample_submission = pd.read_csv(root + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    \n    # group1 tìm lần chơi cuối cùng của một game_session\n    \n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()  \n\n    # group2 tính tổng các phiên event_code của mỗi installation_id\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Understanding the test set\nFrom Kaggle: For each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_small = group_and_reduce(train)\ntest_small = group_and_reduce(test)\n\nprint(train_small.shape)\ntrain_small.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The date range is more or less the same, so we are talking about a dataset that seems (randomly) split on installation_id. Well actually \"sort of\" as Kaggle seems to have done this on installation_id's with assessments first, and added the \"left-overs\" with no assessments taken to the train set.\n\n3. Understanding and visualizing the train labels\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n3: the assessment was solved on the first attempt\n\n2: the assessment was solved on the second attempt\n\n1: the assessment was solved after 3 or more attempts\n\n0: the assessment was never solved\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n# https://www.kaggle.com/caesarlupum/ds-bowl-start-here-a-gentle-introduction\nsmall_labels = train_labels[['installation_id', 'accuracy_group']].set_index('installation_id')\ntrain_joined = train_small.join(small_labels).dropna()\nkf = KFold(n_splits=5, random_state=2019)\nX = train_joined.drop(columns='accuracy_group').values\ny = train_joined['accuracy_group'].values.astype(np.int32)\ny_pred = np.zeros((len(test_small), 4))\nfor train, test in kf.split(X):\n    x_train, x_val, y_train, y_val = X[train], X[test], y[train], y[test]\n    train_set = lgb.Dataset(x_train, y_train)\n    val_set = lgb.Dataset(x_val, y_val)\n\n    params = {\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.9,\n        'feature_fraction': 0.9,\n        'num_leaves': 50,\n        'lambda_l1': 0.1,\n        'lambda_l2': 1,\n        'metric': 'multiclass',\n        'objective': 'multiclass',\n        'num_classes': 4,\n        'random_state': 2019\n    }\n\n    model = lgb.train(params, train_set, num_boost_round=5000, early_stopping_rounds=50, valid_sets=[train_set, val_set], verbose_eval=50)\n    y_pred += model.predict(test_small)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y2 = model.predict(test_small)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y2.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_small['accuracy_group'] = y2.argmax(axis=1)\ntest_small[['accuracy_group']].to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}