{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\nIn this kernel I work with data from 2019 Data Science Bowl Challenge.\n\nThis is quite an interesting data about children playing educational games. We need to predict how will a user perform in the next assessment.\n\nI have decided to try a different approach to feature engineering and modelling in this kernel - most of the code will be written in classes for better usability and reproducibility. These are my first attempts to do it on Kaggle, so the code may evolve into something different in the future.\n\n\n![](https://i.imgur.com/ysD3pe0.png)"},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport copy\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom collections import defaultdict\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\nfrom category_encoders.ordinal import OrdinalEncoder\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom typing import List\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom typing import Any\nfrom itertools import product\npd.set_option('max_rows', 500)\nimport re\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions and classes"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def add_datepart(df: pd.DataFrame, field_name: str,\n                 prefix: str = None, drop: bool = True, time: bool = True, date: bool = True):\n    \"\"\"\n    Helper function that adds columns relevant to a date in the column `field_name` of `df`.\n    from fastai: https://github.com/fastai/fastai/blob/master/fastai/tabular/transform.py#L55\n    \"\"\"\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Is_month_end', 'Is_month_start']\n    if date:\n        attr.append('Date')\n    if time:\n        attr = attr + ['Hour', 'Minute']\n    for n in attr:\n        df[prefix + n] = getattr(field.dt, n.lower())\n    if drop:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ndef ifnone(a: Any, b: Any) -> Any:\n    \"\"\"`a` if `a` is not None, otherwise `b`.\n    from fastai: https://github.com/fastai/fastai/blob/master/fastai/core.py#L92\"\"\"\n    return b if a is None else a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_xgb(y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for xgb.\n    \"\"\"\n    # print('y_true', y_true)\n    # print('y_pred', y_pred)\n    y_true = y_true.get_label()\n    y_pred = y_pred.argmax(axis=1)\n    return 'cappa', -qwk(y_true, y_pred)\n\n\nclass LGBWrapper(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\n\n\nclass CatWrapper(object):\n    \"\"\"\n    A wrapper for catboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = cat.CatBoostClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**{k: v for k, v in params.items() if k != 'cat_cols'})\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = None\n        else:\n            categorical_columns = None\n        \n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       cat_features=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if 'MultiClass' not in self.model.get_param('loss_function'):\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)\n\n\nclass XGBWrapper(object):\n    \"\"\"\n    A wrapper for xgboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = xgb.XGBClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_metric=eval_qwk_xgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n\n        scores = self.model.evals_result()\n        self.best_score_ = {k: {m: m_v[-1] for m, m_v in v.items()} for k, v in scores.items()}\n        self.best_score_ = {k: {m: n if m != 'cappa' else -n for m, n in v.items()} for k, v in self.best_score_.items()}\n\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)\n\n\n\n\nclass MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n\n        data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n        data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n        data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n        data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n        self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n                         or 'attempt' in col]\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class ClassifierModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='auc',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 4# 1 if len(set(y.values)) == 2 else len(set(y.values))\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n            # y = X['accuracy_group']\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n            if X_holdout is not None:\n                X_hold = X_holdout.copy()\n            else:\n                X_hold = None\n            self.folds_dict[fold_n] = {}\n            if params['verbose']:\n                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n            self.folds_dict[fold_n] = {}\n\n            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            if self.train_one_fold:\n                X_train = X[self.original_columns]\n                y_train = y\n                X_valid = None\n                y_valid = None\n\n            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n            model = copy.deepcopy(self.model_wrapper)\n\n            if adversarial:\n                X_new1 = X_train.copy()\n                if X_valid is not None:\n                    X_new2 = X_valid.copy()\n                elif X_holdout is not None:\n                    X_new2 = X_holdout.copy()\n                X_new = pd.concat([X_new1, X_new2], axis=0)\n                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n            self.folds_dict[fold_n]['scores'] = model.best_score_\n            if self.oof.shape[0] != len(X):\n                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n            if not adversarial:\n                self.oof[valid_index] = model.predict_proba(X_valid).reshape(-1, n_target)\n\n            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                           columns=['feature', 'importance'])\n            self.feature_importances = self.feature_importances.append(fold_importance)\n            self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(float)\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n            print(classification_report(y, self.oof.argmax(1)))\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=25)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            g = sns.heatmap(confusion_matrix(y, self.oof.argmax(1)), annot=True, cmap=plt.cm.Blues,fmt=\"d\")\n            g.set(ylim=(-0.5, 4), xlim=(-0.5, 4), title='Confusion matrix')\n\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof.argmax(1))\n            plt.xticks(range(self.n_target), range(self.n_target))\n            plt.title('Distribution of oof predictions');\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n            self.cols_to_drop = cols_to_drop\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n            X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict_proba(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        full_evals_results[self.eval_metric] = np.abs(full_evals_results[self.eval_metric])\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class CategoricalTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, cat_cols=None, drop_original: bool = False, encoder=OrdinalEncoder()):\n        \"\"\"\n        Categorical transformer. This is a wrapper for categorical encoders.\n\n        :param cat_cols:\n        :param drop_original:\n        :param encoder:\n        \"\"\"\n        self.cat_cols = cat_cols\n        self.drop_original = drop_original\n        self.encoder = encoder\n        self.default_encoder = OrdinalEncoder()\n\n    def fit(self, X, y=None):\n\n        if self.cat_cols is None:\n            kinds = np.array([dt.kind for dt in X.dtypes])\n            is_cat = kinds == 'O'\n            self.cat_cols = list(X.columns[is_cat])\n        self.encoder.set_params(cols=self.cat_cols)\n        self.default_encoder.set_params(cols=self.cat_cols)\n\n        self.encoder.fit(X[self.cat_cols], y)\n        self.default_encoder.fit(X[self.cat_cols], y)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        new_cat_names = [f'{col}_encoded' for col in self.cat_cols]\n        encoded_data = self.encoder.transform(data[self.cat_cols])\n        if encoded_data.shape[1] == len(self.cat_cols):\n            data[new_cat_names] = encoded_data\n        else:\n            pass\n\n        if self.drop_original:\n            data = data.drop(self.cat_cols, axis=1)\n        else:\n            data[self.cat_cols] = self.default_encoder.transform(data[self.cat_cols])\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview\n\nLet's have a look at the data at first."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '/kaggle/input/data-science-bowl-2019'\nspecs = pd.read_csv(f'{path}/specs.csv')\nsample_submission = pd.read_csv(f'{path}/sample_submission.csv')\ntrain_labels = pd.read_csv(f'{path}/train_labels.csv')\ntest = pd.read_csv(f'{path}/test.csv')\ntrain = pd.read_csv(f'{path}/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Rows in train data: {train.shape[0]}')\nprint(f'Rows in train labels: {train_labels.shape[0]}')\nprint(f'Rows in specs data: {specs.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the train dataset has more than 11 millions of rows, but there are only ~18k rows with labels. Now we need to find the correct way of combining data with labels.\n\nLet's read the description of the data, particulary at the following:\n\n```\nFor each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.\n\nNote that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment.\n\nThe file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n\n```\n\nWhat can we infer from this information?\n- We predict one value/class per installment - for the last assessment;\n- Not all installment in train dataset are present in labels;\n- Only events with certain event codes have labels;\n- In train data we can have several labels for one installment;"},{"metadata":{},"cell_type":"markdown","source":"## The main idea of data transformation\n\nI have read most of public kernels and think that a different approach for data transformation is necessary.\nWe don't simply predict a label for some assessment of a user: we predict a label for a **future** assessment. This means that when we make a prediction for the test data, we have only data about user activity **up to** the timestamp of the assessment. And, of course, we don't have any information about how user performs in the assessment (we hame only one row - the start of the assessment).\n\nAs a result, if we want to create a good training dataset without leaks, we need to loop over `train_labels` and generate features only on the rows, which are before start of the particular assessment."},{"metadata":{},"cell_type":"markdown","source":"### Preparing the data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rows with attempts\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntrain['attempt'] = 0\ntrain.loc[(train['title'] == 'Bird Measurer (Assessment)') & (train['event_code'] == 4110),\n       'attempt'] = 1\ntrain.loc[(train['type'] == 'Assessment') &\n       (train['title'] != 'Bird Measurer (Assessment)')\n       & (train['event_code'] == 4100), 'attempt'] = 1\n\ntitle_oe = OrdinalEncoder()\ntitle_oe.fit(list(set(train['title'].unique()).union(set(test['title'].unique()))));\nworld_oe = OrdinalEncoder()\nworld_oe.fit(list(set(train['world'].unique()).union(set(test['world'].unique()))));\n\ntrain['correct'] = None\ntrain.loc[(train['attempt'] == 1) & (train['event_data'].str.contains('\"correct\":true')), 'correct'] = True\ntrain.loc[(train['attempt'] == 1) & (train['event_data'].str.contains('\"correct\":false')), 'correct'] = False\n\ntrain['title'] = title_oe.transform(train['title'].values)\ntrain['world'] = world_oe.transform(train['world'].values)\ntrain = train.loc[train['installation_id'].isin(train_labels['installation_id'].unique())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class for generating features\n\nIn the hidden cell below you can find a class for feature generation."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class FeatureGenerator(object):\n    def __init__(self, n_jobs=1, df=None, dataset: str = 'train'):\n        self.n_jobs = n_jobs\n        self.df = df\n        self.dataset = dataset\n\n    def read_chunks(self):\n        for id, user_sample in self.df.groupby('installation_id', sort=False):\n            yield id, user_sample\n\n    def get_features(self, row):\n        \"\"\"\n        Gets three groups of features: from original data and from reald and imaginary parts of FFT.\n        \"\"\"\n        return self.features(row)\n\n    def features(self, id, user_sample):\n        user_data = []\n\n        accuracy_mapping = {0: 0, 1: 3, 0.5: 2}\n\n        user_stats = defaultdict(int)\n        user_stats['installation_id'] = user_sample['installation_id'].unique()[0]\n        user_stats['world'] = user_sample['world'].unique()[0]\n        user_stats['timestamp'] = user_sample['timestamp'].unique()[0]\n\n        temp_dict = defaultdict(int)\n        another_temp_dict = {}\n        another_temp_dict['durations'] = []\n        another_temp_dict['all_durations'] = []\n        another_temp_dict['durations_with_attempts'] = []\n        another_temp_dict['mean_action_time'] = []\n        title_data = defaultdict(dict)\n\n        for i, session in user_sample.groupby('game_session', sort=False):\n            user_stats['last_ass_session_game_time'] = another_temp_dict['durations'][-1] if len(another_temp_dict['durations']) > 0 else 0\n            user_stats['last_session_game_time'] = another_temp_dict['all_durations'][-1] if len(another_temp_dict['all_durations']) > 0 else 0\n\n            # calculate some user_stats and append data\n            if session['attempt'].sum() > 0 or self.dataset == 'test':\n                user_stats['session_title'] = session['title'].values[0]\n                accuracy = np.nan_to_num(session['correct'].sum() / session['attempt'].sum())\n                if accuracy in accuracy_mapping.keys():\n                    user_stats['accuracy_group'] = accuracy_mapping[accuracy]\n                else:\n                    user_stats['accuracy_group'] = 1\n                user_stats['accumulated_accuracy_group'] = temp_dict['accumulated_accuracy_group'] / user_stats['counter'] if user_stats['counter'] > 0 else 0\n                temp_dict['accumulated_accuracy_group'] += user_stats['accuracy_group']\n                user_data.append(copy.copy(user_stats))\n\n            user_stats[session['type'].values[-1]] += 1\n            user_stats['accumulated_correct_attempts'] += session['correct'].sum()\n            user_stats['accumulated_uncorrect_attempts'] += session['attempt'].sum() - session['correct'].sum()\n            event_code_counts = session['event_code'].value_counts()\n            for i, j in zip(event_code_counts.index, event_code_counts.values):\n                user_stats[i] += j\n\n            temp_dict['assessment_counter'] += 1\n            if session['title'].values[-1] in title_data.keys():\n                pass\n            else:\n                title_data[session['title'].values[-1]] = defaultdict(int)\n\n            title_data[session['title'].values[-1]]['duration_all'] += session['game_time'].values[-1]\n            title_data[session['title'].values[-1]]['counter_all'] += 1\n            #user_stats['duration'] += (session['timestamp'].values[-1] - session['timestamp'].values[0]) / np.timedelta64(1, 's')\n\n            user_stats['duration'] = (session.iloc[-1,2] - session.iloc[0,2]).seconds\n            if session['type'].values[0] == 'Assessment' and (len(session) > 1 or self.dataset == 'test'):\n                another_temp_dict['durations'].append(user_stats['duration'])\n                accuracy = np.nan_to_num(session['correct'].sum() / session['attempt'].sum())\n                user_stats['accumulated_accuracy_'] += accuracy\n                user_stats['counter'] += 1\n                if user_stats['counter'] == 0:\n                    user_stats['accumulated_accuracy'] = 0\n                else:\n                    user_stats['accumulated_accuracy'] = user_stats['accumulated_accuracy_'] / user_stats['counter']\n\n                accuracy = np.nan_to_num(session['correct'].sum() / session['attempt'].sum())\n\n                if accuracy in accuracy_mapping.keys():\n                    user_stats[accuracy_mapping[accuracy]] += 1\n                else:\n                    user_stats[1] += 1\n\n                user_stats['accumulated_actions'] += len(session)\n\n                if session['attempt'].sum() > 0:\n                    user_stats['sessions_with_attempts'] += 1\n                    another_temp_dict['durations_with_attempts'].append(user_stats['duration'])\n\n                if session['correct'].sum() > 0:\n                    user_stats['sessions_with_correct_attempts'] += 1\n                    \n                user_stats['title_duration'] = title_data[session['title'].values[-1]]['duration']\n                user_stats['title_counter'] = title_data[session['title'].values[-1]]['counter']\n                user_stats['title_mean_duration'] = user_stats['title_duration'] / user_stats['title_mean_duration']  if user_stats['title_mean_duration'] > 0 else 0\n\n                user_stats['title_duration_all'] = title_data[session['title'].values[-1]]['duration_all']\n                user_stats['title_counter_all'] = title_data[session['title'].values[-1]]['counter_all']\n                user_stats['title_mean_duration_all'] = user_stats['title_duration_all'] / user_stats['title_mean_duration_all']  if user_stats['title_mean_duration_all'] > 0 else 0\n                \n                title_data[session['title'].values[-1]]['duration'] += session['game_time'].values[-1]\n                title_data[session['title'].values[-1]]['counter'] += 1\n\n            elif (len(session) > 1 or self.dataset == 'test'):\n                another_temp_dict['all_durations'].append(user_stats['duration'])\n\n\n            if user_stats['duration'] != 0:\n                temp_dict['nonzero_duration_assessment_counter'] += 1\n            #user_stats['duration_mean'] = user_stats['duration'] / max(temp_dict['nonzero_duration_assessment_counter'], 1)\n            # stats from assessment sessions\n            user_stats['duration_mean'] = np.mean(another_temp_dict['durations'])\n            user_stats['duration_attempts'] = np.mean(another_temp_dict['durations_with_attempts'])\n\n            # stats from all sessions\n            user_stats['all_duration_mean'] = np.mean(another_temp_dict['all_durations'])\n            user_stats['all_accumulated_actions'] += len(session)\n            user_stats['mean_action_time'] = np.mean(another_temp_dict['mean_action_time'])\n            another_temp_dict['mean_action_time'].append(session['game_time'].values[-1] / len(session))\n\n\n        if self.dataset == 'test':\n            user_data = [user_data[-1]]\n\n        return user_data\n\n    def generate(self):\n        feature_list = []\n#         res = Parallel(n_jobs=self.n_jobs, backend='threading')(delayed(self.features)(id, user_sample)\n#                                                                 for id, user_sample in tqdm(self.read_chunks(),\n#                                                                                             total=self.df[\n#                                                                                                 'installation_id'].nunique()))\n        res = Parallel(n_jobs=self.n_jobs, backend='threading')(delayed(self.features)(id, user_sample)\n                                                                for id, user_sample in self.read_chunks())\n        for r in res:\n            for r1 in r:\n                feature_list.append(r1)\n        return pd.DataFrame(feature_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfg = FeatureGenerator(n_jobs=2, df=train)\ndata = fg.generate()\ndata = data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = add_datepart(data, 'timestamp', drop=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting constant columns\nbad_cols = []\nfor col in data:\n    if data[col].nunique() <= 1:\n        print(col)\n        bad_cols.append(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\nHere I'll also use classes for training the data. The code works like this:\n- MainTransformer does some general processing on all the data - for example, processing cyclical features, creating feature interactions and so on;\n- transformers do some processing on folds - for example mean encoding. In the current dataset it generates aggregated features on folds.\n- LGBWrapper is a wrapper over LGBM. This wrapper is necessary, so that all models have a similar API and it is easier to use them;\n- ClassifierModel is the main class for modelling: it trains, applies and saves transformers, it trains and saves the models, plots the results and so on. It is also quite convenient to use in kaggle kernels, because we can train it on the train data (without loading test data) and then we can delete all the train data and apply ClassifierModel to test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'n_estimators':2000,\n         'learning_rate': 0.01,\n         'metric': 'multiclass',\n         'objective': 'multiclass',\n         'max_depth': 15,\n         'num_classes': 4,\n         'feature_fraction': 0.85,\n         'subsample': 0.85,\n         'verbose': 1000,\n         'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\n#folds = StratifiedKFold(n_splits=n_fold)\n#folds = KFold(n_splits=n_fold)\nfolds = RepeatedStratifiedKFold(n_splits=n_fold)\nfolds = GroupKFold(n_splits=n_fold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group', 'timestampDate'] + bad_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = [str(col) for col in data.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['world']\nmt = MainTransformer(create_interactions=False)\nct = CategoricalTransformer(drop_original=True, cat_cols=cat_cols)\nft = FeatureTransformer()\ntransformers = {'ft': ft, 'ct': ct}\nlgb_model = ClassifierModel(model_wrapper=LGBWrapper())\nlgb_model.fit(X=data, y=y, folds=folds, params=param, preprocesser=mt, transformers=transformers,\n                    eval_metric='cappa', cols_to_drop=cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training xgb\n\nI want to note that custom xgb metric is always minimized, so I had to return negative cappa score."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n        'colsample_bytree': 0.8,\n        'learning_rate': 0.05,\n        'max_depth': 7,\n        'subsample': 1,\n        'objective':'multi:softprob',\n        'num_class':4,\n        'eval_metric':'merror',\n        'min_child_weight':10,\n        'gamma':0.25,\n        'n_estimators':500,\n        'nthread': 6,\n        'verbose': 1000,\n        'early_stopping_rounds': 100,\n    }\nfolds = GroupKFold(n_splits=5)\ncat_cols = ['world']\nmt = MainTransformer(create_interactions=False)\nct = CategoricalTransformer(drop_original=True, cat_cols=cat_cols)\nft = FeatureTransformer()\ntransformers = {'ft': ft, 'ct': ct}\nxgb_model = ClassifierModel(model_wrapper=XGBWrapper())\nxgb_model.fit(X=data, y=y, folds=folds, params=xgb_params, preprocesser=mt, transformers=transformers,\n              eval_metric='cappa', cols_to_drop=cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_params = {'loss_function': 'MultiClass',\n          'eval_metric':\"WKappa\",\n          'task_type': \"CPU\",\n          'iterations': 2000,\n          'learning_rate': 0.01,\n          'verbose': 500,\n          'early_stopping_rounds': 100,\n          'cat_cols': ['world_encoded', 'session_title'],\n          'od_type': 'Iter',\n          'early_stopping_rounds': 100\n    }\nfolds = GroupKFold(n_splits=5)\ncat_cols = ['world']\nmt = MainTransformer(create_interactions=False)\nct = CategoricalTransformer(drop_original=True, cat_cols=cat_cols)\nft = FeatureTransformer()\ntransformers = {'ft': ft, 'ct': ct}\ncat_model = ClassifierModel(model_wrapper=CatWrapper())\ncat_model.fit(X=data, y=y, folds=folds, params=cat_params, preprocesser=mt, transformers=transformers,\n              eval_metric='WKappa', cols_to_drop=cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"del data\ndel y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\n# rows with attempts\ntest['attempt'] = 0\ntest.loc[(test['title'] == 'Bird Measurer (Assessment)') & (test['event_code'] == 4110),\n       'attempt'] = 1\ntest.loc[(test['type'] == 'Assessment') &\n       (test['title'] != 'Bird Measurer (Assessment)')\n       & (test['event_code'] == 4100), 'attempt'] = 1\n\ntest['correct'] = None\ntest.loc[(test['attempt'] == 1) & (test['event_data'].str.contains('\"correct\":true')), 'correct'] = True\ntest.loc[(test['attempt'] == 1) & (test['event_data'].str.contains('\"correct\":false')), 'correct'] = False\n\ntest['title'] = title_oe.transform(test['title'].values)\ntest['world'] = world_oe.transform(test['world'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfg = FeatureGenerator(n_jobs=2, df=test, dataset='test')\ntest_data = fg.generate()\ntest_data.columns = [str(col) for col in test_data.columns]\ntest_data = test_data.fillna(0)\ntest_data = add_datepart(test_data, 'timestamp', drop=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = cat_model.predict(test_data) + lgb_model.predict(test_data) + xgb_model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'] = prediction.argmax(1)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}