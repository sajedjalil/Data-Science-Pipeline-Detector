{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is totally based on [Catboost - Some more features](https://www.kaggle.com/braquino/catboost-some-more-features) kernel.\n\nThe main purpose of this kernel is to blend catboost and lightgbm."},{"metadata":{},"cell_type":"markdown","source":"### next steps\n    1. lgbm: use qwk as evaluation metric"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# this function is the quadratic weighted kappa (the metric used for the competition submission)\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    # Calculate the percent each class was tagged each label\n    O = confusion_matrix(act,pred)\n    # normalize to sum 1\n    O = np.divide(O,np.sum(O))\n    \n    # create a new matrix of zeroes that match the size of the confusion matrix\n    # this matriz looks as a weight matrix that give more weight to the corrects\n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            # makes a weird matrix that is bigger in the corners top-right and botton-left (= 1)\n            W[i][j] = ((i-j)**2)/((n-1)**2)\n            \n    # make two histograms of the categories real X prediction\n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    # multiply the two histograms using outer product\n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E)) # normalize to sum 1\n    \n    # apply the weights to the confusion matrix\n    num = np.sum(np.multiply(W,O))\n    # apply the weights to the histograms\n    den = np.sum(np.multiply(W,E))\n    \n    return 1-np.divide(num,den)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\nnrows = 100000\nnrows = None\ntrain = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', nrows=nrows)\ntrain_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv', nrows=nrows)\nspecs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv', nrows=nrows)\ntest = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv', nrows=nrows)\nsubmission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of rows: {}'.format(train.shape[0]))\nkeep_id = train[train.type == \"Assessment\"][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\nprint('Number of rows (after filtering): {}'.format(train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LabelEncode activities and event codes\n# What about OneHotEncoding\n\n# encode title\n# make a list with all the unique 'titles' from the train and test set\nlist_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\n# make a list with all the unique 'event_code' from the train and test set\nlist_of_event_code = list(set(train['event_code'].value_counts().index).union(set(test['event_code'].value_counts().index)))\n# create a dictionary numerating the titles\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\nactivities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n\n# replace the text titles withing the number titles from the dict\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I didnt undestud why, but this one makes a dict where the value of each element is 4100 \nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n# then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert text into datetime\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here the get_data function is applyed to each installation_id and added to the compile_data list\ncompiled_data = []\n# tqdm is the library that draws the status bar below\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False))):\n    # user_sample is a DataFrame that contains only one installation_id\n    compiled_data += get_data(user_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the compiled_data is converted to DataFrame and deleted to save memmory\nnew_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are the features I have generated. Note that all of them are **prior** to each event. For example, the first row shows **before** this assessment, the player have watched 3 clips, did 3 activities, played 4 games and solved 0 assessments, so on so forth."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\nnew_train[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this list comprehension create the list of features that will be used on the input dataset X\n# all but accuracy_group, that is the label y\nall_features = [x for x in new_train.columns if x not in ['accuracy_group']]\n# this cat_feature must be declared to pass later as parameter to fit the model\ncat_features = ['session_title']\n# here the dataset select the features and split the input ant the labels\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\n\nfrom typing import Tuple, Union\n\ndef lgb_classification_qwk(y_true: Union[np.ndarray, list],\n                           y_pred: Union[np.ndarray, list],) -> Tuple[str, float, bool]:\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return \"qwk\", qwk(y_true, y_pred), True\n\n\ndef qwk(y_true: Union[np.ndarray, list],\n        y_pred: Union[np.ndarray, list],\n        max_rat: int = 3) -> float:\n    y_true_ = np.asarray(y_true)\n    y_pred_ = np.asarray(y_pred)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    uniq_class = np.unique(y_true_)\n    for i in uniq_class:\n        hist1[int(i)] = len(np.argwhere(y_true_ == i))\n        hist2[int(i)] = len(np.argwhere(y_pred_ == i))\n\n    numerator = np.square(y_true_ - y_pred_).sum()\n\n    denominator = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            denominator += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    denominator /= y_true_.shape[0]\n    return 1 - numerator / denominator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function makes the model and sets the parameters\n# for configure others parameter consult the documentation below:\n# https://catboost.ai/docs/concepts/python-reference_catboostclassifier.html\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\ndef make_classifier_1(iterations=6000):\n    clf = CatBoostClassifier(\n        loss_function='MultiClass',\n        eval_metric=\"WKappa\",\n        task_type=\"CPU\",\n        #learning_rate=0.01,\n        iterations=iterations,\n        od_type=\"Iter\",\n        #depth=4,\n        early_stopping_rounds=500,\n        #l2_leaf_reg=10,\n        #border_count=96,\n        random_seed=45,\n        #use_best_model=use_best_model,\n        verbose=0\n    )\n        \n    return clf\n\ndef make_classifier_2():\n    params = {\n            'learning_rate': 0.005,\n            'metric': 'multiclass',\n            'objective': 'multiclass',\n            'num_classes': 4,\n            'num_iterations': 5000,\n            # 'feature_fraction': 0.75,\n            'early_stopping_rounds': 20,\n            # 'subsample': 0.75,\n            'n_jobs': -1,\n            'seed': 64,\n            'verbose': 0\n        }\n    clf = LGBMClassifier(**params)\n    return clf\n\ndef make_classifier_3():\n    params = {\n        'colsample_bytree': 0.8,                 \n        'learning_rate': 0.08,\n        'max_depth': 10,\n        'subsample': 1,\n        'objective':'multi:softprob',\n        'eval_metric':'mlogloss',\n        'min_child_weight':3,\n        'gamma':0.25,\n        'n_estimators':500\n    }\n    clf = XGBClassifier(**params)\n    return clf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# CV\nfrom sklearn.model_selection import KFold, StratifiedKFold\n# oof is an zeroed array of the same size of the input dataset\n\nNFOLDS = 7\n# here the KFold class is used to split the dataset in 5 diferents training and validation sets\n# this technique is used to assure that the model isn't overfitting and can performs aswell in \n# unseen data. More the number of splits/folds, less the test will be impacted by randomness\n\ntraining_start_time = time()\npredictions_tr = []\nmodels = []\nfor i in range(2):\n    folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=345 * i)\n    oof = np.zeros(len(X))\n    for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n        # each iteration of folds.split returns an array of indexes of the new training data and validation data\n        start_time = time()\n        print(f'Training on fold {fold+1}')\n        # creates the model\n        for clf_clb in (\n            # make_classifier_3,\n            make_classifier_2,\n            make_classifier_1,\n        ):\n            # fits the model using .loc at the full dataset to select the splits indexes and features used\n            clf = clf_clb()\n            args = (X.loc[trn_idx, all_features], y.loc[trn_idx])\n            kwargs = {\n                'verbose': 0,\n                'eval_set': (X.loc[test_idx, all_features], y.loc[test_idx]),\n                \n            }\n            if clf_clb.__name__.endswith('1'):\n                kw = kwargs.copy()\n                kw.update({\n                    'use_best_model': True,\n                    'cat_features': cat_features,\n                    \n                })\n                clf.fit(*args, **kw)\n            elif clf_clb.__name__.endswith('3'):\n                xgb_train = xgb.DMatrix(X.loc[trn_idx, all_features], y.loc[trn_idx])\n                xgb_eval = xgb.DMatrix(X.loc[test_idx, all_features], y.loc[test_idx])\n                val_X=xgb.DMatrix(X.loc[test_idx, all_features])\n                params = {\n                    'learning_rate': 0.08,\n                    'max_depth': 10,\n                    'subsample': 1,\n                    'objective':'multi:softprob',\n                    'eval_metric':'mlogloss',\n                    'min_child_weight':3,\n                    'gamma':0.25,\n                    'n_estimators':500\n                }\n                xgb_model = xgb.train(params,\n                      xgb_train,\n                      evals=[(xgb_train, 'train'), (xgb_eval, 'val')],\n                      early_stopping_rounds=20,\n                      **params\n                     )\n\n            else:\n                kw = kwargs.copy()\n                clf.fit(*args, **kw)\n\n            # then, the predictions of each split is inserted into the oof array\n            # pr = clf.predict(X.loc[test_idx, all_features]).reshape(len(test_idx))\n            pr = clf.predict(X.loc[test_idx, all_features]).reshape(len(test_idx))\n            if clf_clb.__name__.endswith('3'):\n                pr = np.array(pr).argmax(axis=1)\n            oof[test_idx] = pr[:]\n            models.append(clf)\n    \n    print('-' * 30)\n    # and here, the complete oof is tested against the real data using que metric (quadratic weighted kappa)\n    print('OOF QWK:', i, qwk(y, oof))\n    print('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save predictions (don't ask why)\npredictions_tr = []\nfor model in tqdm(models):\n    pr = model.predict(X)\n    if len(pr.shape) == 1:\n        pr = np.array([[_] for _ in pr])\n    predictions_tr.append(pr)\n\npredictions_tr = np.array(predictions_tr).reshape(X.shape[0], len(models))\n\nnp.savetxt(\"predictions_tr.csv\", predictions_tr, delimiter=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model on all data once\n#clf = make_classifier()\n#clf.fit(X, y, verbose=500, cat_features=cat_features)\n\ndel X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# process test set, the same that was done with the train set\nnew_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'accuracy_group' in X_test.columns:\n    X_test.drop(['accuracy_group'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions on test set once\npredictions = []\nfor model in models:\n    pr = model.predict(X_test)\n    if isinstance(pr[0], (int, float, np.int64)):\n        pr = [[_] for _ in pr]\n    elif 'xgboost' in str(model).lower():\n        pr = list(np.array(pr).argmax(axis=1))\n    predictions.append(pr)\npredictions = np.concatenate(predictions, axis=1)\n\nnp.savetxt(\"predictions_ts.csv\", predictions, delimiter=\",\")\n\nprint(predictions.shape)\npredictions = stats.mode(predictions, axis=1)[0].reshape(-1)\nprint(predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['accuracy_group'] = np.round(predictions).astype('int')\nsubmission.to_csv('submission.csv', index=None)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels['accuracy_group'].plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(oof).plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}