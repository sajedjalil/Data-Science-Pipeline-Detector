{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.getdigitalinfluence.com/wp-content/uploads/2016/12/Boosting-vs-Ads-Manager-vs-Power-Editor-776x415.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What is Boosting?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Boosting (originally called hypothesis boosting) refers to any Ensemble method that\n> can combine several weak learners into a strong learner. The general idea of most\n> boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are AdaBoost(short for Adaptive Boosting) and Gradient Boosting. We will talk about both here, but after reading in the data and pre-processing them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/694/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### First, Importing the required libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport xgboost\nimport gc\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier \n\nimport lightgbm as lgb\nfrom numba import jit \n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/data-science-bowl-2019/train.csv')\ntest_df = pd.read_csv('../input/data-science-bowl-2019/test.csv')\ntrain_labels_df = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\nspecs_df = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\nsample_submission_df = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specs_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### All preprocessing functions below are from [this](https://www.kaggle.com/gpreda/data-science-bowl-fast-compact-solution) wonderful kernel by Gabriel.\n\n\n\nOur focus here is on the different boosting models and see what baseline quadratic kappa scores they give.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    df['dayofyear'] = df['timestamp'].dt.dayofyear\n    df['quarter'] = df['timestamp'].dt.quarter\n    df['is_month_start'] = df['timestamp'].dt.is_month_start    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_object_columns(df, columns):\n    df = df.groupby(['installation_id', columns])['event_id'].count().reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [columns], values = 'event_id')\n    df.columns = list(df.columns)\n    df.fillna(0, inplace = True)\n    return df\n\ndef get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'min', 'max', 'std', 'skew']})\n    df[column].fillna(df[column].mean(), inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_min', f'{column}_max', f'{column}_std', f'{column}_skew']\n    return df\n\ndef get_numeric_columns_add(df, agg_column, column):\n    df = df.groupby(['installation_id', agg_column]).agg({f'{column}': ['mean', 'sum', 'min', 'max', 'std', 'skew']}).reset_index()\n    df = df.pivot_table(index = 'installation_id', columns = [agg_column], values = [col for col in df.columns if col not in ['installation_id', 'type']])\n    df[column].fillna(df[column].mean(), inplace = True)\n    df.columns = list(df.columns)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def perform_features_engineering(train_df, test_df, train_labels_df):\n    print(f'Perform features engineering')\n    numerical_columns = ['game_time']\n    categorical_columns = ['type', 'world']\n\n    comp_train_df = pd.DataFrame({'installation_id': train_df['installation_id'].unique()})\n    comp_train_df.set_index('installation_id', inplace = True)\n    comp_test_df = pd.DataFrame({'installation_id': test_df['installation_id'].unique()})\n    comp_test_df.set_index('installation_id', inplace = True)\n\n    test_df = extract_time_features(test_df)\n    train_df = extract_time_features(train_df)\n\n    for i in numerical_columns:\n        comp_train_df = comp_train_df.merge(get_numeric_columns(train_df, i), left_index = True, right_index = True)\n        comp_test_df = comp_test_df.merge(get_numeric_columns(test_df, i), left_index = True, right_index = True)\n    \n    for i in categorical_columns:\n        comp_train_df = comp_train_df.merge(get_object_columns(train_df, i), left_index = True, right_index = True)\n        comp_test_df = comp_test_df.merge(get_object_columns(test_df, i), left_index = True, right_index = True)\n    \n    for i in categorical_columns:\n        for j in numerical_columns:\n            comp_train_df = comp_train_df.merge(get_numeric_columns_add(train_df, i, j), left_index = True, right_index = True)\n            comp_test_df = comp_test_df.merge(get_numeric_columns_add(test_df, i, j), left_index = True, right_index = True)\n    \n    \n    comp_train_df.reset_index(inplace = True)\n    comp_test_df.reset_index(inplace = True)\n       \n    labels_map = dict(train_labels_df.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n \n    labels = train_labels_df[['installation_id', 'title', 'accuracy_group']]\n    \n    labels['title'] = labels['title'].map(labels_map)\n   \n    comp_test_df['title'] = test_df.groupby('installation_id').last()['title'].map(labels_map).reset_index(drop = True)\n   \n    comp_train_df = labels.merge(comp_train_df, on = 'installation_id', how = 'left')\n    print('We have {} training rows'.format(comp_train_df.shape[0]))\n    \n    return comp_train_df, comp_test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n    e = e / a1.shape[0]\n    return 1 - o / e","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adaboost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> One way for a new predictor to correct its predecessor is to pay a bit more attention\n> to the training instances that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. This is the technique used by Ada‐Boost. \nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\nTree) is trained and used to make predictions on the training set. The relative weight\nof misclassified training instances is then increased. A second classifier is trained\nusing the updated weights and again it makes predictions on the training set, weights\nare updated, and so on ...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://www.github.com/rakash/images1/blob/master/adaboost.jpg?raw=true)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Let us see how decision boundaries are drawn for all the models for adaboost.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://www.github.com/rakash/images1/blob/master/adaboost_db.jpg?raw=true)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> The first classifier(notified by the line) gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, and\n> so on. The plot on the right represents the same sequence of predictors except that the learning rate is halved (i.e., the misclassified instance weights are boosted half as\n> much at every iteration). As you can see, this sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor’s\n> parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,gradually making it better.\n> \n> Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_train_df, ada_test_df = perform_features_engineering(train_df, test_df, train_labels_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns = ada_test_df.columns[ada_test_df.isnull().any()]\nada_test_df[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_test_df['game_time_std'] = ada_test_df['game_time_std'].fillna(0)\nada_test_df['game_time_skew'] = ada_test_df['game_time_skew'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Below is the model function. As you can see, for ada boost we will be using a simple decision tree as the base estimator.\n\n\n#### Like Random Forest, AdaBoost makes predictions by applying multiple decision trees to every sample and combining the predictions made by individual trees. However, rather than taking the average of the predictions made by each decision tree in the forest (or majority in the case of classification), in the AdaBoost algorithm, every decision tree contributes a varying amount to the final prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def adaboost_it(ada_train_df, ada_test_df):\n    print(\"Ada-Boosting...\")\n    t_splits = 5\n    k_scores = []\n    kf = KFold(n_splits = t_splits)\n    features = [i for i in ada_train_df.columns if i not in ['accuracy_group', 'installation_id']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(ada_train_df), 4))\n    y_pred = np.zeros((len(ada_test_df), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(ada_train_df)):\n        print(f'Fold: {fold+1}')\n        x_train, x_val = ada_train_df[features].iloc[tr_ind], ada_train_df[features].iloc[val_ind]\n        y_train, y_val = ada_train_df[target][tr_ind], ada_train_df[target][val_ind]\n               \n        ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,algorithm=\"SAMME.R\", learning_rate=0.5)\n        ada_clf.fit(x_train, y_train)\n        oof_pred[val_ind] = ada_clf.predict_proba(x_val)\n      \n        y_pred += ada_clf.predict_proba(ada_test_df[features]) / t_splits\n        \n        val_crt_fold = qwk3(y_val, oof_pred[val_ind].argmax(axis = 1))\n        print(f'Fold: {fold+1} quadratic weighted kappa score: {np.round(val_crt_fold,4)}')\n        \n    res = qwk3(ada_train_df['accuracy_group'], oof_pred.argmax(axis = 1))\n    print(f'Quadratic weighted score: {np.round(res,4)}')\n        \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = adaboost_it(ada_train_df, ada_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_test_df = ada_test_df.reset_index()\nada_test_df = ada_test_df[['installation_id']]\nada_test_df['accuracy_group'] = y_pred.argmax(axis = 1)\nada_sample_submission_df = sample_submission_df.merge(ada_test_df, on = 'installation_id')\nada_sample_submission_df.to_csv('ada_boost_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/583/1*FLshv-wVDfu-i54OqvZdHg.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Another very popular Boosting algorithm is Gradient Boosting. Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n> \n> \n> Think of XGBoost as gradient boosting on ‘steroids’ (well it is called ‘Extreme Gradient Boosting’ for a reason!). It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_train_df, xgb_test_df = perform_features_engineering(train_df, test_df, train_labels_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [i for i in xgb_train_df.columns if i not in ['accuracy_group', 'installation_id']]\ntarget = 'accuracy_group'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train  = xgb_train_df[features]\ny_train = xgb_train_df[target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Grid search is very time consuming and therefore i have commented it for now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\n#model = xgboost.XGBClassifier()\n\n#param_dist = {\"max_depth\": [10,30,50],\"min_child_weight\" : [1,3,6],\n #             \"n_estimators\": [200],\n  #            \"learning_rate\": [0.05, 0.1,0.16],}\n\n#grid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, verbose=10, n_jobs=-1)\n#grid_search.fit(x_train, y_train)\n#grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb(xgb_train_df, xgb_test_df):\n    print(\"XG-Boosting...\")\n    t_splits = 5\n    k_scores = []\n    kf = KFold(n_splits = t_splits)\n    features = [i for i in xgb_train_df.columns if i not in ['accuracy_group', 'installation_id']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(xgb_train_df), 4))\n    y_pred = np.zeros((len(xgb_test_df), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(xgb_train_df)):\n        print(f'Fold: {fold+1}')\n        x_train, x_val = xgb_train_df[features].iloc[tr_ind], xgb_train_df[features].iloc[val_ind]\n        y_train, y_val = xgb_train_df[target][tr_ind], xgb_train_df[target][val_ind]\n        \n        xgb_clf = xgboost.XGBClassifier()\n        xgb_clf.fit(x_train, y_train)\n        oof_pred[val_ind] = xgb_clf.predict_proba(x_val)\n      \n        y_pred += xgb_clf.predict_proba(xgb_test_df[features]) / t_splits\n        \n        val_crt_fold = qwk3(y_val, oof_pred[val_ind].argmax(axis = 1))\n        print(f'Fold: {fold+1} quadratic weighted kappa score: {np.round(val_crt_fold,4)}')\n        \n    res = qwk3(xgb_train_df['accuracy_group'], oof_pred.argmax(axis = 1))\n    print(f'Quadratic weighted score: {np.round(res,4)}')\n        \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = xgb(xgb_train_df, xgb_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_test_df = xgb_test_df.reset_index()\nxgb_test_df = xgb_test_df[['installation_id']]\nxgb_test_df['accuracy_group'] = y_pred.argmax(axis = 1)\nxgb_sample_submission_df = sample_submission_df.merge(xgb_test_df, on = 'installation_id')\nxgb_sample_submission_df.to_csv('xgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_sample_submission_df = xgb_sample_submission_df.drop('accuracy_group_x', axis=1)\nxgb_sample_submission_df.columns = ['installation_id', 'accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_sample_submission_df.to_csv('xgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Catboost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Catboost yields state-of-the-art results without extensive data training typically required by other machine learning methods, and it Provides powerful out-of-the-box support for the more descriptive data formats that accompany many business problems.\n\n> Major advantage is it handles categorical variables automatically, that is why the name 'CAT-boost'\n\n\nYou can know more about it [here](https://www.youtube.com/watch?time_continue=2&v=s8Q_orF4tcI)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_train_df, cat_test_df = perform_features_engineering(train_df, test_df, train_labels_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xc_train  = cat_train_df[features]\nyc_train = cat_train_df[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\ndef cat(cat_train_df, cat_test_df):\n    print(\"Meeowwww...\")\n    t_splits = 3\n    k_scores = []\n    kf = KFold(n_splits = t_splits)\n    features = [i for i in cat_train_df.columns if i not in ['accuracy_group', 'installation_id']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(cat_train_df), 4))\n    y_pred = np.zeros((len(cat_test_df), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(cat_train_df)):\n        print(f'Fold: {fold+1}')\n        x_train, x_val = cat_train_df[features].iloc[tr_ind], cat_train_df[features].iloc[val_ind]\n        y_train, y_val = cat_train_df[target][tr_ind], cat_train_df[target][val_ind]\n        \n        cat_clf = cb.CatBoostClassifier(depth=10, iterations= 200, l2_leaf_reg= 9, learning_rate= 0.15)\n        cat_clf.fit(xc_train, yc_train)\n        oof_pred[val_ind] = cat_clf.predict_proba(x_val)\n      \n        y_pred += cat_clf.predict_proba(cat_test_df[features]) / t_splits\n        \n        val_crt_fold = qwk3(y_val, oof_pred[val_ind].argmax(axis = 1))\n        print(f'Fold: {fold+1} quadratic weighted kappa score: {np.round(val_crt_fold,4)}')\n        \n    res = qwk3(cat_train_df['accuracy_group'], oof_pred.argmax(axis = 1))\n    print(f'Quadratic weighted score: {np.round(res,4)}')\n        \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_cat = cat(cat_train_df, cat_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_test_df = cat_test_df.reset_index()\ncat_test_df = cat_test_df[['installation_id']]\ncat_test_df['accuracy_group'] = y_pred_cat.argmax(axis = 1)\ncat_sample_submission_df = sample_submission_df.merge(cat_test_df, on = 'installation_id')\ncat_sample_submission_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_sample_submission_df = cat_sample_submission_df.drop('accuracy_group_x', axis=1)\ncat_sample_submission_df.columns = ['installation_id', 'accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_sample_submission_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> It is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word ‘Light’. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### THIS IS HOW IT WORKS IN XGBOOST","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/11194110/leaf.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### HOW IT WORKS IN LIGHTGBM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/11194227/depth.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train_df, lgb_test_df = perform_features_engineering(train_df, test_df, train_labels_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xl_train  = lgb_train_df[features]\nyl_train = lgb_train_df[target]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\ndef lgbc(lgb_train_df, lgb_test_df):\n    print(\"Meeowwww...\")\n    t_splits = 3\n    k_scores = []\n    kf = KFold(n_splits = t_splits)\n    features = [i for i in lgb_train_df.columns if i not in ['accuracy_group', 'installation_id']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(lgb_train_df), 4))\n    y_pred = np.zeros((len(lgb_test_df), 4))\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(lgb_train_df)):\n        print(f'Fold: {fold+1}')\n        x_train, x_val = lgb_train_df[features].iloc[tr_ind], lgb_train_df[features].iloc[val_ind]\n        y_train, y_val = lgb_train_df[target][tr_ind], lgb_train_df[target][val_ind]\n        \n        lg = lgb.LGBMClassifier(silent=False)\n        lg.fit(xl_train, yl_train)\n        oof_pred[val_ind] = lg.predict_proba(x_val)\n      \n        y_pred += lg.predict_proba(lgb_test_df[features]) / t_splits\n        \n        val_crt_fold = qwk3(y_val, oof_pred[val_ind].argmax(axis = 1))\n        print(f'Fold: {fold+1} quadratic weighted kappa score: {np.round(val_crt_fold,4)}')\n        \n    res = qwk3(lgb_train_df['accuracy_group'], oof_pred.argmax(axis = 1))\n    print(f'Quadratic weighted score: {np.round(res,4)}')\n        \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lgb = lgbc(lgb_train_df, lgb_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_test_df = lgb_test_df.reset_index()\nlgb_test_df = lgb_test_df[['installation_id']]\nlgb_test_df['accuracy_group'] = y_pred_lgb.argmax(axis = 1)\nlgb_sample_submission_df = sample_submission_df.merge(lgb_test_df, on = 'installation_id')\nlgb_sample_submission_df.to_csv('lgb_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_sample_submission_df = lgb_sample_submission_df.drop('accuracy_group_x', axis=1)\nlgb_sample_submission_df.columns = ['installation_id', 'accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [['ada', 0.42], ['xgb', 0.44], ['cat', 0.65], ['lgb', 0.62]]\n\ndf = pd.DataFrame(data, columns = ['Model', 'Validation Kappa Score']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=df['Model'], y=df['Validation Kappa Score'], marker_color='#FFD700'))\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}