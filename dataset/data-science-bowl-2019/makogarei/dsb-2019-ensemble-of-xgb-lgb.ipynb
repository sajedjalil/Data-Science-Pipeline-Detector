{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#result\n#ver1:LGBのみ、lgb cv mean QWK score : 0.5208321783509067、LB：0.536(+0.015)\n#ver2:0.8LGB+0.2XGB\n#lgb cv mean QWK score : 0.5076390431140777、xgb cv mean QWK score : 0.49945107302527625\n#LB：0.534\n#ver4:LGB+XGB(LidgeでEnsemble)\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport lightgbm as lgb\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport gc\nimport seaborn as sns\nimport scipy as sp\nimport multiprocessing\nimport scipy as sp\nimport time\nimport random\nimport json\n\nfrom multiprocessing import Lock, Process, Queue, current_process\nfrom math import sqrt\nfrom numba import jit\nfrom functools import partial\nfrom tqdm import tqdm as tqdm\nfrom collections import Counter\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom catboost import CatBoostRegressor\nnp.random.seed(724)\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 1000)\npd.set_option('max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MAKE_DATA\n* Wei_version11_552(https://www.kaggle.com/snakayama/dsb19-lgb-xgb?scriptVersionId=26804302)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data():\n    start = time.time()\n    print(\"Start read data\")\n\n    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n    #train = pd.read_csv('../input/data-science-bowl-2019/train.csv', nrows=1200000)\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n    #test = pd.read_csv('../input/data-science-bowl-2019/test.csv', nrows=30000)\n\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n\n    print(\"read data done, time - \", time.time() - start)\n    return train, test, train_labels, specs, sample_submission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_title(train, test, train_labels):\n    start = time.time()\n\n    print(\"Start encoding data\")\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n\n    train['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), train['type'], train['world']))\n    test['type_world'] = list(map(lambda x, y: str(x) + '_' + str(y), test['type'], test['world']))\n    all_type_world = list(set(train[\"type_world\"].unique()).union(test[\"type_world\"].unique()))\n\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(\n        set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100 * np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    print(\"End encoding data, time - \", time.time() - start)\n\n\n    event_data = {}\n    event_data[\"train_labels\"] = train_labels\n    event_data[\"win_code\"] = win_code\n    event_data[\"list_of_user_activities\"] = list_of_user_activities\n    event_data[\"list_of_event_code\"] = list_of_event_code\n    event_data[\"activities_labels\"] = activities_labels\n    event_data[\"assess_titles\"] = assess_titles\n    event_data[\"list_of_event_id\"] = list_of_event_id\n    event_data[\"all_title_event_code\"] = all_title_event_code\n    event_data[\"activities_map\"] = activities_map\n    event_data[\"all_type_world\"] = all_type_world\n\n    return train, test, event_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_all_features(feature_dict, ac_data):\n    if len(ac_data['durations']) > 0:\n        feature_dict['installation_duration_mean'] = np.mean(ac_data['durations'])\n        feature_dict['installation_duration_sum'] = np.sum(ac_data['durations'])\n    else:\n        feature_dict['installation_duration_mean'] = 0\n        feature_dict['installation_duration_sum'] = 0\n\n    return feature_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(user_sample, event_data, test_set):\n    '''\n    The user_sample is a DataFrame from train or test where the only one\n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_assesment = {}\n\n    last_activity = 0\n\n    user_activities_count = {'Clip': 0, 'Activity': 0, 'Assessment': 0, 'Game': 0}\n\n    assess_4020_acc_dict = {'Cauldron Filler (Assessment)_4020_accuracy': 0,\n                            'Mushroom Sorter (Assessment)_4020_accuracy': 0,\n                            'Bird Measurer (Assessment)_4020_accuracy': 0,\n                            'Chest Sorter (Assessment)_4020_accuracy': 0}\n\n    game_time_dict = {'Clip_gametime': 0, 'Game_gametime': 0,\n                      'Activity_gametime': 0, 'Assessment_gametime': 0}\n\n    last_session_time_sec = 0\n    accuracy_groups = {0: 0, 1: 0, 2: 0, 3: 0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy = 0\n    accumulated_correct_attempts = 0\n    accumulated_uncorrect_attempts = 0\n    accumulated_actions = 0\n\n    # Newly added features\n    accumulated_game_miss = 0\n    Cauldron_Filler_4025 = 0\n    mean_game_round = 0\n    mean_game_duration = 0\n    mean_game_level = 0\n    Assessment_mean_event_count = 0\n    Game_mean_event_count = 0\n    Activity_mean_event_count = 0\n    chest_assessment_uncorrect_sum = 0\n\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    durations_game = []\n    durations_activity = []\n    last_accuracy_title = {'acc_' + title: -1 for title in event_data[\"assess_titles\"]}\n    last_game_time_title = {'lgt_' + title: 0 for title in event_data[\"assess_titles\"]}\n    ac_game_time_title = {'agt_' + title: 0 for title in event_data[\"assess_titles\"]}\n    ac_true_attempts_title = {'ata_' + title: 0 for title in event_data[\"assess_titles\"]}\n    ac_false_attempts_title = {'afa_' + title: 0 for title in event_data[\"assess_titles\"]}\n    event_code_count: dict[str, int] = {ev: 0 for ev in event_data[\"list_of_event_code\"]}\n    event_code_proc_count = {str(ev) + \"_proc\" : 0. for ev in event_data[\"list_of_event_code\"]}\n    event_id_count: dict[str, int] = {eve: 0 for eve in event_data[\"list_of_event_id\"]}\n    title_count: dict[str, int] = {eve: 0 for eve in event_data[\"activities_labels\"].values()}\n    title_event_code_count: dict[str, int] = {t_eve: 0 for t_eve in event_data[\"all_title_event_code\"]}\n    type_world_count: dict[str, int] = {w_eve: 0 for w_eve in event_data[\"all_type_world\"]}\n    session_count = 0\n\n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = event_data[\"activities_labels\"][session_title]\n\n        if session_type == \"Activity\":\n            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n\n        if session_type == \"Game\":\n            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n\n            game_s = session[session.event_code == 2030]\n            misses_cnt = cnt_miss(game_s)\n            accumulated_game_miss += misses_cnt\n\n            try:\n                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n                mean_game_round = (mean_game_round + game_round) / 2.0\n            except:\n                pass\n\n            try:\n                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n                mean_game_duration = (mean_game_duration + game_duration) / 2.0\n            except:\n                pass\n\n            try:\n                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n                mean_game_level = (mean_game_level + game_level) / 2.0\n            except:\n                pass\n\n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session) > 1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {event_data[\"win_code\"][session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens:\n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(last_accuracy_title.copy())\n            features.update(event_code_count.copy())\n            features.update(title_count.copy())\n            features.update(game_time_dict.copy())\n            features.update(event_id_count.copy())\n            features.update(title_event_code_count.copy())\n            features.update(assess_4020_acc_dict.copy())\n            features.update(type_world_count.copy())\n            features.update(last_game_time_title.copy())\n            features.update(ac_game_time_title.copy())\n            features.update(ac_true_attempts_title.copy())\n            features.update(ac_false_attempts_title.copy())\n\n            features.update(event_code_proc_count.copy())\n            features['installation_session_count'] = session_count\n            features['accumulated_game_miss'] = accumulated_game_miss\n            features['mean_game_round'] = mean_game_round\n            features['mean_game_duration'] = mean_game_duration\n            features['mean_game_level'] = mean_game_level\n            features['Assessment_mean_event_count'] = Assessment_mean_event_count\n            features['Game_mean_event_count'] = Game_mean_event_count\n            features['Activity_mean_event_count'] = Activity_mean_event_count\n            features['chest_assessment_uncorrect_sum'] = chest_assessment_uncorrect_sum\n\n            variety_features = [('var_event_code', event_code_count),\n                                ('var_event_id', event_id_count),\n                                ('var_title', title_count),\n                                ('var_title_event_code', title_event_code_count),\n                                ('var_type_world', type_world_count)]\n\n            for name, dict_counts in variety_features:\n                arr = np.array(list(dict_counts.values()))\n                features[name] = np.count_nonzero(arr)\n\n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0]\n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts\n            accumulated_uncorrect_attempts += false_attempts\n\n            # ----------------------------------------------\n            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n\n            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n            # ----------------------------------------------\n\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_std'] = 0\n                features['last_duration'] = 0\n                features['duration_max'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_std'] = np.std(durations)\n                features['last_duration'] = durations[-1]\n                features['duration_max'] = np.max(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n\n            if durations_game == []:\n                features['duration_game_mean'] = 0\n                features['duration_game_std'] = 0\n                features['game_last_duration'] = 0\n                features['game_max_duration'] = 0\n            else:\n                features['duration_game_mean'] = np.mean(durations_game)\n                features['duration_game_std'] = np.std(durations_game)\n                features['game_last_duration'] = durations_game[-1]\n                features['game_max_duration'] = np.max(durations_game)\n\n            if durations_activity == []:\n                features['duration_activity_mean'] = 0\n                features['duration_activity_std'] = 0\n                features['game_activity_duration'] = 0\n                features['game_activity_max'] = 0\n            else:\n                features['duration_activity_mean'] = np.mean(durations_activity)\n                features['duration_activity_std'] = np.std(durations_activity)\n                features['game_activity_duration'] = durations_activity[-1]\n                features['game_activity_max'] = np.max(durations_activity)\n\n            # the accuracy is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy / counter if counter > 0 else 0\n            # --------------------------\n            features['Cauldron_Filler_4025'] = Cauldron_Filler_4025 / counter if counter > 0 else 0\n\n            Assess_4025 = session[(session.event_code == 4025) & (session.title == 'Cauldron Filler (Assessment)')]\n            true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n            false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n\n            cau_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (\n                                                                                                      true_attempts_ + false_attempts_) != 0 else 0\n            Cauldron_Filler_4025 += cau_assess_accuracy_\n\n            chest_assessment_uncorrect_sum += len(session[session.event_id == \"df4fe8b6\"])\n\n            Assessment_mean_event_count = (Assessment_mean_event_count + session['event_count'].iloc[-1]) / 2.0\n            # ----------------------------\n            accuracy = true_attempts / (true_attempts + false_attempts) if (true_attempts + false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group / counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n\n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                last_assesment = features.copy()\n\n            if true_attempts + false_attempts > 0:\n                all_assessments.append(features)\n\n            counter += 1\n\n        if session_type == 'Game':\n            durations_game.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n\n        if session_type == 'Activity':\n            durations_activity.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n\n        session_count += 1\n\n        # this piece counts how many actions was made in each event_code so far\n        def update_counters(counter: dict, col: str):\n            num_of_session_count = Counter(session[col])\n            for k in num_of_session_count.keys():\n                x = k\n                if col == 'title':\n                    x = event_data[\"activities_labels\"][k]\n                counter[x] += num_of_session_count[k]\n            return counter\n\n        def update_proc(count: dict):\n            res = {}\n            for k, val in count.items():\n                res[str(k) + \"_proc\"] = (float(val) * 100.0) / accumulated_actions\n            return res\n\n        event_code_count = update_counters(event_code_count, \"event_code\")\n\n\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_count = update_counters(title_count, 'title')\n        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n        type_world_count = update_counters(type_world_count, 'type_world')\n\n        assess_4020_acc_dict = get_4020_acc(session, assess_4020_acc_dict, event_data)\n        game_time_dict[session_type + '_gametime'] = (game_time_dict[session_type + '_gametime'] + (\n                    session['game_time'].iloc[-1] / 1000.0)) / 2.0\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        event_code_proc_count = update_proc(event_code_count)\n\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n\n            # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return last_assesment, all_assessments\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments\n\ndef cnt_miss(df):\n    cnt = 0\n    for e in range(len(df)):\n        x = df['event_data'].iloc[e]\n        y = json.loads(x)['misses']\n        cnt += y\n    return cnt\n\ndef get_4020_acc(df, counter_dict, event_data):\n    for e in ['Cauldron Filler (Assessment)', 'Bird Measurer (Assessment)',\n              'Mushroom Sorter (Assessment)', 'Chest Sorter (Assessment)']:\n        Assess_4020 = df[(df.event_code == 4020) & (df.title == event_data[\"activities_map\"][e])]\n        true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n        false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n\n        measure_assess_accuracy_ = true_attempts_ / (true_attempts_ + false_attempts_) if (\n                                                                                                      true_attempts_ + false_attempts_) != 0 else 0\n        counter_dict[e + \"_4020_accuracy\"] += (counter_dict[e + \"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n\n    return counter_dict\n\ndef get_users_data(users_list, return_dict,  event_data, test_set):\n    if test_set:\n        for user in users_list:\n            return_dict.append(get_data(user, event_data, test_set))\n    else:\n        answer = []\n        for user in users_list:\n            answer += get_data(user, event_data, test_set)\n        return_dict += answer\n\ndef get_data_parrallel(users_list, event_data, test_set):\n    manager = multiprocessing.Manager()\n    return_dict = manager.list()\n    threads_number = event_data[\"process_numbers\"]\n    data_len = len(users_list)\n    processes = []\n    cur_start = 0\n    cur_stop = 0\n    for index in range(threads_number):\n        cur_stop += (data_len-1) // threads_number\n\n        if index != (threads_number - 1):\n            p = Process(target=get_users_data, args=(users_list[cur_start:cur_stop], return_dict, event_data, test_set))\n        else:\n            p = Process(target=get_users_data, args=(users_list[cur_start:], return_dict, event_data, test_set))\n\n        processes.append(p)\n        cur_start = cur_stop\n\n    for proc in processes:\n        proc.start()\n\n    for proc in processes:\n        proc.join()\n\n    return list(return_dict)\n\ndef get_train_and_test(train, test, event_data):\n    start = time.time()\n    print(\"Start get_train_and_test\")\n\n    compiled_train = []\n    compiled_test = []\n\n    user_train_list = []\n    user_test_list = []\n\n    stride_size = event_data[\"strides\"]\n    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=17000):\n        user_train_list.append(user_sample)\n        if (i + 1) % stride_size == 0:\n            compiled_train += get_data_parrallel(user_train_list, event_data, False)\n            del user_train_list\n            user_train_list = []\n\n    if len(user_train_list) > 0:\n        compiled_train += get_data_parrallel(user_train_list, event_data, False)\n        del user_train_list\n\n    for i, (ins_id, user_sample) in tqdm(enumerate(test.groupby('installation_id', sort=False)), total=1000):\n        user_test_list.append(user_sample)\n        if (i + 1) % stride_size == 0:\n            compiled_test += get_data_parrallel(user_test_list, event_data, True)\n            del user_test_list\n            user_test_list = []\n\n    if len(user_test_list) > 0:\n        compiled_test += get_data_parrallel(user_test_list, event_data, True)\n        del user_test_list\n\n    reduce_train = pd.DataFrame(compiled_train)\n\n    reduce_test = [x[0] for x in compiled_test]\n\n    reduce_train_from_test = []\n    for i in [x[1] for x in compiled_test]:\n        reduce_train_from_test += i\n\n    reduce_test = pd.DataFrame(reduce_test)\n    reduce_train_from_test = pd.DataFrame(reduce_train_from_test)\n    print(\"End get_train_and_test, time - \", time.time() - start)\n    return reduce_train, reduce_test, reduce_train_from_test\n\ndef get_train_and_test_single_proc(train, test, event_data):\n    compiled_train = []\n    compiled_test = []\n    compiled_test_his = []\n    for ins_id, user_sample in tqdm(train.groupby('installation_id', sort=False), total=17000):\n        compiled_train += get_data(user_sample, event_data, False)\n    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n        test_data = get_data(user_sample, event_data, True)\n        compiled_test.append(test_data[0])\n        compiled_test_his += test_data[1]\n\n\n    reduce_train = pd.DataFrame(compiled_train)\n    reduce_test = pd.DataFrame(compiled_test)\n    reduce_test_his = pd.DataFrame(compiled_test_his)\n\n    return reduce_train, reduce_test, reduce_test_his","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_kaggle = True\nrandom.seed(42)\nstart_program = time.time()\n\nevent_data = {}\nif in_kaggle:\n    event_data[\"strides\"] = 300\n    event_data[\"process_numbers\"] = 4\nelse:\n    event_data[\"strides\"] = 300\n    event_data[\"process_numbers\"] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, event_data_update = encode_title(train, test, train_labels)\nevent_data.update(event_data_update)\n\n#reduce_train, reduce_test, reduce_train_from_test = get_train_and_test_single_proc(train, test, event_data)\nreduce_train, reduce_test, reduce_train_from_test = get_train_and_test(train, test, event_data)\ndels = [train, test]\ndel dels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.sort_values(\"installation_id\", axis=0, ascending=True, inplace=True, na_position='last')\nreduce_test.sort_values(\"installation_id\", axis=0, ascending=True, inplace=True, na_position='last')\nreduce_train = pd.concat([reduce_train, reduce_train_from_test], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURES_SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n    n_bins = 10\n    train_data = train[feature]\n    test_data = test[feature]\n    if adjust:\n        test_data *= train_data.mean() / test_data.mean()\n    perc_90 = np.percentile(train_data, 95)\n    train_data = np.clip(train_data, 0, perc_90)\n    test_data = np.clip(test_data, 0, perc_90)\n    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n    msre = mean_squared_error(train_hist, test_hist)\n    if plot:\n        print(msre)\n        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n        plt.show()\n    return msre\nstract_hists('Magma Peak - Level 1_2000', adjust=False, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call feature engineering function\nfeatures = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\nfeatures = [x for x in features if x not in ['accuracy_group', 'installation_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"counter = 0\nto_remove = []\nfor feat_a in features:\n    for feat_b in features:\n        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n            c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n            if c > 0.995:\n                counter += 1\n                to_remove.append(feat_b)\n                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_exclude = [] \najusted_test = reduce_test.copy()\nfor feature in tqdm(ajusted_test.columns):\n    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title']:\n        data = reduce_train[feature]\n        train_mean = data.mean()\n        data = ajusted_test[feature] \n        test_mean = data.mean()\n        try:\n            error = stract_hists(feature, adjust=True)\n            ajust_factor = train_mean / test_mean\n            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n                to_exclude.append(feature)\n#                 print(feature, train_mean, test_mean, error)\n            else:\n                ajusted_test[feature] *= ajust_factor\n        except:\n            to_exclude.append(feature)\n#             print(feature, train_mean, test_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [x for x in features if x not in (to_exclude + to_remove)]\nreduce_train[features].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\najusted_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in ajusted_test.columns]\nfeatures = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define_metric\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1,))\n    hist2 = np.zeros((max_rat + 1,))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o += (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n\n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [1.10, 1.72, 2.25]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead', options={\n            'maxiter': 5000})\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n\n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def soft_kappa_obj(y, p):\n    y = np.asarray(y)\n    p = np.asarray(p.label)\n    norm = p.dot(p) + y.dot(y)\n\n    grad = -2 * y / norm + 4 * p * np.dot(y, p) / (norm ** 2)\n    hess = 8 * p * y / (norm ** 2) + 4 * np.dot(y, p) / (norm ** 2) - (16 * p ** 2 * np.dot(y, p)) / (norm ** 3)\n    return grad, hess\n\ndef eval_qwk_lgb_regr_metric(y_pred, true):\n    y_true=true.label\n\n    dist = Counter(y_true)\n    for k in dist:\n        dist[k] /= len(y_true)\n\n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred)))\n\n    return 'cappa', qwk(y_true, y_pred), True\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    dist = Counter(reduce_train['accuracy_group'])\n    for k in dist:\n        dist[k] /= len(reduce_train)\n#     reduce_train['accuracy_group'].hist()\n    \n    acum = 0\n    bound = {}\n    for i in range(3):\n        acum += dist[i]\n        bound[i] = np.percentile(y_pred, acum * 100)\n\n    def classify(x):\n        if x <= bound[0]:\n            return 0\n        elif x <= bound[1]:\n            return 1\n        elif x <= bound[2]:\n            return 2\n        else:\n            return 3\n\n    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n\n    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n#     kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2,random_state=2020)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], 5))\n    all_coefficients = np.zeros((5, 3))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    ind = []\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '/5')\n        if isinstance(train, pd.DataFrame):\n            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        else:\n            dev_X, val_X = train[dev_index], train[val_index]\n            dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        #Truncated Valをいれるべきか？\n        pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk[1])\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = train.columns.values\n        fold_importance_df['importance'] = importances\n        fold_importance_df['fold'] = i\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n        i += 1\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    \n    pred_full_test = pred_full_test / 5.0\n    results = {'label': label,\n               'train': pred_train, \n               'test': pred_full_test,\n               'cv': cv_scores, \n               'qwk': qwk_scores,\n               'importance': feature_importance_df,\n               'coefficients': all_coefficients\n              }\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGB_MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#一旦決め打ち\nlgb_params = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'max_depth': 11,\n          'learning_rate': 0.01,\n          'feature_fraction': 0.8,\n          'verbosity': -1,\n          'lambda_l1': 1,\n          'lambda_l2': 1,\n          'data_random_seed': 3,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 10000\n             }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = reduce_train['accuracy_group']\ncategoricals = ['session_title']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n    d_train = lgb.Dataset(train_X, label=train_y,categorical_feature=categoricals)\n    d_valid = lgb.Dataset(test_X, label=test_y,categorical_feature=categoricals)\n    watchlist = [d_train, d_valid]\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop\n                     )\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    optR = OptimizedRounder()\n    coefficients = [0.5, 1.5, 2.5]\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = eval_qwk_lgb_regr(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_results = run_cv_model(reduce_train[features], ajusted_test[features], target, runLGB, lgb_params, rmse, 'lgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(lgb_results['coefficients'], axis=0)\nprint(coefficients_)\nlgb_train_predictions = [r[0] for r in lgb_results['train']]\nlgb_train_predictions = optR.predict(lgb_train_predictions, coefficients_).astype(int)\nCounter(lgb_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR.predict(lgb_train_predictions, coefficients_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\nlgb_test_predictions = [r[0] for r in lgb_results['test']]\nlgb_test_predictions = optR.predict(lgb_test_predictions, coefficients_).astype(int)\nCounter(lgb_test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_qwk_lgb_regr(target, lgb_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(target, [r[0] for r in lgb_results['train']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB_MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n    'objective':'reg:squarederror',\n    'colsample_bytree': 0.8,\n    'learning_rate': 0.01,\n    'max_depth': 10,\n    'subsample': 1,\n    'min_child_weight':3,\n    'gamma':0.25,\n    'n_estimators':5000,\n    'num_boost_round':5000,\n    'early_stopping_rounds':100\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runXGB(train_X, train_y, test_X, test_y, test_X2, params):\n    d_train = xgb.DMatrix(train_X, label=train_y)\n    d_valid = xgb.DMatrix(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    num_rounds = params.pop('num_boost_round')\n    verbose_eval = 100\n    early_stop = None\n    if params.get('early_stopping_rounds'):\n        early_stop = params.pop('early_stopping_rounds')\n    model = xgb.train(params,\n                      d_train,\n                      num_boost_round=num_rounds,\n                      evals=[(d_train, 'train'), (d_valid, 'val')],\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop\n                     )\n    \n    pred_test_y = model.predict(xgb.DMatrix(test_X),ntree_limit=model.best_ntree_limit)\n    optR = OptimizedRounder()\n    coefficients = [0.5, 1.5, 2.5]\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = eval_qwk_lgb_regr(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n    pred_test_y2 = model.predict(xgb.DMatrix(test_X2),ntree_limit=model.best_ntree_limit)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), features,coefficients, qwk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_results = run_cv_model(reduce_train[features], ajusted_test[features], target, runXGB, xgb_params, rmse, 'xgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(xgb_results['coefficients'], axis=0)\nprint(coefficients_)\nxgb_train_predictions = [r[0] for r in xgb_results['train']]\nxgb_train_predictions = optR.predict(xgb_train_predictions, coefficients_).astype(int)\nCounter(xgb_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR.predict(xgb_train_predictions, coefficients_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\nxgb_test_predictions = [r[0] for r in xgb_results['test']]\nxgb_test_predictions = optR.predict(xgb_test_predictions, coefficients_).astype(int)\nCounter(xgb_test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_qwk_lgb_regr(target, xgb_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(target, [r[0] for r in xgb_results['train']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CATB_MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_params = {\n          'depth': 9,\n          'eta': 0.05,\n          'random_strength': 1.5,\n          'one_hot_max_size': 2,\n          'reg_lambda': 6,\n          'od_type': 'Iter',\n          'fold_len_multiplier': 2,\n          'bootstrap_type' : \"Bayesian\",\n          'bagging_temperature': 1,\n          'random_seed': 217,\n          'early_stopping_rounds':100, \n          'num_boost_round': 2500\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runCAT(train_X, train_y, test_X, test_y, test_X2, params):\n    watchlist = (test_X, test_y)\n    verbose_eval = 100\n    early_stop = None\n    if params.get('early_stopping_rounds'):\n        early_stop = params.pop('early_stopping_rounds')\n    model = CatBoostRegressor(cat_features=categoricals, **params)\n    model.fit(train_X, train_y, eval_set=watchlist, verbose=verbose_eval)\n    \n    pred_test_y = model.predict(test_X)\n    optR = OptimizedRounder()\n    coefficients = [0.5, 1.5, 2.5]\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = eval_qwk_lgb_regr(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n    pred_test_y2 = model.predict(test_X2)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), features,coefficients, qwk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_results = run_cv_model(reduce_train[features], ajusted_test[features], target, runCAT, cat_params, rmse, 'cat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(cat_results['coefficients'], axis=0)\nprint(coefficients_)\ncat_train_predictions = [r[0] for r in cat_results['train']]\ncat_train_predictions = optR.predict(cat_train_predictions, coefficients_).astype(int)\nCounter(cat_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR.predict(cat_train_predictions, coefficients_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optR = OptimizedRounder()\ncat_test_predictions = [r[0] for r in cat_results['test']]\ncat_test_predictions = optR.predict(cat_test_predictions, coefficients_).astype(int)\nCounter(cat_test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_qwk_lgb_regr(target, cat_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse(target, [r[0] for r in cat_results['train']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble of XGB + LGB +CAT"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_models = [lgb_train_predictions,xgb_train_predictions,cat_train_predictions]\ntest_models = [lgb_test_predictions, xgb_test_predictions,cat_test_predictions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge\nlr = Ridge(fit_intercept=False)\nlr.fit(np.array(oof_models).T, target)\nprint(lr.coef_)\nlr.coef_ = lr.coef_ * 1/(sum(lr.coef_))\nprint(lr.coef_)\noof_lr = lr.predict(np.array(oof_models).T)\ntest_preds_lr = lr.predict(np.array(test_models).T)\n#lr of nn and lgb and xgb\noptR = OptimizedRounder()\noptR.fit(oof_lr, target)\ncoefficients = optR.coefficients()\nprint(coefficients)\noof_rounded = optR.predict(oof_lr, coefficients)\nprint(eval_qwk_lgb_regr(target, oof_rounded))\ntest_rounded_lr = optR.predict(test_preds_lr, coefficients)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['accuracy_group'] = test_rounded_lr.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}