{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ast\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction import text\nfrom joblib import Parallel, delayed\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nfrom numba import jit\n\nimport time\nimport copy\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold\nfrom functools import partial\nimport scipy as sp\n\n# making the workspace wider\n# from IPython.core.display import display, HTML\n# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n\n# more information in the tables\npd.set_option('display.max_columns',150)\npd.set_option('display.max_rows',150)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# PARAMS_CALC = {\n#     'delete from test/train assessment with correct&uncorrect == 0':True\n# }\n\n# if start on kaggle server set True\nKAGGLE_START = True\nBAYES_OPTIMIZATION = False\n\n# if KAGGLE_START:\n#     BAYES_OPTIMIZATION = True\n# else:\n#     BAYES_OPTIMIZATION = False\n    \nn_fold = 5\nfolds = False\n# folds = GroupKFold(n_splits=n_fold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Source: https://www.kaggle.com/artgor/quick-and-dirty-regression\n@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'cappa', qwk(y_true, y_pred), True\n\n\ndef eval_qwk_lgb_regr(y_true, y_pred):\n    \"\"\"\n    Fast cappa eval function for lgb.\n    \"\"\"\n    y_pred[y_pred <= 1.12232214] = 0\n    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n    y_pred[y_pred > 2.22506454] = 3\n\n    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n\n    return 'cappa', qwk(y_true, y_pred), True\n\n\nclass LGBWrapper_regr(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMRegressor()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n        if params['objective'] == 'regression':\n            eval_metric = eval_qwk_lgb_regr\n        else:\n            eval_metric = 'auc'\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict(self, X_test):\n        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n\n    \ndef eval_qwk_xgb(y_pred, y_true):\n    \"\"\"\n    Fast cappa eval function for xgb.\n    \"\"\"\n    # print('y_true', y_true)\n    # print('y_pred', y_pred)\n    y_true = y_true.get_label()\n    y_pred = y_pred.argmax(axis=1)\n    return 'cappa', -qwk(y_true, y_pred)\n\n\nclass LGBWrapper(object):\n    \"\"\"\n    A wrapper for lightgbm model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = lgb.LGBMClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        eval_names = ['train']\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n            eval_names.append('valid')\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n            eval_names.append('holdout')\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = 'auto'\n        else:\n            categorical_columns = 'auto'\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_qwk_lgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       categorical_feature=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, num_iteration=self.model.best_iteration_)\n\n\nclass CatWrapper(object):\n    \"\"\"\n    A wrapper for catboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = cat.CatBoostClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**{k: v for k, v in params.items() if k != 'cat_cols'})\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        if 'cat_cols' in params.keys():\n            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n            if len(cat_cols) > 0:\n                categorical_columns = params['cat_cols']\n            else:\n                categorical_columns = None\n        else:\n            categorical_columns = None\n        \n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n                       cat_features=categorical_columns)\n\n        self.best_score_ = self.model.best_score_\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if 'MultiClass' not in self.model.get_param('loss_function'):\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_end=self.model.best_iteration_)\n\n\nclass XGBWrapper(object):\n    \"\"\"\n    A wrapper for xgboost model so that we will have a single api for various models.\n    \"\"\"\n\n    def __init__(self):\n        self.model = xgb.XGBClassifier()\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n\n        eval_set = [(X_train, y_train)]\n        self.model = self.model.set_params(**params)\n\n        if X_valid is not None:\n            eval_set.append((X_valid, y_valid))\n\n        if X_holdout is not None:\n            eval_set.append((X_holdout, y_holdout))\n\n        self.model.fit(X=X_train, y=y_train,\n                       eval_set=eval_set, eval_metric=eval_qwk_xgb,\n                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'])\n\n        scores = self.model.evals_result()\n        self.best_score_ = {k: {m: m_v[-1] for m, m_v in v.items()} for k, v in scores.items()}\n        self.best_score_ = {k: {m: n if m != 'cappa' else -n for m, n in v.items()} for k, v in self.best_score_.items()}\n\n        self.feature_importances_ = self.model.feature_importances_\n\n    def predict_proba(self, X_test):\n        if self.model.objective == 'binary':\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)[:, 1]\n        else:\n            return self.model.predict_proba(X_test, ntree_limit=self.model.best_iteration)\n\n\n\n\nclass MainTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n        \"\"\"\n        Main transformer for the data. Can be used for processing on the whole data.\n\n        :param convert_cyclical: convert cyclical features into continuous\n        :param create_interactions: create interactions between features\n        \"\"\"\n\n        self.convert_cyclical = convert_cyclical\n        self.create_interactions = create_interactions\n        self.feats_for_interaction = None\n        self.n_interactions = n_interactions\n\n    def fit(self, X, y=None):\n\n        if self.create_interactions:\n            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n                                          or 'mean' in col or 'max' in col or 'std' in col\n                                          or 'attempt' in col]\n            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        if self.create_interactions:\n            for col1 in self.feats_for_interaction1:\n                for col2 in self.feats_for_interaction2:\n                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n\n        if self.convert_cyclical:\n            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n\n#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n\n#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n\n        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n        \"\"\"\n\n        :param main_cat_features:\n        :param num_cols:\n        \"\"\"\n        self.main_cat_features = main_cat_features\n        self.num_cols = num_cols\n\n    def fit(self, X, y=None):\n\n#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n#                          or 'attempt' in col]\n        \n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n#         for col in self.num_cols:\n#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\nclass RegressorModel(object):\n    \"\"\"\n    A wrapper class for classification models.\n    It can be used for training and prediction.\n    Can plot feature importance and training progress (if relevant for model).\n\n    \"\"\"\n\n    def __init__(self, columns: list = None, model_wrapper=None):\n        \"\"\"\n\n        :param original_columns:\n        :param model_wrapper:\n        \"\"\"\n        self.columns = columns\n        self.model_wrapper = model_wrapper\n        self.result_dict = {}\n        self.train_one_fold = False\n        self.preprocesser = None\n\n    def fit(self, X: pd.DataFrame, y,\n            X_holdout: pd.DataFrame = None, y_holdout=None,\n            folds=None,\n            params: dict = None,\n            eval_metric='rmse',\n            cols_to_drop: list = None,\n            preprocesser=None,\n            transformers: dict = None,\n            adversarial: bool = False,\n            plot: bool = True):\n        \"\"\"\n        Training the model.\n\n        :param X: training data\n        :param y: training target\n        :param X_holdout: holdout data\n        :param y_holdout: holdout target\n        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n        :param params: training parameters\n        :param eval_metric: metric for validataion\n        :param cols_to_drop: list of columns to drop (for example ID)\n        :param preprocesser: preprocesser class\n        :param transformers: transformer to use on folds\n        :param adversarial\n        :return:\n        \"\"\"\n\n        if folds is None:\n            folds = KFold(n_splits=3, random_state=42)\n            self.train_one_fold = True\n\n        self.columns = X.columns if self.columns is None else self.columns\n        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n        self.trained_transformers = {k: [] for k in transformers}\n        self.transformers = transformers\n        self.models = []\n        self.folds_dict = {}\n        self.eval_metric = eval_metric\n        n_target = 1\n        self.oof = np.zeros((len(X), n_target))\n        self.n_target = n_target\n\n        X = X[self.columns]\n        if X_holdout is not None:\n            X_holdout = X_holdout[self.columns]\n\n        if preprocesser is not None:\n            self.preprocesser = preprocesser\n            self.preprocesser.fit(X, y)\n            X = self.preprocesser.transform(X, y)\n            self.columns = X.columns.tolist()\n            if X_holdout is not None:\n                X_holdout = self.preprocesser.transform(X_holdout)\n        \n        if folds!=False:\n            # simple folds split\n            for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n                if X_holdout is not None:\n                    X_hold = X_holdout.copy()\n                else:\n                    X_hold = None\n                self.folds_dict[fold_n] = {}\n                if params['verbose']:\n                    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n                self.folds_dict[fold_n] = {}\n\n                X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n                y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n                if self.train_one_fold:\n                    X_train = X[self.original_columns]\n                    y_train = y\n                    X_valid = None\n                    y_valid = None\n\n                datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n                X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n                self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n                model = copy.deepcopy(self.model_wrapper)\n\n                if adversarial:\n                    X_new1 = X_train.copy()\n                    if X_valid is not None:\n                        X_new2 = X_valid.copy()\n                    elif X_holdout is not None:\n                        X_new2 = X_holdout.copy()\n                    X_new = pd.concat([X_new1, X_new2], axis=0)\n                    y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                    X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n                model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n                self.folds_dict[fold_n]['scores'] = model.best_score_\n                if self.oof.shape[0] != len(X):\n                    self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n                if not adversarial:\n                    self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n\n                fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                               columns=['feature', 'importance'])\n                self.feature_importances = self.feature_importances.append(fold_importance)\n                self.models.append(model)\n        \n        else:\n            # split fold with KMeans clusters, and select in val test max assessment per user in group folds.\n            for fold_n in sorted(X['KFold_Group'].astype('int8').unique()):\n                valid_index = X[\n                    X['KFold_Group']==fold_n\n                ][['installation_id', 'timestamp']].groupby('installation_id').idxmax()['timestamp']\n                train_index = X.drop(valid_index).index\n\n                if X_holdout is not None:\n                    X_hold = X_holdout.copy()\n                else:\n                    X_hold = None\n                self.folds_dict[fold_n] = {}\n                if params['verbose']:\n                    print(f'Fold {fold_n + 1} started at {time.ctime()}')\n                self.folds_dict[fold_n] = {}\n\n                X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n                y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n                if self.train_one_fold:\n                    X_train = X[self.original_columns]\n                    y_train = y\n                    X_valid = None\n                    y_valid = None\n\n                datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n                X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n\n                self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n\n                model = copy.deepcopy(self.model_wrapper)\n\n                if adversarial:\n                    X_new1 = X_train.copy()\n                    if X_valid is not None:\n                        X_new2 = X_valid.copy()\n                    elif X_holdout is not None:\n                        X_new2 = X_holdout.copy()\n                    X_new = pd.concat([X_new1, X_new2], axis=0)\n                    y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n                    X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n\n                model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n\n                self.folds_dict[fold_n]['scores'] = model.best_score_\n                if self.oof.shape[0] != len(X):\n                    self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n                if not adversarial:\n                    self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n\n                fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n                                               columns=['feature', 'importance'])\n                self.feature_importances = self.feature_importances.append(fold_importance)\n                self.models.append(model)\n\n        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n\n        # if params['verbose']:\n        self.calc_scores_()\n\n        if plot:\n            # print(classification_report(y, self.oof.argmax(1)))\n            fig, ax = plt.subplots(figsize=(16, 12))\n            plt.subplot(2, 2, 1)\n            self.plot_feature_importance(top_n=20)\n            plt.subplot(2, 2, 2)\n            self.plot_metric()\n            plt.subplot(2, 2, 3)\n            plt.hist(y.values.reshape(-1, 1) - self.oof)\n            plt.title('Distribution of errors')\n            plt.subplot(2, 2, 4)\n            plt.hist(self.oof)\n            plt.title('Distribution of oof predictions');\n\n    def transform_(self, datasets, cols_to_drop):\n        for name, transformer in self.transformers.items():\n            transformer.fit(datasets['X_train'], datasets['y_train'])\n            datasets['X_train'] = transformer.transform(datasets['X_train'])\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n            self.trained_transformers[name].append(transformer)\n        if cols_to_drop is not None:\n            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n\n            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n            if datasets['X_valid'] is not None:\n                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n            if datasets['X_holdout'] is not None:\n                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n        self.cols_to_drop = cols_to_drop\n\n        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n\n    def calc_scores_(self):\n        print()\n        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n        self.scores = {}\n        for d in datasets:\n            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n            self.scores[d] = np.mean(scores)\n\n    def predict(self, X_test, averaging: str = 'usual'):\n        \"\"\"\n        Make prediction\n\n        :param X_test:\n        :param averaging: method of averaging\n        :return:\n        \"\"\"\n        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n        if self.preprocesser is not None:\n            X_test = self.preprocesser.transform(X_test)\n        for i in range(len(self.models)):\n            X_t = X_test.copy()\n            for name, transformers in self.trained_transformers.items():\n                X_t = transformers[i].transform(X_t)\n\n            if self.cols_to_drop is not None:\n                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n                X_t = X_t.drop(cols_to_drop, axis=1)\n            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n\n            # if case transformation changes the number of the rows\n            if full_prediction.shape[0] != len(y_pred):\n                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n\n            if averaging == 'usual':\n                full_prediction += y_pred\n            elif averaging == 'rank':\n                full_prediction += pd.Series(y_pred).rank().values\n\n        return full_prediction / len(self.models)\n\n    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Plot default feature importance.\n\n        :param drop_null_importance: drop columns with null feature importance\n        :param top_n: show top n columns\n        :return:\n        \"\"\"\n\n        top_feats = self.get_top_features(drop_null_importance, top_n)\n        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n        feature_importances['feature'] = feature_importances['feature'].astype(str)\n        top_feats = [str(i) for i in top_feats]\n        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n        plt.title('Feature importances')\n\n    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n        \"\"\"\n        Get top features by importance.\n\n        :param drop_null_importance:\n        :param top_n:\n        :return:\n        \"\"\"\n        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n        if drop_null_importance:\n            grouped_feats = grouped_feats[grouped_feats != 0]\n        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n\n    def plot_metric(self):\n        \"\"\"\n        Plot training progress.\n        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n        :return:\n        \"\"\"\n        full_evals_results = pd.DataFrame()\n        for model in self.models:\n            evals_result = pd.DataFrame()\n            for k in model.model.evals_result_.keys():\n                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n            full_evals_results = full_evals_results.append(evals_result)\n\n        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                            'variable': 'dataset'})\n        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n        plt.title('Training progress')\nclass CategoricalTransformer(BaseEstimator, TransformerMixin):\n\n    def __init__(self, cat_cols=None, drop_original: bool = False, encoder=OrdinalEncoder()):\n        \"\"\"\n        Categorical transformer. This is a wrapper for categorical encoders.\n\n        :param cat_cols:\n        :param drop_original:\n        :param encoder:\n        \"\"\"\n        self.cat_cols = cat_cols\n        self.drop_original = drop_original\n        self.encoder = encoder\n        self.default_encoder = OrdinalEncoder()\n\n    def fit(self, X, y=None):\n\n        if self.cat_cols is None:\n            kinds = np.array([dt.kind for dt in X.dtypes])\n            is_cat = kinds == 'O'\n            self.cat_cols = list(X.columns[is_cat])\n        self.encoder.set_params(cols=self.cat_cols)\n        self.default_encoder.set_params(cols=self.cat_cols)\n\n        self.encoder.fit(X[self.cat_cols], y)\n        self.default_encoder.fit(X[self.cat_cols], y)\n\n        return self\n\n    def transform(self, X, y=None):\n        data = copy.deepcopy(X)\n        new_cat_names = [f'{col}_encoded' for col in self.cat_cols]\n        encoded_data = self.encoder.transform(data[self.cat_cols])\n        if encoded_data.shape[1] == len(self.cat_cols):\n            data[new_cat_names] = encoded_data\n        else:\n            pass\n\n        if self.drop_original:\n            data = data.drop(self.cat_cols, axis=1)\n        else:\n            data[self.cat_cols] = self.default_encoder.transform(data[self.cat_cols])\n\n        return data\n\n    def fit_transform(self, X, y=None, **fit_params):\n        data = copy.deepcopy(X)\n        self.fit(data)\n        return self.transform(data)\n\n# to maximize Quadratic Weighted Kappa (QWK) score\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n        return -qwk(y, X_p)\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# prepare main train_df/test_df"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\n#### 1. GET DATA \nif KAGGLE_START:\n    PATH = '/kaggle/input/data-science-bowl-2019/'\nelse:\n    PATH = './'\n\ntrain_df = pd.read_csv(PATH+'train.csv')\ntrain_df['timestamp'] = pd.to_datetime(train_df['timestamp']).dt.tz_convert(None)\n\ntest_df = pd.read_csv(PATH+'test.csv')\ntest_df['timestamp'] = pd.to_datetime(test_df['timestamp']).dt.tz_convert(None)\n\n# prepare float columns\ntrain_df['event_code'] = train_df['event_code'].astype('uint16')\ntest_df['event_code'] = test_df['event_code'].astype('uint16')\ntrain_df['event_count'] = train_df['event_count'].astype('uint16')\ntest_df['event_count'] = test_df['event_count'].astype('uint16')\ntrain_df['game_time'] = train_df['game_time'].fillna(0).astype('uint32')\ntest_df['game_time'] = test_df['game_time'].fillna(0).astype('uint32')\n\ntrain_labels_df = pd.read_csv(PATH+'train_labels.csv')\nspecs_df = pd.read_csv(PATH+'specs.csv')\nsample_submission_df = pd.read_csv(PATH+'sample_submission.csv')\nprint('1. Read get data')\n\n\n#### 2. Get correct/uncorrect in train/test\nfor df in [train_df, test_df]:\n    \n    df['correct'] = df['event_data'].str.contains('true').astype('uint8')\n    df['correct'] = np.where(\n        (\n            (df['type'] == 'Assessment')&\\\n            (df['title'] == 'Bird Measurer (Assessment)')&\\\n            (df['event_code'] == 4110)\n        ) | (\n            (df['type'] == 'Assessment')&\\\n            (df['title'] != 'Bird Measurer (Assessment)')&\\\n            (df['event_code'] == 4100)\n        )\n        , df['correct']\n        , 0\n    ).astype('uint8')\n    \n    df['uncorrect'] = df['event_data'].str.contains('false').astype('uint8')\n    df['uncorrect'] = np.where(\n        (\n            (df['type'] == 'Assessment')&\\\n            (df['title'] == 'Bird Measurer (Assessment)')&\\\n            (df['event_code'] == 4110)\n        ) | (\n            (df['type'] == 'Assessment')&\\\n            (df['title'] != 'Bird Measurer (Assessment)')&\\\n            (df['event_code'] == 4100)\n        )\n        , df['uncorrect']\n        , 0\n    ).astype('uint8')\n    \n    # df.drop('event_data', axis=1,inplace=True)\n    \ntrain_df['assessment_code'] = np.where(train_df['correct']+train_df['uncorrect']>0, 1,0).astype('uint8')\ntest_df['assessment_code'] = np.where(test_df['correct']+test_df['uncorrect']>0, 1,0).astype('uint8')\ngc.collect();\nprint('2. Get correct/uncorrect in train/test')\n\n# N_JOBS_PARALLEL = 8\n# def mapAst(s,idrow, replace_true = True):\n#     if replace_true:\n#         s = ast.literal_eval(s.replace('true','True').replace('false', 'False'))\n#     else:\n#         s = ast.literal_eval(s)\n#     s.update({'ID_ROW':idrow})\n#     return s\n\n# # train map dict\n# %time train_event_data = Parallel(n_jobs=N_JOBS_PARALLEL)(delayed(mapAst)(s, idrow) for idrow, s in enumerate(tqdm_notebook(train_df[train_df['assessment_code']==1]['event_data'])))\n# %time train_event_data = pd.DataFrame.from_dict(train_event_data)\n\n# # test map dict\n# %time test_event_data = Parallel(n_jobs=N_JOBS_PARALLEL)(delayed(mapAst)(s, idrow) for idrow, s in enumerate(tqdm_notebook(test_df[test_df['assessment_code']==1]['event_data'])))\n# %time test_event_data = pd.DataFrame.from_dict(test_event_data)\n\n# evendata_features = list(set(train_event_data.columns) & set(test_event_data.columns))\n# train_event_data = train_event_data[evendata_features]\n# test_event_data = test_event_data[evendata_features]\n\ntrain_df.drop('event_data', axis=1,inplace=True)\ntest_df.drop('event_data', axis=1,inplace=True)\ngc.collect();\nprint('2.1 parse event_data assessment only in dfs train_event_data/test_event_data')\n\n#### 3. clear Assessment without accuracy_group\ntrain_df = train_df[~((train_df['type']=='Assessment')&(train_df['correct']+train_df['uncorrect']==0))]\n\ntrain_df.sort_values(['installation_id', 'world', 'timestamp'], inplace=True)\ntest_df.sort_values(['installation_id', 'world', 'timestamp'], inplace=True)\n\ntrain_df.reset_index(drop=True, inplace=True);\ntest_df.reset_index(drop=True, inplace=True);\ngc.collect();\nprint('3. clear Assessment without accuracy_group')\n\n\n#### 4. type dummies\nfor feature in ['type']: # 'world', \n    train_df = pd.concat(\n        [\n            train_df,\n            pd.get_dummies(train_df[feature], prefix='Is',prefix_sep='')\n        ],\n        axis = 1,\n        sort=False\n    )\n    test_df = pd.concat(\n        [\n            test_df,\n            pd.get_dummies(test_df[feature], prefix='Is',prefix_sep='')\n        ],\n        axis = 1,\n        sort=False\n    )\n\nprint('4. type dummies')\n\n#### 5. Search last assessment for notes predict rows\ndimension_key = ['installation_id', 'world', 'Assessment_id', 'name_assessment']\n# search predict rows (last assessment's)\ntest_df = pd.merge(\n    test_df,\n    test_df.merge(\n        test_df.groupby('installation_id')['timestamp'].max().reset_index()\n    )[['game_session', 'installation_id', 'IsAssessment']].rename(columns={'IsAssessment':'Predict_rows'}),\n    on = ['game_session', 'installation_id'],\n    how = 'left'\n)\ntest_df['Predict_rows'] = test_df['Predict_rows'].fillna(0).astype('uint8')\n# so that the signs do not differ\ntrain_df['Predict_rows'] = 0\nprint('5. Search last assessment for notes predict rows')\n\n# #### 5.1 delete from test/train assessment with correct&uncorrect\n# train_df = pd.concat(\n#     [\n#         train_df,\n#         train_df.groupby(\n#             ['installation_id', 'game_session', 'type']\n#         )['correct', 'uncorrect', 'Predict_rows'].transform('sum').rename(\n#             columns={\n#                 'correct':'correct_ig',\n#                 'uncorrect':'uncorrect_ig',\n#                 'Predict_rows':'Predict_rows_ig'\n#             }\n#         )\n#     ],\n#     axis=1,\n#     sort=False\n# )\n# test_df = pd.concat(\n#     [\n#         test_df,\n#         test_df.groupby(\n#             ['installation_id', 'game_session', 'type']\n#         )['correct', 'uncorrect', 'Predict_rows'].transform('sum').rename(\n#             columns={\n#                 'correct':'correct_ig',\n#                 'uncorrect':'uncorrect_ig',\n#                 'Predict_rows':'Predict_rows_ig'\n#             }\n#         )\n#     ],\n#     axis=1,\n#     sort=False\n# )\n# train_df['is_drop_assessment_id'] = np.where(\n#     ((train_df['correct_ig'].fillna(0)+train_df['uncorrect_ig'].fillna(0))==0) & (train_df['type']=='Assessment') & (train_df['Predict_rows_ig']==0)\n#     , 1\n#     , 0\n# )\n# test_df['is_drop_assessment_id'] = np.where(\n#     ((test_df['correct_ig'].fillna(0)+test_df['uncorrect_ig'].fillna(0))==0) & (test_df['type']=='Assessment') & (test_df['Predict_rows_ig']==0)\n#     , 1\n#     , 0\n# )\n# train_df.drop(train_df[train_df['is_drop_assessment_id']==1].index, inplace=True)\n# train_df.drop(['is_drop_assessment_id', 'correct_ig', 'uncorrect_ig'], axis=1, inplace=True)\n# test_df.drop(test_df[test_df['is_drop_assessment_id']==1].index, inplace=True)\n# test_df.drop(['is_drop_assessment_id', 'correct_ig', 'uncorrect_ig'], axis=1, inplace=True)\n# print('5.1 delete from test/train assessment with correct&uncorrect')\n\n#### 6. get number assessment_id per installation_id, world, title assessment\ndef get_assessmentid(df):\n    df['IsAssessment'] = np.where(df['type']=='Assessment', 1, 0)\n    df = df.merge(\n        df[df['IsAssessment']==1].groupby(['installation_id', 'game_session'])['event_count'].max().reset_index().rename(columns={'event_count':'max_step_assessment'}),\n        how = 'left',\n        copy = True\n    )\n    \n    # find the last step in each Assessment\n    df['max_step_assessment'] = np.where(df['max_step_assessment']==df['event_count'], 1, 0)\n    # move the unit 1 line down (from the last assessment action) - this will be the beginning of the next one\n    # shift in the context of the person/world, all that is empty is the first line, replace the void with 1.\n    df['firstRow'] = df.groupby(['installation_id', 'world'])['max_step_assessment'].shift(1).fillna(1)\n    # accumulate Assessment id\n    df['Assessment_id'] = df.groupby(['installation_id', 'world'])['firstRow'].cumsum()\n    # in the context of the accumulated Assessment_id (attempts), output the name of the test\n    df['title_assessment'] = np.where(df['IsAssessment']==1, df['title'], '')\n    df['name_assessment'] = df.groupby(['installation_id', 'world', 'Assessment_id'])['title_assessment'].transform('max')\n    df.drop('title_assessment', axis=1, inplace=True)\n    # accumulate the Assessment id in the test section\n    df['Assessment_id'] = df.groupby(['installation_id', 'world', 'name_assessment'])['firstRow'].cumsum().astype('int8')\n    df.drop(['firstRow', 'max_step_assessment'], axis=1, inplace=True)\n    return df\ntrain_df = get_assessmentid(train_df)\ntest_df = get_assessmentid(test_df)\ngc.collect();\nprint('6. get number assessment_id per installation_id, world, title assessment')\n\n\n#### 7. timestamp - duration\nfor df in [train_df, test_df]:\n    df['timestamp_shift'] = df.groupby(dimension_key)['timestamp'].shift()\n    df['duration'] = ((df['timestamp'] - df['timestamp_shift']).fillna(0).astype('int64') / int(1e9)).astype('float32')\n    df['duration'].fillna(0,inplace=True)\n    df.drop('timestamp_shift', axis=1, inplace=True)\ngc.collect();\nprint('7. timestamp - duration')\n\n\n#### 8. name_types first row 1, for unique types with cumsum 1\nname_types = ['IsActivity', 'IsAssessment', 'IsClip', 'IsGame']\nfor df in [train_df, test_df]:\n    for name_type in name_types:\n        df['shift'+name_type] = df.groupby(dimension_key)[name_type].shift(-1).fillna(0).astype('uint8')\n        df[name_type+'UniqueAction'] = (df[name_type]-df['shift'+name_type]).astype('int8')\n        df[name_type+'UniqueAction'] = np.where(df[name_type+'UniqueAction']==1, 1, 0)\n        df.drop(['shift'+name_type], axis=1, inplace=True)\nprint('8. name_types first row 1, for unique types with cumsum 1')\n\n\n#### 9. Del world NONE\ntrain_df = train_df[(train_df.world != 'NONE')]\ntest_df = test_df[(test_df.world != 'NONE')]\nprint('9. Del world NONE')\n\n\n#### 10. get game_time_diff\nfor df in [train_df, test_df]:\n    df['game_time_diff'] = ((df.groupby(['installation_id', 'world', 'Assessment_id', 'name_assessment', 'type'])['game_time'].diff().fillna(0))/1000).astype('float32')\n    df['game_time_diff'] = np.where(\n        df['event_count'] == 1\n        , 0\n        , df['game_time_diff']\n    )\nprint('10. get game_time_diff')\n\n#### 11. top7_event_code\ntop7_event_code = list(train_df['event_code'].value_counts(normalize=True)[:5].index)\nfeatures_event_code = []\nfor e in top7_event_code:\n    train_df['event_code_'+str(e)] = np.where(train_df['event_code']==e, 1, 0)\n    test_df['event_code_'+str(e)] = np.where(test_df['event_code']==e, 1, 0)\n    features_event_code.append('event_code_'+str(e))\nprint('11. top7_event_code')\n\n#### 12. assessment stats on event code\ntrain_df['assessment_event_count'] = np.where(train_df['assessment_code']==1, train_df['event_count'], 0)\ntest_df['assessment_event_count'] = np.where(test_df['assessment_code']==1, test_df['event_count'], 0)\ntrain_df['assessment_game_time'] = np.where(train_df['assessment_code']==1, train_df['game_time'], 0)\ntest_df['assessment_game_time'] = np.where(test_df['assessment_code']==1, test_df['game_time'], 0)\n\n# if PARAMS_CALC['delete from test/train assessment with correct&uncorrect == 0']:    \n# in the test, there are many cases when there was a test (maybe just a person came in and left), for such cases, the score is 0, so we delete it from the sample, re-sort it\n# and mark up the Assessment_id again\n\n# # get stats per dimensions\n# potencial_train = train_df.groupby(['installation_id', 'world', 'name_assessment', 'Assessment_id'])['correct', 'uncorrect', 'Predict_rows'].sum().reset_index()\n# potencial_test = test_df.groupby(['installation_id', 'world', 'name_assessment', 'Assessment_id'])['correct', 'uncorrect', 'Predict_rows'].sum().reset_index()\n\n# # search bad example with 0 correct and 0 uncorrect actions in assessment\n# potencial_train['is_drop_assessment_id'] = np.where(\n#     ((potencial_train['correct'].fillna(0)+potencial_train['uncorrect'].fillna(0))==0) & (potencial_train['name_assessment']!='') & (potencial_train['Predict_rows']==0)\n#     , 1\n#     , 0\n# )\n# potencial_test['is_drop_assessment_id'] = np.where(\n#     ((potencial_test['correct'].fillna(0)+potencial_test['uncorrect'].fillna(0))==0) & (potencial_test['name_assessment']!='') & (potencial_test['Predict_rows']==0)\n#     , 1\n#     , 0\n# )\n\n# # join bad example\n# train_df = train_df.merge(\n#     potencial_train.drop(['correct', 'uncorrect', 'Predict_rows'], axis=1),\n#     how='left'\n# )\n# test_df = test_df.merge(\n#     potencial_test.drop(['correct', 'uncorrect', 'Predict_rows'], axis=1),\n#     how='left'\n# )\n\n# # drop bad example\n# train_df = train_df[train_df['is_drop_assessment_id']==0].copy()\n# train_df.drop('is_drop_assessment_id', axis=1, inplace=True)\n# test_df = test_df[test_df['is_drop_assessment_id']==0].copy()\n# test_df.drop('is_drop_assessment_id', axis=1, inplace=True)\n\n# # del potencial_train, potencial_test;\n# # gc.collect();\n\n# # update Assessment_id before drop\n# train_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n# test_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n# train_df = get_assessmentid(train_df)\n# test_df = get_assessmentid(test_df)\n# gc.collect();\n\n\n# train_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n# test_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n# train_df = pd.concat(\n#     [\n#         train_df,\n#         train_df.groupby(\n#             ['installation_id', 'world', 'name_assessment', 'Assessment_id']\n#         )['correct', 'uncorrect', 'Predict_rows'].transform('sum').rename(\n#             columns={\n#                 'correct':'correct_iwna',\n#                 'uncorrect':'uncorrect_iwna',\n#                 'Predict_rows':'Predict_rows_iwna'\n#             }\n#         )\n#     ],\n#     axis=1,\n#     sort=False\n# )\n\n# test_df = pd.concat(\n#     [\n#         test_df,\n#         test_df.groupby(\n#             ['installation_id', 'world', 'name_assessment', 'Assessment_id']\n#         )['correct', 'uncorrect', 'Predict_rows'].transform('sum').rename(\n#             columns={\n#                 'correct':'correct_iwna',\n#                 'uncorrect':'uncorrect_iwna',\n#                 'Predict_rows':'Predict_rows_iwna'\n#             }\n#         )\n#     ],\n#     axis=1,\n#     sort=False\n# )\n\n# train_df['is_drop_assessment_id'] = np.where(\n#     ((train_df['correct_iwna'].fillna(0)+train_df['uncorrect_iwna'].fillna(0))==0) & (train_df['name_assessment']!='') & (train_df['Predict_rows_iwna']==0)\n#     , 1\n#     , 0\n# )\n# test_df['is_drop_assessment_id'] = np.where(\n#     ((test_df['correct_iwna'].fillna(0)+test_df['uncorrect_iwna'].fillna(0))==0) & (test_df['name_assessment']!='') & (test_df['Predict_rows_iwna']==0)\n#     , 1\n#     , 0\n# )\n\n# train_df.drop(train_df[train_df['is_drop_assessment_id']==1].index, inplace=True)\n# train_df.drop('is_drop_assessment_id', axis=1, inplace=True)\n# test_df.drop(test_df[test_df['is_drop_assessment_id']==1].index, inplace=True)\n# test_df.drop('is_drop_assessment_id', axis=1, inplace=True)\n\n# # update Assessment_id before drop\n# train_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n# test_df.sort_values(['installation_id', 'timestamp'], inplace=True)\n# train_df = get_assessmentid(train_df)\n# test_df = get_assessmentid(test_df)\n# gc.collect();\n\n# print('11. delete from test/train assessment with correct&uncorrect == 0')\n\nprint('Ready')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# create train/test for predict"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#### Create train/test df partition by dimension key and aggregate\n\n# just copy for agg min time\ntrain_df['timestamp_min'] = train_df['timestamp']\ntest_df['timestamp_min'] = test_df['timestamp']\n\ntypes = ['IsActivity', 'IsAssessment', 'IsClip', 'IsGame']\nagg_features = {\n    f:'sum' for f in ['IsActivityUniqueAction', 'IsAssessmentUniqueAction', 'IsClipUniqueAction', 'IsGameUniqueAction']\n}\n\nagg_features.update(\n    {\n        t:'sum' for t in types\n    }\n)\n\nagg_features.update(\n    {\n        'title':'nunique',\n        'correct':'sum', 'uncorrect':'sum', \n        'game_time_diff' : 'mean', 'Predict_rows':'max',\n        'timestamp_min':'min', 'timestamp' : 'max',\n        'assessment_event_count':'max',\n        'assessment_game_time':'max',\n    }\n)\nagg_features.update({f:'sum' for f in features_event_code})\n\ntrain = train_df.groupby(dimension_key).agg(agg_features).reset_index().rename(columns = {f:f+'CountAction' for f in types})\ntrain.rename(columns = {'game_time_diff':'mean_gametime_all'}, inplace=True)\ntest = test_df.groupby(dimension_key).agg(agg_features).reset_index().rename(columns = {f:f+'CountAction' for f in types}).rename(columns = {'game_time_diff':'mean_gametime_all'})\ntest.rename(columns = {'game_time_diff':'mean_gametime_all'}, inplace=True)\n\ntrain['FullTimeSession'] = ((train['timestamp'] - train['timestamp_min']).fillna(0).astype('int64') / int(1e9)).astype('float32')\ntest['FullTimeSession'] = ((test['timestamp'] - test['timestamp_min']).fillna(0).astype('int64') / int(1e9)).astype('float32')\n\ntrain.sort_values(['installation_id', 'timestamp'], inplace=True)\ntrain.reset_index(drop=True, inplace=True)\ntest.sort_values(['installation_id', 'timestamp'], inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\ngc.collect();\n\n# add mean correct & uncorrect all users stats\ndf = pd.concat(\n    [\n        train.query('(name_assessment != \"\") & (Predict_rows == 0)'),\n        test.query('(name_assessment != \"\") & (Predict_rows == 0)')\n    ], \n    axis=0,\n    sort=False\n).groupby(\n    [\n        'world', 'name_assessment', 'Assessment_id'\n    ]\n)['correct', 'uncorrect'].mean().reset_index().rename(\n    columns={\n        'correct':'correct_mean_all_users', 'uncorrect':'uncorrect_mean_all_users'\n    }\n)\n\ntrain = train.merge(\n    df\n    , how='left'\n)\ntest = test.merge(\n    df\n    , how='left'\n)\n\ndel df;\ngc.collect();\n\n\n#### create time-features with cyclical encode\nfrom tqdm import tqdm_notebook\ndef get_cyclical_encode(\n    df,\n    cols_maxval = {},\n    is_drop = False\n):\n    df = df.copy()\n    for col in tqdm_notebook(cols_maxval.keys()):\n        print('Start ', col)\n        df[col + '_sin'] = (np.sin(2 * np.pi * df[col]/cols_maxval[col])).astype('float16')\n        df[col + '_cos'] = (np.cos(2 * np.pi * df[col]/cols_maxval[col])).astype('float16')\n        print('Add', col + '_sin',col + '_cos')\n\n        if is_drop:\n            # drop non-cycle features\n            df.drop(col, axis=1, inplace=True)\n            print('Drop in features')\n    return df\n\n\nfor df in [train, test]:\n    # df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    # df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")\n    # df['is_year_start'] = df['timestamp'].dt.is_year_start.astype(\"uint8\")\n    # df['is_year_end'] = df['timestamp'].dt.is_year_end.astype(\"uint8\")\n    # df['weekofyear'] = df['timestamp'].dt.weekofyear.astype(\"uint8\")\n    # df['is_month_end'] = df['timestamp'].dt.is_month_end.astype(\"uint8\")\n    # df['is_month_start'] = df['timestamp'].dt.is_month_start.astype(\"uint8\")\n    # df['dayofyear'] = df['timestamp'].dt.dayofyear.astype(\"uint16\")\n\n# Not enough memory for commit, so commented out a small number of time features.\ncols_maxval = train[['DayOfWeek', 'Hour']].nunique().to_dict()\ntrain = get_cyclical_encode(train, cols_maxval, is_drop = True)\ntest = get_cyclical_encode(test, cols_maxval, is_drop = True)\n\ngc.collect();\n\n\n#### CUMSUM per user, actions & correct/uncorrect\nagg_features = {\n    f:'cumsum' for f in [\n        'IsActivityUniqueAction', 'IsAssessmentUniqueAction',\n        'IsClipUniqueAction', 'IsGameUniqueAction', 'IsActivityCountAction',\n        'IsAssessmentCountAction', 'IsClipCountAction', 'IsGameCountAction',\n        'assessment_game_time', 'title', 'correct', 'uncorrect'\n    ] + features_event_code\n}\n\ntrain = pd.concat([\n    train,\n    train.groupby(['installation_id', 'world', 'name_assessment']).agg(agg_features).rename(\n        columns={f:f+'_cumsum' for f in train.columns}\n    )\n], axis=1, sort=False)\ntrain = pd.concat([\n    train,\n    train.groupby(['installation_id']).agg({'correct':'cumsum', 'uncorrect':'cumsum'}).rename(\n        columns={f:f+'_user_cumsum' for f in train.columns}\n    )\n], axis=1, sort=False)\ntest = pd.concat([\n    test,\n    test.groupby(['installation_id', 'world', 'name_assessment']).agg(agg_features).rename(\n        columns={f:f+'_cumsum' for f in test.columns}\n    )\n], axis=1, sort=False)\ntest = pd.concat([\n    test,\n    test.groupby(['installation_id']).agg({'correct':'cumsum', 'uncorrect':'cumsum'}).rename(\n        columns={f:f+'_user_cumsum' for f in test.columns}\n    )\n], axis=1,sort=False)\ngc.collect();\n\n\n#### feature generation per TYPE in Action\ndef cols_name_mindex(multiindex):\n    map_cols=[]\n    for i in multiindex:\n        x = i[0]\n        for a in range(1, len(i)):\n            x += '_' + str(i[a])\n        map_cols.append(x)\n    return map_cols\ndef feature_generation(df_agg, df):\n\n    df_agg = df_agg.merge(\n        df.groupby(\n            ['installation_id', 'world', 'Assessment_id']\n        )['game_time_diff'].sum().reset_index().rename(columns={'game_time_diff':'sum_gametime_session'})\n    )\n\n    prepare_df_agg = pd.pivot_table(\n        df.rename(columns={'game_time_diff':'sum_gametime'}),\n        aggfunc=np.sum,\n        index = ['installation_id', 'world', 'Assessment_id'],\n        columns = 'type',\n        values = ['sum_gametime']\n    )\n    prepare_df_agg.columns = cols_name_mindex(prepare_df_agg.columns)\n    prepare_df_agg.reset_index(inplace=True)\n    df_agg = df_agg.merge(prepare_df_agg)\n    return df_agg\n\n%time train = feature_generation(train, train_df)\n%time test = feature_generation(test, test_df)\ngc.collect();\n\n\n#### Drop not need columns\ndrop_cols = ['accuracy_calc', 'sum_gametime_Assessment', 'sum_gametime_Clip']\ndrop_cols = list(set(drop_cols) - (set(drop_cols) - set(train.columns)))\ntrain.drop(drop_cols, axis=1,inplace=True)\ntest.drop(drop_cols, axis=1,inplace=True)\ngc.collect();\n\n\n#### Delete assessment with many assessment per worlds\ntrain = train.merge(\n    test[\n        test['name_assessment']!=''\n    ].groupby(\n        ['world', 'name_assessment']\n    )['Assessment_id'].max().reset_index().rename(columns={'Assessment_id':'TestCountAction'})\n    , how = 'left'\n)\n\ntrain['TestCountAction'].fillna(12, inplace=True)\ntrain = train[train['Assessment_id']<=train['TestCountAction']+1].reset_index(drop=True)\ntrain.drop(['TestCountAction'], axis=1, inplace=True)\n\n\n#### set target\nfor df in [train, test]:\n    \n    df['accuracy_calc'] = (df['correct']/(df['correct']+df['uncorrect']))# .fillna(0)\n    \n    # accuracy group\n    df['accuracy_group'] = np.where(\n        \n        df['accuracy_calc'].isnull()==True,\n        np.nan, \n        np.where(\n            df['accuracy_calc']==0,\n            0,\n            np.where(\n                df['accuracy_calc']==1,\n                3,\n                np.where(\n                    df['accuracy_calc']>=0.5,\n                    2,\n                    1\n                )\n            )\n        )\n    )\ntarget = 'accuracy_group'\n\n#### add cumulative accuracy calc\ntrain['cumulative_accuracy_calc'] = train['correct_cumsum'] / (train['correct_cumsum']+train['uncorrect_cumsum'])\ntest['cumulative_accuracy_calc'] = test['correct_cumsum'] / (test['correct_cumsum']+test['uncorrect_cumsum'])\n\n\n#### with cumsum dummies accuracy\n########### TRAIN\n\n# columnsaccuracy\ntrain[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = pd.get_dummies(train['accuracy_group'])\ntrain[\n    ['accuracy_group_'+format(i, '.0f')+'_cumsum_i' for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = train.groupby(['installation_id'])[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n].cumsum()\ntrain[\n    ['accuracy_group_'+format(i, '.0f')+'_cumsum_iw' for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = train.groupby(['installation_id', 'world'])[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n].cumsum()\ntrain[\n    ['accuracy_group_'+format(i, '.0f')+'_cumsum_iwn' for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = train.groupby(['installation_id', 'world', 'name_assessment'])[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(train[train.accuracy_group.isnull()==False].accuracy_group.unique())]\n].cumsum()\n########### TEST\ntest[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = pd.get_dummies(test['accuracy_group'])\ntest[\n    ['accuracy_group_'+format(i, '.0f')+'_cumsum_i' for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = test.groupby(['installation_id'])[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n].cumsum()\ntest[\n    ['accuracy_group_'+format(i, '.0f')+'_cumsum_iw' for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = test.groupby(['installation_id', 'world'])[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n].cumsum()\ntest[\n    ['accuracy_group_'+format(i, '.0f')+'_cumsum_iwn' for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n] = test.groupby(['installation_id', 'world', 'name_assessment'])[\n    ['accuracy_group_'+format(i, '.0f') for i in sorted(test[test.accuracy_group.isnull()==False].accuracy_group.unique())]\n].cumsum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dum_features = []\nfor column in ['world', 'name_assessment']:\n    dummies = pd.get_dummies(test[column])\n    test[[column+'_'+i for i in dummies.columns]] = dummies\n\n    dummies = pd.get_dummies(train[column])\n    train[[column+'_'+i for i in dummies.columns]] = dummies  \n\n    [dum_features.append(column+'_'+f) for f in dummies.columns if f != '']\n\ntry:\n    train.drop(['name_assessment_'], axis=1, inplace=True)\n    test.drop(['name_assessment_'], axis=1, inplace=True)\nexcept:\n    pass\n\ndel dummies;\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shift_features = [\n    'correct_mean_all_users',\n    'uncorrect_mean_all_users',\n    'IsActivityUniqueAction','IsAssessmentUniqueAction', 'IsClipUniqueAction', 'IsGameUniqueAction',\n    'IsActivityCountAction','IsAssessmentCountAction', 'IsClipCountAction', 'IsGameCountAction',\n    'mean_gametime_all',\n    'IsActivityUniqueAction_cumsum', 'IsAssessmentUniqueAction_cumsum',\n    'IsClipUniqueAction_cumsum', 'IsGameUniqueAction_cumsum',\n    'IsActivityCountAction_cumsum', 'IsAssessmentCountAction_cumsum',\n    'IsClipCountAction_cumsum', 'IsGameCountAction_cumsum',\n    'correct_cumsum', 'uncorrect_cumsum', \n    'correct_user_cumsum',\n    'uncorrect_user_cumsum',\n    'sum_gametime_session',\n    'sum_gametime_Activity',\n    'sum_gametime_Game',\n    'FullTimeSession', \n    'assessment_event_count', 'assessment_game_time', 'assessment_game_time_cumsum', 'title_cumsum',\n\n    'accuracy_calc', 'cumulative_accuracy_calc',\n    'accuracy_group_0',\n    'accuracy_group_1', 'accuracy_group_2', 'accuracy_group_3',\n    'accuracy_group_0_cumsum_i', 'accuracy_group_1_cumsum_i',\n    'accuracy_group_2_cumsum_i', 'accuracy_group_3_cumsum_i',\n    'accuracy_group_0_cumsum_iw', 'accuracy_group_1_cumsum_iw',\n    'accuracy_group_2_cumsum_iw', 'accuracy_group_3_cumsum_iw',\n    'accuracy_group_0_cumsum_iwn', 'accuracy_group_1_cumsum_iwn',\n    'accuracy_group_2_cumsum_iwn', 'accuracy_group_3_cumsum_iwn'\n] + features_event_code + [f+'_cumsum' for f in features_event_code]\n\nall_shift_features = []\n\ntrain['is_train_df'] = 1\ntest['is_train_df'] = 0\n\ndf = pd.concat(\n    [\n        train,\n        test\n    ], \n    axis=0,\n    sort=False\n)\n\ndf.sort_values(['installation_id', 'timestamp'], inplace=True)\n\nfor sf in tqdm_notebook(shift_features):\n    # print(sf)\n    # last assessment per installation, world, name_assessment\n    df[sf+'_shift'] = df.groupby(\n        ['installation_id', 'world', 'name_assessment']\n    )[sf].shift(1).astype('float32')\n    all_shift_features.append(sf+'_shift')\n    \n    # shift per world, installation (without 'name_assessment')\n    df[sf+'_shift_iw'] = df.groupby(\n        ['installation_id', 'world']\n    )[sf].shift(1).astype('float32')\n    all_shift_features.append(sf+'_shift_iw')\n    \n    # mean shift per world, name_assessment, assessment_id\n    df = df.merge(\n        df.groupby(\n            ['world', 'name_assessment', 'Assessment_id']\n        )[sf+'_shift'].mean().astype('float32').reset_index().rename(columns={sf+'_shift':sf+'_shift_mean_wnA'}),\n        how='left'\n    )\n    all_shift_features.append(sf+'_shift_mean_wnA')\n    \n    # mean shift_IW (installation, world) last action per world, name_assessment, assessment_id\n    df = df.merge(\n        df.groupby(\n            ['world', 'name_assessment', 'Assessment_id']\n        )[sf+'_shift_iw'].mean().astype('float32').reset_index().rename(columns={sf+'_shift_iw':sf+'_shift_iw_mean_wnA'}),\n        how='left'\n    )\n    all_shift_features.append(sf+'_shift_iw_mean_wnA')\n    \n    \n    # mean shift per world, name_assessment (without assessment_id)\n    df = df.merge(\n        df.groupby(\n            ['world', 'Assessment_id']\n        )[sf+'_shift'].mean().astype('float32').reset_index().rename(columns={sf+'_shift':sf+'_shift_mean_wA'}),\n        how='left'\n    )\n    all_shift_features.append(sf+'_shift_mean_wA')\n\n    df = df.merge(\n        df.groupby(\n            ['world', 'Assessment_id']\n        )[sf+'_shift_iw'].mean().astype('float32').reset_index().rename(columns={sf+'_shift_iw':sf+'_shift_iw_mean_wA'}),\n        how='left'\n    )\n    all_shift_features.append(sf+'_shift_iw_mean_wA')\n\n    \n#     print(df.shape)\n#     # expanding mean shift per world, name_assessment (without assessment_id)\n#     df = df.merge(\n#         df.groupby(\n#             ['installation_id', 'world']\n#         )[sf+'_shift'].expanding(\n#         ).mean(\n#         ).astype('float32').reset_index().rename(\n#             columns={sf+'_shift':sf+'_shift_ExpandingMean_wi'}\n#         )[['installation_id', 'world',sf+'_shift_ExpandingMean_wi']],\n#         how='left'\n#     )\n    \n\n# # func for shift variables\nagg_shift_features = []\n# func_ag\n# for f_agg in tqdm_notebook(func_agg):\nfuncs = [\n\n    \n    # standard deviation for all people\n    [['world', 'Assessment_id', 'name_assessment'], 'std'],\n    # standard deviation for all people and competitions\n    [['world', 'Assessment_id'], 'std'],\n    \n    # standard deviation and mean per user and competitions\n    # [['installation_id', 'world', 'Assessment_id'], 'mean'],\n    [['installation_id', 'world', 'Assessment_id'], 'std'],\n    [['installation_id', 'name_assessment', 'Assessment_id'], 'mean'],\n    # [['installation_id', 'name_assessment', 'Assessment_id'], 'std'],\n    \n    # person / attempt\n    [['installation_id', 'Assessment_id'], 'mean'],\n    # [['installation_id', 'Assessment_id'], 'cumsum'],\n    # [['installation_id', 'Assessment_id'], 'std'],\n    \n    # average for such tests\n    [['name_assessment', 'Assessment_id'], 'mean'],\n    [['Assessment_id'], 'mean'],\n]\n\nfor dim, f_agg in tqdm_notebook(funcs):\n    for sh in all_shift_features:\n        name_concat = ''.join([d[:1] for d in dim])\n        # print(f_agg+'_'+sh+'_'+name_concat)\n        df[f_agg+'_'+sh+'_'+name_concat] = df.groupby(\n            dim\n        )[sh].transform(f_agg).astype('float32')\n\n        # add new features in list\n        agg_shift_features.append(f_agg+'_'+sh+'_'+name_concat)\n        \ntrain = df[df['is_train_df'] == 1].drop('is_train_df', axis=1).copy()\ntest = df[df['is_train_df'] == 0].drop('is_train_df', axis=1).copy()\n\ndel df;\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"train[train['installation_id']=='0006a69f'][dimension_key + sorted(list(set(train.columns) - set(dimension_key)))].to_excel('train.xlsx', index=None)"},{"metadata":{},"cell_type":"raw","source":"len(agg_shift_features)"},{"metadata":{},"cell_type":"markdown","source":"# kfold per kMeans cluster"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import KMeans\nN_CLUSTER = 5\n\nuser_unique_stats = pd.concat(\n    [\n        train,\n        test\n    ],\n    axis=0    \n).groupby(['installation_id']).agg(\n    {\n        'IsActivityUniqueAction' : 'count',\n        'world':'nunique',\n        'name_assessment':'nunique',\n        'Assessment_id':'max'\n    }\n).reset_index().rename(columns = {'IsActivityUniqueAction':'CountAssessment'})\n\n\n# .to_excel('smart_split_to_5_group.xlsx', index=None)\n# .sort_values('CountAssessment', ascending=False)\n\nkmeans = KMeans(n_clusters=N_CLUSTER, random_state=1).fit(user_unique_stats.drop('installation_id', axis=1))\nuser_unique_stats['Cluster'] = kmeans.labels_\n\nuser_unique_stats.sort_values('Cluster', inplace=True)\nuser_unique_stats['NumInst_per_cluster'] = user_unique_stats.groupby('Cluster').cumcount()\n\nuser_unique_stats = user_unique_stats.merge(\n    user_unique_stats.groupby('Cluster').agg(\n        {\n            'installation_id':'count'\n        }\n    ).reset_index().rename(\n        columns = {\n            'installation_id':'inst_count'\n        }\n    )\n)\n\nuser_unique_stats['RoundGroup'] = user_unique_stats['inst_count'] * (1./n_fold)\nuser_unique_stats['KFold_Group'] = (user_unique_stats['NumInst_per_cluster'] // user_unique_stats['RoundGroup'])\n# user_unique_stats.to_excel('smart_split_to_5_group.xlsx', index=None)\n\ntrain = train.merge(user_unique_stats[['installation_id', 'KFold_Group']], how='left')\ntest = test.merge(user_unique_stats[['installation_id', 'KFold_Group']], how='left')\n\nuser_unique_stats.groupby('Cluster').agg(\n    {\n        'installation_id':'count',\n        'CountAssessment':'mean',\n        'world':'mean',\n        'name_assessment':'mean',\n        'Assessment_id':'mean',\n        # 'Assessment_id':'std',\n    }\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"user_unique_stats.groupby(['KFold_Group', 'Cluster'])['installation_id'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### combine information on existing attempts from the test in the train"},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train[(train.accuracy_group.isnull()==False)]\ntrain = pd.concat(\n    [\n        train,\n        test[(test.accuracy_group.isnull()==False)]\n    ],\n    axis=0,\n    sort=False\n)\ntrain[target] = train[target].astype('uint8')\n\ntest = test[(test['Predict_rows']==1)]\ntest.reset_index(drop=True, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"display(train[train['installation_id'] == '00abaee7'])\ndisplay(test[test['installation_id'] == '00abaee7'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if KAGGLE_START:\n    del train_df, test_df;\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"raw","source":"paint_features = [\n    # 'installation_id_le',\n    'world',\n    'name_assessment',\n    'Assessment_id',\n\n    'IsActivityUniqueAction','IsAssessmentUniqueAction','IsClipUniqueAction', 'IsGameUniqueAction',\n    \n    'IsActivityCountAction','IsAssessmentCountAction', 'IsClipCountAction', 'IsGameCountAction',\n    \n    'mean_gametime_all',\n    \n    'IsActivityUniqueAction_cumsum', 'IsAssessmentUniqueAction_cumsum'\n    , 'IsClipUniqueAction_cumsum', 'IsGameUniqueAction_cumsum',\n    \n    'IsActivityCountAction_cumsum', 'IsAssessmentCountAction_cumsum'\n    , 'IsClipCountAction_cumsum', 'IsGameCountAction_cumsum',\n    'correct_cumsum', 'uncorrect_cumsum',\n    'correct_user_cumsum',\n    'uncorrect_user_cumsum',\n    'sum_gametime_session',\n    'sum_gametime_Activity',\n    'sum_gametime_Game'\n]\n\n# just for classification eda\ndef fast_classification_eda_object(df, target, features = False, max_nunique_val = 20, type_issues = 'Classification'):\n    '''\n        Categorial EDA\n    '''\n    # from matplotlib import plot as plt\n    if features == False or type(features) != list:\n        features = string_variables\n\n    if type_issues == 'Classification':\n        for feature in features:\n            if len(df[feature].unique())>2:\n                a = len(df[feature].unique())\n                if a <= max_nunique_val:\n                    plt.figure(figsize = [15,min(max(8,a),5)])\n\n                    plt.subplot(1,2,1)\n                    x_ = df.groupby([feature])[feature].count()\n                    x_.plot(kind='pie')\n                    plt.title(feature)\n\n                    plt.subplot(1,2,2)\n                    cross_tab = pd.crosstab(df[target],df[feature],normalize=0).reset_index()\n                    x_ = cross_tab.melt(id_vars=[target])\n                    x_['value'] = x_['value']\n\n                    sns.barplot(\n                        x=feature,\n                        y='value', \n                        hue = target if len(df[target].unique()) < 10 else None,\n                        data=x_,\n                        palette = ['b','r','g', 'black'],alpha =0.7\n                    )\n\n                    plt.xticks(rotation='vertical')\n                    plt.title(feature + \" - \" + target)\n\n                    plt.tight_layout()\n                    plt.legend()\n                    plt.show()\n                else:\n                    print('Unique values for',feature,'so much..')\n\nfast_classification_eda_object(train, target, paint_features, max_nunique_val=40)"},{"metadata":{},"cell_type":"raw","source":"#   Anaconda\nimport warnings\nwarnings.simplefilter('ignore')\npaint_features = [\n    'Assessment_id',\n    'IsActivityUniqueAction',\n    'IsAssessmentUniqueAction',\n    'IsClipUniqueAction',\n    'IsGameUniqueAction',\n    'IsActivityCountAction',\n    'IsAssessmentCountAction',\n    'IsClipCountAction',\n    'IsGameCountAction',\n    'correct',\n    'uncorrect',\n    'mean_gametime_all',\n    # 'Predict_rows',\n#     'IsActivityUniqueAction_cumsum',\n#     'IsAssessmentUniqueAction_cumsum',\n#     'IsClipUniqueAction_cumsum',\n#     'IsGameUniqueAction_cumsum',\n#     'IsActivityCountAction_cumsum',\n#     'IsAssessmentCountAction_cumsum',\n#     'IsClipCountAction_cumsum',\n#     'IsGameCountAction_cumsum',\n#     'correct_cumsum',\n#     'uncorrect_cumsum',\n#     'correct_user_cumsum',\n#     'uncorrect_user_cumsum',\n    'sum_gametime_session',\n    'sum_gametime_Activity',\n    'sum_gametime_Game'\n]\nfor feature in paint_features:\n    plot_df = train[[feature, 'accuracy_group']].copy()\n    plot_df[feature+'_round'] = np.round(plot_df[feature]/1, 0)*1\n    plot_df = plot_df.groupby([feature+'_round', 'accuracy_group'])[feature].count().reset_index().rename(columns={feature:'count'})\n    plot_df.sort_values(['count'], ascending=[False], inplace=True)\n#     if feature=='IsGameCountAction':\n#         plot_df=plot_df[plot_df['count']>40]\n    \n    plot_df['count_cumsum'] = plot_df['count'].cumsum()\n    plot_df = plot_df[plot_df['count_cumsum']/plot_df['count_cumsum'].max() < 0.8]\n    plot_df.pivot(index=feature+'_round', columns='accuracy_group', values='count').plot(legend=True);"},{"metadata":{},"cell_type":"markdown","source":"# predict"},{"metadata":{},"cell_type":"raw","source":"# Label Encoder\ndict_unique_world = {e:i for i, e in enumerate(set(list(train['world'].unique())+list(test['world'].unique())))}\ndict_unique_name_assessment = {e:i for i, e in enumerate(set(list(train['name_assessment'].unique()) + list(test['name_assessment'].unique())))}\ndict_unique_instid = {e:i for i, e in enumerate(set(list(train['installation_id'].unique()) + list(test['installation_id'].unique())))}\n\nfor df in [train, test]:\n    df['world_le'] = df['world'].map(dict_unique_world)\n    df['name_assessment_le'] = df['name_assessment'].map(dict_unique_name_assessment)\n    df['installation_id_le'] = df['installation_id'].map(dict_unique_instid)"},{"metadata":{"trusted":false},"cell_type":"code","source":"features = dum_features + [\n    'Assessment_id',\n    'DayOfWeek_sin', 'DayOfWeek_cos', 'Hour_sin',\n    'Hour_cos',\n    'correct_mean_all_users',\n    'uncorrect_mean_all_users',\n    'cumulative_accuracy_calc_shift',\n] + all_shift_features + agg_shift_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\ndef drop_corr_features(df, features, perc = 0.95, is_print = False):\n    def set_to_list(cols, excepted):\n        return list(set(cols) - set(excepted))\n    # Identify Highly Correlated Features\n    # Create correlation matrix\n    corr_matrix = df[features].corr().abs()\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    # Find index of feature columns with correlation greater than 0.95/perc var\n    to_drop_corr_feat = [column for column in upper.columns if any(upper[column] > perc)]\n    if is_print:\n        print(', '.join(to_drop_corr_feat))\n    # Drop Marked Features\n    # df.drop(to_drop_corr_feat, axis=1, inplace = True)\n    features = set_to_list(features, to_drop_corr_feat)\n    return to_drop_corr_feat\n\nto_drop_corr_feat = drop_corr_features(pd.concat([train, test], axis=0, sort=False), features)\nfeatures = list(set(features) - set(to_drop_corr_feat))\n\n# to_drop_corr_feat = drop_corr_features(test, features)\n# features = list(set(features) - set(to_drop_corr_feat))\nprint('del', len(to_drop_corr_feat), 'correlation features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"@jit\ndef data_skew(df):\n    \n    sk_df = pd.DataFrame(\n        [\n            {\n                'column': c,\n                'uniq': df[c].nunique(),\n                'skewness': df[c].value_counts(normalize=True).values[0] * 100\n            } for c in df.columns\n        ]\n    )\n    sk_df = sk_df.sort_values('skewness', ascending=False)\n    return sk_df\n\n\npd.options.display.float_format = '{:,.3f}'.format\ndf_skew = data_skew(train)\n# display(df_skew.head(20))\n\nskewness_columns = list(df_skew[df_skew['skewness']>95.0]['column'].to_numpy())\nfeatures = list(set(features) - set(skewness_columns))\n\nprint('del', len(skewness_columns), 'skewness columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"null_features_in_test = test[features].sum().reset_index().rename(columns={0:'sum_f'}).query('sum_f==0')['index'].values\nfeatures = list(set(features) - set(null_features_in_test))\n\nprint('del', len(null_features_in_test), 'null features in test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Outlier detection\ndef detect_outliers(\n    df\n    , n\n    , features\n):\n\n    df = df.copy()\n\n    outlier_indices = []\n    from collections import Counter\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n\n        # outlier step\n        outlier_step = 1.5 * IQR\n\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n\n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n\n    return multiple_outliers\n\n\noutliers = detect_outliers(df = train, n = 10, features = features)\nlen(outliers)\n# train.drop(outliers, inplace=True)\n# train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\ny = train['accuracy_group']\ncols_to_drop = list(set(train.columns)-set(features))\n\nif BAYES_OPTIMIZATION == False:\n    import warnings\n    # warnings.simplefilter('ignore')\n    warnings.filterwarnings(\"ignore\")\n\n    params = {\n        'n_estimators':2000,\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'subsample': 0.75,\n        'subsample_freq': 1,\n        'learning_rate': 0.04,\n        'feature_fraction': 0.75,\n        'max_depth': 32,\n        'lambda_l1': 0.75,  \n        'lambda_l2': 0.75,\n        'verbose': 100,\n        'early_stopping_rounds': 100,\n        'eval_metric': 'cappa'\n    }\n\n    mt = MainTransformer()\n    ft = FeatureTransformer()\n    transformers = {'ft': ft}\n    regressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\n    regressor_model1.fit(\n        X=train,\n        y=y,\n        folds=folds,\n        params=params,\n        preprocesser=mt,\n        transformers=transformers,\n        eval_metric='cappa',\n        cols_to_drop=cols_to_drop\n    )\n\n\n    # %%time\n    pr1 = regressor_model1.predict(train)\n    optR = OptimizedRounder()\n    optR.fit(pr1.reshape(-1,), y)\n    coefficients = optR.coefficients()\n\n    opt_preds = optR.predict(pr1.reshape(-1, ), coefficients)\n    print('val qwk:', qwk(y, opt_preds))\n\n    # some coefficients calculated by me.\n    pr1 = regressor_model1.predict(test)\n    # pr1[pr1 <= 1.12232214] = 0\n    # pr1[np.where(np.logical_and(pr1 > 1.12232214, pr1 <= 1.73925866))] = 1\n    # pr1[np.where(np.logical_and(pr1 > 1.73925866, pr1 <= 2.22506454))] = 2\n    # pr1[pr1 > 2.22506454] = 3\n\n    print(', '.join(['Coeff '+str(i)+': ' + format(c, '.2f') for i, c in enumerate(coefficients)]))\n\n    pr1[pr1 <= coefficients[0]] = 0\n    pr1[np.where(np.logical_and(pr1 > coefficients[0], pr1 <= coefficients[1]))] = 1\n    pr1[np.where(np.logical_and(pr1 > coefficients[1], pr1 <= coefficients[2]))] = 2\n    pr1[pr1 > coefficients[2]] = 3\n\n\n    test['accuracy_group'] = pr1.astype(int)\n    sample_submission_df = sample_submission_df.drop('accuracy_group', axis=1).merge(\n        test[['installation_id', 'accuracy_group']]\n    )\n    sample_submission_df.to_csv('submission.csv', index=False)\n    print('sample_submission_df')\n    display(sample_submission_df['accuracy_group'].value_counts(normalize=True))\n    print('train distrb')\n    train['accuracy_group'].value_counts(normalize=True)\nelse:\n    from bayes_opt import BayesianOptimization\n\n    def LGB_bayesian(max_depth,\n                     lambda_l1,\n                     lambda_l2,\n                     bagging_fraction,\n                     bagging_freq,\n                     colsample_bytree,\n                     learning_rate):\n\n        params = {\n            'boosting_type': 'gbdt',\n            'metric': 'rmse',\n            'objective': 'regression',\n            'eval_metric': 'cappa',\n            'n_jobs': -1,\n            'seed': 42,\n            'early_stopping_rounds': 100,\n            'n_estimators': 2000,\n            'learning_rate': learning_rate,\n            'max_depth': int(max_depth),\n            'lambda_l1': lambda_l1,\n            'lambda_l2': lambda_l2,\n            'bagging_fraction': bagging_fraction,\n            'bagging_freq': int(bagging_freq),\n            'colsample_bytree': colsample_bytree,\n            'verbose': 0\n        }\n\n        mt = MainTransformer()\n        ft = FeatureTransformer()\n        transformers = {'ft': ft}\n        model = RegressorModel(model_wrapper=LGBWrapper_regr())\n        model.fit(\n            X=train,\n            y=y,\n            folds=folds,\n            params=params,\n            preprocesser=mt,\n            transformers=transformers,\n            eval_metric='cappa',\n            cols_to_drop=cols_to_drop,\n            plot=False\n        )\n\n        return model.scores['valid']\n    \n    # set params\n    init_points = 16\n    n_iter = 16\n    bounds_LGB = {\n        'max_depth': (8, 16),\n        'lambda_l1': (0, 5),\n        'lambda_l2': (0, 5),\n        'bagging_fraction': (0.4, 0.6),\n        'bagging_freq': (1, 10),\n        'colsample_bytree': (0.4, 0.6),\n        'learning_rate': (0.05, 0.1)\n    }\n\n    LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=1029)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n\n\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'eval_metric': 'cappa',\n        'n_jobs': -1,\n        'seed': 42,\n        'early_stopping_rounds': 100,\n        'n_estimators': 2000,\n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']),\n        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n        'bagging_freq': int(LGB_BO.max['params']['bagging_freq']),\n        'colsample_bytree': LGB_BO.max['params']['colsample_bytree'],\n        'verbose': 100\n    }\n\n    mt = MainTransformer()\n    ft = FeatureTransformer()\n    transformers = {'ft': ft}\n    regressor_model = RegressorModel(model_wrapper=LGBWrapper_regr())\n    regressor_model.fit(X=train, \n                        y=y, \n                        folds=folds, \n                        params=params, \n                        preprocesser=mt, \n                        transformers=transformers,\n                        eval_metric='cappa', \n                        cols_to_drop=cols_to_drop)\n\n    pr1 = regressor_model.predict(train)\n    optR = OptimizedRounder()\n    optR.fit(pr1.reshape(-1,), y)\n    coefficients1 = optR.coefficients()\n\n    preds_1 = regressor_model.predict(test)\n    w_1 = LGB_BO.max['target']\n\n    del bounds_LGB, LGB_BO, params, mt, ft, transformers, regressor_model\n    gc.collect();\n\n\n    bounds_LGB = {\n        'max_depth': (11, 14),\n        'lambda_l1': (0, 10),\n        'lambda_l2': (0, 10),\n        'bagging_fraction': (0.7, 1),\n        'bagging_freq': (1, 10),\n        'colsample_bytree': (0.7, 1),\n        'learning_rate': (0.08, 0.2)\n    }\n\n    LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=1030)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'eval_metric': 'cappa',\n        'n_jobs': -1,\n        'seed': 42,\n        'early_stopping_rounds': 100,\n        'n_estimators': 2000,\n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']),\n        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'],\n        'bagging_freq': int(LGB_BO.max['params']['bagging_freq']),\n        'colsample_bytree': LGB_BO.max['params']['colsample_bytree'],\n        'verbose': 100\n    }\n\n    mt = MainTransformer()\n    ft = FeatureTransformer()\n    transformers = {'ft': ft}\n    regressor_model = RegressorModel(model_wrapper=LGBWrapper_regr())\n    regressor_model.fit(X=train, \n                        y=y, \n                        folds=folds, \n                        params=params, \n                        preprocesser=mt, \n                        transformers=transformers,\n                        eval_metric='cappa', \n                        cols_to_drop=cols_to_drop)\n\n    pr2 = regressor_model.predict(train)\n    optR = OptimizedRounder()\n    optR.fit(pr2.reshape(-1,), y)\n    coefficients2 = optR.coefficients()\n\n    preds_2 = regressor_model.predict(test)\n    w_2 = LGB_BO.max['target']\n\n    del bounds_LGB, LGB_BO, params, mt, ft, transformers, regressor_model\n    gc.collect();\n\n    preds = (w_1/(w_1+w_2)) * preds_1 + (w_2/(w_1+w_2)) * preds_2\n\n    del preds_1, preds_2\n    gc.collect();\n\n    coefficients = np.mean([coefficients1, coefficients2], axis=0) # [1.12232214, 1.73925866, 2.22506454]\n    print('Coefs: ', ', '.join(coefficients))\n    preds[preds <= coefficients[0]] = 0\n    preds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\n    preds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\n    preds[preds > coefficients[2]] = 3\n\n    test['accuracy_group'] = preds.astype(int)\n    sample_submission_df = sample_submission_df.drop('accuracy_group', axis=1).merge(\n        test[['installation_id', 'accuracy_group']]\n    )\n    sample_submission_df.to_csv('submission.csv', index=False)\n    print('sample_submission_df')\n    display(sample_submission_df['accuracy_group'].value_counts(normalize=True))\n    print('train distrb')\n    train['accuracy_group'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# test "},{"metadata":{},"cell_type":"raw","source":"fold_n = 4\nvalid_index = train[\n    train['KFold_Group']==fold_n\n][['installation_id', 'timestamp']].groupby('installation_id').idxmax()['timestamp']\ntrain_index = train.drop(valid_index).index\n\nX_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\ny_train, y_valid = train[target].iloc[train_index], train[target].iloc[valid_index]\n\nX_train.reset_index(drop=True, inplace=True)\nX_valid.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\ny_valid.reset_index(drop=True, inplace=True)\ncols_to_drop = list(set(train.columns)-set(features))\n\nimport warnings\n# warnings.simplefilter('ignore')\nwarnings.filterwarnings(\"ignore\")\n\nparams = {\n    'n_estimators':2000,\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'subsample': 0.75,\n    'subsample_freq': 1,\n    'learning_rate': 0.04,\n    'feature_fraction': 0.75,\n    'max_depth': 32,\n    'lambda_l1': 0.75,  \n    'lambda_l2': 0.75,\n    'verbose': 100,\n    'early_stopping_rounds': 100,\n    'eval_metric': 'cappa'\n}\n\nmt = MainTransformer()\nft = FeatureTransformer()\ntransformers = {'ft': ft}\nregressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\nregressor_model1.fit(\n    X=X_train,\n    y=y_train,\n    folds=folds,\n    params=params,\n    preprocesser=mt,\n    transformers=transformers,\n    eval_metric='cappa',\n    cols_to_drop=cols_to_drop\n)\n\n\n# %%time\npr_train_val = regressor_model1.predict(X_train)\noptR = OptimizedRounder()\noptR.fit(pr_train_val.reshape(-1,), y_train)\ncoefficients = optR.coefficients()\n\nopt_preds = optR.predict(pr_train_val.reshape(-1, ), coefficients)\n\n# some coefficients calculated by me.\npr_val = regressor_model1.predict(X_valid)\n# pr1[pr1 <= 1.12232214] = 0\n# pr1[np.where(np.logical_and(pr1 > 1.12232214, pr1 <= 1.73925866))] = 1\n# pr1[np.where(np.logical_and(pr1 > 1.73925866, pr1 <= 2.22506454))] = 2\n# pr1[pr1 > 2.22506454] = 3\n\nprint(', '.join(['Coeff '+str(i)+': ' + format(c, '.2f') for i, c in enumerate(coefficients)]))\n\npr_val[pr_val <= coefficients[0]] = 0\npr_val[np.where(np.logical_and(pr_val > coefficients[0], pr_val <= coefficients[1]))] = 1\npr_val[np.where(np.logical_and(pr_val > coefficients[1], pr_val <= coefficients[2]))] = 2\npr_val[pr_val > coefficients[2]] = 3\n\nprint('val qwk:', qwk(y_valid, pr_val.astype(int))[0])\nX_valid['y_pred']=pr_val.astype(int)\n\nX_valid = X_valid[['installation_id', 'world', 'Assessment_id', 'name_assessment', 'accuracy_group', 'y_pred', 'correct', 'uncorrect']].copy()\n# X_valid['CountAssessments'] = X_valid.groupby('installation_id')['name_assessment'].transform('count')\n# X_valid.to_excel('val_predict.xlsx', index=None)\n\n\nfor a in sorted(X_valid.Assessment_id.unique())[:9]:\n    print(\n        'Count Assessments', int(a),':',\n        format( qwk(X_valid[X_valid['Assessment_id']==a]['accuracy_group'], X_valid[X_valid['Assessment_id']==a]['y_pred']), '.1%'),\n        ', shape:',X_valid[X_valid['Assessment_id']==a].shape[0]\n    )"},{"metadata":{},"cell_type":"raw","source":"@jit\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n    e = e / a1.shape[0]\n    return 1 - o / e\n\ndef run_model(comp_train_df, comp_test_df, features, target, params):\n    print('Run model')\n    num_splits = N_CLUSTER\n    oof_pred = np.zeros((len(comp_train_df), 4))\n    y_pred = np.zeros((len(comp_test_df), 4))\n    all_val_test_index = []\n    for fold in sorted(train['KFold_Group'].astype('int8').unique()):\n\n        val_test_index = train[\n            train['KFold_Group']==fold\n        ][['installation_id', 'timestamp']].groupby('installation_id').idxmax()['timestamp']\n        val_train_index = train.drop(val_test_index).index\n        # print(type(val_train_index))\n        x_train_val, y_train_val = comp_train_df.loc[val_train_index][features], comp_train_df.loc[val_train_index][target]\n        x_test_val, y_test_val = comp_train_df[train['KFold_Group']==fold].loc[val_test_index][features], comp_train_df[train['KFold_Group']==fold].loc[val_test_index][target]\n\n        train_set = lgb.Dataset(x_train_val, y_train_val)\n        val_set = lgb.Dataset(x_test_val, y_test_val)\n\n        model = lgb.train(\n            params,\n            train_set,\n            num_boost_round = 10000,\n            # early_stopping_rounds = 100, \n            valid_sets=[train_set, val_set],\n            verbose_eval = 100\n        )\n\n        oof_pred[val_test_index] = model.predict(x_test_val)\n        y_pred += model.predict(comp_test_df[features]) / num_splits\n\n        val_crt_fold = qwk3(y_test_val, oof_pred[val_test_index].argmax(axis = 1))\n        print(f'Fold: {fold+1} quadratic weighted cappa score: {np.round(val_crt_fold,4)}')\n        all_val_test_index = all_val_test_index + list(val_test_index)\n\n    res = qwk3(comp_train_df.ix[all_val_test_index]['accuracy_group'], oof_pred[all_val_test_index].argmax(axis = 1))\n    print(f'Quadratic weighted score: {np.round(res,4)}')\n        \n    return y_pred, res"},{"metadata":{},"cell_type":"raw","source":"# function for search params\ndef iterating_over_parameters_model(\n    features_calc,\n    name_metric,\n    func_calc,\n    params,\n    iterating_over_parameters\n):\n    best_params_values = {}\n    results = pd.DataFrame()\n    for key in iterating_over_parameters.keys():\n        for param_value in iterating_over_parameters[key]:\n            print('search best param for', key, ', param value =', param_value)\n            params[key] = param_value\n            y_pred, weighted_score = func_calc(train, test, features_calc, target, params)\n            calc_df = pd.DataFrame.from_dict([params])\n            calc_df[name_metric] = weighted_score\n            calc_df['param_key_changed'] = key\n            results = pd.concat([results, calc_df], axis=0, sort=False)    \n\n        best_params_values[key] = results[results['param_key_changed']==key].sort_values(name_metric, ascending=[False])[key].values.astype(type(param_value))[0]\n        print('best param for', key, ', with param value =', best_params_values[key])\n        # fix best param in params\n        params[key] = best_params_values[key]\n    \n    return results, params, best_params_values\n\n\n# base params\nparams = {\n    'n_estimators':1e6,\n    'boosting_type': 'gbdt',\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'subsample': 0.10,\n    'subsample_freq': 5,\n    'learning_rate': 0.04,\n    'feature_fraction': 0.1,\n    'max_depth': 8,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'verbose': 100,\n    'early_stopping_rounds': 100,\n    'eval_metric': 'cappa'\n}\n\n# search params in\niterating_over_parameters = {\n    'max_depth':[i for i in range(4, 20, 4)],\n    # 'n_estimators':[1e2, 1e3],\n    # 'boosting_type': ['gbdt'],\n    'subsample': [0.05, 0.1, 0.15, 0.2, 0.25], # \n    'num_leaves':[25, 50, 150, 250, 350, 450],\n    # 'subsample_freq': 5,\n    # 'learning_rate': 0.04,\n    'feature_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],\n}\n\n# start iterating\ndf_results_iterating, best_params_model, best_iterating_over_parameters = iterating_over_parameters_model(\n    features_calc=features,\n    name_metric='weighted_cappa',\n    func_calc=run_model,\n    params=params,\n    iterating_over_parameters=iterating_over_parameters\n)"},{"metadata":{},"cell_type":"raw","source":"best_params_model = {\n    'n_estimators': 1000000,\n    'boosting_type': 'gbdt',\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'subsample': 0.25,\n    'subsample_freq': 5,\n    'learning_rate': 0.04,\n    'feature_fraction': 0.2,\n    'max_depth': 8,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'verbose': 100,\n    'early_stopping_rounds': 100,\n    'eval_metric': 'cappa',\n    'num_leaves': 25\n}\n\ny_pred, weighted_score = run_model(train, test, features, target, best_params_model)\ntest['accuracy_group'] = y_pred.argmax(axis=1)\nsample_submission_df = sample_submission_df[['installation_id']].merge(test[['installation_id', 'accuracy_group']], how='left')\nsample_submission_df.to_csv('submission.csv', index=False)\nsample_submission_df.groupby('accuracy_group')['installation_id'].count()"},{"metadata":{},"cell_type":"raw","source":"accuracy_features_map = {}\n\n# sorted(features_calc)\ncore_features = [\n    'accuracy_calc_shift', \n    'world_CRYSTALCAVES', 'world_MAGMAPEAK', 'world_TREETOPCITY',\n    'uncorrect_mean_all_users', 'correct_mean_all_users',\n    'name_assessment_Bird Measurer (Assessment)', 'name_assessment_Cart Balancer (Assessment)', 'name_assessment_Chest Sorter (Assessment)', 'name_assessment_Mushroom Sorter (Assessment)',\n    'assessment_game_time_shift',\n    'correct_cumsum_shift','uncorrect_cumsum_shift',    \n    'correct_user_cumsum_shift', 'uncorrect_user_cumsum_shift'\n]\n\nother_features = list(set(features)-set(core_features))\n\nbest_accuracy = .0\nresults = pd.DataFrame()\n\nfor f in tqdm_notebook(other_features):\n    try_features = core_features+[f]\n    \n    y_pred, weighted_score = run_model(train, test, try_features, target, best_params_model)\n    accuracy_features_map['features'] = try_features\n    accuracy_features_map['score'] = weighted_score\n    \n    if weighted_score>best_accuracy:\n        best_accuracy = weighted_score\n        core_features = core_features+[f]\n        results = pd.concat(\n            [\n                results,\n                pd.DataFrame.from_dict([accuracy_features_map])\n            ],\n            axis=0,\n            sort=False\n        )\n\n# features\n# results.sort_values('score', ascending=False).values[0][0]\n# core_features"},{"metadata":{},"cell_type":"raw","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}