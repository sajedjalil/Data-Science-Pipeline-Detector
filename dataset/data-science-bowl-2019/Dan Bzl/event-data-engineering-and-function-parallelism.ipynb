{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h3>**Parallelization of the assessments function**</h3>\n<p>\nIn data science speed is very important because sometimes we have to experiment a lot to find out the best solution for the problem we are trying to solve.\nThat is where the (data-) parallization comes in. It reduces the computation time of a function so that we can try other experiments if we are not satisfied with the result.\n</p>\n<p>\n    After parallelizing the function I could reduce the computation time of the \"add_results(groupedBy, filename)\" method by 75%.\n</p>\n\n<h4 style=\"color:Blue\">\n    **Vote up if you think it is helpfull**\n</h4>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov 11 10:25:56 2019\n\n@author: D. Mp. Bazola\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, precision_recall_curve, auc, make_scorer, confusion_matrix, f1_score, fbeta_score\nimport dask.dataframe as dd\nimport datetime as dt\nfrom sklearn.compose import ColumnTransformer\nimport pickle\nimport os\nimport json\nimport time\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.manifold import MDS\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import cohen_kappa_score\n\nfrom sklearn.metrics import balanced_accuracy_score\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import ADASYN\nfrom dask import delayed # to allow parallel computation\nimport multiprocessing as mp\n\nimport lightgbm\nimport math\nfrom tqdm import tqdm_notebook as tqdm\nfrom multiprocessing import Process, Manager, Queue\nimport multiprocessing as mp\n\n\ncpu_n = mp.cpu_count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nspecs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\nev_ids = specs['event_id'].values\n\n\ntrain_raw = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train.csv\", #nrows=500000,\n                         dtype={\n                                 'event_id': np.object,\n                                 'game_session': np.object,\n                                 'event_data': np.object,\n                                 'installation_id': np.object,\n                                 'event_count': np.int64,\n                                 'event_code': np.int64,\n                                 'game_time': np.int64,\n                                 'title': np.object,\n                                 'type': np.object,\n                                 'world': np.object\n                                }, low_memory=False)\n#timestamp\ntrain_raw['timestamp'] = pd.to_datetime(train_raw['timestamp'], errors='coerce')\n#train_raw['year'] = train_raw.timestamp.dt.year\ntrain_raw['hours'] = train_raw.timestamp.dt.hour\n#train_raw['day'] = train_raw.timestamp.dt.day\n#train_raw['month'] = train_raw.timestamp.dt.month \n#train_raw['min'] = train_raw.timestamp.dt.minute\ntrain_raw['dayofweek'] = train_raw.timestamp.dt.day_name()\n# \nprint('big train shape', train_raw.shape)\n#np.random.seed(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('reading test data set - started -')\npred_raw = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/test.csv\",\n                         dtype={\n                                 'event_id': np.object,\n                                 'game_session': np.object,\n                                 'event_data': np.object,\n                                 'installation_id': np.object,\n                                 'event_count': np.int64,\n                                 'event_code': np.int64,\n                                 'game_time': np.int64,\n                                 'title': np.object,\n                                 'type': np.object,\n                                 'world': np.object\n                                }, low_memory=False)\n#timestamp\npred_raw['timestamp'] = pd.to_datetime(pred_raw['timestamp'], errors='coerce')\n#pred_raw['year'] = pred_raw.timestamp.dt.year\npred_raw['hours'] = pred_raw.timestamp.dt.hour\n#pred_raw['day'] = pred_raw.timestamp.dt.day\n#pred_raw['month'] = pred_raw.timestamp.dt.month\n#pred_raw['min'] = pred_raw.timestamp.dt.minute\npred_raw['dayofweek'] = pred_raw.timestamp.dt.day_name()\nprint('reading test data set - finish -')\n#","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n##########################################################################################################\nglobal df_main\nglobal df_main_columns\n#\ndf_main_columns = ['accuracy_group', 'event_id', 'game_session', 'timestamp', \n                   'event_data', 'installation_id', 'event_count', 'event_code', 'game_time', 'title', \n                  'type', 'world', 'hours', 'dayofweek']\n\n\ntitles = ['12 Monkeys', 'Air Show', 'All Star Sorting', 'Balancing Act', 'Bird Measurer (Assessment)', \n          'Bottle Filler (Activity)', 'Bubble Bath', 'Bug Measurer (Activity)', 'Cart Balancer (Assessment)', \n          'Cauldron Filler (Assessment)', 'Chest Sorter (Assessment)', 'Chicken Balancer (Activity)', \n          'Chow Time', 'Costume Box', 'Crystal Caves - Level 1', 'Crystal Caves - Level 2', \n          'Crystal Caves - Level 3', 'Crystals Rule', 'Dino Dive', 'Dino Drink', 'Egg Dropper (Activity)', \n          'Fireworks (Activity)', 'Flower Waterer (Activity)', 'Happy Camel', 'Heavy, Heavier, Heaviest', \n          'Honey Cake', 'Leaf Leader', 'Lifting Heavy Things', 'Magma Peak - Level 1', 'Magma Peak - Level 2', \n          'Mushroom Sorter (Assessment)', 'Ordering Spheres', 'Pan Balance', \"Pirate's Tale\", 'Rulers', \n          'Sandcastle Builder (Activity)', 'Scrub-A-Dub', 'Slop Problem', 'Treasure Map', \n          'Tree Top City - Level 1', 'Tree Top City - Level 2', 'Tree Top City - Level 3', \n          'Watering Hole (Activity)', 'Welcome to Lost Lagoon!']\n\n\n'''\ntrain_labels_raw = pd.read_csv(\"/kaggle/input/data-science-bowl-2019/train_labels.csv\", \n                         dtype={\n                                 'game_session': np.object,\n                                 'installation_id': np.object,\n                                 'title': np.object,\n                                 'num_correct': np.int64,\n                                 'num_incorrect': np.int64,\n                                 'accuracy': np.float64,\n                                 'accuracy_group': np.int64\n                                }, low_memory=False)\n\n'''\n\n#__________________________________________________________________________________________________\n\n\nevent_data_incorrect = ['df4fe8b6', 'd88e8f25', 'c277e121', '160654fd', 'ea296733', '5859dfb6', \n      'e04fb33d', '28a4eb9a', '7423acbc', 'e57dd7af', '04df9b66', '2230fab4', \n      'c51d8688', '1af8be29', '89aace00', '763fc34e', '5290eab1', '90ea0bac', \n      '8b757ab8', 'e5734469', '9de5e594', 'd45ed6a1', 'ac92046e', 'ad2fc29c', \n      '5de79a6a', '88d4a5be', '907a054b', 'e37a2b78', '31973d56', '44cb4907', \n      '0330ab6a', '3bf1cf26']\n#\nevent_data_correct = ['df4fe8b6', 'd88e8f25', 'c277e121', '160654fd', 'ea296733', '5859dfb6', \n      'e04fb33d', '28a4eb9a', '7423acbc', 'e57dd7af', '04df9b66', '2230fab4', \n      'c51d8688', '1af8be29', '89aace00', '763fc34e', '5290eab1', '90ea0bac', \n      '8b757ab8', 'e5734469', '9de5e594', 'd45ed6a1', 'ac92046e', 'ad2fc29c', \n      '5de79a6a', '88d4a5be', '907a054b', 'e37a2b78', '31973d56', '44cb4907', \n      '0330ab6a', '3bf1cf26', 'df4fe8b6', 'd88e8f25', 'c277e121', '160654fd', \n      'ea296733', '5859dfb6', 'e04fb33d', '28a4eb9a', '7423acbc', 'e57dd7af', \n      '04df9b66', '2230fab4', 'c51d8688', '1af8be29', '89aace00', '763fc34e', \n      '5290eab1', '90ea0bac', '8b757ab8', 'e5734469', '9de5e594', 'd45ed6a1', \n      'ac92046e', 'ad2fc29c', '5de79a6a', '88d4a5be', '907a054b', 'e37a2b78', \n      '31973d56', '44cb4907', '0330ab6a', '3bf1cf26']\n#\n \n\n\n# ASSESSMENTS\ndef add_results(groupedBy, filename):\n    print('add_results - started -')\n    assessments = groupedBy[((groupedBy['event_code']==4100) & (groupedBy['type']=='Assessment') & (groupedBy['title']!='Bird Measurer (Assessment)')) | ((groupedBy['event_code']==4110) & (groupedBy['type']=='Assessment') & (groupedBy['title']=='Bird Measurer (Assessment)'))]  \n    #\n    for column in assessments:\n        if column == 'event_data':\n            correct_df = pd.io.json.json_normalize(assessments[column].apply(json.loads))\n            assessments['num_correct'] = [1 if c==True else 0 for c in correct_df['correct']]\n            assessments['num_incorrect'] = [1 if c==False else 0 for c in correct_df['correct']]\n    #    \n    df_main_columns = assessments.columns.values\n    print('add_results - finish -')\n    return make_assessment2(assessments, filename)\n#\n\n\n\n\ndef make_assessment2(assessments, filename):\n    #print('make_assessment - started -')\n    df_main = pd.DataFrame(columns=df_main_columns)\n    users = np.unique(assessments.index.get_level_values(0))\n    for idx,u in tqdm(enumerate(users), total=len(users)):\n        gss = np.unique(assessments.loc[u].index.get_level_values(0))\n        for idxx,ii in enumerate(gss):\n            #SUM THEM\n            corrects = list()\n            in_corrects = list() \n            in_corrects.append(assessments.loc[(u,ii), 'num_incorrect'].sum())\n            corrects.append(assessments.loc[(u,ii), 'num_correct'].sum())  \n            #\n            new_df = assessments.loc[(u,ii)].tail(1)\n            if isinstance(assessments.loc[(u,ii)], pd.Series):\n                new_df = assessments.loc[(u,ii)].to_frame().T\n            #\n            # LABEL THEM\n            new_df[\"accuracy_group\"] = np.where((np.asarray(corrects) == 1) & (np.asarray(in_corrects) == 0), 3,\n            np.where((np.asarray(corrects) == 1) & (np.asarray(in_corrects) == 1), 2,\n            np.where((np.asarray(corrects) == 1) & (np.asarray(in_corrects) > 1), 1, 0)))\n            #\n            df_main = df_main.append(new_df, ignore_index=True, sort=False)\n            #\n        #\n        df_main = add_assessment_count_cumsum(df_main)   \n        #\n        #print('*', (idx + 1), '/', len(users), '*')\n    # \n    df_main.reset_index(drop=True, inplace=True)\n    #\n    return df_main\n\n\ndef make_assessment(groupedBy, filename):\n    print('make_assessment - starting... -')\n    df_main = pd.DataFrame(columns=df_main_columns)\n    #print('groupedBy.index', groupedBy.index)\n    l = [i for i,g in groupedBy.index]\n    ll = list(dict.fromkeys(l))\n    for ii in ll:\n        #SUM THEM\n        corrects = list()\n        in_corrects = list() \n        in_corrects.append(groupedBy['num_incorrect'][ii].sum())\n        corrects.append(groupedBy['num_correct'][ii].sum())\n        #add new rows to main dataframe\n        new_df = groupedBy.loc[ii].tail(1)\n        # LABEL THEM\n        new_df[\"accuracy_group\"] = np.where((np.asarray(corrects) == 1) & (np.asarray(in_corrects) == 0), 3,\n        np.where((np.asarray(corrects) == 1) & (np.asarray(in_corrects) == 1), 2,\n        np.where((np.asarray(corrects) == 1) & (np.asarray(in_corrects) > 1), 1, 0)))\n        #append that row\n        df_main = df_main.append(new_df, ignore_index=True, sort=False)\n    #\n    df_main.reset_index(drop=True, inplace=True)\n    df_main = df_main.drop([v for v in df_main.columns.values if v not in df_main_columns], 1)\n    #print(df_main['accuracy_group'])\n    #history_old *starts*\n    pd.DataFrame(df_main).to_csv(filename + '.csv', encoding='utf-8', index=False) #\n    print('history_old file created!')\n    #history_old *ends*\n    print('make_assessment - finish -')\n    return df_main\n    #return df_main\n\n\n\ndef train_set_gen_cat(user, start_date, end_date):\n    tsg_df = pd.DataFrame(columns=['title'])\n    #\n    ranged_user = user[(user.index >= start_date) & (user.index <= end_date)]\n    ranged_user['timestamp_diff'] = (end_date - start_date).total_seconds()\n    ranged_user = ranged_user.reset_index(drop=True)\n    #\n    aggregations = {\n        'game_time': ['mean'],\n        'event_count': ['mean']\n    }          \n    #\n    tsg_df=tsg_df.append(super_df_title).reset_index(drop=True)\n    return tsg_df\n\n\n\n \n\n\ndef train_set_gen(train_raw, history, filename):  \n    t1 = time.time()\n    print('train_set_gen - starting... -')\n    tsg_df = pd.DataFrame(columns=['game_time', 'event_count', 'event_code'])\n    print('userData reading...')\n    userData = train_raw.groupby(['installation_id']).apply(lambda x: x.reset_index(drop=True))\n    print('userData finish')\n    #\n    for idx, user_history_value in tqdm(enumerate(np.array(history.values)), total=history.shape[0]): \n        if filename == 'test':\n            user_id = user_history_value[0]\n            game_session = user_history_value[2]\n        else:\n            user_id = user_history_value[5]\n            game_session = user_history_value[2]\n        #\n        user = userData.loc[user_id] #get the first action \n        user = user.set_index(pd.DatetimeIndex(user['timestamp']))\n        #start and end date to get users dataframe before the assessment\n        start_date = user.index[0] #get the first action timestamp\n        end_date = (user.loc[((user.game_session==game_session)&(user.event_count==1)&(user.event_code==2000)&(user.game_time==0))]['timestamp'])[0]\n        #get the users dataframe before the assessment\n        ranged_user = user[(user.index >= start_date) & (user.index <= end_date)]\n        ranged_user['timestamp_diff'] = (end_date - start_date).total_seconds()\n        ranged_user = ranged_user.reset_index(drop=True)\n        #\n        aggregations = {\n            'game_time': ['mean'],\n            'event_count': ['mean'],\n            'title': ['count']\n        }\n        #\n        ############### numeric and title ###########\n        super_df = (ranged_user.groupby(['installation_id']).agg(aggregations).reset_index(drop=True))\n        super_df.columns = [\"_\".join(x) for x in super_df.columns.ravel()]\n        #\n        for n in ['title', 'type', 'world', 'event_code', 'event_id']:\n            for agg2 in [{'game_time': ['mean']}, {'event_count': ['mean']}, {'title': ['count']}]:\n                name = ranged_user.groupby([n]).agg(agg2)\n                name = pd.DataFrame(name).transpose()  \n                name.columns = [''.join(n)+\"_\"+str(x).replace(\" \", \"_\") for x in name.columns.ravel()]  \n                name=name.reset_index(drop=True) \n                super_df[name.columns.values] = name\n        \n        ############### categories #################\n        for n in ['title', 'type', 'world', 'event_code', 'event_id']:\n            name = ranged_user.groupby([n]).agg(aggregations).mean()  \n            name = pd.DataFrame(name).transpose()  \n            name.columns = [n+\"_\".join(x) for x in name.columns.ravel()]  \n            super_df[name.columns.values] = name \n        #\n        for n in [['title', 'type', 'world'],['title', 'world'],['title', 'type'], ['event_code', 'event_id'], ['event_id', 'world'], ['event_code', 'world'],['title', 'event_code'],['title', 'event_id'], ['type', 'event_id'], ['type', 'event_code']]:\n            name = ranged_user.groupby(n).agg(aggregations).mean()  \n            name = pd.DataFrame(name).transpose()  \n            name.columns = [''.join(n)+\"_\".join(str(x)) for x in name.columns.ravel()]  \n            super_df[name.columns.values] = name \n        #\n        super_df['timestamp_diff_mean'] = ranged_user['timestamp_diff'].mean()\n        #\n        ############### another method -Done!- #################\n        \n        \n        ############## event data generation starts ##################\n        user_ev_data = pd.io.json.json_normalize(ranged_user.event_data.apply(json.loads))\n        user_ev_data = pd.DataFrame(user_ev_data)\n        col_list = user_ev_data.columns.values \n        for c in col_list:\n            l_list = user_ev_data[c].values.tolist()\n            arr = []\n            for l in l_list:\n                if isinstance(l, list):\n                    l = ''.join(str(e) for e in l)\n                arr.append(l)\n            user_ev_data[c] = arr\n            ev_da_list = user_ev_data[c].value_counts().values\n            #\n            super_df['ev_data_mean_'+c] = sum(ev_da_list)/len(ev_da_list) \n            super_df['ev_data_count_'+c] = len(ev_da_list) \n            super_df['ev_data_sum_'+c] = sum(ev_da_list) \n            super_df['ev_data_max_'+c] = np.amax(ev_da_list) \n            super_df['ev_data_min_'+c] = np.amin(ev_da_list) \n        ############## event data generaton - ends - #################\n                \n        #append the row\n        tsg_df=tsg_df.append(super_df).reset_index(drop=True)\n        #tsg_df=tsg_df.dropna(axis='columns')\n        tsg_df=tsg_df.fillna(0)\n        #\n        #print((idx + 1), ' von ', history.shape[0])\n        #\n        if filename == 'train':\n            pd.DataFrame(tsg_df).to_csv('train_set_gen' + '.csv', encoding='utf-8', index=False) #\n    print('train_set_gen - finish -')\n    print(time.time() - t1)\n    return tsg_df\n\n\n\ndef train_set_ev_data_gen(train_raw, history, filename):  \n    t1 = time.time()\n    print('train_set_gen - starting... -')\n    tsg_df = pd.DataFrame(columns=['timestamp_diff'])\n    print('userData reading...')\n    userData = train_raw.groupby(['installation_id']).apply(lambda x: x.reset_index(drop=True))\n    print('userData finish')\n    #\n    for idx, user_history_value in tqdm(enumerate(np.array(history.values)), total=history.shape[0]): \n        if filename == 'test':\n            user_id = user_history_value[0]\n            game_session = user_history_value[2]\n        else:\n            user_id = user_history_value[5]\n            game_session = user_history_value[2]\n        #\n        user = userData.loc[user_id] #get the first action \n        user = user.set_index(pd.DatetimeIndex(user['timestamp']))\n        #start and end date to get users dataframe before the assessment\n        start_date = user.index[0] #get the first action timestamp\n        end_date = (user.loc[((user.game_session==game_session)&(user.event_count==1)&(user.event_code==2000)&(user.game_time==0))]['timestamp'])[0]\n        #get the users dataframe before the assessment\n        ranged_user = user[(user.index >= start_date) & (user.index <= end_date)]\n        ranged_user['timestamp_diff'] = (end_date - start_date).total_seconds()\n        ranged_user = ranged_user.reset_index(drop=True)\n        #\n        super_df = pd.DataFrame(data=[ranged_user['timestamp_diff'].mean()], columns=['timestamp_diff_mean'])\n        #\n        user_ev_data = pd.io.json.json_normalize(ranged_user.event_data.apply(json.loads))\n        user_ev_data = pd.DataFrame(user_ev_data)\n        col_list = user_ev_data.columns.values \n        #\n        for c in col_list:\n            l_list = user_ev_data[c].values.tolist()\n            arr = []\n            for l in l_list:\n                if isinstance(l, list):\n                    l = ''.join(str(e) for e in l)\n                arr.append(l)\n            user_ev_data[c] = arr\n            #print(user_ev_data[c].values.tolist())\n            ev_da_list = user_ev_data[c].value_counts().values\n            #\n            super_df['ev_data_mean_'+c] = sum(ev_da_list)/len(ev_da_list) \n            super_df['ev_data_count_'+c] = len(ev_da_list) \n            super_df['ev_data_sum_'+c] = sum(ev_da_list) \n            super_df['ev_data_max_'+c] = np.amax(ev_da_list) \n            super_df['ev_data_min_'+c] = np.amin(ev_da_list) \n         \n        ############### another method -Done!- #################\n        #append the row\n        tsg_df=tsg_df.append(super_df).reset_index(drop=True)\n        #tsg_df=tsg_df.dropna(axis='columns')\n        tsg_df=tsg_df.fillna(0)\n        #\n        #print((idx + 1), ' von ', history.shape[0])\n        #\n        if filename == 'train':\n            pd.DataFrame(tsg_df).to_csv('train_set_event_data' + '.csv', encoding='utf-8', index=False) #\n        #\n    print(tsg_df.columns.values)\n    print(tsg_df)\n    print('train_set_gen - finish -')\n    print(time.time() - t1)\n    return tsg_df\n\n\ndef train_parallel_feat_gen(x):\n    return train_set_gen(train_raw, x,'train')\n\ndef test_parallel_feat_gen(x):\n    return train_set_gen(train_raw,x,'test')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n##RUN ASSEMENTS METHODS \n####################################################### \n#history_old = add_results(train_raw.groupby(['game_session']).apply(lambda x: x.reset_index()), 'history')#history data set\nhistory_old = pd.read_csv(\"/kaggle/input/historiaa/history.csv\")\nprint('history_old', history_old.shape)\n#######################################################\ntsg_res = pd.read_csv(\"/kaggle/input/train-feat-gen/train_feat_gen.csv\")\n#tsg_res = train_set_gen(train_raw, history_old, 'train')\n#######################################################\n#event_data_df = train_set_ev_data_gen(train_raw, history_old, 'train')#1090\nevent_data_df = pd.read_csv(\"/kaggle/input/event-data-df/event_data_df.csv\")\n\n#########################################################\ntrain_raw = history_old\n#########################################################\n#\n#tsg_res = tsg_res.drop(['game_time', 'event_count'], 1)\n#\ntrain_raw[tsg_res.columns.values] = tsg_res\ntrain_raw[event_data_df.columns.values] = event_data_df\n######################################################### \n\n\ntrain_raw = train_raw.dropna(0)\nprint('- TRAINSET - ', train_raw.shape)\n#print(train_raw.head())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_cumsum_columns(df):\n    df['accuracy_group_cumsum'] = df.groupby(['installation_id'])['accuracy_group'].apply(lambda x: x.cumsum())\n    df['game_time_cumsum'] = df.groupby(['installation_id'])['game_time'].apply(lambda x: x.cumsum())\n    df['event_count_cumsum'] = df.groupby(['installation_id'])['event_count'].apply(lambda x: x.cumsum())#\n    return df\n\n\ndef add_assessment_count_cumsum(df):\n    types = np.unique(df['type'].values).tolist()\n    titles = np.unique(df['title'].values).tolist()\n    worlds = np.unique(df['world'].values).tolist()\n    hours_=[str(v) for v in np.unique(df['hours'].values).tolist()]\n    dayofweek_ = np.unique(df['dayofweek'].values).tolist()\n    \n    for c in types:\n        df['counter_'+c] = 1\n        df['counter_cumsum_'+c] = df.groupby(['installation_id'])['counter_'+c].apply(lambda x: x.cumsum())\n        df = df.drop(['counter_'+c],1)\n    #\n    return df\ntrain_raw = add_assessment_count_cumsum(train_raw)\n\n\n\ndef drop_multi_colinear_features(df):\n    print('dropping multi colinear features...')\n    # Create correlation matrix\n    corr_matrix = df.corr().abs()\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    # Find features with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.995)]\n    # Drop features \n    print('multi colinear features dropped!')\n    print('to_drop', to_drop)\n    df = df.drop(to_drop, axis=1)\n    return df\n\n###################################################\nprint('Remove colinear features')\n#train_raw = drop_multi_colinear_features(train_raw)\n###################################################\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#FEATURE ENGINEERING: TRAIN SET\nnot_to_labels = ['accuracy_group', 'game_time', 'event_count', 'installation_id', 'event_code']  \nnot_to_labels += tsg_res.columns.values.tolist()\n#\nno_trainable_features = ['game_session', 'accuracy_group', 'timestamp', 'event_id', 'event_data', 'event_count', 'event_code', 'game_time'] #'event_id', 'event_data', 'title' \n\n#\nle_title = LabelEncoder()\nle_type = LabelEncoder()\nle_world = LabelEncoder()\nle_event_id = LabelEncoder()\nle_event_data = LabelEncoder()\nle_dayofweek = LabelEncoder()\nle_game_session = LabelEncoder()\nle_timestamp = LabelEncoder()\nle_installation_id = LabelEncoder()\n#\ntrain_raw.loc[:, 'title'] = le_title.fit_transform(train_raw.loc[:, 'title'])\ntrain_raw.loc[:, 'type'] = le_type.fit_transform(train_raw.loc[:, 'type'])\ntrain_raw.loc[:, 'world'] = le_world.fit_transform(train_raw.loc[:, 'world'])\ntrain_raw.loc[:, 'event_id'] = le_event_id.fit_transform(train_raw.loc[:, 'event_id'])\ntrain_raw.loc[:, 'dayofweek'] = le_dayofweek.fit_transform(train_raw.loc[:, 'dayofweek'])\ntrain_raw.loc[:, 'event_data'] = le_event_data.fit_transform(train_raw.loc[:, 'event_data'])\ntrain_raw.loc[:, 'game_session'] = le_game_session.fit_transform(train_raw.loc[:, 'game_session'])\ntrain_raw.loc[:, 'timestamp'] = le_timestamp.fit_transform(train_raw.loc[:, 'timestamp'])\ntrain_raw.loc[:, 'installation_id'] = le_installation_id.fit_transform(train_raw.loc[:, 'installation_id'])\n\n#\ny = train_raw.loc[:, 'accuracy_group'].values\ny=y.astype('int')\n\n#\nhm_data = train_raw.drop(['installation_id'], 1)\n#\n#regression data\ntrain_data_for_regressor_after_label_encoder = train_raw.copy()\n#\ntrain_raw = train_raw.drop(no_trainable_features, 1)\n#Features\nx_values = train_raw.values\ncols = train_raw.columns.values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### EXPLORATORY DATA ANALYSIS (EDA)\n\ndef qwk_loss(a1, a2):\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n    e = e / a1.shape[0]\n    return 1 - o / e\n\n\n\ndef plotter(x_std, y_train):\n    plt.scatter(x_std[y_train==0, 0], x_std[y_train==0,1], color=\"red\", marker='^', alpha=0.5, label='class 0')\n    plt.scatter(x_std[y_train==1, 0], x_std[y_train==1,1], color=\"blue\", marker='o', alpha=0.5, label='class 1')\n    plt.scatter(x_std[y_train==2, 0], x_std[y_train==2,1], color=\"green\", marker='x', alpha=0.5, label='class 2')\n    plt.scatter(x_std[y_train==3, 0], x_std[y_train==3,1], color=\"darkmagenta\", marker='+', alpha=0.5, label='class 3')\n    plt.xlabel('PC1')\n    plt.ylabel('PC2')\n    plt.legend()\n    plt.show()\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('IMPORTANT FEATURES SELECTION USING RANDOM FOREST -starting...-')\n  \n'''\n##heatmap\nsns.set(font_scale=1.5)\nprint('Korrelationismatrix: Interesse an hohe Korrelationen mit der Zielvariable')\nall_num_data=hm_data.values.astype('int')  \ncm = np.corrcoef(all_num_data.T)\nplt.figure(figsize = (18,10))\nhm = sns.heatmap(cm,cbar=True,annot=True,square=True,fmt='.2f',\n         annot_kws={'size':12},yticklabels=hm_data.columns.values,xticklabels=hm_data.columns.values)\n\n##data description\n##Streudiagrammatrix: Grafische Zusammenfassung \nsns.set(style='whitegrid')\nsns.pairplot(hm_data, height=2.5, hue=\"accuracy_group\", palette=\"husl\") \n'''\n\n\n#FEATURE IMPORTANCE AND SELECTION WITH RANDOM FOREST\nx_labels = [c for c in cols if c != 'accuracy_group']\nforest = RandomForestClassifier(n_estimators=2000, random_state=0,n_jobs=-1)\n\nforest.fit(x_values, y)\n#print('Forest Training',forest.score(x_values, y))\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\nfeat_to_drop = []\nprint('Feature ranking:')\nfor f in range(x_values.shape[1]):\n    #if f > 2:\n    print(\"%2d) %-*s %f\" % (f, 30, x_labels[indices[f]], importances[indices[f]])) #indices[f] 0.036529 bei 0.496\n    if f > 35:\n        feat_to_drop.append(x_labels[indices[f]])\nprint(feat_to_drop)\n#___________________________________________________________#\n\n\n\n'''\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(x_values.shape[1]), importances[indices], color=\"lightblue\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_values.shape[1]), [x_labels[i] for i in indices], rotation=90)\nplt.xlim([-1, x_values.shape[1]])\nplt.tight_layout()\nplt.show()\n#___________________________________________________________#\n'''\n\n\nprint('IMPORTANT FEATURES SELECTION USING RANDOM FOREST -finish-')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IMPORTANT FEATURES \n#FEATURE SELECTION\nprint('selecting best features based on forest random model...')\n#mfs = [300, 600, 900, 1200, None]\n#mfs = [300, 400, 500, 600]\n#mfs = range(80, 100, 2)\n#mfs = range(88, 92, 1)\n#mfs = [10, 20, 30, 40, 50]\n#mfs = [2, 4, 6, 8, 10]\n#mfs = [None]\nmfs = [600]\n#mfs = [400]\n#\nfeaties = []\nfor idmf,mf in enumerate(mfs):\n    sfm = SelectFromModel(forest, threshold=None, max_features=mf, prefit=True)# best score: 0.491\n    x_selected = sfm.transform(x_values)\n    feature_idx = sfm.get_support()\n    feature_names = cols[feature_idx] \n    ##print('feature_names', feature_names)\n    #print('feature_idx', feature_idx)\n    #feature_names = np.append(feature_names, ['installation_id']) \n    featies.append(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get LDA values\n#x_train_std = lda_x\n#x_test_std = lda_x_test\n\n# REGRESSION\nscores = []\ndef predict(models, X_test, averaging: str = 'usual'):\n    full_prediction = np.zeros((X_test.shape[0], 1))\n    for i in range(len(models)):\n        X_t = X_test.copy()\n        y_pred = models[i].predict(X_t).reshape(-1, full_prediction.shape[1])\n        if full_prediction.shape[0] != len(y_pred):\n            full_prediction = np.zeros((y_pred.shape[0], 1))\n        if averaging == 'usual':\n            full_prediction += y_pred\n        elif averaging == 'rank':\n            full_prediction += pd.Series(y_pred).rank().values\n    return full_prediction / len(models)\n\n\ndef get_pred_classes(preds):    \n    coefficients = [1.12232214, 1.73925866, 2.22506454]\n    preds[preds <= coefficients[0]] = 0\n    preds[np.where(np.logical_and(preds > coefficients[0], preds <= coefficients[1]))] = 1\n    preds[np.where(np.logical_and(preds > coefficients[1], preds <= coefficients[2]))] = 2\n    preds[preds > coefficients[2]] = 3\n    return preds.astype(int)\n#\n\nparams = {'verbose': 0,\n          'learning_rate': 0.010514633017309072,\n          'metric': 'rmse',\n          'bagging_freq': 3,\n          'boosting_type': 'gbdt',\n          'eval_metric': 'cappa',\n          'lambda_l1': 4.8999704874480745,\n          'colsample_bytree': 0.4236269531042225,\n          'early_stopping_rounds': 100,\n          'lambda_l2': 0.054084652510602016,\n          'bagging_fraction': 0.7931423220563563,\n          'n_jobs': -1,\n          'n_estimators': 5000,\n          'objective': 'regression',\n          'seed': 42,\n          'num_leaves':5, \n          'max_depth':3,\n          'class_weight':'balanced',\n          'learning_rate':.005}\n#\n\nqwk_means = []\nfor feat_names in featies:\n    #SPLITTING\n    categorical_features = [idx for idx,col in enumerate(feat_names) if col in ['title']]\n    #\n    x_regr_df = train_data_for_regressor_after_label_encoder[feat_names]\n    print('Distribution of classes in training set')\n    y_df = pd.DataFrame(data=y, columns=['accuracy_group'])\n    y_df.hist()\n    #\n    x_train, x_test, y_train, y_test = train_test_split(x_regr_df, y, test_size=0.3, random_state=0)\n    #train and validation set\n    groups_train = x_train[['installation_id']]\n    x_train = x_train.drop(['installation_id'], 1)\n    #the test set\n    groups_test = x_test[['installation_id']]\n    x_test = x_test.drop(['installation_id'], 1)\n    #\n    ##STANDARDISIEREN\n    stdsc = StandardScaler(with_mean=False)\n    ##stdsc = StandardScaler()\n    x_train_std = (stdsc.fit_transform(x_train))\n    x_test_std = (stdsc.transform(x_test))\n    #ONE-HOT-ENCODE wird in dem lightgbm-Modell geregelt\n    #\n    plotter(x_train_std, y_train)\n    #\n    # CROSS VALIDATION\n    folds = GroupKFold(n_splits=8)\n    model_from_folds = []\n    y_pred_from_folds = []\n    cappas = []\n    i=0\n    for train_index, test_index in tqdm(folds.split(x_train_std, y_train, groups_train)):\n        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train_std_fold, X_test_std_fold = x_train_std[train_index], x_train_std[test_index]\n        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n        #\n        plotter(X_train_std_fold, y_train_fold)\n        #\n        print('Distribution of classes in a training set per folder')\n        y_df = pd.DataFrame(data=y_train_fold, columns=['accuracy_group'])\n        y_df.hist()\n        print(y_df['accuracy_group'].value_counts())\n        #\n        print(X_train_std_fold, X_test_std_fold, y_train_fold, y_test_fold)\n        # train and predict\n        train_data = lightgbm.Dataset(X_train_std_fold, label=y_train_fold, categorical_feature=categorical_features)\n        valid_data = lightgbm.Dataset(X_test_std_fold, label=y_test_fold, categorical_feature=categorical_features)\n        # train the model with the train folder set and validate with the validation set\n        model = lightgbm.train(params,train_data, valid_sets=valid_data, num_boost_round=5000, early_stopping_rounds=1000, categorical_feature=categorical_features)\n        #test the model with the test set\n        y_pred_result = model.predict(X_test_std_fold).reshape(-1, 1)\n        y_pred_from_folds.append(y_pred_result)\n        #\n        print('Distribution of prediction classes per folder')\n        y_df = pd.DataFrame(data=y_train_fold, columns=['accuracy_group'])\n        y_df.hist()\n        print(y_df['accuracy_group'].value_counts())\n        #\n        cappa = cohen_kappa_score(y_test_fold, get_pred_classes(y_pred_result), labels=None, weights='quadratic', sample_weight=None)\n        cappas.append(cappa)\n        print('index:', i)\n        print('quadric cohen_kappa_score: %.3f' % cappa)\n        #\n        i += 1\n        model_from_folds.append(model)\n        #\n        \n    #\n    qwk_mean = (sum(cappas)/len(cappas))\n    qwk_means.append(qwk_mean)\n    print('index:', idmf, 'qwk_mean', qwk_mean)\n#\nprint('qwk_means:', qwk_means)\n\n\nfeature_names = [ x for x in feature_names if x != 'installation_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parallel_process_func(chunk, q):\n    pur_t = time.time()\n    n_df = pd.DataFrame(columns=['event_count_cumsum'])\n    q.put(add_results(chunk, 'test'))\n    print(time.time()-pur_t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('function area')\ndef multiprocess(big_array, process_n):\n    q = Queue()\n    process_list = [Process(target=parallel_process_func, args=(chunk,q,)) for chunk in np.array_split(big_array, (process_n))]\n    [p.start() for p in process_list]\n    [results_mp.append(q.get()) for r in process_list]\n    [p.join() for p in process_list]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('reading aggr_test_set...')\naggr_test_set = pred_raw.groupby(['installation_id', 'game_session']).apply(lambda x: x.reset_index(drop=True))\nprint('aggr_test_set read')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RUN multi process feature generation')\nresults_mp = []\nmultiprocess(aggr_test_set, cpu_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('mergin...')\nprint('1. results_mp', results_mp[0].shape)\nprint('2. results_mp', results_mp[1].shape)\nprint('3. results_mp', results_mp[2].shape)\nprint('4. results_mp', results_mp[3].shape)\n#\ndf_concat_res = pd.concat([v for v in results_mp if (v.empty) is False], sort=True)\nprint(df_concat_res.shape)\nprint('dataframes merged')\n#\ndist_cols = ['game_session', 'installation_id', 'timestamp', 'counter_cumsum_Assessment']\ncumsum_df = df_concat_res[dist_cols]\ncumsum_df[['counter_cumsum_Assessment']].astype(float).hist()\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('feature engineering for test data - starting... -')  \n\n#FEATURE ENGINEERING: PRED SET\n######################################################### \ntest_user_history = pred_raw\n#test_user_to_predict\ntest_user_history.sort_values(['installation_id', 'timestamp'], inplace=True)\npred_user_raw = test_user_history.groupby(\"installation_id\").last().reset_index()\nlast_ids = pred_user_raw['installation_id']\n#\nprint(pred_user_raw)\ntsg_res = train_set_gen(pred_raw, pred_user_raw, 'test')\n#\n\n######################################################### \npred_user_raw[tsg_res.columns.values] = tsg_res\n#_______________________________________________________________________________________________#\n\npred_user_raw = pred_user_raw.fillna(0)\n\nprint(pred_user_raw.head())\nprint(pred_user_raw.shape)\nprint('feature engineering for test data - finish -')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_user_raw_copy = pred_user_raw.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cumsum_cols = ['counter_cumsum_Assessment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cumsum_df_copy = cumsum_df.copy()\ncum_ersatz = pd.DataFrame(columns=cumsum_cols, data=[0])\n#\n#get the last assessemtns\ncumsum_df_copy.sort_values(['installation_id', 'timestamp'], inplace=True)\ncumsum_df_copy_last = cumsum_df_copy.groupby(\"installation_id\").last().reset_index()\n#\nmatched_list = [cumsum_df_copy_last.loc[cumsum_df_copy_last['installation_id']==v, cumsum_cols] if v in \n                np.unique(cumsum_df_copy_last['installation_id'].values.tolist()) else cum_ersatz for v in pred_user_raw['installation_id'].values.tolist()]\nmatched_list = pd.concat(matched_list).reset_index(drop=True)\nmatched_list['counter_cumsum_Assessment'] += 1\n#\npred_user_raw = pred_user_raw_copy\npred_user_raw[cumsum_cols] = matched_list\n#\nprint(matched_list)\nmatched_list[cumsum_cols].astype(float).hist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_user_raw[cumsum_cols].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PREDICTION\n#########################################################\nprint('labeled test data features - started -')\nprint('pred_raw-columns:', pred_user_raw.columns.values) \n#label: string to int\npred_user_raw.loc[:, 'title'] = le_title.transform(pred_user_raw.loc[:, 'title'])\npred_user_raw.loc[:, 'type'] = le_type.transform(pred_user_raw.loc[:, 'type'])\npred_user_raw.loc[:, 'world'] = le_world.transform(pred_user_raw.loc[:, 'world'])\n#pred_user_raw.loc[:, 'event_id'] = le_event_id.transform(pred_user_raw.loc[:, 'event_id'])\npred_user_raw.loc[:, 'dayofweek'] = le_dayofweek.transform(pred_user_raw.loc[:, 'dayofweek'])\n#pred_user_raw.loc[:, 'game_session'] = le_game_session.transform(pred_user_raw.loc[:, 'game_session'])\n#pred_user_raw.loc[:, 'timestamp'] = le_timestamp.transform(pred_user_raw.loc[:, 'timestamp'])\n#pred_user_raw.loc[:, 'event_data'] = le_event_data.transform(pred_user_raw.loc[:, 'event_data'])\n#\npred_user_raw = pred_user_raw.drop([v for v in no_trainable_features if v != 'accuracy_group'], 1)\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LIGHTGBM\nprint('LIGHTGBM is starting...')\n#\n#STANDARDISIEREN\nprint(pred_user_raw[feature_names])\nstdsc = StandardScaler(with_mean=False)\npred_x_std = stdsc.fit_transform(pred_user_raw[feature_names]) \n#\npreds = predict(model_from_folds, pred_x_std)\n#\nresultList = get_pred_classes(preds)\nresultList = [v[0] for v in resultList]\n\nprint('length', len(resultList))\n\n\n#SUBMISSION\nprint(last_ids[:500].tolist())\nprint(resultList[:1000])\nprint('predict the future - finish -')\n#\nsub = pd.DataFrame({'installation_id':last_ids, 'accuracy_group': resultList})\nsub.to_csv('submission.csv', encoding='utf-8', index=False) #\nprint('Submission file created!')\n#\nprint('LIGHTGBM ends')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['accuracy_group'].hist()\nprint(sub['accuracy_group'].value_counts())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}