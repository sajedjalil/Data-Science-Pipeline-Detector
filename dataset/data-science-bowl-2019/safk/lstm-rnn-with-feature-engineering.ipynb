{"cells":[{"metadata":{},"cell_type":"markdown","source":"## LSTM RNN with feature engineering\nSimple LSTM RNN with cumulative feature aggregation for each game session. Each training sample represents a sequence of 250 latest game sessions for specific installation_id. \n\n**Features include:** \n* \tCount game sessions for each world:\n\t*'activities_by_world_{world}'*, *'{world}_Clip'*, *'{world}_Activity'*, *'{world}_Game'*, *'{world}_Assessment'*\n* \tCount game sessions for each session type (Clip, Activity, Game, Assessment): *'activities_by_type_{type}'*\n* \tTime since first game session, time since previous game session\n* \tMean, median, std for deltas between *'game_time'* within session: *'game_time_mean'*, *'game_time_median'*, *'game_time_std'*, \n*   Session day of week and hour: *'time_start_dow'*, *'time_start_hour'*\n* \tEvaluation event stats for each world:\n\t*'{world}_eval_invalid'*, *'{world}_eval_valid'*, *'{world}_eval_sum'*, *'{world}_eval_ratio'*\n* \tTotal evaluation events stats across all worlds:\n\t*'Total_eval_invalid'*, *'Total_eval_valid'*, *'Total_eval_sum'*, *'Total_eval_ratio'*\n\t\nThe model was inspired by the following kernel: [bowl-lstm-prediction](https://www.kaggle.com/nikitagrec/bowl-lstm-prediction)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, SpatialDropout1D, Masking, Bidirectional, Dropout\nimport datetime\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nimport gc\nfrom datetime import datetime\nfrom keras.callbacks import *\nimport keras.backend as K\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"input_dir = '../input/data-science-bowl-2019/'\n\ndef create_features(df, world_names, type_names, event_code_idx_lookup, event_codes, win_codes):\n    world_by_idx = {k:v for k,v in zip(world_names, range(len(world_names)))}\n    type_by_idx = {k:v for k,v in zip(type_names, range(len(type_names)))}\n\n    new_col_names = ['activities_by_world_NONE', 'NONE_Clip', 'NONE_Activity', 'NONE_Game', 'NONE_Assessment', \n         'activities_by_world_MAGMAPEAK', 'MAGMAPEAK_Clip', 'MAGMAPEAK_Activity', 'MAGMAPEAK_Game', 'MAGMAPEAK_Assessment', \n         'activities_by_world_CRYSTALCAVES', 'CRYSTALCAVES_Clip', 'CRYSTALCAVES_Activity', 'CRYSTALCAVES_Game', 'CRYSTALCAVES_Assessment', \n         'activities_by_world_TREETOPCITY', 'TREETOPCITY_Clip', 'TREETOPCITY_Activity', 'TREETOPCITY_Game', 'TREETOPCITY_Assessment', \n         'activities_by_type_Clip', 'activities_by_type_Activity', 'activities_by_type_Game', 'activities_by_type_Assessment', \n         'first_install_event_time', 'time_since_previous_game']\n    new_col_names += ['time_start', 'game_time_mean', 'game_time_median', 'game_time_std', 'time_start_dow', 'time_start_hour', 'distinct_event_codes']    \n    new_col_names += event_codes\n    new_col_names += ['NONE_eval_invalid', 'NONE_eval_valid', 'NONE_eval_sum', 'NONE_eval_ratio',\n          'MAGMAPEAK_eval_invalid', 'MAGMAPEAK_eval_valid', 'MAGMAPEAK_eval_sum', 'MAGMAPEAK_eval_ratio',\n          'TREETOPCITY_eval_invalid', 'TREETOPCITY_eval_valid', 'TREETOPCITY_eval_sum', 'TREETOPCITY_eval_ratio',\n          'CRYSTALCAVES_eval_invalid', 'CRYSTALCAVES_eval_valid', 'CRYSTALCAVES_eval_sum', 'CRYSTALCAVES_eval_ratio',\n          'Total_eval_invalid', 'Total_eval_valid', 'Total_eval_sum', 'Total_eval_ratio']\n    \n    df_new = df.reindex(columns=[*df.columns.tolist(), *new_col_names], fill_value=0, copy=True)\n    \n    #table to aggregate activities stats per world/type\n    sess_agg = np.zeros((len(world_names), len(type_names)), dtype=np.int)\n    \n    #table to aggregate evaluation performance stats per world\n    eval_agg = np.zeros((len(world_names), 2), dtype=np.int)\n    \n    idx_agg = list()\n    new_values_agg = list()\n    second = np.timedelta64(1, 's')\n    \n    for install_id, install in df.groupby('installation_id', sort=False):\n        first_install_event_time = install['timestamp'].values[0] \n        groupby_session = install.groupby('game_session', sort=False)\n        previous_sess_time = first_install_event_time\n        \n        # reset stats for each new install_id\n        sess_agg.fill(0)\n        eval_agg.fill(0)\n        \n        for sess_id, sess in groupby_session:            \n            new_values = list()\n            \n            # extract activities stats per world\n            for w in world_names:\n                world_stats = sess_agg[world_by_idx[w], :]\n                #activities_by_world_{w}\n                new_values.append(np.sum(world_stats))   \n                #['{w}_Clip', '{w}_Activity', '{w}_Game', '{w}_Assessment']\n                new_values.extend(world_stats)                \n            \n            # extract activities stats per activity type\n            for t in type_names:\n                # 'activities_by_type_{t}'\n                new_values.append(np.sum(sess_agg[:, type_by_idx[t]]))                                  \n                \n            sess_world = sess['world'].iloc[0]        \n            sess_agg[world_by_idx[sess_world], type_by_idx[sess['type'].iloc[0]]] += 1            \n            \n            new_values.append((sess['timestamp'].values[0] - first_install_event_time)/second)            \n            new_values.append((sess['timestamp'].values[0] - previous_sess_time)/second)\n\n            previous_sess_time = sess['timestamp'].values[-1]            \n            \n            time_start = sess['timestamp'].values[0]\n            new_values.append(time_start)\n            \n            game_time_stats = sess['game_time'].diff().agg(['mean','median', 'std']).fillna(0)\n            new_values.append(game_time_stats.loc['mean'])\n            new_values.append(game_time_stats.loc['median'])\n            new_values.append(game_time_stats.loc['std'])\n            dt_start = datetime.utcfromtimestamp(time_start.astype(int) * 1e-9)\n            new_values.append(dt_start.weekday())\n            new_values.append(dt_start.hour)\n            \n            # count event codes per session\n            event_counts = Counter(sess['event_code'])\n            ec = np.repeat(0, len(event_codes))\n            for key,val in event_counts.items():\n                ec[event_code_idx_lookup[key]] = val\n            new_values.append(len(event_counts.keys()))\n            new_values.extend(ec)\n            \n            # calculate evaluations\n            # ['{w}_eval_invalid', '{w}_eval_valid', '{w}_eval_sum', '{w}_eval_ratio']\n            for w in world_names:                \n                evals_world = eval_agg[world_by_idx[w], :]\n                new_values.extend(evals_world)\n                evals_sum = np.sum(evals_world)\n                new_values.append(evals_sum)\n                new_values.append(np.nan_to_num(evals_world[1]/evals_sum))\n                        \n            # total evaluations across all worlds\n            # 'Total_eval_invalid', 'Total_eval_valid', 'Total_eval_sum', 'Total_eval_ratio'\n            evals_total = np.sum(eval_agg, axis=0)\n            new_values.extend(evals_total)\n            evals_total_sum = np.sum(evals_total)\n            new_values.append(evals_total_sum)\n            new_values.append(np.nan_to_num(evals_total[1]/evals_total_sum))\n            \n            all_evaluations = sess[sess['event_code'] == win_codes[sess['title'].iloc[0]]]            \n            eval_agg[world_by_idx[sess_world], 0] += all_evaluations['event_data'].str.contains('false').sum()\n            eval_agg[world_by_idx[sess_world], 1] += all_evaluations['event_data'].str.contains('true').sum()\n            \n            idx_agg.append(sess.index[-1])\n            new_values_agg.append(new_values)     \n        \n    df_new.set_value(idx_agg, new_col_names, new_values_agg)\n    return df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_event_codes = pd.read_csv(input_dir+'test.csv', low_memory=True, usecols=['event_code'], dtype={'event_code': str})\ntrain_df_temp = pd.read_csv(input_dir+'train.csv', low_memory=True, usecols=['event_code', 'type', 'world', 'title'], dtype={'event_code': str})\nevent_codes = [str(i) for i in sorted(set(test_df_event_codes['event_code'].unique()).union(set(train_df_temp['event_code'].unique())))]\nevent_code_idx_lookup = {v:i for v,i in zip(event_codes, range(len(event_codes)))}\n\n# map title to evaluation event code\nwin_code = dict(zip(train_df_temp['title'].unique(), np.repeat('4100', len(train_df_temp['title'].unique()))))\nwin_code['Bird Measurer (Assessment)'] = '4110'\n\nworld_names = train_df_temp['world'].unique()\ntype_names = train_df_temp['type'].unique()\ndel  train_df_temp, test_df_event_codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_in_batches(filename, installation_ids, event_codes, win_codes):\n    merges = []\n    \n    for b in np.array_split(installation_ids, 5):\n        df = pd.read_csv(input_dir+filename, parse_dates=['timestamp'], low_memory=True, \n                              dtype={'event_count': int, 'game_time': float, 'event_code': str}, \n                              usecols=['game_session','timestamp', 'installation_id','event_count','event_code',\n                                       'game_time','title','type','world', 'event_data'])\n        df_batch = df[df['installation_id'].isin(b)]\n        df_batch.sort_values('timestamp', inplace=True)\n        df_batch_agg = create_features(df_batch, world_names, type_names, event_code_idx_lookup, event_codes, win_codes)\n        df_batch_agg_tail = df_batch_agg.groupby(['installation_id', 'game_session']).tail(1)\n        merges.append(df_batch_agg_tail)\n        \n    agg_df = merges[0]\n    for i in merges[1:]:    \n        agg_df = agg_df.append(i)\n    return agg_df\n\ninstallation_ids_test = pd.read_csv(input_dir+'test.csv', usecols=['installation_id'])['installation_id'].unique()\ntest_sess_agg_df = process_in_batches('test.csv', installation_ids_test, event_codes, win_code)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(input_dir+ 'train.csv', parse_dates=['timestamp'], low_memory=True, \n                       dtype={'event_count': int, 'game_time': float, 'event_code': str}, \n                       usecols=['game_session','timestamp', 'installation_id','event_count','event_code','game_time',\n                                                                             'title','type','world'])\ntrain_df.sort_values('timestamp', inplace=True)\n\nlabels = pd.read_csv(input_dir+'train_labels.csv', low_memory=True, usecols=['installation_id', 'game_session', 'accuracy_group'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_outliers(df):\n    vc = df.groupby(['installation_id']).apply(lambda x: len(x))\n    print('installation ids: ' + str(len(vc)))\n    installation_ids_inclue = vc[vc<15000].index.values\n    print('installation ids to inclue: ' + str(len(installation_ids_inclue)))\n    return df[df['installation_id'].isin(installation_ids_inclue)]\n\ntrain_df = filter_outliers(train_df)\n\nprint(len(train_df))\ni_ids = train_df.groupby('installation_id').tail(1).merge(labels, how='inner', on='installation_id')['installation_id'].unique()\ntrain_df = train_df[train_df['installation_id'].isin(i_ids)]\nprint(len(train_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"installation_ids_train = train_df['installation_id'].unique()\ndel train_df\ntrain_sess_agg_df = process_in_batches('train.csv', installation_ids_train, event_codes, win_code)\ngame_sessions_with_label = set(train_sess_agg_df\n                               .merge(labels, how='inner', on=['installation_id', 'game_session'])['game_session']\n                               .unique()\n                               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trim installation_id so that it ends with assessment with label\ndef trim_train_sequences(x, game_sessions_with_label):\n    x.reset_index(inplace=True, drop=True)    \n    assessments = x[(x['type'] == 'Assessment') & (x['game_session'].isin(game_sessions_with_label))]    \n    if len(assessments) == 0:\n        return pd.DataFrame()\n    else:\n        idx = np.random.choice(len(assessments), 1)[0]  \n        return x.iloc[:assessments.index[idx] + 1]\n    \ngame_stats_trimmed_train = train_sess_agg_df\\\n    .groupby('installation_id')\\\n    .apply(lambda x: trim_train_sequences(x, game_sessions_with_label))\\\n    .reset_index(drop=True)\n\ndel train_sess_agg_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ohe(train, test, target, features):\n    if not train.columns.isin([target]).any():\n        return train    \n    \n    train_objs_num = len(train)\n    dataset = pd.concat(objs=[train[target], test[target]], axis=0, copy=False)\n    dataset_preprocessed = pd.get_dummies(dataset, prefix= '{}_'.format(target))\n    train_preprocessed = dataset_preprocessed[:train_objs_num]\n    test_preprocessed = dataset_preprocessed[train_objs_num:]    \n    \n    merge = train.merge(train_preprocessed, left_index=True, right_index=True)\n    train_result = merge[merge.columns[~merge.columns.isin([target])]]\n    \n    merge = test.merge(test_preprocessed, left_index=True, right_index=True)\n    test_result = merge[merge.columns[~merge.columns.isin([target])]]\n\n    features.extend(train_preprocessed.columns.values)\n    return train_result, test_result\n\ndef mean_encoding(train_df, test_df, target):\n    train_df[target] = (train_df[target]-train_df[target].mean())/train_df[target].std()\n    test_df[target] = (test_df[target]-test_df[target].mean())/test_df[target].std()      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\nmask_cols = ['event_count', 'game_time', 'game_time_mean', 'game_time_median', 'game_time_std', '2000', '2010', '2020', '2025', '2030', '2035', '2040', '2050', '2060', '2070', '2075', '2080', '2081', '2083', '3010', '3020', '3021', '3110', '3120', '3121', '4010', '4020', '4021', '4022', '4025', '4030', '4031', '4035', '4040', '4045', '4050', '4070', '4080', '4090', '4095', '4100', '4110', '4220', '4230', '4235', '5000', '5010', 'distinct_event_codes']\n\n# mask aggregation stats in train for last session in each installation_id\ntile = np.tile(mask, (len(game_stats_trimmed_train.groupby('installation_id').tail(1).index), 1))\nframe = pd.DataFrame(tile, columns=mask_cols, index= game_stats_trimmed_train.groupby('installation_id').tail(1).index)\ngame_stats_trimmed_train.update(frame)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['event_count', 'game_time', 'game_time_mean',\n                   'game_time_median', 'game_time_std', 'time_start_dow',\n                   'time_start_hour', '2000', '2010', '2020', '2025', '2030', '2035',\n                   '2040', '2050', '2060', '2070', '2075', '2080', '2081', '2083', '3010',\n                   '3020', '3021', '3110', '3120', '3121', '4010', '4020', '4021', '4022',\n                   '4025', '4030', '4031', '4035', '4040', '4045', '4050', '4070', '4080',\n                   '4090', '4095', '4100', '4110', '4220', '4230', '4235', '5000', '5010',\n                   'distinct_event_codes', \n            'activities_by_world_NONE', 'NONE_Clip', 'NONE_Activity', 'NONE_Game', 'NONE_Assessment', \n            'activities_by_world_MAGMAPEAK', 'MAGMAPEAK_Clip', 'MAGMAPEAK_Activity', 'MAGMAPEAK_Game', 'MAGMAPEAK_Assessment', \n            'activities_by_world_CRYSTALCAVES', 'CRYSTALCAVES_Clip', 'CRYSTALCAVES_Activity', 'CRYSTALCAVES_Game', 'CRYSTALCAVES_Assessment', \n            'activities_by_world_TREETOPCITY', 'TREETOPCITY_Clip', 'TREETOPCITY_Activity', 'TREETOPCITY_Game', 'TREETOPCITY_Assessment', \n            'activities_by_type_Clip', 'activities_by_type_Activity', 'activities_by_type_Game', 'activities_by_type_Assessment', \n            'first_install_event_time', 'time_since_previous_game', \n            'NONE_eval_invalid', 'NONE_eval_valid', 'NONE_eval_sum', 'NONE_eval_ratio',\n            'MAGMAPEAK_eval_invalid', 'MAGMAPEAK_eval_valid', 'MAGMAPEAK_eval_sum', 'MAGMAPEAK_eval_ratio',\n            'TREETOPCITY_eval_invalid', 'TREETOPCITY_eval_valid', 'TREETOPCITY_eval_sum', 'TREETOPCITY_eval_ratio',\n            'CRYSTALCAVES_eval_invalid', 'CRYSTALCAVES_eval_valid', 'CRYSTALCAVES_eval_sum', 'CRYSTALCAVES_eval_ratio',\n            'Total_eval_invalid', 'Total_eval_valid', 'Total_eval_sum', 'Total_eval_ratio']\ntrain_clean, test_clean = ohe(game_stats_trimmed_train, test_sess_agg_df, 'title', features)\ndel game_stats_trimmed_train\ndel test_sess_agg_df\ntrain_clean, test_clean = ohe(train_clean, test_clean, 'type', features)\ntrain_clean, test_clean = ohe(train_clean, test_clean, 'world', features)\n\nmean_encoding(train_clean, test_clean ,'distinct_event_codes')\nmean_encoding(train_clean, test_clean ,'time_start_hour')\nmean_encoding(train_clean, test_clean ,'event_count')\nmean_encoding(train_clean, test_clean ,'game_time')\nmean_encoding(train_clean, test_clean ,'first_install_event_time')\nmean_encoding(train_clean, test_clean ,'time_since_previous_game')\n\nmean_encoding(train_clean, test_clean ,'game_time_mean')\nmean_encoding(train_clean, test_clean ,'game_time_median')\nmean_encoding(train_clean, test_clean ,'game_time_std')\ntest_clean.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = 250 \npadding_stub = np.repeat(-1., len(features)).astype(np.int8, copy=False)\n\ndef create_sequential_input(df, features, pad_val):\n    df_grouped = df.groupby('installation_id')            \n    \n    rnn_input = np.zeros((len(df_grouped), sequence_length, len(features)))\n    y_map = list()\n    for g, i in zip(df_grouped, range(len(df_grouped))):\n        sequence_full = g[1][features].values\n        sequence_trimmed = sequence_full[max(0, len(sequence_full)-sequence_length):]        \n        current_length = sequence_trimmed.shape[0]       \n        \n        # padding\n        rnn_input[i, :sequence_length-current_length] = np.tile(pad_val, (sequence_length-current_length, 1))\n        rnn_input[i, sequence_length-current_length:] = sequence_trimmed\n        y_map.append((g[0],g[1].iloc[-1]['game_session']))    \n    \n    return rnn_input, pd.DataFrame(y_map, columns=['installation_id', 'game_session'])\n\nX_train_input, y_train_map = create_sequential_input(train_clean, features, padding_stub)\nX_test_input, y_test_map = create_sequential_input(test_clean, features, padding_stub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge_labels = y_train_map.merge(labels, how='inner', on=['installation_id', 'game_session'])\ny_train_input = train_merge_labels['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2           \n            else:\n                X_p[i] = 3\n\n        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n        return -ll\n\n    def fit(self, X, y, initial_coef):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)        \n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2            \n            else:\n                X_p[i] = 3\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n\ndef get_class_bounds(y, y_pred, N=4, class0_fraction=-1):\n    \"\"\"\n    Find boundary values for y_pred to match the known y class percentiles.\n    Returns N-1 boundaries in y_pred values that separate y_pred\n    into N classes (0, 1, 2, ..., N-1) with same percentiles as y has.\n    Can adjust the fraction in Class 0 by the given factor (>=0), if desired. \n    \"\"\"\n    ysort = np.sort(y)\n    predsort = np.sort(y_pred)\n    bounds = []\n    for ibound in range(N-1):\n        iy = len(ysort[ysort <= ibound])\n        # adjust the number of class 0 predictions?\n        if (ibound == 0) and (class0_fraction >= 0.0) :\n            iy = int(class0_fraction * iy)\n        bounds.append(predsort[iy])\n    return bounds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        print(self.clr())\n\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        # self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        # self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        # for k, v in logs.items():\n        #     self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n        \n    def on_epoch_end(self, epoch, logs=None):\n        print(self.clr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n\nepochs = 128\nbatch_size = 512\n\ndef get_model():\n    model = Sequential()    \n    model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2, input_shape=(sequence_length, len(features))))\n    model.add(Dense(1, activation='linear'))  \n    model.compile(loss=tf.keras.metrics.mean_squared_error, optimizer='adam', metrics=['mae'])\n    \n    checkpointer = ModelCheckpoint(filepath='weights.hdf5',verbose=2,save_best_only=True,monitor='val_mae')\n#     clr = CyclicLR(base_lr=0.00001, max_lr=0.01,step_size=5, mode='exp_range',gamma=0.99994)\n    clr = CyclicLR(base_lr=0.00001, max_lr=0.01,step_size=70., mode='triangular2')\n    stopping = EarlyStopping(monitor='val_mae', patience=25, min_delta=0.0001)\n    \n    callbacks = [checkpointer, clr, stopping]\n    return model, callbacks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nkf = StratifiedKFold(n_splits=n_folds)\nfinals = pd.DataFrame()\n\nfor (train_index, test_index), i in zip(kf.split(X_train_input, y_train_input), range(n_folds)):\n    model, callbacks = get_model()\n    \n    X_train, X_test = X_train_input[train_index], X_train_input[test_index]\n    y_train, y_test = y_train_input[train_index], y_train_input[test_index]\n    \n    history = model.fit(X_train, y_train,\n                        epochs=epochs,\n                        batch_size=batch_size,\n                        validation_data=(X_test, y_test),\n                        callbacks=callbacks,\n                        verbose=2)\n    \n    model.load_weights(\"weights.hdf5\")\n    \n    test_pred = model.predict(X_test)\n    optR = OptimizedRounder()\n    bounds = get_class_bounds(y_test, test_pred.reshape(-1))\n    optR.fit(test_pred.reshape(-1), y_test, bounds)\n    coefficients = optR.coefficients()    \n    \n    pred_final = model.predict(X_test_input)\n    final = optR.predict(pred_final.reshape(-1), coefficients)    \n    finals[str(i)] = final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = {k: v/len(y_train_input) for k,v in Counter(y_train_input.values).items()}\n\nsample_submission = pd.DataFrame()\nsample_submission['installation_id']= y_test_map['installation_id']\nfinal_mode_values = finals.mode(axis=1, dropna=True)\\\n    .apply(lambda x: int(sorted(x[~np.isnan(x)].values, key=lambda x: probs[x])[-1]), axis=1).values\nsample_submission[\"accuracy_group\"] = final_mode_values\nsample_submission.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}