{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's import the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main Libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Utility libs\nfrom tqdm import tqdm\nimport time\nimport datetime\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\nfrom skopt.plots import plot_convergence\nfrom copy import deepcopy\nimport pprint\nimport shap\nimport os\n\n# You might have to do !pip install catboost\n# If you don't have it on your local machine\n# nevertheless Kaggle runtimes come preinstalled with CatBoost\nimport catboost\n\nfrom pathlib import Path\ndata_dir = Path('../input/data-science-bowl-2019')\nos.listdir(data_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv')\nlabels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv')\nspecs = pd.read_csv('../input/data-science-bowl-2019/specs.csv')\nsample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and extracting features"},{"metadata":{},"cell_type":"markdown","source":"How we are going to extract features is through cumulating various features present in the data as grouped by unique sessions. \n\nIt is being modeled as how many games a user has played, or clips they have watched etc. up until they take an assessment. \n\nThis truncated history is condensed into one row of the prepared data.\n\nLet's see how this works out!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# thank you for this data preparation function \n# https://www.kaggle.com/mhviraf/a-new-baseline-for-dsb-2019-catboost-model\ndef get_data(user_sample, test_set=False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    durations = []\n    for i, session in user_sample.groupby('game_session', sort=False):\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        if test_set == True:\n            second_condition = True\n        else:\n            if len(session)>1:\n                second_condition = True\n            else:\n                second_condition= False\n            \n        if (session_type == 'Assessment') & (second_condition):\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            features = user_activities_count.copy()\n            features['session_title'] = session['title'].iloc[0] \n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n\n            features.update(accuracy_groups)\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            features['accumulated_actions'] = accumulated_actions\n            accumulated_accuracy_group += features['accuracy_group']\n            accuracy_groups[features['accuracy_group']] += 1\n            if test_set == True:\n                all_assessments.append(features)\n            else:\n                if true_attempts+false_attempts > 0:\n                    all_assessments.append(features)\n                \n            counter += 1\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    if test_set:\n        return all_assessments[-1] \n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"list_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\nlabels['title'] = labels['title'].map(activities_map)\nwin_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110\n\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\ntest['timestamp'] = pd.to_datetime(test['timestamp'])\ncompiled_data = []\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=17000):\n    compiled_data += get_data(user_sample)\nnew_train = pd.DataFrame(compiled_data)\ndel compiled_data\nprint(\"Train Data:\")\nnew_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we can see, this is what we have gathered. Let's plot these features and see if there are trends within these features as grouped by their accuracy groups.\n\nLet's begin."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height/total),\n                ha=\"center\") \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now let's plot and visualize each feature"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'Clip'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that there is a positively skewed gaussian.\nThis is fine. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'Activity'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another, more prominent positively skewed gaussian."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'Assessment'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'Game'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_count('session_title', 'Session Titles', new_train, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the count of session_titles is not all that variable afterall."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'accumulated_correct_attempts'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'accumulated_uncorrect_attempts'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'duration_mean'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'accumulated_accuracy'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is an interesting plot, we can see that a lot of users with accuracy group 3 have also a high accumulated accuracy of 0.\n\nI will leave it up to you guys to find more inferences. \nLet me know something in the comments if I am missing out on it. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_count('accuracy_group', 'Accuracy Groups', new_train, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 0\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 1\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 2\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 3\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'accumulated_accuracy_group'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\ncol = 'accumulated_actions'\n_accuracy_groups = new_train.accuracy_group.unique()\nplt.title(\"Distribution of %s values (grouped by accuracy group) in the data\"%col)\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = new_train.loc[new_train.accuracy_group == _accuracy_group]\n    sns.distplot(red_comp_train_df[col], kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's try a Gaussian Mixture Model Classification on these features and find covariances between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.externals.six.moves import xrange\nimport matplotlib as mpl\nfrom itertools import combinations\nfrom sklearn.mixture import GaussianMixture\n\ncolors = ['navy', 'turquoise', 'darkorange', 'yellow']\nfeat1 = 'Game'\nfeat2 = 'Assessment'\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n][:2])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n        ax.set_aspect('equal', 'datalim')\n\n# Break up the dataset into non-overlapping training (75%) and testing\n# (25%) sets.\nskf = StratifiedKFold()\nskf.get_n_splits(new_train.drop(['accuracy_group'], axis=1), new_train['accuracy_group'])\n# Only take the first fold.\nfor train_index, test_index in skf.split(new_train.drop(['accuracy_group'], axis=1), new_train['accuracy_group']):\n    X = new_train.drop(['accuracy_group'], axis=1)\n    y = new_train['accuracy_group']\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    n_classes = len(np.unique(y_train))\n\n    # Try GMMs using different types of covariances.\n    estimators = {cov_type: GaussianMixture(n_components=n_classes,\n              covariance_type=cov_type, max_iter=100, random_state=0)\n              for cov_type in ['spherical', 'diag', 'tied', 'full']}\n\n    n_estimators = len(estimators)\n\n    plt.figure(figsize=(3 * n_estimators // 2, 6))\n    plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                        left=.01, right=.99)\n\n\n    for index, (name, estimator) in enumerate(estimators.items()):\n        # Since we have class labels for the training data, we can\n        # initialize the GMM parameters in a supervised manner.\n        estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n                                        for i in range(n_classes)])\n\n        # Train the other parameters using the EM algorithm.\n        estimator.fit(X_train)\n\n        h = plt.subplot(2, n_estimators // 2, index + 1)\n        make_ellipses(estimator, h)\n        \n        for n, color in enumerate(colors):\n            data = new_train.loc[new_train['accuracy_group'] == n]\n            plt.scatter(data[feat1], data[feat2], s=0.8, color=color,\n                        label=n)\n            # Plot the test data with crosses\n        for n, color in enumerate(colors):\n            data = X_test[y_test == n]\n            plt.scatter(data[feat1], data[feat2], marker='x', color=color)\n    break\nplt.xticks(())\nplt.yticks(())\nplt.title(name)\nplt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thank You\n\nNext I will be posting a baseline Random Forest and AdaBoost model using these features and perhaps we can extract and engineer new features.\n\n## Feel free to ask any questions, I'll be happy to answer.\n\n## Please leave an upvote for appreciation if you thought this notebook was helpful so others can see it as well. :) \n\n# Happy Kaggling"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}