{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description\n\n\nThis kernel is based on the result of [Catboost - Some more features](https://www.kaggle.com/braquino/catboost-some-more-features).  \nThe largest difference between this kernel and the aforementioned reference kernel is the use of [GroupKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html). It prevents us from overfitting, and eventually bump up the score."},{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import abc\nimport codecs\nimport inspect\nimport json\nimport logging\nimport gc\nimport pickle\nimport sys\nimport time\nimport warnings\n\nimport catboost as cat\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport yaml\n\nfrom abc import abstractmethod\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom numba import jit\nfrom typing import List, Optional, Union, Tuple, Dict\n\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.model_selection import train_test_split, GroupKFold\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Config\n\nIt seems a bit strange but I give configuration for this model with 'yaml like' string, since I usually work on data pipeline which takes yaml config file as input. I got this data pipeline idea from the repository [pudae/kaggle-hpa](https://github.com/pudae/kaggle-hpa)."},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_string = '''\ndataset:\n  dir: \"../input/data-science-bowl-2019/\"\n  feature_dir: \"features\"\n  params:\n\nfeatures:\n  - Basic\n\nav:\n  split_params:\n    test_size: 0.33\n    random_state: 42\n\n  model_params:\n    objective: \"binary\"\n    metric: \"auc\"\n    boosting: \"gbdt\"\n    max_depth: 7\n    num_leaves: 75\n    learning_rate: 0.01\n    colsample_bytree: 0.7\n    subsample: 0.1\n    subsample_freq: 1\n    seed: 111\n    feature_fraction_seed: 111\n    drop_seed: 111\n    verbose: -1\n    first_metric_only: True\n\n  train_params:\n    num_boost_round: 1000\n    early_stopping_rounds: 100\n    verbose_eval: 100\n\nmodel:\n  name: \"catboost\"\n  model_params:\n    loss_function: \"MultiClass\"\n    eval_metric: \"WKappa\"\n    task_type: \"CPU\"\n    iterations: 6000\n    early_stopping_rounds: 500\n    random_seed: 42\n\n  train_params:\n    mode: \"classification\"\n\nval:\n  name: \"group_kfold\"\n  params:\n    n_splits: 5\n\noutput_dir: \"output\"\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = dict(yaml.load(conf_string, Loader=yaml.SafeLoader))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions and Classes"},{"metadata":{},"cell_type":"markdown","source":"### utils"},{"metadata":{},"cell_type":"markdown","source":"#### checker"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def feature_existence_checker(feature_path: Path,\n                              feature_names: List[str]) -> bool:\n    features = [f.name for f in feature_path.glob(\"*.ftr\")]\n    for f in feature_names:\n        if f + \"_train.ftr\" not in features:\n            return False\n        if f + \"_test.ftr\" not in features:\n            return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### jsonutil"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n\ndef save_json(config: dict, save_path: Union[str, Path]):\n    f = codecs.open(str(save_path), mode=\"w\", encoding=\"utf-8\")\n    json.dump(config, f, indent=4, cls=MyEncoder, ensure_ascii=False)\n    f.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### logger"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def configure_logger(config_name: str, log_dir: Union[Path, str], debug: bool):\n    if isinstance(log_dir, str):\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n    else:\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n    log_filename = config_name.split(\"/\")[-1].replace(\".yml\", \".log\")\n    log_filepath = log_dir / log_filename \\\n        if isinstance(log_dir, Path) else Path(log_dir) / log_filename\n\n    # delete the old log\n    if log_filepath.exists():\n        with open(log_filepath, mode=\"w\"):\n            pass\n\n    level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        filename=str(log_filepath),\n        level=level,\n        format=\"%(asctime)s %(levelname)s %(message)s\",\n        datefmt=\"%m/%d/%Y %I:%M:%S %p\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### timer"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"@contextmanager\ndef timer(name: str, log: bool = False):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if not log:\n        print(msg)\n    else:\n        logging.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if not log:\n        print(msg)\n    else:\n        logging.info(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation\n\nThis kernel uses GroupKFold as validation strategy.  \nIn this kernel, I grouped up the training sample with `installation_id` so that samples with certain `installation_id` do not exist in both train and val set in the same fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_kfold(df: pd.DataFrame, groups: pd.Series,\n                config: dict) -> List[Tuple[np.ndarray, np.ndarray]]:\n    params = config[\"val\"][\"params\"]\n    kf = GroupKFold(n_splits=params[\"n_splits\"])\n    split = list(kf.split(df, groups=groups))\n    return split\n\n\ndef get_validation(df: pd.DataFrame,\n                   config: dict) -> List[Tuple[np.ndarray, np.ndarray]]:\n    name: str = config[\"val\"][\"name\"]\n\n    func = globals().get(name)\n    if func is None:\n        raise NotImplementedError\n\n    if \"group\" in name:\n        cols = df.columns.tolist()\n        cols.remove(\"group\")\n        groups = df[\"group\"]\n        return func(df[cols], groups, config)\n    else:\n        return func(df, config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### evaluation\n\nCode from [Ultra Fast QWK Calc Method](https://www.kaggle.com/cpmpml/ultra-fast-qwk-calc-method)."},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit\ndef qwk(y_true: Union[np.ndarray, list],\n        y_pred: Union[np.ndarray, list],\n        max_rat: int = 3) -> float:\n    y_true_ = np.asarray(y_true, dtype=int)\n    y_pred_ = np.asarray(y_pred, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    numerator = 0\n    for k in range(y_true_.shape[0]):\n        i, j = y_true_[k], y_pred_[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        numerator += (i - j) * (i - j)\n\n    denominator = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            denominator += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    denominator /= y_true_.shape[0]\n    return 1 - numerator / denominator\n\n\ndef calc_metric(y_true: Union[np.ndarray, list],\n                y_pred: Union[np.ndarray, list]) -> float:\n    return qwk(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### models"},{"metadata":{},"cell_type":"markdown","source":"#### base\n\n\nCode taken from [hakubishin/kaggle_ieee](https://github.com/hakubishin3/kaggle_ieee)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# type alias\nAoD = Union[np.ndarray, pd.DataFrame]\nAoS = Union[np.ndarray, pd.Series]\nCatModel = Union[cat.CatBoostClassifier, cat.CatBoostRegressor]\nLGBModel = Union[lgb.LGBMClassifier, lgb.LGBMRegressor]\nModel = Union[CatModel, LGBModel]\n\n\nclass BaseModel(object):\n    @abstractmethod\n    def fit(self, x_train: AoD, y_train: AoS, x_valid: AoD, y_valid: AoS,\n            config: dict) -> Tuple[Model, dict]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_best_iteration(self, model: Model) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model: Model, features: AoD) -> np.ndarray:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self, model: Model) -> np.ndarray:\n        raise NotImplementedError\n\n    def cv(self,\n           y_train: AoS,\n           train_features: AoD,\n           test_features: AoD,\n           feature_name: List[str],\n           folds_ids: List[Tuple[np.ndarray, np.ndarray]],\n           config: dict,\n           log: bool = True\n           ) -> Tuple[List[Model], np.ndarray, np.ndarray, pd.DataFrame, dict]:\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        oof_preds = np.zeros(len(train_features))\n        importances = pd.DataFrame(index=feature_name)\n        best_iteration = 0.0\n        cv_score_list: List[dict] = []\n        models: List[Model] = []\n\n        X = train_features.values if isinstance(train_features, pd.DataFrame) \\\n            else train_features\n        y = y_train.values if isinstance(y_train, pd.Series) \\\n            else y_train\n\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            # get train data and valid data\n            x_trn = X[trn_idx]\n            y_trn = y[trn_idx]\n            x_val = X[val_idx]\n            y_val = y[val_idx]\n\n            # train model\n            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n            cv_score_list.append(best_score)\n            models.append(model)\n            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n\n            # predict oof and test\n            oof_preds[val_idx] = self.predict(model, x_val).reshape(-1)\n            test_preds += self.predict(\n                model, test_features).reshape(-1) / len(folds_ids)\n\n            # get feature importances\n            importances_tmp = pd.DataFrame(\n                self.get_feature_importance(model),\n                columns=[f\"gain_{i_fold+1}\"],\n                index=feature_name)\n            importances = importances.join(importances_tmp, how=\"inner\")\n\n        # summary of feature importance\n        feature_importance = importances.mean(axis=1)\n\n        # print oof score\n        oof_score = calc_metric(y_train, oof_preds)\n        print(f\"oof score: {oof_score:.5f}\")\n\n        if log:\n            logging.info(f\"oof score: {oof_score:.5f}\")\n\n        evals_results = {\n            \"evals_result\": {\n                \"oof_score\":\n                oof_score,\n                \"cv_score\": {\n                    f\"cv{i + 1}\": cv_score\n                    for i, cv_score in enumerate(cv_score_list)\n                },\n                \"n_data\":\n                len(train_features),\n                \"best_iteration\":\n                best_iteration,\n                \"n_features\":\n                len(train_features.columns),\n                \"feature_importance\":\n                feature_importance.sort_values(ascending=False).to_dict()\n            }\n        }\n\n        return models, oof_preds, test_preds, feature_importance, evals_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### cat"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"CatModel = Union[CatBoostClassifier, CatBoostRegressor]\n\n\nclass CatBoost(BaseModel):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            x_valid: np.ndarray, y_valid: np.ndarray,\n            config: dict) -> Tuple[CatModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        mode = config[\"model\"][\"train_params\"][\"mode\"]\n        if mode == \"regression\":\n            model = CatBoostRegressor(**model_params)\n        else:\n            model = CatBoostClassifier(**model_params)\n\n        model.fit(\n            x_train,\n            y_train,\n            eval_set=(x_valid, y_valid),\n            use_best_model=True,\n            verbose=model_params[\"early_stopping_rounds\"])\n        best_score = model.best_score_\n        return model, best_score\n\n    def get_best_iteration(self, model: CatModel):\n        return model.best_iteration_\n\n    def predict(self, model: CatModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        return model.predict(features)\n\n    def get_feature_importance(self, model: CatModel) -> np.ndarray:\n        return model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### factory"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def catboost() -> CatBoost:\n    return CatBoost()\n\n\ndef get_model(config: dict):\n    model_name = config[\"model\"][\"name\"]\n    func = globals().get(model_name)\n    if func is None:\n        raise NotImplementedError\n    return func()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### features"},{"metadata":{},"cell_type":"markdown","source":"#### base"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Feature(metaclass=abc.ABCMeta):\n    prefix = \"\"\n    suffix = \"\"\n    save_dir = \"features\"\n    is_feature = True\n\n    def __init__(self):\n        self.name = self.__class__.__name__\n        self.train = pd.DataFrame()\n        self.test = pd.DataFrame()\n        Path(self.save_dir).mkdir(parents=True, exist_ok=True)\n        self.train_path = Path(self.save_dir) / f\"{self.name}_train.ftr\"\n        self.test_path = Path(self.save_dir) / f\"{self.name}_test.ftr\"\n\n    def run(self,\n            train_df: pd.DataFrame,\n            test_df: Optional[pd.DataFrame] = None,\n            log: bool = False):\n        with timer(self.name, log=log):\n            self.create_features(train_df, test_df)\n            prefix = self.prefix + \"_\" if self.prefix else \"\"\n            suffix = self.suffix + \"_\" if self.suffix else \"\"\n            self.train.columns = [str(c) for c in self.train.columns]\n            self.test.columns = [str(c) for c in self.test.columns]\n            self.train.columns = prefix + self.train.columns + suffix\n            self.test.columns = prefix + self.test.columns + suffix\n        return self\n\n    @abc.abstractmethod\n    def create_features(self, train_df: pd.DataFrame,\n                        test_df: Optional[pd.DataFrame]):\n        raise NotImplementedError\n\n    def save(self):\n        self.train.to_feather(str(self.train_path))\n        self.test.to_feather(str(self.test_path))\n\n\nclass PartialFeature(metaclass=abc.ABCMeta):\n    def __init__(self):\n        self.df = pd.DataFrame\n\n    @abc.abstractmethod\n    def create_features(self, df: pd.DataFrame, test: bool = False):\n        raise NotImplementedError\n\n\ndef is_feature(klass) -> bool:\n    return \"is_feature\" in set(dir(klass))\n\n\ndef get_features(namespace: dict):\n    for v in namespace.values():\n        if inspect.isclass(v) and is_feature(v) and not inspect.isabstract(v):\n            yield v()\n\n\ndef generate_features(train_df: pd.DataFrame,\n                      test_df: pd.DataFrame,\n                      namespace: dict,\n                      overwrite: bool,\n                      log: bool = False):\n    for f in get_features(namespace):\n        if f.train_path.exists() and f.test_path.exists() and not overwrite:\n            if not log:\n                print(f.name, \"was skipped\")\n            else:\n                logging.info(f\"{f.name} was skipped\")\n        else:\n            f.run(train_df, test_df, log).save()\n\n\ndef load_features(config: dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    feather_path = config[\"dataset\"][\"feature_dir\"]\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_train.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_train.ftr\").exists()\n    ]\n    x_train = pd.concat(dfs, axis=1)\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_test.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_test.ftr\").exists()\n    ]\n    x_test = pd.concat(dfs, axis=1)\n    return x_train, x_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### basic\n\n\nThe features used in this kernel is all the same as those used in [Catboost - Some more features](https://www.kaggle.com/braquino/catboost-some-more-features)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"IoF = Union[int, float]\nIoS = Union[int, str]\n\n\nclass Basic(Feature):\n    def create_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n        all_activities = set(train_df[\"title\"].unique()).union(\n            set(test_df[\"title\"].unique()))\n        all_event_codes = set(train_df[\"event_code\"].unique()).union(\n            test_df[\"event_code\"].unique())\n        activities_map = dict(\n            zip(all_activities, np.arange(len(all_activities))))\n        inverse_activities_map = dict(\n            zip(np.arange(len(all_activities)), all_activities))\n\n        compiled_data_train: List[List[IoF]] = []\n        compiled_data_test: List[List[IoF]] = []\n\n        installation_ids_train = []\n        installation_ids_test = []\n\n        train_df[\"title\"] = train_df[\"title\"].map(activities_map)\n        test_df[\"title\"] = test_df[\"title\"].map(activities_map)\n\n        train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\n        test_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\n\n        for ins_id, user_sample in tqdm_notebook(\n                train_df.groupby(\"installation_id\", sort=False),\n                total=train_df[\"installation_id\"].nunique(),\n                desc=\"train features\"):\n            if \"Assessment\" not in user_sample[\"type\"].unique():\n                continue\n            feats = KernelFeatures(all_activities, all_event_codes,\n                                   activities_map, inverse_activities_map)\n            feat_df = feats.create_features(user_sample, test=False)\n            installation_ids_train.extend([ins_id] * len(feat_df))\n            compiled_data_train.append(feat_df)\n        self.train = pd.concat(compiled_data_train, axis=0, sort=False)\n        self.train[\"installation_id\"] = installation_ids_train\n        self.train.reset_index(drop=True, inplace=True)\n\n        for ins_id, user_sample in tqdm_notebook(\n                test_df.groupby(\"installation_id\", sort=False),\n                total=test_df[\"installation_id\"].nunique(),\n                desc=\"test features\"):\n            feats = KernelFeatures(all_activities, all_event_codes,\n                                   activities_map, inverse_activities_map)\n            feat_df = feats.create_features(user_sample, test=True)\n            installation_ids_test.extend([ins_id] * len(feat_df))\n            compiled_data_test.append(feat_df)\n        self.test = pd.concat(compiled_data_test, axis=0, sort=False)\n        self.test[\"installation_id\"] = installation_ids_test\n        self.test.reset_index(drop=True, inplace=True)\n\n\nclass KernelFeatures(PartialFeature):\n    def __init__(self, all_activities: set, all_event_codes: set,\n                 activities_map: Dict[str, float],\n                 inverse_activities_map: Dict[float, str]):\n        self.all_activities = all_activities\n        self.all_event_codes = all_event_codes\n        self.activities_map = activities_map\n        self.inverse_activities_map = inverse_activities_map\n\n        win_code = dict(\n            zip(activities_map.values(),\n                (4100 * np.ones(len(activities_map))).astype(int)))\n        win_code[activities_map[\"Bird Measurer (Assessment)\"]] = 4110\n        self.win_code = win_code\n\n        super().__init__()\n\n    def create_features(self, df: pd.DataFrame, test: bool = False):\n        time_spent_each_act = {act: 0 for act in self.all_activities}\n        event_code_count = {ev: 0 for ev in self.all_event_codes}\n        user_activities_count: Dict[IoS, IoF] = {\n            \"Clip\": 0,\n            \"Activity\": 0,\n            \"Assessment\": 0,\n            \"Game\": 0\n        }\n\n        all_assesments = []\n\n        accumulated_acc_groups = 0\n        accumulated_acc = 0\n        accumulated_correct_attempts = 0\n        accumulated_failed_attempts = 0\n        accumulated_actions = 0\n\n        counter = 0\n\n        accuracy_group: Dict[int, int] = {0: 0, 1: 0, 2: 0, 3: 0}\n\n        durations: List[float] = []\n        last_activity = \"\"\n\n        for i, sess in df.groupby(\"game_session\", sort=False):\n            sess_type = sess[\"type\"].iloc[0]\n            sess_title = sess[\"title\"].iloc[0]\n\n            if sess_type != \"Assessment\":\n                time_spent = int(sess[\"game_time\"].iloc[-1] / 1000)\n                time_spent_each_act[\n                    self.inverse_activities_map[sess_title]] += time_spent\n\n            if sess_type == \"Assessment\" and (test or len(sess) > 1):\n                all_attempts: pd.DataFrame = sess.query(\n                    f\"event_code == {self.win_code[sess_title]}\")\n                true_attempt = all_attempts[\"event_data\"].str.contains(\n                    \"true\").sum()\n                false_attempt = all_attempts[\"event_data\"].str.contains(\n                    \"false\").sum()\n\n                features = user_activities_count.copy()\n                features.update(time_spent_each_act.copy())\n                features.update(event_code_count.copy())\n\n                features[\"session_title\"] = sess_title\n\n                features[\"accumulated_correct_attempts\"] = \\\n                    accumulated_correct_attempts\n                features[\"accumulated_failed_attempts\"] = \\\n                    accumulated_failed_attempts\n\n                accumulated_correct_attempts += true_attempt\n                accumulated_failed_attempts += false_attempt\n\n                features[\"duration_mean\"] = np.mean(\n                    durations) if durations else 0\n                durations.append((sess.iloc[-1, 2] - sess.iloc[0, 2]).seconds)\n\n                features[\"accumulated_acc\"] = \\\n                    accumulated_acc / counter if counter > 0 else 0\n\n                acc = true_attempt / (true_attempt + false_attempt) \\\n                    if (true_attempt + false_attempt) != 0 else 0\n                accumulated_acc += acc\n\n                if acc == 0:\n                    features[\"accuracy_group\"] = 0\n                elif acc == 1:\n                    features[\"accuracy_group\"] = 3\n                elif acc == 0.5:\n                    features[\"accuracy_group\"] = 2\n                else:\n                    features[\"accuracy_group\"] = 1\n\n                features.update(accuracy_group.copy())\n                accuracy_group[features[\"accuracy_group\"]] += 1\n\n                features[\"accumulated_accuracy_group\"] = \\\n                    accumulated_acc_groups / counter if counter > 0 else 0\n                accumulated_acc_groups += features[\"accuracy_group\"]\n\n                features[\"accumulated_actions\"] = accumulated_actions\n\n                if test:\n                    all_assesments.append(features)\n                elif true_attempt + false_attempt > 0:\n                    all_assesments.append(features)\n\n                counter += 1\n\n            num_event_codes: dict = sess[\"event_code\"].value_counts().to_dict()\n            for k in num_event_codes.keys():\n                event_code_count[k] += num_event_codes[k]\n\n            accumulated_actions += len(sess)\n            if last_activity != sess_type:\n                user_activities_count[sess_type] + +1\n                last_activity = sess_type\n\n        if test:\n            self.df = pd.DataFrame([all_assesments[-1]])\n        else:\n            self.df = pd.DataFrame(all_assesments)\n\n        return self.df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## main"},{"metadata":{},"cell_type":"markdown","source":"### Settings"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\ndebug = True\nconfig_path = \"../config/cat_0.yml\"\nlog_dir = \"../log/\"\n\nconfigure_logger(config_path, log_dir, debug)\n\nlogging.info(f\"config: {config_path}\")\nlogging.info(f\"debug: {debug}\")\n\nconfig[\"args\"] = dict()\nconfig[\"args\"][\"config\"] = config_path\n\n# make output dir\noutput_root_dir = Path(config[\"output_dir\"])\nfeature_dir = Path(config[\"dataset\"][\"feature_dir\"])\n\nconfig_name: str = config_path.split(\"/\")[-1].replace(\".yml\", \"\")\noutput_dir = output_root_dir / config_name\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nlogging.info(f\"model output dir: {str(output_dir)}\")\n\nconfig[\"model_output_dir\"] = str(output_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data/Feature Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = Path(config[\"dataset\"][\"dir\"])\n\nif not feature_existence_checker(feature_dir, config[\"features\"]):\n    with timer(name=\"load data\", log=True):\n        train = pd.read_csv(input_dir / \"train.csv\")\n        test = pd.read_csv(input_dir / \"test.csv\")\n        specs = pd.read_csv(input_dir / \"specs.csv\")\n        \n    generate_features(\n        train, test, namespace=globals(), overwrite=False, log=True)\n\n    del train, test\n    gc.collect()\n\nwith timer(\"feature laoding\", log=True):\n    x_train = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_train.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ],\n                        axis=1,\n                        sort=False)\n    x_test = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_test.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ])\n\ngroups = x_train[\"installation_id\"].values\ny_train = x_train[\"accuracy_group\"].values.reshape(-1)\ncols: List[str] = x_train.columns.tolist()\ncols.remove(\"installation_id\")\ncols.remove(\"accuracy_group\")\nx_train, x_test = x_train[cols], x_test[cols]\n\nassert len(x_train) == len(y_train)\nlogging.debug(f\"number of features: {len(cols)}\")\nlogging.debug(f\"number of train samples: {len(x_train)}\")\nlogging.debug(f\"numbber of test samples: {len(x_test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adversarial Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Adversarial Validation\")\ntrain_adv = x_train.copy()\ntest_adv = x_test.copy()\n\ntrain_adv[\"target\"] = 0\ntest_adv[\"target\"] = 1\ntrain_test_adv = pd.concat([train_adv, test_adv], axis=0,\n                           sort=False).reset_index(drop=True)\n\nsplit_params: dict = config[\"av\"][\"split_params\"]\ntrain_set, val_set = train_test_split(\n    train_test_adv,\n    random_state=split_params[\"random_state\"],\n    test_size=split_params[\"test_size\"])\nx_train_adv = train_set[cols]\ny_train_adv = train_set[\"target\"]\nx_val_adv = val_set[cols]\ny_val_adv = val_set[\"target\"]\n\nlogging.debug(f\"The number of train set: {len(x_train_adv)}\")\nlogging.debug(f\"The number of valid set: {len(x_val_adv)}\")\n\ntrain_lgb = lgb.Dataset(x_train_adv, label=y_train_adv)\nvalid_lgb = lgb.Dataset(x_val_adv, label=y_val_adv)\n\nmodel_params = config[\"av\"][\"model_params\"]\ntrain_params = config[\"av\"][\"train_params\"]\nclf = lgb.train(\n    model_params,\n    train_lgb,\n    valid_sets=[train_lgb, valid_lgb],\n    valid_names=[\"train\", \"valid\"],\n    **train_params)\n\n# Check the feature importance\nfeature_imp = pd.DataFrame(\n    sorted(zip(clf.feature_importance(importance_type=\"gain\"), cols)),\n    columns=[\"value\", \"feature\"])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"LightGBM Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_adv.png\")\n\nconfig[\"av_result\"] = dict()\nconfig[\"av_result\"][\"score\"] = clf.best_score\nconfig[\"av_result\"][\"feature_importances\"] = \\\n    feature_imp.set_index(\"feature\").sort_values(\n        by=\"value\",\n        ascending=False\n    ).head(100).to_dict()[\"value\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Train model\")\n\n# get folds\nx_train[\"group\"] = groups\nsplits = get_validation(x_train, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nmodel = get_model(config)\nmodels, oof_preds, test_preds, feature_importance, eval_results = model.cv(\n    y_train, x_train, x_test, cols, splits, config, log=True)\n\nconfig[\"eval_results\"] = dict()\nfor k, v in eval_results.items():\n    config[\"eval_results\"][k] = v\n\nfeature_imp = feature_importance.reset_index().rename(columns={\n    \"index\": \"feature\",\n    0: \"value\"\n})\nplt.figure(figsize=(20, 10))\nsns.barplot(\n    x=\"value\",\n    y=\"feature\",\n    data=feature_imp.sort_values(by=\"value\", ascending=False).head(50))\nplt.title(\"Model Features\")\nplt.tight_layout()\nplt.savefig(output_dir / \"feature_importance_model.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save"},{"metadata":{"trusted":true},"cell_type":"code","source":"save_path = output_dir / \"output.json\"\nsave_json(config, save_path)\nnp.save(output_dir / \"oof_preds.npy\", oof_preds)\n\nwith open(output_dir / \"model.pkl\", \"wb\") as m:\n    pickle.dump(models, m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\n    input_dir / \"sample_submission.csv\")\nsample_submission[\"accuracy_group\"] = np.round(test_preds).astype('int')\nsample_submission.to_csv('submission.csv', index=None)\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}