{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import abc\nimport codecs\nimport inspect\nimport json\nimport logging\nimport gc\nimport os\nimport pickle\nimport random\nimport sys\nimport time\nimport warnings\n\nimport catboost as cat\nimport category_encoders as ce\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.optimize\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as torchdata\nimport yaml\n\nfrom abc import abstractmethod\nfrom collections import Counter\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import List, Optional, Union, Tuple, Dict, Sequence, Set\n\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom fastprogress import progress_bar\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.utils.multiclass import unique_labels\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_string = '''\ndataset:\n  dir: \"../input/data-science-bowl-2019/\"\n  feature_dir: \"features\"\n  params:\n\nfeatures:\n  - PastSummary3\n  - NakamaV8\n\nav:\n  split_params:\n    n_splits: 5\n    random_state: 42\n\n  model_params:\n    objective: \"binary\"\n    metric: \"auc\"\n    boosting: \"gbdt\"\n    max_depth: 7\n    num_leaves: 75\n    learning_rate: 0.01\n    colsample_bytree: 0.7\n    subsample: 0.1\n    subsample_freq: 1\n    seed: 111\n    feature_fraction_seed: 111\n    drop_seed: 111\n    verbose: -1\n    n_jobs: -1\n    first_metric_only: True\n\n  train_params:\n    num_boost_round: 50000\n    early_stopping_rounds: 200\n    verbose_eval: 200\n\nmodel:\n  name: \"mlp\"\n  mode: \"ovr\"\n  save_path: \"pth/\"\n  policy: \"best_score\"\n\n  model_params:\n    emb_drop: 0.3\n    drop: 0.5\n\n  train_params:\n    batch_size: 256\n    n_epochs: 50\n    lr: 0.001\n    scheduler:\n      name: \"cosine\"\n      T_max: 10\n      eta_min: 0.00001\n\npost_process:\n  params:\n    reverse: False\n    n_overall: 20\n    n_classwise: 20\n\nval:\n  name: \"group_kfold\"\n  n_delete: 0.9\n  percentile: 60\n  params:\n    n_splits: 5\n    random_state: 111\n\noutput_dir: \"output\"\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = dict(yaml.load(conf_string, Loader=yaml.SafeLoader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n    \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions and Classes"},{"metadata":{},"cell_type":"markdown","source":"### utils"},{"metadata":{},"cell_type":"markdown","source":"#### checker"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_existence_checker(feature_path: Path,\n                              feature_names: List[str]) -> bool:\n    features = [f.name for f in feature_path.glob(\"*.ftr\")]\n    for f in feature_names:\n        if f + \"_train.ftr\" not in features:\n            return False\n        if f + \"_valid.ftr\" not in features:\n            return False\n        if f + \"_test.ftr\" not in features:\n            return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### jsonutil"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\n\ndef save_json(config: dict, save_path: Union[str, Path]):\n    f = codecs.open(str(save_path), mode=\"w\", encoding=\"utf-8\")\n    json.dump(config, f, indent=4, cls=MyEncoder, ensure_ascii=False)\n    f.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### logger"},{"metadata":{"trusted":true},"cell_type":"code","source":"def configure_logger(config_name: str, log_dir: Union[Path, str], debug: bool):\n    if isinstance(log_dir, str):\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n    else:\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n    log_filename = config_name.split(\"/\")[-1].replace(\".yml\", \".log\")\n    log_filepath = log_dir / log_filename \\\n        if isinstance(log_dir, Path) else Path(log_dir) / log_filename\n\n    # delete the old log\n    if log_filepath.exists():\n        with open(log_filepath, mode=\"w\"):\n            pass\n\n    level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        filename=str(log_filepath),\n        level=level,\n        format=\"%(asctime)s %(levelname)s %(message)s\",\n        datefmt=\"%m/%d/%Y %I:%M:%S %p\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### timer"},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(name: str, log: bool = False):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if not log:\n        print(msg)\n    else:\n        logging.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if not log:\n        print(msg)\n    else:\n        logging.info(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true,\n                          y_pred,\n                          classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues,\n                          save_path: Path = Path(\"./\")):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, fontsize=25)\n    plt.yticks(tick_marks, fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)\n    plt.ylabel('True label', fontsize=25)\n    plt.title(title, fontsize=30)\n\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes('right', size=\"5%\", pad=0.15)\n    cbar = ax.figure.colorbar(im, ax=ax, cax=cax)\n    cbar.ax.tick_params(labelsize=20)\n\n    # We want to show all ticks...\n    ax.set(\n        xticks=np.arange(cm.shape[1]),\n        yticks=np.arange(cm.shape[0]),\n        # ... and label them with the respective list entries\n        xticklabels=classes,\n        yticklabels=classes,\n        #            title=title,\n        ylabel='True label',\n        xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), ha=\"right\", rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(\n                j,\n                i,\n                format(cm[i, j], fmt),\n                fontsize=20,\n                ha=\"center\",\n                va=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    plt.savefig(save_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True,\n                     debug: bool = True) -> pd.DataFrame:\n    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == \"int\":\n                if (c_min > np.iinfo(np.int8).min\n                        and c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min\n                      and c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    reduction = (start_mem - end_mem) / start_mem\n\n    msg = f\"Mem. usage decreased to {end_mem:5.2f} MB\" + \\\n        f\" ({reduction * 100:.1f} % reduction)\"\n    if verbose:\n        print(msg)\n\n    if debug:\n        logging.debug(msg)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### duplicate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def delete_duplicated_columns(df: pd.DataFrame):\n    df = df.loc[:, ~df.columns.duplicated()]\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_kfold(df: pd.DataFrame, groups: pd.Series,\n                config: dict) -> List[Tuple[np.ndarray, np.ndarray]]:\n    params = config[\"val\"][\"params\"]\n    kf = KFold(\n        n_splits=params[\"n_splits\"],\n        random_state=params[\"random_state\"],\n        shuffle=True)\n    uniq_groups = groups.unique()\n    split = []\n    for trn_grp_idx, val_grp_idx in kf.split(uniq_groups):\n        trn_grp = uniq_groups[trn_grp_idx]\n        val_grp = uniq_groups[val_grp_idx]\n        trn_idx = df[df[\"group\"].isin(trn_grp)].index.values\n        val_idx = df[df[\"group\"].isin(val_grp)].index.values\n        split.append((trn_idx, val_idx))\n\n    return split\n\n\ndef get_validation(df: pd.DataFrame,\n                   config: dict) -> List[Tuple[np.ndarray, np.ndarray]]:\n    name: str = config[\"val\"][\"name\"]\n\n    func = globals().get(name)\n    if func is None:\n        raise NotImplementedError\n\n    if \"group\" in name:\n        groups = df[\"group\"]\n        return func(df, groups, config)\n    else:\n        return func(df, config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### feature_selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_features(cols: List[str],\n                    feature_importance: pd.DataFrame,\n                    config: dict,\n                    delete_higher_importance: bool = False) -> List[str]:\n    if config[\"val\"].get(\"n_delete\") is None:\n        return cols\n\n    n_delete = config[\"val\"].get(\"n_delete\")\n    importance_sorted_cols = feature_importance.sort_values(\n        by=\"value\",\n        ascending=not (delete_higher_importance))[\"feature\"].tolist()\n    if isinstance(n_delete, int):\n        remove_cols = importance_sorted_cols[:n_delete]\n        cols = [col for col in cols if col not in remove_cols]\n    elif isinstance(n_delete, float):\n        n_delete_int = int(n_delete * len(importance_sorted_cols))\n        remove_cols = importance_sorted_cols[:n_delete_int]\n        cols = [col for col in cols if col not in remove_cols]\n    return cols\n\n\ndef remove_correlated_features(df: pd.DataFrame, features: List[str]):\n    counter = 0\n    to_remove: List[str] = []\n    for i in tqdm_notebook(range(len(features) - 1)):\n        feat_a = features[i]\n        for j in range(i + 1, len(features)):\n            feat_b = features[j]\n            if feat_a in to_remove or feat_b in to_remove:\n                continue\n            c = np.corrcoef(df[feat_a], df[feat_b])[0][1]\n            if c > 0.995:\n                counter += 1\n                to_remove.append(feat_b)\n                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(\n                    counter, feat_a, feat_b, c))\n    return to_remove","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### nth_assessment"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_assessment_number(valid_df: pd.DataFrame, test_df: pd.DataFrame):\n    valid_nth_assessment: List[int] = []\n    for _, sample in valid_df.groupby(\"installation_id\"):\n        valid_nth_assessment.extend(np.arange(len(sample)) + 1)\n\n    valid_df_ = valid_df.copy()\n    valid_df_[\"nth_assessment\"] = valid_nth_assessment\n\n    test_nth_assessment = []\n    for inst_id in test_df[\"installation_id\"].values:\n        if inst_id in valid_df_[\"installation_id\"].values:\n            test_nth_assessment.append(\n                valid_df_.query(f\"installation_id == '{inst_id}'\")\n                [\"nth_assessment\"].max() + 1)\n        else:\n            test_nth_assessment.append(1)\n    return np.array(test_nth_assessment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### sampling"},{"metadata":{},"cell_type":"markdown","source":"#### factory"},{"metadata":{"trusted":true},"cell_type":"code","source":"def smote(x_trn: np.ndarray, y_trn: np.ndarray,\n          config: dict) -> Tuple[np.ndarray, np.ndarray]:\n    params = config[\"model\"][\"sampling\"][\"params\"]\n    sm = SMOTE(\n        k_neighbors=params[\"k_neighbors\"], random_state=params[\"random_state\"])\n    sampled_x, sampled_y = sm.fit_resample(x_trn, y_trn)\n    return sampled_x, sampled_y\n\n\ndef random_under_sample(x_trn: np.ndarray, y_trn: np.ndarray,\n                        config: dict) -> Tuple[np.ndarray, np.ndarray]:\n    params = config[\"model\"][\"sampling\"][\"params\"]\n    acc_0 = (y_trn == 0).sum().astype(int)\n    acc_1 = (y_trn == 1).sum().astype(int)\n    acc_2 = (y_trn == 2).sum().astype(int)\n    acc_3 = (y_trn == 3).sum().astype(int)\n    rus = RandomUnderSampler({\n        0: int(params[\"acc_0_coef\"] * acc_0),\n        1: int(params[\"acc_1_coef\"] * acc_1),\n        2: int(params[\"acc_2_coef\"] * acc_2),\n        3: int(params[\"acc_3_coef\"] * acc_3)\n    },\n                             random_state=params[\"random_state\"])\n    sampled_x, sampled_y = rus.fit_resample(x_trn, y_trn)\n    return sampled_x, sampled_y\n\n\ndef random_under_sample_and_smote(\n        x_trn: np.ndarray, y_trn: np.ndarray,\n        config: dict) -> Tuple[np.ndarray, np.ndarray]:\n    sampled_x, sampled_y = random_under_sample(x_trn, y_trn, config)\n    sampled_x, sampled_y = smote(sampled_x, sampled_y, config)\n    return sampled_x, sampled_y\n\n\ndef get_sampling(x_trn: np.ndarray, y_trn: np.ndarray,\n                 config: dict) -> Tuple[np.ndarray, np.ndarray]:\n    if config[\"model\"][\"sampling\"][\"name\"] == \"none\":\n        return x_trn, y_trn\n\n    policy = config[\"model\"][\"sampling\"][\"name\"]\n    func = globals().get(policy)\n    if func is None:\n        raise NotImplementedError\n    return func(x_trn, y_trn, config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### evaluation"},{"metadata":{},"cell_type":"markdown","source":"#### metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def qwk(y_true: Union[np.ndarray, list],\n        y_pred: Union[np.ndarray, list],\n        max_rat: int = 3) -> float:\n    y_true_ = np.asarray(y_true)\n    y_pred_ = np.asarray(y_pred)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    uniq_class = np.unique(y_true_)\n    for i in uniq_class:\n        hist1[int(i)] = len(np.argwhere(y_true_ == i))\n        hist2[int(i)] = len(np.argwhere(y_pred_ == i))\n\n    numerator = np.square(y_true_ - y_pred_).sum()\n\n    denominator = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            denominator += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    denominator /= y_true_.shape[0]\n    return 1 - numerator / denominator\n\n\ndef calc_metric(y_true: Union[np.ndarray, list],\n                y_pred: Union[np.ndarray, list]) -> float:\n    return qwk(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self,\n                 n_overall: int = 5,\n                 n_classwise: int = 5,\n                 reverse: bool = False):\n        self.n_overall = n_overall\n        self.n_classwise = n_classwise\n        self.coef = [0.25, 0.5, 0.75]\n        self.reverse = reverse\n\n    def _loss(self, X: np.ndarray, y: np.ndarray) -> float:\n        X_p = np.digitize(X, self.coef)\n        ll = -calc_metric(y, X_p)\n        return ll\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [(0.01, 0.5), (0.5, 0.7), (0.7, 0.9)]\n        for _ in range(self.n_overall):\n            if self.reverse:\n                search = reversed(range(3))\n            else:\n                search = iter(range(3))\n            for idx in search:\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                self.coef[idx] = a\n                la = self._loss(X, y)\n                self.coef[idx] = b\n                lb = self._loss(X, y)\n                for it in range(self.n_classwise):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        self.coef[idx] = a\n                        la = self._loss(X, y)\n                    else:\n                        b = b - (b - a) * golden2\n                        self.coef[idx] = b\n                        lb = self._loss(X, y)\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        X_p = np.digitize(X, self.coef)\n        return X_p\n    \n    \nclass GroupWiseOptimizer(object):\n    def __init__(self, n_overall: int = 5, n_classwise: int = 5):\n        self.n_overall = n_overall\n        self.n_classwise = n_classwise\n\n    def fit(self, X: np.ndarray, y: np.ndarray, group: np.ndarray):\n        self.rounders = {\n            gp: OptimizedRounder(\n                n_overall=self.n_overall, n_classwise=self.n_classwise)\n            for gp in np.unique(group)\n        }\n        for gp in self.rounders.keys():\n            X_gp = X[group == gp]\n            y_gp = y[group == gp]\n            self.rounders[gp].fit(X_gp, y_gp)\n\n    def predict(self, X: np.ndarray, group: np.ndarray) -> np.ndarray:\n        result = np.zeros_like(X)\n        for gp in self.rounders.keys():\n            X_gp = X[group == gp]\n            result[group == gp] = self.rounders[gp].predict(X_gp)\n        return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### cat"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CatBoostOptimizedQWKMetric(object):\n    def __init__(self,\n                 n_overall: int = 5,\n                 n_classwise: int = 5,\n                 reverse: bool = False):\n        self.n_overall = n_overall\n        self.n_classwise = n_classwise\n        self.reverse = reverse\n\n    def get_final_error(self, error: float, weight: float) -> float:\n        return error / (weight + 1e-38)\n\n    def is_max_optimal(self):\n        return True\n\n    def evaluate(self, approxes, target,\n                 weight: Optional[Sequence[float]]) -> Tuple[float, float]:\n        assert len(approxes) == 1\n        assert len(target) == len(approxes[0])\n\n        approx = approxes[0]\n        approx_np = np.array(approx)\n        target_np = (np.array(target) * 3).astype(int)\n\n        OptR = OptimizedRounder(\n            n_classwise=self.n_classwise,\n            n_overall=self.n_overall,\n            reverse=self.reverse)\n        OptR.fit(approx_np, target_np)\n\n        y_pred = OptR.predict(approx_np).astype(int)\n        y_true = target_np.astype(int)\n\n        weight_sum = 1.0\n        qwk = calc_metric(y_true, y_pred)\n        return qwk, weight_sum\n    \n    \nclass CatBoostMulticlassOptimizedQWK(object):\n    def __init__(self,\n                 n_overall: int = 5,\n                 n_classwise: int = 5,\n                 reverse: bool = False):\n        self.n_overall = n_overall\n        self.n_classwise = n_classwise\n        self.reverse = reverse\n\n    def get_final_error(self, error: float, weight: float) -> float:\n        return error / (weight + 1e-38)\n\n    def is_max_optimal(self):\n        return True\n\n    def evaluate(self, approxes, target,\n                 weight: Optional[Sequence[float]]) -> Tuple[float, float]:\n        approx_np = np.array(approxes)\n        target_np = np.array(target)\n\n        exponent = np.exp(approx_np)\n        softmax = exponent / np.sum(exponent, axis=0)\n        y_pred = np.arange(4) @ softmax / 3\n\n        OptR = OptimizedRounder(\n            n_classwise=self.n_classwise,\n            n_overall=self.n_overall,\n            reverse=self.reverse)\n        OptR.fit(y_pred, target_np)\n\n        y_pred = OptR.predict(y_pred).astype(int)\n        y_true = target_np.astype(int)\n\n        weight_sum = 1.0\n        qwk = calc_metric(y_true, y_pred)\n        return qwk, weight_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### lgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgb_classification_qwk(y_pred: np.ndarray,\n                           data: lgb.Dataset) -> Tuple[str, float, bool]:\n    y_true = data.get_label()\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return \"qwk\", calc_metric(y_true, y_pred), True\n\n\ndef lgb_regression_qwk(y_pred: np.ndarray,\n                       data: lgb.Dataset) -> Tuple[str, float, bool]:\n    y_true = (data.get_label() * 3).astype(int)\n    y_pred = y_pred.reshape(-1)\n\n    OptR = OptimizedRounder(n_classwise=3, n_overall=3)\n    OptR.fit(y_pred, y_true)\n\n    y_pred = OptR.predict(y_pred).astype(int)\n    qwk = calc_metric(y_true, y_pred)\n\n    return \"qwk\", qwk, True\n\n\ndef lgb_multiclass_qwk(y_pred: np.ndarray,\n                       data: lgb.Dataset) -> Tuple[str, float, bool]:\n    y_true = data.get_label()\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1)\n    y_pred_regr = np.arange(4) @ y_pred / 3\n\n    OptR = OptimizedRounder(n_classwise=3, n_overall=3)\n    OptR.fit(y_pred_regr, y_true)\n\n    y_pred = OptR.predict(y_pred_regr).astype(int)\n    return \"qwk\", calc_metric(y_true, y_pred), True\n\n\ndef lgb_residual_qwk_closure(mean_target: np.ndarray):\n    def lgb_residual_qwk(y_pred: np.ndarray,\n                         data: lgb.Dataset) -> Tuple[str, float, bool]:\n        y_true = (data.get_label() * 3).astype(int)\n        y_pred = y_pred.reshape(-1)\n\n        y_true = (y_true + mean_target).astype(int)\n        y_pred = y_pred + mean_target\n\n        OptR = OptimizedRounder(n_classwise=5, n_overall=5)\n        OptR.fit(y_pred, y_true)\n\n        y_pred = OptR.predict(y_pred).astype(int)\n        qwk = calc_metric(y_true, y_pred)\n\n        return \"qwk\", qwk, True\n\n    return lgb_residual_qwk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### truncate"},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_with_truncated_data(\n        y_pred: np.ndarray,\n        y_true: np.ndarray,\n        groups: np.ndarray,\n        n_trials: int = 10) -> Dict[str, Union[List[float], float]]:\n    eval_result: Dict[str, Union[List[float], float]] = {}\n    trials: List[float] = []\n\n    index = np.arange(len(y_pred))\n    gp_idx_df = pd.DataFrame({\"groups\": groups, \"index\": index})\n    dice_results = []\n    for _, df in gp_idx_df.groupby(\"groups\"):\n        dice_result = np.random.choice(df[\"index\"], size=n_trials)\n        dice_results.append(dice_result)\n\n    idx_choice = np.vstack(dice_results)\n    for i in range(n_trials):\n        y_pred_choice = y_pred[idx_choice[:, i]]\n        y_true_choice = y_true[idx_choice[:, i]]\n        trials.append(calc_metric(y_true_choice, y_pred_choice))\n\n    mean_score = np.mean(trials)\n    median_score = np.median(trials)\n    std = np.std(trials)\n    eval_result[\"mean\"] = mean_score\n    eval_result[\"median\"] = median_score\n    eval_result[\"all_trials\"] = trials\n    eval_result[\"0.95lower_bound\"] = mean_score - 2 * std\n    eval_result[\"0.95upper_bound\"] = mean_score + 2 * std\n    eval_result[\"std\"] = std\n    return eval_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### models"},{"metadata":{},"cell_type":"markdown","source":"#### base"},{"metadata":{"trusted":true},"cell_type":"code","source":"# type alias\nAoD = Union[np.ndarray, pd.DataFrame]\nAoS = Union[np.ndarray, pd.Series]\nCatModel = Union[cat.CatBoostClassifier, cat.CatBoostRegressor]\nLGBModel = Union[lgb.LGBMClassifier, lgb.LGBMRegressor]\nModel = Union[CatModel, LGBModel]\n\n\nclass BaseModel(object):\n    @abstractmethod\n    def fit(self, x_train: AoD, y_train: AoS, x_valid: AoD, y_valid: AoS,\n            x_valid2: Optional[AoD], y_valid2: Optional[AoS],\n            config: dict) -> Tuple[Model, dict]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_best_iteration(self, model: Model) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model: Model, features: AoD) -> np.ndarray:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self, model: Model) -> np.ndarray:\n        raise NotImplementedError\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     valid_preds: Optional[np.ndarray], y: np.ndarray,\n                     y_valid: Optional[np.ndarray], config: dict\n                     ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:\n        return oof_preds, test_preds, valid_preds\n\n    def cv(self,\n           y_train: AoS,\n           train_features: AoD,\n           test_features: AoD,\n           y_valid: Optional[AoS],\n           valid_features: Optional[AoD],\n           feature_name: List[str],\n           folds_ids: List[Tuple[np.ndarray, np.ndarray]],\n           config: dict,\n           log: bool = True\n           ) -> Tuple[List[Model], np.ndarray, np.\n                      ndarray, Optional[np.ndarray], pd.DataFrame, dict]:\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        oof_preds = np.zeros(len(train_features))\n        if valid_features is not None:\n            valid_preds = np.zeros(len(valid_features))\n        else:\n            valid_preds = None\n        importances = pd.DataFrame(index=feature_name)\n        best_iteration = 0.0\n        cv_score_list: List[dict] = []\n        models: List[Model] = []\n\n        if config[\"model\"][\"mode\"] == \"residual\":\n            self.mean_targets: Dict[str, List[np.ndarray]] = {\n                \"train\": [],\n                \"valid\": [],\n                \"valid2\": [valid_features[\"mean_target\"].values],\n                \"test\": [test_features[\"mean_target\"].values]\n            }\n            valid_features.drop(\"mean_target\", axis=1, inplace=True)\n            test_features.drop(\"mean_target\", axis=1, inplace=True)\n            for t_idx, v_idx in folds_ids:\n                self.mean_targets[\"train\"].append(\n                    train_features.loc[t_idx, \"mean_target\"].values)\n                self.mean_targets[\"valid\"].append(\n                    train_features.loc[v_idx, \"mean_target\"].values)\n            train_features.drop(\"mean_target\", axis=1, inplace=True)\n            feature_name.remove(\"mean_target\")\n\n        X = train_features.values if isinstance(train_features, pd.DataFrame) \\\n            else train_features\n        y = y_train.values if isinstance(y_train, pd.Series) \\\n            else y_train\n\n        X_valid = valid_features.values if isinstance(\n            valid_features, pd.DataFrame) else valid_features\n        y_valid = y_valid.values if isinstance(y_valid, pd.Series) \\\n            else y_valid\n\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            self.fold = i_fold\n            # get train data and valid data\n            x_trn = X[trn_idx]\n            y_trn = y[trn_idx]\n            x_val = X[val_idx]\n            y_val = y[val_idx]\n\n            # train model\n            model, best_score = self.fit(\n                x_trn, y_trn, x_val, y_val, X_valid, y_valid, config=config)\n            cv_score_list.append(best_score)\n            models.append(model)\n            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n\n            # predict oof and test\n            oof_preds[val_idx] = self.predict(model, x_val).reshape(-1)\n            test_preds += self.predict(\n                model, test_features).reshape(-1) / len(folds_ids)\n\n            if valid_features is not None:\n                valid_preds += self.predict(\n                    model, valid_features).reshape(-1) / len(folds_ids)\n\n            if config[\"model\"][\"mode\"] == \"residual\":\n                oof_preds[val_idx] += self.mean_targets[\"valid\"][self.fold]\n                test_preds += self.mean_targets[\"test\"][0]\n                valid_preds += self.mean_targets[\"valid2\"][0]\n\n            # get feature importances\n            importances_tmp = pd.DataFrame(\n                self.get_feature_importance(model),\n                columns=[f\"gain_{i_fold+1}\"],\n                index=feature_name)\n            importances = importances.join(importances_tmp, how=\"inner\")\n\n        # summary of feature importance\n        feature_importance = importances.mean(axis=1)\n\n        # save raw prediction\n        self.raw_oof_preds = oof_preds\n        self.raw_test_preds = test_preds\n        self.raw_valid_preds = valid_preds\n\n        # post_process (if you have any)\n        oof_preds, test_preds, valid_preds = self.post_process(\n            oof_preds, test_preds, valid_preds, y_train, y_valid, config)\n\n        # print oof score\n        oof_score = calc_metric(y_train, oof_preds)\n        print(f\"oof score: {oof_score:.5f}\")\n        if valid_features is not None:\n            valid_score = calc_metric(y_valid, valid_preds)\n            print(f\"valid score: {valid_score:.5f}\")\n\n        if log:\n            logging.info(f\"oof score: {oof_score:.5f}\")\n            if valid_features is not None:\n                logging.info(f\"valid score: {valid_score:.5f}\")\n\n        evals_results = {\n            \"evals_result\": {\n                \"oof_score\":\n                oof_score,\n                \"cv_score\": {\n                    f\"cv{i + 1}\": cv_score\n                    for i, cv_score in enumerate(cv_score_list)\n                },\n                \"n_data\":\n                len(train_features),\n                \"best_iteration\":\n                best_iteration,\n                \"n_features\":\n                len(train_features.columns),\n                \"feature_importance\":\n                feature_importance.sort_values(ascending=False).to_dict()\n            }\n        }\n\n        if valid_features is not None:\n            evals_results[\"valid_score\"] = valid_score\n        return (models, oof_preds, test_preds, valid_preds, feature_importance,\n                evals_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### classwise"},{"metadata":{"trusted":true},"cell_type":"code","source":"# type alias\nAoD = Union[np.ndarray, pd.DataFrame]\nAoS = Union[np.ndarray, pd.Series]\nCatModel = Union[cat.CatBoostClassifier, cat.CatBoostRegressor]\nLGBModel = Union[lgb.LGBMClassifier, lgb.LGBMRegressor]\nModel = Union[CatModel, LGBModel]\n\n\nclass ClassWiseBase(object):\n    @abstractmethod\n    def fit(self, x_train: AoD, y_train: AoS,\n            valid_sets: List[Tuple[AoD, AoS]],\n            config: dict) -> Tuple[Model, dict]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_best_iteration(self, model: Model) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model: Model, features: AoD) -> np.ndarray:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self, model: Model) -> np.ndarray:\n        raise NotImplementedError\n\n    def post_process(self, preds_set: List[Tuple[np.ndarray, np.ndarray]],\n                     test_preds: np.ndarray,\n                     config: dict) -> Tuple[List[np.ndarray], np.ndarray]:\n        return [p[0] for p in preds_set], test_preds\n\n    def cv(self,\n           y_train: AoS,\n           train_features: AoD,\n           test_features: AoD,\n           y_valid: AoS,\n           valid_features: AoD,\n           feature_name: List[str],\n           folds_ids: List[Tuple[np.ndarray, np.ndarray]],\n           config: dict,\n           log: bool = True) -> Tuple[Dict[int, List[Model]], np.ndarray, np.\n                                      ndarray, np.ndarray, dict, dict]:\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        oof_preds = np.zeros(len(train_features))\n        valid_preds = np.zeros(len(valid_features))\n\n        all_classes = train_features[\"session_title\"].unique()\n\n        classwise_mean_importances = {}\n        classwise_best_iteration = {c: 0.0 for c in all_classes}\n        classwise_cv_score_list = {c: [] for c in all_classes}\n        classwise_models = {c: [] for c in all_classes}\n\n        X = train_features.values if isinstance(train_features, pd.DataFrame) \\\n            else train_features\n        y = y_train.values if isinstance(y_train, pd.Series) else y_train\n\n        X_valid = valid_features.values if isinstance(valid_features, pd.DataFrame) \\\n            else valid_features\n        y_valid = y_valid.values if isinstance(y_valid, pd.Series) else y_valid\n\n        for c in all_classes:\n            importances = pd.DataFrame(index=feature_name)\n            train_c_idx = train_features.query(\n                f\"session_title == {c}\").index.values\n            valid_c_idx = valid_features.query(\n                f\"session_title == {c}\").index.values\n            test_c_idx = test_features.query(\n                f\"session_title == {c}\").index.values\n\n            X_c_valid = X_valid[valid_c_idx]\n            y_c_valid = y_valid[valid_c_idx]\n\n            print(f\"Assessment Class: {c}\")\n\n            for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n                print(\"=\" * 15)\n                print(f\"Fold: {i_fold + 1}\")\n                print(\"=\" * 15)\n\n                trn_c_idx = np.intersect1d(trn_idx, train_c_idx)\n                val_c_idx = np.intersect1d(val_idx, train_c_idx)\n\n                # get train data and valid data\n                X_trn = X[trn_c_idx]\n                y_trn = y[trn_c_idx]\n                X_val = X[val_c_idx]\n                y_val = y[val_c_idx]\n\n                # train model\n                model, best_score = self.fit(\n                    X_trn,\n                    y_trn,\n                    valid_sets=[(X_val, y_val), (X_c_valid, y_c_valid)],\n                    config=config)\n                classwise_cv_score_list[c].append(best_score)\n                classwise_models[c].append(model)\n                classwise_best_iteration[c] += self.get_best_iteration(model)\n\n                # predict oof and test, valid\n                oof_preds[val_c_idx] = self.predict(model, X_val).reshape(-1)\n                test_preds[test_c_idx] += self.predict(\n                    model, test_features.loc[test_c_idx, :]).reshape(-1) / len(\n                        folds_ids)\n                valid_preds[valid_c_idx] += self.predict(\n                    model, X_c_valid).reshape(-1) / len(folds_ids)\n\n                # get feature importances\n                importances_tmp = pd.DataFrame(\n                    self.get_feature_importance(model),\n                    columns=[f\"class_{c}_gain_{i_fold+1}\"],\n                    index=feature_name)\n                importances = importances.join(importances_tmp, how=\"inner\")\n\n            # summary of feature importance\n            classwise_mean_importances[c] = importances.mean(axis=1)\n\n        # save raw prediction\n        self.raw_oof_preds = oof_preds\n        self.raw_test_preds = test_preds\n        self.raw_valid_preds = valid_preds\n\n        # post_process\n        [oof_preds, valid_preds], test_preds = self.post_process(\n            [(oof_preds, y_train), (valid_preds, y_valid)], test_preds, config)\n\n        # print oof score\n        oof_score = calc_metric(y_train, oof_preds)\n        print(f\"oof score: {oof_score:.5f}\")\n        valid_score = calc_metric(y_valid, valid_preds)\n        print(f\"valid score: {valid_score:.5f}\")\n\n        if log:\n            logging.info(f\"oof score: {oof_score:.5f}\")\n            logging.info(f\"valid score: {valid_score:.5f}\")\n\n        eval_results = {\n            \"eval_result\": {\n                \"oof_score\": oof_score,\n                \"valid_score\": valid_score,\n                \"cv_results\": {},\n                \"n_data\": len(train_features),\n                \"n_features\": len(train_features.columns),\n                \"best_iterations\": {\n                    f\"Assessment {c}\": v\n                    for c, v in classwise_best_iteration.items()\n                },\n                \"feature_importances\": {}\n            }\n        }\n\n        for c, v in classwise_cv_score_list.items():\n            eval_results[\"eval_result\"][\"cv_results\"][f\"Assessment {c}\"] = \\\n                {f\"cv{i + 1}\": cv_score for i, cv_score in enumerate(v)}\n\n        for c, fi in classwise_mean_importances.items():\n            eval_results[\"eval_result\"][\"feature_importances\"][\n                f\"Assessment {c}\"] = fi.sort_values(ascending=False).to_dict()\n\n        return (classwise_models, oof_preds, test_preds, valid_preds,\n                classwise_mean_importances, eval_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Base2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaseModel2(object):\n    @abstractmethod\n    def fit(self, x_train: AoD, y_train: AoS, x_valid: AoD, y_valid: AoS,\n            config: dict) -> Tuple[Model, dict]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_best_iteration(self, model: Model) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model: Model, features: AoD) -> np.ndarray:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self, model: Model) -> np.ndarray:\n        raise NotImplementedError\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     y: np.ndarray,\n                     config: dict) -> Tuple[np.ndarray, np.ndarray]:\n        return oof_preds, test_preds\n\n    def cv(self,\n           y_train: AoS,\n           train_features: AoD,\n           test_features: AoD,\n           groups: np.ndarray,\n           feature_name: List[str],\n           folds_ids: List[Tuple[np.ndarray, np.ndarray]],\n           threshold: float,\n           config: dict,\n           log: bool = True) -> Tuple[List[Model], np.ndarray, np.ndarray, np.\n                                      ndarray, pd.DataFrame, dict]:\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        normal_oof_preds = np.zeros(len(train_features))\n        oof_preds_list: List[np.ndarray] = []\n        y_val_list: List[np.ndarray] = []\n        idx_val_list: List[np.ndarray] = []\n\n        importances = pd.DataFrame(index=feature_name)\n        best_iteration = 0.0\n        cv_score_list: List[dict] = []\n        models: List[Model] = []\n\n        X = train_features.values if isinstance(train_features, pd.DataFrame) \\\n            else train_features\n        y = y_train.values if isinstance(y_train, pd.Series) \\\n            else y_train\n\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            self.fold = i_fold\n            # get train data and valid data\n            x_trn = X[trn_idx]\n            y_trn = y[trn_idx]\n\n            val_idx = np.sort(val_idx)\n            gp_val = groups[val_idx]\n\n            new_idx: List[int] = []\n            for gp in np.unique(gp_val):\n                gp_idx = val_idx[gp_val == gp]\n                new_idx.extend(gp_idx[:int(threshold)])\n\n            x_val = X[new_idx]\n            y_val = y[new_idx]\n            x_val_normal = X[val_idx]\n\n            # train model\n            model, best_score = self.fit(\n                x_trn, y_trn, x_val, y_val, config=config)\n            cv_score_list.append(best_score)\n            models.append(model)\n            best_iteration += self.get_best_iteration(model) / len(folds_ids)\n\n            # predict oof and test\n            oof_preds_list.append(self.predict(model, x_val).reshape(-1))\n            y_val_list.append(y_val)\n            idx_val_list.append(new_idx)\n            normal_oof_preds[val_idx] = self.predict(model,\n                                                     x_val_normal).reshape(-1)\n            test_preds += self.predict(\n                model, test_features).reshape(-1) / len(folds_ids)\n\n            # get feature importances\n            importances_tmp = pd.DataFrame(\n                self.get_feature_importance(model),\n                columns=[f\"gain_{i_fold+1}\"],\n                index=feature_name)\n            importances = importances.join(importances_tmp, how=\"inner\")\n\n        # summary of feature importance\n        feature_importance = importances.mean(axis=1)\n\n        oof_preds = np.concatenate(oof_preds_list)\n        y_oof = np.concatenate(y_val_list)\n        idx_val = np.concatenate(idx_val_list)\n\n        sorted_idx = np.argsort(idx_val)\n        oof_preds = oof_preds[sorted_idx]\n        y_oof = y_oof[sorted_idx]\n\n        # save raw prediction\n        self.raw_oof_preds = oof_preds\n        self.raw_test_preds = test_preds\n\n        # post_process (if you have any)\n        oof_preds, test_preds = self.post_process(oof_preds, test_preds, y_oof,\n                                                  config)\n\n        # print oof score\n        oof_score = calc_metric(y_oof, oof_preds)\n        print(f\"oof score: {oof_score:.5f}\")\n        \n        # normal oof score\n        self.raw_normal_oof = normal_oof_preds\n\n        # normal oof score\n        OptR = OptimizedRounder()\n        OptR.fit(normal_oof_preds, y_train)\n        normal_oof_preds = OptR.predict(normal_oof_preds)\n        normal_oof_score = calc_metric(y_train, normal_oof_preds)\n        print(f\"normal oof score: {normal_oof_score:.5f}\")\n\n        # truncated score\n        truncated_result = eval_with_truncated_data(\n            normal_oof_preds, y_train, groups, n_trials=100)\n        print(f\"truncated mean: {truncated_result['mean']:.5f}\")\n        print(f\"truncated std: {truncated_result['std']:.5f}\")\n\n        if log:\n            logging.info(f\"oof score: {oof_score:.5f}\")\n            logging.info(f\"normal oof score: {normal_oof_score:.5f}\")\n\n        evals_results = {\n            \"evals_result\": {\n                \"oof_score\":\n                oof_score,\n                \"normal_oof_score\":\n                normal_oof_score,\n                \"truncated_eval_mean\":\n                truncated_result[\"mean\"],\n                \"truncated_eval_0.95upper\":\n                truncated_result[\"0.95upper_bound\"],\n                \"truncated_eval_0.95lower\":\n                truncated_result[\"0.95lower_bound\"],\n                \"truncated_eval_std\":\n                truncated_result[\"std\"],\n                \"cv_score\": {\n                    f\"cv{i + 1}\": cv_score\n                    for i, cv_score in enumerate(cv_score_list)\n                },\n                \"n_data\":\n                len(train_features),\n                \"best_iteration\":\n                best_iteration,\n                \"n_features\":\n                len(train_features.columns),\n                \"feature_importance\":\n                feature_importance.sort_values(ascending=False).to_dict()\n            }\n        }\n\n        return (models, oof_preds, y_oof, test_preds, feature_importance,\n                evals_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### cat"},{"metadata":{"trusted":true},"cell_type":"code","source":"CatModel = Union[CatBoostClassifier, CatBoostRegressor]\n\n\nclass CatBoostModel(BaseModel):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            x_valid: np.ndarray, y_valid: np.ndarray,\n            x_valid2: Optional[np.ndarray], y_valid2: Optional[np.ndarray],\n            config: dict) -> Tuple[CatModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        mode = config[\"model\"][\"mode\"]\n        self.mode = mode\n        if mode == \"regression\":\n            model = CatBoostRegressor(\n                eval_metric=CatBoostOptimizedQWKMetric(\n                    reverse=config[\"post_process\"][\"params\"][\"reverse\"]),\n                **model_params)\n            self.denominator = y_train.max()\n            y_train = y_train / y_train.max()\n            y_valid = y_valid / self.denominator\n            if y_valid2 is not None:\n                y_valid2 = y_valid2 / self.denominator\n        elif mode == \"multiclass\":\n            model = CatBoostClassifier(**model_params)\n        else:\n            model = CatBoostClassifier(**model_params)\n\n        if x_valid2 is not None:\n            eval_sets = [(x_valid2, y_valid2)]\n        else:\n            eval_sets = [(x_valid, y_valid)]\n\n        model.fit(\n            x_train,\n            y_train,\n            eval_set=eval_sets,\n            use_best_model=True,\n            verbose=model_params[\"early_stopping_rounds\"])\n        best_score = model.best_score_\n        return model, best_score\n\n    def get_best_iteration(self, model: CatModel):\n        return model.best_iteration_\n\n    def predict(self, model: CatModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        if self.mode != \"multiclass\":\n            return model.predict(features)\n        else:\n            preds = model.predict_proba(features)\n            return preds @ np.arange(4) / 3\n\n    def get_feature_importance(self, model: CatModel) -> np.ndarray:\n        return model.feature_importances_\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     valid_preds: Optional[np.ndarray], y: np.ndarray,\n                     y_valid: Optional[np.ndarray], config: dict\n                     ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:\n        # Override\n        if self.mode == \"regression\" or self.mode == \"multiclass\":\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(oof_preds, y)\n            oof_preds_ = OptR.predict(oof_preds)\n            test_preds_ = OptR.predict(test_preds)\n            if valid_preds is not None:\n                valid_preds = OptR.predict(valid_preds)\n            return oof_preds_, test_preds_, valid_preds\n        return oof_preds, test_preds, valid_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### cat2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CatBoostModel2(BaseModel2):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            x_valid: np.ndarray, y_valid: np.ndarray,\n            config: dict) -> Tuple[CatModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        mode = config[\"model\"][\"mode\"]\n        self.mode = mode\n        if mode == \"regression\":\n            model = CatBoostRegressor(**model_params)\n            self.denominator = y_train.max()\n            y_train = y_train / y_train.max()\n            y_valid = y_valid / self.denominator\n\n        else:\n            model = CatBoostClassifier(**model_params)\n\n        eval_sets = [(x_valid, y_valid)]\n\n        model.fit(\n            x_train,\n            y_train,\n            eval_set=eval_sets,\n            use_best_model=True,\n            verbose=model_params[\"early_stopping_rounds\"])\n        best_score = model.best_score_\n        return model, best_score\n\n    def get_best_iteration(self, model: CatModel):\n        return model.best_iteration_\n\n    def predict(self, model: CatModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        if self.mode != \"multiclass\":\n            return model.predict(features)\n        else:\n            preds = model.predict_proba(features)\n            return preds @ np.arange(4) / 3\n\n    def get_feature_importance(self, model: CatModel) -> np.ndarray:\n        return model.feature_importances_\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     y: np.ndarray,\n                     config: dict) -> Tuple[np.ndarray, np.ndarray]:\n        # Override\n        if self.mode == \"regression\" or self.mode == \"multiclass\":\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(oof_preds, y)\n            oof_preds_ = OptR.predict(oof_preds)\n            test_preds_ = OptR.predict(test_preds)\n            return oof_preds_, test_preds_\n        return oof_preds, test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### lgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBMModel = Union[lgb.LGBMClassifier, lgb.LGBMRegressor]\n\n\nclass LightGBM(BaseModel):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            x_valid: np.ndarray, y_valid: np.ndarray,\n            x_valid2: Optional[np.ndarray], y_valid2: Optional[np.ndarray],\n            config: dict) -> Tuple[LGBMModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        train_params = config[\"model\"][\"train_params\"]\n\n        mode = config[\"model\"][\"mode\"]\n        self.mode = mode\n        if mode == \"regression\":\n            self.denominator = y_train.max()\n            y_train = y_train / self.denominator\n            y_valid = y_valid / self.denominator\n            if y_valid2 is not None:\n                y_valid2 = y_valid2 / self.denominator\n        elif mode == \"residual\":\n            y_train = y_train - self.mean_targets[\"train\"][self.fold]\n            y_valid = y_valid - self.mean_targets[\"valid\"][self.fold]\n            y_valid2 = y_valid2 - self.mean_targets[\"valid2\"][0]\n\n        d_train = lgb.Dataset(x_train, label=y_train)\n        d_valid = lgb.Dataset(x_valid, label=y_valid)\n\n        valid_sets: List[lgb.Dataset] = []\n        valid_names: List[str] = []\n        if x_valid2 is not None:\n            d_valid2 = lgb.Dataset(x_valid2, label=y_valid2)\n            valid_sets += [d_valid2, d_valid]\n            valid_names += [\"data_from_test\", \"data_from_train\"]\n        else:\n            valid_sets.append(d_valid)\n            valid_names.append(\"valid\")\n\n        if mode == \"regression\" or mode == \"residual\":\n            feval = lgb_regression_qwk if (mode == \"regression\") else None\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=valid_sets,\n                valid_names=valid_names,\n                feval=feval,  # FIXME: support for residual\n                **train_params)\n        elif mode == \"multiclass\":\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=valid_sets,\n                valid_names=valid_names,\n                **train_params)\n        else:\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=valid_sets,\n                valid_names=valid_names,\n                feval=lgb_classification_qwk,\n                **train_params)\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def get_best_iteration(self, model: LGBMModel) -> int:\n        return model.best_iteration\n\n    def predict(self, model: LGBMModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        if self.mode == \"regression\" or self.mode == \"residual\":\n            return model.predict(features)\n        elif self.mode == \"multiclass\":\n            pred = model.predict(features) @ np.arange(4) / 3\n            return pred\n        else:\n            return model.predict(features).reshape(4, -1).argmax(axis=0)\n\n    def get_feature_importance(self, model: LGBMModel) -> np.ndarray:\n        return model.feature_importance(importance_type=\"gain\")\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     valid_preds: Optional[np.ndarray], y: np.ndarray,\n                     y_valid: Optional[np.ndarray], config: dict\n                     ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:\n        # Override\n        if (self.mode == \"regression\" or self.mode == \"residual\"\n                or self.mode == \"multiclass\"):\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(oof_preds, y)\n            oof_preds_ = OptR.predict(oof_preds)\n            test_preds_ = OptR.predict(test_preds)\n            if valid_preds is not None:\n                valid_preds = OptR.predict(valid_preds)\n            return oof_preds_, test_preds_, valid_preds\n        return oof_preds, test_preds, valid_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### lgbm2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LightGBM2(BaseModel2):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            x_valid: np.ndarray, y_valid: np.ndarray,\n            config: dict) -> Tuple[LGBMModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        train_params = config[\"model\"][\"train_params\"]\n\n        mode = config[\"model\"][\"mode\"]\n        self.mode = mode\n        if mode == \"regression\":\n            self.denominator = y_train.max()\n            y_train = y_train / self.denominator\n            y_valid = y_valid / self.denominator\n\n        d_train = lgb.Dataset(x_train, label=y_train)\n        d_valid = lgb.Dataset(x_valid, label=y_valid)\n\n        if mode == \"regression\":\n            feval = lgb_regression_qwk if (mode == \"regression\") else None\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=[d_valid],\n                valid_names=[\"val\"],\n                feval=feval,\n                **train_params)\n        elif mode == \"multiclass\":\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=[d_valid],\n                valid_names=[\"val\"],\n                feval=lgb_multiclass_qwk,\n                **train_params)\n        else:\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=[d_valid],\n                valid_names=[\"val\"],\n                feval=lgb_classification_qwk,\n                **train_params)\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def get_best_iteration(self, model: LGBMModel) -> int:\n        return model.best_iteration\n\n    def predict(self, model: LGBMModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        if self.mode == \"regression\":\n            return model.predict(features)\n        elif self.mode == \"multiclass\":\n            pred = model.predict(features) @ np.arange(4) / 3\n            return pred\n        else:\n            return model.predict(features).reshape(4, -1).argmax(axis=0)\n\n    def get_feature_importance(self, model: LGBMModel) -> np.ndarray:\n        return model.feature_importance(importance_type=\"gain\")\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     y: np.ndarray,\n                     config: dict) -> Tuple[np.ndarray, np.ndarray]:\n        # Override\n        if (self.mode == \"regression\" or self.mode == \"multiclass\"):\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(oof_preds, y)\n            oof_preds_ = OptR.predict(oof_preds)\n            test_preds_ = OptR.predict(test_preds)\n            return oof_preds_, test_preds_\n        return oof_preds, test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### classwise_cat"},{"metadata":{"trusted":true},"cell_type":"code","source":"CatModel = Union[CatBoostClassifier, CatBoostRegressor]\n\n\nclass ClassWiseCatBoost(ClassWiseBase):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            valid_sets: List[Tuple[np.ndarray, np.ndarray]],\n            config: dict) -> Tuple[CatModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        mode = config[\"model\"][\"mode\"]\n        self.mode = mode\n\n        if mode == \"regression\":\n            model = CatBoostRegressor(\n                eval_metric=CatBoostOptimizedQWKMetric(\n                    reverse=config[\"post_process\"][\"params\"][\"reverse\"]),\n                **model_params)\n            self.denominator = y_train.max()\n            y_train = y_train / y_train.max()\n            eval_sets = []\n            for x, y in valid_sets:\n                eval_sets.append((x, y / self.denominator))\n        elif mode == \"multiclass\":\n            model = CatBoostClassifier(**model_params)\n            eval_sets = valid_sets\n        else:\n            model = CatBoostClassifier(**model_params)\n            eval_sets = valid_sets\n\n        model.fit(\n            x_train,\n            y_train,\n            eval_set=eval_sets,\n            use_best_model=True,\n            verbose=model_params[\"early_stopping_rounds\"])\n        best_score = model.best_score_\n        return model, best_score\n\n    def get_best_iteration(self, model: CatModel):\n        return model.best_iteration_\n\n    def predict(self, model: CatModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        if self.mode != \"multiclass\":\n            return model.predict(features)\n        else:\n            preds = model.predict_proba(features)\n            return preds @ np.arange(4) / 3\n\n    def get_feature_importance(self, model: CatModel) -> np.ndarray:\n        return model.feature_importances_\n\n    def post_process(\n            self, preds_set: List[Tuple[np.ndarray, np.ndarray]],\n            test_preds: np.ndarray, config: dict\n    ) -> Tuple[List[Tuple[np.ndarray, np.ndarray]], np.ndarray]:\n        # Override\n        if self.mode == \"regression\" or self.mode == \"multiclass\":\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(preds_set[0][0], preds_set[0][1])\n\n            return_set = [OptR.predict(l[0]) for l in preds_set]\n            test_preds = OptR.predict(test_preds)\n            return return_set, test_preds\n        return preds_set, test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### classwise_lgbm"},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBMModel = Union[lgb.LGBMClassifier, lgb.LGBMRegressor]\n\n\nclass ClassWiseLightGBM(ClassWiseBase):\n    def fit(self, x_train: np.ndarray, y_train: np.ndarray,\n            valid_sets: List[Tuple[np.ndarray, np.ndarray]],\n            config: dict) -> Tuple[LGBMModel, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        train_params = config[\"model\"][\"train_params\"]\n\n        mode = config[\"model\"][\"mode\"]\n        self.mode = mode\n        if mode == \"regression\":\n            self.denominator = y_train.max()\n            y_train = y_train / self.denominator\n            eval_sets = []\n            for x, y in valid_sets:\n                eval_sets.append(lgb.Dataset(x, label=y / self.denominator))\n        elif mode == \"multiclass\":\n            eval_sets = []\n            for x, y in valid_sets:\n                eval_sets.append(lgb.Dataset(x, label=y))\n\n        d_train = lgb.Dataset(x_train, label=y_train)\n\n        \n        if mode == \"regression\":\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=eval_sets,\n                feval=lgb_regression_qwk,\n                **train_params)\n        elif mode == \"multiclass\":\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=eval_sets,\n                **train_params)\n        else:\n            model = lgb.train(\n                params=model_params,\n                train_set=d_train,\n                valid_sets=eval_sets,\n                feval=lgb_classification_qwk,\n                **train_params)\n\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def get_best_iteration(self, model: LGBMModel) -> int:\n        return model.best_iteration\n\n    def predict(self, model: LGBMModel,\n                features: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n        if self.mode == \"regression\":\n            return model.predict(features)\n        elif self.mode == \"multiclass\":\n            pred = model.predict(features) @ np.arange(4) / 3\n            return pred\n        else:\n            return model.predict(features).reshape(4, -1).argmax(axis=0)\n\n    def get_feature_importance(self, model: LGBMModel) -> np.ndarray:\n        return model.feature_importance(importance_type=\"gain\")\n\n    def post_process(\n            self, preds_set: List[Tuple[np.ndarray, np.ndarray]],\n            test_preds: np.ndarray, config: dict\n    ) -> Tuple[List[Tuple[np.ndarray, np.ndarray]], np.ndarray]:\n        # Override\n        if self.mode == \"regression\" or self.mode == \"multiclass\":\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(preds_set[0][0], preds_set[0][1])\n\n            return_set = [OptR.predict(l[0]) for l in preds_set]\n            test_preds = OptR.predict(test_preds)\n            return return_set, test_preds\n        return preds_set, test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### nn"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DSBDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, categorical_features: List[str],\n                 y: Optional[AoS]=None):\n        non_categorical = [\n            col for col in df.columns\n            if col not in categorical_features\n        ]\n        self.non_categorical = df[non_categorical].values\n        self.categorical = df[categorical_features].values\n        self.y = y\n\n    def __len__(self):\n        return len(self.categorical)\n\n    def __getitem__(self, idx):\n        categorical = self.categorical[idx, :]\n        non_categorical = self.non_categorical[idx, :]\n        if self.y is not None:\n            y = self.y[idx]\n            return non_categorical, categorical, y\n        else:\n            return non_categorical, categorical\n\n\nclass DSBBase(nn.Module):\n    def __init__(\n            self,\n            cat_dims: List[Tuple[int, int]],\n            n_non_categorical: int,\n            emb_drop: float,\n            drop: float):\n        super().__init__()\n        self.n_non_categorical = n_non_categorical\n\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(x, y) for x, y in cat_dims\n        ])\n        n_emb_out = sum([y for x, y in cat_dims])\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.cat_bn = nn.BatchNorm1d(n_non_categorical + n_emb_out)\n        self.lin1 = nn.Linear(n_non_categorical + n_emb_out, 150)\n        self.bn1 = nn.BatchNorm1d(150)\n        self.drop = nn.Dropout(drop)\n        self.lin2 = nn.Linear(150, 50)\n        self.bn2 = nn.BatchNorm1d(50)\n\n    def forward(self, non_cat, cat) -> torch.Tensor:\n        emb = [\n            emb_layer(cat[:, j]) for j, emb_layer in enumerate(self.embeddings)\n        ]\n        emb = self.emb_drop(torch.cat(emb, 1))\n        concat = torch.cat([non_cat, emb], 1)\n        x = F.relu(self.bn1(self.lin1(concat)))\n        x = self.drop(x)\n        x = F.relu(self.bn2(self.lin2(x)))\n        return x\n\n\nclass DSBRegressor(nn.Module):\n    def __init__(\n            self,\n            cat_dims: List[Tuple[int, int]],\n            n_non_categorical: int,\n            **params):\n        super().__init__()\n        self.base = DSBBase(cat_dims, n_non_categorical, **params)\n        self.drop = nn.Dropout(0.3)\n        self.head = nn.Linear(50, 1)\n\n    def forward(self, non_cat, cat) -> torch.Tensor:\n        x = self.base(non_cat, cat)\n        x = self.drop(x)\n        x = F.relu(self.head(x))\n        return torch.clamp(x.view(-1), 0.0, 1.0)\n\n\nclass DSBClassifier(nn.Module):\n    def __init__(\n            self,\n            cat_dims: List[Tuple[int, int]],\n            n_non_categorical: int,\n            **params):\n        super().__init__()\n        self.base = DSBBase(cat_dims, n_non_categorical, **params)\n        self.drop = nn.Dropout(0.3)\n        self.head = nn.Linear(50, 4)\n\n    def forward(self, non_cat, cat) -> torch.Tensor:\n        x = self.base(non_cat, cat)\n        x = self.drop(x)\n        x = F.softmax(self.head(x))\n        return x\n    \n    \nclass DSBBinary(nn.Module):\n    def __init__(self, cat_dims: List[Tuple[int, int]], n_non_categorical: int,\n                 **params):\n        super().__init__()\n        self.base = DSBBase(cat_dims, n_non_categorical, **params)\n        self.drop = nn.Dropout(0.3)\n        self.head = nn.Linear(50, 1)\n\n    def forward(self, non_cat, cat) -> torch.Tensor:\n        x = self.base(non_cat, cat)\n        x = self.drop(x)\n        x = F.sigmoid(self.head(x))\n        return x.view(-1)\n\n\nclass DSBOvR(nn.Module):\n    def __init__(self, cat_dims: List[Tuple[int, int]], n_non_categorical: int,\n                 **params):\n        super().__init__()\n        self.base = DSBBase(cat_dims, n_non_categorical, **params)\n        self.drop = nn.Dropout(0.3)\n        self.head = nn.Linear(50, 4)\n\n    def forward(self, non_cat, cat) -> torch.Tensor:\n        x = self.base(non_cat, cat)\n        x = self.drop(x)\n        x = F.sigmoid(self.head(x))\n        return x\n\n\nclass DSBMultiTaskA(nn.Module):\n    def __init__(\n            self,\n            cat_dims: List[Tuple[int, int]],\n            n_non_categorical: int,\n            **params):\n        super().__init__()\n        self.base = DSBBase(cat_dims, n_non_categorical, **params)\n        self.drop = nn.Dropout(0.3)\n        self.regression_head = nn.Linear(50, 1)\n        self.multiclass_head = nn.Linear(50, 4)\n\n    def forward(self, non_cat, cat):\n        x = self.base(non_cat, cat)\n        x = self.drop(x)\n        x_regr = F.relu(self.regression_head(x)).view(-1)\n        x_multi = F.softmax(self.multiclass_head(x))\n        return x_regr, x_multi\n\n\nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y_pred, y_true):\n        return ((y_pred - y_true) ** 2).mean().sqrt()\n\n\nclass NNTrainer(object):\n    def fit(\n            self,\n            x_train: pd.DataFrame,\n            y_train: AoS,\n            x_valid: pd.DataFrame,\n            y_valid: AoS,\n            config: dict) -> Tuple[nn.Module, dict]:\n        model_params = config[\"model\"][\"model_params\"]\n        train_params = config[\"model\"][\"train_params\"]\n        mode = config[\"model\"][\"mode\"]\n        save_path = Path(config[\"model\"][\"save_path\"])\n        save_path.mkdir(parents=True, exist_ok=True)\n        self.mode = mode\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        n_categorical = len(self.categorical_features)\n        n_non_categorical = len(x_train.columns) - n_categorical\n\n        cat_dims = []\n        for col in self.categorical_features:\n            dim = x_train[col].nunique()\n            cat_dims.append((dim, (dim // 2 + 1)))\n        if mode == \"regression\":\n            model = DSBRegressor(\n                cat_dims=cat_dims,\n                n_non_categorical=n_non_categorical,\n                **model_params).to(device)\n            loss_fn = RMSELoss().to(device)\n            self.denominator = y_train.max()\n            y_train_ = y_train / self.denominator\n            y_valid_ = y_valid / self.denominator\n        elif mode == \"multiclass\":\n            model = DSBClassifier(\n                cat_dims=cat_dims,\n                n_non_categorical=n_non_categorical,\n                **model_params).to(device)\n            loss_fn = nn.CrossEntropyLoss().to(device)\n            y_train_ = y_train\n            y_valid_ = y_valid\n        elif mode == \"binary\":\n            model = DSBBinary(\n                cat_dims=cat_dims,\n                n_non_categorical=n_non_categorical,\n                **model_params).to(device)\n            loss_fn = nn.BCELoss().to(device)\n            y_train_ = (y_train > 0).astype(float)\n            y_valid_ = (y_valid > 0).astype(float)\n        elif mode == \"ovr\":\n            model = DSBOvR(\n                cat_dims=cat_dims,\n                n_non_categorical=n_non_categorical,\n                **model_params).to(device)\n            loss_fn = nn.BCELoss().to(device)\n            y_train_ = np.asarray([\n                (y_train == 0).astype(float),\n                (y_train == 1).astype(float),\n                (y_train == 2).astype(float),\n                (y_train == 3).astype(float)\n            ]).T.astype(np.float32)\n            y_valid_ = np.asarray([\n                (y_valid == 0).astype(float),\n                (y_valid == 1).astype(float),\n                (y_valid == 2).astype(float),\n                (y_valid == 3).astype(float)\n            ]).T.astype(np.float32)\n        else:\n            raise NotImplementedError\n\n        train_dataset = DSBDataset(\n            df=x_train,\n            categorical_features=self.categorical_features,\n            y=y_train_)\n        train_loader = torchdata.DataLoader(\n            train_dataset,\n            batch_size=train_params[\"batch_size\"],\n            shuffle=True,\n            num_workers=4)\n        valid_dataset = DSBDataset(\n            df=x_valid,\n            categorical_features=self.categorical_features,\n            y=y_valid_)\n        valid_loader = torchdata.DataLoader(\n            valid_dataset,\n            batch_size=512,\n            shuffle=False,\n            num_workers=4)\n\n        optimizer = optim.Adam(model.parameters(), lr=train_params[\"lr\"])\n        if train_params[\"scheduler\"].get(\"name\") is not None:\n            scheduler_params = train_params.get(\"scheduler\")\n            name = scheduler_params[\"name\"]\n            if name == \"cosine\":\n                scheduler = optim.lr_scheduler.CosineAnnealingLR(\n                    optimizer,\n                    T_max=scheduler_params[\"T_max\"],\n                    eta_min=scheduler_params[\"eta_min\"])\n            else:\n                raise NotImplementedError\n\n        best_score = -np.inf\n        best_loss = np.inf\n        for epoch in range(train_params[\"n_epochs\"]):\n            model.train()\n            avg_loss = 0.0\n            for non_cat, cat, target in progress_bar(train_loader):\n                y_pred = model(non_cat.float(), cat)\n                if self.mode == \"multiclass\":\n                    target = target.long()\n\n                loss = loss_fn(y_pred, target)\n                optimizer.zero_grad()\n\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item() / len(train_loader)\n\n            model.eval()\n            if self.mode == \"multiclass\" or self.mode == \"ovr\":\n                valid_preds = np.zeros((len(x_valid), 4))\n            else:\n                valid_preds = np.zeros(len(x_valid))\n            avg_val_loss = 0.0\n            for i, (non_cat, cat, target) in enumerate(progress_bar(\n                    valid_loader)):\n                with torch.no_grad():\n                    y_pred = model(non_cat.float(), cat).detach()\n                    if self.mode == \"multiclass\":\n                        target = target.long()\n\n                    loss = loss_fn(y_pred, target)\n                    avg_val_loss += loss.item() / len(valid_loader)\n                    valid_preds[\n                        i*512:(i+1)*512\n                    ] = y_pred.cpu().numpy()\n\n            if self.mode == \"regression\" or self.mode == \"binary\":\n                OptR = OptimizedRounder(n_overall=20, n_classwise=20)\n                OptR.fit(valid_preds, y_valid.astype(int))\n                valid_preds = OptR.predict(valid_preds)\n                score = calc_metric(y_valid.astype(int), valid_preds)\n            elif self.mode == \"multiclass\":\n                valid_preds = valid_preds @ np.arange(4) / 3\n                OptR = OptimizedRounder(n_overall=20, n_classwise=20)\n                OptR.fit(valid_preds, y_valid.astype(int))\n                valid_preds = OptR.predict(valid_preds)\n                score = calc_metric(y_valid.astype(int), valid_preds)\n            elif self.mode == \"ovr\":\n                valid_preds = valid_preds / np.repeat(\n                    valid_preds.sum(axis=1), 4).reshape(-1, 4)\n                valid_preds = valid_preds @ np.arange(4) / 3\n                OptR = OptimizedRounder(n_overall=20, n_classwise=20)\n                OptR.fit(valid_preds, y_valid.astype(int))\n                valid_preds = OptR.predict(valid_preds)\n                score = calc_metric(y_valid.astype(int), valid_preds)\n\n            print(\n                \"epoch: {} loss: {:.4f} val_loss: {:.4f} qwk: {:.4f}\".format(\n                    epoch, avg_loss, avg_val_loss, score\n                ))\n            if score > best_score and avg_val_loss < best_loss:\n                torch.save(\n                    model.state_dict(),\n                    save_path / f\"best_weight_fold_{self.fold}.pth\")\n            if score > best_score:\n                torch.save(\n                    model.state_dict(),\n                    save_path / f\"best_score_fold_{self.fold}.pth\")\n                print(\"Achieved best score\")\n                best_score = score\n            if avg_val_loss < best_loss:\n                torch.save(\n                    model.state_dict(),\n                    save_path / f\"best_loss_fold_{self.fold}.pth\")\n                print(\"Achieved best loss\")\n                best_loss = avg_val_loss\n\n            if train_params[\"scheduler\"].get(\"name\") is not None:\n                scheduler.step()\n\n        if config[\"model\"][\"policy\"] == \"best_loss\":\n            weight = save_path / f\"best_loss_fold_{self.fold}.pth\"\n        else:\n            weight = save_path / f\"best_score_fold_{self.fold}.pth\"\n        model.load_state_dict(torch.load(weight))\n\n        model.eval()\n        if self.mode == \"multiclass\" or self.mode == \"ovr\":\n            valid_preds = np.zeros((len(x_valid), 4))\n        else:\n            valid_preds = np.zeros(len(x_valid))\n        avg_val_loss = 0.0\n        for i, (non_cat, cat, target) in enumerate(\n                progress_bar(valid_loader)):\n            with torch.no_grad():\n                y_pred = model(non_cat.float(), cat).detach()\n                if self.mode == \"multiclass\":\n                    target = target.long()\n                loss = loss_fn(y_pred, target)\n                avg_val_loss += loss.item() / len(valid_loader)\n                valid_preds[\n                    i*512:(i+1)*512\n                ] = y_pred.cpu().numpy()\n        if self.mode == \"regression\" or self.mode == \"binary\":\n            OptR = OptimizedRounder(n_overall=20, n_classwise=20)\n            OptR.fit(valid_preds, y_valid.astype(int))\n            valid_preds = OptR.predict(valid_preds)\n            score = calc_metric(y_valid.astype(int), valid_preds)\n        elif self.mode == \"multiclass\":\n            valid_preds = valid_preds @ np.arange(4) / 3\n            OptR = OptimizedRounder(n_overall=20, n_classwise=20)\n            OptR.fit(valid_preds, y_valid.astype(int))\n            valid_preds = OptR.predict(valid_preds)\n            score = calc_metric(y_valid.astype(int), valid_preds)\n        elif self.mode == \"ovr\":\n            valid_preds = valid_preds / np.repeat(\n                valid_preds.sum(axis=1), 4).reshape(-1, 4)\n            valid_preds = valid_preds @ np.arange(4) / 3\n            OptR = OptimizedRounder(n_overall=20, n_classwise=20)\n            OptR.fit(valid_preds, y_valid.astype(int))\n            valid_preds = OptR.predict(valid_preds)\n            score = calc_metric(y_valid.astype(int), valid_preds)\n        best_score_dict = {\n            \"loss\": avg_val_loss,\n            \"qwk\": score\n        }\n        return model, best_score_dict\n\n    def predict(self, model: nn.Module, features: pd.DataFrame) -> np.ndarray:\n        batch_size = 512\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        dataset = DSBDataset(\n            df=features,\n            categorical_features=self.categorical_features)\n        loader = torchdata.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=4,\n            drop_last=False)\n        model.eval()\n        model = model.to(device)\n        if self.mode == \"regression\" or self.mode == \"binary\":\n            predictions = np.zeros(len(features))\n            for i, (non_cat, cat) in enumerate(loader):\n                with torch.no_grad():\n                    non_cat = non_cat.to(device)\n                    cat = cat.to(device)\n                    pred = model(non_cat.float(), cat).detach()\n                    predictions[\n                        i*batch_size: (i + 1)*batch_size\n                    ] = pred.cpu().numpy()\n            return predictions\n        elif self.mode == \"multiclass\":\n            predictions = np.zeros((len(features), 4))\n            for i, (non_cat, cat) in enumerate(loader):\n                with torch.no_grad():\n                    non_cat = non_cat.to(device)\n                    cat = cat.to(device)\n                    pred = model(non_cat.float(), cat).detach()\n                    predictions[\n                        i * batch_size:(i+1) * batch_size, :\n                    ] = pred.cpu().numpy()\n            return predictions @ np.arange(4) / 3\n        elif self.mode == \"ovr\":\n            predictions = np.zeros((len(features), 4))\n            for i, (non_cat, cat) in enumerate(loader):\n                with torch.no_grad():\n                    non_cat = non_cat.to(device)\n                    cat = cat.to(device)\n                    pred = model(non_cat.float(), cat).detach()\n                    predictions[\n                        i * batch_size:(i+1) * batch_size, :\n                    ] = pred.cpu().numpy()\n            predictions = predictions / np.repeat(\n                predictions.sum(axis=1), 4).reshape(-1, 4)\n            return predictions @ np.arange(4) / 3\n        else:\n            raise NotImplementedError\n\n    def preprocess(\n            self, train_features: pd.DataFrame,\n            test_features: pd.DataFrame,\n            categorical_features: List[str]\n            ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        fill_rule = {\n            \"mean_var\": 0.0,\n            \"var\": np.max,\n            \"time_to\": np.max,\n            \"timte_to\": np.max,  # typo\n            \"action_time\": np.mean,\n            \"Counter\": 0.0,\n            \"nunique\": 0.0,\n            \"accuracy\": 0.0,\n            (\"num\", \"min\"): 0.0,\n            (\"num\", \"median\"): 0.0,\n            (\"num\", \"max\"): 0.0,\n            (\"num\", \"sum\"): 0.0,\n            (\"num\", \"last\"): 0.0,\n            \"Ratio\": 0.0,\n            \"ratio\": 0.0,\n            \"mean\": 0.0,\n            \"raito\": 0.0,  # typo\n            \"No\": 0.0,\n            \"n_\": 0.0\n        }\n\n        n_train = len(train_features)\n        n_test = len(test_features)\n        concatenated_features = pd.concat([train_features, test_features],\n                                          ignore_index=True)\n        concatenated_features.reset_index(drop=True, inplace=True)\n\n        # fillna\n        for col in concatenated_features.columns:\n            if concatenated_features[col].min() == -1.0:\n                concatenated_features[col] = concatenated_features[\n                    col].replace({-1.0: np.nan})\n\n            if concatenated_features[col].hasnans:\n                for key in fill_rule.keys():\n                    if isinstance(key, str):\n                        if key in col:\n                            if isinstance(fill_rule[key], float):\n                                concatenated_features[col].fillna(\n                                    fill_rule[key], inplace=True)\n                            else:\n                                concatenated_features[col].fillna(\n                                    fill_rule[key](concatenated_features[col]),\n                                    inplace=True)\n                    elif isinstance(key, tuple):\n                        if isinstance(fill_rule[key], float):\n                            concatenated_features[col].fillna(\n                                fill_rule[key], inplace=True)\n                        else:\n                            concatenated_features[col].fillna(\n                                fill_rule[key](concatenated_features[col]),\n                                inplace=True)\n        # log transformation\n        for col in concatenated_features.columns:\n            skewness = concatenated_features[col].skew()\n            if -1 > skewness or 1 < skewness:\n                concatenated_features[col] = np.log1p(\n                    concatenated_features[col])\n\n        scale_cols = [\n            col for col in concatenated_features.columns\n            if col not in categorical_features\n        ]\n        ss = StandardScaler()\n        concatenated_features[scale_cols] = ss.fit_transform(\n            concatenated_features[scale_cols])\n        for col in categorical_features:\n            count = 0\n            ordering_dict = {}\n            uniq = concatenated_features[col].unique()\n            for v in uniq:\n                ordering_dict[v] = count\n                count += 1\n            concatenated_features[col] = concatenated_features[col].map(\n                lambda x: ordering_dict[x])\n\n        train_features = concatenated_features.iloc[:n_train, :].reset_index(\n            drop=True)\n        test_features = concatenated_features.iloc[n_train:, :].reset_index(\n            drop=True)\n\n        assert len(train_features) == n_train\n        assert len(test_features) == n_test\n\n        return train_features, test_features\n\n    def post_process(self, oof_preds: np.ndarray, test_preds: np.ndarray,\n                     y: np.ndarray,\n                     config: dict) -> Tuple[np.ndarray, np.ndarray]:\n        if self.mode in [\"multiclass\", \"regression\", \"binary\", \"ovr\"]:\n            params = config[\"post_process\"][\"params\"]\n            OptR = OptimizedRounder(**params)\n            OptR.fit(oof_preds, y)\n            oof_preds_ = OptR.predict(oof_preds)\n            test_preds_ = OptR.predict(test_preds)\n            return oof_preds_, test_preds_\n        return oof_preds, test_preds\n\n    def cv(self, y_train: AoS, train_features: pd.DataFrame,\n           test_features: pd.DataFrame, groups: np.ndarray,\n           feature_name: List[str],\n           categorical_features: List[str],\n           folds_ids: List[Tuple[np.ndarray, np.ndarray]], threshold: float,\n           config: dict) -> Tuple[List[nn.Module], np.ndarray, np.ndarray, np.\n                                  ndarray, dict]:\n        test_preds = np.zeros(len(test_features))\n        normal_oof_preds = np.zeros(len(train_features))\n        oof_preds_list: List[np.ndarray] = []\n        y_val_list: List[np.ndarray] = []\n        idx_val_list: List[np.ndarray] = []\n\n        cv_score_list: List[dict] = []\n        models: List[nn.Module] = []\n\n        self.categorical_features = categorical_features\n\n        train_features, test_features = self.preprocess(\n            train_features, test_features, categorical_features)\n        X = train_features\n        y = y_train.values if isinstance(y_train, pd.Series) \\\n            else y_train\n\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            self.fold = i_fold\n            print(\"=\" * 20)\n            print(f\"Fold: {self.fold}\")\n            print(\"=\" * 20)\n\n            x_trn = X.loc[trn_idx, :]\n            y_trn = y[trn_idx]\n\n            val_idx = np.sort(val_idx)\n            gp_val = groups[val_idx]\n\n            new_idx: List[int] = []\n            for gp in np.unique(gp_val):\n                gp_idx = val_idx[gp_val == gp]\n                new_idx.extend(gp_idx[:int(threshold)])\n\n            x_val = X.loc[new_idx, :]\n            y_val = y[new_idx]\n            x_val_normal = X.loc[val_idx, :]\n\n            model, best_score = self.fit(\n                x_trn, y_trn, x_val, y_val, config=config)\n            cv_score_list.append(best_score)\n            models.append(model)\n\n            oof_preds_list.append(self.predict(model, x_val).reshape(-1))\n            y_val_list.append(y_val)\n            idx_val_list.append(new_idx)\n            normal_oof_preds[val_idx] = self.predict(\n                model, x_val_normal).reshape(-1)\n            test_preds += self.predict(\n                model, test_features).reshape(-1) / len(folds_ids)\n\n        oof_preds = np.concatenate(oof_preds_list)\n        y_oof = np.concatenate(y_val_list)\n        idx_val = np.concatenate(idx_val_list)\n\n        sorted_idx = np.argsort(idx_val)\n        oof_preds = oof_preds[sorted_idx]\n        y_oof = y_oof[sorted_idx]\n\n        self.raw_oof_preds = oof_preds\n        self.raw_test_preds = test_preds\n\n        oof_preds, test_preds = self.post_process(\n            oof_preds, test_preds, y_oof, config)\n\n        oof_score = calc_metric(y_oof, oof_preds)\n        print(f\"oof score: {oof_score:.5f}\")\n\n        self.raw_normal_oof = normal_oof_preds\n\n        OptR = OptimizedRounder()\n        OptR.fit(normal_oof_preds, y_train)\n        normal_oof_preds = OptR.predict(normal_oof_preds)\n        normal_oof_score = calc_metric(y_train, normal_oof_preds)\n        print(f\"normal oof score: {normal_oof_score:.5f}\")\n\n        # truncated score\n        truncated_result = eval_with_truncated_data(\n            normal_oof_preds, y_train, groups, n_trials=100)\n        print(f\"truncated mean: {truncated_result['mean']:.5f}\")\n        print(f\"truncated std: {truncated_result['std']:.5f}\")\n\n        evals_results = {\n            \"evals_result\": {\n                \"oof_score\":\n                oof_score,\n                \"normal_oof_score\":\n                normal_oof_score,\n                \"truncated_eval_mean\":\n                truncated_result[\"mean\"],\n                \"truncated_eval_0.95upper\":\n                truncated_result[\"0.95upper_bound\"],\n                \"truncated_eval_0.95lower\":\n                truncated_result[\"0.95lower_bound\"],\n                \"truncated_eval_std\":\n                truncated_result[\"std\"],\n                \"cv_score\": {\n                    f\"cv{i + 1}\": cv_score\n                    for i, cv_score in enumerate(cv_score_list)\n                },\n                \"n_data\":\n                len(train_features),\n                \"n_features\":\n                len(train_features.columns),\n                # \"feature_importance\":\n                # feature_importance.sort_values(ascending=False).to_dict()\n            }\n        }\n\n        return (models, oof_preds, y_oof, test_preds,  # feature_importance,\n                evals_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### factory"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgbm() -> LightGBM:\n    return LightGBM()\n\n\ndef lgbm2() -> LightGBM2:\n    return LightGBM2()\n\n\ndef catboost() -> CatBoostModel:\n    return CatBoostModel()\n\n\ndef catboost2() -> CatBoostModel2:\n    return CatBoostModel2()\n\n\ndef classwise_cat() -> ClassWiseCatBoost:\n    return ClassWiseCatBoost()\n\n\ndef classwise_lgbm() -> ClassWiseLightGBM:\n    return ClassWiseLightGBM()\n\n\ndef mlp() -> NNTrainer:\n    return NNTrainer()\n\n\ndef get_model(config: dict):\n    model_name = config[\"model\"][\"name\"]\n    func = globals().get(model_name)\n    if func is None:\n        raise NotImplementedError\n    return func()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### features"},{"metadata":{},"cell_type":"markdown","source":"#### modules"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TargetEncoder():\n    def __init__(self, n_splits: int = 5, random_state: int = 128):\n        self.class_dict: Dict[str, List[float]] = {}\n        self.column = \"\"\n        self.n_splits = n_splits\n        self.random_state = random_state\n\n    def transform(self, X_: pd.DataFrame) -> np.ndarray:\n        kf = KFold(\n            n_splits=self.n_splits,\n            random_state=self.random_state,\n            shuffle=True)\n        X = X_.copy()\n        X = X.reset_index(drop=True)\n        converted = np.zeros(len(X))\n        for i, (_, v_idx) in enumerate(kf.split(X)):\n            converted[v_idx] = X.loc[v_idx, self.column].map(\n                lambda x: self.class_dict[x][i])\n        return converted\n\n    def fit_transform(self, X_: pd.DataFrame, y: Union[pd.Series, np.ndarray],\n                      column: str) -> np.ndarray:\n        self.column = column\n        uniq_class = X_[column].unique()\n        for c in uniq_class:\n            self.class_dict[c] = []\n        kf = StratifiedKFold(\n            n_splits=self.n_splits,\n            shuffle=True,\n            random_state=self.random_state)\n        X = X_.copy()\n        X = X.reset_index(drop=True)\n        yy = y.values if isinstance(y, pd.Series) else y\n        converted = np.zeros(len(X))\n        # import pdb\n        # pdb.set_trace()\n        for t_idx, v_idx in kf.split(X, y):\n            X_t = X.loc[t_idx, column]\n            y_t = yy[t_idx]\n            X_v = X.loc[v_idx, column]\n            cvtd = converted[v_idx]\n\n            for c in uniq_class:\n                target_mean = y_t[X_t == c].mean()\n                self.class_dict[c].append(target_mean)\n                cvtd[X_v == c] = target_mean\n            converted[v_idx] = cvtd\n        return converted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### base"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Feature(metaclass=abc.ABCMeta):\n    prefix = \"\"\n    suffix = \"\"\n    save_dir = \"features\"\n    is_feature = True\n\n    def __init__(self):\n        self.name = self.__class__.__name__\n        Path(self.save_dir).mkdir(exist_ok=True, parents=True)\n        self.train = pd.DataFrame()\n        self.valid = pd.DataFrame()\n        self.test = pd.DataFrame()\n        self.train_path = Path(self.save_dir) / f\"{self.name}_train.ftr\"\n        self.valid_path = Path(self.save_dir) / f\"{self.name}_valid.ftr\"\n        self.test_path = Path(self.save_dir) / f\"{self.name}_test.ftr\"\n\n    def run(self,\n            train_df: pd.DataFrame,\n            test_df: Optional[pd.DataFrame] = None,\n            log: bool = False):\n        with timer(self.name, log=log):\n            self.create_features(train_df, test_df)\n            prefix = self.prefix + \"_\" if self.prefix else \"\"\n            suffix = self.suffix + \"_\" if self.suffix else \"\"\n            self.train.columns = pd.Index([str(c) for c in self.train.columns])\n            self.valid.columns = pd.Index([str(c) for c in self.valid.columns])\n            self.test.columns = pd.Index([str(c) for c in self.test.columns])\n            self.train.columns = prefix + self.train.columns + suffix\n            self.valid.columns = prefix + self.valid.columns + suffix\n            self.test.columns = prefix + self.test.columns + suffix\n        return self\n\n    @abc.abstractmethod\n    def create_features(self, train_df: pd.DataFrame,\n                        test_df: Optional[pd.DataFrame]):\n        raise NotImplementedError\n\n    def save(self):\n        self.train.to_feather(str(self.train_path))\n        self.valid.to_feather(str(self.valid_path))\n        self.test.to_feather(str(self.test_path))\n\n\ndef is_feature(klass) -> bool:\n    return \"is_feature\" in set(dir(klass))\n\n\ndef get_features(namespace: dict):\n    features = []\n    for v in namespace.values():\n        if inspect.isclass(v) and is_feature(v) and not inspect.isabstract(v):\n            features.append(v())\n    return features\n\n\ndef generate_features(train_df: pd.DataFrame,\n                      test_df: pd.DataFrame,\n                      namespace: dict,\n                      required: list,\n                      overwrite: bool,\n                      log: bool = False):\n    for f in get_features(namespace):\n        if (f.name not in required) or (f.train_path.exists()\n                                        and f.valid_path.exists()\n                                        and f.test_path.exists()\n                                        and not overwrite):\n            if not log:\n                print(f.name, \"was skipped\")\n            else:\n                logging.info(f\"{f.name} was skipped\")\n        else:\n            f.run(train_df, test_df, log).save()\n\n\ndef load_features(\n        config: dict) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    feather_path = config[\"dataset\"][\"feature_dir\"]\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_train.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_train.ftr\").exists()\n    ]\n    x_train = pd.concat(dfs, axis=1)\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_valid.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_valid.ftr\").exists()\n    ]\n    x_valid = pd.concat(dfs, axis=1)\n\n    dfs = [\n        pd.read_feather(f\"{feather_path}/{f}_test.ftr\", nthreads=-1)\n        for f in config[\"features\"]\n        if Path(f\"{feather_path}/{f}_test.ftr\").exists()\n    ]\n    x_test = pd.concat(dfs, axis=1)\n    return x_train, x_valid, x_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PastSummary3"},{"metadata":{"trusted":true},"cell_type":"code","source":"IoF = Union[int, float]\nIoS = Union[int, str]\n\n\nclass PastSummary3(Feature):\n    def create_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n        train_df[\"title_event_code\"] = list(\n            map(lambda x, y: str(x) + \"_\" + str(y), train[\"title\"],\n                train[\"event_code\"]))\n        test_df[\"title_event_code\"] = list(\n            map(lambda x, y: str(x) + \"_\" + str(y), test[\"title\"],\n                test[\"event_code\"]))\n\n        event_codes = [\n            2000, 4070, 3120, 3121, 4020, 4035, 4030, 3110, 4022, 2010, 4090\n        ]\n        assessments = [\n            \"Mushroom Sorter (Assessment)\", \"Bird Measurer (Assessment)\",\n            \"Cauldron Filler (Assessment)\", \"Cart Balancer (Assessment)\",\n            \"Chest Sorter (Assessment)\"\n        ]\n        title_event_codes = [\n            \"Crystal Caves - Level 1_2000\", \"Crystal Caves - Level 2_2000\",\n            \"Crystal Caves - Level 3_2000\",\n            \"Cauldron Filler (Assessment)_3020\",\n            \"Sandcastle Builder (Activity)_4070\",\n            \"Sandcastle Builder (Activity)_4020\",\n            \"Bug Measurer (Activity)_4035\", \"Chow Time_4070\",\n            \"Bug Measurer (Activity)_4070\", \"All Star Sorting_2025\",\n            \"Leaf Leader_4070\"\n        ]\n\n        for assessment in assessments:\n            for code in [\"4020\", \"4070\"]:\n                title_event_codes.append(assessment + \"_\" + code)\n\n        event_ids = [\"27253bdc\"]\n\n        dfs_train: List[pd.DataFrame] = []\n        dfs_valid: List[pd.DataFrame] = []\n        dfs_test: List[pd.DataFrame] = []\n\n        inst_ids_train: List[str] = []\n        inst_ids_valid: List[str] = []\n        inst_ids_test: List[str] = []\n\n        train_df[\"timestamp\"] = pd.to_datetime(train_df[\"timestamp\"])\n        test_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"])\n\n        train_df = add_date_features(train_df)\n        test_df = add_date_features(test_df)\n\n        for inst_id, user_sample in tqdm_notebook(\n                train_df.groupby(\"installation_id\", sort=False),\n                total=train_df[\"installation_id\"].nunique(),\n                desc=\"train features\"):\n            if \"Assessment\" not in user_sample[\"type\"].unique():\n                continue\n            feats, _ = past_summary_features(\n                user_sample,\n                event_codes,\n                event_ids,\n                title_event_codes,\n                test=False)\n            inst_ids_train.extend([inst_id] * len(feats))\n            dfs_train.append(feats)\n\n        le, le_world = LabelEncoder(), LabelEncoder()\n        self.train = pd.concat(dfs_train, axis=0, sort=False)\n        self.train[\"session_title\"] = le.fit_transform(\n            self.train[\"session_title\"])\n        self.train[\"world\"] = le_world.fit_transform(self.train[\"world\"])\n        self.train[\"installation_id\"] = inst_ids_train\n        self.train.reset_index(drop=True, inplace=True)\n\n        for inst_id, user_sample in tqdm_notebook(\n                test_df.groupby(\"installation_id\", sort=False),\n                total=test_df[\"installation_id\"].nunique(),\n                desc=\"test features\"):\n            feats, valid_feats = past_summary_features(\n                user_sample,\n                event_codes,\n                event_ids,\n                title_event_codes,\n                test=True)\n\n            inst_ids_valid.extend([inst_id] * len(valid_feats))  # type: ignore\n            inst_ids_test.extend([inst_id] * len(feats))\n            dfs_valid.append(valid_feats)\n            dfs_test.append(feats)\n\n        self.valid = pd.concat(dfs_valid, axis=0, sort=False)\n        self.valid[\"session_title\"] = le.transform(self.valid[\"session_title\"])\n        self.valid[\"world\"] = le_world.transform(self.valid[\"world\"])\n        self.valid[\"installation_id\"] = inst_ids_valid\n        self.valid.reset_index(drop=True, inplace=True)\n\n        self.test = pd.concat(dfs_test, axis=0, sort=False)\n        self.test[\"session_title\"] = le.transform(self.test[\"session_title\"])\n        self.test[\"world\"] = le_world.transform(self.test[\"world\"])\n        self.test[\"installation_id\"] = inst_ids_test\n        self.test.reset_index(drop=True, inplace=True)\n\n        # pseudo target\n        te = TargetEncoder(n_splits=10, random_state=4222)\n        self.train[\"mean_target\"] = te.fit_transform(\n            self.train, self.train[\"accuracy_group\"], column=\"session_title\")\n        self.valid[\"mean_target\"] = te.transform(self.valid)\n        self.test[\"mean_target\"] = te.transform(self.test)\n\n\ndef past_summary_features(\n        user_sample: pd.DataFrame,\n        event_codes: list,\n        event_ids: list,\n        title_event_code: list,\n        test: bool = False) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    event_code_count = {ev: 0 for ev in event_codes}\n    event_id_count = {ev: 0 for ev in event_ids}\n    title_event_code_count = {ev: 0 for ev in title_event_code}\n\n    activities_count: Dict[IoS, IoF] = {}\n\n    all_assessments: List[Dict[IoS, IoF]] = []\n\n    accumulated_acc_groups = 0.0\n    accumulated_acc = 0.0\n    accumulated_correct_attempts = 0\n    accumulated_failed_attempts = 0\n    accumulated_actions = 0\n\n    counter = 0\n\n    accuracy_group: Dict[IoS, IoF] = {0: 0, 1: 0, 2: 0, 3: 0}\n\n    durations: List[float] = []\n    last_activity = \"\"\n\n    assessments = [\n        \"Mushroom Sorter (Assessment)\", \"Bird Measurer (Assessment)\",\n        \"Cauldron Filler (Assessment)\", \"Cart Balancer (Assessment)\",\n        \"Chest Sorter (Assessment)\"\n    ]\n\n    action_code = {\n        \"Mushroom Sorter (Assessment)\": [4020],\n        \"Bird Measurer (Assessment)\": [4025],\n        \"Cauldron Filler (Assessment)\": [4020],\n        \"Cart Balancer (Assessment)\": [4020],\n        \"Chest Sorter (Assessment)\": [4020, 4025]\n    }\n\n    games = [\n        \"Air Show\", \"All Star Sorting\", \"Bubble Bath\", \"Chow Time\",\n        \"Crystals Rule\", \"Dino Drink\", \"Dino Dive\", \"Happy Camel\",\n        \"Leaf Leader\", \"Pan Balance\", \"Scrub-A-Dub\"\n    ]\n\n    game_count_unit = {\n        \"All Star Sorting\": \"round\",\n        \"Scrub-A-Dub\": \"level\",\n        \"Air Show\": \"round\",\n        \"Crystals Rule\": \"round\",\n        \"Dino Drink\": \"round\",\n        \"Bubble Bath\": \"round\",\n        \"Dino Dive\": \"round\",\n        \"Chow Time\": \"round\",\n        \"Pan Balance\": \"round\",\n        \"Happy Camel\": \"round\",\n        \"Leaf Leader\": \"round\"\n    }\n\n    activities = [\n        \"Bottle Filler (Activity)\", \"Bug Measurer (Activity)\",\n        \"Chicken Balancer (Activity)\", \"Egg Dropper (Activity)\",\n        \"Fireworks (Activity)\", \"Flower Waterer (Activity)\",\n        \"Sandcastle Builder (Activity)\", \"Watering Hole (Activity)\"\n    ]\n\n    past_assess_summary: Dict[str, List[Dict[str, Any]]] = {\n        title: []\n        for title in assessments\n    }\n    last_assessment: Tuple[str, Dict[str, Any]] = (\"\", {})\n\n    past_game_summarys: Dict[str, List[Dict[str, IoF]]] = {\n        title: []\n        for title in games\n    }\n\n    past_activity_summarys: Dict[str, List[Dict[str, IoF]]] = {\n        title: []\n        for title in activities\n    }\n\n    for sess_id, sess in user_sample.groupby(\"game_session\", sort=False):\n        sess_type = sess[\"type\"].iloc[0]\n\n        if sess_type == \"Activity\":\n            act_title = sess[\"title\"].iloc[0]\n\n            summary = {}\n            if act_title == \"Sandcastle Builder (Activity)\":\n                event_data = get_event_data(sess)\n                summary[\"duration\"] = event_data[\"game_time\"].max()\n                if \"filled\" not in event_data.columns:\n                    summary[\"filled_eq_True\"] = 0\n                    summary[\"filled_eq_False\"] = 0\n                else:\n                    for v in [True, False]:\n                        summary[f\"filled_eq_{str(v)}\"] = \\\n                            (event_data[\"filled\"] == v).sum()\n            elif act_title == \"Fireworks (Activity)\":\n                event_data = get_event_data(sess)\n                if \"launched\" not in event_data.columns:\n                    summary[\"launched_eq_True\"] = 0\n                    summary[\"launched_eq_False\"] = 0\n                else:\n                    for v in [True, False]:\n                        summary[f\"launched_eq_{str(v)}\"] = \\\n                            (event_data[\"launched\"] == v).sum()\n            elif act_title == \"Bug Measurer (Activity)\":\n                event_data = get_event_data(sess)\n                summary[\"duration\"] = event_data[\"game_time\"].max()\n            else:\n                pass\n\n            past_activity_summarys[act_title].append(summary)\n\n        if sess_type == \"Game\":\n            game_title = sess[\"title\"].iloc[0]\n            summary = {}\n            event_data = get_event_data(sess)\n\n            n_round = event_data[game_count_unit[game_title]].max()\n\n            n_correct = len(event_data.query(\"event_code == 3021\"))\n            n_incorrect = len(event_data.query(\"event_code == 3020\"))\n            summary[\"n_correct\"] = n_correct\n            summary[\"n_incorrect\"] = n_incorrect\n\n            if n_round > 0:\n                summary[\"mean_correct\"] = n_correct / n_round\n                summary[\"mean_incorrect\"] = n_incorrect / n_round\n            else:\n                summary[\"mean_correct\"] = 0\n                summary[\"mean_incorrect\"] = 0\n\n            if (n_correct + n_incorrect) > 0:\n                summary[\"mean_success_ratio\"] = n_correct / (\n                    n_correct + n_incorrect)\n            else:\n                summary[\"mean_success_ratio\"] = 0.0\n            summary[\"count_4070\"] = (sess[\"event_code\"] == 4070).sum()\n\n            action_diff = sess[sess[\"event_code\"].isin(\n                [4020, 4025])][\"timestamp\"].diff().map(\n                    lambda x: x.seconds).fillna(0).tolist()\n            action_diff = list(filter(lambda x: x != 0.0, action_diff))\n            summary[\"mean_action_time\"] = np.mean(action_diff)\n            past_game_summarys[game_title].append(summary)\n\n        if sess_type == \"Assessment\" and (test or len(sess) > 1):\n            sess_title = sess[\"title\"].iloc[0]\n            world = sess[\"world\"].iloc[0]\n\n            attempt_code = 4110 if (\n                sess_title == \"Bird Measurer (Assessment)\") else 4100\n            all_attempts: pd.DataFrame = sess.query(\n                f\"event_code == {attempt_code}\")\n            correct_attempt = all_attempts[\"event_data\"].str.contains(\n                \"true\").sum()\n            failed_attempt = all_attempts[\"event_data\"].str.contains(\n                \"false\").sum()\n\n            # Basic Features\n            features = activities_count.copy()\n            features.update(event_code_count.copy())\n            features.update(event_id_count.copy())\n            features.update(title_event_code_count.copy())\n\n            features[\"session_title\"] = sess_title\n            features[\"world\"] = world\n            features[\"month\"] = sess[\"month\"].iloc[0]\n            features[\"year\"] = sess[\"year\"].iloc[0]\n            features[\"hour\"] = sess[\"hour\"].iloc[0]\n            features[\"dayofweek\"] = sess[\"dayofweek\"].iloc[0]\n            features[\"weekofyear\"] = sess[\"weekofyear\"].iloc[0]\n\n            features[\"accumulated_correct_attempts\"] = \\\n                accumulated_correct_attempts\n            features[\"accumulated_failed_attempts\"] = \\\n                accumulated_failed_attempts\n\n            accumulated_correct_attempts += correct_attempt\n            accumulated_failed_attempts += failed_attempt\n\n            features[\"duration_mean\"] = np.mean(durations) if durations else 0\n            durations.append((sess.iloc[-1, 2] - sess.iloc[0, 2]).seconds)\n\n            features[\"accumulated_acc\"] = \\\n                accumulated_acc / counter if counter > 0 else 0\n            acc = correct_attempt / (correct_attempt + failed_attempt) \\\n                if (correct_attempt + failed_attempt) != 0 else 0\n            accumulated_acc += acc\n\n            if acc == 0:\n                features[\"accuracy_group\"] = 0\n            elif acc == 1:\n                features[\"accuracy_group\"] = 3\n            elif acc == 0.5:\n                features[\"accuracy_group\"] = 2\n            else:\n                features[\"accuracy_group\"] = 1\n\n            features.update(accuracy_group.copy())\n            accuracy_group[features[\"accuracy_group\"]] += 1  # type: ignore\n\n            features[\"accumulated_accuracy_group\"] = \\\n                accumulated_acc_groups / counter if counter > 0 else 0\n            accumulated_acc_groups += features[\"accuracy_group\"]\n\n            features[\"accumulated_actions\"] = accumulated_actions\n\n            # PastAssessment\n            dur_from_last_assessment = np.iinfo(np.int).max\n            if last_assessment[0] != \"\":\n                delta = (\n                    sess.timestamp.min() - last_assessment[1][\"timestamp\"])\n                dur_from_last_assessment = delta.seconds\n            features[\"memory_decay_coeff_from_last_assess\"] = np.exp(\n                -dur_from_last_assessment / 86400)\n\n            dur_from_last_assessment = np.iinfo(np.int).max\n            if len(past_assess_summary[sess_title]) > 0:\n                last_same_assess = past_assess_summary[sess_title][-1]\n                delta = (sess.timestamp.min() - last_same_assess[\"timestamp\"])\n                dur_from_last_assessment = delta.seconds\n            memory_decay_coeff_from_last_same_assess = np.exp(\n                -dur_from_last_assessment / 86400)\n            features[\"memory_decay_coeff_from_last_same_assess\"] = \\\n                memory_decay_coeff_from_last_same_assess\n\n            # work on the same assessments\n            if len(past_assess_summary[sess_title]) > 0:\n                same_assessments = past_assess_summary[sess_title]\n                features[\"n_failure_same_assess\"] = sum(\n                    collect(same_assessments, \"failed_attempts\"))\n                features[\"success_ratio_same_assess\"] = np.mean(\n                    collect(same_assessments, \"success_ratio\"))\n                features[\"success_var_same_assess\"] = np.var(\n                    collect(same_assessments, \"success_ratio\"))\n                features[\"mean_accuracy_group_same_assess\"] = np.mean(\n                    collect(same_assessments, \"accuracy_group\"))\n                features[\"mean_timte_to_get_success_same_assess\"] = np.mean(\n                    collect(same_assessments, \"time_to_get_success\"))\n                features[\"var_time_to_get_success_same_assess\"] = np.var(\n                    collect(same_assessments, \"time_to_get_success\"))\n                features[\"mean_action_time_same_assess\"] = np.mean(\n                    collect(same_assessments, \"mean_action_time\"))\n                features[\"var_action_time_same_assess\"] = np.var(\n                    collect(same_assessments, \"mean_action_time\"))\n                features[\"mean_var_action_time_same_assess\"] = np.mean(\n                    collect(same_assessments, \"var_action_time\"))\n\n                # work on last same assess\n                features[\"n_failure_last_same_assess\"] = \\\n                    same_assessments[-1][\"failed_attempts\"]\n                features[\"success_ratio_last_same_assess\"] = \\\n                    same_assessments[-1][\"success_ratio\"]\n                features[\"accuracy_group_last_same_assess\"] = \\\n                    same_assessments[-1][\"accuracy_group\"]\n                features[\"time_to_get_success_last_same_assess\"] = \\\n                    same_assessments[-1][\"time_to_get_success\"]\n                features[\"mean_action_time_last_same_assess\"] = \\\n                    same_assessments[-1][\"mean_action_time\"]\n                features[\"var_action_time_last_same_assess\"] = \\\n                    same_assessments[-1][\"var_action_time\"]\n            else:\n                features[\"n_failure_same_assess\"] = -1.0\n                features[\"success_ratio_same_assess\"] = -1.0\n                features[\"success_var_same_assess\"] = -1.0\n                features[\"mean_accuracy_group_same_assess\"] = -1.0\n                features[\"mean_timte_to_get_success_same_assess\"] = -1.0\n                features[\"var_time_to_get_success_same_assess\"] = -1.0\n                features[\"mean_action_time_same_assess\"] = -1.0\n                features[\"var_action_time_same_assess\"] = -1.0\n                features[\"mean_var_action_time_same_assess\"] = -1.0\n                features[\"n_failure_last_same_assess\"] = -1.0\n                features[\"success_ratio_last_same_assess\"] = -1.0\n                features[\"accuracy_group_last_same_assess\"] = -1.0\n                features[\"time_to_get_success_last_same_assess\"] = -1.0\n                features[\"mean_action_time_last_same_assess\"] = -1.0\n                features[\"var_action_time_last_same_assess\"] = -1.0\n\n            for assess in assessments:\n                summs = past_assess_summary[assess]\n                if len(summs) > 0:\n                    features[assess + \"_success_ratio\"] = np.mean(\n                        collect(summs, \"success_ratio\"))\n                    features[assess + \"_accuracy_group\"] = np.mean(\n                        collect(summs, \"accuracy_group\"))\n                    features[assess + \"_time_to_get_success\"] = np.mean(\n                        collect(summs, \"time_to_get_success\"))\n                    features[assess + \"_mean_action_time\"] = np.mean(\n                        collect(summs, \"mean_action_time\"))\n                    features[assess + \"_var_mean_action_time\"] = np.var(\n                        collect(summs, \"mean_action_time\"))\n                    features[assess + \"_mean_var_action_time\"] = np.mean(\n                        collect(summs, \"var_action_time\"))\n                    features[assess + \"_4070_mean\"] = np.mean(\n                        collect(summs, \"4070\"))\n                    features[assess + \"_4070_var\"] = np.var(\n                        collect(summs, \"4070\"))\n                    if assess == \"Cauldron Filler (Assessment)\":\n                        features[assess + \"_3020_mean\"] = np.mean(\n                            collect(summs, \"3020\"))\n                        features[assess + \"_3020_var\"] = np.var(\n                            collect(summs, \"3020\"))\n                else:\n                    features[assess + \"_success_raito\"] = -1.0\n                    features[assess + \"_accuracy_group\"] = -1.0\n                    features[assess + \"_time_to_get_success\"] = -1.0\n                    features[assess + \"_mean_action_time\"] = -1.0\n                    features[assess + \"_var_mean_action_time\"] = -1.0\n                    features[assess + \"_mean_var_action_time\"] = -1.0\n                    features[assess + \"_4070_mean\"] = -1.0\n                    features[assess + \"_4070_var\"] = -1.0\n                    if assess == \"Cauldron Filler (Assessment)\":\n                        features[assess + \"_3020_mean\"] = -1.0\n                        features[assess + \"_3020_var\"] = -1.0\n\n                for col in [\n                        \"accuracy_group_last_same_assess\",\n                        \"n_failure_last_same_assess\",\n                        \"success_ratio_last_same_assess\"\n                ]:\n                    features[\"decayed_\" + col] = (\n                        features[col] *\n                        memory_decay_coeff_from_last_same_assess)\n\n            summary_: Dict[str, Any] = {}\n            summary_[\"timestamp\"] = sess[\"timestamp\"].iloc[-1]\n            summary_[\"n_attempts\"] = len(all_attempts)\n            if len(all_attempts) == 0:\n                success_attempts = -1\n                failed_attempts = -1\n                success_ratio = -1.0\n                accuracy_group_ = -1\n                time_to_get_success = -1\n            else:\n                success_attempts = all_attempts[\"event_data\"].str.contains(\n                    \"true\").sum()\n                failed_attempts = all_attempts[\"event_data\"].str.contains(\n                    \"false\").sum()\n                success_ratio = success_attempts / len(all_attempts)\n                if success_ratio == 0:\n                    accuracy_group_ = 0\n                elif success_ratio == 1:\n                    accuracy_group_ = 3\n                elif success_ratio == 0.5:\n                    accuracy_group_ = 2\n                else:\n                    accuracy_group_ = 1\n\n                if success_attempts > 0:\n                    successed_att = all_attempts[all_attempts[\"event_data\"].\n                                                 str.contains(\"true\")]\n                    duration = (successed_att[\"timestamp\"].iloc[0] -\n                                sess[\"timestamp\"].iloc[0]).seconds\n                    time_to_get_success = duration\n                else:\n                    time_to_get_success = -1\n\n            summary_[\"success_attempts\"] = success_attempts\n            summary_[\"failed_attempts\"] = failed_attempts\n            summary_[\"success_ratio\"] = success_ratio\n            summary_[\"accuracy_group\"] = accuracy_group_\n            summary_[\"time_to_get_success\"] = time_to_get_success\n            summary_[\"4070\"] = (sess[\"event_code\"] == 4070).sum()\n            summary_[\"3020\"] = (sess[\"event_code\"] == 3020).sum()\n\n            action_diff = sess[sess[\"event_code\"].isin(\n                action_code[sess_title])][\"timestamp\"].diff().map(\n                    lambda x: x.seconds).fillna(0).tolist()\n            action_diff = list(filter(lambda x: x != 0.0, action_diff))\n            summary_[\"mean_action_time\"] = np.mean(action_diff)\n            summary_[\"var_action_time\"] = np.var(action_diff)\n            if len(all_attempts) > 0:\n                past_assess_summary[sess_title].append(summary_)\n                last_assessment = (sess_title, summary_)\n\n            # PastGame\n            for game in games:\n                features[\"n_last_correct_\" + game] = 0.0\n                features[\"mean_correct_\" + game] = 0.0\n                features[\"mean_incorrect_\" + game] = 0.0\n                features[\"var_correct\" + game] = 0.0\n                features[\"var_incorrect_\" + game] = 0.0\n                features[\"n_incorrect_\" + game] = 0.0\n                features[\"n_last_incorrect_\" + game] = 0.0\n                features[\"success_ratio_\" + game] = 0.0\n                features[\"var_success_ratio_\" + game] = 0.0\n                features[\"last_success_ratio_\" + game] = 0.0\n                features[\"count_4070_\" + game] = 0.0\n                features[\"mean_4070_\" + game] = 0.0\n                features[\"var_4070_\" + game] = 0.0\n                features[\"mean_action_time_\" + game] = 0.0\n                features[\"var_action_time_\" + game] = 0.0\n\n            for game, summ in past_game_summarys.items():\n                if len(summ) == 0:\n                    continue\n                features[\"n_incorrect_\" + game] = sum(\n                    collect(summ, \"n_incorrect\"))\n                features[\"n_last_correct_\" + game] = collect(\n                    summ, \"n_correct\")[-1]\n                features[\"n_last_incorrect_\" + game] = collect(\n                    summ, \"n_incorrect\")[-1]\n                features[\"mean_correct_\" + game] = np.mean(\n                    collect(summ, \"mean_correct\"))\n                features[\"mean_incorrect_\" + game] = np.mean(\n                    collect(summ, \"mean_incorrect\"))\n                features[\"var_correct_\" + game] = np.var(\n                    collect(summ, \"mean_correct\"))\n                features[\"var_incorrect_\" + game] = np.var(\n                    collect(summ, \"mean_incorrect\"))\n                features[\"success_ratio_\" + game] = np.mean(\n                    collect(summ, \"mean_success_ratio\"))\n                features[\"var_success_ratio_\" + game] = np.var(\n                    collect(summ, \"mean_success_ratio\"))\n                features[\"last_success_ratio_\" + game] = collect(\n                    summ, \"mean_success_ratio\")[-1]\n                features[\"count_4070_\" + game] = sum(\n                    collect(summ, \"count_4070\"))\n                features[\"mean_4070_\" + game] = np.mean(\n                    collect(summ, \"count_4070\"))\n                features[\"var_4070_\" + game] = np.var(\n                    collect(summ, \"count_4070\"))\n                features[\"mean_action_time_\" + game] = np.mean(\n                    collect(summ, \"mean_action_time\"))\n                features[\"var_action_time_\" + game] = np.var(\n                    collect(summ, \"mean_action_time\"))\n\n            # PastActivity\n            for key, summs in past_activity_summarys.items():\n                if key == \"Fireworks (Activity)\":\n                    if len(summs) == 0:\n                        features[\"n_launched_True\"] = 0\n                        features[\"n_launched_False\"] = 0\n                        features[\"launched_ratio\"] = 0.0\n                    else:\n                        features[\"n_launched_True\"] = sum(\n                            collect(summs, \"launched_eq_True\"))\n                        features[\"n_launched_False\"] = sum(\n                            collect(summs, \"launched_eq_False\"))\n                        total = features[\"n_launched_False\"] + \\\n                            features[\"n_launched_True\"]\n                        features[\"launched_ratio\"] = \\\n                            features[\"n_launched_True\"] / total \\\n                            if total > 0 else 0\n                elif key == \"Sandcastle Builder (Activity)\":\n                    if len(summs) == 0:\n                        features[key + \"_duration\"] = 0\n                        features[\"sand_filled_ratio\"] = 0.0\n                    else:\n                        features[key + \"_duration\"] = sum(\n                            collect(summs, \"duration\"))\n                        n_sand_filled_True = sum(\n                            collect(summs, \"filled_eq_True\"))\n                        n_sand_filled_False = sum(\n                            collect(summs, \"filled_eq_False\"))\n                        total = n_sand_filled_False + n_sand_filled_True\n                        features[\"sand_filled_ratio\"] = \\\n                            n_sand_filled_True / total \\\n                            if total > 0 else 0\n                elif key == \"Bug Measurer (Activity)\":\n                    if len(summs) == 0:\n                        features[key + \"_duration\"] = 0\n                    else:\n                        features[key + \"_duration\"] = sum(\n                            collect(summs, \"duration\"))\n\n            if len(sess) == 1:\n                all_assessments.append(features)\n            elif correct_attempt + failed_attempt > 0:\n                all_assessments.append(features)\n\n            counter += 1\n\n        def update_counters(counter: dict, col: str):\n            num_of_session_count = Counter(sess[col])\n            for k in num_of_session_count.keys():\n                x = k\n                if counter.get(k) is None:\n                    continue\n                counter[x] += num_of_session_count[k]\n            return counter\n\n        event_code_count = update_counters(event_code_count, \"event_code\")\n        event_id_count = update_counters(event_id_count, \"event_id\")\n        title_event_code_count = update_counters(title_event_code_count,\n                                                 \"title_event_code\")\n\n        accumulated_actions += len(sess)\n        if last_activity != sess_type:\n            last_activity = sess_type\n\n    if test:\n        df = pd.DataFrame([all_assessments[-1]])\n        valid_df = pd.DataFrame(all_assessments[:-1])\n        return df, valid_df\n    else:\n        df = pd.DataFrame(all_assessments)\n        return df, None\n\n\ndef add_date_features(df: pd.DataFrame):\n    df[\"date\"] = df[\"timestamp\"].dt.date\n    df[\"month\"] = df[\"timestamp\"].dt.month\n    df[\"year\"] = df[\"timestamp\"].dt.year\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n    df[\"weekofyear\"] = df[\"timestamp\"].dt.weekofyear\n    return df\n\n\ndef get_event_data(df: pd.DataFrame) -> pd.DataFrame:\n    return pd.io.json.json_normalize(df.event_data.apply(json.loads))\n\n\ndef collect(lod: List[Dict[str, IoF]], name: str) -> List[IoF]:\n    return [d[name] for d in lod]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### nakama_v8"},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = \"installation_id\"\nTARGET = \"accuracy_group\"\n\nclass NakamaV8(Feature):\n    def create_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n        input_dir = Path(\"../input/data-science-bowl-2019\")\n        if not input_dir.exists():\n            input_dir = Path(\"input/data-science-bowl-2019\")\n\n        train_labels = pd.read_csv(input_dir / \"train_labels.csv\")\n        sample_submission = pd.read_csv(input_dir / \"sample_submission.csv\")\n        if \"title_event_code\" not in train_df.columns:\n            train_df[\"title_event_code\"] = list(\n                map(lambda x, y: str(x) + \"_\" + str(y), train_df[\"title\"],\n                    train_df[\"event_code\"]))\n            test_df[\"title_event_code\"] = list(\n                map(lambda x, y: str(x) + \"_\" + str(y), test_df[\"title\"],\n                    test_df[\"event_code\"]))\n\n        with timer(\"creating test_labels\"):\n            test_df = assessment(test_df)\n            test_df, test_labels = create_test_labels(test_df,\n                                                      sample_submission)\n\n        with timer(\"extract time features\"):\n            if \"date\" not in train_df.columns:\n                train_df = extract_time_features(train_df)\n                test_df = extract_time_features(test_df)\n                print(\"skip extracting time features\")\n            else:\n                train_df[\"timestamp\"] = train_df[\"timestamp\"].astype(int)\n                test_df[\"timestamp\"] = test_df[\"timestamp\"].astype(int)\n\n        with timer(\"Extract user logs\"):\n            train = extract_user_logs(train_df, train_labels)\n            test = extract_user_logs(test_df, test_labels)\n\n        with timer(\"Ratio features\"):\n            counter_cols = [\n                c for c in train.columns if str(c).find(\"_Counter\") >= 0\n            ]\n            train[\"sum_counter\"] = train[counter_cols].sum(axis=1)\n            for c in counter_cols:\n                train[f\"Ratio_{c}\"] = train[c] / train[\"sum_counter\"]\n            counter_cols = [\n                c for c in test.columns if str(c).find(\"_Counter\") >= 0\n            ]\n            test[\"sum_counter\"] = test[counter_cols].sum(axis=1)\n            for c in counter_cols:\n                test[f\"Ratio_{c}\"] = test[c] / test[\"sum_counter\"]\n\n        with timer(\"Past solved features\"):\n            train = past_solved_features(train)\n            train = clean_title_m(train)\n            train.columns = [\n                \"\".join(c if c.isalnum() else \"_\" for c in str(x))\n                for x in train.columns\n            ]\n            train = train.sort_index(axis='columns')\n\n            test = past_solved_features(test)\n            test = clean_title_m(test)\n            test.columns = [\n                \"\".join(c if c.isalnum() else \"_\" for c in str(x))\n                for x in test.columns\n            ]\n            test = test.sort_index(axis='columns')\n\n        train, test = train.align(test, join=\"left\", axis=1)\n        valid = test[test[TARGET].notnull()].reset_index(drop=True)\n        test = test[test[TARGET].isnull()].reset_index(drop=True)\n\n        train[TARGET] = train[TARGET].astype(int)\n        valid[TARGET] = valid[TARGET].astype(int)\n\n        num_features = [c for c in test.columns if test.dtypes[c] != 'object']\n        cat_features = ['title', 'world']\n        features = num_features + cat_features\n        drop_features = [\n            ID, TARGET, 'accuracy', 'num_correct', 'num_incorrect',\n            'year', 'game_time', 'event_code', 'type',\n            'timestamp', 'event_count']\n        features = [c for c in features if c not in drop_features]\n\n        train = train[features]\n        valid = valid[features]\n        test = test[features]\n\n        ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='impute')\n        ce_oe.fit(train)\n\n        train = ce_oe.transform(train)\n        valid = ce_oe.transform(valid)\n        test = ce_oe.transform(test)\n\n        self.train = train\n        self.valid = valid\n        self.test = test\n\n\ndef extract_time_features(df: pd.DataFrame):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n\n    df['timestamp'] = df['timestamp'].astype(int)\n\n    return df\n\n\ndef assessment(df: pd.DataFrame):\n\n    df['num_correct'] = 0\n    df['num_incorrect'] = 0\n    df.loc[(((df.event_code == 4100) &\n            (df.title != 'Bird Measurer (Assessment)')) &\n            (df.type == 'Assessment')), 'num_correct'] = \\\n        df.loc[(((df.event_code == 4100) &\n                (df.title != 'Bird Measurer (Assessment)')) &\n                (df.type == 'Assessment'))]['event_data'].apply(\n                    lambda x: x.find('\"correct\":true') >= 0) * 1\n    df.loc[(((df.event_code == 4110) &\n            (df.title == 'Bird Measurer (Assessment)')) &\n            (df.type == 'Assessment')), 'num_correct'] = \\\n        df.loc[(((df.event_code == 4110) &\n                (df.title == 'Bird Measurer (Assessment)')) &\n                (df.type == 'Assessment'))]['event_data'].apply(\n                    lambda x: x.find('\"correct\":true') >= 0) * 1\n    df.loc[(((df.event_code == 4100) &\n            (df.title != 'Bird Measurer (Assessment)')) &\n            (df.type == 'Assessment')), 'num_incorrect'] = \\\n        df.loc[(((df.event_code == 4100) &\n                (df.title != 'Bird Measurer (Assessment)')) &\n                (df.type == 'Assessment'))]['event_data'].apply(\n                    lambda x: x.find('\"correct\":false') >= 0) * 1\n    df.loc[(((df.event_code == 4110) &\n            (df.title == 'Bird Measurer (Assessment)')) &\n            (df.type == 'Assessment')), 'num_incorrect'] = \\\n        df.loc[(((df.event_code == 4110) &\n                (df.title == 'Bird Measurer (Assessment)')) &\n                (df.type == 'Assessment'))]['event_data'].apply(\n                    lambda x: x.find('\"correct\":false') >= 0) * 1\n\n    return df\n\n\ndef create_test_labels(test: pd.DataFrame, sample_submission: pd.DataFrame):\n\n    # assessment\n    cols = [\n        'installation_id', 'game_session', 'title', 'num_correct',\n        'num_incorrect'\n    ]\n    test_labels = pd.concat(\n        [\n            test[((test.event_code == 4100) &\n                  (test.title != 'Bird Measurer (Assessment)'))\n                 & (test.type == 'Assessment')][cols].groupby(\n                     ['installation_id', 'game_session', 'title'],\n                     as_index=False).sum(),\n            test[((test.event_code == 4110) &\n                  (test.title == 'Bird Measurer (Assessment)'))\n                 & (test.type == 'Assessment')][cols].groupby(\n                     ['installation_id', 'game_session', 'title'],\n                     as_index=False).sum()\n        ])\n    test_labels['accuracy'] = test_labels['num_correct'] / (\n        test_labels['num_correct'] + test_labels['num_incorrect'])\n    test_labels['accuracy_group'] = np.nan\n    test_labels.loc[(test_labels['num_correct'] == 1) &\n                    (test_labels['num_incorrect'] == 0), 'accuracy_group'] = 3\n    test_labels.loc[(test_labels['num_correct'] == 1) &\n                    (test_labels['num_incorrect'] == 1), 'accuracy_group'] = 2\n    test_labels.loc[(test_labels['num_correct'] == 1) &\n                    (test_labels['num_incorrect'] >= 2), 'accuracy_group'] = 1\n    test_labels.loc[(test_labels['num_correct'] == 0), 'accuracy_group'] = 0\n    test_labels['accuracy_group'] = test_labels['accuracy_group'].astype(int)\n\n    # no assessment ( what we have to predict )\n    key_cols = [ID, 'timestamp', 'event_code', 'type']\n    last_assesment = test[test.event_code == 2000][\n        test.type == 'Assessment'][key_cols].groupby(\n            ID, as_index=False).max()\n    last_assesment_df = last_assesment.merge(\n        test[key_cols + ['game_session', 'title']], on=key_cols,\n        how='left')[['installation_id', 'game_session', 'title']]\n\n    # concat them\n    test_labels = pd.concat([test_labels,\n                             last_assesment_df]).reset_index(drop=True)\n\n    # drop ['num_correct', 'num_incorrect'] after assessment\n    test = test.drop(columns=['num_correct', 'num_incorrect']).reset_index(\n        drop=True)\n\n    return test, test_labels\n\n\ndef extract_user_logs(df: pd.DataFrame, df_labels: pd.DataFrame):\n\n    logs = pd.DataFrame()\n    nunique_cols = [\n        'event_id', 'game_session', 'timestamp', 'event_data', 'event_count',\n        'event_code', 'title', 'world', 'date', 'month', 'hour', 'dayofweek',\n        'weekofyear', 'title_event_code'\n    ]\n    sum_cols = ['title_event_code', 'title', 'event_code', 'world', 'type']\n    sum_values: Set[IoS] = set()\n    for c in sum_cols:\n        sum_values = sum_values | set(df[c].unique())\n\n    def extract_user_log(tmp: pd.DataFrame, tmp_df: pd.DataFrame, days=None):\n\n        sum_df = pd.DataFrame()\n        if days is None:\n            _sum_df = Counter({value: 0 for value in list(sum_values)})\n            for i in range(len(tmp_df)):\n                if i == 0:\n                    tmp_past = tmp[tmp.timestamp < tmp_df.loc[i, 'timestamp']]\n                else:\n                    tmp_past = tmp[\n                        tmp_df.loc[i - 1, 'timestamp'] <= tmp.timestamp][\n                            tmp.timestamp < tmp_df.loc[i, 'timestamp']]\n                if len(tmp_past) == 0:\n                    sum_df = pd.concat(\n                        [sum_df,\n                         pd.DataFrame({\n                             'No_playing_logs': [1]\n                         })],\n                        axis=0)\n                else:\n                    nunique_df = pd.DataFrame(\n                        tmp[tmp.timestamp < tmp_df.loc[i, 'timestamp']]\n                        [nunique_cols].nunique()).T.add_prefix('nunique_')\n                    for c in sum_cols:\n                        _sum_df.update(Counter(tmp_past[c].values))\n                    concat_df = pd.concat([\n                        nunique_df,\n                        pd.DataFrame.from_dict(\n                            _sum_df, orient='index').T.add_suffix('_Counter')\n                    ],\n                                          axis=1)\n                    sum_df = pd.concat([sum_df, concat_df], axis=0)\n        else:\n            past_days = days * 24 * 60**2 * 10**9\n            for i in range(len(tmp_df)):\n                if i == 0:\n                    tmp_past = tmp[\n                        (tmp_df.loc[i, 'timestamp'] - past_days) < tmp.\n                        timestamp][tmp.timestamp < tmp_df.loc[i, 'timestamp']]\n                if len(tmp_past) == 0:\n                    sum_df = pd.concat([\n                        sum_df,\n                        pd.DataFrame({\n                            f'{days}day_No_playing_logs': [1]\n                        })\n                    ],\n                                       axis=0)\n                else:\n                    nunique_df = pd.DataFrame(\n                        tmp_past[nunique_cols].nunique()).T.add_prefix(\n                            f'nunique_{days}day_')\n                    _sum_df = Counter({value: 0 for value in list(sum_values)})\n                    for c in sum_cols:\n                        _sum_df.update(Counter(tmp_past[c].values))\n                    concat_df = pd.concat([\n                        nunique_df,\n                        pd.DataFrame.from_dict(\n                            _sum_df, orient='index').T.add_suffix('_Counter')\n                    ],\n                                          axis=1).add_prefix(f'{days}day_')\n                    sum_df = pd.concat([sum_df, concat_df], axis=0)\n\n        return sum_df\n\n    for (_, tmp) in tqdm_notebook(\n            df.groupby('installation_id'),\n            total=df[\"installation_id\"].nunique()):\n\n        tmp = tmp.sort_values('timestamp').reset_index(drop=True)\n        tmp_df = tmp[tmp.event_code == 2000][\n            tmp.type == 'Assessment'].reset_index(drop=True)\n        sum_df = extract_user_log(tmp, tmp_df, days=None)\n\n        # concat\n        _log = pd.concat([tmp_df, sum_df.reset_index(drop=True)], axis=1)\n        logs = pd.concat([logs, _log], axis=0)\n\n    not_merge_columns = ['installation_id', 'title']\n    output = df_labels.merge(\n        logs.drop(columns=not_merge_columns), on='game_session', how='left')\n\n    return output.reset_index(drop=True)\n\n\ndef past_solved_features(df: pd.DataFrame):\n\n    output = pd.DataFrame()\n    target_cols = ['num_correct', 'num_incorrect', 'accuracy_group']\n    title_cols = [\n        'Cart Balancer (Assessment)', 'Cauldron Filler (Assessment)',\n        'Mushroom Sorter (Assessment)', 'Chest Sorter (Assessment)',\n        'Bird Measurer (Assessment)'\n    ]\n\n    def past_solved_feature(tmp, days=None):\n        for i in range(len(tmp)):\n            if i != 0:\n                if days is None:\n                    tmp_past = tmp[tmp.timestamp < tmp.loc[i, 'timestamp']]\n                    if len(tmp_past) != 0:\n                        for c in target_cols:\n                            tmp_past_values = tmp_past[c].values\n                            tmp.loc[i, c + '_sum'] = tmp_past_values.sum()\n                            tmp.loc[i, c + '_max'] = tmp_past_values.max()\n                            tmp.loc[i, c + '_min'] = tmp_past_values.min()\n                            tmp.loc[i, c + '_mean'] = tmp_past_values.mean()\n                            tmp.loc[i, c + '_median'] = tmp_past[c].median()\n                            tmp.loc[i, c + '_var'] = tmp_past_values.var()\n                            tmp.loc[i, c + '_last'] = tmp_past_values[-1]\n                        tmp.loc[i, 'total_accuracy'] = \\\n                            tmp.loc[i, 'num_correct_sum'] / (\n                                tmp.loc[i, 'num_correct_sum'] +\n                                tmp.loc[i, 'num_incorrect_sum'])\n                    for t in title_cols:\n                        _tmp_past = tmp_past[tmp_past.title == t]\n                        if len(_tmp_past) != 0:\n                            for c in target_cols:\n                                tmp_past_values = _tmp_past[c].values\n                                tmp.loc[i, c + '_sum_' +\n                                        t] = tmp_past_values.sum()\n                                tmp.loc[i, c + '_max_' +\n                                        t] = tmp_past_values.max()\n                                tmp.loc[i, c + '_min_' +\n                                        t] = tmp_past_values.min()\n                                tmp.loc[i, c + '_mean_' +\n                                        t] = tmp_past_values.mean()\n                                tmp.loc[i, c + '_median_' +\n                                        t] = _tmp_past[c].median()\n                                tmp.loc[i, c + '_var_' +\n                                        t] = tmp_past_values.var()\n                                tmp.loc[i, c + '_last_' +\n                                        t] = tmp_past_values[-1]\n                            tmp.loc[i, 'total_accuracy_' + t] = \\\n                                tmp.loc[i, 'num_correct_sum_' + t] / (\n                                    tmp.loc[i, 'num_correct_sum_' + t] +\n                                    tmp.loc[i, 'num_incorrect_sum_' + t])\n                else:\n                    past_days = days * 24 * 60**2 * 10**9\n                    tmp_past = tmp[(\n                        tmp.loc[i, 'timestamp'] - past_days) < tmp.timestamp][\n                            tmp.timestamp < tmp.loc[i, 'timestamp']]\n                    if len(tmp_past) != 0:\n                        for c in target_cols:\n                            tmp_past_values = tmp_past[c].values\n                            tmp.loc[i, c +\n                                    f'_sum_{days}day'] = tmp_past_values.sum()\n                            tmp.loc[i, c +\n                                    f'_max_{days}day'] = tmp_past_values.max()\n                            tmp.loc[i, c +\n                                    f'_min_{days}day'] = tmp_past_values.min()\n                            tmp.loc[i, c +\n                                    f'_mean_{days}day'] = tmp_past_values.mean(\n                                    )\n                            tmp.loc[i, c +\n                                    f'_median_{days}day'] = tmp_past[c].median(\n                                    )\n                            tmp.loc[i, c +\n                                    f'_var_{days}day'] = tmp_past_values.var()\n                            tmp.loc[i, c +\n                                    f'_last_{days}day'] = tmp_past_values[-1]\n                        tmp.loc[i, f'total_accuracy_{days}day'] = \\\n                            tmp.loc[i, f'num_correct_sum_{days}day'] / (\n                                tmp.loc[i, f'num_correct_sum_{days}day'] +\n                                tmp.loc[i, f'num_incorrect_sum_{days}day'])\n                    for t in title_cols:\n                        _tmp_past = tmp_past[tmp_past.title == t]\n                        if len(_tmp_past) != 0:\n                            for c in target_cols:\n                                tmp_past_values = _tmp_past[c].values\n                                tmp.loc[i, c + f'_sum_{days}day_' +\n                                        t] = tmp_past_values.sum()\n                                tmp.loc[i, c + f'_max_{days}day_' +\n                                        t] = tmp_past_values.max()\n                                tmp.loc[i, c + f'_min_{days}day_' +\n                                        t] = tmp_past_values.min()\n                                tmp.loc[i, c + f'_mean_{days}day_' +\n                                        t] = tmp_past_values.mean()\n                                tmp.loc[i, c + f'_median_{days}day_' +\n                                        t] = _tmp_past[c].median()\n                                tmp.loc[i, c + f'_var_{days}day_' +\n                                        t] = tmp_past_values.var()\n                                tmp.loc[i, c + f'_last_{days}day_' +\n                                        t] = tmp_past_values[-1]\n                            tmp.loc[i, f'total_accuracy_{days}day_' + t] = \\\n                                tmp.loc[\n                                    i, f'num_correct_sum_{days}day_' + t\n                                    ] / (\n                                    tmp.loc[\n                                        i, f'num_correct_sum_{days}day_' + t\n                                        ] +\n                                    tmp.loc[\n                                        i, f'num_incorrect_sum_{days}day_' + t\n                                        ])\n        return tmp\n\n    for (_, tmp) in tqdm_notebook(\n            df.groupby('installation_id'),\n            total=df[\"installation_id\"].nunique()):\n\n        tmp = tmp.sort_values('timestamp').reset_index(drop=True).reset_index()\n        tmp = tmp.rename(columns={'index': 'count'})\n        tmp = past_solved_feature(tmp, days=None)\n        tmp = past_solved_feature(tmp, days=7)\n\n        output = pd.concat([output, tmp])\n\n    return output.reset_index(drop=True)\n\n\ndef clean_title_m(df: pd.DataFrame):\n\n    title_cols = [\n        'Cart Balancer (Assessment)', 'Cauldron Filler (Assessment)',\n        'Mushroom Sorter (Assessment)', 'Chest Sorter (Assessment)',\n        'Bird Measurer (Assessment)'\n    ]\n\n    for title in title_cols:\n        for c in ['num_correct', 'num_incorrect', 'accuracy_group']:\n            for m in ['mean', 'max', 'min', 'median', 'sum', 'var', 'last']:\n                replace_index = df[df['title'] == title][\n                    df[f'{c}_{m}_{title}'].notnull()].index\n                df.loc[replace_index, f'{c}_title_{m}'] = df.loc[\n                    replace_index, f'{c}_{m}_{title}']\n                del df[f'{c}_{m}_{title}']\n                replace_index = df[df['title'] == title][\n                    df[f'{c}_{m}_7day_{title}'].notnull()].index\n                df.loc[replace_index, f'{c}_title_7day_{m}'] = df.loc[\n                    replace_index, f'{c}_{m}_7day_{title}']\n                del df[f'{c}_{m}_7day_{title}']\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\n# args\ndebug = True\nconfig_path = \"../config/past_summary3.yml\"\nlog_dir = \"log/\"\n\nconfigure_logger(config_path, log_dir, debug)\n\nlogging.info(f\"config: {config_path}\")\nlogging.info(f\"debug: {debug}\")\n\nconfig[\"args\"] = dict()\nconfig[\"args\"][\"config\"] = config_path\n\n# make output dir\noutput_root_dir = Path(config[\"output_dir\"])\nfeature_dir = Path(config[\"dataset\"][\"feature_dir\"])\n\nconfig_name: str = config_path.split(\"/\")[-1].replace(\".yml\", \"\")\noutput_dir = output_root_dir / config_name\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nlogging.info(f\"model output dir: {str(output_dir)}\")\n\nconfig[\"model_output_dir\"] = str(output_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data/Feature Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = Path(config[\"dataset\"][\"dir\"])\n\nif not feature_existence_checker(feature_dir, config[\"features\"]):\n    if globals().get(\"train\") is None:\n        with timer(name=\"load train\"):\n            train = pd.read_csv(input_dir / \"train.csv\")\n            \n        with timer(name=\"load test\"):\n            test = pd.read_csv(input_dir / \"test.csv\")\n    \n    train = reduce_mem_usage(train, verbose=True)\n    test = reduce_mem_usage(test, verbose=True)\n    \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not feature_existence_checker(feature_dir, config[\"features\"]):\n    with timer(\"generate features\"):\n        generate_features(\n            train, \n            test, \n            namespace=globals(),\n            required=config[\"features\"],\n            overwrite=False, \n            log=True)\n\n    if globals().get(\"train\") is not None:\n        del train, test\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer(\"feature loading\", log=True):\n    x_train = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_train.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ],\n                        axis=1,\n                        sort=False)\n    x_valid = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_valid.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ],\n                        axis=1,\n                        sort=False)\n    x_test = pd.concat([\n        pd.read_feather(feature_dir / (f + \"_test.ftr\"), nthreads=-1)\n        for f in config[\"features\"]\n    ],\n                        axis=1,\n                        sort=False)\n\nx_train = delete_duplicated_columns(x_train)\nx_valid = delete_duplicated_columns(x_valid)\nx_test = delete_duplicated_columns(x_test)\n\ngroups = x_train[\"installation_id\"].values\ngroups_valid = x_valid[\"installation_id\"].values\n\ntest_nth_assessment = get_assessment_number(x_valid, x_test)\nthreshold = np.percentile(test_nth_assessment, config[\"val\"][\"percentile\"])\n\ny_train = x_train[\"accuracy_group\"].values.reshape(-1)\ny_valid = x_valid[\"accuracy_group\"].values.reshape(-1)\ncols: List[str] = x_train.columns.tolist()\ncols.remove(\"installation_id\")\ncols.remove(\"accuracy_group\")\nx_train, x_valid, x_test = x_train[cols], x_valid[cols], x_test[cols]\n\nassert len(x_train) == len(y_train)\nlogging.debug(f\"number of features: {len(cols)}\")\nlogging.debug(f\"number of train samples: {len(x_train)}\")\nlogging.debug(f\"number of test samples: {len(x_test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer(\"Feature Selection\"):\n    to_remove = remove_correlated_features(x_train, cols)\n\ncols = [col for col in cols if col not in to_remove]\nprint('Training with {} features'.format(len(cols)))\n\nx_train, x_valid, x_test = x_train[cols], x_valid[cols], x_test[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[\"group\"] = groups\nsplits = get_validation(x_train, config)\nx_train.drop(\"group\", axis=1, inplace=True)\n\nfeature_selection_config = {\n    \"model\": {\n        \"name\": \"lgbm2\",\n        \"mode\": \"regression\",\n        \"sampling\": {\n            \"name\": \"none\"\n        },\n        \"model_params\": {\n            \"boosting_type\": \"gbdt\",\n            \"max_depth\": 6,\n            \"metrics\": \"rmse\",\n            \"num_leaves\": 25,\n            \"learning_rate\": 0.01,\n            \"subsample\": 0.8,\n            \"subsample_freq\": 1,\n            \"colsample_bytree\": 0.7,\n            \"data_random_seed\": 9999,\n            \"seed\": 9999,\n            \"bagging_seed\": 9999,\n            \"feature_fraction_seed\": 9999,\n            \"reg_alpha\": 0.1,\n            \"min_split_gain\": 0.5,\n            \"reg_lambda\": 0.1,\n            \"min_data_in_leaf\": 100,\n            \"n_jobs\": -1,\n            \"verbose\": -1,\n            \"first_metric_only\": True\n        },\n        \"train_params\": {\n            \"num_boost_round\": 5000,\n            \"early_stopping_rounds\": 100,\n            \"verbose_eval\": 100\n        }\n    },\n    \"post_process\": {\n        \"params\": {\n            \"reverse\": False,\n            \"n_overall\": 20,\n            \"n_classwise\": 20\n        }\n    }\n}\nwith timer(\"Feature Selection with importance\"):\n    model = get_model(feature_selection_config)\n    _, _, _, _, feature_importance, _ = model.cv(\n        y_train,\n        x_train[cols],\n        x_test[cols],\n        groups,\n        feature_name=cols,\n        folds_ids=splits,\n        threshold=threshold,\n        config=feature_selection_config,\n        log=True)\n\n    feature_imp = feature_importance.reset_index().rename(\n        columns={\n            \"index\": \"feature\",\n            0: \"value\"\n        })\n    cols = select_features(\n        cols,\n        feature_imp,\n        config,\n        delete_higher_importance=False)\n    print(f\"Train cols: {len(cols)}\")\n    x_train, x_valid, x_test = x_train[cols], x_valid[cols], x_test[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train model"},{"metadata":{},"cell_type":"markdown","source":"### NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.info(\"Train model\")\nlogging.info(\"NN model\")\n\nmodel = get_model(config)\n\nnn_avg_oof_preds = []\nnn_avg_test_preds = np.zeros(len(x_test))\nnn_normal_avg_oof = np.zeros(len(x_train))\n\nseed_sets = [888, 110, 225]\n\nimportances = pd.DataFrame(index=cols)\n\nfor seed in seed_sets:\n    print(\"+\" * 25)\n    print(f\"seed: {seed}\")\n    print(\"+\" * 25)\n    \n    config[\"val\"][\"params\"][\"random_state\"] = seed\n    \n    # get folds\n    x_train[\"group\"] = groups\n    splits = get_validation(x_train, config)\n    x_train.drop(\"group\", axis=1, inplace=True)\n    \n    with timer(f\"nn - {seed}\"):\n        _, _, y_oof, _, eval_results = model.cv(\n            y_train,\n            x_train[cols],\n            x_test[cols],\n            groups,\n            feature_name=cols,\n            categorical_features=[\"world\", \"session_title\", \"title\"],\n            folds_ids=splits,\n            threshold=threshold,\n            config=config)\n    nn_avg_oof_preds.append(model.raw_oof_preds)\n    nn_avg_test_preds += model.raw_test_preds / len(seed_sets)\n    nn_normal_avg_oof += model.raw_normal_oof / len(seed_sets)\n    \n    config[f\"eval_results_seed{seed}\"] = dict()\n    for k, v in eval_results.items():\n        config[f\"eval_results_seed{seed}\"][k] = v","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save"},{"metadata":{"trusted":true},"cell_type":"code","source":"save_path = output_dir / \"output.json\"\nsave_json(config, save_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_avg_oof = np.zeros(len(nn_avg_oof_preds[0]))\nfor oof in nn_avg_oof_preds:\n    nn_avg_oof += oof / len(nn_avg_oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions = [cat_avg_oof, lgb_avg_oof]\n\ndef search_averaging_weights(predictions, target: np.ndarray, trials=100):\n    lls = []\n    weights = []\n    \n    def rmse(weights):\n        final_prediction = 0\n        target_ = target / target.max()\n        for weight, prediction in zip(weights, predictions):\n            final_prediction += weight * prediction\n        return np.sqrt(mean_squared_error(target_, final_prediction))\n    \n    def qwk_score(weights):\n        final_prediction = 0\n        for weight, prediction in zip(weights, predictions):\n            final_prediction += weight * prediction\n        OptR = OptimizedRounder(n_overall=5, n_classwise=5)\n        OptR.fit(final_prediction, target)\n        final_prediction = OptR.predict(final_prediction)\n        return -calc_metric(final_prediction, target)\n    \n    for i in range(trials):\n        seed_everything(i)\n        start_values = np.random.uniform(size=len(predictions))\n        cons = ({\"type\": \"eq\", \"fun\": lambda w: 1 - sum(w)})\n        bounds = [(0, 1)] * len(predictions)\n        res = scipy.optimize.minimize(\n            qwk_score, start_values, constraints=cons, bounds=bounds, method=\"L-BFGS-B\")\n        lls.append(res[\"fun\"])\n        weights.append(res[\"x\"])\n        \n    best_loss = min(lls)\n    best_weights = weights[np.argmin(lls)]\n    \n    print(f\"Best Loss: {best_loss:.5f}\")\n    print(f\"Best Weights: {best_weights}\")\n    \n    return best_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weights = search_averaging_weights(predictions, y_oof)\n\n# avg_oof_preds_ = weights[0] * cat_avg_oof + weights[1] * lgb_avg_oof\n# avg_test_preds_ = weights[0] * cat_avg_test_preds2 + weights[1] * lgb_avg_test_preds2\n# avg_normal_oof_ = weights[0] * cat_normal_avg_oof + weights[1] * lgb_normal_avg_oof\navg_oof_preds_ = nn_avg_oof\navg_test_preds_ = nn_avg_test_preds\navg_normal_oof_ = nn_normal_avg_oof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OptR = OptimizedRounder(n_classwise=20, n_overall=20)\nOptR.fit(avg_oof_preds_, y_oof)\n\noof_preds_optimized = OptR.predict(avg_oof_preds_)\ntest_preds_optimized = OptR.predict(avg_test_preds_)\navg_normal_oof_optimized = OptR.predict(avg_normal_oof_)\n\noof_score = calc_metric(oof_preds_optimized, y_oof)\nprint(f\"Optimized OOF QWK: {oof_score:.4f}\")\nnormal_oof_score = calc_metric(avg_normal_oof_optimized, y_train)\nprint(f\"Optimized normal OOF QWK: {normal_oof_score:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"truncated_score = eval_with_truncated_data(\n    avg_normal_oof_optimized, y_train, groups, n_trials=100)\nprint(f\"Truncated OOF QWK: {truncated_score['mean']:.4f}\")\nprint(f\"Truncated OOF QWK median: {truncated_score['median']:.4f}\")\nprint(f\"Truncated OOF QWK 0.95 upper: {truncated_score['0.95upper_bound']:.4f}\")\nprint(f\"Truncated OOF QWK 0.95 lower: {truncated_score['0.95lower_bound']:.4f}\")\nprint(f\"Truncated OOF QWK std: {truncated_score['std']:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\nplot_confusion_matrix(\n    y_oof,\n    oof_preds_optimized,\n    classes=np.array([\"acc_0\", \"acc_1\", \"acc_2\", \"acc_3\"]),\n    normalize=True,\n    save_path=output_dir / \"confusion_matrix_oof.png\")\n\nplot_confusion_matrix(\n    y_train,\n    avg_normal_oof_optimized,\n    classes=np.array([\"acc_0\", \"acc_1\", \"acc_2\", \"acc_3\"]),\n    normalize=True,\n    save_path=output_dir / \"confusion_matrix_oof.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(\"oof_preds.npy\", oof_preds_optimized)\nnp.save(\"normal_oof_preds.npy\", avg_normal_oof_optimized)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\n    input_dir / \"sample_submission.csv\")\nsample_submission[\"accuracy_group\"] = test_preds_optimized\nsample_submission.to_csv('submission.csv', index=None)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"45214a9f30404102916eb10b0f085d2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"51564b6d8695430cb250da4c1232b057":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60982eab4a6f406f9febe1a86ea63b1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"test features: 100%","description_tooltip":null,"layout":"IPY_MODEL_b3b0f3ff20144c819c551cad66902743","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45214a9f30404102916eb10b0f085d2b","value":1000}},"66431bb221c14b19a0f577e914a33ed3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c12622c632a4fdaa952ec59cdb45aa3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbc0d64f4c2a48a286449e99dcc84f81","placeholder":"","style":"IPY_MODEL_51564b6d8695430cb250da4c1232b057","value":" 17000/17000 [09:32&lt;00:00, 29.68it/s]"}},"82521431336541fd908888a46aa2a0a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d91f0624606b4d6384bcff8bb35daaa0","IPY_MODEL_6c12622c632a4fdaa952ec59cdb45aa3"],"layout":"IPY_MODEL_e4bc3c00b28545c8bf050d331f9c1915"}},"843afa29b9734ba5a6468a552bd58d1b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84ebf240d70342ba8840f7de38f00aa8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b3b0f3ff20144c819c551cad66902743":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4eacdd28efa42d6a4ad543668374eb7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60982eab4a6f406f9febe1a86ea63b1c","IPY_MODEL_d9d5b10890424d63acf2308d04cd449d"],"layout":"IPY_MODEL_eb87945990be43d6a081841a095cf44d"}},"d5d28bdb0f1747b79b27cf4d0556dc06":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91f0624606b4d6384bcff8bb35daaa0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"train features: 100%","description_tooltip":null,"layout":"IPY_MODEL_843afa29b9734ba5a6468a552bd58d1b","max":17000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84ebf240d70342ba8840f7de38f00aa8","value":17000}},"d9d5b10890424d63acf2308d04cd449d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5d28bdb0f1747b79b27cf4d0556dc06","placeholder":"","style":"IPY_MODEL_66431bb221c14b19a0f577e914a33ed3","value":" 1000/1000 [01:34&lt;00:00, 10.57it/s]"}},"dbc0d64f4c2a48a286449e99dcc84f81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4bc3c00b28545c8bf050d331f9c1915":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb87945990be43d6a081841a095cf44d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}