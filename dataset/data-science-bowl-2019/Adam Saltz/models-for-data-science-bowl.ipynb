{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm #progress bars\nimport datetime as dt\nimport tensorflow as tf\n\nimport collections\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers, Model\n# Input data files are available in the \"../input/\" directory.\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_feather(\"../input/preprocessor-for-data-bowl-2019/train_processed.fth\")\n#test = pd.read_feather(\"../input/preprocessor-for-data-bowl-2019/test_processed.fth\")\ntrain_labels = pd.read_feather(\"../input/preprocessor-for-data-bowl-2019/train_labels_processed.fth\").set_index('installation_id')\n#test_sessions_map = pd.read_feather(\"../input/preprocessor-for-data-bowl-2019/test_labels_processed.fth\").to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The longest sequence has length 58988\nSEQ_LENGTH = 2000 #13000\n# see preprocessor -- this is 99 or 95 percentile of length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Parameters\nparams = {'dim': (SEQ_LENGTH,4),\n          'batch_size': 16,  \n          'shuffle': True\n}\n\nmodel_params = {\n          'LEARNING_RATE': 0.0001, #default is 0.001\n          'LOSS_FN': tf.keras.losses.CategoricalCrossentropy(),\n          'METRICS': ['categorical_accuracy'],\n                     #{'output0': ['accuracy', qwk],\n                     # 'output1': ['accuracy', qwk],\n                     # 'output2': ['accuracy', qwk],\n                     # 'output3': ['accuracy', qwk],\n                     # 'output4': ['accuracy', qwk],}\n         'CLIP_NORM': 1,\n         'LSTM_L2': 0.000001,\n    'DENSE_DROPOUT': 0.5,\n    'LSTM_DROPOUT': 0.4\n}\n\nBATCH_SIZE = params['batch_size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.regularizers import L1L2\n\nnp.random.seed(0) # set random seed for reproducibility\n# input which receives correct_assessment, session_number, game_time\n# these are not embedded!\nnon_embedded_input = tf.keras.Input(dtype='float32',name='non_embedded_input', shape=(SEQ_LENGTH, 3))# batch_shape=(BATCH_SIZE, SEQ_LENGTH, 3))\n\nevent_id_input = tf.keras.Input(dtype='float32',name='event_id_input',shape=(SEQ_LENGTH))# batch_shape=(BATCH_SIZE, SEQ_LENGTH))\ntype_input = tf.keras.Input(dtype='float32',name='type_input',  shape=(SEQ_LENGTH))\nworld_input = tf.keras.Input(dtype='float32',name='world_input',shape=(SEQ_LENGTH))\n\nevent_id_embedding_layer = (tf.keras.layers.Embedding(input_dim=390, # +1 bc of masking\n                                                     output_dim=10,  # TODO: is this too high?\n                                                     input_length=SEQ_LENGTH, # should I set this?\n                                                     embeddings_initializer='uniform', \n                                                     #embeddings_regularizer=l2(.000001), \n                                                     #activity_regularizer=l2(.000001), \n                                                     embeddings_constraint=None, \n                                                     mask_zero=True \n                                                    )(event_id_input))\ntype_embedding_layer =      (tf.keras.layers.Embedding(input_dim=5, # +1 bc of masking\n                                                     output_dim=3, \n                                                     input_length=SEQ_LENGTH, # should I set this?\n                                                     embeddings_initializer='uniform', \n                                                     #embeddings_regularizer=l2(.000001), \n                                                     #activity_regularizer=l2(.000001), \n                                                     embeddings_constraint=None, \n                                                     mask_zero=True \n                                                    )(type_input))\nworld_embedding_layer =    (tf.keras.layers.Embedding(input_dim=5, # +1 bc of masking\n                                                     output_dim=3, \n                                                     input_length=SEQ_LENGTH, # should I set this?\n                                                     embeddings_initializer='uniform', \n                                                     #embeddings_regularizer=l2(.000001), \n                                                     #activity_regularizer=l2(.000001), \n                                                     embeddings_constraint=None, \n                                                     mask_zero=True \n                                                    )(world_input))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# should consider regularizing here or with an auxiliary output?\nfull_input = tf.keras.layers.concatenate([non_embedded_input,\n                                         event_id_embedding_layer,\n                                         type_embedding_layer,\n                                         world_embedding_layer], axis=-1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MASK_VALUE = 0\n#masked_input = tf.keras.layers.Masking(mask_value=MASK_VALUE)(full_input)\nLSTM_LAYERS = 32 #\nlstm_layer = tf.keras.layers.LSTM(units=LSTM_LAYERS,\n                                 kernel_regularizer=l2(model_params['LSTM_L2']),\n                                 #activity_regularizer=l2(model_params['LSTM_L2']),# there's a bengio article saying that this is bad, could be added to the dense layer?\n                               #  dropout=model_params['LSTM_DROPOUT'],\n                                 #kernel_regularizer = L1L2(l1=0.01, l2=0.01),\n                                  return_sequences=True)(full_input) \nlstm_layer2 = tf.keras.layers.LSTM(units=LSTM_LAYERS,\n                                 kernel_regularizer=l2(model_params['LSTM_L2']),\n                                 #activity_regularizer=l2(model_params['LSTM_L2']),# there's a bengio article saying that this is bad, could be added to the dense layer?\n                                 dropout=model_params['LSTM_DROPOUT'],\n                                 #kernel_regularizer = L1L2(l1=0.01, l2=0.01\n                                                           )(lstm_layer) \n\n\n# should consider:\n#  stateful = True\n## DO NOT:  this holds the state *between aligned samples*.  E.g. you slice your long sequence into several, then feed those sequences in one at a time.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_layer = tf.keras.layers.BatchNormalization()(lstm_layer2)\nx = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(normalized_layer))\n#x = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(x))\n#x = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(x))\n#x = tf.keras.layers.Dropout(model_params['DENSE_DROPOUT'])(tf.keras.layers.Dense(64, activation='relu')(x))\n\noutput0 = tf.keras.layers.Dropout(0)(tf.keras.layers.Dense(4, activation='softmax', name='output0')(lstm_layer2))\n#output1 = tf.keras.layers.Dense(4, activation='softmax', name='output1')(x)\n#output2 = tf.keras.layers.Dense(4, activation='softmax', name='output2')(x)\n#output3 = tf.keras.layers.Dense(4, activation='softmax', name='output3')(x)\n#output4 = tf.keras.layers.Dense(4, activation='softmax', name='output4')(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_labels.groupby('installation_id')['accuracy_group'].apply(lambda x : x.to_numpy()).to_frame()\ntrain_labels = train_labels.to_dict(orient='dict')['accuracy_group']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_inputs = [non_embedded_input, event_id_input, type_input, world_input]\nmodel = tf.keras.Model(inputs=model_inputs, outputs=output0)\n\nmy_optimizer = tf.keras.optimizers.Adam(learning_rate=model_params['LEARNING_RATE'],  \n                                        beta_1=0.9, \n                                        beta_2=0.999, \n                                        amsgrad=True,)\n#                                        clipnorm = model_params['CLIP_NORM'])\n\n#my_optimizer_2 = tf.keras.optimizers.Adamax(learning_rate=model_params['LEARNING_RATE'])\n\nimport pickle\nwith open('../input/preprocessor-for-data-bowl-2019/event_ids_map.pkl', 'rb') as file:\n    event_ids_map = pickle.load(file)\nwith open('../input/preprocessor-for-data-bowl-2019/took_assessments_map.pkl','rb') as file:\n    took_assessments_map = pickle.load(file)\n#assessment_codes = [k for k in reverse_activities_map if 'Assessment' in reverse_activities_map[k]]\nthe_models = {i : tf.keras.models.clone_model(model) for i in took_assessments_map}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\ndef save_name(activity):\n    return ModelCheckpoint(filepath=str(activity), save_best_only = True)\n\ndef useful_callbacks(activity):\n    return [#save_name(activity), \n            EarlyStopping(patience=50, restore_best_weights=True)]\n\nhistory = {}\n\nfor assessment in the_models:  \n    print(\"Starting model for \" + assessment + \".\")\n    the_models[assessment].compile(optimizer = my_optimizer,\n                        loss = model_params['LOSS_FN'],\n                        metrics= model_params['METRICS']\n                            )\n    print(\"Compiled model for \" + assessment + \".\")\n    print(\"Time to fit.\")\n    with np.load(\"../input/preprocessor-for-data-bowl-2019/data/X_\" + assessment + \".npz\", allow_pickle=True) as data_X: # TODO: could consider mmap_mode?  dunno\n        X0 = data_X['x0']#[:,-SEQ_LENGTH:]\n        X1 = data_X['x1']#[:,-SEQ_LENGTH:]\n        X2 = data_X['x2']#[:,-SEQ_LENGTH:]\n        X3 = data_X['x3']#[:,-SEQ_LENGTH:]\n    Y = np.load(\"../input/preprocessor-for-data-bowl-2019/data/Y_\" + assessment + \".npy\", allow_pickle=True)\n    history[assessment] = the_models[assessment].fit([X0, X1, X2, X3], Y,\n                         validation_split=.1,\n                         epochs = 200,\n                         callbacks = useful_callbacks(assessment),\n                         verbose=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for assessment in the_models:\n    tf.keras.models.save_model(the_models[assessment], assessment + '.h5', save_format='h5')\n    with open('history_' + assessment + '.pkl', 'wb') as open_file:\n        pickle.dump(history[assessment].history,open_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for assessment in history:\n    # Plot training & validation accuracy values\n    plt.plot(history[assessment].history['accuracy'])\n    plt.plot(history[assessment].history['validation_accuracy'])\n    plt.title('Model accuracy: ' + assessment)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n# Plot training & validation loss values\n    plt.plot(history[assessment].history['loss'])\n    plt.plot(history[assessment].history['val_loss'])\n    plt.title('Model loss: ' + assessment)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}