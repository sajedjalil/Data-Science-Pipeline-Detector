{"cells":[{"metadata":{},"cell_type":"markdown","source":"- Adversarial validation based on **DSB2019_XGB_c08-01**"},{"metadata":{},"cell_type":"markdown","source":"# Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport math\nimport pandas as pd\npd.set_option(\"display.max_rows\", 1000)\npd.set_option(\"display.max_columns\", 10000)\npd.set_option(\"display.max_colwidth\", 200)\n\nfrom numba import jit\n\nimport copy\nimport gc\ngc.collect()\n\nfrom contextlib import contextmanager\nimport time\nfrom datetime import datetime, timedelta, timezone\nJST = timezone(timedelta(hours=+9), 'JST')\nnotebookstart = time.time()\nfrom timeit import default_timer as timer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport itertools\nimport pickle, gzip\nimport glob\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport category_encoders as ce\nimport json\nimport scipy as sp\nfrom functools import partial\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"SEED = 100\nFOLDS = 5\nN_FOLD = 0\n\ntrain_dir = '/kaggle/input/data-science-bowl-2019/train.csv'\ntrain_labels_dir = '/kaggle/input/data-science-bowl-2019/train_labels.csv'\nspecs_dir = '/kaggle/input/data-science-bowl-2019/specs.csv'\ntest_dir = '/kaggle/input/data-science-bowl-2019/test.csv'\nsample_submission_dir = '/kaggle/input/data-science-bowl-2019/sample_submission.csv'\n\nout_dir = '/kaggle/working'\n\nDEBUG = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions: Helper"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_torch(s):\n    os.environ['PYTHONHASHSEED'] = str(s)\n    np.random.seed(s)\n#     torch.manual_seed(s)\n#     torch.cuda.manual_seed(s)\n#     torch.backends.cudnn.benchmark = False\n#     torch.backends.cudnn.deterministic = True\nseed_torch(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_pickle(obj, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj , f)\n        \ndef read_pickle(filename):\n    with open(filename, 'rb') as f:\n        obj = pickle.load(f)\n    return obj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def high_corr(df, th):\n    a = df.corr(method='pearson')\n    a = a*np.tri(len(a), k=-1)\n    b = a[abs(a)>=th]\n    display(b.loc[b.notnull().any(axis=1), b.notnull().any(axis=0)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions: Data import & general processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_HISTORY_GROUP = 0\n\ndef data_prep(is_train=True):\n    print(f'\\n{\"*\"*30}\\n{\"*\"*10} Prepare X {\"*\"*10}\\n{\"*\"*30}')\n    ##### Import & Cast dtypes #####\n    df = pd.read_csv(train_dir if is_train else test_dir)\n    print(f'length:\\n{len(df):,}')\n    \n    df['event_code'] = df['event_code'].astype(str)\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    print(f'dtypes:\\n{df.dtypes}')\n    display(df.head())\n    \n\n    if is_train:\n        ##### Import labels #####\n        train_labels_df = pd.read_csv(train_labels_dir)\n        session_has_labels = list(set(train_labels_df['game_session'].values))\n        print(f'Length of train_labels:\\n\\t{len(train_labels_df):,}')\n        print(f'Unique game_session in train_labels:\\n\\t{len(session_has_labels):,}')\n        \n        ##### Extract the final assessment & Delete after the final assessment #####\n        max_timestamp = df[\n            (df['event_count']==1) & (df['game_session'].isin(session_has_labels))\n                ].groupby('installation_id')[['timestamp']].max().reset_index()\n        max_timestamp.columns = ['installation_id', 'max_timestamp']\n        df['flg_non_future'] = pd.merge(df[['installation_id']], max_timestamp, on='installation_id', how='left')['max_timestamp']\n        \n        print(f'All rows:\\n\\t{len(df):,}')\n        df = df[(df['flg_non_future'].notnull()) & (df['timestamp']<=df['flg_non_future'])]\n        print(f'After exclude future rows:\\n\\t{len(df):,}')\n        \n    else:\n        df['flg_non_future'] = 0\n        \n    ##### Sort records along with \"installation_id\" & \"timestamp\" #####\n    df.sort_values(['installation_id','timestamp'], inplace=True)\n    \n    \n    ##### Create \"history_group\" #####\n    if is_train:\n        df['history_group'] = 0\n        df.loc[(df['event_count']==1) & (df['game_session'].isin(session_has_labels)), ['history_group']] = 1\n\n        df.sort_values(['installation_id','timestamp'], inplace=True, ascending=[True,False])\n        df['history_group'] = df.groupby(['installation_id'])['history_group'].cumsum()\n        \n    else:\n        df['history_group'] = 1\n        \n\n    df.sort_values(['installation_id','timestamp'], inplace=True)\n    display(df.head())\n    global MAX_HISTORY_GROUP\n    MAX_HISTORY_GROUP = df[\"history_group\"].max()\n    print(f'Max assessment in history:\\t{MAX_HISTORY_GROUP}')\n    \n    if is_train:\n        print(f'\\n{\"*\"*30}\\n{\"*\"*10} Prepare y {\"*\"*10}\\n{\"*\"*30}')\n        y = pd.merge(\n            train_labels_df[['installation_id','game_session','accuracy_group']],\n            df.loc[(df['event_count']==1), ['installation_id','game_session','history_group']].drop_duplicates(),\n            on=['installation_id','game_session'],\n            how='left')\n        y = y[['installation_id','history_group','accuracy_group']]\n\n        y['sample_weight'] = y.groupby('installation_id').transform('count')['history_group']\n        y['sample_weight'] = 1/y['sample_weight']\n        y = y.set_index(['installation_id','history_group']).sort_index()\n\n        display(y.head())\n        print(f'Shape of y:\\n\\t{y.shape}')\n        \n        return df, y\n    else:\n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Create fold infomation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_INS_ID_LIST_TRAIN(df, seed):\n    INS_ID_LIST_TRAIN = df['installation_id'].unique()\n    INS_ID_LIST_TRAIN.sort()\n    random.Random(seed).shuffle(INS_ID_LIST_TRAIN)\n    return INS_ID_LIST_TRAIN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df, y = data_prep(is_train=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prep features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def import_FE(FE_dir_c, pkl_name, score_df_01dir, score_df_02dir, cutoff_01, cutoff_02):\n#     drop_col_list = []\n    \n#     scores_df = pd.read_csv(score_df_01dir+'/scores_df.csv')\n#     drop_col_list_ni01 = scores_df.loc[int(cutoff_01*1.2):, 'feature'].tolist()\n\n#     scores_df = pd.read_csv(score_df_02dir+'/scores_df.csv')\n#     drop_col_list_ni02 = scores_df.loc[int(cutoff_02):, 'feature'].tolist()\n\n#     drop_col_list = list(set(drop_col_list+drop_col_list_ni01+drop_col_list_ni02))\n    \n#     FE_train = read_pickle(FE_dir_c+'/'+pkl_name)\n#     FE_train.drop(columns=[c for c in FE_train.columns if c in drop_col_list], inplace=True)\n#     return FE_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FE-c08"},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_dir_c = '/kaggle/input/dsb2019-fe-c08'\npkl_name = 'FE_XGB_c02-07_train.pkl'\n\n\nFE = read_pickle(FE_dir_c+'/'+pkl_name)\nX = FE\n\ndisplay(X.head())\nprint(f'X.shape\\t:{X.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_drop_col_list(score_df_01dir, type_01='gain>0', score_df_02dir=None, type_02='gain>0', cutoff_01=None, cutoff_02=None):\n    drop_col_list = []\n    drop_col_list_ni01 = []\n    drop_col_list_ni02 = []\n    \n    scores_df = pd.read_csv(score_df_01dir+'/scores_df.csv')\n    if type_01=='cutoff':\n        drop_col_list_ni01 = scores_df.loc[int(cutoff_01*1.2):, 'feature'].tolist()\n    elif type_01=='gain>0':\n        drop_col_list_ni01 = scores_df.loc[scores_df['gain_score']<=0, 'feature'].tolist()\n\n    if score_df_02dir!=None:\n        scores_df = pd.read_csv(score_df_02dir+'/scores_df.csv')\n        if type_02=='cutoff':\n            drop_col_list_ni02 = scores_df.loc[int(cutoff_02):, 'feature'].tolist()\n        elif type_02=='gain>0':\n            drop_col_list_ni02 = scores_df.loc[scores_df['gain_score']<=0, 'feature'].tolist()\n            \n    drop_col_list = list(set(drop_col_list+drop_col_list_ni01+drop_col_list_ni02))\n        \n    return drop_col_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FE-c08"},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df_01dir = '/kaggle/input/dsb2019-xgb-fs-c08-ni01-03'\nscore_df_02dir = '/kaggle/input/dsb2019-xgb-fs-c08-ni02-03'\ncutoff_01 = 350\ncutoff_02 = 170\ntype_01 = 'cutoff'\ntype_02 = 'cutoff'\n\n\ndrop_col_list = create_drop_col_list(score_df_01dir, type_01, score_df_02dir, type_02, cutoff_01, cutoff_02)\nX.drop(columns=drop_col_list, inplace=True, errors='ignore')\ndisplay(X.head())\nprint(f'X.shape\\t:{X.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_df = pd.DataFrame({'col_name':X.columns.tolist()})\n\ncols_df.to_csv(out_dir+'/cols_df.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare test"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_dir = '/kaggle/input/dsb2019-lgb-c08-01'\nxgb_dir = '/kaggle/input/dsb2019-xgb-c08-01'\n\nLGB_FEAT = pd.read_csv(lgb_dir+'/cols_df.csv')['col_name'].tolist()\nXGB_FEAT = pd.read_csv(xgb_dir+'/cols_df.csv')['col_name'].tolist()\nXGB_FEAT = [c.replace('~','[').replace('|',']') for c in XGB_FEAT]\n\nFEATURE_ORDER = list(set(LGB_FEAT + XGB_FEAT + ['title_from','event_code_from','event_id_from']))\n\nprint(f'# of lgb_feat:\\t{len(LGB_FEAT)}')\nprint(f'# of xgb_feat:\\t{len(XGB_FEAT)}')\nprint(f'# of FEATURE_ORDER:\\t{len(FEATURE_ORDER)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_ohe(df):\n    cols_ohe = [c for c in df.columns if df[c].dtype.name in ['object','category']]\n    ce_ohe = ce.OneHotEncoder(cols=cols_ohe, handle_unknown='error', use_cat_names=True)\n    out_df_tmp = ce_ohe.fit_transform(df[cols_ohe])\n    out_df_tmp.sort_index(axis=1, inplace=True)\n    df.drop(columns=cols_ohe, inplace=True)\n    df = pd.concat([df, out_df_tmp], axis=1)\n    df.columns = [c.replace('[','~').replace(']','|') for c in df.columns]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_ohe(df, cols, n_chank=100):\n    start = time.time()\n    out_df = pd.DataFrame()\n    ins_id = []\n    ins_id_list = df['installation_id'].unique().tolist()\n    \n    for i, i_id in enumerate(ins_id_list):\n        print('\\r',end='',flush=True)\n        print(f'{i+1} / {len(ins_id_list)}  {(time.time()-start)/60:.1f} mins', end='',flush=True)\n        ins_id.append(i_id)\n        \n        if ((i+1)%n_chank!=0)&((i+1)!=len(ins_id_list)):\n            continue\n\n        df_tmp = df[df['installation_id'].isin(ins_id)]\n        ce_ohe = ce.OneHotEncoder(cols=cols, handle_unknown='error', use_cat_names=True)\n        out_df_tmp = ce_ohe.fit_transform(df_tmp[['installation_id']+cols])\n        out_df_tmp = out_df_tmp.groupby(['installation_id']).agg([np.mean,np.sum]) # DSB2019_FE_02 -> mean&sum\n        created_cols = [c[0]+'_'+c[1] for c in out_df_tmp.columns]\n        out_df_tmp.reset_index(inplace=True)\n        out_df_tmp.columns = ['installation_id']+created_cols\n        out_df_tmp.set_index('installation_id', inplace=True)\n        out_df = pd.concat([out_df, out_df_tmp], axis=0)\n        \n        ins_id = []\n    \n    out_df = out_df.reset_index().set_index('installation_id').sort_index()\n    return out_df\ndef create_last(df):\n    out_df = df.groupby(['installation_id'])[['timestamp']].max().reset_index()\n    out_df = pd.merge(out_df, df, on=['installation_id','timestamp'], how='left')\n    out_df.set_index('installation_id', inplace=True)\n    return out_df\ndef create_assess(df):\n    out_df = df.loc[(df['type']=='Assessment')&(df['title']!='Bird Measurer (Assessment)')&(df['event_code']=='4100')|\n                    (df['type']=='Assessment')&(df['title']=='Bird Measurer (Assessment)')&(df['event_code']=='4110'),\n                   ['installation_id','game_session','event_data']]\n    \n    out_df['num_correct']   = out_df['event_data'].str.contains('true')*1\n    out_df['num_incorrect'] = out_df['event_data'].str.contains('false')*1\n    \n    out_df = out_df.groupby(['installation_id','game_session'])[['num_correct','num_incorrect']].sum().reset_index()\n    out_df['accuracy'] = out_df['num_correct']/(out_df['num_correct']+out_df['num_incorrect'])\n    \n    out_df['accuracy_group'] = 0\n    out_df.loc[(out_df['accuracy']==1.0), ['accuracy_group']] = 3\n    out_df.loc[(out_df['accuracy']==0.5), ['accuracy_group']] = 2\n    out_df.loc[(out_df['accuracy']>0.0)&(out_df['accuracy']<0.5), ['accuracy_group']] = 1\n    \n    out_df = out_df.groupby(['installation_id'])[['num_correct','num_incorrect','accuracy','accuracy_group']].agg([np.mean]) # .agg([np.sum,np.mean])\n    created_cols = [c[0]+'_'+c[1] for c in out_df.columns]\n    out_df.reset_index(inplace=True)\n    out_df.columns = ['installation_id']+created_cols\n    out_df.set_index('installation_id', inplace=True)\n    return out_df\ndef create_assess_match_last(df):\n    last_assess = df[df['type']=='Assessment'].groupby(['installation_id'])['timestamp'].max().reset_index()\n    last_assess = pd.merge(last_assess, df[['installation_id','timestamp','title']], \n                           on=['installation_id','timestamp'], how='left')[['installation_id','title']]\n    last_assess['match_last'] = 1\n    \n    out_df = df.loc[(df['type']=='Assessment')&(df['title']!='Bird Measurer (Assessment)')&(df['event_code']=='4100')|\n                    (df['type']=='Assessment')&(df['title']=='Bird Measurer (Assessment)')&(df['event_code']=='4110'),\n                   ['installation_id','game_session','event_data','title']]\n    \n    out_df = pd.merge(out_df, last_assess, on=['installation_id','title'], how='left')\n    out_df = out_df[out_df['match_last']==1]\n    \n    out_df['num_correct_match_last']   = out_df['event_data'].str.contains('true')*1\n    out_df['num_incorrect_match_last'] = out_df['event_data'].str.contains('false')*1\n    \n    out_df = out_df.groupby(['installation_id','game_session'])[['num_correct_match_last','num_incorrect_match_last']].sum().reset_index()\n    out_df['accuracy_match_last'] = out_df['num_correct_match_last']/(out_df['num_correct_match_last']+out_df['num_incorrect_match_last'])\n    \n    out_df = out_df.groupby(['installation_id'])[['num_correct_match_last','num_incorrect_match_last','accuracy_match_last']].agg([np.mean]) # .agg([np.sum,np.mean])\n    created_cols = [c[0]+'_'+c[1] for c in out_df.columns]\n    out_df.reset_index(inplace=True)\n    out_df.columns = ['installation_id']+created_cols\n    out_df.set_index('installation_id', inplace=True)\n    return out_df\ndef create_diff_timestamp(df):\n    wo_clip = df['type']!='Clip'\n    out_df = df[wo_clip].groupby(['installation_id','game_session'])[['timestamp']].diff().fillna(0)\n    out_df['timestamp'] = out_df['timestamp'].dt.seconds\n    \n    out_df = pd.concat([df.loc[wo_clip, ['installation_id','world','type','title','event_code','event_id','game_session']], out_df], axis=1)\n    out_df = out_df.groupby(['installation_id','world','type','title','event_code','event_id','game_session'])[['timestamp']].agg([np.sum])\n    \n    out_df.columns = ['timespend']\n    out_df.reset_index(inplace=True)\n    return out_df\ndef create_timespend(diff_FE, col_comb):\n    v = 'timespend'\n    out_df = pd.DataFrame()\n    for c_comb in col_comb:\n        tmp = pd.pivot_table(diff_FE, index=['installation_id'], columns=c_comb, values=v, aggfunc=[np.mean,np.sum]) # [np.mean,np.sum])\n        cols = []\n        for c in tmp.columns:\n            col = v\n            for i in range(len(c)):\n                col += '_'+c[-(i+1)]\n            cols += [col]\n        tmp.columns = cols\n        out_df = pd.concat([out_df, tmp], axis=1)\n    return out_df\ndef create_timepast(df, n_from, n_to):\n    game_number_df = df.reset_index()\n    game_number_df['game_number'] = game_number_df.groupby('installation_id')['index'].rank(ascending=False)\n    \n    game_number_df.drop(columns=['index'], inplace=True)\n    \n    out_df = game_number_df[game_number_df['game_number']==n_to]\n    out_df.columns = [c+'_to' if c!='installation_id' else c for c in out_df.columns]\n    \n    from_df = game_number_df[game_number_df['game_number']==n_from]\n    from_df.columns = [c+'_from' if c!='installation_id' else c for c in from_df.columns]\n    \n    out_df = pd.merge(out_df, from_df, on='installation_id', how='left')\n        \n    out_df.set_index('installation_id', inplace=True)\n    return out_df\ndef create_json(df, n_chank=100, session_wise=False):\n    start = time.time()\n    out_df = pd.DataFrame()\n    ins_id = []\n    ins_id_list = df['installation_id'].unique().tolist()\n    \n    for i, i_id in enumerate(ins_id_list):\n        print(f'{i+1} / {len(ins_id_list)}  {(time.time()-start)/60:.1f} mins\\r', end='',flush=True)\n        ins_id.append(i_id)\n        \n        if ((i+1)%n_chank!=0)&((i+1)!=len(ins_id_list)):\n            continue\n        \n        df_tmp = df[df['installation_id'].isin(ins_id)].reset_index(drop=True)\n        out_df_tmp = pd.io.json.json_normalize(df_tmp['event_data'].apply(json.loads))\n        out_df_tmp = pd.concat([df_tmp[['installation_id','game_session']], out_df_tmp], axis=1)\n        ##### Turn on when session-wise #####\n        if session_wise:\n            out_df_tmp = out_df_tmp.groupby(['installation_id','game_session']).agg([np.sum])\n            out_df_tmp.columns = [c[0] for c in out_df_tmp.columns]\n            out_df_tmp.reset_index(inplace=True)\n        #####\n        \n        out_df_tmp = out_df_tmp.groupby(['installation_id']).agg([np.mean,np.sum])\n        created_cols = [c[0]+'_'+c[1] for c in out_df_tmp.columns]\n        out_df_tmp.reset_index(inplace=True)\n        out_df_tmp.columns = ['installation_id']+created_cols\n        out_df_tmp.set_index('installation_id', inplace=True)\n        out_df = pd.concat([out_df, out_df_tmp], axis=0)\n        \n        ins_id = []\n    \n    out_df = out_df.reset_index().set_index('installation_id').sort_index()\n    return out_df\ndef create_json_ohe(df, n_chank=100, session_wise=False):\n    start = time.time()\n    out_df = pd.DataFrame(index=[], columns=['installation_id'])\n    out_df.set_index(['installation_id'], inplace=True)\n    ins_id = []\n    ins_id_list = df['installation_id'].unique().tolist()\n    \n    OHE_cols = [\n        \"object_type\",\n        \"layout.row1\",\n        \"chests\",\n        \"gate.side\",\n        \"layout.row2\",\n        \"target_distances\",\n        \"diet\",\n        \"holding_shell\",\n        \"layout.right.pig\",\n        \"object\",\n        \"toy_earned\",\n        \"source\",\n        \"mode\",\n        \"round_target.type\",\n        \"location\",\n        \"has_water\",\n        \"shells\",\n        \"dinosaur\",\n        \"jar_filled\",\n        \"bowls\",\n    ]\n    \n    \n    for i, i_id in enumerate(ins_id_list):\n        print(f'\\r{i+1} / {len(ins_id_list)}  {(time.time()-start)/60:.1f} mins', end='',flush=True)\n        ins_id.append(i_id)\n        \n        if ((i+1)%n_chank!=0)&((i+1)!=len(ins_id_list)):\n            continue\n\n        df_tmp = df.loc[df['installation_id'].isin(ins_id), ['installation_id','game_session','event_data']].reset_index(drop=True)\n        out_df_tmp = pd.io.json.json_normalize(df_tmp['event_data'].apply(json.loads))\n        \n        cols_in_tmp = [c for c in out_df_tmp.columns if c in OHE_cols]\n        if len(cols_in_tmp)==0:\n            ins_id = []\n            continue\n            \n        out_df_tmp = out_df_tmp[cols_in_tmp]\n        out_df_tmp = out_df_tmp.astype(str)\n        ce_ohe = ce.OneHotEncoder(cols=out_df_tmp.columns.tolist(), handle_unknown='error', use_cat_names=True)\n        out_df_tmp = ce_ohe.fit_transform(out_df_tmp)\n        out_df_tmp = pd.concat([df_tmp[['installation_id','game_session']], out_df_tmp], axis=1)\n        \n        ##### Turn on when session-wise #####\n        if session_wise:\n            out_df_tmp = out_df_tmp.groupby(['installation_id','game_session']).agg([np.sum])\n            out_df_tmp.columns = [c[0] for c in out_df_tmp.columns]\n            out_df_tmp.reset_index(inplace=True)\n        #####\n        \n        out_df_tmp = out_df_tmp.groupby(['installation_id']).agg([np.mean,np.sum])\n        created_cols = [c[0]+'_'+c[1] for c in out_df_tmp.columns]\n        out_df_tmp.reset_index(inplace=True)\n        out_df_tmp.columns = ['installation_id']+created_cols\n        out_df_tmp.set_index('installation_id', inplace=True)\n        out_df = pd.concat([out_df, out_df_tmp], axis=0)\n        \n        ins_id = []\n    \n    out_df = out_df.reset_index().set_index('installation_id').sort_index()\n    return out_df\ndef create_ohe_session(df, cols, n_chank=100):\n    start = time.time()\n    out_df = pd.DataFrame()\n    ins_id = []\n    ins_id_list = df['installation_id'].unique().tolist()\n    \n    for i, i_id in enumerate(ins_id_list):\n        print('\\r',end='',flush=True)\n        print(f'{i+1} / {len(ins_id_list)}  {(time.time()-start)/60:.1f} mins', end='',flush=True)\n        ins_id.append(i_id)\n        \n        if ((i+1)%n_chank!=0)&((i+1)!=len(ins_id_list)):\n            continue\n\n        df_tmp = df.loc[df['installation_id'].isin(ins_id), ['installation_id','game_session']+cols]\n        df_tmp.drop_duplicates(subset=['installation_id','game_session']+cols, inplace=True)\n        ce_ohe = ce.OneHotEncoder(cols=cols, handle_unknown='error', use_cat_names=True)\n        out_df_tmp = ce_ohe.fit_transform(df_tmp[['installation_id','game_session']+cols])\n        out_df_tmp = out_df_tmp.groupby(['installation_id']).agg([np.mean,np.sum]) # DSB2019_FE_02 -> mean&sum\n        created_cols = [c[0]+'_'+c[1] for c in out_df_tmp.columns]\n        out_df_tmp.reset_index(inplace=True)\n        out_df_tmp.columns = ['installation_id']+created_cols\n        out_df_tmp.set_index('installation_id', inplace=True)\n        out_df = pd.concat([out_df, out_df_tmp], axis=0)\n        \n        ins_id = []\n    \n    out_df = out_df.reset_index().set_index('installation_id').sort_index()\n    return out_df\ndef create_ohe_session_mean(df, cols, n_chank=100):\n    start = time.time()\n    out_df = pd.DataFrame()\n    ins_id = []\n    ins_id_list = df['installation_id'].unique().tolist()\n    \n    for i, i_id in enumerate(ins_id_list):\n        print('\\r',end='',flush=True)\n        print(f'{i+1} / {len(ins_id_list)}  {(time.time()-start)/60:.1f} mins', end='',flush=True)\n        ins_id.append(i_id)\n        \n        if ((i+1)%n_chank!=0)&((i+1)!=len(ins_id_list)):\n            continue\n\n        df_tmp = df.loc[df['installation_id'].isin(ins_id), ['installation_id','game_session']+cols]\n        df_tmp.drop_duplicates(subset=['installation_id','game_session']+cols, inplace=True)\n        ce_ohe = ce.OneHotEncoder(cols=cols, handle_unknown='error', use_cat_names=True)\n        out_df_tmp = ce_ohe.fit_transform(df_tmp[['installation_id','game_session']+cols])\n        out_df_tmp = out_df_tmp.groupby(['installation_id','game_session']).agg([np.sum])\n        out_df_tmp.columns = [c[0]+'_'+c[1] for c in out_df_tmp.columns]\n        out_df_tmp.reset_index(inplace=True)\n        \n        out_df_tmp = out_df_tmp.groupby(['installation_id']).agg([np.mean])\n        created_cols = [c[0]+'_'+c[1] for c in out_df_tmp.columns]\n        out_df_tmp.reset_index(inplace=True)\n        \n        out_df_tmp.columns = ['installation_id']+created_cols\n        out_df_tmp.set_index('installation_id', inplace=True)\n        out_df = pd.concat([out_df, out_df_tmp], axis=0)\n        \n        ins_id = []\n    \n    out_df = out_df.reset_index().set_index('installation_id').sort_index()\n    return out_df\n\n\ndef hst_ctg_FE(df):\n    cols = ['type','title','event_code']\n    ctg_FE = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_ohe(tmp, cols, n_chank=100)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        ctg_FE = pd.concat([ctg_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    ctg_FE = ctg_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(ctg_FE.head())\n    print(f'ctg_FE.shape:\\t{ctg_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return ctg_FE\ndef hst_ctg_FE_03(df):\n    df['title_event_code'] = df['title']+'_'+df['event_code']\n    \n    cols = ['title_event_code']\n    ctg_FE = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_ohe(tmp, cols, n_chank=100)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        ctg_FE = pd.concat([ctg_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    ctg_FE = ctg_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(ctg_FE.head())\n    print(f'ctg_FE.shape:\\t{ctg_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return ctg_FE\ndef hst_ctg_FE_04(df):\n    cols = ['event_id']\n    ctg_FE = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_ohe(tmp, cols, n_chank=100)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        ctg_FE = pd.concat([ctg_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    ctg_FE = ctg_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(ctg_FE.head())\n    print(f'ctg_FE.shape:\\t{ctg_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return ctg_FE\ndef hst_last_FE(df):\n    last_FE = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_last(tmp)[['title','timestamp']]\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        last_FE = pd.concat([last_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    # last_FE_train['hour'] = (last_FE_train['timestamp'].dt.hour).astype(str)\n    # last_FE_train['daysofweek'] = (last_FE_train['timestamp'].dt.dayofweek).astype(str)\n\n    last_FE = last_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(last_FE.head())\n    print(f'last_FE.shape:\\t{last_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return last_FE\ndef hst_assess_FE(df):\n    assess_FE = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_assess(tmp)\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        assess_FE = pd.concat([assess_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    assess_FE = assess_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(assess_FE.head())\n    print(f'assess_FE.shape:\\t{assess_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return assess_FE\ndef hst_assess_match_last_FE(df):\n    assess_match_last_FE = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_assess_match_last(tmp)\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        assess_match_last_FE = pd.concat([assess_match_last_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    assess_match_last_FE = assess_match_last_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(assess_match_last_FE.head())\n    print(f'assess_match_last_FE.shape:\\t{assess_match_last_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return assess_match_last_FE\ndef hst_diff_FE(df):\n    col_comb = [\n        ['type'],\n        ['title'],\n        ['world','type'],\n    ]\n\n    diff_FE_1 = pd.DataFrame()\n    diff_FE_2 = pd.DataFrame()\n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp0 = create_diff_timestamp(tmp)\n        tmp1 = create_timespend(tmp0, col_comb)\n\n        tmp2 = tmp0.groupby('installation_id')[['timespend']].sum()\n        tmp2.columns = ['timespend_total']\n\n        tmp1['history_group'] = i+1\n        tmp2['history_group'] = i+1\n        tmp1.drop(columns=[c for c in tmp1.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        tmp2.drop(columns=[c for c in tmp2.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        diff_FE_1 = pd.concat([diff_FE_1, tmp1], axis=0)\n        diff_FE_2 = pd.concat([diff_FE_2, tmp2], axis=0)\n\n        del tmp; gc.collect();\n\n    diff_FE_1 = diff_FE_1.reset_index().set_index(['installation_id','history_group']).sort_index()\n    diff_FE_2 = diff_FE_2.reset_index().set_index(['installation_id','history_group']).sort_index()\n\n    display(diff_FE_1.head())\n    display(diff_FE_2.head())\n    print(f'diff_FE_1.shape:\\t{diff_FE_1.shape}')\n    print(f'diff_FE_2.shape:\\t{diff_FE_2.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return diff_FE_1, diff_FE_2\ndef hst_timepast_1(df):\n    cols = ['world','type','title']\n    timepast_1 = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_timepast(tmp,2,1)\n        for c in cols:\n            tmp[c+'_match_from_to'] = (tmp[c+'_to']==tmp[c+'_from'])*1\n\n        tmp['history_group'] = i+1\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        timepast_1 = pd.concat([timepast_1, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    timepast_1 = timepast_1.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(timepast_1.head())\n    print(f'timepast_1.shape:\\t{timepast_1.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return timepast_1\ndef hst_json(df, session_wise=False, col_surfix='_00'):\n    json_FE = pd.DataFrame()\n\n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_json(tmp, n_chank=100, session_wise=session_wise)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.columns = [c+col_surfix if c not in ['installation_id','history_group'] else c for c in tmp.columns]\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        json_FE = pd.concat([json_FE, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    json_FE = json_FE.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(json_FE.head())\n    print(f'json_FE.shape:\\t{json_FE.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return json_FE\ndef hst_json_ohe(df, session_wise=True, col_surfix='_00'):\n    json_ohe = pd.DataFrame()\n\n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_json_ohe(tmp, n_chank=100, session_wise=session_wise)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.columns = [c+col_surfix if c not in ['installation_id','history_group'] else c for c in tmp.columns]\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        json_ohe = pd.concat([json_ohe, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    json_ohe = json_ohe.reset_index().set_index(['installation_id','history_group']).sort_index()\n\n    display(json_ohe.head())\n    print(f'json_ohe.shape:\\t{json_ohe.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return json_ohe\ndef hst_ohe_sw(df, col_surfix='_00'):\n    cols = ['type','title']\n    ctg_FE_session = pd.DataFrame()\n    \n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_ohe_session(tmp, cols, n_chank=100)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.columns = [c+col_surfix if c not in ['installation_id','history_group'] else c for c in tmp.columns]\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        ctg_FE_session = pd.concat([ctg_FE_session, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    ctg_FE_session = ctg_FE_session.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(ctg_FE_session.head())\n    print(f'ctg_FE_session.shape:\\t{ctg_FE_session.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return ctg_FE_session\ndef hst_ohe_sw_mean(df, col_surfix='_00'):\n    cols = ['event_code']\n\n    ctg_FE_session_mean = pd.DataFrame()\n    for i in range(MAX_HISTORY_GROUP):\n        tmp = df[df['history_group']>=(i+1)].drop(columns=['history_group'])\n\n        tmp = create_ohe_session_mean(tmp, cols, n_chank=100)\n\n        col_drop = [c for c in tmp.columns if ' Level ' in c]\n        tmp.drop(columns=col_drop, inplace=True)\n\n        tmp['history_group'] = i+1\n        tmp.columns = [c+col_surfix if c not in ['installation_id','history_group'] else c for c in tmp.columns]\n        tmp.drop(columns=[c for c in tmp.columns if c not in FEATURE_ORDER+['installation_id','history_group']], inplace=True)\n        ctg_FE_session_mean = pd.concat([ctg_FE_session_mean, tmp], axis=0)\n\n        del tmp; gc.collect();\n\n    ctg_FE_session_mean = ctg_FE_session_mean.reset_index().set_index(['installation_id','history_group']).sort_index()\n    display(ctg_FE_session_mean.head())\n    print(f'ctg_FE_session_mean.shape:\\t{ctg_FE_session_mean.shape}')\n    print(f'***** History group {MAX_HISTORY_GROUP}: Done *****')\n    return ctg_FE_session_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_df = data_prep(is_train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    ids_list = list(set(test_df['installation_id'].tolist()))\n    print(f'All test ids (before):\\t{len(ids_list)}')\n    \n    test_df = test_df[test_df['installation_id'].isin(ids_list[:50])]\n    ids_list = list(set(test_df['installation_id'].tolist()))\n    print(f'All test ids (after):\\t{len(ids_list)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nctg_FE_test                    = hst_ctg_FE(test_df)\nlast_FE_test                   = hst_last_FE(test_df)\nassess_FE_test                 = hst_assess_FE(test_df)\nassess_match_last_FE_test      = hst_assess_match_last_FE(test_df)\ndiff_FE_1_test, diff_FE_2_test = hst_diff_FE(test_df)\ntimepast_1_test                = hst_timepast_1(test_df)\nctg_FE_03_test                 = hst_ctg_FE_03(test_df)\nctg_FE_04_test                 = hst_ctg_FE_04(test_df)\n\n##### FE-c05 #####\njson_FE_00_test                = hst_json(test_df, session_wise=True, col_surfix='_00')\njson_FE_01_test                = hst_json(test_df, session_wise=False, col_surfix='_01')\n\n##### FE-c06 #####\njson_ohe_00_test               = hst_json_ohe(test_df, session_wise=True, col_surfix='_00')\njson_ohe_01_test               = hst_json_ohe(test_df, session_wise=False, col_surfix='_01')\n\n##### FE-c07 #####\nctg_FE_sw_test                 = hst_ohe_sw(test_df, col_surfix='_sw_00')\nctg_FE_sw_mean_test            = hst_ohe_sw_mean(test_df, col_surfix='_sw_01')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test = pd.concat([ctg_FE_test.fillna(0), last_FE_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, assess_FE_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, assess_match_last_FE_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, diff_FE_1_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, diff_FE_2_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, timepast_1_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, ctg_FE_03_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, ctg_FE_04_test], axis=1, levels=['installation_id','history_group'])\n\n##### FE-c05 #####\nTest = pd.concat([Test, json_FE_00_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, json_FE_01_test], axis=1, levels=['installation_id','history_group'])\n\n##### FE-c06 #####\nTest = pd.concat([Test, json_ohe_00_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, json_ohe_01_test], axis=1, levels=['installation_id','history_group'])\n\n##### FE-c07 #####\nTest = pd.concat([Test, ctg_FE_sw_test], axis=1, levels=['installation_id','history_group'])\nTest = pd.concat([Test, ctg_FE_sw_mean_test], axis=1, levels=['installation_id','history_group'])\n\n\nTest.sort_index(inplace=True)\n\ncat_list = [c for c in Test.columns if Test[c].dtype.name in ['object','category']]\nfor c in cat_list:\n    Test[c] = Test[c].astype('category')\n\n##### XGB\nTest = xgb_ohe(Test)\n    \nto_pickle(Test, out_dir+'/Test_all.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test = read_pickle('/kaggle/input/dsb2019-ens-01/Test_all.pkl')\n# Test = Test[X.columns]\n# print(f'Test.shape\\t:{Test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Original y:\\t{y.shape}')\n\ny_X = pd.DataFrame(np.zeros(len(X)), columns=['accuracy_group'], index=X.index)\ny_T = pd.DataFrame(np.ones(len(Test)), columns=['accuracy_group'], index=Test.index)\ny = pd.concat([y_X, y_T], axis=0)\ny['sample_weight'] = 1\n\ndisplay(y.head())\nprint(f'Adversarial y:\\t{y.shape}')\n\n\nX = pd.concat([X, Test], axis=0)\n\nfor c in X.columns:\n    if X[c].dtype.name in ['object','category']:\n        X[c] = X[c].astype('category')\n    \nprint(f'Adversarial X:\\t{X.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_pickle(X, '/X.pkl')\nto_pickle(y, '/y.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self, method):\n        self.coef_ = 0\n        self.method = method\n        \n    def _kappa_loss(self, coef, X, y, w=None):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            else:\n                X_p[i] = 3\n\n        ll = metrics.cohen_kappa_score(y, X_p, weights='quadratic', sample_weight=w)\n        return -ll\n\n    def fit(self, X, y, w=None):\n        loss_partial = partial(self._kappa_loss, X=X, y=y, w=w)\n        initial_coef = VALID_TH\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method=self.method)\n        \n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            else:\n                X_p[i] = 3\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@jit\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return 1 - o / e\n\ndef eval_qwk_lgb(y_true, y_pred):\n    \"\"\"\n    Fast kappa eval function for lgb.\n    \"\"\"\n\n    y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n    return 'kappa', qwk(y_true, y_pred), True\n\ndef eval_qwk_lgb_regr(y_true_ori, y_pred_ori, w=None):\n    \"\"\"\n    Fast kappa eval function for lgb.\n    \"\"\"\n    try:\n        y_true = y_pred_ori.get_label()\n        y_pred = y_true_ori.copy()\n    except:\n        y_true = y_true_ori.copy()\n        y_pred = y_pred_ori.copy()\n    \n    y_pred[y_pred <= VALID_TH[0]] = 0\n    y_pred[np.where(np.logical_and(y_pred > VALID_TH[0], y_pred <= VALID_TH[1]))] = 1\n    y_pred[np.where(np.logical_and(y_pred > VALID_TH[1], y_pred <= VALID_TH[2]))] = 2\n    y_pred[y_pred > VALID_TH[2]] = 3\n    \n    return 'kappa', cohen_kappa_score(y_true, y_pred, weights='quadratic', sample_weight=w), True # return 'kappa', qwk(y_true, y_pred), True\n\ndef rmse(preds, data):\n    y_true = data.get_label()\n    metric = (((preds-y_true)**2/len(preds)).sum())**0.5\n    return 'rmse', metric, False\n\ndef lgb_metrics(preds, data):\n    return [\n        eval_qwk_lgb_regr(preds, data),\n        rmse(preds, data),\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class eval_qwk_lgb_regr_weight(object):\n    def __init__(self, trn_w=None, val_w=None):\n        self.trn_w = trn_w\n        self.val_w = val_w\n        \n    def __call__(self, y_true_ori, y_pred_ori):\n        \"\"\"\n        Fast kappa eval function for lgb.\n        \"\"\"\n        try:\n            y_true = y_pred_ori.get_label()\n            y_pred = y_true_ori.copy()\n        except:\n            y_true = y_true_ori.copy()\n            y_pred = y_pred_ori.copy()\n\n#         ##### For log target #####\n#         y_true = np.exp(y_true)-1\n#         y_pred = np.exp(y_pred)-1\n#         ##########################\n            \n        y_pred[y_pred <= VALID_TH[0]] = 0\n        y_pred[np.where(np.logical_and(y_pred > VALID_TH[0], y_pred <= VALID_TH[1]))] = 1\n        y_pred[np.where(np.logical_and(y_pred > VALID_TH[1], y_pred <= VALID_TH[2]))] = 2\n        y_pred[y_pred > VALID_TH[2]] = 3\n        \n        if len(y_true_ori)==len(self.trn_w):\n            w = self.trn_w\n        else:\n            w = self.val_w\n\n        return 'kappa', cohen_kappa_score(y_true, y_pred, weights='quadratic', sample_weight=w), True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_importances(importances_, figsize=(8, 12)):\n    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()\n    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])\n    plt.figure(figsize=figsize)\n    sns.barplot(x='gain', y='feature', data=importances_.sort_values('mean_gain', ascending=False))\n    plt.tight_layout()\n#     plt.savefig('importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_iter_scores(iter_scores, figsize=(8, 12)):\n    plt.figure(figsize=figsize)\n    sns.lineplot(x=\"iteration\", y=\"score\", hue='fold', style=\"trn_val\", data=iter_scores, palette=sns.color_palette(\"muted\")[:FOLDS]);\n    plt.tight_layout()\n    plt.show();\n#     plt.savefig('importances.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_metric(self):\n    \"\"\"\n    Plot training progress.\n    Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n\n    :return:\n    \"\"\"\n    full_evals_results = pd.DataFrame()\n    for model in self.models:\n        evals_result = pd.DataFrame()\n        for k in model.model.evals_result_.keys():\n            evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n        evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n        full_evals_results = full_evals_results.append(evals_result)\n\n    full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n                                                                                        'variable': 'dataset'})\n    sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n    plt.title('Training progress')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class ReduceLearningRateCallback(object):\n#     def __init__(self, monitor_metric, reduce_every=100, ratio=0.5):\n#         self._monitor_metric = monitor_metric\n#         self._reduce_every = reduce_every\n#         self._ratio = ratio\n#         self._best_score = None\n#         self._counter = 0\n\n#     def _is_higher_score(self, metric_score, is_higher_better):\n#         if self._best_score is None:\n#             return True\n\n#         if is_higher_better:\n#             return self._best_score < metric_score\n#         else:\n#             return self._best_score > metric_score\n\n#     def __call__(self, env):\n#         evals = env.evaluation_result_list\n#         lr = env.params['learning_rate']\n#         for tra_val, name, score, is_higher_better in evals:\n# #         for pp in evals:\n# #             print(pp)\n#             # チェックするメトリックを選別する\n#             if (env.iteration<100):\n#                 return \n            \n#             if (name!=self._monitor_metric) | (tra_val!='valid'):\n#                 continue\n#             # 対象のメトリックが見つかっても過去のスコアよりも性能が悪ければ何もしない\n#             if not self._is_higher_score(score, is_higher_better):\n#                 self.counter += 1\n# #                 print(f'\\r{self.counter}', end='', flush=True)\n#                 if self.counter==self._reduce_every:\n#                     new_parameters = {'learning_rate':lr*self._ratio}\n#                     env.model.reset_parameter(new_parameters)\n#                     env.params.update(new_parameters)\n#                     print(f'[{env.iteration}]\\tReduce learning rate {lr} -> {env.params[\"learning_rate\"]}')\n#                     self.counter = 0\n#                     return\n#                 else:\n#                     return\n#             else:\n#                 self._best_score = score\n# #                 print(f'\\r{self._best_score}', end='', flush=True)\n#                 self.counter = 0\n#                 return\n            \n#         raise ValueError('monitoring metric not found')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VALID_TH = [1.1, 1.6, 2.2]\n\ndef train_cv(params, shuffle=False, random_state=0, drop_cols=None, th_optimize=False):\n#     oof_train = pd.Series(np.zeros(len(X), dtype=float), index=X.index, name='accuracy_group')\n    oof_valid = pd.Series(np.zeros(len(X), dtype=float), index=X.index, name='accuracy_group')\n\n    len_df = len(INS_ID_LIST_TRAIN)\n    seq_len_df = np.arange(len_df)\n    len_each = math.ceil(len_df/FOLDS)\n\n    trn_scores = []\n    val_scores = []\n    best_iter = []\n    clfs = []\n    importances = pd.DataFrame()\n    iter_scores = pd.DataFrame()\n\n    for f in range(FOLDS):\n        print(f'{\"*\"*5} Fold {f} {\"*\"*5}')\n        evals_result    = {}\n        counter         = 0\n        params['seed'] += 1\n        \n        print(f'\\r                                                               \\r', end='',flush=True)\n        ### Split ##########\n        s, e = f*(len_each), (f+1)*len_each\n\n        trn_ = ~((seq_len_df>=s)&(seq_len_df<e))\n        val_ =  ((seq_len_df>=s)&(seq_len_df<e))\n\n        trn_x, trn_y = X.loc[INS_ID_LIST_TRAIN[trn_], :], y.loc[INS_ID_LIST_TRAIN[trn_], :]\n        val_x, val_y = X.loc[INS_ID_LIST_TRAIN[val_], :], y.loc[INS_ID_LIST_TRAIN[val_], :]\n\n        if drop_cols!=None:\n            trn_x.drop(columns=drop_cols, inplace=True)\n            val_x.drop(columns=drop_cols, inplace=True)\n\n        if shuffle:\n            trn_y['accuracy_group'] = trn_y.sample(frac=1, random_state=random_state).values\n            val_y['accuracy_group'] = val_y.sample(frac=1, random_state=random_state).values\n        #####################\n        \n#         ##### For log target #####\n#         trn_y[['accuracy_group']] = np.log(trn_y[['accuracy_group']]+1)\n#         val_y[['accuracy_group']] = np.log(val_y[['accuracy_group']]+1)\n#         ##########################\n        \n#         ##### LightGBM dataset #####\n#         dtrain = lgb.Dataset(data=trn_x, label=trn_y[['accuracy_group']], weight=trn_y['sample_weight'], free_raw_data=False, silent=True)\n#         dvalid = lgb.Dataset(data=val_x, label=val_y[['accuracy_group']], weight=val_y['sample_weight'], free_raw_data=False, silent=True, reference=dtrain)\n        ##### XGB dataset\n        def xgb_ohe(df):\n            cols_ohe = [c for c in df.columns if df[c].dtype.name=='category']\n            ce_ohe = ce.OneHotEncoder(cols=cols_ohe, handle_unknown='error', use_cat_names=True)\n            out_df_tmp = ce_ohe.fit_transform(df[cols_ohe])\n            out_df_tmp.sort_index(axis=1, inplace=True)\n            df.drop(columns=cols_ohe, inplace=True)\n            df = pd.concat([df, out_df_tmp], axis=1)\n            return df\n        trn_x = xgb_ohe(trn_x)\n        val_x = xgb_ohe(val_x)\n        dtrain = xgb.DMatrix(data=trn_x, label=trn_y[['accuracy_group']], weight=trn_y['sample_weight'], silent=True)\n        dvalid = xgb.DMatrix(data=val_x, label=val_y[['accuracy_group']], weight=val_y['sample_weight'], silent=True)\n        \n        ##### Custom metric with weight #####\n        eqlrw = eval_qwk_lgb_regr_weight(trn_y['sample_weight'].values, val_y['sample_weight'].values)\n        def lgb_metrics_weight(preds, data):\n            return [eqlrw(preds, data), rmse(preds, data),]\n        #####################################\n\n        print(f'Shape of trn_x:\\t{trn_x.shape}')\n        \n#         reducelr_cb = ReduceLearningRateCallback(monitor_metric='kappa', reduce_every=40, ratio=0.5)\n#         callbacks = [reducelr_cb]\n    \n        params['seed'] +=1\n#         clf = lgb.train(\n#             params                =params,\n#             train_set             =dtrain,\n#             valid_sets            =[dvalid, dtrain],\n#             valid_names           =['valid', 'train'],\n# #             feval                 =lgb_metrics_weight,\n#             init_model            =None,\n#             early_stopping_rounds =params['early_stopping_rounds'],\n#             evals_result          =evals_result,\n#             verbose_eval          =params['verbose'],\n#             keep_training_booster =True,\n# #             callbacks             =callbacks,\n#         )\n        ##### XGB train\n        clf = xgb.train(\n            params                =params,\n            dtrain                =dtrain,\n            num_boost_round       =params['n_estimators'],\n            evals                 =[(dtrain,'train'),(dvalid,'valid')],\n#             feval                 =lgb_metrics_weight,\n            maximize              =True,\n            xgb_model             =None,\n            early_stopping_rounds =params['early_stopping_rounds'],\n            evals_result          =evals_result,\n            verbose_eval          =params['verbose'],\n        )\n#         clf.save_model(out_dir+'/LGB_'+'{:02}_'.format(params['seed_avg_times'])+'{:02}'.format(f)+'.txt')\n        \n        clfs.append(clf)\n        best_iter.append(clf.best_iteration)\n\n#         ##### LGBM\n#         oof_valid_tmp = clf.predict(dvalid.data)\n        ##### XGB\n        oof_valid_tmp = clf.predict(dvalid)\n        \n#         ##### For log target #####\n#         oof_valid_tmp = np.exp(oof_valid_tmp)-1\n#         ##########################\n        \n        oof_valid.loc[INS_ID_LIST_TRAIN[[val_]], :] = oof_valid_tmp\n\n        t = np.array(evals_result['train']['auc'])\n        v = np.array(evals_result['valid']['auc'])\n        trn_scores.append(t[v.argmax()])\n        val_scores.append(v.max())\n\n#         ##### LGBM\n#         imp_df = pd.DataFrame({\n#                 'feature': trn_x.columns,\n#                 'gain': clf.feature_importance(importance_type='gain'),\n#                 'fold': [f] * len(trn_x.columns),\n#                 })\n        ##### XGB\n        imp_dict = clf.get_score(fmap='', importance_type='total_gain')\n        imp_df = pd.DataFrame({\n                'feature': [i[0] for i in imp_dict.items()],\n                'gain': [i[1] for i in imp_dict.items()],\n                'fold': [f] * len(imp_dict),\n                })\n        imp_df = pd.merge(pd.DataFrame({'feature': trn_x.columns.tolist()}), imp_df, how='left', on='feature').fillna(0)\n        \n        importances = pd.concat([importances, imp_df], axis=0, sort=False).reset_index(drop=True)\n\n        iter_scores_tmp = pd.DataFrame()\n        for k in ['valid', 'train']:\n            tmp = pd.DataFrame()\n#             tmp['loss'] = evals_result[k]['rmse']\n            tmp['score'] = evals_result[k]['auc']\n            tmp['trn_val'] = k\n            iter_scores_tmp = pd.concat([iter_scores_tmp, tmp], axis=0)\n        iter_scores_tmp = iter_scores_tmp.reset_index().rename(columns={'index': 'iteration'})\n        iter_scores_tmp['fold'] = f\n        iter_scores_tmp['id'] = 'trn_val_'+str(f)\n        iter_scores = pd.concat([iter_scores, iter_scores_tmp], axis=0).reset_index(drop=True)\n        print(f'\\n')\n\n    print(f'\\n{\"-\"*10}Train{\"-\"*10}')\n    for i in np.arange(len(trn_scores)):\n        print(f'\\tFold {i}:\\t{trn_scores[i]:.5f}')\n    print(f'Average train:\\t{np.average(trn_scores):.5f}')\n    print(f'\\n')\n\n    print(f'\\n{\"-\"*10}Validation{\"-\"*10}')\n    for i in np.arange(len(val_scores)):\n        print(f'\\tFold {i}:\\t{val_scores[i]:.5f}')\n    score_valid = eval_qwk_lgb_regr(y[['accuracy_group']].values, oof_valid.values, y['sample_weight'])[1].item()\n    print(f'Average valid:\\t{np.average(val_scores):.5f}')\n    print(f'Score valid:\\t{score_valid:.5f}')\n#     print(f'Threshold while training:\\t{VALID_TH}')\n\n    optR, oof_opt_valid = _, _\n    if th_optimize:\n#         print(f'\\n{\"-\"*10}Optimized with validation{\"-\"*10}')\n        optR = OptimizedRounder('nelder-mead') # 'Powell'\n        optR.fit(oof_valid.values, y['accuracy_group'].values, y['sample_weight'].values)\n        oof_opt_valid = optR.predict(oof_valid.values, optR.coefficients())\n        score_valid = cohen_kappa_score(y['accuracy_group'], oof_opt_valid, weights='quadratic', sample_weight=y['sample_weight'])\n#         print(f'Optimized threshold:\\t{[round(coef,3) for coef in optR.coefficients().tolist()]}')\n#         print(f'Score valid:\\t\\t{score_valid:.5f}')\n#         to_pickle(optR, 'optR_'+'{:02}_'.format(params['seed_avg_times'])+'{:.5f}'.format(score_valid)+'.pkl')\n\n    print(f'params\\n{params}')\n    return clfs, iter_scores, optR, importances, (trn_scores), (val_scores, score_valid, X.index, oof_valid, oof_opt_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finetuned training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_INS_ID_LIST_TRAIN(df, seed):\n    df = df.reset_index()\n    INS_ID_LIST_TRAIN = df['installation_id'].unique()\n    INS_ID_LIST_TRAIN.sort()\n    random.Random(seed).shuffle(INS_ID_LIST_TRAIN)\n    return INS_ID_LIST_TRAIN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augmentation(_x, _y, _seed):\n    _x.reset_index(inplace=True)\n    _x = _x.sample(frac=1, random_state=_seed)\n    _x.drop_duplicates(subset='installation_id', inplace=True)\n    _x.set_index(['installation_id','history_group'], inplace=True)\n    _y = _y.loc[_x.index, :]\n    return _x, _y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = []\n\n# params = {\n#     'n_estimators': 2000,\n#     'boosting_type': 'gbdt',\n#     'objective': 'binary', # 'regression',\n#     'metric': 'auc', # 'kappa',\n#     'first_metric_only': True,\n#     'subsample': 1.0,\n#     'subsample_freq': 1,\n#     'learning_rate': 0.01,\n#     'feature_fraction': 0.8,\n#     'max_depth': -1,\n#     'lambda_l1': 1,  \n#     'lambda_l2': 5,\n#     'min_data_in_leaf': 15,\n# #     'min_child_weight': 0.1,\n#     'verbose': -1,\n#     'early_stopping_rounds': 100,\n# #     'importance_type': 'gain', \n# #     'eval_metric': 'kappa',\n#     'seed': SEED,\n#     'seed_avg_times':0,\n# }\n##### XGB param #####\nparams = {\n    'n_estimators': 2000,\n    'objective': 'binary:logistic', # 'reg:squarederror', \n    'eval_metric': 'auc', # 'rmse',\n    'learning_rate': 0.01,\n    'max_depth': 10, \n    'subsample': 1.0, \n    'colsample_bytree': 0.8,\n    'alpha': 1, # 'lambda_l1'\n    'lambda': 5, # 'lambda_l2'\n    'verbose': 10000,\n    'early_stopping_rounds': 100,\n#     'eval_metric': ['rmse','kappa'],\n    'seed': SEED, \n    'silent': True,\n    'nthread': 4,\n}\n\nSEED_AVG_TIMES = 5\nDROP_DUP_TIMES = 5\n\nactual_importances = pd.DataFrame()\n\nstart = time.time()\nfor j in range(DROP_DUP_TIMES):\n    for i in range(SEED_AVG_TIMES):\n        ### Augmentation ####\n        X = read_pickle('/X.pkl')\n        y = read_pickle('/y.pkl')\n        \n        X.drop(columns=[c for c in X.columns if c not in [c.replace('[','~').replace(']','|') for c in XGB_FEAT]+['installation_id','history_group']], inplace=True)\n        \n        X, y = augmentation(X, y, SEED+i+j*SEED_AVG_TIMES)\n        #####################\n        \n        print(f'{\"*\"*30}\\n{\"*\"*10} Trial {i} {\"*\"*10}\\n{\"*\"*30}')\n\n        INS_ID_LIST_TRAIN = create_INS_ID_LIST_TRAIN(X, SEED+i+j*SEED_AVG_TIMES)\n        params['seed_avg_times'] = i\n        clfs, iter_scores, optR, importances, trn_tup, val_tup = train_cv(params, shuffle=False, random_state=SEED, drop_cols=drop_cols, th_optimize=True)\n\n        importances['trial'] = i\n        actual_importances = pd.concat([actual_importances, importances], axis=0, sort=False).reset_index(drop=True)\n\n        print(f'{\"*\"*5} Time:\\t{(time.time()-start)/60:.1f} mins {\"*\"*5}')\n        print(f'\\n')\n    \n\nactual_importances.to_csv(out_dir+'/actual_importances.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save_iter_scores(pd.concat(sa_iter_scores, axis=0).groupby(['iteration','trn_val','fold','id']).mean().reset_index(), figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save_importances(importances_=pd.concat(sa_importances, axis=0), figsize=(10,45))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shuffle importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = []\n\n# params = {\n#     'n_estimators': 2000,\n#     'boosting_type': 'gbdt',\n#     'objective': 'binary', # 'regression',\n#     'metric': 'auc', # 'kappa',\n#     'first_metric_only': True,\n#     'subsample': 1.0,\n#     'subsample_freq': 1,\n#     'learning_rate': 0.01,\n#     'feature_fraction': 0.8,\n#     'max_depth': -1,\n#     'lambda_l1': 1,  \n#     'lambda_l2': 5,\n#     'min_data_in_leaf': 15,\n# #     'min_child_weight': 0.1,\n#     'verbose': -1,\n#     'early_stopping_rounds': 100,\n# #     'importance_type': 'gain', \n# #     'eval_metric': 'kappa',\n#     'seed': SEED,\n#     'seed_avg_times':0,\n# }\n##### XGB param #####\nparams = {\n    'n_estimators': 2000,\n    'objective': 'binary:logistic', # 'reg:squarederror', \n    'eval_metric': 'auc', # 'rmse',\n    'learning_rate': 0.01,\n    'max_depth': 10, \n    'subsample': 1.0, \n    'colsample_bytree': 0.8,\n    'alpha': 1, # 'lambda_l1'\n    'lambda': 5, # 'lambda_l2'\n    'verbose': 10000,\n    'early_stopping_rounds': 100,\n#     'eval_metric': ['rmse','kappa'],\n    'seed': SEED, \n    'silent': True,\n    'nthread': 4,\n}\n\nSEED_AVG_TIMES = 5\nDROP_DUP_TIMES = 5\n\nnull_importances = pd.DataFrame()\n\nstart = time.time()\nfor j in range(DROP_DUP_TIMES):\n    for i in range(SEED_AVG_TIMES):\n        ### Augmentation ####\n        X = read_pickle('/X.pkl')\n        y = read_pickle('/y.pkl')\n        \n        X.drop(columns=[c for c in X.columns if c not in [c.replace('[','~').replace(']','|') for c in XGB_FEAT]+['installation_id','history_group']], inplace=True)\n        \n        X, y = augmentation(X, y, SEED+i+j*SEED_AVG_TIMES)\n        #####################\n        \n        print(f'{\"*\"*30}\\n{\"*\"*10} Trial {i} {\"*\"*10}\\n{\"*\"*30}')\n\n        INS_ID_LIST_TRAIN = create_INS_ID_LIST_TRAIN(X, SEED+i+j*SEED_AVG_TIMES)\n        params['seed_avg_times'] = i\n        clfs, iter_scores, optR, importances, trn_tup, val_tup = train_cv(params, shuffle=True, random_state=SEED, drop_cols=drop_cols, th_optimize=True)\n\n        importances['trial'] = i\n        null_importances = pd.concat([null_importances, importances], axis=0, sort=False).reset_index(drop=True)\n\n        print(f'{\"*\"*5} Time:\\t{(time.time()-start)/60:.1f} mins {\"*\"*5}')\n        print(f'\\n')\n    \n\nnull_importances.to_csv(out_dir+'/null_importances.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare Actual vs Null"},{"metadata":{},"cell_type":"markdown","source":"- Calculate $log\\frac{1+Actual}{1+Null}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_scores = []\n\nfor _f in actual_importances['feature'].unique():\n#     f_null_imps_gain =   null_importances.loc[  null_importances['feature']==_f, 'gain'].mean()\n#     f_act_imps_gain  = actual_importances.loc[actual_importances['feature']==_f, 'gain'].mean()\n    f_null_imps_gain =   np.median(null_importances.loc[  null_importances['feature']==_f, 'gain'].values)\n    f_act_imps_gain  = np.median(actual_importances.loc[actual_importances['feature']==_f, 'gain'].values)\n    \n    gain_score = np.log(1 + f_act_imps_gain / (1 + f_null_imps_gain))\n    \n    feature_scores.append((_f, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'gain_score']).sort_values('gain_score', ascending=False)\nscores_df.to_csv(out_dir+'/scores_df.csv', index=False)\n\nfig, ax = plt.subplots(1, 1, figsize=(10,120))\nplt.tight_layout()\nsns.barplot(x='gain_score', y='feature', data=scores_df, ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chk_null_imp(feature, ax):    \n    sns.distplot(  null_importances.loc[  null_importances['feature']==feature, 'gain'], kde=False, norm_hist=True, label='Null importance',   ax=ax, color='blue')\n    for _idx, _val in actual_importances.loc[actual_importances['feature']==feature, 'gain'].iteritems():\n        ax.axvline(x=_val, color='red', alpha=0.25)\n    \n    ax.set_title(feature, fontsize=12)\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordered_features = scores_df['feature'].unique().tolist()\nv, h = 1, 5\n\nfor j in range(math.ceil(len(ordered_features)/h)):\n    s, e = j*h, (j+1)*h\n    fig, ax = plt.subplots(v, h, figsize=(30,2))\n    for i, _f in enumerate(ordered_features[s:e]):\n        chk_null_imp(_f, ax[i])\n#     for k in np.arange(i+1,h):\n#         fig.delaxes(ax[k])\n    plt.show();\n    if j>10:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_df.head(40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chk_distribution(X_train, X_test, chk_fold=0, figsize=(8,5), chk_cols=None):\n    if chk_fold!=None:\n        v = 3\n    else:\n        v = 2\n    h = 1\n    len_df = len(X_train)\n    seq_len_df = np.arange(len_df)\n    len_each = math.ceil(len_df/FOLDS)\n    \n    chk_col = []\n    for i, X in enumerate([X_train, X_test]):\n        dtype_dict = {c:X[c].dtype.name for c in X.columns}\n        col_list = [i for i in dtype_dict.items()]\n        num_list = [i[0] for i in np.array(col_list) if i[1] in ('int64', 'float64')]\n        chk_col.append(set(num_list))\n\n    if len(chk_col[0]-chk_col[1])!=0:\n        raise ValueError(\"Columns are not equal between train and test\")\n        \n    if chk_cols!=None:\n        num_list=chk_cols\n        \n    for col in num_list:\n        fig, ax = plt.subplots(v, h, figsize=figsize, sharex=True, sharey=True)\n        fig.subplots_adjust(wspace=0.5,hspace=0.0)\n\n        max_value = max(X_train[col].max(), X_test[col].max())\n        min_value = min(X_train[col].min(), X_test[col].min())\n\n        bins = np.linspace(min_value, max_value, 101)\n\n        if chk_fold!=None:\n            s, e = chk_fold*(len_each), (chk_fold+1)*len_each\n\n            trn_ = seq_len_df[~((seq_len_df>=s)&(seq_len_df<e))]\n            val_ = seq_len_df[ ((seq_len_df>=s)&(seq_len_df<e))]\n\n            for i, idx in enumerate([INS_ID_LIST_TRAIN[[trn_]], INS_ID_LIST_TRAIN[[val_]], INS_ID_LIST_TEST]):\n                if i in [0,1]:\n                    null_count = X_train.loc[idx, col].isnull().sum()\n                    if i == 0: l = 'Train'\n                    else: l = 'Valid'\n                    sns.distplot(X_train.loc[idx, col].dropna(), bins=bins, kde=False, norm_hist=False, \n                                 label=l+' (NaN: {}, {:.1f}%)'.format(null_count, null_count/len(X_train.loc[idx, col])*100), ax=ax[i], color='blue')\n                if i in [2]:\n                    null_count = X_test.loc[idx, col].isnull().sum()\n                    l = 'Test'\n                    sns.distplot(X_test.loc[idx, col].dropna(), bins=bins, kde=False, norm_hist=False, \n                                 label=l+' (NaN: {}, {:.1f}%)'.format(null_count, null_count/len(X_test.loc[idx, col])*100), ax=ax[i], color='orange')\n\n                ax[i].set_yscale('log')\n                ax[i].legend()\n        else:\n            for i in range(2):\n                if i in [0]:\n                    null_count = X_train.loc[:, col].isnull().sum()\n                    l = 'Train'\n                    sns.distplot(X_train.loc[:, col].dropna(), bins=bins, kde=False, norm_hist=True, \n                                 label=l+' (NaN: {}, {:.1f}%)'.format(null_count, null_count/len(X_train.loc[:, col])*100), ax=ax[i], color='blue')\n                if i in [1]:\n                    null_count = X_test.loc[:, col].isnull().sum()\n                    l = 'Test'\n                    sns.distplot(X_test.loc[:, col].dropna(), bins=bins, kde=False, norm_hist=True, \n                                 label=l+' (NaN: {}, {:.1f}%)'.format(null_count, null_count/len(X_test.loc[:, col])*100), ax=ax[i], color='orange')\n\n                ax[i].set_yscale('log')\n                ax[i].legend()\n\n        ax[0].set_title(col, fontsize=16)\n        print(f'{col}')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_idx = np.array(y[y['accuracy_group']==0].index.tolist())[:,0].tolist()\nT_ids = np.array(y[y['accuracy_group']==1].index.tolist())[:,0].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disp_feats = scores_df.loc[scores_df['gain_score']>=2, 'feature'].tolist()\nchk_distribution(X.loc[X_idx, disp_feats], X.loc[T_ids, disp_feats], chk_fold=None, figsize=(8,5), chk_cols=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}