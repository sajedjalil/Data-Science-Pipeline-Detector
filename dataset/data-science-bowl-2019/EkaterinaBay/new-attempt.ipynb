{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Includes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial Imports\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\nnrows = 50000\ntrain = pd.read_csv('../input/data-science-bowl-2019/train.csv', nrows=nrows)\ntrain_labels = pd.read_csv('../input/data-science-bowl-2019/train_labels.csv')\ntest = pd.read_csv('../input/data-science-bowl-2019/test.csv', nrows=nrows)\nsample_submission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select installation_ids that have taken an assessment\nkeep_id = train[train.type == 'Assessment'][['installation_id']].drop_duplicates()\ntrain = pd.merge(train, keep_id, on='installation_id', how='inner')\ndel keep_id\n\n# convert timestamp to datetime\ntrain['timestamp'] = pd.to_datetime(train['timestamp'])\n\n# filter the train dataset for values whose installation_id appears in train_labels\ntrain = train[train['installation_id'].isin(list(train_labels['installation_id'].unique()))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert timestamp to datetime\ntest['timestamp'] = pd.to_datetime(test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a list with all the unique 'titles' from the train and test set\nunique_train_title = set(train['title'].value_counts().index)\nunique_test_title = set(test['title'].value_counts().index)\ntitle_list = list(unique_train_title.union(unique_test_title))\n\ndel unique_train_title\ndel unique_test_title\n\n# make a list with all the unique 'event_code' from the train and test set\nunique_train_event_code = set(train['event_code'].value_counts().index)\nunique_test_event_code = set(test['event_code'].value_counts().index)\nevent_code_list = list(unique_train_event_code.union(unique_test_event_code))\n\ndel unique_train_event_code\ndel unique_test_event_code\n\n# encode titles\ntitle_map = dict(zip(title_list, np.arange(len(title_list))))\ntitle_labels = dict(zip(np.arange(len(title_list)), title_list))\n\ntrain['title'] = train['title'].map(title_map)\ntest['title'] = test['title'].map(title_map)\n#train_labels['title'] = train_labels['title'].map(title_map)\n\n# write codes which mean win for each title\nwin_code = dict(zip(title_map.values(), (4100*np.ones(len(title_map))).astype('int')))\n\n# then, it set one element, the 'Bird Measurer (Assessment)' as 4110\nwin_code[title_map['Bird Measurer (Assessment)']] = 4110\n\n# function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test with the only one installation_id\n    \n    The test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # new features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in title_list}\n    event_code_count = {eve: 0 for eve in event_code_list}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get type of current session\n        session_type = session['type'].iloc[0]\n        # get type of activity of current session\n        session_title = session['title'].iloc[0]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            #how much time spent on current session\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            #how much time spent on each activity\n            time_spent_each_act[title_labels[session_title]] += time_spent\n        \n        # for each assessment, and only this kind of session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            \n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            \n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            \n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            \n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            \n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            \n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n            \n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get_data function is applyed to each installation_id and added to the new_train list\nnew_train=[]\n\nfor i, (ins_id, user_sample) in enumerate(train.groupby('installation_id', sort=False)):\n    # user_sample is a DataFrame that contains only one installation_id\n    new_train+=get_data(user_sample)\n    \nnew_train = pd.DataFrame(new_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#same for test dataset\nnew_test = []\n\nfor ins_id, user_sample in test.groupby('installation_id', sort=False):\n    new_test.append(get_data(user_sample, test_set=True))\n    \nnew_test = pd.DataFrame(new_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of the features\nfeatures = list(new_train.columns.values)\nfeatures.remove('accuracy_group')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removes accuracy_group from the train data\nX_train = new_train[features]\n# create a variable to contain just the accuracy_group label of the train data\ny_train = new_train['accuracy_group']\n# remove accuracy_group from the test data\nX_test = new_test[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN = KNeighborsClassifier(n_neighbors=7)\n#KNN.fit(X_train, y_train)\n#y_pred = KNN.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_gbc = GradientBoostingClassifier(random_state=42, n_estimators=100)\nclf_gbc.fit(X_train, y_train)\ny_pred = clf_gbc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(sample_submission['installation_id'])\ny_pred = pd.DataFrame({'accuracy_group':y_pred[:]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.join(y_pred)\nprint(submission)\nsubmission['accuracy_group'] = submission['accuracy_group'].astype(int)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}