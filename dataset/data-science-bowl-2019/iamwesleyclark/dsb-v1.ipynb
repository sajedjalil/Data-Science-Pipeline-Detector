{"cells":[{"metadata":{},"cell_type":"markdown","source":"# DSB Team Peggy Peg Notebook\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Get environment ready\n* Import Packages\n* Create data variables\n* Display kaggle directory"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import packages\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nimport shap\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport json\nfrom IPython.display import Image\n\n\n\n# So we are all on the same page, use the walk method to generate files within this kaggle directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n#Create data variables\n\n\n\nsample_submission = pd.read_csv(\"../input/data-science-bowl-2019/sample_submission.csv\")\nspecs = pd.read_csv(\"../input/data-science-bowl-2019/specs.csv\")\ntest = pd.read_csv(\"../input/data-science-bowl-2019/test.csv\")\ntrain = pd.read_csv(\"../input/data-science-bowl-2019/train.csv\")\ntrain_labels = pd.read_csv(\"../input/data-science-bowl-2019/train_labels.csv\")\npd.set_option('display.max_columns', 1000)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List our data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/data-science-bowl-2019')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display our hero, Peggy Peg, and her cat\n\nImage(\"../input/peggypeg/peggypeg.jpeg\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preperation\n* Find installation id's contained in train.csv which are not in train_labels.csv.  Remove these rows(train_labels.csv contains our target variable, accuracy group)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\ndf_test=pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\nspec=pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    #In the Spec file there are 387 unique event_ids but unique info is only 167\n    # Merge redundant event_ids\n    \n    #spec=None\n    spec['info']=spec['info'].str.upper()\n    spec['hashed_info']=spec['info'].transform(hash)\n    spec_unique=pd.DataFrame(spec[['hashed_info']].drop_duplicates())\n    spec_unique['deduped_event_id']=np.arange(len(spec_unique))\n    spec=pd.merge(spec,spec_unique,on='hashed_info',how='left')\n    z=dict(zip(spec.event_id,spec.deduped_event_id))\n    df_train['event_id']=df_train['event_id'].map(z)\n    df_test['event_id']=df_test['event_id'].map(z)\n        #df_train=df_train[df_train['event_id'].isin(df_test['event_id'])]\n    df_train=df_train[df_train['event_id']!=137]  # this particular event id only has 2 records in train and none in test....\n    df_event_id_train=pd.pivot_table(df_train.loc[:,['installation_id','game_session','event_id']],aggfunc=len,columns=['event_id'],index=['installation_id','game_session']).add_prefix('event_id_').rename_axis(None,axis=1).reset_index()\n    df_event_id_test=pd.pivot_table(df_test.loc[:,['installation_id','game_session','event_id']],aggfunc=len,columns=['event_id'],index=['installation_id','game_session']).add_prefix('event_id_').rename_axis(None,axis=1).reset_index()\n    df_event_id_train=df_event_id_train.fillna(0)\n    df_event_id_train=df_event_id_train.fillna(0)\n    df_event_id_test=df_event_id_test.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features, baby"},{"metadata":{},"cell_type":"markdown","source":"# Time, World, Correct/Incorrect, Accuracy\n\n* Weekend\n* Phase of Day\n\n\n* Extract hour_of_day from timestamp and drop timestamp\n* on_hot encoding on event_code.  Group df by installation_id and game_session\n* Define agg dictionary to define the aggregate functions to be performed after grouping df\n* Take first value, as it's unique for installation_id and game_session pair\n* Join this together and return df"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features(df):\n    df['timestamp']=pd.to_datetime(df['timestamp'])\n    df['Incorrect_Game_Attempt']=np.where((df['event_data'].str.contains('\"correct\":false')&(df['type']=='Game')),1,0)\n    df['Correct_Game_Attempt']=np.where((df['event_data'].str.contains('\"correct\":true')&(df['type']=='Game')),1,0)\n    df['Is_Weekend']=np.where((df['timestamp'].dt.day_name()=='Sunday')|(df['timestamp'].dt.day_name()=='Saturday'),1,0)\n    df['Phase_Of_Day']=np.where(df['timestamp'].dt.hour.isin(range(6,12)),'Morning',np.where(df['timestamp'].dt.hour.isin(range(13,19)),'Evening','Night'))\n    df_world=pd.pivot_table(df.loc[df['world']!='NONE',['installation_id','game_session','world']].drop_duplicates(),index=['installation_id','game_session'],columns=['world'],aggfunc=len).add_prefix('rolling_').rename_axis(None, axis=1).reset_index()\n    \n    df_type_world=pd.merge(df_world,pd.pivot_table(df.loc[:,['installation_id','game_session','type']].drop_duplicates(),index=['installation_id','game_session'],columns=['type'],fill_value=0,aggfunc=len).rename_axis(None, axis=1).reset_index(),on=['installation_id','game_session'],how='right')\n    df_type_world_title=pd.merge(df_type_world,pd.pivot_table(df.loc[:,['installation_id','game_session','title']].drop_duplicates(),index=['installation_id','game_session'],columns=['title'],fill_value=0,aggfunc=len).add_prefix('rolling_').rename_axis(None, axis=1).reset_index(),on=['installation_id','game_session'],how='right')\n\n    df_activity_weekend=pd.merge(df_type_world_title,pd.DataFrame(pd.pivot_table(df.loc[:,['installation_id','game_session','Is_Weekend']].drop_duplicates(),index=['installation_id','game_session'],columns=['Is_Weekend'],fill_value=0,aggfunc=len)).add_prefix('Weekend_').rename_axis(None, axis=1).reset_index(),on=['installation_id','game_session'],how='right')\n    df_activity_weekend_phase_of_day=pd.merge(pd.DataFrame(pd.pivot_table(df.loc[:,['installation_id','game_session','Phase_Of_Day']].drop_duplicates(),index=['installation_id','game_session'],columns=['Phase_Of_Day'],fill_value=0,aggfunc=len)).rename_axis(None, axis=1).reset_index(),df_activity_weekend,on=['installation_id','game_session'],how='left')\n    df_train_Assessed=df.copy()\n    df_train_Assessed['Incorrect_Attempt']=np.where((df['event_data'].str.contains('\"correct\":false'))&(((df['title'] != \"Bird Measurer (Assessment)\")&(df['event_code']==4100))|((df['title'] == \"Bird Measurer (Assessment)\")&(df['event_code']==4110))),1,0)\n    df_train_Assessed['Correct_Attempt']=np.where((df['event_data'].str.contains('\"correct\":true'))&(((df['title'] != \"Bird Measurer (Assessment)\")&(df['event_code']==4100))|((df['title'] == \"Bird Measurer (Assessment)\")&(df['event_code']==4110))),1,0)\n    df_train_acc=df_train_Assessed[df_train_Assessed['title'].isin(['Bird Measurer (Assessment)','Mushroom Sorter (Assessment)','Cauldron Filler (Assessment)','Chest Sorter (Assessment)','Cart Balancer (Assessment)'])].groupby(['installation_id','title','game_session'])['Incorrect_Attempt','Correct_Attempt'].sum().rename_axis(None, axis=1).reset_index()\n    df_train_acc['Total_Attempts']=df_train_acc.apply(lambda x: x['Incorrect_Attempt'] + x['Correct_Attempt'], axis=1)\n\n    \n    df_train_acc['accuracy']=np.where(df_train_acc['Total_Attempts']>0,df_train_acc['Correct_Attempt']/ df_train_acc['Total_Attempts'],0)\n    df_train_acc['accuracy_group']=np.where(df_train_acc['accuracy']==1,3,np.where(df_train_acc['accuracy']==.5,2,np.where(df_train_acc['accuracy']==0,0,1)))\n    df_game_attempt=df.groupby(['installation_id','game_session'])['Incorrect_Game_Attempt','Correct_Game_Attempt'].sum().rename_axis(None, axis=1).reset_index()\n\n    df_event_codes=pd.pivot_table(df_train_Assessed.loc[:,['installation_id','game_session','event_code']],index=['installation_id','game_session'],columns=['event_code'],fill_value=0,aggfunc=len).add_prefix('event_code_').rename_axis(None, axis=1).reset_index()\n    df_final=pd.merge(pd.merge(df_train_acc,df_activity_weekend_phase_of_day,on=['installation_id','game_session'],how='right'),df_event_codes,on=['installation_id','game_session'],how='right')\n    df_gametime=df.groupby(['installation_id','game_session'])['game_time','timestamp','event_count'].max().reset_index()\n    df_final=pd.merge(df_final,df_gametime,on=['installation_id','game_session'],how='left')\n    df_final=df_final.fillna(value=0)\n\n    df_final=pd.merge(df_final,df.loc[df['world']!=\"NONE\",['installation_id','game_session','world']].drop_duplicates(),on=['installation_id','game_session'],how='left')\n    df_final=pd.merge(df_final,df_game_attempt,on=['installation_id','game_session'],how='left')\n    df_final=df_final.fillna(value=0)\n    return(df_final)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Event Code 4020 Feature\n\ntrain[(train.type==\"Assessment\") & (train.event_code==4020) ]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specs[specs.event_id==\"5f0eb72c\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This event occurs when the player places a mushroom on one of the three stumps. It contains information about the mushroom that was placed, the correctness of the action, and where the placement occurred. This event is used to calculate accuracy and to diagnose player strategies and understanding."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_4020_acc(df):\n     \n    counter_dict = {'Cauldron Filler (Assessment)_4020_accuracy':0,\n                    'Mushroom Sorter (Assessment)_4020_accuracy':0,\n                    'Bird Measurer (Assessment)_4020_accuracy':0,\n                    'Chest Sorter (Assessment)_4020_accuracy':0 }\n        \n    for e in ['Cauldron Filler (Assessment)','Bird Measurer (Assessment)','Mushroom Sorter (Assessment)','Chest Sorter (Assessment)']:\n        \n        Assess_4020 = df[(df.event_code == 4020) & (df.title==activities_map[e])]   \n        true_attempts_ = Assess_4020['event_data'].str.contains('true').sum()\n        false_attempts_ = Assess_4020['event_data'].str.contains('false').sum()\n\n        measure_assess_accuracy_ = true_attempts_/(true_attempts_+false_attempts_) if (true_attempts_+false_attempts_) != 0 else 0\n        counter_dict[e+\"_4020_accuracy\"] += (counter_dict[e+\"_4020_accuracy\"] + measure_assess_accuracy_) / 2.0\n    \n    return counter_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Event_code 4025 Feature at Cauldron Filler Assesment\n\ntrain[ (train.event_code==4025) & (train.title == 'Cauldron Filler (Assessment)')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specs[specs.event_id==\"91561152\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This event occurs when the player clicks on a filled bucket when prompted to select a bucket. It contains information about the bucket that was clicked and the correctness of the action. This event is used to calculate accuracy and to diagnose player strategies and understanding."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_accuracy(session):\n    Assess_4025 = session[(session.event_code == 4025) & (session.title=='Cauldron Filler (Assessment)')]   \n    true_attempts_ = Assess_4025['event_data'].str.contains('true').sum()\n    false_attempts_ = Assess_4025['event_data'].str.contains('false').sum()\n\n    accuracy_ = true_attempts_/(true_attempts_+false_attempts_) if (true_attempts_+false_attempts_) != 0 else 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}