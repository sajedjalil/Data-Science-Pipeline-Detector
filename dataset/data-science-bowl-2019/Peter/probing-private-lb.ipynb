{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Probe the private dataset\nBecause of the way how Kaggle's evaluation system works (in case of kernels only competitions), we can find valuable information about the private test set. In this notebook, I'll try to show you a method of how you can do it.\n\nOriginally this was [Chris Deotte's idea](https://www.kaggle.com/cdeotte/private-lb-probing-0-950)."},{"metadata":{},"cell_type":"markdown","source":"Before coding, here is some useful information.\n\n## Code Competitions\n> Some competitions are code competitions. In these competitions all submissions are made from inside of a Kaggle Notebook, and it is not possible to upload submissions to the Competition directly.\n>\n> Following the competition deadline, your code will be rerun by Kaggle on a private test set that is not provided to you. Your model's score against this private test set will determine your ranking on the private leaderboard and final standing in the competition.\n\n### Synchronous Code Competitions\n> When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time.\n>\n> In a synchronous Kernels-only competition, the files you can observe and download will be different than the private test set and sample submission. The files may have different ids, may be a different size, and may vary in other ways, depending on the problem. You should structure your code so that it predicts on the public test.csv in the format specified by the public sample_submission.csv, but does not hard code aspects like the id or number of rows. When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones.\n\n![PublicPrivate](https://storage.googleapis.com/kaggle-media/competitions/general/public_vs_private.png)\n\n#### Refrences:\n- [Code Competition](https://www.kaggle.com/docs/competitions#kernels-only-competitions)\n- [Code Competition FAQ](https://www.kaggle.com/docs/competitions#kernels-only-FAQ)\n- [Instant Gratification Competition](https://www.kaggle.com/c/instant-gratification/overview/description)\n- [Instant Gratification Data description](https://www.kaggle.com/c/instant-gratification/data)"},{"metadata":{},"cell_type":"markdown","source":"## Clarifications\n\n> When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set\n\nThere is only one run. The dataset (usually `test.csv`, `sample_submission.csv`) contains **both** the private and the public part of the test set.\n\n> When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones.\n\nBy \"private\" they mean **both** public and private\n\n\n### Public run\n(left side of the image above)\nKaggle executes a public run when you are commiting your notebook.\n- **Test data**: public (blue)\n- **Generated output**: submission.csv\n- **Leaderboard**: none\n \n\n### Private run\n(right side of the image above)\nKaggle executes a private rerun when you are submitting one of the outputs of your notebook for evaluation.\n- **Test data**: public + private (blue + red)\n- **Generated output**: None (erased after evaluation)\n- **Leaderboard**: public and private (calculated on the 'blue' and 'red' part of the test set)"},{"metadata":{},"cell_type":"markdown","source":"## Probing private dataset\nBecause Kaggle calculates the public LB score based on the private run, we can exploit it by report private dataset findings via our public score."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Earlier submissions, with known public scores.\n# Link to the dataset: https://www.kaggle.com/dataset/5f77b2e0af4234095883bd1cc65c0e20b8a74b44cc10048f14e9d654132114b0\nSUBMISSION_FILES = [\n    'submission-0.csv',  # 0.401\n    'submission-1.csv',  # 0.344\n    'submission-2.csv',  # 0.444\n    'submission-3.csv',  # 0.457\n    'submission-4.csv',  # 0.483\n    'submission-5.csv',  # 0.479\n    'submission-6.csv',  # 0.480\n    'submission-7.csv',  # 0.507\n    'submission-8.csv',  # 0.500\n    'submission-9.csv',  # 0.017\n]\n\n# Note: Every submission has a different score.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How many samples we have in the test set?"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = labels.shape[0]\n\n# We know that the public test set is ~14% of the data, so the full dataset is ~7143 samples\n# It can be less or more than 7143, so we have to probe some values.\n\n# Let's check the possible values between 7100 and 7200\nif target < 7100:\n    submission_idx = 0\nelif target <= 7100 and target < 7110:\n    submission_idx = 1\nelif target <= 7110 and target < 7120:\n    submission_idx = 2\nelif target <= 7120 and target < 7130:\n    submission_idx = 3\nelif target <= 7130 and target < 7140:\n    submission_idx = 4\nelif target <= 7140 and target < 7150:\n    submission_idx = 5\nelif target <= 7150 and target < 7160:\n    submission_idx = 6\nelif target <= 7160 and target < 7170:\n    submission_idx = 7\nelif target <= 7170 and target < 7180:\n    submission_idx = 8\nelif target <= 7180 and target < 7190:\n    submission_idx = 9\nelse:\n    submission_idx = -1 # 0.0 score\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if submission_idx >= 0:\n    CSV_FILE = '../input/2019-dsb-private-probing/{}'.format(SUBMISSION_FILES[submission_idx])\nelse:\n    CSV_FILE = '../input/data-science-bowl-2019/sample_submission.csv'\n\n# Public (or private) test sample_submission file.\nsubmission = pd.read_csv('../input/data-science-bowl-2019/sample_submission.csv')\n\n# Your predictions\ndf_predict = pd.read_csv(CSV_FILE)\n\n# Defaults\nsubmission['accuracy_group'] = 3\n\nfor i, row in df_predict.iterrows():\n    submission.loc[submission['installation_id'] == row['installation_id'], 'accuracy_group'] = row['accuracy_group']\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThe public score of the above is **0.444**, which was the score of the `submission-2.csv`.\nAnd `submission_idx=2` was used in the `target <= 7110 and target < 7120` if statement.\n\nNow, we know that the number of samples in the private run is between 7110 and 7120.\n\n\n**Notes**\n- The private-run contains both the public and the private test set. So the actual number of private samples is 1000 less.\n- You can refine the values with a 2nd run (target between 7110 and 7120)\n- You don't have to, I've already did, it is 7112.\n\n\nWith this technique, you can find out some interesting facts about the private dataset."},{"metadata":{},"cell_type":"markdown","source":"-------------------"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Thanks for reading, please vote if you find this notebook useful.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}