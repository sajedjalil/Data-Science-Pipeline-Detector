{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats as stats\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import IPython\n\ndef display(*dfs):\n    for df in dfs:\n        IPython.display.display(df)\n\nimport cufflinks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time df = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv', engine='c')\nlabels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = reduce_mem_usage(df)\nlabels = reduce_mem_usage(labels)\ndf_test = reduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Number of unique users","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = df.installation_id.unique()\ntest_id = df_test.installation_id.unique()\n\nprint('# of unique ids in train:', train_id.shape[0])\nprint('# of unique ids in test:', test_id.shape[0])\nprint('# of unique ids from test set in train set:', np.isin(test_id, train_id).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timestamp","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"time = df.timestamp.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['timestamp'] = pd.to_datetime(df['timestamp'])\ndf_test['timestamp'] = pd.to_datetime(df_test['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.timestamp.min(), df_test.timestamp.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.timestamp.max(), df_test.timestamp.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.timestamp.max() - df.timestamp.min(), df_test.timestamp.max() - df_test.timestamp.min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same time period for train and test","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Installation_id vs game_session\nCheck that game_sessions are not repeated for different users","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.game_session.nunique() == \\\ndf.drop_duplicates(['game_session', 'installation_id']).game_session.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.game_session.nunique() == \\\ndf_test.drop_duplicates(['game_session', 'installation_id']).game_session.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Number of user with attempts\nHow many installation_id in test set have assessment and assessment with finished code?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def event_finished(df):\n    df_wt_BM = df[(df.event_code == 4100) & (df.title.str.find('Bird Measurer')==-1)]\n    df_BM = df[(df.event_code == 4110) & (df.title.str.find('Bird Measurer')!=-1)]\n    return df_wt_BM.append(df_BM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test\nassessment = df_test[df_test.type=='Assessment']\nassessment_finished = event_finished(assessment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('# ids in TEST\\t # ids in assessment\\t # ids in finished assessments')\nprint(test_id.shape,'\\t', assessment.installation_id.unique().shape, '\\t\\t',\n      assessment_finished.installation_id.unique().shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train\nassessment = df[df.type=='Assessment']\nassessment_finished = event_finished(assessment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('# ids in TRAIN\\t # ids in assessment\\t # ids in finished assessments')\nprint(train_id.shape,'\\t', assessment.installation_id.unique().shape, '\\t\\t',\n      assessment_finished.installation_id.unique().shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For test set - each id took assessments, whereas a lot of ids in train set never took assesments (only 25% did it) and  only around 21% of ids finished assessments (correct or not). We can train model on these 21% ids.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will look only on users which have info about attempts\ndf = df[df.installation_id.isin(labels.installation_id)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test overview\nWe need to predict accuracy for each installation id. Look at last sample for all installation_ids in test and train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_last = df_test.groupby('installation_id').last()\nprint(test_last.event_count.nunique(), test_last.event_code.nunique(), test_last.type.nunique())\ntest_last","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks that for each installation id last sample is 1 line in game session which just started (event_code=200) and we want to predict which accuracy user can achieve.\n\nLet's look on train set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_last = df.groupby('installation_id').last()\nprint(train_last.event_count.nunique(), train_last.event_code.nunique(), train_last.type.nunique())\ntrain_last","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For train set there are more information and after last asssessment. After it user can do some Activity or see Clip etc.\n\n**Note:** One can create labels and after delete these information to make the same train and test sets\n\nFor current analysis we don't need information after last attempt. We can't use information from fufure and this info cant influence on last attempt. So, we delete all info after last attempt include other assessments which were started but attempt wasn't started. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_attempt(df):\n    ''' indicate each line in assessment are finished; don't care it's correct ot not '''\n    \n    df['attempt'] = ((df.type == 'Assessment') &\n                       (((df.event_code == 4100) & (df.title != 'Bird Measurer (Assessment)')) |\n                        ((df.event_code == 4110)&(df.title == 'Bird Measurer (Assessment)')) )\n                   ).astype('int8')\n\ndef check_attempt_pre_assessment(df):\n    ''' indicate which assessments have finished code;\n    don't care it's correct ot not, don't care was 1 attempt or more\n    \n    !!! Need info about attempts - use check_attempt(df) before '''\n    \n    df['got_attempt'] =(df.groupby('game_session').attempt.transform('sum') >0).astype('int8')\n\ndef check_correct(df):\n    ''' indicate that attempt was correct'''\n    \n    df['correct'] = 0\n    df.loc[df.attempt == 1, 'correct'] = df[df.attempt == 1].event_data.str.contains('\"correct\":true')\\\n                                                                             .astype('int8')\n    \n\ndef check_start_assessment(df):\n    ''' indicate when each assessment is started'''\n    df['assessment_start'] = 0\n    df.loc[(df.type == 'Assessment') & (df.event_count == 1), 'assessment_start'] = 1\n    assert df.assessment_start.sum(), df[df.type == 'Assessment'].game_session.nunique()\n    \n    \ndef get_last_assessment(df, test=False):\n    if test:\n        groups = df.groupby('installation_id')\n    else:\n        groups = df[df.attempt == 1].groupby('installation_id')\n    \n    df = df.merge(groups.title.last().rename('last_title'), on='installation_id', how='outer')\n    df = df.merge(groups.game_session.last().rename('last_game_session'), on='installation_id', how='outer')\n    df = df.merge(groups.timestamp.last().rename('last_timestamp'), on='installation_id', how='outer')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncheck_start_assessment(df_test)\ncheck_attempt(df_test)\ncheck_attempt_pre_assessment(df_test)\ncheck_correct(df_test)\n\ncheck_start_assessment(df)\ncheck_attempt(df)\ncheck_attempt_pre_assessment(df)\ncheck_correct(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check that at each game session can be only 1 correct attempt\nassert (df.groupby('game_session').correct.sum() > 1).sum() == 0\nassert (df_test.groupby('game_session').correct.sum() > 1).sum() == 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ini = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df_ini.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.shape)\ndf = get_last_assessment(df)\ndf_test = get_last_assessment(df_test, test=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['to_cut'] = 0\ndf.loc[(df.timestamp >= df.last_timestamp) & (df.attempt != 1), 'to_cut'] = 1\ndf = df[df.to_cut == 0]\nassert df.to_cut.sum() == 0\ndf = df.drop('to_cut', axis=1)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loo at one user from test:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"user_id = '01242218'\n\nuser = df_test[df_test.installation_id == user_id].copy()\nuser[user.title == user.last_title]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look on data: we can see that \n* user start first assessment (title='Cart Balancer (Assessment)', event_count=1), \n* finished it (event_code=4100, event_count=9), \n* still play in this assessment (game_session is the same, event_counts=10-13)\n\n* and last line, other game_session with assessment (the same title='Cart Balancer (Assessment)', event_count=1). For this game session need we to predict acuuracy group\n\nSo, information that user already passed/tried to pass/started last assessment can influence on result\n\n**Note:** store info about last_assessment type","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Attempt and correct attempt counter\nLet's look on test set to see, how many attempts (correct or not) already have user up to moment last attempt. \nWhy? Logically, one can  assume, that previous attempts (as correct well as not) may improve user's knowledge and increase chance to pass attemp. Let's check it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_group = df_test.groupby('installation_id')\ntrain_group = df.groupby('installation_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2)\nfig.add_trace(go.Histogram(x=test_group.attempt.sum(), name='test'), row=1,col=1)\nfig.add_trace(go.Histogram(x=train_group.attempt.sum(), name='train'), row=1, col=2)\nfig.update_xaxes(title_text='number of attempt',row=1, col=1)\nfig.update_xaxes(title_text='number of attempt',row=1,col=2)\nfig.update_layout(title='Histogram of number attempt per user',\n                  font=dict(family=\"Courier New, monospace\",\n                           size=18,color=\"#7f7f7f\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'Mode:', test_group.attempt.sum().mode()[0],  train_group.attempt.sum().mode()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Prevail number of users in test set get 0 attempt before last one. It means that we have to good predict accuracy for users with no history of attempts.\n* Prevail number of users in train set get 1 attemp before last one. Number of users with 0 attemps is large too, but we can use history of attempts as train set too.\n\n**Note:** user attempt history of users as train set to increase information about users with 0 attemps\n\n\n**Note2:** One can use accuracy_group histry for current user as new features. Looks closer on test histigram to choose how many previous accuracy group one can use for 1 user\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = test_group.attempt.sum().astype('int')\ntemp = (temp.value_counts().sort_index()/temp.shape[0]*100).head(15)\nprint('will be with nan (%)')\ntemp.cumsum().shift()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the same analysis for number correct attempt:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2)\nfig.add_trace(go.Histogram(x=test_group.correct.sum(), name='test'), row=1,col=1)\nfig.add_trace(go.Histogram(x=train_group.correct.sum(), name='train'), row=1, col=2)\n\nfig.update_xaxes(title_text='number of correct attempt',row=1, col=1)\nfig.update_xaxes(title_text='number of correct attempt',row=1,col=2)\nfig.update_layout(title='Histogram of correct attempt number per user',\n                  font=dict(family=\"Courier New, monospace\",\n                           size=18,color=\"#7f7f7f\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'Mode:', test_group.correct.sum().mode()[0],  train_group.correct.sum().mode()[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The same situation with number od correct attempts - in train set ahve not enought data for 0 correct attempts.\n\n# Repeting last assessmet before\n\nOne interesting fact that most of users have more than 0 correct attempts. It can be correct assessment differ from last one but also can be the same. User can repeat assessment or as described in info competition it can be other user from the same device.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_last_assessment_repeat(df):\n    '''df - DataFrmae with info about lase assessment: last_title and last_game_session'''\n    \n    temp = df[df.type == 'Assessment']\n    grouped = temp.groupby(['installation_id', 'game_session',  'title',\n                           'last_title', 'last_game_session'], as_index=False)\\\n                                        [['assessment_start', 'got_attempt','correct']].sum()\n    grouped.loc[grouped.got_attempt > 0, 'got_attempt'] = 1\n\n    repeated = grouped[(grouped.title == grouped.last_title) & (grouped.game_session != grouped.last_game_session)]\n    results = repeated.pivot_table(index=['installation_id', 'last_game_session'], columns='title', \n                                values=['assessment_start', 'got_attempt','correct'], \n                                aggfunc=np.sum, fill_value=0)\n\n    totals = grouped.groupby('title')[['assessment_start', 'got_attempt', 'correct']].sum()\n    \n    assert (totals.sum().values == \\\n           np.array([df.assessment_start.sum(), \n                     df.groupby('game_session').got_attempt.last().sum(), \n                     df.correct.sum()])).all()\n    return results, totals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_repeat_test, total_assessment_test = count_last_assessment_repeat(df_test) \ntotal_repeat_train, total_assessment_train = count_last_assessment_repeat(df) \n\nlst_assessment_repeat_test = (total_repeat_test != 0).sum().unstack(level=0)[['assessment_start', 'got_attempt', 'correct']]\nlst_assessment_repeat_train = (total_repeat_train != 0).sum().unstack(level=0)[['assessment_start', 'got_attempt', 'correct']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(lst_assessment_repeat_test, total_assessment_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_annotation(ax):\n    for p in ax.patches:\n        ax.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width()/2, p.get_height()),\n               ha='center', va='center', xytext=(0,5), textcoords='offset points')\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax = plt.subplot(121)\nsns.barplot(y=lst_assessment_repeat_test.sum() / total_assessment_test.sum() * 100,\n            x=lst_assessment_repeat_test.columns, ax=ax);\nax.set(ylabel='%')\nax.set_title('Test\\nRelation number of repeated assessments to its initial number, %')\nax = set_annotation(ax)    \n\nax = plt.subplot(122)\nsns.barplot(y=lst_assessment_repeat_train.sum() / total_assessment_train.sum() * 100,\n            x=lst_assessment_repeat_train.columns, ax=ax);\nax.set(ylabel='%')\nax.set_title('Train\\nRelation number of repeated assessments to its initial number, %')\nax = set_annotation(ax)    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy \ndef proportions_diff_confint_ind(*data, sample=None, proportion=None, alpha = 0.05):\n    '''data - list of data; \n            if sample=True, data = [sample1, sample],\n            if proportion=True, data[p1, p2, n1, n2] '''\n    \n    if proportion: p1, p2, n1, n2 = data\n    if sample: \n        sample1, sample2 = data\n        n1 = len(sample1)\n        n2 = len(sample2)\n        p1 = float(sum(sample1)) / n1\n        p2 = float(sum(sample2)) / n2\n    if sample is None and proportion is None:\n        print('Choose sample or proportion')\n        return None\n    \n    z = scipy.stats.norm.ppf(1 - alpha / 2.)\n    left_boundary = (p1 - p2) - z * np.sqrt(p1 * (1 - p1)/n1 + p2 * (1 - p2)/ n2)\n    right_boundary = (p1 - p2) + z * np.sqrt(p1 * (1 - p1)/ n1 + p2 * (1 - p2)/ n1)\n    \n    return (left_boundary, right_boundary)\n\n\ndef proportions_diff_z_stat_ind(*data, sample=None, proportion=None):\n    '''data - list of data; \n            if sample=True, data = [sample1, sample],\n            if proportion=True, data[p1, p2, n1, n2]  '''\n    \n    if proportion: p1, p2, n1, n2 = data\n    if sample: \n        sample1, sample2 = data\n        n1 = len(sample1)\n        n2 = len(sample2)\n        p1 = float(sum(sample1)) / n1\n        p2 = float(sum(sample2)) / n2\n    if sample is None and proportion is None:\n        print('Choose sample or proportion')\n        return None\n    \n    P = float(p1*n1 + p2*n2) / (n1 + n2)\n    return (p1 - p2) / np.sqrt(P * (1 - P) * (1. / n1 + 1. / n2))\n\n\ndef proportions_diff_z_test(z_stat, alternative = 'two-sided'):\n    if alternative not in ('two-sided', 'less', 'greater'):\n        raise ValueError(\"alternative not recognized\\n\"\n                         \"should be 'two-sided', 'less' or 'greater'\")\n    \n    if alternative == 'two-sided':\n        return 2 * (1 - scipy.stats.norm.cdf(np.abs(z_stat)))\n    \n    if alternative == 'less':\n        return scipy.stats.norm.cdf(z_stat)\n\n    if alternative == 'greater':\n        return 1 - scipy.stats.norm.cdf(z_stat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for test,total_test, train,total_train, idx in zip(lst_assessment_repeat_test.sum(), \n                                                   total_assessment_test.sum(),\n                                                   lst_assessment_repeat_train.sum(), \n                                                   total_assessment_train.sum(),\n                                                   total_assessment_test.columns):\n    print(idx)\n    print('ci p_test-p_train:', \n          proportions_diff_confint_ind(test/total_test, train/total_train, total_test, total_train,\n                                       proportion=True))\n    statistics = proportions_diff_z_stat_ind(test/total_test, train/total_train, \n                                             total_test, total_train,\n                                             proportion=True)\n    print('p_value:', proportions_diff_z_test(statistics), '\\n')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numbers repeated assessments without attempts, with attempts and correct attempts for test are significant more than the same numbers for train test (*p_val* < *alpha*=0.05, *H0 rejected:* *p1*=*p2*, **H1 accepted**: *p1* <!= > *p2*). It means that in test test generally more users which early tried to pass last attempt","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,12))\n\ntemp = (lst_assessment_repeat_test/total_assessment_test *100).unstack().reset_index()\\\n                                        .rename(columns={'level_0': 'action',\n                                                        0: 'count'})\nax = plt.subplot(211)\nsns.barplot(data=temp, x='action', y='count', hue='title', ax=ax);\nax.set(ylabel='%')\nax.set_title('Test\\nRelation number of repeated assessments to its initial number, %')\nax = set_annotation(ax)\n\ntemp = (lst_assessment_repeat_train/total_assessment_train *100).unstack().reset_index()\\\n                                        .rename(columns={'level_0': 'action',\n                                                        0: 'count'})\nax = plt.subplot(212)\nsns.barplot(data=temp, x='action', y='count', hue='title', ax=ax);\nax.set(ylabel='%')\nax.set_title('Train\\nRelation number of repeated assessments to its initial number, %')\nax = set_annotation(ax)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check influence on accuracy that user already tried to take/ to pass/ or already correcly finishid last assessment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_repeat_train = total_repeat_train.reset_index()\\\n                                       .rename(columns={'last_game_session': 'game_session'})\n\nassert total_repeat_train.game_session.nunique() == total_repeat_train.shape[0]\nassert labels.game_session.nunique() == labels.shape[0]\n\n# add accuracy_group to repeating info\nprint(total_repeat_train.shape)\ntotal_repeat_train = total_repeat_train.merge(labels[['game_session', 'accuracy_group']], on='game_session', how='left')\ntotal_repeat_train = total_repeat_train.rename(columns={'accuracy_group': ('accuracy_group', '')})\\\n                                       .drop('game_session',axis=1)\ntotal_repeat_train.columns = pd.MultiIndex.from_tuples(total_repeat_train.columns)\nassert total_repeat_train.accuracy_group.isna().sum() == 0\ntotal_repeat_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum along title assessments\ntemp = total_repeat_train.sum(axis=1, level=0)\n\n#plot\n_, axes = plt.subplots(5, 3, gridspec_kw={'height_ratios': [4, 1,1,1,1]}, figsize=(15,10))\naxes = axes.flatten()\nfor i, action in enumerate(['assessment_start', 'got_attempt', 'correct']):\n#     ax = plt.subplot(2,3,i+1)\n    sns.barplot(x='accuracy_group', y=action, data=temp, ax=axes[i])\n    \n    colors=sns.color_palette()\n    for acc in range(0,4):\n        sns.distplot(temp.loc[temp.accuracy_group==acc, action], kde=False, \n                     color=colors[acc],\n                     label='acc_group'+str(acc), ax=axes[i+3+acc*3])\n        axes[i+3+acc*3].legend()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Mean number of repeated last assessment (fig1) and mean number of repeted attempts (as correct well as not) (fig2) don't influence on accuracy group. \n* Whereas if user didn't pass attempt correct previously (fig3) than probably user get low accuracy. Mean number of repeating correct attemts is lower significant lower for accuracy_group=0.\n\nCheck it by statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc0 = temp.loc[temp.accuracy_group == 0]\nacc1 = temp.loc[temp.accuracy_group == 1]#, 'attempt_start']\nacc2 = temp.loc[temp.accuracy_group == 2]#, 'attempt_start']\nacc3 = temp.loc[temp.accuracy_group == 3]#, 'attempt_start']\n\nfor action in ['assessment_start', 'got_attempt', 'correct']:\n    p = scipy.stats.f_oneway(acc0[action], acc1[action],\n                               acc2[action],acc3[action])[1]\n    reject = '' if p > 0.05 else '!!reject'\n    print(f'{action}  p-value: {p} {reject}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.multicomp import pairwise_tukeyhsd\npairwise_tukeyhsd(temp.correct,temp.accuracy_group).plot_simultaneous();#.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check unique titles in different worlds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = df_test[df_test.type=='Assessment'].groupby('world').title.unique()\nprint('Test')\nfor i in titles.index:\n    print(i, temp[i])\n    \ntitles = df[df.type=='Assessment'].groupby('world').title.unique()\nprint('Train')\nfor i in titles.index:\n    print(i, temp[i])\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}