{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nMost competitors optimize the kappa by optimizing the MSE first then optimize the thresholds. \n\nHowever, it's still better to optimize the Kappa metric directly. I have done a lot survey. And thanks for this paper: https://www.sciencedirect.com/science/article/abs/pii/S0167865517301666\n\nI implemented the Kappa loss proposed in the paper. From the training logs you can see the loss descresing is almost consistent with the Kappa metric increasing, so I think it's quite an effective Soft Kappa Loss. The CV & LB scores are not good enough, I think it's mostly due to the features, Model and optimizer that I'm current using.\n\nIf you are also interested in optimize Kappa metric directly, please vote this notebook in order more competitors can join this topic."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## Kappa Loss and Kappa Metric"},{"metadata":{},"cell_type":"markdown","source":"### Kappa Loss\n\nAccording to \"Weighted kappa loss function for multi-class classification of ordinal data in deep learning\", the optimization problem can be formulated as:\n$$\n\\text { maximize } \\quad \\kappa=1-\\frac{\\sum_{i, j} \\omega_{i, j} O_{i, j}}{\\sum_{i, j} \\omega_{i, j} E_{i, j}}, \\quad \\text { where } \\quad \\kappa \\in[-1,1]\n$$\n\nThey propose to take the logarithm of the index, in order to increase the penalization of incorrect assignments.\n$$\n\\text { minimize } \\quad \\mathscr{L}=\\log (1-\\kappa) \\quad \\text { where } \\quad \\mathscr{L} \\in(-\\infty, \\log 2]\n$$\n\nIn neural networks for multi-class classiï¬cation the model constructed does not give a unique predicted class as output, but a probability distribution over the set of possible classes. Consequently, we need to rewrite $\\kappa$ in terms of probability distributions. Having $\\kappa=1-\\mathscr{N} / \\mathscr{D}$, the experession of numerator $\\mathscr{N}$ in terms of the probilities of the prediction and the denominator $\\mathscr{D}$ which takes account the probilities given by model are defined as following:\n\n$$\n\\mathscr{N}=\\sum_{i, j} \\omega_{i, j} O_{i, j}=\\sum_{k=1}^{N} \\sum_{c=1}^{C} \\omega_{t_{k}, c} P_{c}\\left(X_{k}\\right)\n$$\n\n$$\n\\mathscr{D}=\\sum_{i, j} \\omega_{i, j} E_{i, j}=\\sum_{i=1}^{C} \\hat{N}_{i} \\sum_{j=1}^{C}\\left(\\omega_{i, j} \\sum_{k=1}^{N} P_{j}\\left(X_{k}\\right)\\right)\n$$\n\nwhere\n\n$X_{k}$: input data of the $k$th sample, $E_{i, j}=\\frac{N_{i} \\sum_{k=1}^{N} P_{j}\\left(X_{k}\\right)}{N}= \\hat{N}_{i} \\sum_{k=1}^{N} P_{j}\\left(X_{k}\\right)$\n\n$N$: number of samples\n\n$N_i$: number of samples of the $i$th class $\\hat{N}_{i}=\\frac{N_{i}}{N}$\n\n$t_k$: correct class number for sample $k$\n\n$P_{c}\\left(X_{k}\\right)$: conditional probability that the $k$th sample belongs to class $c$ given that the true class is $t_k$"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.metrics import Metric\nimport tensorflow.keras.backend as K\nRANDOM_SEED = 3\ntf.random.set_seed(RANDOM_SEED)\ntf.keras.backend.set_floatx('float64')\n\n@tf.function\ndef cohen_kappa_loss(y_true, y_pred, row_label_vec, col_label_vec, weight_mat,  eps=1e-6, dtype=tf.float64):\n    labels = tf.matmul(y_true, col_label_vec)\n    weight = tf.pow(tf.tile(labels, [1, tf.shape(y_true)[1]]) - tf.tile(row_label_vec, [tf.shape(y_true)[0], 1]), 2)\n    weight /= tf.cast(tf.pow(tf.shape(y_true)[1] - 1, 2), dtype=dtype)\n    numerator = tf.reduce_sum(weight * y_pred)\n    \n    denominator = tf.reduce_sum(\n        tf.matmul(\n            tf.reduce_sum(y_true, axis=0, keepdims=True),\n            tf.matmul(weight_mat, tf.transpose(tf.reduce_sum(y_pred, axis=0, keepdims=True)))\n        )\n    )\n    \n    denominator /= tf.cast(tf.shape(y_true)[0], dtype=dtype)\n    \n    return tf.math.log(numerator / denominator + eps)\n\nclass CohenKappaLoss(tf.keras.losses.Loss):\n    def __init__(self,\n                 num_classes,\n                 name='cohen_kappa_loss',\n                 eps=1e-6,\n                 dtype=tf.float64):\n        super(CohenKappaLoss, self).__init__(name=name, reduction=tf.keras.losses.Reduction.NONE)\n        \n        self.num_classes = num_classes\n        self.eps = eps\n        self.dtype = dtype\n        label_vec = tf.range(num_classes, dtype=dtype)\n        self.row_label_vec = tf.reshape(label_vec, [1, num_classes])\n        self.col_label_vec = tf.reshape(label_vec, [num_classes, 1])\n        self.weight_mat = tf.pow(\n            tf.tile(self.col_label_vec, [1, num_classes]) - tf.tile(self.row_label_vec, [num_classes, 1]),\n        2) / tf.cast(tf.pow(num_classes - 1, 2), dtype=dtype)\n\n\n    def call(self, y_true, y_pred, sample_weight=None):\n        return cohen_kappa_loss(\n            y_true, y_pred, self.row_label_vec, self.col_label_vec, self.weight_mat, self.eps, self.dtype\n        )\n\n\n    def get_config(self):\n        config = {\n            \"num_classes\": self.num_classes,\n            \"eps\": self.eps,\n            \"dtype\": self.dtype\n        }\n        base_config = super(CohenKappaLoss, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Kappa Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CohenKappa(Metric):\n    \"\"\"\n    This metric is copied from TensorFlow Addons\n    \"\"\"\n    def __init__(self,\n                 num_classes,\n                 name='cohen_kappa',\n                 weightage=None,\n                 dtype=tf.float32):\n        super(CohenKappa, self).__init__(name=name, dtype=dtype)\n\n        if weightage not in (None, 'linear', 'quadratic'):\n            raise ValueError(\"Unknown kappa weighting type.\")\n        else:\n            self.weightage = weightage\n\n        self.num_classes = num_classes\n        self.conf_mtx = self.add_weight(\n            'conf_mtx',\n            shape=(self.num_classes, self.num_classes),\n            initializer=tf.keras.initializers.zeros,\n            dtype=tf.int32)\n    \n    def update_state(self, y_true, y_pred, sample_weight=None):\n        if len(y_true.shape) == 2:\n            y_true = tf.argmax(y_true, axis=1)\n        if len(y_pred.shape) == 2:\n            y_pred = tf.argmax(y_pred, axis=1)\n        \n        y_true = tf.cast(y_true, dtype=tf.int32)\n        y_pred = tf.cast(y_pred, dtype=tf.int32)\n        \n        if y_true.shape.as_list() != y_pred.shape.as_list():\n            raise ValueError(\n                \"Number of samples in y_true and y_pred are different\")\n\n        # compute the new values of the confusion matrix\n        new_conf_mtx = tf.math.confusion_matrix(\n            labels=y_true,\n            predictions=y_pred,\n            num_classes=self.num_classes,\n            weights=sample_weight)\n\n        # update the values in the original confusion matrix\n        return self.conf_mtx.assign_add(new_conf_mtx)\n    \n    def result(self):\n        nb_ratings = tf.shape(self.conf_mtx)[0]\n        weight_mtx = tf.ones([nb_ratings, nb_ratings], dtype=tf.int32)\n\n        # 2. Create a weight matrix\n        if self.weightage is None:\n            diagonal = tf.zeros([nb_ratings], dtype=tf.int32)\n            weight_mtx = tf.linalg.set_diag(weight_mtx, diagonal=diagonal)\n            weight_mtx = tf.cast(weight_mtx, dtype=tf.float32)\n\n        else:\n            weight_mtx += tf.range(nb_ratings, dtype=tf.int32)\n            weight_mtx = tf.cast(weight_mtx, dtype=tf.float32)\n\n            if self.weightage == 'linear':\n                weight_mtx = tf.abs(weight_mtx - tf.transpose(weight_mtx))\n            else:\n                weight_mtx = tf.pow((weight_mtx - tf.transpose(weight_mtx)), 2)\n            weight_mtx = tf.cast(weight_mtx, dtype=tf.float32)\n\n        # 3. Get counts\n        actual_ratings_hist = tf.reduce_sum(self.conf_mtx, axis=1)\n        pred_ratings_hist = tf.reduce_sum(self.conf_mtx, axis=0)\n\n        # 4. Get the outer product\n        out_prod = pred_ratings_hist[..., None] * \\\n                    actual_ratings_hist[None, ...]\n\n        # 5. Normalize the confusion matrix and outer product\n        conf_mtx = self.conf_mtx / tf.reduce_sum(self.conf_mtx)\n        out_prod = out_prod / tf.reduce_sum(out_prod)\n\n        conf_mtx = tf.cast(conf_mtx, dtype=tf.float32)\n        out_prod = tf.cast(out_prod, dtype=tf.float32)\n\n        # 6. Calculate Kappa score\n        numerator = tf.reduce_sum(conf_mtx * weight_mtx)\n        denominator = tf.reduce_sum(out_prod * weight_mtx)\n        kp = 1 - (numerator / denominator)\n        return kp\n    \n    def get_config(self):\n        \"\"\"Returns the serializable config of the metric.\"\"\"\n\n        config = {\n            \"num_classes\": self.num_classes,\n            \"weightage\": self.weightage,\n        }\n        base_config = super(CohenKappa, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def reset_states(self):\n        \"\"\"Resets all of the metric state variables.\"\"\"\n\n        for v in self.variables:\n            K.set_value(\n                v, np.zeros((self.num_classes, self.num_classes), np.int32))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generation\n### Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nimport gc\nfrom collections import Counter\nfrom multiprocessing import Pool, cpu_count\nfrom typing import Tuple\n\nDAY_OF_WEEKS = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\nDAY_OF_WEEKS_MAP = {day: i for (i, day) in enumerate(DAY_OF_WEEKS)}\n\ndef gen_user_samples(user_events: pd.DataFrame, num2title: dict, assess_num_lis:list, title2win_code: dict,\n                     event_code_list: list, specs: pd.DataFrame=None, is_test_set=False) -> list:\n    user_samples = []\n    last_type = 0\n    types_count = {'Clip':0, 'Activity':0, 'Assessment':0, 'Game':0}\n\n    # time_first_activity = float(user_events['timestamp'].values[0])\n    time_spent_each_title = {f'time_spent_title{title_num}':0 for title_num in num2title}\n    event_code_count = {f'event_{code}_cnt':0 for code in event_code_list}\n    accuracy_group_cnt = {f'acc_grp_{grp}_cnt':0 for grp in [0, 1, 2, 3] }\n\n    atmpts_each_assess = {f'atmpts_each_assess{assess_num}': 0 for assess_num in assess_num_lis}\n    wins_each_assess = {f'wins_each_assess{assess_num}': 0 for assess_num in assess_num_lis}\n    losses_each_assess = {f'losses_each_assess{assess_num}': 0 for assess_num in assess_num_lis}\n\n    accumu_acc_grp = 0\n    accumu_acc = 0\n    accumu_win_n = 0\n    accumu_loss_n = 0\n    accumu_actions = 0\n    durations = []\n    non_assess_durations = []\n    counter = 0\n\n    for session_id, session in user_events.groupby('game_session', sort=False):\n        # sort inside to achieve better performace\n        session = session.sort_values(by='timestamp')\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] / 1000)\n            time_spent_each_title[f'time_spent_title{session_title}'] += time_spent\n            non_assess_durations.append((session.iloc[-1]['timestamp'] - session.iloc[0]['timestamp']).seconds)\n\n        if (session_type == 'Assessment') & (is_test_set or len(session) > 1):\n            # all_4100 = session.query(f'event_code == {title2win_code[session_title]}')\n            all_4100 = session[session['event_code'] == title2win_code[session_title]]\n            #numbers of wins and losses (globally)\n            # TODO: count on each title since some of them maybe similar\n            win_n = all_4100['event_data'].str.contains('true').sum()\n            loss_n = all_4100['event_data'].str.contains('false').sum()\n\n            # init feature then update\n            features = types_count.copy()\n            features['installation_id'] = session['installation_id'].iloc[-1]\n            features.update(time_spent_each_title.copy())\n            features.update(event_code_count.copy())\n            features.update(atmpts_each_assess.copy())\n            features.update(wins_each_assess.copy())\n            features.update(losses_each_assess.copy())\n            features['session_title'] = session_title\n            features['accumu_win_n'] = accumu_win_n\n            features['accumu_loss_n'] = accumu_loss_n\n            accumu_win_n += win_n\n            accumu_loss_n += loss_n\n            atmpts_each_assess[f'atmpts_each_assess{session_title}'] += 1\n            wins_each_assess[f'wins_each_assess{session_title}'] += win_n\n            losses_each_assess[f'losses_each_assess{session_title}'] += loss_n\n\n            features['day_of_the_week'] = DAY_OF_WEEKS_MAP[(session['timestamp'].iloc[-1]).strftime(\"%A\")]\n            features['hour'] = session['timestamp'].iloc[-1].hour\n            features['month'] = session['timestamp'].iloc[-1].month\n\n            if durations == []:\n                features['duration_mean'] = 0\n                features['duration_sum'] = 0\n                features['duration_std'] = 0\n                features['duration_min'] = 0\n                features['duration_max'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n                features['duration_sum'] = np.sum(durations)\n                features['duration_std'] = np.std(durations)\n                features['duration_min'] = np.min(durations)\n                features['duration_max'] = np.max(durations)\n            durations.append((session.iloc[-1]['timestamp'] - session.iloc[0]['timestamp']).seconds)\n\n            if non_assess_durations == []:\n                features['non_assess_duration_mean'] = 0\n                features['non_assess_duration_sum'] = 0\n                features['non_assess_duration_std'] = 0\n                features['non_assess_duration_min'] = 0\n                features['non_assess_duration_max'] = 0\n            else:\n                features['non_assess_duration_mean'] = np.mean(non_assess_durations)\n                features['non_assess_duration_sum'] = np.sum(non_assess_durations)\n                features['non_assess_duration_std'] = np.std(non_assess_durations)\n                features['non_assess_duration_min'] = np.min(non_assess_durations)\n                features['non_assess_duration_max'] = np.max(non_assess_durations)\n\n\n            # average of the all accuracy of this player\n            features['accuracy_ave'] = accumu_acc / counter if counter > 0 else 0\n            accuracy = win_n / (win_n + loss_n) if (win_n + loss_n) > 0 else 0\n            accumu_acc += accuracy\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_group_cnt.copy())\n            accuracy_group_cnt['acc_grp_{}_cnt'.format(features['accuracy_group'])] += 1\n            # average of accuracy_groups of this player\n            features['accuracy_group_ave'] = accumu_acc_grp / counter if counter > 0 else 0\n            accumu_acc_grp += features['accuracy_group']\n\n\n            # how many actions the player has done in this game_session\n            features['accumu_actions'] = accumu_actions\n\n            # if test_set, all sessions belong to the final dataset\n            # elif train, needs to be passed throught this clausule\n\n            if is_test_set or (win_n + loss_n) > 0:\n                user_samples.append(features)\n\n            counter += 1\n\n        # how many actions was made in each event_code\n        event_codes = Counter(session['event_code'])\n        for key in event_codes.keys():\n            event_code_count[f'event_{key}_cnt'] += event_codes[key]\n\n        # how many actions the player has done\n        accumu_actions += len(session)\n        if last_type != session_type:\n            types_count[session_type] += 1\n            last_type = session_type\n\n    # if test_set, only the last assessment must be predicted,\n    # the previous are scraped\n    if is_test_set:\n        return user_samples[-1]\n    return user_samples\n\ndef gen_assess_avg_acc(train_labels, title2num):\n    \"\"\"\n    Generate average accuracy of each assessment\n    \"\"\"\n    print('Calculate average accuracy of each assessment')\n    return {\n        title2num[title]: group['accuracy'].mean() \\\n        for (title, group) in train_labels.groupby('title', sort=False)\n    }\n\ndef gen_assess_corr_rate(train_labels, title2num):\n    \"\"\"\n    Generate correct rate of each assessment\n    \"\"\"\n    print('Calculate correct rate of each assessment')\n    df = train_labels\n    acc_assessment_dict = dict()\n    for title, group in df.groupby('title'):\n        num_correct = group['num_correct'].sum()\n        num_incorrect = group['num_incorrect'].sum()\n        acc_assessment_dict[title2num[title]] = num_correct / (num_correct + num_incorrect + 1E-6)\n    return acc_assessment_dict\n\n\ndef gen_data_sets(train_events: pd.DataFrame, test_events: pd.DataFrame, train_labels: pd.DataFrame,\n                  spec_data: pd.DataFrame = None, n_jobs=-1) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    # unique title list\n    print('starting generate data sets...')\n\n    title_list = np.unique(np.hstack([\n        train_events['title'].values, test_events['title'].values\n    ])).tolist()\n\n    # num <-> title\n    # num <-> title\n    title2num = {title: num for (num, title) in enumerate(title_list)}\n    num2title = {num: title for (num, title) in enumerate(title_list)}\n\n    assess_avg_acc = gen_assess_avg_acc(train_labels, title2num)\n    assess_corr_rate = gen_assess_corr_rate(train_labels, title2num)\n\n    assess_titles = np.unique(\n        np.hstack([\n            train_events[train_events['type'] == 'Assessment']['title'].values,\n            test_events[test_events['type'] == 'Assessment']['title'].values,\n        ])\n    )\n    print(f'assessment titles: {assess_titles}')\n    assess_num_lis = [title2num[title] for title in assess_titles]\n    print(f'assessment num list: {assess_num_lis}')\n\n    # title num to event code\n    title2win_code = {num : 4100 for num in num2title}\n    title2win_code[title2num['Bird Measurer (Assessment)']] = 4110\n\n    # unique event code list\n    event_code_list = np.unique(np.hstack([\n        train_events['event_code'].values, test_events['event_code'].values\n    ])).tolist()\n\n    # map title to title number\n    train_events['title'] = train_events['title'].map(dict(title2num)).astype(np.int16)\n    test_events['title'] = test_events['title'].map(dict(title2num)).astype(np.int16)\n\n    train_events['timestamp'] = pd.to_datetime(train_events['timestamp'])\n    test_events['timestamp'] = pd.to_datetime(test_events['timestamp'])\n\n    print('start generating samples...')\n    num_process = cpu_count() if n_jobs == -1 else n_jobs\n    with Pool(processes=num_process) as pool:\n        processed_users = 0\n        train_samples = []\n        res = [pool.apply_async(gen_user_samples, args=(events, num2title, assess_num_lis, title2win_code, event_code_list)) \\\n               for (install_id, events) in train_events.groupby('installation_id')]\n        for rr in res:\n            train_samples += rr.get()\n            processed_users += 1\n            if not (processed_users % 1000):\n                print(f'Proessed {processed_users} users in train')\n\n        test_samples = []\n        processed_users = 0\n        res = [pool.apply_async(gen_user_samples, args=(events, num2title, assess_num_lis, title2win_code, event_code_list, None, True)) \\\n               for (install_id, events) in test_events.groupby('installation_id')]\n        for rr in res:\n            test_samples.append(rr.get())\n            processed_users += 1\n            if not (processed_users % 100):\n                print(f'Proessed {processed_users} users in test')\n\n        train_output, test_output = pd.DataFrame(train_samples), pd.DataFrame(test_samples)\n        train_output['assess_avg_acc'] = train_output['session_title'] \\\n                                         .map(assess_avg_acc).astype(np.float32)\n        train_output['assess_corr_rate'] = train_output['session_title'] \\\n                                         .map(assess_corr_rate).astype(np.float32)\n\n        test_output['assess_avg_acc'] = test_output['session_title'] \\\n                                         .map(assess_avg_acc).astype(np.float32)\n        test_output['assess_corr_rate'] = test_output['session_title'] \\\n                                         .map(assess_corr_rate).astype(np.float32)\n    return train_output, test_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/data-science-bowl-2019'\n\n# specify column type to reduce memory usage\nCOL_TYPES = {\n    'game_session': 'object',\n    'timestamp': 'object',\n    'event_data': 'object',\n    'installation_id': 'object',\n    'title': 'category',\n    'type': 'category',\n    'game_time': 'int64',\n    'event_code': 'int32'\n}\nuse_cols = list(COL_TYPES.keys())\nprint(f'USE COLS: {use_cols}')\nprint('loading train data...')\ntrain = pd.read_csv(f'{data_dir}/train.csv', usecols=use_cols, dtype=COL_TYPES)\nprint('loading test data...')\ntest = pd.read_csv(f'{data_dir}/test.csv', usecols=use_cols, dtype=COL_TYPES)\n\nprint('loading train label data...')\ntrain_labels = pd.read_csv(f'{data_dir}/train_labels.csv', usecols=['title', 'num_correct', 'num_incorrect', 'accuracy'],\n                           dtype={'title': 'object', 'num_correct': np.int32, 'num_incorrect': np.int32, 'accuracy': np.float32})\ntrain_set, test_set = gen_data_sets(train, test, train_labels)\ndel train\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"category_features = ['session_title','day_of_the_week', 'hour', 'month']\nfeature_blacklist = frozenset(['accuracy_group', 'installation_id', 'game_session'])\nall_features = [col for col in train_set.columns if col not in  feature_blacklist]\nmulti_val_fea = [col for col in all_features if train_set[col].nunique() > 1]\nprint(f'totally {len(all_features)}, {len(multi_val_fea)} of them have multiple values')\n\nused_features = [fea for fea in multi_val_fea if fea not in feature_blacklist]\nprint(f'totally {len(used_features)} features are used for training')\nneed_log_pat = re.compile(r'time_spent.*|duration.*|non_assess_duration.*')\nneed_log_fea = [fea for fea in used_features if need_log_pat.match(fea)]\nprint(f'{len(need_log_fea)} need log features: \\n{need_log_fea}')\nnumeric_fea = [fea for fea in used_features if fea not in category_features]\nprint(f'{len(numeric_fea)} numeric features: \\n{numeric_fea}')\n\ntmp_df  = pd.concat([train_set[used_features], test_set[used_features]])\nfeature2vocab = {}\nfor feature in category_features:\n    feature2vocab[feature] = np.unique(tmp_df[feature].values).tolist()\ntrain_df = tmp_df.iloc[:len(train_set), :].copy()\ntrain_df['label'] = train_set['accuracy_group']\ntest_df = tmp_df.iloc[len(train_set):, :].copy()\nsubmission = test_set[['installation_id']].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=True, batch_size=32, num_classes=None):\n    dataframe = dataframe.copy()\n    if 'label' in dataframe.columns:\n        labels = dataframe.pop('label')\n        if num_classes and num_classes > 2:\n            labels = tf.keras.utils.to_categorical(labels, num_classes=num_classes)\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds\n\ndef build_nn_model(feature_columns, num_classes=None, dtype=tf.float64):\n    dtype = tf.float64\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype=dtype)\n    model = tf.keras.Sequential([\n        feature_layer,\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(1024, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(512, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(256, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(128, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(64, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(32, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n        tf.keras.layers.Dense(16, activation='relu', dtype=dtype),\n        tf.keras.layers.BatchNormalization(dtype=dtype),\n    ])\n    if not num_classes:\n        model.add(tf.keras.layers.Dense(1, dtype=dtype))\n    elif num_classes == 2:\n        model.add(tf.keras.layers.Dense(1, activation='sigmoid',dtype=dtype))\n    else:\n        model.add(tf.keras.layers.Dense(num_classes, activation='softmax', dtype=dtype))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ntrain_df[need_log_fea] = np.log(train_df[need_log_fea].values + 1.0)\ntest_df[need_log_fea] = np.log(test_df[need_log_fea].values + 1.0)\nscaler = MinMaxScaler()\ntrain_df[numeric_fea] = scaler.fit_transform(train_df[numeric_fea].values.astype(np.float64))\ntest_df[numeric_fea] = scaler.transform(test_df[numeric_fea].values.astype(np.float64))\n\nemb_size = 32\nfeature_columns = []\nfor fea in used_features:\n    if fea in category_features:\n        categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(fea, feature2vocab[fea])\n        feature_columns.append(tf.feature_column.embedding_column(categorical_col, emb_size))\n    else:\n        feature_columns.append(tf.feature_column.numeric_column(fea))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\ninstall_ids = train_set['installation_id'].copy()\ntrain_df, install_ids = shuffle(train_df, install_ids, random_state=RANDOM_SEED)\ntrain_df.reset_index(inplace=True, drop=True)\n\nnum_epoch = 64\nfrom sklearn.model_selection import GroupKFold\ngkf = GroupKFold(n_splits=5)\nval_kappas = []\ntest_set_ = df_to_dataset(test_df, shuffle=False, num_classes=4, batch_size=test_df.shape[0])\n\ntest_preds = np.zeros((test_df.shape[0], 4))\nfor train_idx, val_idx in gkf.split(train_df, groups=install_ids):\n    train_set_ = df_to_dataset(train_df.iloc[train_idx].copy(), num_classes=4, batch_size=64)\n    val_set_ = df_to_dataset(train_df.iloc[val_idx].copy(), shuffle=False, num_classes=4, batch_size=64)\n    model = build_nn_model(feature_columns, num_classes=4)\n    model.compile(\n        optimizer=tf.keras.optimizers.Nadam(),\n        loss=CohenKappaLoss(4),\n        metrics=[CohenKappa(num_classes=4, weightage='quadratic')]\n    )\n    model.fit(train_set_, epochs=num_epoch, verbose=2)\n    loss, kappa = model.evaluate(val_set_, verbose=2)\n    val_kappas.append(kappa)\n    print(f'validation result, loss: {loss}, kappa: {kappa}')\n    test_preds += model.predict(test_set_)\nprint(f'validation mean: {np.mean(val_kappas)}, std: {np.std(val_kappas)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict Test Set and Submit Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = test_preds.argmax(axis=1).astype(np.int8)\nprint(f'predicted accuracy_group distribution:\\n\\n{pd.Series(preds).value_counts(normalize=True)} \\n\\n')\nsubmission['accuracy_group'] = preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}