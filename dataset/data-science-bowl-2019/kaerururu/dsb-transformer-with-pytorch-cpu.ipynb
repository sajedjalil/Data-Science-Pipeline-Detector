{"cells":[{"metadata":{},"cell_type":"markdown","source":"## TL;DR\n\n### This is inspired by https://www.kaggle.com/toshik/37th-place-solution/notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nimport gc\nimport json\nimport math\nimport os\nimport pickle\nimport random\nimport re\nimport six\nimport time\nimport warnings\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import (Dataset,DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom pathlib import Path\nfrom functools import partial, reduce\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\nfrom torch.autograd import Variable\n\nimport multiprocessing\nfrom multiprocessing import Process\n\nfrom tqdm import tqdm_notebook as tqdm\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom scipy import optimize\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split\nfrom numba import jit\nfrom IPython.display import display\n\n\n\nSEED = 1129\n\ndef seed_everything(seed=1129):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(SEED)\n\n\ndef to_pickle(filename, obj):\n    with open(filename, mode='wb') as f:\n        pickle.dump(obj, f)\n\ndef unpickle(filename):\n    with open(filename, mode='rb') as fo:\n        p = pickle.load(fo)\n    return p  \n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print('[{}] done in {} s'.format(name, round(time.time() - t0, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\ndir_dataset = Path('/kaggle/input/data-science-bowl-2019')\n\ndir_model = Path('/kaggle/input/dsb2019-37th-models')\n\ninput_path = \"/kaggle/input/data-science-bowl-2019/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading test.csv file....')\n    test = pd.read_csv(input_path + 'test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv(input_path + 'train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv(input_path + 'specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv(input_path + 'sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return test, train_labels, specs, sample_submission\n\n\ndef read_pickle_data():\n    print('Reading train.pkl file....')\n    train = unpickle('../input/dsb2019-raw-data-pickled/train.pkl')\n    print('Training.pkl file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n    train_feature_gp = unpickle('../input/dsb2019-raw-data-pickled/train_feature_gp.pkl')\n    train_history = unpickle('../input/dsb2019-raw-data-pickled/train_history.pkl') \n    train_current = unpickle('../input/dsb2019-raw-data-pickled/train_current.pkl') \n    return train, train_feature_gp, train_history, train_current","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The entire model receive query features and history features.\nEncoder layer extract high level features from history and they are concatenated with query features.\nSubsequently, output values are calculated throught FC layers."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Feature Extraction\nSince history data is given to the model directly, feature extraction is very simple.\n\nAs a history feature, the number of event code, title, types are counted for each game_session.\nDuration of geme_session is also calculated and concatenated with history features.\n\nAs a query feature, the number of `correct` and `incorrect` are counted respectively. "},{"metadata":{"trusted":true},"cell_type":"code","source":"session_merge = partial(pd.merge, on='game_session')\n\ndef extract_features(data, data_labels, event_codes, titles, types, num_history_step: int):\n\n    data['timestamp'] = pd.to_datetime(data['timestamp'])\n    data['correct'] = data['event_data'].map(lambda x: '\"correct\":true' in x)\n    data['incorrect'] = data['event_data'].map(lambda x: '\"correct\":false' in x)\n\n    data_gp = data.groupby(['installation_id', 'game_session'])\n    data_time = data_gp['timestamp'].agg(min).reset_index()\n\n    data_time_max = data_gp['timestamp'].agg(max).reset_index()[['game_session', 'timestamp']]\n    data_time_max.columns = ['game_session', 'timestamp_end']\n\n    data_level = data_gp['f_level'].agg(np.max).reset_index()[['game_session', 'f_level']]\n    data_level = data_level.fillna(0.0)\n\n    data_count = data_gp[['correct', 'incorrect']].agg(sum).reset_index()[['game_session', 'correct', 'incorrect']]\n\n    data_code = pd.crosstab(data['game_session'], data['event_code']).astype(np.float32)\n    data_title = pd.crosstab(data['game_session'], data['title']).astype(np.float32)\n    data_type = pd.crosstab(data['game_session'], data['type']).astype(np.float32)\n\n    data_title_str = data.drop_duplicates('game_session', keep='last').copy()[['game_session', 'title']]\n\n    data_feature = reduce(\n        session_merge,\n        [data_time, data_code, data_title, data_type, data_time_max, data_title_str, data_count, data_level]\n    )\n    data_feature.index = data_feature['game_session']\n    data_feature_gp = data_feature.groupby('installation_id')\n\n    list_history = list()\n    list_current = list()\n\n    num_unique_geme_session = len(set(data_feature['game_session']))\n    num_unique_id_and_game_session = len(set(zip(data_feature['installation_id'], data_feature['game_session'])))\n    assert num_unique_geme_session == num_unique_id_and_game_session\n\n    assessments = [\n        'Mushroom Sorter (Assessment)',\n        'Bird Measurer (Assessment)',\n        'Cauldron Filler (Assessment)',\n        'Cart Balancer (Assessment)',\n        'Chest Sorter (Assessment)'\n    ]\n\n    for _, row in tqdm(data_labels.iterrows(), total=len(data_labels), miniters=100):\n\n        same_id = data_feature_gp.get_group(row['installation_id'])\n\n        target_timestamp = same_id.loc[row['game_session'], 'timestamp']\n\n        same_id_before = same_id.loc[same_id['timestamp'] < target_timestamp].copy()\n        same_id_before.sort_values('timestamp', inplace=True)\n\n        same_id_before['duration'] = (same_id_before['timestamp_end'] - same_id_before['timestamp']).dt.total_seconds()\n        same_id_before['duration'] = np.log1p(same_id_before['duration'])\n\n        h_feature = same_id_before.iloc[-num_history_step:][event_codes + titles + types + ['duration']]\n        h_feature = np.log1p(h_feature.values)\n\n        c_feature = (same_id.loc[row['game_session']][assessments].values != 0).astype(np.int32)\n\n        query_title = row['title']\n        success_exp = np.sum(same_id_before.query('title==@query_title')['correct'])\n        failure_exp = np.sum(same_id_before.query('title==@query_title')['incorrect'])\n\n        c_feature = np.append(c_feature, np.log1p(success_exp))\n        c_feature = np.append(c_feature, np.log1p(failure_exp))\n        c_feature = np.append(c_feature, (success_exp + 1) / (success_exp + failure_exp + 2) - 0.5)\n        c_feature = np.append(c_feature, (target_timestamp.hour - 12.0) / 10.0)\n\n        if len(h_feature) < num_history_step:\n            h_feature = np.pad(h_feature, ((num_history_step - len(h_feature), 0), (0, 0)),\n                               mode='constant', constant_values=0)\n\n        list_history.append(h_feature)\n        list_current.append(c_feature)\n\n    history = np.asarray(list_history)\n    current = np.asarray(list_current)\n\n    return data_feature_gp, history, current","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n\ndef stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('load data'):\n    seed = 3048\n    fold_id = 0\n    epochs = 3\n    batch_size = 64\n\n    with open(str(dir_model / f'parameters_seed{seed}.json'), 'r') as f:\n                hyper_params = json.load(f)\n\n    num_history_step = hyper_params['num_history_step']\n    \n    # read data\n    test, train_labels, specs, sample_submission = read_data()\n    train, train_feature_gp, train_history, train_current = read_pickle_data()\n    y = train_labels[\"accuracy_group\"].values\n\n    num_folds = 5\n    # kf = StratifiedGroupKFold(n_splits = num_folds)\n    # splits = list(kf.split(X=train_labels, y=y, groups=train_labels.installation_id))\n    \n    splits = stratified_group_k_fold(train_labels, y, train_labels.installation_id, num_folds, seed=SEED)\n    # train_idx = splits[fold_id][0]\n    # val_idx = splits[fold_id][1]\n    # train_idx, val_idx = train_test_split(train_labels.index.tolist(), test_size=0.2, \n    #                                       random_state=SEED, stratify=y)\n    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train data feature engineering ###\n### you can skip here with read_pickle_data() function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_installation_id = list(set(train_labels[\"installation_id\"]))\n# len(train_installation_id)\n\n# train = train[train.installation_id.isin(train_installation_id)]\n# print('train shape: {}'.format(train.shape))\n\n# train.sort_values(['installation_id', 'timestamp'], inplace=True)\n# print('train shape: {}'.format(train.shape))\n\nevent_codes = pd.read_csv(dir_model / f'event_codes.csv')['event_code'].tolist()\ntitles = pd.read_csv(dir_model / 'media_sequence.csv')['title'].tolist()\ntypes = ['Activity', 'Assessment', 'Clip', 'Game']\n# re_level = re.compile(r'.*\"level\":([0-9]+).*')\n\n# train['event_code'] = pd.Categorical(train['event_code'], categories=event_codes)\n# train['title'] = pd.Categorical(train['title'], categories=titles)\n# train['type'] = pd.Categorical(train['type'], categories=types)\n# train['f_level'] = train['event_data'].map(\n#         lambda x: int(re.sub(re_level, '\\\\1', x)) + 1 if '\"level\"' in x else np.nan)\n\n# print(' train shape: {}'.format(train.shape))\n\n# train_feature_gp, train_history, train_current = extract_features(train, train_labels, event_codes, titles, types, num_history_step)\n\n# train\n# result_path = \"train_history.pkl\"\n# to_pickle(result_path, train_history)\n\n# result_path = \"train_current.pkl\"\n# to_pickle(result_path, train_current)\n\n# result_path = \"train_feature_gp.pkl\"\n# to_pickle(result_path, train_feature_gp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare Transformer model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvolutionSentence(nn.Conv2d):\n    \"\"\" Position-wise Linear Layer for Sentence Block\n    Position-wise linear layer for array of shape\n    (batchsize, dimension, sentence_length)\n    can be implemented a convolution layer.\n    \"\"\"\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=True):\n        super(ConvolutionSentence, self).__init__(\n             in_channels, out_channels,\n             kernel_size, stride, padding, dilation, groups, bias)\n\n    def __call__(self, x):\n        \"\"\"Applies the linear layer.\n        Args:\n            x (~chainer.Variable): Batch of input vector block. Its shape is\n                (batchsize, in_channels, sentence_length).\n        Returns:\n            ~chainer.Variable: Output of the linear layer. Its shape is\n                (batchsize, out_channels, sentence_length).\n        \"\"\"     \n        x = x.unsqueeze(3)\n        y = super(ConvolutionSentence, self).__call__(x)\n        y = torch.squeeze(y, 3)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\" Multi Head Attention Layer for Sentence Blocks\n    For batch computation efficiency, dot product to calculate query-key\n    scores is performed all heads together.\n    \"\"\"\n\n    def __init__(self, n_units, h=8, dropout=0.1, self_attention=True):\n        super(MultiHeadAttention, self).__init__()\n        \n        if self_attention:\n            self.W_QKV = ConvolutionSentence(\n                n_units, n_units * 3, kernel_size=1, bias=False)\n        else:\n            self.W_Q = ConvolutionSentence(\n                n_units, n_units, kernel_size=1, bias=False)\n            self.W_KV = ConvolutionSentence(\n                n_units, n_units * 2, kernel_size=1, bias=False)\n        self.finishing_linear_layer = ConvolutionSentence(\n            n_units, n_units, bias=False)\n        self.h = h\n        self.scale_score = 1. / (n_units // h) ** 0.5\n        self.dropout = dropout\n        self.is_self_attention = self_attention\n\n    def __call__(self, x, z=None, mask=None):\n        # xp = self.xp\n        h = self.h\n\n        # temporary mask\n        mask = np.zeros((8, x.shape[2], x.shape[2]), dtype=np.bool)\n\n        if self.is_self_attention:\n            # print(f'self.W_QKV(x) shape : {self.W_QKV(x).shape}')\n            Q, K, V = torch.chunk(self.W_QKV(x), 3, axis=1)\n        else:\n            Q = self.W_Q(x)\n            K, V = torch.chunk(self.W_KV(z), 2, axis=1)\n        batch, n_units, n_querys = Q.shape\n        _, _, n_keys = K.shape\n\n        # Calculate Attention Scores with Mask for Zero-padded Areas\n        # Perform Multi-head Attention using pseudo batching\n        # all together at once for efficiency\n        \n        batch_Q = torch.cat(torch.chunk(Q, h, axis=1), axis=0)\n        batch_K = torch.cat(torch.chunk(K, h, axis=1), axis=0)\n        batch_V = torch.cat(torch.chunk(V, h, axis=1), axis=0)\n        assert(batch_Q.shape == (batch * h, n_units // h, n_querys))\n        assert(batch_K.shape == (batch * h, n_units // h, n_keys))\n        assert(batch_V.shape == (batch * h, n_units // h, n_keys))\n\n        # print(f'batch_Q shape : {batch_Q.shape}')\n        # print(f'batch_K shape : {batch_K.shape}')\n        # print(f'batch_V shape : {batch_V.shape}')\n        # mask = xp.concatenate([mask] * h, axis=0)\n        batch_A = torch.matmul(batch_Q.permute(0, 2, 1), batch_K) * self.scale_score\n        # print(f'batch_A shape : {batch_A.shape}')\n        # Calculate Weighted Sum\n        batch_A = batch_A.unsqueeze(1)\n        batch_V = batch_V.unsqueeze(2)\n        batch_C = torch.sum(batch_A * batch_V, axis=3)\n        assert(batch_C.shape == (batch * h, n_units // h, n_querys))\n        \n        # print(f'batch_C shape : {batch_C.shape}')\n        \n        C = torch.cat(torch.chunk(batch_C, h, axis=0), axis=1)\n        assert(C.shape == (batch, n_units, n_querys))\n        C = self.finishing_linear_layer(C)\n        # print(f'C shape : {C.shape}')\n        return C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeedForwardLayer(nn.Module):\n    def __init__(self, n_units: int, ff_inner: int, ff_slope: float):\n        super(FeedForwardLayer, self).__init__()\n        n_inner_units = n_units * ff_inner\n        self.slope = ff_slope\n        self.W_1 = ConvolutionSentence(n_units, n_inner_units)\n        self.W_2 = ConvolutionSentence(n_inner_units, n_units)\n        self.act = F.leaky_relu\n\n    def __call__(self, e):\n        e = self.W_1(e)\n        e = self.act(e, negative_slope=self.slope)\n        e = self.W_2(e)\n        return e\n    \n\ndef seq_func(func, x, reconstruct_shape=True):\n    \"\"\" Change implicitly function's target to ndim=3\n    Apply a given function for array of ndim 3,\n    shape (batchsize, dimension, sentence_length),\n    instead for array of ndim 2.\n    \"\"\"\n\n    batch, units, length = x.shape\n    e = x.permute(0, 2, 1).reshape(batch * length, units)\n    e = func(e)\n    if not reconstruct_shape:\n        return e\n    out_units = e.shape[1]\n    e = e.reshape((batch, length, out_units)).permute(0, 2, 1)\n    assert(e.shape == (batch, out_units, length))\n    return e\n\n\nclass LayerNorm(nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\n\nclass LayerNormalizationSentence(LayerNorm):\n    \"\"\" Position-wise Linear Layer for Sentence Block\n    Position-wise layer-normalization layer for array of shape\n    (batchsize, dimension, sentence_length).\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(LayerNormalizationSentence, self).__init__(*args, **kwargs)\n\n    def __call__(self, x):\n        y = seq_func(super(LayerNormalizationSentence, self).__call__, x)\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, n_units, ff_inner: int, ff_slope: float, h: int, dropout1: float, dropout2: float):\n        super(EncoderLayer, self).__init__()\n\n        self.self_attention = MultiHeadAttention(n_units, h)\n        self.feed_forward = FeedForwardLayer(n_units, ff_inner, ff_slope)\n        self.ln_1 = LayerNormalizationSentence(n_units, eps=1e-6)\n        self.ln_2 = LayerNormalizationSentence(n_units, eps=1e-6)\n        self.dropout1 = nn.Dropout(dropout1)\n        self.dropout2 = nn.Dropout(dropout2)\n\n    def __call__(self, e, xx_mask):\n        sub = self.self_attention(e, e, xx_mask)\n        e = e + self.dropout1(sub)\n        e = self.ln_1(e)\n\n        sub = self.feed_forward(e)\n        e = e + self.dropout2(sub)\n        e = self.ln_2(e)\n        return e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DSB2019Net(nn.Module):\n\n    def __init__(self, dim_input: int, dim_enc: int, dim_fc: int,\n                 ff_inner: int, ff_slope: float, head: int,\n                 dropout1: float, dropout2: float, dropout3: float, **kwargs):\n        '''\n        from https://www.kaggle.com/toshik/37th-place-solution/notebook\n        thank you for @toshi_k 's nice solution and imprements\n        '''\n        super(DSB2019Net, self).__init__()\n\n        self.dropout3 = nn.Dropout(dropout3)\n\n        self.cur_fc1 = nn.Linear(9, 128)\n        self.cur_fc2 = nn.Linear(128, dim_input)\n\n        self.hist_conv1 = ConvolutionSentence(dim_input, int(dim_enc))\n        self.hist_enc1 = EncoderLayer(int(dim_enc), ff_inner, ff_slope, head, dropout1, dropout2)\n\n        self.fc1 = nn.Linear(283, dim_fc)\n        self.fc2 = nn.Linear(dim_fc, 1)\n\n    def __call__(self, query, history, targets):\n\n        out = self.predict(query, history)\n        return out\n\n    def predict(self, query, history, **kwargs):\n        \"\"\"\n            query: [batch_size, feature]\n            history: [batch_size, time_step, feature]\n        \"\"\"\n\n        h_cur = F.leaky_relu(self.cur_fc1(query))\n        h_cur = self.cur_fc2(h_cur)\n\n        h_hist = history.permute(0, 2, 1)\n\n        h_hist = self.hist_conv1(h_hist)\n        \n        # print(h_hist.shape, h_cur.shape)\n\n        h_hist = self.hist_enc1(h_hist, xx_mask=None)\n    \n        h_hist_ave = torch.mean(h_hist, axis=2)\n        h_hist_max, _ = torch.max(h_hist, axis=2)\n\n        h = torch.cat([h_cur, h_hist_ave, h_hist_max], axis=1)\n        # print(f'h shape : {h.shape}')\n        \n        h = self.dropout3(F.leaky_relu(self.fc1(h)))\n        out = self.fc2(h)\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training part"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DictDataset(object):\n\n    \"\"\"Dataset of a dictionary of datasets.\n    It combines multiple datasets into one dataset. Each example is represented\n    by a dictionary mapping a key to an example of the corresponding dataset.\n    Args:\n        datasets: Underlying datasets. The keys are used as the keys of each\n            example. All datasets must have the same length.\n    \"\"\"\n\n    def __init__(self, **datasets):\n        if not datasets:\n            raise ValueError('no datasets are given')\n        length = None\n        for key, dataset in six.iteritems(datasets):\n            if length is None:\n                length = len(dataset)\n            elif length != len(dataset):\n                raise ValueError(\n                    'dataset length conflicts at \"{}\"'.format(key))\n        self._datasets = datasets\n        self._length = length\n\n    def __getitem__(self, index):\n        batches = {key: dataset[index]\n                   for key, dataset in six.iteritems(self._datasets)}\n        if isinstance(index, slice):\n            length = len(six.next(six.itervalues(batches)))\n            return [{key: batch[i] for key, batch in six.iteritems(batches)}\n                    for i in six.moves.range(length)]\n        else:\n            return batches\n\n    def __len__(self):\n        return self._length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, device, steps_upd_logging=500, accumulation_steps=1):\n    '''\n    from : https://github.com/okotaku/kaggle_rsna2019_3rd_solution/blob/master/src/trainer.py\n    '''\n    model.train()\n\n    total_loss = 0.0\n    for step, (input_dic) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        for k in input_dic.keys():\n            input_dic[k] = input_dic[k].to(device)\n            \n        history = input_dic['history']\n        query = input_dic['query']\n        targets = input_dic['targets']\n        \n        optimizer.zero_grad()\n\n        logits = model(query, history, targets)\n\n        loss = criterion(logits, targets)\n        loss.backward()\n\n        if (step + 1) % accumulation_steps == 0:  # Wait for several backward steps\n            optimizer.step()  # Now we can do an optimizer step\n\n        total_loss += loss.item()\n\n        if (step + 1) % steps_upd_logging == 0:\n            print('Train loss on step {} was {}'.format(step + 1, round(total_loss / (step + 1), 5)))\n\n\n    return total_loss / (step + 1)\n\n\ndef validate(model, val_loader, criterion, device):\n    '''\n    from : https://github.com/okotaku/kaggle_rsna2019_3rd_solution/blob/master/src/trainer.py\n    '''\n    model.eval()\n\n    val_loss = 0.0\n    true_ans_list = []\n    preds_cat = []\n    for step, (input_dic) in tqdm(enumerate(val_loader), total=len(val_loader)):\n        for k in input_dic.keys():\n            input_dic[k] = input_dic[k].to(device)\n            \n        history = input_dic['history']\n        query = input_dic['query']\n        targets = input_dic['targets']\n\n        logits = model(query, history, targets)\n\n        loss = criterion(logits, targets)\n        val_loss += loss.item()\n\n        targets = targets.float().cpu().detach().numpy()\n        logits = logits.float().cpu().detach().numpy().astype(\"float32\")\n        true_ans_list.append(targets)\n        preds_cat.append(logits)\n\n        del input_dic, targets, logits\n        gc.collect()\n\n    all_true_ans = np.concatenate(true_ans_list, axis=0)\n    all_preds = np.concatenate(preds_cat, axis=0)\n\n    return all_preds, all_true_ans, val_loss / (step + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://gist.github.com/jamesr2323/33c67ba5ac29880171b63d2c7f1acdc5\n# Thanks https://discuss.pytorch.org/t/rmse-loss-function/16540\n\nclass RMSELoss(torch.nn.Module):\n    def __init__(self):\n        super(RMSELoss,self).__init__()\n\n    def forward(self,x,y):\n        criterion = nn.MSELoss()\n        loss = torch.sqrt(criterion(x, y))\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_id, (train_idx, val_idx) in enumerate(splits):\n    \n    print(f'fold {fold_id} : start !')\n    \n    with timer('prepare validation data'):\n        y_train = y[train_idx]\n        train_dataset = DictDataset(history=train_history.astype(np.float32)[train_idx],\n                                    query=train_current.astype(np.float32)[train_idx],\n                                    targets=np.asarray(y_train, dtype=np.float32))\n\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size*8, shuffle=True, num_workers=0, pin_memory=True)\n\n        y_val = y[val_idx]\n        val_dataset = DictDataset(history=train_history.astype(np.float32)[val_idx],\n                                  query=train_current.astype(np.float32)[val_idx],\n                                  targets=np.asarray(y_val, dtype=np.float32))\n\n        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size*4, shuffle=False, num_workers=0, pin_memory=True)\n\n        del train_dataset, val_dataset\n        gc.collect()\n        \n        \n    with timer('create model'):\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n        model = DSB2019Net(len(event_codes + titles + types) + 1, **hyper_params)\n        model = model.to(device)\n\n        criterion = RMSELoss().to(device)\n        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=10)\n        \n        \n    with timer('training loop'):\n        EXP_ID = 'DSB transformer approach with pytorch'\n        OUT_DIR = '/kaggle/working'\n        best_score = 999\n        best_epoch = 0\n        for epoch in range(1, epochs + 1):\n\n            print(\"Starting {} epoch...\".format(epoch))\n            tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n            print('Mean train loss: {}'.format(round(tr_loss, 5)))\n\n            val_pred, y_true, val_loss = validate(model, val_loader, criterion, device)\n            score = np.sqrt(mean_squared_error(y_true, val_pred))\n            print('Mean valid loss: {} score: {}'.format(round(val_loss, 5), round(score, 5)))\n            if score < best_score:\n                best_score = score\n                best_epoch = epoch\n                torch.save(model.state_dict(), os.path.join(OUT_DIR, '{}_fold{}.pth'.format(EXP_ID, fold_id)))\n                np.save(os.path.join(OUT_DIR, \"{}_fold{}_val.npy\".format(EXP_ID, fold_id)), val_pred)\n                np.save(os.path.join(OUT_DIR, \"{}_fold{}_true.npy\".format(EXP_ID, fold_id)), y_true)\n            scheduler.step()\n\n        print(\"best score={} on epoch={}\".format(best_score, best_epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_pred_path = [\n    'DSB transformer approach with pytorch_fold0_true.npy',\n    'DSB transformer approach with pytorch_fold1_true.npy',\n    'DSB transformer approach with pytorch_fold2_true.npy',\n    'DSB transformer approach with pytorch_fold3_true.npy',\n    'DSB transformer approach with pytorch_fold4_true.npy',\n]\n\ntrue_pred_list = []\nfor p in true_pred_path:\n    true_pred_list.append(np.load(p))\n    \ntrue_preds = np.concatenate(true_pred_list)\ntrue_preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pred_path = [\n    'DSB transformer approach with pytorch_fold0_val.npy',\n    'DSB transformer approach with pytorch_fold1_val.npy',\n    'DSB transformer approach with pytorch_fold2_val.npy',\n    'DSB transformer approach with pytorch_fold3_val.npy',\n    'DSB transformer approach with pytorch_fold4_val.npy',\n]\n\nval_pred_list = []\nfor p in val_pred_path:\n    val_pred_list.append(np.load(p))\n    \nval_preds = np.concatenate(val_pred_list)\nval_preds.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict part"},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('make test features'):\n    tic = time.time()\n\n    sub = pd.read_csv(dir_dataset / 'sample_submission.csv')\n\n    test_installation_id = list(set(sub.installation_id))\n\n    print('test installation id: {}'.format(test_installation_id[:10]))\n\n    test = pd.read_csv(dir_dataset / 'test.csv')\n    test = test[test.installation_id.isin(test_installation_id)]\n    print('test shape: {}'.format(test.shape))\n\n    test.sort_values(['installation_id', 'timestamp'], inplace=True)\n    test_labels = test.drop_duplicates('installation_id', keep='last').copy()\n    test_labels.reset_index(drop=True, inplace=True)\n    test_labels['accuracy_group'] = -1  # dummy label\n\n    event_codes = pd.read_csv(dir_model / f'event_codes.csv')['event_code'].tolist()\n    titles = pd.read_csv(dir_model / 'media_sequence.csv')['title'].tolist()\n    types = ['Activity', 'Assessment', 'Clip', 'Game']\n    re_level = re.compile(r'.*\"level\":([0-9]+).*')\n\n    test['event_code'] = pd.Categorical(test['event_code'], categories=event_codes)\n    test['title'] = pd.Categorical(test['title'], categories=titles)\n    test['type'] = pd.Categorical(test['type'], categories=types)\n    test['f_level'] = test['event_data'].map(\n        lambda x: int(re.sub(re_level, '\\\\1', x)) + 1 if '\"level\"' in x else np.nan)\n\n    print(' test shape: {}'.format(test.shape))\n\n    data_feature_gp, test_history, test_current = extract_features(test, test_labels, event_codes, titles, types, num_history_step)\n\n    print(test_history.shape, test_current.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = DictDataset(history=test_history.astype(np.float32),\n                           query=test_current.astype(np.float32),\n                           targets=np.asarray(test_labels[['accuracy_group']], dtype=np.float32))\n\nbatch_size = 32\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size*6, shuffle=False, num_workers=0, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('create test model'):\n    \n    n_units = hyper_params['dim_enc']\n    dim_input = len(event_codes + titles + types) + 1\n        \n    model_path = [\n        'DSB transformer approach with pytorch_fold0.pth',\n        'DSB transformer approach with pytorch_fold1.pth',\n        'DSB transformer approach with pytorch_fold2.pth',\n        'DSB transformer approach with pytorch_fold3.pth',\n        'DSB transformer approach with pytorch_fold4.pth',\n    ]\n    \n    models = []\n    for p in model_path:\n        model = DSB2019Net(dim_input, **hyper_params)\n        model.load_state_dict(torch.load(p))\n        model.to(device)\n        model.eval()\n        models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(models, test_loader, device):\n    '''\n    from : https://github.com/okotaku/kaggle_rsna2019_3rd_solution/blob/master/src/trainer.py\n    '''\n    preds_cat = []\n    for step, (input_dic) in tqdm(enumerate(test_loader), total=len(test_loader)):\n        for k in input_dic.keys():\n            input_dic[k] = input_dic[k].to(device)\n            \n        history = input_dic['history']\n        query = input_dic['query']\n        targets = input_dic['targets']\n    \n        logits = []\n        for m in models:\n            logits_ = m(query, history, targets)\n            logits_ = logits_.float().cpu().detach().numpy().astype(\"float32\")\n            logits.append(logits_)\n        preds_cat.append(np.mean(logits, axis=0))\n\n        del input_dic, logits\n        gc.collect()\n\n    all_preds = np.concatenate(preds_cat, axis=0)\n\n    return all_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('nn predict'):\n    all_preds = predict(models, test_loader, device)\n    print(all_preds.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.accuracy_group = np.round(all_preds).astype('int')\nprint(sub.accuracy_group.value_counts())\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}