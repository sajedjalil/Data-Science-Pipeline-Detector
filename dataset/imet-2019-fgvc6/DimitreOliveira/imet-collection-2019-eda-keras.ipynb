{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>iMet Collection 2019 - FGVC6</center></h1>\n<h2><center>Recognize artwork attributes from The Metropolitan Museum of Art</center></h2>\n![](https://raw.githubusercontent.com/visipedia/imet-fgvcx/master/assets/banner.png)\n\n#### In this competition we are charged to build models to add fine-grained attributes to aid in the visual understanding of the museum objects, from the 1.5M objects, 200k were digitized, and are provided here.\n#### In this notebook I will be using a basic deep learning convolutional model to create a baseline."},{"metadata":{},"cell_type":"markdown","source":"### Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation, BatchNormalization\n\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n# Set seeds to make the experiment more reproducible.\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(0)\nseed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\nlabels = pd.read_csv('../input/labels.csv')\ntest = pd.read_csv('../input/sample_submission.csv')\n\nprint('Number of train samples: ', train.shape[0])\nprint('Number of test samples: ', test.shape[0])\nprint('Number of labels: ', labels.shape[0])\ndisplay(train.head())\ndisplay(labels.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 30 most frequent attributes\n- First, let's see between the 1103 attributes which are the most frequent ones."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"attribute_ids = train['attribute_ids'].values\nattributes = []\nfor item_attributes in [x.split(' ') for x in attribute_ids]:\n    for attribute in item_attributes:\n        attributes.append(int(attribute))\n        \natt_pd = pd.DataFrame(attributes, columns=['attribute_id'])\natt_pd = att_pd.merge(labels)\ntop30 = att_pd['attribute_name'].value_counts()[:30].to_frame()\nN_unique_att = att_pd['attribute_id'].nunique()\nprint('Number of unique attributes: ', N_unique_att)\nf, ax = plt.subplots(figsize=(12, 8))\nax = sns.barplot(y=top30.index, x=\"attribute_name\", data=top30, palette=\"rocket\", order=reversed(top30.index))\nax.set_ylabel(\"Surface type\")\nax.set_xlabel(\"Count\")\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"att_pd['tag'] = att_pd['attribute_name'].apply(lambda x:x.split('::')[0])\ngp_att = att_pd.groupby('tag').count()\n\nprint('Number of attributes groups: ', gp_att.shape[0])\nf, ax = plt.subplots(figsize=(12, 8))\nax = sns.barplot(y=gp_att.index, x=\"attribute_name\", data=gp_att, palette=\"rocket\")\nax.set_ylabel(\"Attribute group\")\nax.set_xlabel(\"Count\")\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of tags per item\n- We saw on the training set that some of the items have more than one attribute tag, let's see the attribute tag distribution."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train['Number of Tags'] = train['attribute_ids'].apply(lambda x: len(x.split(' ')))\nf, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"Number of Tags\", data=train, palette=\"GnBu_d\")\nax.set_ylabel(\"Surface type\")\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's see some of the items"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set_style(\"white\")\ncount = 1\nplt.figure(figsize=[20,20])\nfor img_name in os.listdir(\"../input/train/\")[:20]:\n    img = cv2.imread(\"../input/train/%s\" % img_name)[...,[2, 1, 0]]\n    plt.subplot(5, 5, count)\n    plt.imshow(img)\n    plt.title(\"Item %s\" % count)\n    count += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train[\"id\"] = train[\"id\"].apply(lambda x:x+\".png\")\ntest[\"id\"] = test[\"id\"].apply(lambda x:x+\".png\")\ntrain[\"attribute_ids\"] = train[\"attribute_ids\"].apply(lambda x:x.split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model parameters\nBATCH_SIZE = 128\nEPOCHS = 30\nLEARNING_RATE = 0.0001\nHEIGHT = 64\nWIDTH = 64\nCANAL = 3\nN_CLASSES = N_unique_att\nclasses = list(map(str, range(N_CLASSES)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(5,5),padding='Same', input_shape=(HEIGHT, WIDTH, CANAL)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(filters=32, kernel_size=(5,5),padding='Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(filters=64, kernel_size=(4,4),padding='Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(filters=64, kernel_size=(4,4),padding='Same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(N_CLASSES, activation=\"sigmoid\"))\nmodel.summary()\n\noptimizer = optimizers.adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer , loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen=ImageDataGenerator(rescale=1./255, validation_split=0.25)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator=train_datagen.flow_from_dataframe(\n    dataframe=train,\n    directory=\"../input/train\",\n    x_col=\"id\",\n    y_col=\"attribute_ids\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode=\"categorical\",\n    classes=classes,\n    target_size=(HEIGHT, WIDTH),\n    subset='training')\n\nvalid_generator=train_datagen.flow_from_dataframe(\n    dataframe=train,\n    directory=\"../input/train\",\n    x_col=\"id\",\n    y_col=\"attribute_ids\",\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    class_mode=\"categorical\",    \n    classes=classes,\n    target_size=(HEIGHT, WIDTH),\n    subset='validation')\n\ntest_generator = test_datagen.flow_from_dataframe(  \n        dataframe=test,\n        directory = \"../input/test\",    \n        x_col=\"id\",\n        target_size = (HEIGHT, WIDTH),\n        batch_size = 1,\n        shuffle = False,\n        class_mode = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size\nSTEP_SIZE_VAL = valid_generator.n // valid_generator.batch_size\n\nhistory = model.fit_generator(generator=train_generator,\n                    steps_per_epoch=STEP_SIZE_TRAIN,\n                    validation_data=valid_generator,\n                    validation_steps=STEP_SIZE_VAL,\n                    epochs=EPOCHS,\n                    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model graph loss"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(20,7))\n\nax1.plot(history.history['acc'], label='Train Accuracy')\nax1.plot(history.history['val_acc'], label='Validation accuracy')\nax1.legend(loc='best')\nax1.set_title('Accuracy')\n\nax2.plot(history.history['loss'], label='Train loss')\nax2.plot(history.history['val_loss'], label='Validation loss')\nax2.legend(loc='best')\nax2.set_title('Loss')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply model to test set and output predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator.reset()\nn_steps = len(test_generator.filenames)\npreds = model.predict_generator(test_generator, steps = n_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor pred_ar in preds:\n    valid = ''\n    for idx, pred in enumerate(pred_ar):\n        if pred > 0.3:  # Using 0.3 as threshold\n            if len(valid) == 0:\n                valid += str(idx)\n            else:\n                valid += (' %s' % idx)\n    if len(valid) == 0:\n        valid = str(np.argmax(pred_ar))\n    predictions.append(valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames=test_generator.filenames\nresults=pd.DataFrame({'id':filenames, 'attribute_ids':predictions})\nresults['id'] = results['id'].map(lambda x: str(x)[:-4])\nresults.to_csv('submission.csv',index=False)\nresults.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}