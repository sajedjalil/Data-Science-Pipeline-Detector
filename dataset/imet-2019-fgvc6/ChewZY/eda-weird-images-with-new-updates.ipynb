{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Don't skip the analysis!!!\n### Updated on 21 Apr\n\nWhen it comes to image data, people tend to skip the data understanding step and goes straight into using powerful models for transfer learning or feature extractions. This is alright, but wouldn't you want to understand what goes on under the hood?\n\nIn this kernel, I attempt to understand the data in a conventional data analysis approach (on the raw image metadata etc) and highlight interesting observations which can help your pre-processing pipeline and feature engineering.\n\nWhat is covered & what I plan to do over the next few weeks (or whenever I have time):\n* Image Shape **(done!)**\n* Understanding Target Variable **(done!)** **<---------------------------- NEW!!!!**\n* RGB Pixel Statistics **(done!)** **<--------------------------------------- NEW!!!!**\n* Edge Detection Pixel Statistics **(done!)** **<-------------------------- NEW!!!!**\n* Intensity Histogram\n* Correlations of the above with Target Variable\n\n### TL;DR - Summary of Insights\n1. Super long images exist in the dataset\n2. Highly imbalanced dataset, over 1000 labels, with 90% images having less than 5 labels\n3. Similar target classes (e.g. men, women, portraits, human figures)\n4. RGB pixel statistics - normal distribution with different mean"},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"## Load the necessary packages and files"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \nimport gc\nimport os\nimport PIL\n\nfrom scipy import stats\nfrom multiprocessing import Pool\nfrom PIL import ImageOps, ImageFilter\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud\n\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train-file-with-labels-and-meta-data/weird_images_w_labels.csv')\ntrain_path = '../input/imet-2019-fgvc6/train/'\nlabel_df = pd.read_csv('../input/imet-2019-fgvc6/labels.csv')\n\nprint('Files loaded!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fun Fact : PIL.Image.open() is faster than plt.imread()\n\nFor image reading, PIL averages at 400 iter/second, while plt only managed around 100 iter/second.\n\nSomething that I just found out, but I'm not too sure why it is that way. Appreciate if someone could enlighten me. \n\nUpdate 1 : I just learnt how to use multiprocessing with pandas, now it is much faster!!\nUpdate 2 : I moved the meta data extraction and one-hot encoding of labels to another file to another kernel"},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"# Long tail distribution for image width and height\n\nLooking at the KDE plot, you can see that there is a bump at ~5000 for width and ~7000 for height.\n\nWe must have some abnormally huge images in our dataset. (Or are they?)\n\n![](https://media1.tenor.com/images/39bac10b152f60e65329dfff93c7bf18/tenor.gif?itemid=4780502)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nplt.subplot(121)\nsns.distplot(df_train['width'],kde=False, label='Width')\nsns.distplot(df_train['height'], kde=False, label='Height')\nplt.legend()\nplt.title('Image Dimension Histogram', fontsize=15)\n\nplt.subplot(122)\nsns.kdeplot(df_train['width'], label='Width')\nsns.kdeplot(df_train['height'], label='Height')\nplt.legend()\nplt.title('Image Dimension KDE Plot', fontsize=15)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Turns out they are only huge in either width or height\n\nSomething pretty long?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train[['width','height']].sort_values(by='width',ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train[['width','height']].sort_values(by='height',ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets check out these weird images."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"weird_height_id = [v for v in df_train.sort_values(by='height',ascending=False).head(20)['id'].values]\nweird_width_id = [v for v in df_train.sort_values(by='width',ascending=False).head(20)['id'].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nfor num, img_id in enumerate(weird_height_id):\n    img = PIL.Image.open(f'{train_path}{img_id}.png')\n    plt.subplot(1,20,num + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \nplt.suptitle('Images with HUGE Height', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nfor num, img_id in enumerate(weird_width_id):\n    img = PIL.Image.open(f'{train_path}{img_id}.png')\n    plt.subplot(20,1,num + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \nplt.suptitle('Images with HUGE Width', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Turns out they are all long artifacts/ accessories\n\nDuring the pre-processing steps, these images should be padded instead of being resized to a square. \n\nOr perhaps a custom pre-processing steps can be applied to images of vastly different size like the weird images above? I will leave that to you to explore."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"img = PIL.Image.open(f'{train_path}{weird_height_id[0]}.png')\n\nw_resized = int(img.size[0] * 300 / img.size[1])\nresized = img.resize((w_resized ,300))\npad_width = 300 - w_resized\npadding = (pad_width // 2, 0, pad_width-(pad_width//2), 0)\nresized_w_pad = ImageOps.expand(resized, padding)\n\nresized_wo_pad = img.resize(size=(300,300))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nplt.subplot(131)\nplt.imshow(img)\nplt.axis('off')\nplt.title('Original Image',fontsize=15)\n\nplt.subplot(132)\nplt.imshow(resized_wo_pad)\nplt.axis('off')\nplt.title('A bent flat head screw?',fontsize=15)\n\nplt.subplot(133)\nplt.imshow(resized_w_pad)\nplt.axis('off')\nplt.title('Padded Image',fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target Variable - Cultures and Tags\n\nBefore we move on to subsequent analysis, let's take a look at the target variable for this challenge. There is a total of **1103 categories**, of which **398 are cultures** and **705 are tags**.\n\n* Cultures - The arts and other manifestations of human intellectual achievement regarded collectively. **Erm, pretty abstract**\n* Tags - A label attached to someone or something for the purpose of identification or to give other information. **Tags as in, you know, Facebook tags**\n\nLet's visualize them with a word cloud!\n\n*Note: The font sizes are arbitary and does not correlate to the category distribution.*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"label_names = label_df['attribute_name'].values\n\nnum_labels = np.zeros((df_train.shape[0],))\ntrain_labels = np.zeros((df_train.shape[0], len(label_names)))\n\nfor row_index, row in enumerate(df_train['attribute_ids']):\n    num_labels[row_index] = len(row.split())    \n    for label in row.split():\n        train_labels[row_index, int(label)] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"culture, tag, unknown = 0, 0, 0\n\nfor l in label_names:\n    if l[:3] == 'cul':\n        culture += 1\n    elif l[:3] == 'tag':\n        tag += 1\n    else:\n        unknown += 1\n        \nprint(f'Culture : {culture}')\nprint(f'Tag     : {tag}')\nprint(f'Unknown : {unknown}')\nprint(f'Total   : {culture + tag + unknown}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"label_sum = np.sum(train_labels, axis=0)\n\nculture_sequence = label_sum[:398].argsort()[::-1]\ntag_sequence = label_sum[398:].argsort()[::-1]\n\nculture_labels = [label_names[x][9:] for x in culture_sequence]\nculture_counts = [label_sum[x] for x in culture_sequence]\n\ntag_labels = [label_names[x + 398][5:] for x in tag_sequence]\ntag_counts = [label_sum[x + 398] for x in tag_sequence]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"culture_labels_dict = dict((l,1) for l in culture_labels)\ntag_labels_dict = dict((l,1) for l in tag_labels)\n\nculture_labels_dict['<CULTURE>'] = 50\ntag_labels_dict['<TAG>'] = 50\n\nculture_cloud = WordCloud(background_color='Black', colormap='Paired', width=1600, height=800, random_state=123).generate_from_frequencies(culture_labels_dict)\ntag_cloud = WordCloud(background_color='Black', colormap='Paired', width=1600, height=800, random_state=123).generate_from_frequencies(tag_labels_dict)\n\nplt.figure(figsize=(24,24))\nplt.subplot(211)\nplt.imshow(culture_cloud,interpolation='bilinear')\nplt.axis('off')\n\nplt.subplot(212)\nplt.imshow(tag_cloud, interpolation='bilinear')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Highly Imbalanced Data\n\nIn the plots below, only the top 20 cultures and tags with the highest counts were shown. You can see that the **20th place** for both label types only accounts for **0.72% (culture)** and **1.83% (tag)** of the entire dataset. In other words, majority of the labels are very rare.\n\n**Culture** : Western cultures are the most common culture labels. This is of no surprise, considering the dataset origins from the Metropolitan Museum of Art in New York. Some of the cultures are subsets of another culture (e.g. London - subset of British, Paris - subset of French). There is even a culture label named 'Turkish or Venice', seems like even the subject matter experts had a hard time labelling these images! \n\n**Tag** : Men, women and flowers have been the most interesting since the beginning of time, thus their dominance on the tag labels. Similar to the culture, there seem to be a significant amount (considering we are only viewing 20 out of 705 tags) of tags with overlapping meaning (e.g. men, women, human figures, portraits, profiles). \n\nTo tackle the challenge effectively, perhaps it is crucial to deal with the similar labels (or those with subset-superset relationship). At this point, I can think of 2 possible methods to deal with them:\n* Using word2vec to represent the label vocab (e.g. french), and check for its cosine similarity with the other labels\n* Using collocation matrix of the train set and see if any sets of labels often appear together (well, I doubt this will work, considering that many images only have 2-5 labels, which is shown in the subsequent plot)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\n\nplt.subplot(1,2,1)\nax1 = sns.barplot(y=culture_labels[:20], x=culture_counts[:20], orient=\"h\")\nplt.title('Label Counts by Culture (Top 20)',fontsize=15)\nplt.xlim((0, max(culture_counts)*1.15))\nplt.yticks(fontsize=15)\n\nfor p in ax1.patches:\n    ax1.annotate(f'{int(p.get_width())}\\n{p.get_width() * 100 / df_train.shape[0]:.2f}%',\n                (p.get_width(), p.get_y() + p.get_height() / 2.), \n                ha='left', \n                va='center', \n                fontsize=12, \n                color='black',\n                xytext=(7,0), \n                textcoords='offset points')\n\nplt.subplot(1,2,2)    \nax2 = sns.barplot(y=tag_labels[:20], x=tag_counts[:20], orient=\"h\")\nplt.title('Label Counts by Tag (Top 20)',fontsize=15)\nplt.xlim((0, max(tag_counts)*1.15))\nplt.yticks(fontsize=15)\n\nfor p in ax2.patches:\n    ax2.annotate(f'{int(p.get_width())}\\n{p.get_width() * 100 / df_train.shape[0]:.2f}%',\n                (p.get_width(), p.get_y() + p.get_height() / 2.), \n                ha='left', \n                va='center', \n                fontsize=12, \n                color='black',\n                xytext=(7,0), \n                textcoords='offset points')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most Images only have 2 - 5 labels.\n\n90% of the images have 2 to 5 labels.\n\nAnd there is an image with 11 labels."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\n\nax = sns.countplot(num_labels)\nplt.xlabel('Number of Labels')\nplt.title('Number of Labels per Image', fontsize=20)\n\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height() * 100 / df_train.shape[0]:.3f}%',\n            (p.get_x() + p.get_width() / 2., p.get_height()), \n            ha='center', \n            va='center', \n            fontsize=11, \n            color='black',\n            xytext=(0,7), \n            textcoords='offset points')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mother of Images - 11 labels on a Single Image\n\nThat has to be some extraordinary confusing image. Lets take a look.\n\n![](https://my350z.com/forum/attachments/exterior-and-interior/435557d1501884683-bensopra-fenders-and-bodykit-thread-mother-of-god-meme_zpsa0e4dcb9.png)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"weird_img_index = np.nonzero(num_labels == 11)[0][0]\n\nimg_w_11_labels_path = df_train.iloc[weird_img_index,0]\nimg_labels = df_train.iloc[weird_img_index,1]\n\nimg = PIL.Image.open(f'{train_path}{img_w_11_labels_path}.png')\n\nprint('LABELS OF IMAGE\\n', *[label_names[int(l)] for l in sorted([int(l) for l in img_labels.split()])],sep='\\n')\n\nplt.figure(figsize=(20,6))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The image doesn't look as weird afterall!\n\nIt looks like a piece of fabric, with multiple types of objects and animals printed on it. \n\nOf the 11 labels, 9 of them belong to 'tags', due to the diversity of the printed patterns. \n\nOnce again, it seems like '**culture**' constitutes something more abstract, like an art style (which can probably be explained by the use of colour, medium etc). On the other hand, '**tag**' is more straightforward, they literally means what you see from the image (shapes, patterns). \n\nAs such, in terms of modelling, I'm guessing that tags can be more easily predicted by CNN based models (compared to culture). I shall do an analysis on the available public kernels to verify this in the future."},{"metadata":{},"cell_type":"markdown","source":"# Onwards to Pixel Statistics!\n\nThe provided images comes in RGB channels, which stands for red, green and blue channel respectively. The pixel value ranges from 0 to 255. The mean and standard deviation for each colour channel is extracted and visualized.\n\n### Normal distributions, with a little skewness\n\nSurprise surprise. I wasn't expecting any normal distributions for the pixel values per channel, let alone 3 normal distributions! Perhaps it is the norm for large enough dataset? Let me know what you think about this, I'm not sure if I'm missing any stuff here."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pal = ['red', 'green', 'blue']\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nsns.kdeplot(df_train['r_mean'], color=pal[0])\nsns.kdeplot(df_train['g_mean'], color=pal[1])\nsns.kdeplot(df_train['b_mean'], color=pal[2])\nplt.ylabel('Density')\nplt.xlabel('Mean Pixel Value')\nplt.title('KDE Plot - Mean Pixel Value by Channel', fontsize=15)\n\nplt.subplot(1,2,2)\nsns.kdeplot(df_train['r_std'], color=pal[0])\nsns.kdeplot(df_train['g_std'], color=pal[1])\nsns.kdeplot(df_train['b_std'], color=pal[2])\nplt.ylabel('Density')\nplt.xlabel('Standard Deviation of Pixel Value')\nplt.title('KDE Plot - Stdev Pixel Value by Channel', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's checkest the REDDEST, GREENEST and BLUEST image."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"reddest = (df_train['r_mean'] - df_train['g_mean'] - df_train['b_mean'] + 255*2)\ngreenest = (df_train['g_mean'] - df_train['r_mean'] - df_train['b_mean'] + 255*2)\nbluest = (df_train['b_mean'] - df_train['g_mean'] - df_train['r_mean'] + 255*2)\n\nreddest_img_path = df_train.iloc[reddest.idxmax(),0]\ngreenest_img_path = df_train.iloc[greenest.idxmax(),0]\nbluest_img_path = df_train.iloc[bluest.idxmax(),0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"reddest_im = PIL.Image.open(f'{train_path}{reddest_img_path}.png')\ngreenest_im = PIL.Image.open(f'{train_path}{greenest_img_path}.png')\nbluest_im = PIL.Image.open(f'{train_path}{bluest_img_path}.png')\n\nplt.figure(figsize=(20,6))\nplt.subplot(131)\nplt.imshow(reddest_im)\nplt.axis('off')\nplt.title('REDDEST glass-like object')\n\nplt.subplot(132)\nplt.imshow(greenest_im)\nplt.axis('off')\nplt.title('GREENEST piece of ancient writing')\n\nplt.subplot(133)\nplt.imshow(bluest_im)\nplt.axis('off')\nplt.title('BLUEST dark sky with some flags')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty interesting, but other than the green piece of ancient writing, the other 2 images are intensely red or blue simply because of their background colour. \n\nDon't think this is very useful for prediction, but we shall see."},{"metadata":{},"cell_type":"markdown","source":"### Edge Characteristics\n\nIn traditional vision domain (pre-CNN era), edge detection of an image is important in helping to identify the shapes or patterns. This can serve as an alternative image representation that we train our models on. But for now, let's check out if its statistics are of any use.\n\nSome samples of the edge version of the images can be seen below. You can see the edges (in white) can approximate the outlines and distinct geometric features of the object in the image. Note that the edge quality can be further improved (for instance, do a gausian blur before applying edge detection, to remove noisy features)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,4))\n\nrandom_image_paths = df_train['id'].sample(n=3, random_state=123).values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(f'{train_path}{path}.png')\n    plt.subplot(1,6, index*2 + 1)\n    plt.imshow(im)\n    plt.axis('off')\n    plt.title('Original')\n\n    plt.subplot(1,6, index*2 + 2)\n    plt.imshow(im.filter(ImageFilter.FIND_EDGES))\n    plt.axis('off')\n    plt.title('Edge Only')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The KDE plots show that the mean and standard deviations of edge pixel values in all 3 channels are almost the same. Thus we will only use a single channel to visualize the 3 sample images (lowest mean edge pixel value, median and highest)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pal = ['red', 'green', 'blue']\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nsns.kdeplot(df_train['r_edge_mean'], color=pal[0])\nsns.kdeplot(df_train['g_edge_mean'], color=pal[1])\nsns.kdeplot(df_train['b_edge_mean'], color=pal[2])\nplt.ylabel('Density')\nplt.xlabel('Mean Pixel Value')\nplt.title('KDE Plot - Mean Pixel Value (Edge) by Channel', fontsize=15)\n\nplt.subplot(1,2,2)\nsns.kdeplot(df_train['r_edge_std'], color=pal[0])\nsns.kdeplot(df_train['g_edge_std'], color=pal[1])\nsns.kdeplot(df_train['b_edge_std'], color=pal[2])\nplt.ylabel('Density')\nplt.xlabel('Standard Deviation of Pixel Value')\nplt.title('KDE Plot - Stdev Pixel Value (Edge) by Channel', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"edge_min, edge_median, edge_max = df_train['r_edge_mean'].min(), df_train['r_edge_mean'].median(), df_train['r_edge_mean'].max()\n\nlow_mean_edge = df_train.loc[df_train['r_edge_mean'] == edge_min, 'id'].values[0]\nmed_mean_edge = df_train.loc[df_train['r_edge_mean'] == edge_median, 'id'].values[0]\nhigh_mean_edge = df_train.loc[df_train['r_edge_mean'] == edge_max, 'id'].values[0]\n\nlow_mean_edge_im = PIL.Image.open(f'{train_path}{low_mean_edge}.png')\nmed_mean_edge_im = PIL.Image.open(f'{train_path}{med_mean_edge}.png')\nhigh_mean_edge_im = PIL.Image.open(f'{train_path}{high_mean_edge}.png')\n\nplt.figure(figsize=(20,16))\nplt.subplot(231)\nplt.imshow(low_mean_edge_im)\nplt.axis('off')\nplt.title(f'Mean Edge = {edge_min:.2f} (Raw)')\n\nplt.subplot(232)\nplt.imshow(med_mean_edge_im)\nplt.axis('off')\nplt.title(f'Mean Edge = {edge_median:.2f} (Raw)')\n\nplt.subplot(233)\nplt.imshow(high_mean_edge_im)\nplt.axis('off')\nplt.title(f'Mean Edge = {edge_max:.2f} (Raw)')\n\nplt.subplot(234)\nplt.imshow(low_mean_edge_im.filter(ImageFilter.FIND_EDGES))\nplt.axis('off')\nplt.title(f'Mean Edge = {edge_min:.2f} (Edge)')\n\nplt.subplot(235)\nplt.imshow(med_mean_edge_im.filter(ImageFilter.FIND_EDGES))\nplt.axis('off')\nplt.title(f'Mean Edge = {edge_median:.2f} (Edge)')\n\nplt.subplot(236)\nplt.imshow(high_mean_edge_im.filter(ImageFilter.FIND_EDGES))\nplt.axis('off')\nplt.title(f'Mean Edge = {edge_max:.2f} (Edge)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spooky!\n\nThe image with the highest mean edge pixel value (right most) seems like some sort of optical illusion."},{"metadata":{},"cell_type":"markdown","source":"# To be continued!\n\n### Do give me an upvote if you find it insightful/ interesting! :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}