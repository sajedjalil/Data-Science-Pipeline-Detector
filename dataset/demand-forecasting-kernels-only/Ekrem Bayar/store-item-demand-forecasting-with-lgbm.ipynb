{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src='https://storage.googleapis.com/kaggle-competitions/kaggle/9999/logos/header.png?t=2018-06-28-21-19-41' />\n\n# Store Item Demand Forecasting\n\n**Description**\n\nThis competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.\n\nYou are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n\nWhat's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?\n\n**Evaluation**\n\nSubmissions are evaluated on [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) between forecasts and actual values. We define [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) = 0 when the actual and predicted values are both 0.\n\n**Variables:**\n- date\n- store\n- item\n- sales","metadata":{}},{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"# Base\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model\nimport lightgbm as lgb\nimport shap\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n\n# Configuration\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:21.227587Z","iopub.execute_input":"2021-06-02T17:49:21.22805Z","iopub.status.idle":"2021-06-02T17:49:21.232277Z","shell.execute_reply.started":"2021-06-02T17:49:21.22802Z","shell.execute_reply":"2021-06-02T17:49:21.231696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])\ntest = pd.read_csv('../input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'])\n# sample_sub = pd.read_csv('./input/demand-forecasting-kernels-only/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)\n\nprint(train.shape, test.shape, df.shape, \"\\n\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:21.469406Z","iopub.execute_input":"2021-06-02T17:49:21.469993Z","iopub.status.idle":"2021-06-02T17:49:21.829895Z","shell.execute_reply.started":"2021-06-02T17:49:21.469959Z","shell.execute_reply":"2021-06-02T17:49:21.82897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# How many stores and items are there?\ntrain.store.nunique(), test.store.nunique(), train.item.nunique(), test.item.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time Range\ntrain[\"date\"].min(), train[\"date\"].max(), test[\"date\"].min(), test[\"date\"].max()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:22.001557Z","iopub.execute_input":"2021-06-02T17:49:22.001904Z","iopub.status.idle":"2021-06-02T17:49:22.014995Z","shell.execute_reply.started":"2021-06-02T17:49:22.001874Z","shell.execute_reply":"2021-06-02T17:49:22.014093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many items are in the store?\ndf.groupby([\"store\"])[\"item\"].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:22.177828Z","iopub.execute_input":"2021-06-02T17:49:22.178152Z","iopub.status.idle":"2021-06-02T17:49:22.23729Z","shell.execute_reply.started":"2021-06-02T17:49:22.178124Z","shell.execute_reply":"2021-06-02T17:49:22.236312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary Stats for each store\ndf.groupby([\"store\"]).agg({\"sales\": [\"count\",\"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:22.362973Z","iopub.execute_input":"2021-06-02T17:49:22.363333Z","iopub.status.idle":"2021-06-02T17:49:22.497321Z","shell.execute_reply.started":"2021-06-02T17:49:22.363304Z","shell.execute_reply":"2021-06-02T17:49:22.4965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Summary Stats for each item\ndf.groupby([\"item\"]).agg({\"sales\": [\"count\",\"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:22.525996Z","iopub.execute_input":"2021-06-02T17:49:22.526314Z","iopub.status.idle":"2021-06-02T17:49:22.62926Z","shell.execute_reply.started":"2021-06-02T17:49:22.526286Z","shell.execute_reply":"2021-06-02T17:49:22.628281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Histogram: Store Sales","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 5, figsize=(20, 10))\nfor i in range(1,11):\n    if i < 6:\n        train[train.store == i].sales.hist(ax=axes[0, i-1])\n        axes[0,i-1].set_title(\"Store \" + str(i), fontsize = 15)\n        \n    else:\n        train[train.store == i].sales.hist(ax=axes[1, i - 6])\n        axes[1,i-6].set_title(\"Store \" + str(i), fontsize = 15)\nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Histogram: Sales\");","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:22.818446Z","iopub.execute_input":"2021-06-02T17:49:22.818801Z","iopub.status.idle":"2021-06-02T17:49:24.049085Z","shell.execute_reply.started":"2021-06-02T17:49:22.818769Z","shell.execute_reply":"2021-06-02T17:49:24.048164Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sales distribution for each item in the 1st store","metadata":{}},{"cell_type":"code","source":"store = 1\nsub = train[train.store == store].set_index(\"date\")\n\nfig, axes = plt.subplots(10, 5, figsize=(20, 35))\nfor i in range(1,51):\n    if i < 6:\n        sub[sub.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n    if i >= 6 and i<11:\n        sub[sub.item == i].sales.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n    if i >= 11 and i<16:\n        sub[sub.item == i].sales.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 16 and i<21:\n        sub[sub.item == i].sales.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 21 and i<26:\n        sub[sub.item == i].sales.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\")  \n    if i >= 26 and i<31:\n        sub[sub.item == i].sales.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 31 and i<36:\n        sub[sub.item == i].sales.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 36 and i<41:\n        sub[sub.item == i].sales.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 41 and i<46:\n        sub[sub.item == i].sales.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\") \n    if i >= 46 and i<51:\n        sub[sub.item == i].sales.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\") \nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Store 1 Item Satış Dağılımı\");","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:24.050592Z","iopub.execute_input":"2021-06-02T17:49:24.050925Z","iopub.status.idle":"2021-06-02T17:49:34.770245Z","shell.execute_reply.started":"2021-06-02T17:49:24.050893Z","shell.execute_reply":"2021-06-02T17:49:34.76912Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation between total sales of stores","metadata":{}},{"cell_type":"code","source":"storesales = train.groupby([\"date\", \"store\"]).sales.sum().reset_index().set_index(\"date\")\ncorr =  pd.pivot_table(storesales, values = \"sales\", columns=\"store\", index=\"date\").corr(method = \"spearman\")\nplt.figure(figsize = (7,7))\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.5)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 9}, square=True);","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:34.772243Z","iopub.execute_input":"2021-06-02T17:49:34.772634Z","iopub.status.idle":"2021-06-02T17:49:35.45833Z","shell.execute_reply.started":"2021-06-02T17:49:34.772596Z","shell.execute_reply":"2021-06-02T17:49:35.45737Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hypothesis Testing\n\n##### Stores","metadata":{}},{"cell_type":"code","source":"# T Test\ndef CompareTwoGroups(dataframe, group, target):\n    \n    import itertools\n    from scipy.stats import shapiro\n    import scipy.stats as stats\n    \n    # 1. Normality Test: Shapiro Test\n    # 2. Homogeneity Test: Levene Test\n    # 3. Parametric or Non-Parametric T Test: T-Test, Welch Test, Mann Whitney U\n    \n    # Create Combinations\n    item_comb = list(itertools.combinations(dataframe[group].unique(), 2))\n    \n    AB = pd.DataFrame()\n    for i in range(0, len(item_comb)):\n        # Define Groups\n        groupA = dataframe[dataframe[group] == item_comb[i][0]][target]\n        groupB = dataframe[dataframe[group] == item_comb[i][1]][target]\n        \n        # Assumption: Normality\n        ntA = shapiro(groupA)[1] < 0.05\n        ntB = shapiro(groupB)[1] < 0.05\n        # H0: Distribution is Normal! - False\n        # H1: Distribution is not Normal! - True\n        \n        if (ntA == False) & (ntB == False): # \"H0: Normal Distribution\"\n            # Parametric Test\n            # Assumption: Homogeneity of variances\n            leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n            # H0: Homogeneity: False\n            # H1: Heterogeneous: True\n            if leveneTest == False:\n                # Homogeneity\n                ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n                # H0: M1 = M2 - False\n                # H1: M1 != M2 - True\n            else:\n                # Heterogeneous\n                ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n                # H0: M1 = M2 - False\n                # H1: M1 != M2 - True\n        else:\n            # Non-Parametric Test\n            ttest = stats.mannwhitneyu(groupA, groupB)[1] \n            # H0: M1 = M2 - False\n            # H1: M1 != M2 - True\n            \n        temp = pd.DataFrame({\"Compare Two Groups\":[ttest < 0.05], \n                             \"p-value\":[ttest],\n                             \"GroupA_Mean\":[groupA.mean()], \"GroupB_Mean\":[groupB.mean()],\n                             \"GroupA_Median\":[groupA.median()], \"GroupB_Median\":[groupB.median()],\n                             \"GroupA_Count\":[groupA.count()], \"GroupB_Count\":[groupB.count()]\n                            }, index = [item_comb[i]])\n        temp[\"Compare Two Groups\"] = np.where(temp[\"Compare Two Groups\"] == True, \"Different Groups\", \"Similar Groups\")\n        temp[\"TestType\"] = np.where((ntA == False) & (ntB == False), \"Parametric\", \"Non-Parametric\")\n        \n        AB = pd.concat([AB, temp[[\"TestType\", \"Compare Two Groups\", \"p-value\",\"GroupA_Median\", \"GroupB_Median\",\"GroupA_Mean\", \"GroupB_Mean\",\n                                 \"GroupA_Count\", \"GroupB_Count\"]]])\n        \n    return AB\n    \n    \nCompareTwoGroups(storesales, group = \"store\", target = \"sales\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:35.45983Z","iopub.execute_input":"2021-06-02T17:49:35.460089Z","iopub.status.idle":"2021-06-02T17:49:35.816857Z","shell.execute_reply.started":"2021-06-02T17:49:35.460064Z","shell.execute_reply":"2021-06-02T17:49:35.815876Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Items","metadata":{}},{"cell_type":"code","source":"itemsales = train.groupby([\"date\", \"item\"]).sales.sum().reset_index().set_index(\"date\")\nctg_is = CompareTwoGroups(itemsales, group = \"item\", target = \"sales\")\nctg_is[ctg_is[\"Compare Two Groups\"] == \"Similar Groups\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:35.818274Z","iopub.execute_input":"2021-06-02T17:49:35.818648Z","iopub.status.idle":"2021-06-02T17:49:44.705025Z","shell.execute_reply.started":"2021-06-02T17:49:35.818605Z","shell.execute_reply":"2021-06-02T17:49:44.70393Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\n- Time Related Features\n- Lagged Features\n- Moving Average Features\n- Hypothesis Testing: Similarity Features\n- Exponentially Weighted Mean Features","metadata":{}},{"cell_type":"code","source":"# 1. Time Related Features\n#####################################################\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek + 1\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday // 4\n    df[\"quarter\"] = df.date.dt.quarter\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(int)\n    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(int)\n    df['is_year_start'] = df.date.dt.is_year_start.astype(int)\n    df['is_year_end'] = df.date.dt.is_year_end.astype(int)\n    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n    df[\"season\"] = np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])\n    return df\ndf = create_date_features(df)\n\n\n# Rolling Summary Stats Features\n#####################################################\nfor i in [91, 98, 105, 112, 119, 126, 186, 200, 210, 250, 300, 365, 546, 700]:\n    df[\"sales_roll_mean_\"+str(i)]=df.groupby([\"store\", \"item\"]).sales.rolling(i).mean().shift(1).values\n    #df[\"sales_roll_std_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).std().shift(1).values\n    #df[\"sales_roll_max_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).max().shift(1).values\n    #df[\"sales_roll_min_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).min().shift(1).values\n\n\n# 2. Hypothesis Testing: Similarity\n#####################################################\n\n# Store Based\nstoresales = train.groupby([\"date\", \"store\"]).sales.sum().reset_index()\nctg_ss = CompareTwoGroups(storesales, group=\"store\", target=\"sales\")\ndel storesales\n\ndf[\"StoreSalesSimilarity\"] = np.where(df.store.isin([3,10]), 1, 0)\ndf[\"StoreSalesSimilarity\"] = np.where(df.store.isin([4,9]), 2, df[\"StoreSalesSimilarity\"])\ndf[\"StoreSalesSimilarity\"] = np.where(df.store.isin([5,6]), 3, df[\"StoreSalesSimilarity\"])\n\n# Item Based\n\nitemsales = train.groupby([\"date\", \"item\"]).sales.sum().reset_index()\nctg_is = CompareTwoGroups(itemsales, group = \"item\", target = \"sales\")\ndel itemsales\n\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([1,4,27,41,47]), 1, 0)\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([2,6,7,14,31,46]), 2, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([3,42]), 3, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([8,36]), 4, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([9,43,48]), 5, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([11,12,29,33]), 6, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([13,18]), 7, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([15,28]), 8, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([16,34]), 9, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([19,21,30]), 10, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([20,26]), 11, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([22,25,38,45]), 12, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([23,37,40,44,49]), 13, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([24,35,50]), 14, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([32,39]), 15, df[\"ItemSalesSimilarity\"])\n\n# 3. Lag/Shifted Features\n#####################################################\n\n# test.groupby([\"store\", \"item\"]).date.count()\n# Test verisinde +90 gün tahmin edilmesi isteniyor bu yüzden\n# Lag featureları en az 91 olmalı!\n\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\ndef lag_features(dataframe, lags, groups = [\"store\", \"item\"], target = \"sales\", prefix = ''):\n    dataframe = dataframe.copy()\n    for lag in lags:\n        dataframe[prefix + str(lag)] = dataframe.groupby(groups)[target].transform(\n            lambda x: x.shift(lag))\n    return dataframe\n\ndf = lag_features(df, lags = [91, 92,93,94,95,96, 97, 98, 100, 105, 112, 119, 126, 150,\n                              182,200,220, 250, 300, 350, 355, 360,361,362,363, 364,\n                              365, 370, 375,380, 546, 600, 650, 680, 690, 700, 710, 728,\n                              730, 800, 900, 950, 990, 1000, 1050, 1090, 1095],\n                  groups = [\"store\", \"item\"], target = 'sales', prefix = 'sales_lag_')\n\ndef drop_cor(dataframe, name, index):\n    ind = dataframe[dataframe.columns[dataframe.columns.str.contains(name)].tolist()+[\"sales\"]].corr().sales.sort_values(ascending = False).index[1:index]\n    ind = dataframe.drop(ind, axis = 1).columns[dataframe.drop(ind, axis = 1).columns.str.contains(name)]\n    dataframe.drop(ind, axis = 1, inplace = True)\n\ndrop_cor(df, \"sales_lag\", 16)\n\n\n# 4. Last i. Months\n#####################################################\ndf[\"monthyear\"] = df.date.dt.to_period('M')\n\n# Store-Item Based\nfor i in [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36]:\n    last_months = df.groupby([\"store\", \"item\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['store', 'item', 'monthyear', 'last_'+str(i)+'months_sales_sum',\n                           'last_'+str(i)+'months_sales_mean', 'last_'+str(i)+'months_sales_std',\n                           'last_'+str(i)+'months_sales_min', 'last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how   = \"left\", on = [\"store\", \"item\", \"monthyear\"])\ndel last_months, i\n\ndrop_cor(df, \"last_\", 15)\n\n# Store Based\n\n\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"store\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['store', 'monthyear', 'store_last_'+str(i)+'months_sales_sum',\n                           'store_last_'+str(i)+'months_sales_mean', 'store_last_'+str(i)+'months_sales_std',\n                           'store_last_'+str(i)+'months_sales_min', 'store_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"store\", \"monthyear\"])\ndel last_months, i\n\n# Item Based\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"item\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['item', 'monthyear', 'item_last_'+str(i)+'months_sales_sum',\n                           'item_last_'+str(i)+'months_sales_mean', 'item_last_'+str(i)+'months_sales_std',\n                           'item_last_'+str(i)+'months_sales_min', 'item_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"item\", \"monthyear\"])\ndel last_months, i\n\n# Similarity Based\n\n\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"StoreSalesSimilarity\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['StoreSalesSimilarity', 'monthyear', 'storesim_last_'+str(i)+'months_sales_sum',\n                           'storesim_last_'+str(i)+'months_sales_mean', 'storesim_last_'+str(i)+'months_sales_std',\n                           'storesim_last_'+str(i)+'months_sales_min', 'storesim_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"StoreSalesSimilarity\", \"monthyear\"])\ndel last_months, i\n\n\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"ItemSalesSimilarity\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['ItemSalesSimilarity', 'monthyear', 'itemsim_last_'+str(i)+'months_sales_sum',\n                           'itemsim_last_'+str(i)+'months_sales_mean', 'itemsim_last_'+str(i)+'months_sales_std',\n                           'itemsim_last_'+str(i)+'months_sales_min', 'itemsim_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"ItemSalesSimilarity\", \"monthyear\"])\ndel last_months, i\n\ndf.drop(\"monthyear\", axis = 1, inplace = True)\n\n\n# 5. Last i. day of week\n#####################################################\ndf.sort_values([\"store\", \"item\", \"day_of_week\", \"date\"], inplace = True)\n\ndf = lag_features(df, lags = np.arange(12,41, 1).tolist()+[91, 92, 95, 98, 99, 100, 105, 112, 119, 126, 133, 140, 200, 205, 210, 215, 220, 250],\n                  groups = [\"store\", \"item\", \"day_of_week\"], target = 'sales', prefix = 'dayofweek_sales_lag_')\n\ndf[df.columns[df.columns.str.contains(\"dayofweek_sales_lag_\")].tolist()+[\"sales\"]].corr().sales.sort_values(ascending = False)\n\ndrop_cor(df, \"dayofweek_sales_lag_\", 16)\n\ndf.sort_values([\"store\", \"item\", \"date\"], inplace = True)\n\n\n#####################################################\n# Exponentially Weighted Mean Features\n#####################################################\ndef ewm_features(dataframe, alphas, lags):\n    dataframe = dataframe.copy()\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\n\n# Day of year \ndf.sort_values([\"day_of_year\", \"store\", \"item\"], inplace = True)\ndf = lag_features(df, lags = [1,2,3,4],\n                  groups = [\"day_of_year\", \"store\", \"item\"], target = 'sales', prefix = 'dayofyear_sales_lag_')\n\n\n# pd.cut\nclus = df.groupby([\"store\"]).sales.mean().reset_index()\nclus[\"store_cluster\"] =  pd.cut(clus.sales, bins = 4, labels = range(1,5))\nclus.drop(\"sales\", axis = 1, inplace = True)\ndf = pd.merge(df, clus, how = \"left\")\nclus = df.groupby([\"item\"]).sales.mean().reset_index()\nclus[\"item_cluster\"] =  pd.cut(clus.sales, bins = 5, labels = range(1,6))\nclus.drop(\"sales\", axis = 1, inplace = True)\ndf = pd.merge(df, clus, how = \"left\")\ndel clus\n\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-02T17:49:44.706783Z","iopub.execute_input":"2021-06-02T17:49:44.70716Z","iopub.status.idle":"2021-06-02T17:54:50.260855Z","shell.execute_reply.started":"2021-06-02T17:49:44.707119Z","shell.execute_reply":"2021-06-02T17:54:50.259919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-Validation Split","metadata":{}},{"cell_type":"code","source":"# Dataframe must be sorted by date because of Time Series Split \ndf = df.sort_values(\"date\").reset_index(drop = True)\n\n# Train Validation Split\n# Validation set includes 3 months (Oct. Nov. Dec. 2017)\ntrain = df.loc[(df[\"date\"] < \"2017-10-01\"), :]\nval = df.loc[(df[\"date\"] >= \"2017-10-01\") & (df[\"date\"] < \"2018-01-01\"), :]\n\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Cost Function\n\nIf you want to see the SMAPE formula, click [here](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error).","metadata":{}},{"cell_type":"code","source":"# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num/denom))/n\n    return smape_val\n\ndef lgbm_smape(y_true, y_pred):\n    smape_val = smape(y_true, y_pred)\n    return 'SMAPE', smape_val, False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Model\n\n### Default Parameters","metadata":{}},{"cell_type":"code","source":"first_model = lgb.LGBMRegressor(random_state=384).fit(X_train, Y_train, \n                                                      eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, first_model.predict(X_train)))\nprint(\"VALID SMAPE:\", smape(Y_val, first_model.predict(X_val)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Importance","metadata":{}},{"cell_type":"code","source":"\ndef plot_lgb_importances(model, plot=False, num=10):\n    from matplotlib import pyplot as plt\n    import seaborn as sns\n    \n    # LGBM API\n    #gain = model.feature_importance('gain')\n    #feat_imp = pd.DataFrame({'feature': model.feature_name(),\n    #                         'split': model.feature_importance('split'),\n    #                         'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n    \n    # SKLEARN API\n    gain = model.booster_.feature_importance(importance_type='gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name_,\n                             'split': model.booster_.feature_importance(importance_type='split'),\n                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n        return feat_imp\n\nfeature_imp_df = plot_lgb_importances(first_model, num=50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp_df.shape, feature_imp_df[feature_imp_df.gain > 0].shape, feature_imp_df[feature_imp_df.gain > 0.57].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_lgb_importances(first_model, plot=True, num=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Shap","metadata":{}},{"cell_type":"code","source":"explainer = shap.Explainer(first_model)\nshap_values_train = explainer(X_train)\nshap_values_valid = explainer(X_val)\n\nlen(shap_values_train), len(shap_values_valid)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize the effects of all the features\nshap.plots.beeswarm(shap_values_train, max_display=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize the effects of all the features\nshap.plots.beeswarm(shap_values_valid, max_display=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.bar(shap_values_train, max_display=30)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Error Analysis","metadata":{}},{"cell_type":"code","source":"error = pd.DataFrame({\n    \"date\":val.date,\n    \"store\":X_val.store,\n    \"item\":X_val.item,\n    \"actual\":Y_val,\n    \"pred\":first_model.predict(X_val)\n}).reset_index(drop = True)\n\nerror[\"error\"] = np.abs(error.actual-error.pred)\n\nerror.sort_values(\"error\", ascending=False).head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error[[\"actual\", \"pred\", \"error\"]].describe([0.7, 0.8, 0.9, 0.95, 0.99]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.waterfall(shap_values_valid[30125])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the first prediction's explanation with a force plot\nshap.initjs()\nshap.plots.force(shap_values_valid[30125])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.waterfall(shap_values_valid[20669])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the first prediction's explanation with a force plot\nshap.initjs()\nshap.plots.force(shap_values_valid[20669])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.waterfall(shap_values_valid[9009])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the first prediction's explanation with a force plot\nshap.initjs()\nshap.plots.force(shap_values_valid[9009])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean Absolute Error\nerror.groupby([\"store\", \"item\"]).error.mean().sort_values(ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean Absolute Error\nerror.groupby([\"store\"]).error.mean().sort_values(ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean Absolute Error\nerror.groupby([\"item\"]).error.mean().sort_values(ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Store 1 Validation Set: Actual & Pred","metadata":{}},{"cell_type":"code","source":"# Store 1 Actual - Pred\nsub = error[error.store == 1].set_index(\"date\")\nfig, axes = plt.subplots(10, 5, figsize=(20, 35))\nfor i in range(1,51):\n    if i < 6:\n        sub[sub.item == i].actual.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[0, i - 1], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle = \"dashed\")\n    if i >= 6 and i<11:\n        sub[sub.item == i].actual.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[1, i - 6], legend=True, label=\"Item \" + str(i) + \" Pred\",  linestyle=\"dashed\")\n    if i >= 11 and i<16:\n        sub[sub.item == i].actual.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[2, i - 11], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 16 and i<21:\n        sub[sub.item == i].actual.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[3, i - 16], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 21 and i<26:\n        sub[sub.item == i].actual.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[4, i - 21], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 26 and i<31:\n        sub[sub.item == i].actual.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[5, i - 26], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 31 and i<36:\n        sub[sub.item == i].actual.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[6, i - 31], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 36 and i<41:\n        sub[sub.item == i].actual.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[7, i - 36], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 41 and i<46:\n        sub[sub.item == i].actual.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[8, i - 41], legend=True, label=\"Item \" + str(i) + \" Pred\",linestyle=\"dashed\")\n    if i >= 46 and i<51:\n        sub[sub.item == i].actual.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[9, i - 46], legend=True, label=\"Item \" + str(i) + \" Pred\",linestyle=\"dashed\")\nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Store 1 Item Satış Dağılımı\");\nplt.show()","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(4, 2, figsize = (20,20))\nfor axi in axes.flat:\n    axi.ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0,10))\n    axi.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0,10))\n    axi.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n    axi.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n    \n(error.actual-error.pred).hist(ax = axes[0, 0], color = \"steelblue\", bins = 20)\nerror.error.hist(ax = axes[0,1], color = \"steelblue\", bins = 20)\nsr = error.copy()\nsr[\"StandardizedR\"] = (sr.error / (sr.actual-sr.pred).std())\nsr[\"StandardizedR2\"] = ((sr.error / (sr.actual-sr.pred).std())**2)\nsr.plot.scatter(x = \"pred\",y = \"StandardizedR\", color = \"red\", ax = axes[1,0])\nsr.plot.scatter(x = \"pred\",y = \"StandardizedR2\", color = \"red\", ax = axes[1,1])\nerror.actual.hist(ax = axes[2, 0], color = \"purple\", bins = 20)\nerror.pred.hist(ax = axes[2, 1], color = \"purple\", bins = 20)\nerror.plot.scatter(x = \"actual\",y = \"pred\", color = \"seagreen\", ax = axes[3,0]);\n# QQ Plot\nimport statsmodels.api as sm\nimport pylab\nsm.qqplot(sr.pred, ax = axes[3,1], c = \"seagreen\")\nplt.suptitle(\"ERROR ANALYSIS\", fontsize = 20)\naxes[0,0].set_title(\"Error Histogram\", fontsize = 15)\naxes[0,1].set_title(\"Absolute Error Histogram\", fontsize = 15)\naxes[1,0].set_title(\"Standardized Residuals & Fitted Values\", fontsize = 15)\naxes[1,1].set_title(\"Standardized Residuals^2 & Fitted Values\", fontsize = 15)\naxes[2,0].set_title(\"Actual Histogram\", fontsize = 15)\naxes[2,1].set_title(\"Pred Histogram\", fontsize = 15);\naxes[3,0].set_title(\"Actual Pred Relationship\", fontsize = 15);\naxes[3,1].set_title(\"QQ Plot\", fontsize = 15);\naxes[1,0].set_xlabel(\"Fitted Values (Pred)\", fontsize = 12)\naxes[1,1].set_xlabel(\"Fitted Values (Pred)\", fontsize = 12)\naxes[3,0].set_xlabel(\"Actual\", fontsize = 12)\naxes[1,0].set_ylabel(\"Standardized Residuals\", fontsize = 12)\naxes[1,1].set_ylabel(\"Standardized Residuals^2\", fontsize = 12)\naxes[3,0].set_ylabel(\"Pred\", fontsize = 12)\nfig.tight_layout(pad=3.0)\nplt.savefig(\"errors.png\")\nplt.show()","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Model\n\n### Default Parameters & Feature Selection with LGBM Feature Importance","metadata":{}},{"cell_type":"code","source":"# First model feature importance\ncols = feature_imp_df[feature_imp_df.gain > 0.015].feature.tolist()\nprint(\"Independent Variables:\", len(cols))\n\nsecond_model = lgb.LGBMRegressor(random_state=384).fit(\n    X_train[cols], Y_train, \n    eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, second_model.predict(X_train[cols])))\nprint(\"VALID SMAPE:\", smape(Y_val, second_model.predict(X_val[cols])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**First Model Scores**\n\n- TRAIN SMAPE: 13.1559\n- VALID SMAPE: 12.7387\n- 200 Features\n\n**Second Model Scores**\n- TRAIN SMAPE: 13.1598\n- VALID SMAPE: 12.7291\n- 89 Features","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\nThere are two steps to tune LGBM models!\n\n- **1st Optimization:** Finding other parameters when the number of iterations is constant (GridSearchedCV, RandomSearchedCV etc.)\n- **2nd Optimization:** Finding best iteration number by using early stopping round\n\n**Hyperparameter tuning takes too long because of high iteration number and data dimension, that's why I add the Random Search CV algorithm below as text.**","metadata":{}},{"cell_type":"markdown","source":"#### First Optimization: Hyperparameter Tuning with Random Searched\nlgbm_params = {\n    \n    \"num_leaves\":[20,31], # Default 31\n    \"max_depth\":[-1, 20, 30], # Default -1\n    \"learning_rate\":[0.1, 0.05], # Default 0.1\n    \"n_estimators\":[10000,15000], # Default 100\n    \"min_split_gain\":[0.0, 2,5], # Default 0\n    \"min_child_samples\":[10, 20, 30], # Default 20\n    \"colsample_bytree\":[0.5, 0.8, 1.0], # Default 1\n    \"reg_alpha\":[0.0, 0.5, 1], # Default 0\n    \"reg_lambda\":[0.0, 0.5, 1] # Default 0\n}\n\nmodel = lgb.LGBMRegressor(random_state=384)\n\n- from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n- from sklearn.metrics import make_scorer\n\ntscv = TimeSeriesSplit(n_splits=3)\n\nrsearch = RandomizedSearchCV(model, lgbm_params, random_state=384, \n                             cv=tscv, scoring=make_scorer(smape),\n                             verbose = True, n_jobs = -1).fit(\n    X_train[cols], Y_train\n)\n\nprint(rsearch.best_params_)\n\n{'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n\n**When Random Searched CV finished running, it gives us best parameters in sample paramater space. Then, we should train a new model with best parameters and evaluate model performance.**\n\nmodel_tuned = lgb.LGBMRegressor(**rsearch.best_params_, random_state=384).fit(X_train[cols], Y_train)\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, model_tuned.predict(X_train[cols])))\n\nprint(\"VALID SMAPE:\", smape(Y_val, model_tuned.predict(X_val[cols])))\n\n- TRAIN SMAPE: 9.424761658139554\n- VALID SMAPE: 12.784089823495902","metadata":{}},{"cell_type":"markdown","source":"\n\n**First Model Scores**\n\n- TRAIN SMAPE: 13.1559\n- VALID SMAPE: 12.7387\n- 200 Features\n- Default hyperparameters\n\n**Second Model Scores**\n\n- TRAIN SMAPE: 13.1598\n- VALID SMAPE: 12.7291\n- 89 Features\n- Default hyperparameters\n\n**Third Model Scores**\n\n- TRAIN SMAPE: 9.4247\n- VALID SMAPE: 12.7840\n- 89 Features\n- Hyperparameter Tuning: Random Searched CV \n- {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n- Overfitting","metadata":{}},{"cell_type":"markdown","source":"#### Second Optimization: Find best iteration number","metadata":{}},{"cell_type":"code","source":"# Best Params: {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n# model_tuned2 = lgb.LGBMRegressor(**rsearch.best_params_, random_state=384, metric = \"custom\")\n\nmodel_tuned2 = lgb.LGBMRegressor(num_leaves=31, n_estimators=15000, max_depth=20, random_state=384, metric = \"custom\")\n              \nmodel_tuned2.fit(\n    X_train[cols], Y_train,\n    eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)],\n    eval_set = [(X_train[cols], Y_train), (X_val[cols], Y_val)],\n    eval_names = [\"Train\", \"Valid\"],\n    early_stopping_rounds= 1000, verbose = 500\n)\nprint(\"Best Iteration:\", model_tuned2.booster_.best_iteration)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**First Model Scores**\n\n- TRAIN SMAPE: 13.1559\n- VALID SMAPE: 12.7387\n- 200 Features\n- Default hyperparameters\n\n**Second Model Scores**\n\n- TRAIN SMAPE: 13.1598\n- VALID SMAPE: 12.7291\n- 89 Features\n- Default hyperparameters\n\n**Third Model Scores**\n\n- TRAIN SMAPE: 9.4247\n- VALID SMAPE: 12.7840\n- 89 Features\n- Hyperparameter Tuning: Random Searched CV \n- {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n- Overfitting\n\n**Fourth Model Scores**\n\n- TRAIN SMAPE: 12.5183\n- VALID SMAPE: 12.4974\n- 89 Features\n- Hyperparameter Tuning: Random Searched CV \n- Best iteration number: 983\n\n","metadata":{}},{"cell_type":"markdown","source":"# Final Model","metadata":{}},{"cell_type":"code","source":"df.sort_values([\"store\", \"item\", \"date\"], inplace = True)\n\ntrain_final = df.loc[(df[\"date\"] < \"2018-01-01\"), :]\ntest_final = df.loc[(df[\"date\"] >= \"2018-01-01\"), :]\n\nX_train_final = train_final[cols]\nY_train_final = train_final.sales\nX_test_final = test_final[cols]\n\n\n#final_model = lgb.LGBMRegressor(**rsearch.best_params_, random_state=384, metric = \"custom\") # Tuned parameters\n# Best Params: {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\nfinal_model = lgb.LGBMRegressor(num_leaves=31, n_estimators=15000, max_depth=20, random_state=384, metric = \"custom\")\nfinal_model.set_params(n_estimators=model_tuned2.booster_.best_iteration) # Best Iteration: 983\nfinal_model.fit(X_train_final[cols], Y_train_final,\n                eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\":test_final.id.astype(int),\n    \"sales\":final_model.predict(X_test_final)\n})\nsubmission.to_csv(\"submission.csv\", index = None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[[\"sales\"]].describe([0.1, 0.75, 0.8, 0.9, 0.95, 0.99]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.sales.hist(color = \"g\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = pd.DataFrame({\n    \"date\":test_final.date,\n    \"store\":test_final.store,\n    \"item\":test_final.item,\n    \"sales\":final_model.predict(X_test_final)\n})\n\nforecast[(forecast.store == 1) & (forecast.item == 1)].set_index(\"date\").sales.plot(color = \"orange\", figsize = (20,9),legend=True, label = \"Store 1 Item 1 Forecast\");","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_final[(train_final.store == 1) & (train_final.item == 17)].set_index(\"date\").sales.plot(figsize = (20,9),legend=True, label = \"Store 1 Item 1 Sales\")\nforecast[(forecast.store == 1) & (forecast.item == 17)].set_index(\"date\").sales.plot(legend=True, label = \"Store 1 Item 1 Forecast\");","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store = 1\nsub = train[train.store == store].set_index(\"date\")\nforc = forecast[forecast.store == store].set_index(\"date\")\n\n\nfig, axes = plt.subplots(10, 5, figsize=(20, 35))\nfor i in range(1,51):\n    if i < 6:\n        sub[sub.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Forecast\")\n    if i >= 6 and i<11:\n        sub[sub.item == i].sales.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[1, i-6], legend=True, label = \"Forecast\")\n    if i >= 11 and i<16:\n        sub[sub.item == i].sales.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[2, i-11], legend=True, label = \"Forecast\")\n    if i >= 16 and i<21:\n        sub[sub.item == i].sales.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n        forc[forc.item == i].sales.plot(ax=axes[3, i-16], legend=True, label = \"Forecast\")\n    if i >= 21 and i<26:\n        sub[sub.item == i].sales.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[4, i-21], legend=True, label = \"Forecast\")\n    if i >= 26 and i<31:\n        sub[sub.item == i].sales.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[5, i-26], legend=True, label = \"Forecast\")\n    if i >= 31 and i<36:\n        sub[sub.item == i].sales.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")  \n        forc[forc.item == i].sales.plot(ax=axes[6, i-31], legend=True, label = \"Forecast\")\n    if i >= 36 and i<41:\n        sub[sub.item == i].sales.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[7, i-36], legend=True, label = \"Forecast\")\n    if i >= 41 and i<46:\n        sub[sub.item == i].sales.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[8, i-41], legend=True, label = \"Forecast\")\n    if i >= 46 and i<51:\n        sub[sub.item == i].sales.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[9, i-46], legend=True, label = \"Forecast\")\nplt.tight_layout(pad=6.5)\nplt.suptitle(\"Store 1 Items Actual & Forecast\");","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}