{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy.ma as ma\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn import preprocessing\nimport os #modulos de gestion de directorios\nimport glob #modulo de visualizaci√≥n de directorios\n#import xgboost as xgb\ncolor = sns.color_palette()\nimport sys\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"986015401ff8afdf271f421adb6b617267062729"},"cell_type":"code","source":"#cambiamos al directorio de trabajo donde tenemos los datos\nos.chdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfaa2fc6f8c8d57b528e2d9f417bf04ab17895af"},"cell_type":"code","source":"os.getcwd() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"082da3b1a6047a20482301b4f3742e2671c2bebf"},"cell_type":"code","source":"print(glob.glob(\"../input/*.*\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a451737d8aa31632863c80c62f5dad4f0c4eab3c"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", \\\n                      parse_dates=True, index_col=0)\ntest_df = pd.read_csv(\"../input/test.csv\", \\\n                     parse_dates=True, index_col=0 )\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06510bd00cbd502201de3b0efe0e4604f334833e"},"cell_type":"code","source":"train_df=train_df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4f2fca329a16a99fcd1fcd0846562ee9092aadb"},"cell_type":"code","source":"#we create some new fields to easy manipulate\n#forecasting probably should be at item-store because demand pattens could vary much dep. items and store \ntrain_df['weekday']=pd.DatetimeIndex(train_df['date']).weekday\ntrain_df['month']=pd.DatetimeIndex(train_df['date']).month \ntrain_df['year']=pd.DatetimeIndex(train_df['date']).year\ntrain_df['itemstore']=train_df.item.astype(str)+\"-\"+train_df.store.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a837913c1bfbf28d49395d4dcec958daf5201bd"},"cell_type":"code","source":"#overview of data\nprint(\"number of different items: %i\" %(len(np.unique(train_df.item))))\nprint(\"number of different stores: %i\" %(len(np.unique(train_df.store))))\nprint(\"number of different dates: %i\" %(len(np.unique(train_df.date))))\nprint(\"maximun date in data: %s\" %(max(train_df.date)))\nprint(\"minimum date in data: %s\" %(min(train_df.date)))\nprint(\"number of different itemstore: %i\" %(len(np.unique(train_df.itemstore))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b1991449f51e0ad5346fa527b81284a62ee6d8c"},"cell_type":"code","source":"#create some lists to see range of unique values\nstores = list(set(train_df.store))\nitem = list(set(train_df.item))\nitemstore = list(set(train_df.itemstore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ad3a5b79013e8d91801f635745ad4f13224a586"},"cell_type":"code","source":"#we check anual sales profile comparing stores\nc=train_df.groupby(['year','store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a01b90d270ddef4ce3b1cbb813916dae9c38db3a"},"cell_type":"code","source":"#we check seasonal sales profile comparing stores\nc=train_df.groupby(['month', 'store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4fd2ab2bbf7853ae2e04d45e374a2ba9b05287a"},"cell_type":"code","source":"#we check seasonal sales profile comparing stores\nc=train_df.groupby(['weekday', 'store']).sum()\nplt.figure(figsize=(15,10))\nd=c.unstack()\nd.plot(y='sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"962983f6365a4cb7fb28a115d8624af267b27e81"},"cell_type":"code","source":"#we evaluate increase in anual sales at itemstore level\nb =train_df.drop(columns=['store', 'item','weekday','date','month'])\nc=b.groupby(['year', 'itemstore']).sum()\nd=c.unstack()\nsales_itemstore_year=d.T\nsales_itemstore_year['delta_2014/2013']=((sales_itemstore_year[2014]-sales_itemstore_year[2013])/sales_itemstore_year[2013])*100\nsales_itemstore_year['delta_2015/2014']=((sales_itemstore_year[2015]-sales_itemstore_year[2014])/sales_itemstore_year[2014])*100\nsales_itemstore_year['delta_2016/2015']=((sales_itemstore_year[2016]-sales_itemstore_year[2015])/sales_itemstore_year[2015])*100\nsales_itemstore_year['delta_2017/2016']=((sales_itemstore_year[2017]-sales_itemstore_year[2016])/sales_itemstore_year[2016])*100\nsales_itemstore_year_deltas =sales_itemstore_year.drop(columns=[2013, 2014, 2015, 2016, 2017], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f159a4b387bcc70695eba7ffae24729a2cbfe6c"},"cell_type":"code","source":"sales_itemstore_year_deltas =sales_itemstore_year.drop(columns=[2013, 2014, 2015, 2016, 2017], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e998daf5752cb3c399cc6216278becc433baa285"},"cell_type":"code","source":"#heat-maps to compare deltas anual and bet. itemstore each year\nsales_itemstore_year_deltas=sales_itemstore_year_deltas.sort_values('delta_2014/2013')\nplt.figure(figsize=(8,10))\nsns.heatmap(sales_itemstore_year_deltas)\nplt.title(\"Percentage variation sales-itemstore. Sort 2014/2013\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b4c7532203574c1c820f574da0995bd301c90d5"},"cell_type":"code","source":"sales_itemstore_year_deltas=sales_itemstore_year_deltas.sort_values('delta_2017/2016')\nplt.figure(figsize=(8,10))\nsns.heatmap(sales_itemstore_year_deltas)\nplt.title(\"Percentage variation sales-itemstore. Sort 2017/2016\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4942b083812b8405eed3dafd9baedd122c4f70a7"},"cell_type":"code","source":"#we pivot, group to weeks\ntrain_df['date'] = pd.to_datetime(train_df['date'])\ntrain_df_train=train_df.pivot(index='date', columns='itemstore', values='sales')\ntrain_df_train=train_df_train.resample('W').sum()\ntrain_df_train = train_df_train[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"662d17c6eaaf71ba7b283cb2565ab654c05dc176"},"cell_type":"code","source":"train_df_train_V1 = train_df_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be8dae457164a39789c83828f728f9fec3142953"},"cell_type":"code","source":"# we search ARIMA parameters for item 1 store 1 with 52 weeks differentation for stationary hipotesis\nimport warnings\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom pandas import tseries\n\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return np.array(diff)\n\ndef inverse_difference(history, yhat, interval=1):\n    return yhat + history[-interval]\n\n# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\ndef evaluate_arima_model(X, arima_order):\n# prepare training dataset\n    X = X.astype('float32')\n    train_size = int(len(X) * 0.7)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n    # difference data\n        weeks_in_year = 52\n        diff = difference(history, weeks_in_year)\n        model = ARIMA(diff, order=arima_order)\n        model_fit = model.fit(trend='nc', disp=0)\n        yhat = model_fit.forecast()[0]\n        yhat = inverse_difference(history, yhat, weeks_in_year)\n        predictions.append(yhat)\n        history.append(test[t])\n        # calculate out of sample error\n    rmse = sqrt(mean_squared_error(test, predictions))\n    return rmse\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    rmse = evaluate_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n                except:\n                    continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550ad454c6869707bc9c72820f62c29d0b2d5937"},"cell_type":"code","source":"#evaluate models\np_values = range(0, 6)\nd_values = range(0, 2)\nq_values = range(0, 6)\nt = '1-1'\n\nwarnings.filterwarnings(\"ignore\")\n\nevaluate_models(train_df_train_V1[t].values, p_values, d_values, q_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4361d73941c0cf950e6b3615b1651833b9e6b41b"},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n#Procedure to predict values\ndef do_predictions_join_unpack(t, database1, database2, dictionary, database3):\n    X = database1[itemstore[t]].values\n    X = X.astype('float32')\n    weeks_in_year = 52\n    diff = difference(X, weeks_in_year)\n    model = ARIMA(diff, order=(0,0,3))\n    model_fit = model.fit(trend='nc', disp=0)\n    # bias constant, could be calculated from in-sample mean residual\n    bias = 0\n    # save model\n    #model_fit.save('model.pkl')\n    #np.save('model_bias.npy', [bias])    \n    # load and prepare datasets\n    X = database1[itemstore[t]].values.astype('float32')\n    history = [x for x in X]\n    weeks_in_year = 52\n    y = database2[itemstore[t]].values.astype('float32')\n    # load model\n    #model_fit = ARIMAResults.load('model.pkl')\n    #bias = np.load('model_bias.npy')\n    # forecast 13 periods\n    predictions = list()\n    forecast = model_fit.forecast(steps=13)[0]\n    for yhat in forecast:\n        yhat = bias + inverse_difference(history, yhat, weeks_in_year)\n        history.append(yhat)\n        predictions.append(yhat)\n    #turn to daily with weekly pattern and copy in summary\n    database2 = database2.reset_index()\n    predictions = pd.DataFrame(predictions)\n    train_df_test_V1_pred = pd.concat([database2['date'], database2[itemstore[t]], predictions], axis=1)\n    train_df_test_V1_pred['date'] = pd.to_datetime(train_df_test_V1_pred['date'])\n    train_df_test_V1_pred=train_df_test_V1_pred.set_index('date')\n    new_dates = pd.date_range('2018-01-01', '2018-04-01', name='date')\n    train_df_test_V1_pred_daily = train_df_test_V1_pred.reindex(new_dates, method='ffill')\n    for k in range (13):\n        for j in range (7):\n            train_df_test_V1_pred_daily[0][(k*7)+j] = round(train_df_test_V1_pred_daily[0][(k*7)+j]*dictionary[itemstore[t]][j])\n    database3[[itemstore[t]]] = train_df_test_V1_pred_daily[0]\n    return database3, train_df_test_V1_pred_daily, predictions, train_df_test_V1_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6ea40d6c3e72edd35dde488e53722cf1a302e0d"},"cell_type":"code","source":"train_df = train_df.set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f6939b119ae2dafb0e4ece69aa4776a295dcb2d"},"cell_type":"code","source":"#we asign in a dictionary for each item-store the de-composition of sales for SUN-MON-TUE.......-SAT-SUMA. we use 2017 weekly pattern\ndictionary_week_sales_itemstore={}\ndictionary_week_sales_itemstore_reparto={}\nfor i in range (len(itemstore)):\n    dictionary_week_sales_itemstore.update({itemstore[i]:[0, 0, 0, 0, 0, 0, 0, 0]})\n    dictionary_week_sales_itemstore_reparto.update({itemstore[i]:[0, 0, 0, 0, 0, 0, 0, 0]})\n\n#Now we group sales at item-store level and week-day    \n#train_df=train_df.set_index('date')\ntrain_sales_weekday=train_df['01-01-2017':'31-12-2017'].groupby(['weekday', 'itemstore']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"283532a30bb19b1094e9da3e0c7996afcbb95021"},"cell_type":"code","source":"#def update_dictionary_week_sales_itemstore(itemstore, train_sales_weekday)\nfor i in range (len(itemstore)):\n    for j in range (0,7):\n        dictionary_week_sales_itemstore[itemstore[i]][j]= train_sales_weekday.loc[(j, itemstore[i]),['sales']][0]\n    dictionary_week_sales_itemstore[itemstore[i]][7]= sum(dictionary_week_sales_itemstore[itemstore[i]][0:7])   \n    \n#Now we update second dictionary dictionary_week_sales_itemstore_reparto={}\nfor i in range (len(itemstore)):\n    for j in range (0,7):\n        dictionary_week_sales_itemstore_reparto[itemstore[i]][j]= (dictionary_week_sales_itemstore[itemstore[i]][j]/   \\\n            dictionary_week_sales_itemstore[itemstore[i]][7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b68db6ca593dd8d5599f3d7737aec59c4126b15"},"cell_type":"code","source":"#we prepare dataframe for integrate all results\ntest_df['itemstore']=test_df.item.astype(str)+\"-\"+test_df.store.astype(str)\ntest_df['sales'] = 0\ntrain_df_test_V2  = test_df.drop(columns=['store', 'item'])\ntrain_df_test_V2['date'] = pd.to_datetime(train_df_test_V2['date'])\ntrain_df_test_V2 = train_df_test_V2.pivot(index='date', columns='itemstore', values='sales')\ntrain_df_test_V1 = train_df_test_V2.resample('W').sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9997467b9ab221fd9cc6a7cf7077b80e85322626"},"cell_type":"code","source":"#calculation of all itemstore predictions\npredictions = list()\nfor t in range (len(itemstore)):\n        do_predictions_join_unpack(t, train_df_train_V1, train_df_test_V1, dictionary_week_sales_itemstore_reparto, train_df_test_V2)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26fc66b60eeaa8e79784b49dec1d7cdaf0b83575"},"cell_type":"code","source":"#copy same pattern for first week\ntrain_df_test_V2.ix['2018-01-01']=train_df_test_V2.ix['2018-01-08']\ntrain_df_test_V2.ix['2018-01-02']=train_df_test_V2.ix['2018-01-09']\ntrain_df_test_V2.ix['2018-01-03']=train_df_test_V2.ix['2018-01-10']\ntrain_df_test_V2.ix['2018-01-04']=train_df_test_V2.ix['2018-01-11']\ntrain_df_test_V2.ix['2018-01-05']=train_df_test_V2.ix['2018-01-12']\ntrain_df_test_V2.ix['2018-01-06']=train_df_test_V2.ix['2018-01-13']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"345e9152fd64aff0723fe727f7439a6f11911e6c"},"cell_type":"code","source":"for i in range (len(test_df)):\n    test_df['sales'][(i)] = train_df_test_V2.loc[test_df['date'][(i)], test_df['itemstore'][(i)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbda2c08b75cd76bc5a097379798bd8ca80ccff5"},"cell_type":"code","source":"submission = test_df.drop(columns=['date', 'store', 'item', 'itemstore'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa2774e91c48ca69a37bf61a2dccf18aebc4b718"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}