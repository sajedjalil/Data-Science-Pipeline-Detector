{"cells":[{"metadata":{"_uuid":"1f478f3704e2e5f096b6c00dd723c40701dff539"},"cell_type":"markdown","source":"So, we are dealing with time series of sales, which of course present some periodicities (I am not going to show these in this notebook, just plot the time series to visualize what I am talking about !) :\n* within a year, there is a clear monthly trend (with a peak in summer), that can also be analyzed week by week,\n* within a week, there is generally a peak during the week-end.\n\nSince ARIMA is pretty slow, I thought I might try something \"manually\". Let's give it a shot."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf_train = pd.read_csv(\"../input/train.csv\", sep=\",\")\ndf_test = pd.read_csv(\"../input/test.csv\", sep=\",\")\nids = df_test.pop(\"id\")\ndf_train['date'] = pd.to_datetime(df_train['date'], infer_datetime_format=True)\ndf_test['date'] = pd.to_datetime(df_test['date'], infer_datetime_format=True)\ndf_train['isTrain'] = True\ndf_test['isTrain'] = False\ndf = df_train.append(df_test, ignore_index=True, sort=True)\n\n# Create some additional columns\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['week'] = df['date'].dt.week\n\ndf['day'] = df['date'].dt.day\ndf['weekday'] = df['date'].dt.dayofweek\ndf['dayofyear'] = df['date'].dt.dayofyear\ndf['weekend'] = (df['weekday'] >= 5).astype(int)\n\n# Some weird columns which help a bit\nfor div in np.arange(2, 6, 1):\n    df['dayb%i' % div] = df['dayofyear'] % div","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f2db3cbe116d552fbc6181666230814a2902776"},"cell_type":"markdown","source":"Now we normalize the data with regards to the store-specific and item-specific scales, and visualize the normalized sales afterwards."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19bef12d9021f326fc57ef341bea947b7c8333d1"},"cell_type":"code","source":"all_means = df.groupby(['store', 'item'])['sales'].mean().unstack()\nall_stds = df.groupby(['store', 'item'])['sales'].std().unstack()\n\ndf['normed'] = df['sales'] - all_means.values[df['store'].values - 1, df['item'].values - 1]\ndf['normed'] = df['normed'] / all_stds.values[df['store'].values - 1, df['item'].values - 1]\n\ndf_train = df.groupby('isTrain').get_group(True).copy()\ndf_test = df.groupby('isTrain').get_group(False).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e452700b5ee02528aa77da75d05cdf6179d7930"},"cell_type":"code","source":"print('Factor plot - year vs normed')\nsns.factorplot(x=\"year\", y=\"normed\", data=df_train, kind=\"box\", hue=\"store\", size=10)\n\nprint('Factor plot - weekday vs normed')\nsns.factorplot(x=\"weekday\", y=\"normed\", data=df_train, kind=\"box\", size=10)\n\ndf_train_ = df_train.copy()\ndf_train_['jfm'] = df_train_['month'] < 4\nprint('Joint plot - dayofyear vs normed')\nsns.jointplot(x=\"dayofyear\", y=\"normed\", data=df_train_.groupby('jfm').get_group(True), size=10)\n\ndel df_train_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a9298d221010f91233bfe78893253c3f3d5c11b"},"cell_type":"markdown","source":"Sales vary significantly from one year to another, so we need to account for that when predicting the sales in 2018. Here is where the \"homemade-ARIMA\" starts. First we get the annual mean (normalized) sales and try to recognize a pattern."},{"metadata":{"trusted":true,"_uuid":"cb5707b933c7be96155580fc2b8eec33e1b83006"},"cell_type":"code","source":"annual_means = df_train.groupby('year')['normed'].mean()\nannual_means.name = \"Annual mean normalized sales\"\nannual_stds = df_train.groupby('year')['normed'].std()\nannual_stds.name = \"Annual std normalized sales\"\n\nfig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)\nannual_means.plot(ax=axs[0], title=\"Annual mean normalized sales\")\nannual_stds.plot(ax=axs[1], title=\"Annual std normalized sales\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed5952e130a36b7c491a91a9e37ad4335a0ef2b4"},"cell_type":"markdown","source":"We have a limited number of data points (5) here, but from these we can reckon that a linear regression would probably over-estimate the next annual mean & std in 2018. So we are going to use a linear regression but on the consecutive differences instead."},{"metadata":{"trusted":true,"_uuid":"19b00a3b0cab9e9262582fcf5e192f5272ab15f3"},"cell_type":"code","source":"annual_mean_diffs = annual_means.diff().dropna()\nannual_std_diffs = annual_stds.diff().dropna()\n\nfig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)\nannual_mean_diffs.plot(ax=axs[0], title=\"Annual mean normalized sales variations\")\nannual_std_diffs.plot(ax=axs[1], title=\"Annual std normalized sales variations\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e6333437148b82daf3390d19438373bfd95cbf4"},"cell_type":"markdown","source":"It is difficult to see if the linear regression is appropriate, because now we only have 4 data points, but we just keep going to see what we get in the end."},{"metadata":{"trusted":true,"_uuid":"52396f8543f2270005b3fde5e526262e7b2b0242"},"cell_type":"code","source":"# Linear model for inter-annual variability\nfrom sklearn.linear_model import LinearRegression\n\nfig, axs = plt.subplots(figsize=(10, 10), nrows=2, sharex=True)\nannual_means.plot(ax=axs[0])\nannual_stds.plot(ax=axs[0])\nplt.legend(['Means', 'Stds'])\n\nannual_mean_diffs.plot(ax=axs[1])\nannual_std_diffs.plot(ax=axs[1])\n\nlr_mean_diffs = LinearRegression().fit(annual_mean_diffs.index.values.reshape(-1, 1),\n                                       annual_mean_diffs.values)\nlr_std_diffs = LinearRegression().fit(annual_std_diffs.index.values.reshape(-1, 1),\n                                      annual_std_diffs.values)\n\nmean_diff_2018 = lr_mean_diffs.predict([[2018]])[0]\nmean_2018 = annual_means[2017] + mean_diff_2018\n\nstd_diff_2018 = lr_std_diffs.predict([[2018]])[0]\nstd_2018 = annual_stds[2017] + std_diff_2018\n\nplt.sca(axs[0])\nplt.plot(2018, mean_2018, 'ko')\nplt.plot(2018, std_2018, 'rd')\nplt.plot([2017, 2018], [annual_means[2017], mean_2018], 'k--')\nplt.plot([2017, 2018], [annual_stds[2017], std_2018], 'r--')\n\nplt.sca(axs[1])\nplt.plot(2018, mean_diff_2018, \"ko\")\nplt.plot(2018, std_diff_2018, \"ro\")\nplt.plot([2017, 2018], [annual_mean_diffs[2017], mean_diff_2018], 'k--')\nplt.plot([2017, 2018], [annual_std_diffs[2017], std_diff_2018], 'r--')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f8393e0b6d19ab5ca668a6aa7682c7d574ee0ff"},"cell_type":"markdown","source":"I usually forget to annotate graphs but the two previous ones show the annual mean (blue) and std (orange) to the top, their variations (same color code) to the bottom ; the forecasted values are shown in black and red respectively. It kind of makes sense !"},{"metadata":{"trusted":true,"_uuid":"9d91b28ceaa2f6a703245cab51c5653ffef9ebd8"},"cell_type":"code","source":"annual_means[2018] = mean_2018\nannual_stds[2018] = std_2018\ndf['residuals_year'] = df['normed'] - annual_means.values[df['year'].values - 2013]\ndf['residuals_year'] = df['residuals_year'] / annual_stds.values[df['year'].values - 2013]\ndf['residuals_year'].hist(bins=100, figsize=(10, 5))\nplt.title(\"Distribution of residuals (year variations removed)\")\npd.concat([annual_means, annual_stds], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63adf7e6c5b03cb9f56d81ccba4bf49f72336806"},"cell_type":"markdown","source":"Now, we remove the sales variations within a year (we choose to do that per week instead of per month but it would probably give similar results), because we want the ML model to focus on the variations of other time scales that we cannot anticipate (apart from the business day / weekend variations)."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"02c5a34565c70e84770c0107e97da990a15145e3"},"cell_type":"code","source":"df_train = df.groupby('isTrain').get_group(True).copy()\nweekly_means = df_train.groupby('week')['residuals_year'].mean()\nweekly_stds = df_train.groupby('week')['residuals_year'].std()\n\nweekly_means.plot()\nweekly_stds.plot()\nplt.xlabel('Week')\nplt.legend(['Means', 'Stds'])\n\ndf['residuals'] = df['residuals_year'] - weekly_means.values[df['week'].values - 1]\ndf['residuals'] = df['residuals'] / weekly_stds.values[df['week'].values - 1]\n\ndf_train = df.groupby('isTrain').get_group(True).copy().drop('isTrain', axis=1)\ndf_test = df.groupby('isTrain').get_group(False).copy().drop('isTrain', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"013069cd9c705c1d14622ef20f6fe18919ea0f10"},"cell_type":"markdown","source":"So now we have the residuals which :\n* have the same scale, for all items and stores,\n* do not vary from one year to another (in mean and std)\n* do not vary from one week to another (in mean and std)\nWe are going to use a LightGBM model to predict the other variations."},{"metadata":{"trusted":true,"_uuid":"b6d295a57e7140edf93f2e301faac6a3ea514fea","collapsed":true},"cell_type":"code","source":"predictors = [c for c in df_train.columns\n              if c not in ['sales',\n                           'normed',\n                           'residuals_year',\n                           'residuals',\n                           'year',\n                           'date',\n                           'isTrain']]\ncategories = [c for c in predictors\n              if c not in ['month', 'week', 'dayofyear']]\nprint('Categorical predictors:', categories)\nprint('Predictors:', predictors)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56dd7e5e7caab8409e8e0318cee5b530a627c62c","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom lightgbm import Dataset, train\n\ndf_train['jfm'] = df_train['month'] < 4\ndf_train = df_train.groupby('jfm').get_group(True).copy().drop('jfm', axis=1)\n\nX_train = df_train[predictors].values\nX_test = df_test[predictors].values\ny_train = df_train['residuals'].values # we predict the residuals (after removing yearly and weekly trends\n\nnfolds = 10\nfolds = KFold(n_splits=nfolds, shuffle=True)\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'mae'},\n    'num_leaves': 45,\n    'learning_rate': 0.02,\n    'feature_fraction': 0.9,\n    'max_depth': 6,\n    'verbose': 0,\n    'num_boost_round': 15000,\n    'early_stopping_rounds': 100,\n    'nthread': -1}\n\nresiduals = []\nscores = []\nft_imp_split = []\nft_imp_gain = []\n\nprint('\\tRunning %i K-folds...' % nfolds)\nfor ifold, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n\n    lgb_train = Dataset(\n        data=X_train[trn_idx, :],\n        label=y_train[trn_idx],\n        feature_name=predictors)\n    lgb_val = Dataset(\n        data=X_train[val_idx, :],\n        label=y_train[val_idx],\n        feature_name=predictors)\n\n    model = train(\n        params,\n        lgb_train,\n        num_boost_round=15000,\n        early_stopping_rounds=100,\n        valid_sets=[lgb_train, lgb_val],\n        verbose_eval=20,\n        categorical_feature=categories,\n    )\n\n    y_pred = model.predict(X_train[val_idx, :], num_iteration=model.best_iteration)\n    score = model.best_score['valid_1']['l1']\n    print('\\toof best score is: {:6.4f} after {:6d} iterations'.format(score, model.best_iteration))\n    \n    ft_imp_split.append(model.feature_importance(importance_type=\"split\", iteration=model.best_iteration))\n    ft_imp_gain.append(model.feature_importance(importance_type=\"gain\", iteration=model.best_iteration))\n\n    residuals.append(model.predict(X_test, num_iteration=model.best_iteration))\n    scores.append(score)\n\nresiduals = np.average(residuals, axis=0, weights=(1./np.array(scores))**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5789ca5d313e38198b4ef70bbc845268cd961456","collapsed":true},"cell_type":"code","source":"df_test['residuals'] = residuals\n\ndf_test['residuals_year'] = df_test['residuals'] * weekly_stds.values[df_test['week'].values - 1]\ndf_test['residuals_year'] += weekly_means.values[df_test['week'].values - 1]\n\ndf_test['normed'] = df_test['residuals_year'] * annual_stds[2018] + annual_means[2018]\n\ndf_test['sales'] = df_test['normed'] * all_stds.values[df_test['store'].values - 1, df_test['item'].values - 1]\ndf_test['sales'] += all_means.values[df_test['store'].values - 1, df_test['item'].values - 1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38a859c8e46b6e7e0f90e3a0c20063c2f4bf9a37"},"cell_type":"markdown","source":"Let's visualize the results and see if we managed to capture significant variations.\n\nFig. 1 : In average, across all items and stores, it seems we are in a range that makes sense, however the amplitude of the variations seems very under-estimated.\nFig. 2 : Focus on the item 1 ; it seems we have a bias here.\nFig. 3 : Focus on the item 1 sold in store 1 ; same conclusions as in Fig1 and Fig2."},{"metadata":{"trusted":true,"_uuid":"ed097ad00c99173993a8638b8658e37784d163f5"},"cell_type":"code","source":"df = df_train.append(df_test, ignore_index=True, sort=True)\n\nsns.factorplot(x=\"weekday\", y=\"residuals\", hue=\"year\", data=df, kind=\"box\", size=8)\nplt.title(\"All items & stores\")\n\nsns.factorplot(x=\"weekday\", y=\"residuals\", hue=\"year\", data=df.groupby('item').get_group(1), kind=\"box\", size=8)\nplt.title(\"Item 1, all stores\")\n\nsns.factorplot(x=\"weekday\", y=\"residuals\", hue=\"year\", data=df.groupby(['store', 'item']).get_group((1, 1)), kind=\"box\", size=8)\nplt.title(\"Item 1, store 1\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e06c8d9fd4b45725cac174ee96576184f9dfe400"},"cell_type":"markdown","source":"Let's visualize how important our features were (log-scaled)."},{"metadata":{"trusted":true,"_uuid":"9f0b966186c09f54bb969604fae2011ab2358995","collapsed":true},"cell_type":"code","source":"# Show feature importance\nsns.barplot(x=np.log1p(np.mean(ft_imp_split, axis=0)),\n            y=predictors)\nplt.title('Feature importance (log) by split')\nplt.figure()\nsns.barplot(x=np.log1p(np.mean(ft_imp_gain, axis=0)),\n            y=predictors)\nplt.title('Feature importance (log) by gain')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7841d5d4cde8b949b1026e064a50ac796de591d9"},"cell_type":"markdown","source":"Et voilÃ  ! This model is quite simple and doesn't perform too poorly. Before tuning the ML model hyperparameters, we can investigate other ways of dealing with the periodicities, and try to find solutions to that bias / amplitude of variations problem which we saw in the Figs1, 2, 3.\n\n**What do you think ?**"},{"metadata":{"trusted":true,"_uuid":"2f7729822657dcc6186f98572f7dce78987ac3f2","collapsed":true},"cell_type":"code","source":"df_test['ID'] = ids.values\nsub = df_test[['ID', 'sales']]\nprint(sub.head())\nsub.to_csv('homemade-arima.csv', sep=',', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55246dd9291c3e2dcc345f7f077a3410b4efc403","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a3f34d67de6beddd10ea5ddaca90bf42c336a070"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}