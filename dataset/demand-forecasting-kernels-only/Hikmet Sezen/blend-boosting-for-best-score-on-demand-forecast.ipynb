{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataset of the \"Store Item Demand Forecasting Challenge\" (https://www.kaggle.com/c/demand-forecasting-kernels-only/) is one of time series related case study during my Data Science and Machine Learning bootcamp. \n\nI develop a LigthGBM model including feature engineering about different type approaches ie. smoothings, lag/shift injections, nonlinear projections, encodings, model tuning etc. My base notebook is able to reach public scores between 13.84000 - 13.86000, and you can check it (https://www.kaggle.com/hikmetsezen/base-model-with-lightgbm-on-demand-forecasting). \n\nHere I only share with you my blend boosting study to further enhance the score. For that reason, I use my 32 best scores, and after making a basic correlation analysis I separate them into four groups and figure out a best linear combination. I am able to get the best score on the Kaggle, 13.83657 (public).","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# loading data including 32 best scores\ndf_sub = pd.read_csv('../input/demand-forecasting-better-scores/better_scores_than_13.6.csv')\n\n# a rough correlation based visualization of 32 best scores\nplt.figure(figsize=(10,10))\nsns.heatmap(df_sub.corr(), cmap='Spectral')\nplt.ylabel('file index numbers')\nplt.xlabel('file index numbers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nFor simplicity I calculate mean of correlation values, and convert these values to index seek of an easy recognition. With this procedure K separate these 32 best scores into four distinct groups. After that I figure out a best linear combination. By spending much more time an acquisition much better score is always possible. \n","metadata":{}},{"cell_type":"code","source":"# basic analysis and visualization  of four groups in different color.\nsub_mean_corr = (1-df_sub.corr().mean().sort_values())*1e6\nplt.figure(figsize=(12,6))\nplt.plot(sub_mean_corr.index[:3], sub_mean_corr.values[:3], 's-')\nplt.plot(sub_mean_corr.index[3:15], sub_mean_corr.values[3:15], 's-')\nplt.plot(sub_mean_corr.index[15:27], sub_mean_corr.values[15:27], 's-')\nplt.plot(sub_mean_corr.index[30:], sub_mean_corr.values[30:], 's-')\nplt.title('determination of sub_groups')\nplt.ylabel('a corellation ralated index')\nplt.xlabel('file index numbers')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a best linear combination to achieve much better score\ndf_sub['weighted_avg'] = 1*( \n                            4*( df_sub['9'] + df_sub['17'] + df_sub['23'] )/3 +\n                            \n                            1*( df_sub['2'] +  df_sub['5'] + df_sub['15'] + df_sub['16'] + df_sub['18'] + df_sub['20'] + \n                               df_sub['21'] + df_sub['22'] + df_sub['26'] + df_sub['27'] + df_sub['29'] + df_sub['30'])/12 +\n                            \n                            3*( df_sub['3'] +  df_sub['4'] +  df_sub['6'] +  df_sub['7'] +  df_sub['8'] + df_sub['10'] + df_sub['11'] + df_sub['12'] + \n                               df_sub['13'] + df_sub['14'] + df_sub['19'] + df_sub['24'] + df_sub['25'] + df_sub['28'] + df_sub['31'] )/15 +\n                            \n                            6*( df_sub['0'] + df_sub['1'] )/2  \n                        ) / 14\n\nsubmission = pd.DataFrame({'id': [*range(45000)], 'sales': df_sub['weighted_avg'].round().astype('int')})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## It gets 13.83657 as public score, and looks the best score on Kaggle so far ;-)","metadata":{}}]}