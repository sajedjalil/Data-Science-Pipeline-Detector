{"nbformat_minor":0,"cells":[{"outputs":[],"metadata":{"_uuid":"3e17a45b006b3fbcc550284747e278ae71908e3e","_execution_state":"idle","_cell_guid":"04577c7b-a97d-4676-ba2d-2af26e58d762"},"cell_type":"markdown","source":"# Getting Started with the NIPS 2017 Adversarial Learning Challenges\n\nCurrent image classifiers can easily be tricked using carefully crafted adversarial images. These images add small changes to the original, correctly-classified image that are virtually imperceptible to the human eye but cause image classifiers to become wrong with high confidence in the incorrect class.\n\nThere's three related adversarial learning challenges in NIPS 2017.  The first two focus on successfully generating adversarial images. The [non-targeted challenge](https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack/) focuses on tricking the classifier with any other class, while the [targeted challenge](https://www.kaggle.com/c/nips-2017-targeted-adversarial-attack) focuses on tricking the classifier into thinking the image is a specific target class. The third [defense challenge](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack) focuses on training classifiers that are robust against adversarial attacks.\n\nThe defense challenge is scored based on how well the classifiers work in the face of adversarial attacks from the first two challenges, and the first two challenges are scored based on how well the adversarial attacks trick the classifiers in the third challenge.\n\nHere, we'll walk through some code examples on generating non-targeted and targeted adversarial images, and then seeing how the [Inception V3](https://www.kaggle.com/google-brain/inception-v3) model classifies them.\n\nMuch of this code is based on [Alex's](https://www.kaggle.com/alexey2004) [samples](https://github.com/tensorflow/cleverhans/blob/master/examples/nips17_adversarial_competition/sample_attacks/fgsm/attack_fgsm.py).\n\n*To get started, we'll import the necessary libraries and define some parameters / useful functions.*","execution_count":null},{"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"27b91f5c92f0feb6facf702f889fe14714630d46","trusted":false,"_cell_guid":"f7d78e7a-00d3-406d-8158-249b6d8a9368","collapsed":false},"cell_type":"code","source":"import os\nfrom cleverhans.attacks import FastGradientMethod\nfrom io import BytesIO\nimport IPython.display\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom scipy.misc import imread\nfrom scipy.misc import imsave\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.nets import inception\n\nslim = tf.contrib.slim\ntensorflow_master = \"\"\ncheckpoint_path   = \"../input/inception-v3/inception_v3.ckpt\"\ninput_dir         = \"../input/nips-2017-adversarial-learning-development-set/images/\"\nmax_epsilon       = 16.0\nimage_width       = 299\nimage_height      = 299\nbatch_size        = 16\n\neps = 2.0 * max_epsilon / 255.0\nbatch_shape = [batch_size, image_height, image_width, 3]\nnum_classes = 1001\n\ndef load_images(input_dir, batch_shape):\n    images = np.zeros(batch_shape)\n    filenames = []\n    idx = 0\n    batch_size = batch_shape[0]\n    for filepath in sorted(tf.gfile.Glob(os.path.join(input_dir, '*.png'))):\n        with tf.gfile.Open(filepath, \"rb\") as f:\n            images[idx, :, :, :] = imread(f, mode='RGB').astype(np.float)*2.0/255.0 - 1.0\n        filenames.append(os.path.basename(filepath))\n        idx += 1\n        if idx == batch_size:\n            yield filenames, images\n            filenames = []\n            images = np.zeros(batch_shape)\n            idx = 0\n    if idx > 0:\n        yield filenames, images\n\ndef show_image(a, fmt='png'):\n    a = np.uint8((a+1.0)/2.0*255.0)\n    f = BytesIO()\n    Image.fromarray(a).save(f, fmt)\n    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n\nclass InceptionModel(object):\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.built = False\n\n    def __call__(self, x_input):\n        \"\"\"Constructs model and return probabilities for given input.\"\"\"\n        reuse = True if self.built else None\n        with slim.arg_scope(inception.inception_v3_arg_scope()):\n            _, end_points = inception.inception_v3(\n                            x_input, num_classes=self.num_classes, is_training=False,\n                            reuse=reuse)\n        self.built = True\n        output = end_points['Predictions']\n        probs = output.op.inputs[0]\n        return probs","execution_count":1},{"outputs":[],"metadata":{"_uuid":"bb68943da495a8830718b965b485037b99e171c6","_execution_state":"idle","_cell_guid":"57f44796-8187-4844-a155-f445901b49d6","collapsed":false},"cell_type":"markdown","source":"Next, we'll load in the metadata along with a single batch of images to work on in this example.","execution_count":null},{"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"d1aa19f973e41609d3aaf9a47bd9ce93c14f4b90","trusted":false,"_cell_guid":"49130ff3-836c-4c40-bbfd-2bbb9a974afb","collapsed":false},"cell_type":"code","source":"categories = pd.read_csv(\"../input/nips-2017-adversarial-learning-development-set/categories.csv\")\nimage_classes = pd.read_csv(\"../input/nips-2017-adversarial-learning-development-set/images.csv\")\nimage_iterator = load_images(input_dir, batch_shape)\n\n# get first batch of images\nfilenames, images = next(image_iterator)\n\nimage_metadata = pd.DataFrame({\"ImageId\": [f[:-4] for f in filenames]}).merge(image_classes,\n                                                                              on=\"ImageId\")\ntrue_classes = image_metadata[\"TrueLabel\"].tolist()\ntarget_classes = true_labels = image_metadata[\"TargetClass\"].tolist()\ntrue_classes_names = (pd.DataFrame({\"CategoryId\": true_classes})\n                        .merge(categories, on=\"CategoryId\")[\"CategoryName\"].tolist())\ntarget_classes_names = (pd.DataFrame({\"CategoryId\": target_classes})\n                          .merge(categories, on=\"CategoryId\")[\"CategoryName\"].tolist())\n\nprint(\"Here's an example of one of the images in the development set\")\nshow_image(images[0])","execution_count":2},{"outputs":[],"metadata":{"_uuid":"b64e30468275e37101021f36a36bceedb6018b4b","_execution_state":"idle","_cell_guid":"f6d704d4-0022-47e1-a3f8-c374b63d0707","collapsed":false},"cell_type":"markdown","source":"## Generating non-targeted adversarial images\n\nThe below code runs a Tensorflow session to generate non-targeted adversarial images. These non-targeted images are designed to trick the original classifier but don't have a specific class in mind.","execution_count":null},{"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"c212d5dd0dcc06e5e78e6ab6f2d0819875284038","trusted":false,"_cell_guid":"6cc643aa-f71b-4427-9cb6-b1c9d94db6de","collapsed":false},"cell_type":"code","source":"tf.logging.set_verbosity(tf.logging.INFO)\n\nwith tf.Graph().as_default():\n    x_input = tf.placeholder(tf.float32, shape=batch_shape)\n    model = InceptionModel(num_classes)\n\n    fgsm  = FastGradientMethod(model)\n    x_adv = fgsm.generate(x_input, eps=eps, clip_min=-1., clip_max=1.)\n\n    saver = tf.train.Saver(slim.get_model_variables())\n    session_creator = tf.train.ChiefSessionCreator(\n                      scaffold=tf.train.Scaffold(saver=saver),\n                      checkpoint_filename_with_path=checkpoint_path,\n                      master=tensorflow_master)\n\n    with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n        nontargeted_images = sess.run(x_adv, feed_dict={x_input: images})\n\nprint(\"The original image is on the left, and the nontargeted adversarial image is on the right. They look very similar, don't they? It's very clear both are gondolas\")\nshow_image(np.concatenate([images[1], nontargeted_images[1]], axis=1))","execution_count":3},{"outputs":[],"metadata":{"_uuid":"5b510a764b5d6e2957b7147ab2b0f3c5e94f9467","_execution_state":"idle","_cell_guid":"b9aa6b5b-192f-42d4-b202-cf7d31b7bf74","collapsed":false},"cell_type":"markdown","source":"## Generating targeted adversarial images\n\nThe below code runs a Tensorflow session to generate targeted adversarial images. In each case, there's a specific target class that we're trying to trick the image classifier to output.\n\n*Note - this currently isn't working - it's generating adversarial images, but they aren't correctly targeted*","execution_count":null},{"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"56868cb0a1ad4f8cc258ae9e2bc622a17fc28381","trusted":false,"_cell_guid":"4e385681-0c3f-46f0-aea4-7c05789f25e5","collapsed":false},"cell_type":"code","source":"all_images_target_class = {image_metadata[\"ImageId\"][i]+\".png\": image_metadata[\"TargetClass\"][i]\n                           for i in image_metadata.index}\n\nwith tf.Graph().as_default():\n    x_input = tf.placeholder(tf.float32, shape=batch_shape)\n\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n        logits, end_points = inception.inception_v3(\n            x_input, num_classes=num_classes, is_training=False)\n\n    target_class_input = tf.placeholder(tf.int32, shape=[batch_size])\n    one_hot_target_class = tf.one_hot(target_class_input, num_classes)\n    cross_entropy = tf.losses.softmax_cross_entropy(one_hot_target_class,\n                                                    logits,\n                                                    label_smoothing=0.1,\n                                                    weights=1.0)\n    cross_entropy += tf.losses.softmax_cross_entropy(one_hot_target_class,\n                                                     end_points['AuxLogits'],\n                                                     label_smoothing=0.1,\n                                                     weights=0.4)\n    x_adv = x_input - eps * tf.sign(tf.gradients(cross_entropy, x_input)[0])\n    x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n\n    saver = tf.train.Saver(slim.get_model_variables())\n    session_creator = tf.train.ChiefSessionCreator(\n        scaffold=tf.train.Scaffold(saver=saver),\n        checkpoint_filename_with_path=checkpoint_path,\n        master=tensorflow_master)\n\n    with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n        target_class_for_batch = ([all_images_target_class[n] for n in filenames]\n                                  + [0] * (batch_size - len(filenames)))\n        targeted_images = sess.run(x_adv,\n                                   feed_dict={x_input: images,\n                                              target_class_input: target_class_for_batch})\n        \nprint(\"The original image is on the left, and the targeted adversarial image is on the right. Again, they look very similar, don't they? It's very clear both are butterflies\")\nshow_image(np.concatenate([images[2], targeted_images[2]], axis=1))","execution_count":null},{"outputs":[],"metadata":{"_uuid":"af29c7c0dfca42de239a548fef9c35fc4888a313","_execution_state":"idle","_cell_guid":"b69e590a-784e-4e85-a041-819374257de6","collapsed":false},"cell_type":"markdown","source":"## Classifying the adversarial images\n\nNow, we'll see what happens when we run these generated adversarial images through the original classifier.","execution_count":null},{"outputs":[],"metadata":{"_execution_state":"busy","_uuid":"41f82290817d268366826a7a74eaedcc1e76cec5","trusted":false,"_cell_guid":"4ab65408-3feb-4918-80c5-239591e3678f","collapsed":false},"cell_type":"code","source":"with tf.Graph().as_default():\n    x_input = tf.placeholder(tf.float32, shape=batch_shape)\n\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n        _, end_points = inception.inception_v3(x_input, num_classes=num_classes, is_training=False)\n    \n    predicted_labels = tf.argmax(end_points['Predictions'], 1)\n\n    saver = tf.train.Saver(slim.get_model_variables())\n    session_creator = tf.train.ChiefSessionCreator(\n                      scaffold=tf.train.Scaffold(saver=saver),\n                      checkpoint_filename_with_path=checkpoint_path,\n                      master=tensorflow_master)\n\n    with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n        predicted_classes = sess.run(predicted_labels, feed_dict={x_input: images})\n        predicted_nontargeted_classes = sess.run(predicted_labels, feed_dict={x_input: nontargeted_images})\n        predicted_targeted_classes = sess.run(predicted_labels, feed_dict={x_input: targeted_images})\n\npredicted_classes_names = (pd.DataFrame({\"CategoryId\": predicted_classes})\n                           .merge(categories, on=\"CategoryId\")[\"CategoryName\"].tolist())\n\npredicted_nontargeted_classes_names = (pd.DataFrame({\"CategoryId\": predicted_nontargeted_classes})\n                          .merge(categories, on=\"CategoryId\")[\"CategoryName\"].tolist())\n\npredicted_targeted_classes_names = (pd.DataFrame({\"CategoryId\": predicted_targeted_classes})\n                          .merge(categories, on=\"CategoryId\")[\"CategoryName\"].tolist())","execution_count":null},{"outputs":[],"metadata":{"_uuid":"78df9a8fdea786c26f73166cd0c4572f7a8d0cee","_execution_state":"idle","_cell_guid":"59155f45-86d5-4726-96b0-8ab042a3213e","collapsed":false},"cell_type":"markdown","source":"Below we'll show all the images in this batch along with their classifications. The left image in each set is the original image. The middle one is the non-targeted adversarial image. The right one is the targeted adversarial image.","execution_count":null},{"outputs":[],"metadata":{"_execution_state":"busy","_uuid":"8b076a8f30120eb7e25b46bd3d55230b0a080bb9","trusted":false,"_cell_guid":"b5bdaf59-8481-4b92-8463-b81a0276f55b","collapsed":false},"cell_type":"code","source":"for i in range(len(images)):\n    print(\"UNMODIFIED IMAGE (left)\",\n          \"\\n\\tPredicted class:\", predicted_classes_names[i],\n          \"\\n\\tTrue class:     \", true_classes_names[i])\n    print(\"NONTARGETED ADVERSARIAL IMAGE (center)\",\n          \"\\n\\tPredicted class:\", predicted_nontargeted_classes_names[i])\n    print(\"TARGETED ADVERSARIAL IMAGE (right)\",\n          \"\\n\\tPredicted class:\", predicted_targeted_classes_names[i],\n          \"\\n\\tTarget class:   \", target_classes_names[i])\n    show_image(np.concatenate([images[i], nontargeted_images[i], targeted_images[i]], axis=1))\n","execution_count":null}],"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4}