{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Getting started with the PANDA dataset\n\nThis notebook shows a few methods to load and display images from the PANDA challenge dataset. The dataset consists of around 11.000 whole-slide images (WSI) of prostate biopsies from Radboud University Medical Center and the Karolinska Institute. \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\n\n# There are two ways to load the data from the PANDA dataset:\n# Option 1: Load images using openslide\nimport openslide\n# Option 2: Load images using skimage (requires that tifffile is installed)\nimport skimage.io\nimport skimage\n\n# General packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport PIL\nfrom IPython.display import Image, display\n\n# Plotly for the interactive viewer (see last section)\nimport plotly.graph_objs as go\n\nimport cv2\n\nimport tensorflow as tf\n\nprint(tf.__version__)\n\n!pip install tensorflow-transform\nimport tensorflow_transform as tft\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary describing the features.\nimage_mask_feature_description = {\n    'height': tf.io.FixedLenFeature([], tf.int64),\n    'width': tf.io.FixedLenFeature([], tf.int64),\n    'depth': tf.io.FixedLenFeature([], tf.int64),\n    'label': tf.io.FixedLenFeature([], tf.int64),\n    'image_raw': tf.io.FixedLenFeature([], tf.string),\n}\n\ndef my_numpy_func(x): \n    # x will be a numpy array with the contents of the input to the \n    # tf.function \n    print(\"in my_numpy_func\")\n    return x\n\n\ndef parse_image_function(example_proto1, example_proto2):\n    # Parse the input tf.Example proto using the dictionary above.\n    #(example_proto1, example_proto2) = example_proto\n    image_features = tf.io.parse_example(example_proto1, image_mask_feature_description)\n    mask_features = tf.io.parse_example(example_proto2, image_mask_feature_description)\n        \n    image_raw = image_features['image_raw']\n    image_width = image_features['width'] \n    image_height = image_features['height']\n    image_depth = image_features['depth']\n    \n    mask_raw = mask_features['image_raw']\n    mask_width = mask_features['width']\n    mask_height = mask_features['height']\n    mask_depth = mask_features['depth']\n       \n    data = tf.io.decode_image(image_raw)\n    print(data.shape)\n    data = tf.image.convert_image_dtype(data, tf.float32)\n    data = tf.image.resize(data, (128, 128), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, preserve_aspect_ratio=True, antialias=True)\n    data = tf.image.pad_to_bounding_box(data, 0, 0, 128, 128)\n    \n    \n    mask_data = tf.io.decode_image(mask_raw)\n    print(mask_data.shape)\n    #mask_data = tf.image.convert_image_dtype(mask_data, tf.float32)\n    mask_data = tf.image.resize(mask_data, (128, 128), preserve_aspect_ratio=True)\n    mask_data = tf.image.pad_to_bounding_box(mask_data, 0, 0, 128, 128)\n\n    \n    print(\"displaying image\")\n    \n    plt.figure()\n    plt.subplot(1,3,1)\n    plt.imshow(data)\n    \n    cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n    plt.subplot(1,3,2)\n    plt.imshow(mask_data[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n    \n    plt.show()\n    \n    #data = tf.reshape(data, [128,128,3])\n    #mask_data = tf.reshape(mask_data, [128,128,3])\n    \n    return data.numpy(), mask_data.numpy(), data.shape\n    #return data, mask_data, data.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Location of the training images\ntfrecord_dir = '/kaggle/input/tfrecords-panda'\n\nimg_test_tf_records = [\"images.4.tfrecords\"]\nimg_test_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in img_test_tf_records ]\nimg_train_tf_records = [\"images.2.tfrecords\", \"images.3.tfrecords\"]\nimg_train_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in img_train_tf_records ]\n\nmask_test_tf_records = [\"images_mask.4.tfrecords\"]\nmask_test_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in mask_test_tf_records ]\nmask_train_tf_records = [\"images_mask.2.tfrecords\", \"images_mask.3.tfrecords\"]\nmask_train_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in mask_train_tf_records ]\n\n\n\nimage_train_dataset = tf.data.TFRecordDataset(img_train_tf_records)\nprint(image_train_dataset)\nmask_train_dataset = tf.data.TFRecordDataset(mask_train_tf_records)\n\nimage_test_dataset = tf.data.TFRecordDataset(img_test_tf_records)\nmask_test_dataset = tf.data.TFRecordDataset(mask_test_tf_records)\n\ntrain_dataset = tf.data.Dataset.zip((image_train_dataset, mask_train_dataset))\nprint(train_dataset)\ntest_dataset = tf.data.Dataset.zip((image_test_dataset, mask_test_dataset))\n\ndef set_shapes(img, label, img_shape):\n    img.shape=img_shape\n    label.shape=img_shape\n    return img, label\n\n\ntrain=train_dataset.map(lambda x1,x2: tf.py_function(func = parse_image_function , inp=[x1,x2], Tout=[tf.float32, tf.float64, tf.int64]))\nprint(train)\n\ntrain_x_img=[]\ntrain_x_mask=[]\nfor image, mask, shape in train.take(2):\n    train_x_img.append(image)\n    train_x_mask.append(mask)\n    \ntrain_x_img = np.array(train_x_img)\ntrain_x_mask = np.array(train_x_mask)\n#print(train_x_img.shape)\n    \ndef gen(): \n  for image, mask, shape in train: \n    yield image, np.expand_dims(mask[:,:,0],2)\n    \ntrain_x = tf.data.Dataset.from_generator( \n    gen, \n    (tf.int64, tf.int64),\n    (tf.TensorShape([128, 128, 3]), tf.TensorShape([128, 128, 1]))) \nprint(train_x)\n\n\ntest=test_dataset.map(lambda x1,x2: tf.py_function(func=parse_image_function, inp=[x1,x2], Tout=[tf.float32, tf.float64, tf.int64]))\n\ntest_x_img=[]\ntest_x_mask=[]\nfor image, mask, shape in test.take(2):\n    test_x_img.append(image)\n    test_x_mask.append(mask)\n\ntest_x_img = np.array(test_x_img)\ntest_x_mask = np.array(test_x_mask)\n#print(test_x_img.shape)\n\n    \ndef gen(): \n  for image, mask, shape in test: \n    print(image.shape, mask.shape)\n    yield image, np.expand_dims(mask[:,:,0],2)\ntest_x = tf.data.Dataset.from_generator( \n     gen, \n     (tf.int64, tf.int64),\n     (tf.TensorShape([128, 128, 3]), tf.TensorShape([128, 128, 1]))) \n\nprint(test_x)\n#test=test.map(lambda img, mask, shape:tf.numpy_function(func = set_shapes, inp = [img, mask, shape], Tout=[tf.float32, tf.float64]))\n#test=test_dataset.map(parse_image_function)\n#for image, mask in test:\n#    print(\"hereee\")\n\n#img_shape = images[0].shape  # images is a list of numpy.ndarray\n#ds = ds.map(lambda img, label: tf.py_function( ... ) )\n#ds = ds.map(lambda img, label: set_shapes(img, label, img_shape) )\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q git+https://github.com/tensorflow/examples.git\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n\ndown_stack.trainable = False\n\nup_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]\n\ndef unet_model(output_channels):\n    inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n    x = inputs\n\n    # Downsampling through the model\n    skips = down_stack(x)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        concat = tf.keras.layers.Concatenate()\n        x = concat([x, skip])\n\n    # This is the last layer of the model\n    last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n    x = last(x)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n\n\ndef create_mask(pred_mask):\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]\n\ndef show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT_CHANNELS = 6\nBATCH_SIZE = 4\ntrain_dataset = train_x.cache().shuffle(100).batch(4, drop_remainder=True).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset = test_x.batch(4)\nprint(train_dataset)\nprint(test_dataset)\n\n\nmodel = unet_model(OUTPUT_CHANNELS)\n\nfor data, label in test_x.take(1):\n    pred_mask = model.predict(data[tf.newaxis, ...])\n    pred_mask = create_mask(pred_mask)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(pred_mask))\n    #plt.imshow(pred_mask, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n    plt.show()\n\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n#tf.keras.utils.plot_model(model, show_shapes=True)\n\n#print(train_x_mask.shape)\n\nmodel_history = model.fit(train_dataset, epochs=1,\n                          steps_per_epoch=2,\n                          validation_steps=2,\n                          validation_data=test_dataset)\n\n#model_history = model.fit(train_x_img, train_x_mask[:,:,:,0], epochs=1,\n#                          steps_per_epoch=5,\n#                          validation_steps=2,\n#                          validation_data=(test_x_img, test_x_mask[:,:,:,0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Location of the training images\ntfrecord_dir = '/kaggle/input/tfrecords-panda'\n\n\nimg_test_tf_records = [\"images.4.tfrecords\"]\nimg_test_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in img_test_tf_records ]\nimg_train_tf_records = [\"images.2.tfrecords\", \"images.3.tfrecords\"]\nimg_train_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in img_train_tf_records ]\n\nmask_test_tf_records = [\"images_mask.4.tfrecords\"]\nmask_test_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in mask_test_tf_records ]\nmask_train_tf_records = [\"images_mask.2.tfrecords\", \"images_mask.3.tfrecords\"]\nmask_train_tf_records = [os.path.join(tfrecord_dir, tf_record) for tf_record in mask_train_tf_records ]\n\n\n\nimage_test_dataset = tf.data.TFRecordDataset(img_test_tf_records)\nmask_test_dataset = tf.data.TFRecordDataset(mask_test_tf_records)\n    \nimage_train_dataset = tf.data.TFRecordDataset(img_train_tf_records)\nmask_train_dataset = tf.data.TFRecordDataset(mask_train_tf_records)\n\n\n\ndef _parse_image_function(example_proto):\n    # Parse the input tf.Example proto using the dictionary above.\n   return tf.io.parse_single_example(example_proto, image_mask_feature_description)\n\n\nparsed_image_test_dataset = image_test_dataset.map(_parse_image_function)\nparsed_mask_test_dataset = mask_test_dataset.map(_parse_image_function)\nprint(parsed_image_test_dataset)\n\nparsed_image_train_dataset = image_train_dataset.map(_parse_image_function)\nparsed_mask_train_dataset = mask_train_dataset.map(_parse_image_function)\nprint(parsed_image_train_dataset)\n\n\n\nfor image_features, mask_features in zip(parsed_image_train_dataset.take(2), parsed_mask_train_dataset.take(2)):\n    image_raw = image_features['image_raw']\n    image_width = image_features['width'].numpy()\n    image_height = image_features['height'].numpy()\n    image_depth = image_features['depth'].numpy()\n    \n    mask_raw = mask_features['image_raw']\n    mask_width = mask_features['width'].numpy()\n    mask_height = mask_features['height'].numpy()\n    mask_depth = mask_features['depth'].numpy()\n       \n    data = tf.io.decode_image(image_raw)\n    data = tf.image.resize(data, (1280, 1280), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, preserve_aspect_ratio=True, antialias=True)\n    data = tf.image.pad_to_bounding_box(data, 0, 0, 1280, 1280)\n    #img = img - tft.mean(img)\n    print(data.shape, image_width*image_height*image_depth)\n    mask_data = tf.io.decode_image(mask_raw)\n    mask_data = tf.image.resize(mask_data, (1280, 1280), preserve_aspect_ratio=True)\n    mask_data = tf.image.pad_to_bounding_box(mask_data, 0, 0, 1280, 1280)\n    print(mask_data.shape, mask_width*mask_height*mask_depth)\n    \n    #img = cv2.imdecode(data.numpy(), cv2.IMREAD_UNCHANGED)\n    print(data.shape)\n    #mask_img = cv2.imdecode(mask_data.numpy(), cv2.IMREAD_UNCHANGED)\n    print(mask_data.shape)\n    print(\"displaying image\")\n    \n    plt.figure()\n    plt.subplot(1,3,1)\n    plt.imshow(data)\n    #plt.show()\n    \n    cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n    plt.subplot(1,3,2)\n    plt.imshow(mask_data[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n    #plt.show()\n    \n    pred_mask = model.predict(data[tf.newaxis, ...])\n    pred_mask = create_mask(pred_mask)\n    plt.subplot(1,3,3)\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(pred_mask))\n    #plt.imshow(pred_mask, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_file = 'images_mask.tfrecords'\nwith tf.io.TFRecordWriter(record_file) as writer:\n\n    i = 0\n    for index, row in train_labels.iterrows():\n        mask_file = os.path.join(mask_dir, f'{index}_mask.tiff')\n        file = os.path.join(data_dir, f'{index}.tiff')\n        print(i, mask_file, row[\"data_provider\"])  \n        i=i+1\n        if i<=150:\n            continue\n        if i>180:\n            break;\n\n        if os.path.isfile(mask_file):\n            biopsy = skimage.io.MultiImage(mask_file)\n            skimage.io.imsave(f'./{index}_mask.bmp', biopsy[1])\n\n            #image_raw = tf.io.read_file(f'./{index}.bmp')\n            image_raw = open(f'./{index}_mask.bmp', 'rb').read()\n            filename = f'./{index}_mask.bmp'\n            ! rm {filename}\n            image_decoded = tf.image.decode_image(image_raw)\n            image_shape = image_decoded.shape\n\n\n\n            feature = {\n                'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[0]])),\n                'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[1]])),\n                'depth': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[2]])),\n                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[row[\"isup_grade\"]])),\n                'image_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_raw])),\n            }\n\n            tf_example = tf.train.Example(features=tf.train.Features(feature=feature))     \n            writer.write(tf_example.SerializeToString())   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"record_file = 'images.tfrecords'\nwith tf.io.TFRecordWriter(record_file) as writer:\n\n    i = 0\n    for index, row in train_labels.iterrows():\n        mask_file = os.path.join(mask_dir, f'{index}_mask.tiff')\n        file = os.path.join(data_dir, f'{index}.tiff')\n        print(i, mask_file, row[\"data_provider\"])  \n        i=i+1\n        if i<=150:\n            continue\n        if i>180:\n            break;\n\n        if os.path.isfile(file):\n            biopsy = skimage.io.MultiImage(file)\n            skimage.io.imsave(f'./{index}.bmp', biopsy[1])\n\n            #image_raw = tf.io.read_file(f'./{index}.bmp')\n            image_raw = open(f'./{index}.bmp', 'rb').read()\n            filename = f'./{index}.bmp'\n            ! rm {filename}\n            image_decoded = tf.image.decode_image(image_raw)\n            image_shape = image_decoded.shape\n\n\n\n            feature = {\n                'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[0]])),\n                'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[1]])),\n                'depth': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_shape[2]])),\n                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[row[\"isup_grade\"]])),\n                'image_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_raw])),\n            }\n\n            tf_example = tf.train.Example(features=tf.train.Features(feature=feature))     \n            writer.write(tf_example.SerializeToString())   \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Location of the training images\ndata_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_images'\nmask_dir = '/kaggle/input/prostate-cancer-grade-assessment/train_label_masks'\noutput_dir = '/kaggle/working'\n\n# Location of training labels\ntrain_labels = pd.read_csv('/kaggle/input/prostate-cancer-grade-assessment/train.csv').set_index('image_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quickstart: reading a patch in 4 lines\n\nThe example below shows in 4 lines how to extract a patch from one of the slides using OpenSlide."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open the image (does not yet read the image into memory)\nimage = openslide.OpenSlide(os.path.join(data_dir, '005e66f06bce9c2e49142536caf2f6ee.tiff'))\n\n# Read a specific region of the image starting at upper left coordinate (x=17800, y=19500) on level 0 and extracting a 256*256 pixel patch.\n# At this point image data is read from the file and loaded into memory.\npatch = image.read_region((17800,19500), 0, (256, 256))\n\n# Display the image\ndisplay(patch)\n\n# Close the opened slide after use\nimage.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using OpenSlide to load the data\n\nIn the following sections we will load data from the slides with [OpenSlide](https://openslide.org/api/python/). The benefit of OpenSlide is that we can load arbitrary regions of the slide, without loading the whole image in memory. Want to interactively view a slide? We have added an [interactive viewer](#Interactive-viewer-for-slides) to this notebook in the last section.\n\nYou can read more about the OpenSlide python bindings in the documentation: https://openslide.org/api/python/\n\n## Loading a slide\n\nBefore we can load data from a slide, we need to open it. After a file is open we can retrieve data from it at arbitratry positions and levels.\n\n```python\nbiopsy = openslide.OpenSlide(path)\n# do someting with the slide here\nbiopsy.close()\n```"},{"metadata":{},"cell_type":"markdown","source":"For this tutorial, we created a small function to show some basic information about a slide. Additionally, this function display a small thumbnail of the slide. All images in the dataset contain this metadata and you can use this in your data pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_slide_details(slide, show_thumbnail=True, max_size=(600,400)):\n    \"\"\"Print some basic information about a slide\"\"\"\n    # Generate a small image thumbnail\n    if show_thumbnail:\n        display(slide.get_thumbnail(size=max_size))\n\n    # Here we compute the \"pixel spacing\": the physical size of a pixel in the image.\n    # OpenSlide gives the resolution in centimeters so we convert this to microns.\n    spacing = 1 / (float(slide.properties['tiff.XResolution']) / 10000)\n    \n    print(f\"File id: {slide}\")\n    print(f\"Dimensions: {slide.dimensions}\")\n    print(f\"Microns per pixel / pixel spacing: {spacing:.3f}\")\n    print(f\"Number of levels in the image: {slide.level_count}\")\n    print(f\"Downsample factor per level: {slide.level_downsamples}\")\n    print(f\"Dimensions of levels: {slide.level_dimensions}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the cell below loads four example biopsies using OpenSlide. Some things you can notice:\n\n- The image dimensions are quite large (typically between 5.000 and 40.000 pixels in both x and y).\n- Each slide has 3 levels you can load, corresponding to a downsampling of 1, 4 and 16. Intermediate levels can be created by downsampling a higher resolution level.\n- The dimensions of each level differ based on the dimensions of the original image.\n- Biopsies can be in different rotations. This rotation has no clinical value, and is only dependent on how the biopsy was collected in the lab.\n- There are noticable color differences between the biopsies, this is very common within pathology and is caused by different laboratory procedures.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"example_slides = [\n    '005e66f06bce9c2e49142536caf2f6ee',\n    '00928370e2dfeb8a507667ef1d4efcbb',\n    '007433133235efc27a39f11df6940829',\n    '024ed1244a6d817358cedaea3783bbde',\n]\n\nfor case_id in example_slides:\n    biopsy = openslide.OpenSlide(os.path.join(data_dir, f'{case_id}.tiff'))\n    print_slide_details(biopsy)\n    biopsy.close()\n    \n    # Print the case-level label\n    print(f\"ISUP grade: {train_labels.loc[case_id, 'isup_grade']}\")\n    print(f\"Gleason score: {train_labels.loc[case_id, 'gleason_score']}\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading image regions/patches\n\nWith OpenSlide we can easily extract patches from the slide from arbitrary locations. Loading a specific region is done using the [read_region](https://openslide.org/api/python/#openslide.OpenSlide.read_region) function.\n\nAfter opening the slide we can, for example, load a 512x512 patch from the lowest level (level 0) at a specific coordinate.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"biopsy = openslide.OpenSlide(os.path.join(data_dir, '00928370e2dfeb8a507667ef1d4efcbb.tiff'))\n\nx = 5150\ny = 21000\nlevel = 0\nwidth = 512\nheight = 512\n\nregion = biopsy.read_region((x,y), level, (width, height))\ndisplay(region)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the `level` argument we can easily load in data from any level that is present in the slide. Coordinates passed to `read_region` are always relative to level 0 (the highest resolution)."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = 5140\ny = 21000\nlevel = 1\nwidth = 512\nheight = 512\n\nregion = biopsy.read_region((x,y), level, (width, height))\ndisplay(region)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"biopsy.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading label masks\n\nApart from the slide-level label (present in the csv file), almost all slides in the training set have an associated mask with additional label information. These masks directly indicate which parts of the tissue are healthy and which are cancerous. The information in the masks differ from the two centers:\n\n- **Radboudumc**: Prostate glands are individually labelled. Valid values are:\n  - 0: background (non tissue) or unknown\n  - 1: stroma (connective tissue, non-epithelium tissue)\n  - 2: healthy (benign) epithelium\n  - 3: cancerous epithelium (Gleason 3)\n  - 4: cancerous epithelium (Gleason 4)\n  - 5: cancerous epithelium (Gleason 5)\n- **Karolinska**: Regions are labelled. Valid values:\n  - 0: background (non tissue) or unknown\n  - 1: benign tissue (stroma and epithelium combined)\n  - 2: cancerous tissue (stroma and epithelium combined)\n\nThe label masks of Radboudumc were semi-automatically generated by several deep learning algorithms, contain noise, and can be considered as weakly-supervised labels. The label masks of Karolinska were semi-autotomatically generated based on annotations by a pathologist.\n\nThe label masks are stored in an RGB format so that they can be easily opened by image readers. The label information is stored in the red (R) channel, the other channels are set to zero and can be ignored. As with the slides itself, the label masks can be opened using OpenSlide."},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the masks (using PIL)\n\nUsing a small helper function we can display some basic information about a mask. To more easily inspect the masks, we map the int labels to RGB colors using a color palette. If you prefer something like `matplotlib` you can also use `plt.imshow()` to directly show a mask (without converting it to an RGB image)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mask_details(slide, center='radboud', show_thumbnail=True, max_size=(400,400)):\n    \"\"\"Print some basic information about a slide\"\"\"\n\n    if center not in ['radboud', 'karolinska']:\n        raise Exception(\"Unsupported palette, should be one of [radboud, karolinska].\")\n\n    # Generate a small image thumbnail\n    if show_thumbnail:\n        # Read in the mask data from the highest level\n        # We cannot use thumbnail() here because we need to load the raw label data.\n        mask_data = slide.read_region((0,0), slide.level_count - 1, slide.level_dimensions[-1])\n        # Mask data is present in the R channel\n        mask_data = mask_data.split()[0]\n\n        # To show the masks we map the raw label values to RGB values\n        preview_palette = np.zeros(shape=768, dtype=int)\n        if center == 'radboud':\n            # Mapping: {0: background, 1: stroma, 2: benign epithelium, 3: Gleason 3, 4: Gleason 4, 5: Gleason 5}\n            preview_palette[0:18] = (np.array([0, 0, 0, 0.5, 0.5, 0.5, 0, 1, 0, 1, 1, 0.7, 1, 0.5, 0, 1, 0, 0]) * 255).astype(int)\n        elif center == 'karolinska':\n            # Mapping: {0: background, 1: benign, 2: cancer}\n            preview_palette[0:9] = (np.array([0, 0, 0, 0.5, 0.5, 0.5, 1, 0, 0]) * 255).astype(int)\n        mask_data.putpalette(data=preview_palette.tolist())\n        mask_data = mask_data.convert(mode='RGB')\n        mask_data.thumbnail(size=max_size, resample=0)\n        display(mask_data)\n\n    # Compute microns per pixel (openslide gives resolution in centimeters)\n    spacing = 1 / (float(slide.properties['tiff.XResolution']) / 10000)\n    \n    print(f\"File id: {slide}\")\n    print(f\"Dimensions: {slide.dimensions}\")\n    print(f\"Microns per pixel / pixel spacing: {spacing:.3f}\")\n    print(f\"Number of levels in the image: {slide.level_count}\")\n    print(f\"Downsample factor per level: {slide.level_downsamples}\")\n    print(f\"Dimensions of levels: {slide.level_dimensions}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cells below shows two example masks from the dataset. The first mask is from Radboudumc and shows two different grades of cancer (shown in yellow and orange). The second mask is from Karolinska, the region that contains cancer is higlighted in red.\n\nNote that, eventhough a biopsy contains cancer, not all epithelial tissue has to be cancerous. Biopsies can contain a mix of cancerous and healthy tissue."},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = openslide.OpenSlide(os.path.join(mask_dir, '08ab45297bfe652cc0397f4b37719ba1_mask.tiff'))\nprint_mask_details(mask, center='radboud')\nmask.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = openslide.OpenSlide(os.path.join(mask_dir, '090a77c517a7a2caa23e443a77a78bc7_mask.tiff'))\nprint_mask_details(mask, center='karolinska')\nmask.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing masks (using matplotlib)\n\nGiven that the masks are just integer matrices, you can also use other packages to display the masks. For example, using matplotlib and a custom color map we can quickly visualize the different cancer regions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = openslide.OpenSlide(os.path.join(mask_dir, '08ab45297bfe652cc0397f4b37719ba1_mask.tiff'))\nmask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n\nplt.figure()\nplt.title(\"Mask with default cmap\")\nplt.imshow(np.asarray(mask_data)[:,:,0], interpolation='nearest')\nplt.show()\n\nplt.figure()\nplt.title(\"Mask with custom cmap\")\n# Optional: create a custom color map\ncmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\nplt.imshow(np.asarray(mask_data)[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\nplt.show()\n\nmask.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overlaying masks on the slides\n\nAs the masks have the same dimension as the slides, we can overlay the masks on the tissue to directly see which areas are cancerous. This overlay can help you identifying the different growth patterns. To do this, we load both the mask and the biopsy and merge them using PIL.\n\n**Tip:** Want to view the slides in a more interactive way? Using a WSI viewer you can interactively view the slides. Examples of open source viewers that can open the PANDA dataset are [ASAP](https://github.com/computationalpathologygroup/ASAP) and [QuPath](https://qupath.github.io/). ASAP can also overlay the masks on top of the images using the \"Overlay\" functionality. If you use Qupath, and the images do not load, try changing the file extension to `.vtif`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def overlay_mask_on_slide(slide, mask, center='radboud', alpha=0.8, max_size=(800, 800)):\n    \"\"\"Show a mask overlayed on a slide.\"\"\"\n\n    if center not in ['radboud', 'karolinska']:\n        raise Exception(\"Unsupported palette, should be one of [radboud, karolinska].\")\n\n    # Load data from the highest level\n    slide_data = slide.read_region((0,0), slide.level_count - 1, slide.level_dimensions[-1])\n    mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n\n    # Mask data is present in the R channel\n    mask_data = mask_data.split()[0]\n\n    # Create alpha mask\n    alpha_int = int(round(255*alpha))\n    if center == 'radboud':\n        alpha_content = np.less(mask_data.split()[0], 2).astype('uint8') * alpha_int + (255 - alpha_int)\n    elif center == 'karolinska':\n        alpha_content = np.less(mask_data.split()[0], 1).astype('uint8') * alpha_int + (255 - alpha_int)\n    \n    alpha_content = PIL.Image.fromarray(alpha_content)\n    preview_palette = np.zeros(shape=768, dtype=int)\n    \n    if center == 'radboud':\n        # Mapping: {0: background, 1: stroma, 2: benign epithelium, 3: Gleason 3, 4: Gleason 4, 5: Gleason 5}\n        preview_palette[0:18] = (np.array([0, 0, 0, 0.5, 0.5, 0.5, 0, 1, 0, 1, 1, 0.7, 1, 0.5, 0, 1, 0, 0]) * 255).astype(int)\n    elif center == 'karolinska':\n        # Mapping: {0: background, 1: benign, 2: cancer}\n        preview_palette[0:9] = (np.array([0, 0, 0, 0, 1, 0, 1, 0, 0]) * 255).astype(int)\n    \n    mask_data.putpalette(data=preview_palette.tolist())\n    mask_rgb = mask_data.convert(mode='RGB')\n\n    overlayed_image = PIL.Image.composite(image1=slide_data, image2=mask_rgb, mask=alpha_content)\n    overlayed_image.thumbnail(size=max_size, resample=0)\n\n    display(overlayed_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: In the example below you can also observe a few pen markings on the slide (dark green smudges). These markings are not part of the tissue but were made by the pathologists who originally checked this case. These pen markings are available on some slides in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"slide = openslide.OpenSlide(os.path.join(data_dir, '08ab45297bfe652cc0397f4b37719ba1.tiff'))\nmask = openslide.OpenSlide(os.path.join(mask_dir, '08ab45297bfe652cc0397f4b37719ba1_mask.tiff'))\noverlay_mask_on_slide(slide, mask, center='radboud')\nslide.close()\nmask.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"slide = openslide.OpenSlide(os.path.join(data_dir, '090a77c517a7a2caa23e443a77a78bc7.tiff'))\nmask = openslide.OpenSlide(os.path.join(mask_dir, '090a77c517a7a2caa23e443a77a78bc7_mask.tiff'))\noverlay_mask_on_slide(slide, mask, center='karolinska', alpha=0.6)\nslide.close()\nmask.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using scikit-image & tifffile to load the data\n\nAs an alternative to OpenSlide, the slides in the PANDA dataset can also be loaded using [scikit-image](https://scikit-image.org/) with [tifffile](https://pypi.org/project/tifffile/) as the backend.\n\n> **Note:** scikit-image (<= 0.16.x) uses an internal version of the tif loader if the tifffile packages is not installed. This internal version does not support JPEG compression and can not be used to load the images in the dataset. Make sure tifffile is installed before running the examples below. This requirement is already met when running this code in a notebook on Kaggle.\n\n## Loading a slide\n\nLoading a slide with [scikit-image](https://scikit-image.org/) is similar to loading slides with OpenSlide. The major difference between scikit-image and OpenSlide is that scikit-image loads the image into memory. To extract a certain region of the image, you will need to load the whole image at one of the levels.\n\nThe images in the PANDA dataset are relatively small because each biopsy was individually extracted from the source slide. The small size makes it possible to load the slides directly into memory. Still, upon loading the image is uncompressed resulting in larger memory usage.\n\nSlides are loaded using the [MultiImage](https://scikit-image.org/docs/0.16.x/api/skimage.io.html?highlight=multiimage#skimage.io.MultiImage) class; this class gives the ability to access the individual levels of the image. By default, MultiImage tries to conserve memory usage by only caching the last image level that was accessed.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"biopsy = skimage.io.MultiImage(os.path.join(data_dir, '0b373388b189bee3ef6e320b841264dd.tiff'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The code below loads each individual level. You can check the memory usage of the kernel to see that loading the lowest level can require a considerate amount of memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,level in enumerate(biopsy):\n    print(f\"Biopsy level {i} dimensions: {level.shape}\")\n    print(f\"Biopsy level {i} memory size: {level.nbytes / 1024**2:.1f}mb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(PIL.Image.fromarray(biopsy[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting the object frees up memory\ndel biopsy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you are only interested in the lowest level (highest magnification), you can also load level 0 using [imread](https://scikit-image.org/docs/0.16.x/api/skimage.io.html?highlight=imread#skimage.io.imread):"},{"metadata":{"trusted":true},"cell_type":"code","source":"biopsy_level_0 = skimage.io.imread(os.path.join(data_dir, '0b373388b189bee3ef6e320b841264dd.tiff'))\nprint(biopsy_level_0.shape)\ndel biopsy_level_0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading image regions\n\nSimilar to OpenSlide, we can extract regions from the whole image. Because the image is already in memory, this boils down to a slice on the numpy array. To illustrate we use the same coordinates as in the OpenSlide example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"biopsy = skimage.io.MultiImage(os.path.join(data_dir, '00928370e2dfeb8a507667ef1d4efcbb.tiff'))\n\nx = 5150\ny = 21000\nlevel = 0\nwidth = 512\nheight = 512\n\npatch = biopsy[0][y:y+width, x:x+height]\n\n# You can also visualize patches with matplotlib\nplt.figure()\nplt.imshow(patch)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To load the same region from level 1, we have to devide the coordinates with the downsample factor (4 per level). This is different from Openslide that always works with coordinates from level 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = 5150 // 4\ny = 21000 // 4\nwidth = 512\nheight = 512\n\npatch = biopsy[1][y:y+width, x:x+height]\n\nplt.figure()\nplt.imshow(patch)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = 5150 // (4*4)\ny = 21000 // (4*4)\nwidth = 512\nheight = 512\n\npatch = biopsy[2][y:y+width, x:x+height]\n\nplt.figure()\nplt.imshow(patch)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Free up memory\ndel biopsy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading label masks\n\nLoading label masks using scikit-image is similar to loading the slides. As the label information is in the R channel, other channels can be discarded. Please refer to the \"OpenSlide - Loading label masks\" section for more information about the contents of the label masks."},{"metadata":{"trusted":true},"cell_type":"code","source":"maskfile = skimage.io.MultiImage(os.path.join(mask_dir, '090a77c517a7a2caa23e443a77a78bc7_mask.tiff'))\nmask_level_2 = maskfile[-1][:,:,0]\n\nplt.figure()\nplt.imshow(mask_level_2)\nplt.colorbar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del maskfile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interactive viewer for slides\n\nUsing [Plotly](https://kite.com/python/docs/plotly.graph_objs) we can make an interactive viewer that works inside a notebook. Using this viewer you can load any image from the PANDA dataset and interactively zoom in to specific regions. This viewer is a great way of inspecting the data in more detail.\n\n> **Note:** The code below only works when you run this notebook yourself. The output is not shown when purely viewing the notebook as it requires access to the source image.\n\nWant to investigate slides locally on your machine? Using a WSI viewer you can interactively view the slides on your own machine. Examples of open source viewers that can open the PANDA dataset are [ASAP](https://github.com/computationalpathologygroup/ASAP) and [QuPath](https://qupath.github.io/). ASAP can also overlay the masks on top of the images using the \"Overlay\" functionality. If you use Qupath, and the images do not load, try changing the file extension to `.vtif`.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class WSIViewer(object):\n    def __init__(self, plot_size = 1000):\n        self._plot_size = plot_size\n        \n    def set_slide(self, slide_path):      \n        self._slide = openslide.open_slide(slide_path)\n        self._base_dims = self._slide.level_dimensions[-1]\n        self._base_ds = self._slide.level_downsamples[-1]\n        img_arr = self._slide.read_region((0,0), len(self._slide.level_dimensions[-1]), (self._base_dims[0], self._base_dims[1]))\n        \n        self._fig = go.FigureWidget(data=[{'x': [0, self._base_dims[0]], \n                                           'y': [0, self._base_dims[1]], \n                                           'mode': 'markers',\n                                           'marker': {'opacity': 0}}], # invisible trace to init axes and to support autoresize\n                                    layout={'width': self._plot_size, 'height': self._plot_size, 'yaxis' : dict(scaleanchor = \"x\", scaleratio = 1)})  \n        # Set background image\n        self._fig.layout.images = [go.layout.Image(\n            source = img_arr,  # plotly now performs auto conversion of PIL image to png data URI\n            xref = \"x\",\n            yref = \"y\",\n            x = 0,\n            y = 0,\n            sizex = self._base_dims[0],\n            sizey = self._base_dims[1],\n            sizing = \"stretch\",\n            layer = \"below\")]\n        self._fig.update_layout(plot_bgcolor='rgba(0,0,0,0)',xaxis_showgrid=False, yaxis_showgrid=False, xaxis_zeroline=False, yaxis_zeroline=False);        \n        self._fig.layout.on_change(self._update_image, 'xaxis.range', 'yaxis.range', 'width', 'height')          \n\n    def _gen_zoomed_image(self, x_range, y_range):\n        # Below is a workaround which rounds image requests to multiples of 4, once the libpixman fix is in place these can be removed\n        #xstart = x_range[0] * self._base_ds\n        #ystart = (self._base_dims[1] - y_range[1]) * self._base_ds \n        xstart = 4 * round(x_range[0] * self._base_ds / 4)\n        ystart = 4 * round((self._base_dims[1] - y_range[1]) * self._base_ds / 4)\n        xsize0 = (x_range[1] - x_range[0]) * self._base_ds\n        ysize0 = (y_range[1] - y_range[0]) * self._base_ds\n        if (xsize0 > ysize0):\n            req_downs = xsize0 / self._plot_size\n        else:\n            req_downs = ysize0 / self._plot_size\n        req_level = self._slide.get_best_level_for_downsample(req_downs)\n        level_downs = self._slide.level_downsamples[req_level]\n        # Nasty workaround for buggy container\n        level_size_x = int(xsize0 / level_downs)\n        level_size_y = int(ysize0 / level_downs)\n        new_img = self._slide.read_region((int(xstart), int(ystart)), req_level, (level_size_x, level_size_y)).resize((1000,1000)) # Letting PIL do the resize is faster than plotly\n        return new_img\n    \n    def _update_image(self, layout, x_range, y_range, plot_width, plot_height):\n        img = self._fig.layout.images[0]\n        # Update with batch_update so all updates happen simultaneously\n        with self._fig.batch_update():\n            new_img = self._gen_zoomed_image(x_range, y_range)\n            img.x = x_range[0]\n            img.y = y_range[1]\n            img.sizex = x_range[1] - x_range[0]\n            img.sizey = y_range[1] - y_range[0]\n            img.source = new_img\n\n    def show(self):\n        return self._fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"viewer = WSIViewer()\nviewer.set_slide(os.path.join(data_dir, '08ab45297bfe652cc0397f4b37719ba1.tiff'))\nviewer.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}