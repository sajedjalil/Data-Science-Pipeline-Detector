{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f5563cf5-bf48-83fc-4777-c7e1693ce219"},"source":"# Zillow EDA\n\n_By [Michael Rosenberg](mailto:mmrosenb@andrew.cmu.edu)._\n\n_**Description**: Contains my exploration of the Zillow dataset for the Kaggle Competition._"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb929b77-816b-627e-c3b9-0a4141c39487"},"outputs":[],"source":"#imports\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#constants\n%matplotlib inline\nsns.set_style(\"dark\")\npercentLev = 100\nfigWidth = figHeight = 8"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09a7908e-7bce-9323-5cb8-c62e09be2210"},"outputs":[],"source":"#load in the dataset\ntrainFrame = pd.read_csv(\"../input/train_2016.csv\")\nsampleSubmission = pd.read_csv(\"../input/sample_submission.csv\")\npropFrame = pd.read_csv(\"../input/properties_2016.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"fafbcf05-18e6-9a89-9d39-c6ebe3c9d8de"},"source":"## Metadata Analysis\n\nWe will first explore some of the more general components of the dataset before giving summary statistics on relevant variables in the dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1302723-65e6-ed36-6f82-a7980eb0d402"},"outputs":[],"source":"print(trainFrame.shape)\nprint(trainFrame.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"db5a61b7-c38d-d2f5-cc3f-06fbc6b970a4"},"source":"We have around 90000 observations to consider in the training dataset. Let's see how many unique parcel IDs are accounted for."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"627d9f42-deef-ffdc-0200-3c8a05a44cf2"},"outputs":[],"source":"print(len(trainFrame[\"parcelid\"].unique()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e7a404a-7de4-9098-c0ce-d08aab102720"},"source":"We see that we have about the same number of unique properties as there are observations in this dataset. This suggests that there is a minimal number of repeats to consider. Given the time-sensitive nature of the [submission](https://www.kaggle.com/c/zillow-prize-1#evaluation), this is slightly concerning that we have a limited number of time points for each property. That being said, let's go see what property IDs have multiple observations and whether this is an error in the dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"326a3efe-abb3-b477-9df0-791446bc4d47"},"outputs":[],"source":"idCountFrame = trainFrame.groupby(\"parcelid\",as_index = False)[\"logerror\"].count()\nidCountFrame = idCountFrame.rename(columns = {\"logerror\":\"count\"})\n#get max count observations\nprint(idCountFrame[\"count\"].max())\n#get the observations with count greater than 1\nmoreThanOneFrame = idCountFrame[idCountFrame[\"count\"] > 1]\nprint(moreThanOneFrame)\nprint(moreThanOneFrame[\"count\"].mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"693ed6cf-9912-73d1-cc38-39b2eeefa7ca"},"source":"It looks like most of our observations with multiple entries have around 2 entries, while the maximum number of entries found in our training set for a given property is 3."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20edb180-5c33-d745-0f51-a5689e122d25"},"outputs":[],"source":"dateCountFrame = trainFrame.groupby(\"transactiondate\")[\"parcelid\"].count()\nplt.figure(figsize=(figWidth,figHeight))\ndateCountFrame.plot()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Date in Training Set\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a42f22e-38bc-a494-c320-17fd25c36299"},"source":"_Figure 1: Distribution of training set by date._\n\nWe see two interesting components occurring here:\n\n*  The granularity of each property is by day, when we are expected to predict by month. This may suggest that if we want to use some time-dependent features in our dataset, we will need to look into a transaction date that is refined only to the month. We should take a look at whether the test dataset is at this level of granularity as well.\n\n* We see that our observation rate drops off for the months in which we are supposed to be predicting. This definitely suggests that we should be accounting for some time-dependent features in our dataset given that we have such limited information on our target future."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a249c694-7b93-de37-e206-3d71d1021e78"},"outputs":[],"source":"#get parcel IDs from sample submission, find if there is overlap with training data\nsampleSubmissionIDSet = set(sampleSubmission[\"ParcelId\"].unique())\ntrainingIDSet = set(trainFrame[\"parcelid\"].unique())\n#get overlap\noverlap = trainingIDSet & sampleSubmissionIDSet\nprint(len(overlap))\n#get difference\ndiff = sampleSubmissionIDSet - trainingIDSet\nprint(len(diff))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff6f7f90-3952-410c-7647-1e48172b053b"},"source":"Interestingly, we see that the test set has many more observations than the training set and yet also contains the training set itself! This suggests to me that not only do we need to do well at predicting out-of-time for our current observations, but also out-of-sample. That is two problems (i.e. more than one)! How tf do I do two problems! We will try."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d71a6506-937c-eb9a-54d9-c10a8ae6f62c"},"outputs":[],"source":"plt.figure(figsize=(figWidth,figHeight))\nplt.hist(trainFrame[\"logerror\"])\nplt.xlabel(\"$logerror$\")\nplt.ylabel(\"count\")\nplt.title(\"Distribution of $logerror$\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"545573d6-17f8-2f0d-3510-e1961c830104"},"source":"_Figure 2: Distribution of our Target Variable._\n\nWe see that the domain for our target variable is rather small. This may help to inform the size of the weights on our features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6608d033-a95d-0481-81f2-7d16962ed30a"},"outputs":[],"source":"propFrame.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"53de0976-0d13-83ce-caa6-334dd99ae37e"},"source":"We see that we have close to 3 million properties in the overall set, with 58 features to choose form. The number of observations suggests that a neural architecture could be useful later down the line, but the out-of-time problem and the relatively small number of features suggests that a shallower model might be also useful."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8b202b7-bc98-38a1-0812-c930b925920b"},"outputs":[],"source":"#get number of missing in each column\npropFrame.isnull().sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd2e6727-2307-8c3d-a605-ea22233ff66d"},"source":"_Table 1: Number of missing observations per feature in the properties dataset._\n\nWe see that we have missing observations across the board for all of our variables. This suggests that we will need to likely do a large amount of feature reduction simply based on the availability of certain variables and that we will need to offer a different model when a selected feature is missing. To simply put, this is fucking gross."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3f0e1f0-2fd7-db92-88d9-fc3a4b4af8f4"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}