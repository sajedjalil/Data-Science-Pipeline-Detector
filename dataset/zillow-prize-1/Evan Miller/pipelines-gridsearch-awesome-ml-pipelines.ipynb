{"nbformat":4,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","version":"3.6.1","file_extension":".py","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9b75c8bd-f0a2-4905-987b-e7360c394d2c","_uuid":"03347c4a62865ae41d3df6bbe93f130a8e2808c8"},"source":"# Creating some ML pipelines\n\n### Introduction"},{"cell_type":"markdown","metadata":{"_cell_guid":"4661aa0f-01d5-410b-9dad-0321f49c13a9","_uuid":"80d0b5816a6831396a7386264f397e7348475b91"},"source":"I thought I'd do a quick write up on how you can build some simple and effective ML pipelines using sklearn.\n\nI discovered the pipeline/gridsearch combo a few weeks ago after sending off some of my code for review.\nIn sending off my code I realized that were a few things that I had tweaked for performance, but weren't obvious to the reviewer.\n\n- I had median imputed some variables (continuous features), while other variables were filled by mode\n- I did a large amount of feature engineering, only to use a subset of those features in my model building (they were the best I swear)\n\nSo even though I did some work to get to that reviewed copy, these experiments that I went through during the process wouldn't be easy to understand unless the reviewer looked at all my commits etc.\n\nBut then I found **pipelines / gridsearch** and all was good in the world.\n\n**Pipelines** let you combine all your feature engineering / pre-processing / modelling into one object\n\n**Gridsearch** then lets you test all your assumptions / hyperparameters to find out which combinations generate the best result"},{"cell_type":"markdown","metadata":{"_cell_guid":"244bf196-b69b-49fd-bf24-e00adad4c399","_uuid":"b7cdfec14fa6426abaa3643e78f9b269f681355a"},"source":"I haven't seen many write ups about them so I thought I'd do one myself.\n\n1. **References:**\n\n- http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n- http://scikit-learn.org/stable/modules/pipeline.html\n- http://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n- https://stackoverflow.com/questions/33091376/python-what-is-exactly-sklearn-pipeline-pipeline\n- http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n- https://michelleful.github.io/code-blog/2015/06/20/pipelines/"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ac5e41-74a7-4d3d-903e-6dd3829e1094","_uuid":"8bd05815de53fdfb8c4ea626b1ca438a413b7f7e"},"source":"# Libraries + MAE"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"0536a6c4-9352-4fa1-9a91-c92ffe8db4ca","collapsed":true,"_uuid":"ab5cb2eebc9bdcdbe25661e7bda3106fd93a1b9e"},"source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, VarianceThreshold\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nimport os\nimport time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef MAE(y, ypred):\n    \n    import numpy as np\n    \n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)   ","execution_count":1},{"cell_type":"markdown","metadata":{"_cell_guid":"9eed3411-889b-4a9d-81ce-9fb536763722","_uuid":"5c5226c26e309572bec09d58780b2008b4527e80"},"source":"# Data read in and prep\n### Making sure the data is in a lightGBM friendly format"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c01bc81b-0122-4442-96f7-c5fd61c07a8c","collapsed":true,"_uuid":"d0498c0248e5a231f419444f64c488c495e13b53"},"source":"train = pd.read_csv(\"../input/train_2016_v2.csv\")\nproperties = pd.read_csv('../input/properties_2016.csv')\n\nfor c, dtype in zip(properties.columns, properties.dtypes):\t\n    if dtype == np.float64:\n        properties[c] = properties[c].astype(np.float32)\n\ndf_train = (train.merge(properties, how='left', on='parcelid')\n            .drop(['parcelid', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1))\n\ntrain_columns = df_train.columns ","execution_count":2},{"cell_type":"markdown","metadata":{"_cell_guid":"0c7a2c82-3802-4e89-9d1b-72c1a4773fab","_uuid":"31e326a7919d6346891c3cda4fd0ca0e2a158238"},"source":"# Splitting the dataset to train/valid sets"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d49c5643-7379-4e65-b955-57bfee2a22cd","collapsed":true,"_uuid":"d27ffab58012a4014368a95eb95354b7652b5470"},"source":"valid = df_train.iloc[1:20000, :]\ntrain = df_train.iloc[20001:90275, :]\n\ny_train = train['logerror'].values\ny_valid = valid['logerror'].values\n\nx_train = train.drop('logerror', axis = 1)\nx_valid = valid.drop('logerror', axis = 1)\n\nidVars = [i for e in ['id',  'flag', 'has'] for i in list(train_columns) if e in i] + ['fips', 'hashottuborspa']\ncountVars = [i for e in ['cnt',  'year', 'nbr', 'number'] for i in list(train_columns) if e in i]\ntaxVars = [col for col in train_columns if 'tax' in col and 'flag' not in col]\n          \nttlVars = idVars + countVars + taxVars\ndropVars = [i for e in ['census',  'tude', 'error'] for i in list(train_columns) if e in i]\ncontVars = [col for col in train_columns if col not in ttlVars + dropVars]\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n    \nfor c in x_valid.dtypes[x_valid.dtypes == object].index.values:\n    x_valid[c] = (x_valid[c] == True)   ","execution_count":3},{"cell_type":"markdown","metadata":{"_cell_guid":"29944826-85f5-4259-9e9c-8770e9536f02","_uuid":"2f0db6ad26f0e17cb7b29a76fd1e6b14d042c642"},"source":"# The first pipeline\n\nSince everyone is using lightGBM I'll use that. \nInitially we'll just look at the continuous variables in model building, but we'll extend that out too.\n\nSo let's start with the easy pipeline that:\n\n- Imputes the missing values with the median\n- Selects the best 5 features\n- Builds a LightGBM model"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"4335dbf9-b749-465d-b709-6c53d4b07c5d","_uuid":"a7edc669ea1abb2f2318b4b49d8ed88e94241acf"},"source":"print(contVars)\n\nx_train_cont = x_train[contVars]\nx_valid_cont = x_valid[contVars]","execution_count":4},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3d5f19c0-a1c9-462e-a06f-886f0b3ece6e","_uuid":"e3b72912e1af6ab402ba37505921bcce965c1dd9"},"source":"pipeline = Pipeline(\n                    [('imp', Imputer(missing_values='NaN', strategy = 'median', axis=0)),\n                     ('feat_select', SelectKBest(k = 5)),\n                     ('lgbm', LGBMRegressor())\n                     \n])\n\npipeline.fit(x_train_cont, y_train)   \n\ny_pred = pipeline.predict(x_valid_cont)\nprint('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))","execution_count":5},{"cell_type":"markdown","metadata":{"_cell_guid":"1696907d-7d3d-47fa-ae48-d496b5094a14","_uuid":"b858f9db8792999c58dd06143697cf5e4e7b70c8"},"source":"## Pipeline 2.0 - oh hai there gridsearch"},{"cell_type":"markdown","metadata":{"_cell_guid":"eb7a1f64-49d7-4aa7-b5d3-40efa520f948","_uuid":"76729e352b3d514755c5dd6ff23cd5ae7fb57dc4"},"source":"But from the above code we have made a few assumptions that haven't been tested.\n\n**We assume that:**\n- Median is the best way of imputing the variables\n- Only 5 variables needed for the lowest error \n\nBut we don't need to assume these, we can test these assumptions and find out which actually results in the lowest error."},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3406bc4d-c515-426b-828b-20305832eb00","_uuid":"b73cad02a02c8bec291c1d0cfb4c5a44733a0e1e"},"source":"pipeline = Pipeline(\n                    [('imp', Imputer(missing_values='NaN', axis=0)),\n                     ('feat_select', SelectKBest()),\n                     ('lgbm', LGBMRegressor())\n                     \n])\n\nparameters = {}\nparameters['imp__strategy'] = ['mean', 'median', 'most_frequent']\nparameters['feat_select__k'] = [5, 10]\n\nCV = GridSearchCV(pipeline, parameters, scoring = 'mean_absolute_error', n_jobs= 1)\nCV.fit(x_train_cont, y_train)   \n\nprint('Best score and parameter combination = ')\n\nprint(CV.best_score_)    \nprint(CV.best_params_)    \n\ny_pred = CV.predict(x_valid_cont)\nprint('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))","execution_count":6},{"cell_type":"markdown","metadata":{"_cell_guid":"73ad20cd-df5f-4294-8eec-91a081f7154f","_uuid":"211c6dd07672c97b70d8b94a501cd1fcc33bd9b6"},"source":"Interesting, I never thought that using a mode would come out on top.\n\nBut we since we're also here we let's also test and see what is the best imputation policy for tax variables. First I'll quickly write a column extractor that plays nicely with the pipeline.\n\nThese can look hard at first, but they definitely get easier as you write a few. \nAnd since we're writing some code we should probably do some testing to make sure that it works the way we think it will.\n\n# Column extractor\n\n## Takes a list of columns and returns a df with those cols"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"27981adb-de94-4b6e-a6d6-fd03736e04dc","collapsed":true,"_uuid":"90c48b58a04fd5c41398ec293b9c734f449e87ed"},"source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, subset):\n        self.subset = subset\n\n    def transform(self, X, *_):\n        return X.loc[:, self.subset]\n\n    def fit(self, *_):\n        return self","execution_count":11},{"cell_type":"markdown","metadata":{"_cell_guid":"10b59873-35cb-4931-b49b-61bf3488677b","_uuid":"df2092a823b681fd3449bd682e70c1554c24af49"},"source":"# Testing\n\nSince we've already created x_train_cont I'll test the column extractor on this case"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"fac6f21a-0734-4cfa-8e48-d64b701d95ef","_uuid":"3bfa806209c0429f03d35e782a2918afcb9eeb93"},"source":"contExtractor = ColumnSelector(contVars)\nx_train_cont_test = contExtractor.transform(x_train).head()\n\nx_train_cont.head().equals(x_train_cont_test)","execution_count":12},{"cell_type":"markdown","metadata":{"_cell_guid":"2c1fb2f9-187d-44fe-809d-7a6a8748de9d","_uuid":"39f6d5fd3f3ad4d780f65fa7e6af32793367f40b"},"source":"# Pipeline 3.0 - taxes\n\nSo let's use the ColumnExtractor we created earlier to run the same analysis we did earlier on the tax variables"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a58ddde3-a127-4b22-9897-d4dcd8f91071","_uuid":"1d74714c636e5f50ac52abc677eb9a84514dd634"},"source":"pipeline = Pipeline([\n                    ('tax_dimension', ColumnSelector(taxVars)),\n                    ('imp', Imputer(missing_values='NaN', axis=0)),\n                    ('column_purge', SelectKBest()),\n                    ('lgbm', LGBMRegressor())\n                     \n])\n\nparameters = dict(imp__strategy=['mean', 'median', 'most_frequent'],\n                    column_purge__k=[5, 2, 1] \n\n)   \n\nCV = GridSearchCV(pipeline, parameters, scoring = 'neg_mean_absolute_error', n_jobs= 1)\nCV.fit(x_train, y_train)   \n\nprint(CV.best_params_)    \nprint(CV.best_score_)    \n\ny_pred = CV.predict(x_valid)\nprint('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))","execution_count":14},{"cell_type":"markdown","metadata":{"_cell_guid":"f3310325-0c07-47d9-9e06-171dc3412066","_uuid":"099f3ab879cd50ad3926405c1ab4d986a912f4d8"},"source":"# Pipeline 4.0 - contVars + taxes (FeatureUnion intro)\n\nLet's use FeatureUnion to apply different pre-processing pipelines to different types of variables"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"756f249e-9803-4a54-aece-081a79cfffd3","_uuid":"e158bb63967ab76573f33057364ef1b93a023948"},"source":"pipeline = Pipeline([\n        \n    ('unity', FeatureUnion(\n        transformer_list=[\n\n            ('cont_portal', Pipeline([\n                ('selector', PortalToColDimension(contVars)),\n                ('cont_imp', Imputer(missing_values='NaN', strategy = 'median', axis=0)),\n                ('scaler', StandardScaler())             \n            ])),\n            ('tax_portal', Pipeline([\n                ('selector', PortalToColDimension(taxVars)),\n                ('tax_imp', Imputer(missing_values='NaN', strategy = 'most_frequent', axis=0)),\n                ('scaler', MinMaxScaler(copy=True, feature_range=(0, 3)))\n            ])),\n        ],\n    )),\n    ('column_purge', SelectKBest(k = 5)),    \n    ('lgbm', LGBMRegressor()),\n])\n\nparameters = {}\nparameters['column_purge__k'] = [5, 10]\n\ngrid = GridSearchCV(pipeline, parameters, scoring = 'neg_mean_absolute_error', n_jobs= 2)\ngrid.fit(x_train, y_train)   \n\nprint('Best score and parameter combination = ')\n\nprint(grid.best_score_)    \nprint(grid.best_params_)    \n\ny_pred = grid.predict(x_valid)\nprint('MAE on validation set: %s' % (round(MAE(y_valid, y_pred), 5)))","execution_count":16},{"cell_type":"markdown","metadata":{"_cell_guid":"152cab74-15db-47a0-b8d1-6be46bd836eb","_uuid":"bca2b195f35ffb810afab46122b71da87be8b055"},"source":"# Finished with your model and push it to prod?\n\nYou can use the joblib library to serialize this best case pipeline and push it to a .pkl file.\n\nYou easily move this to your prod server and just re-open it and your pipeline will will work like a charm (assuming no changing in data types, and that's a pretty big IF).\n\nBut when the new data comes through you can open up the following pickle and easily score the model you built against the new data"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"44e59c6a-e707-4d42-80e9-9ece90d9ea0a","_uuid":"d0d4ae4ef95b2ae6ec653d14d391670300e72f0f"},"source":"from sklearn.externals import joblib\njoblib.dump(grid.best_estimator_, 'rick.pkl')","execution_count":17},{"cell_type":"markdown","metadata":{"_cell_guid":"4e59de25-233a-484a-a88c-2b11414743c4","_uuid":"77e32c69fdc16929122d43559370bd85c3d6d5bd"},"source":"# Summary\n\nIn hindsight, I'd be surprised if anyone used these approaches right out of the box to win this competition.\n\nBUT, hopefully you see some duplication in your feature engineering / modelling pipeline and that this type of set up might help you re-use some of that code for a better result in your future comps.\n\nMiller out.\n\nPS: If you want any of this explained just add a comment and I'll get around to it when I can."}],"nbformat_minor":1}