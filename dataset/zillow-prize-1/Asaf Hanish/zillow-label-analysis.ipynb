{"cells":[{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"d0fcfacdc4c9398b066db964832363a1eaad7094","_execution_state":"idle"},"source":"# This notebook explore the Zillow logerror prediction data\n\n*  [Logerror labels](#labelexplore)\n    * [Distribution](#labeldistribution)\n    * [Secular trends](#labelsecular)","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"e67fb0d0f12a1ed93ab2a5bc07a34711e1111e1f","_execution_state":"idle"},"source":"**\n\nExplore the logerror label\n--------------------------\n\n**\n<a id='labelexplore'> </a>\n* Distribution\n* Secular trends","outputs":[],"cell_type":"markdown"},{"execution_count":null,"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n","outputs":[],"metadata":{"_uuid":"f54e0019da98bdbb8954713e66f092033b1d6ff0","_execution_state":"idle"}},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"e6393647f79b35f9a3fe9ae6a9658248344c5104","_execution_state":"idle"},"source":"\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nfrom sklearn import linear_model","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"3ea85cd6cfbe7f9ebc3f0bd3cc7a1c4c5ad35859","_execution_state":"idle"},"source":"%matplotlib inline\nimport matplotlib.pyplot as plt","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"ab0a4ceb2296f649f576516e09dc512da09621a0","_execution_state":"idle"},"source":"train = pd.read_csv(\"../input/train_2016.csv\", parse_dates=[\"transactiondate\"])\ntrain.shape","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"ca794041d42e9f08bdc9528abd939d2f76d3129c","_execution_state":"idle"},"source":"<a id='labeldistribution'> </a>\n\n\n\n**Lets look at the distribution of logerror values**\n----------------------------------------------------\n\nLooks like there is tight clustering around zero, but with some outliers","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"dce5ff15b8d5a761a28ac768cda6a45ad550b1c5","_execution_state":"idle"},"source":"fig,ax = plt.subplots(ncols=2,nrows=1,figsize=(16,5))\nax[0].hist(train['logerror'],bins=100)\nax[0].set_title('Full range or logerror',fontsize=18,fontweight='bold')\nax[1].hist(train['logerror'],bins=100,range=(-.6,.6))\nax[1].set_title('Zoom in',fontsize=18,fontweight='bold')\nplt.tight_layout()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"a3c68050bc6ae677bcf348da98b2989c1b2160a7","_execution_state":"idle"},"source":"print('There is a slight right shift in the data...')\nmedian = np.percentile(train['logerror'],50)\nprint('* The median value is: {:.3f}'.format(median))\nabove_zero = (train['logerror']>0).mean()\nprint('* Values above zero: {:.0%}'.format(above_zero))\nrange95 = np.percentile(train['logerror'],[2.5,97.5])\nprint('* 95% of values are between: {:.2f}'.format(range95[0]) + ' and {:.2f}'.format(range95[1]))","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"ed354e4d0c372be1b37653bfbeb5ba7284ff07d5","_execution_state":"idle"},"source":"**\n\nlog error is kind of opaque. What does it mean in terms of delta dollar error (actual zestimate - actual sales price)?\n------------------------------------------------------------------------\n\n**\nFor more context, lets convert various log error scenarios to dollar amounts","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"40ef6cdde71cd3a61f90f2f4ad733cf02d67fb65","_execution_state":"idle"},"source":"def get_zest_price(sales_price,log_error):\n    log_sales_price  = np.log10(sales_price)\n    log_zest_price = log_sales_price + log_error\n    return 10**log_zest_price\n    \ndef get_delta_error(sales_price, log_error):\n    zest_price = get_zest_price(sales_price, log_error)\n    return zest_price - sales_price\n\ndef get_percent_diff(sales_price, log_error):\n    delta = get_delta_error(sales_price, log_error)\n    return float(delta) / sales_price\n\ndef print_stories(logerror):\n    sales_points = [150000, 250000, 500000, 750000, 1000000]\n    print('an absolute log error of ' + str(logerror) +' is in the {:.0%}'.format((abs(train['logerror']) <=logerror).mean())+ ' percentile of error, which means...')\n    for sales_price in sales_points:\n        zest_price = get_zest_price(sales_price, logerror)\n        delta = zest_price - sales_price\n        print('a sales price of ${:,.0f}'.format(sales_price) +  ' means the zestimate was ${:,.0f}'.format(zest_price) + ', with the difference of ${:,.0f}'.format(delta))","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"df43f3ea2286691975a451064e5001952d6d41cd","_execution_state":"idle"},"source":"plt.figure(figsize=(16,7))\nvalues = np.linspace(-.2,.2,100)\nplt.plot(values, [get_percent_diff(100000, le) for le in values])\nyticks = np.linspace(-.7,.7,15)\nplt.yticks(yticks,['{:.0%}'.format(yt) for yt in yticks])\nplt.ylabel('% error (Zestimat - sales price)',fontsize=16)\nxticks = np.linspace(-.2,.2,17)\nplt.xticks(xticks)\nplt.xlabel('logerror',fontsize=16)\nplt.tight_layout()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"ed5cce575d2cff83fc9f01ae755fde9354318bf5","_execution_state":"idle"},"source":"a log error of 0.1 means the zestimate was about 25% higher the the sales price.  #perspective","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"bd6b52dae5313dd8f95a2a9eaa608ddc434a4a64","_execution_state":"idle"},"source":"# low error of .005\nprint('Low error')\nprint('-----------')\nprint_stories(.005)\nprint('#####################')\nprint(' ')\n\n# median error of .0325\nprint('median error')\nprint('-----------')\nprint_stories(.0325)\nprint('#####################')\nprint(' ')\n\n\n# high error of .2\nprint('high error')\nprint('-----------')\nprint_stories(.2)\nprint('#####################')\nprint(' ')","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"057a5980f7006dacc2e6edf73d8116e4c07c4bcb","_execution_state":"idle"},"source":"I don't know about you guys, but I feel better after having done this translation.  An outlier logerror of 0.2 seems so small, but on a $500k house means the zestimate was 290k too high","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"2309efcffec6eb8176308522e1cccc41196f265e","_execution_state":"idle"},"source":"<a id='labelsecular'> </a>\n\n\n**\n\nSecular trends\n--------------\n\n**\nAny changes to the label over time?","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"e99e8123e851ee626ba3c14e3902de9ee232065e","_execution_state":"idle"},"source":"# I want to trend weekday sales because of low activity on weekends\ntrain['WeekDay'] = [d.weekday()<5 for d in train['transactiondate']]\n\npiv = pd.pivot_table(train[train['WeekDay']==True], index='transactiondate',values='logerror',aggfunc=[np.size,np.sum,np.median])","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"594a9b4e49332e303cf96e7f219291fee6a053bd","_execution_state":"idle"},"source":"plt.figure(figsize=(16,5))\nplt.title('Daily (weekday) homesales volume',fontsize=20,fontweight='bold' )\nplt.plot_date(piv.index,piv['size'],alpha=.6)\nplt.plot_date(piv.index,piv['size'].rolling(window=30).mean(),'r-',linewidth=5)\nplt.tight_layout()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"2176fe149c0cf54f9bed45ac9c98bbdaed5c919a","_execution_state":"idle"},"source":"There is the data dropoff in October.  Also, sale volume seems stable through late sprint into early fall?","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"9113f4b11ec98ecc76990a6ee3b02676a9da420c","_execution_state":"idle"},"source":"**\n\nWhat about changes in the log error over time?\n----------------------------------------------\n\n**\nThe graph below is rolling 30 day average of log errors\n* rolling 30 days average is calculated as sum of log errors divided by count of all records.","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"7dfba2b603fa2d86ebd7c8f6eaaed4b7f4450aca","_execution_state":"idle"},"source":"piv_all = pd.pivot_table(train, index='transactiondate',values='logerror',aggfunc=[np.size,np.sum])\npiv_restricted = pd.pivot_table(train[abs(train['logerror'])<.2], index='transactiondate',values='logerror',aggfunc=[np.size,np.sum])","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"07924d2ca1893d3442c469f49c76a31c143368d2","_execution_state":"idle"},"source":"plt.figure(figsize=(16,5))\nrolling_avg_all = piv_all['sum'].rolling(window=30).sum() / piv_all['size'].rolling(window=30).sum()\nrolling_avg_rest = piv_restricted['sum'].rolling(window=30).sum() / piv_restricted['size'].rolling(window=30).sum()\nplt.plot_date(piv_all.index,rolling_avg_all,label = '30 Day rolling Avg: All values')\nplt.plot_date(piv_restricted.index,rolling_avg_rest,label = '30 Day rolling Avg: No Outliers')\nplt.ylim(0,0.03)\nplt.legend(loc=0,fontsize=16)\nplt.title('Logerror secular trends', fontsize=20,fontweight='bold')\nplt.tight_layout()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"e7a433d6bbc5d3c74008e103597e24d53ac3a2a0","_execution_state":"idle"},"source":"Cool!  It looks like the logerror is the lowest (zestimate is most accurate) in the late spring.  I wonder what would cause these secular trends?","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"154778b33bc037274148be8156f64b28baa557ea","_execution_state":"idle"},"source":"#### Lets look into this trend a bit more\n* restrict this analysis to pre Oct data to work around the missing data","outputs":[],"cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"2febe43a9ac48f27203732bb856935245133e369","_execution_state":"idle"},"source":"train_pre_oct_16 = train[(train['transactiondate']<datetime(2016,10,16))]\npiv_pre = pd.pivot_table(train_pre_oct_16, index='transactiondate',values='logerror',aggfunc=[np.size,np.sum])\nrolling_avg_all_pre = piv_pre['sum'].rolling(window=30).sum() / piv_pre['size'].rolling(window=30).sum()\nscatter_df = pd.DataFrame(list(zip(piv_pre['size'].rolling(window=30).mean(),rolling_avg_all_pre)),columns=['Rolling size','Rolling avg'])\nscatter_df = scatter_df[scatter_df['Rolling size'].notnull()]","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"c8a5044cd96c2da86fa4021c5819ab08eca406e8","_execution_state":"idle"},"source":"\nsns.jointplot(\"Rolling size\", \"Rolling avg\", data=scatter_df, kind='scatter',\n                  xlim=(0,600), ylim=(0,.03),color=\"r\", size=7)\n\nplt.ylabel('Logerror')\nplt.xlabel('Sales volume ')\nplt.tight_layout()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"53fb92eed6245dc480d5e00ec098f4b0552c1efe","_execution_state":"idle"},"source":"X_input = np.array([[v] for v in scatter_df['Rolling size'].values])\ny_input = np.array(scatter_df['Rolling avg'].values)\nregr = linear_model.LinearRegression()\nregr.fit(X_input, y_input)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"12e479c5d4f23d1a36e4084da332b85e73043821","_execution_state":"idle"},"source":"plt.figure(figsize=(8,8))\nplt.scatter(X_input,y_input)\nplt.plot(X_input, regr.predict(X_input), color='blue',\n         linewidth=3)\nplt.xlabel('Daily Sales volume')\nplt.ylabel('Log error')\nplt.title('Modeling log error by daily sales volume', fontsize=18, fontweight='bold')\nplt.text(225,.025,'change of {:,.3f}'.format(regr.coef_[0]* 100)+' for ever 100 additional daily sales')\nplt.tight_layout()","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"collapsed":false,"_uuid":"874b6f65d5d4597af3f852700cdcfd709dea5d38","_execution_state":"idle"},"source":"There you have it.  Additional sales associated with lower logerror.  I wish there were multiple years of data to start whittling away at all the hidden confounders and biases.","outputs":[],"cell_type":"markdown"}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.6.0"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}