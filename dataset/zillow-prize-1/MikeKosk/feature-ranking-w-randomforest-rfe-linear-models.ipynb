{"cells":[{"execution_count":null,"metadata":{"_uuid":"cde1f7416981edcc7951bbc7a4928933b7ba3760","_cell_guid":"6ea51bc1-c7e6-5022-f91e-5933202f9f3a"},"outputs":[],"source":"# INTRODUCTION \n\nThis notebook will aim to provide an explanation and application of different feature ranking methods, namely that of Recursive Feature Elimination (RFE), Stability Selection, linear models as well as Random Forest.\n\nThis notebook borrows heavily from an article by Ando Saabas on feature selection found here: http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/ as well as Anisotropic's work on feature selection found here: https://www.kaggle.com/arthurtok/feature-ranking-w-randomforest-rfe-linear-models/code.  A lot of Anisotropic's comments are used in this kernel to explain what is being done.\n\nMy work is an application of the techniques and methods written by those gentlemen to the Zillow data competition. I have included additional competition specific data cleaning and comments in addition to the feature selection code.\n\nThe contents of this notebook are as follows: \n\n 1. **Data Cleaning and Visualisation** : This section will revolve around exploring the data and visualising some summary statistics. \n 2. **Stability Selection via Randomised Lasso Method** : Introduce a relatively new feature selection method called \"Stability Selection\" and using the Randomised Lasso in its implementation\n 3. **Recursive Feature Elimination** : Implementing the Recursive Feature Elimination method of feature ranking via the use of basic Linear Regression \n 4. **Linear Model Feature Coefficients** : Implementing 3 of Sklearn's linear models (Linear Regression, Lasso and Ridge) and using the inbuilt estimated coefficients for our feature selection\n 5. **Random Forest Feature Selection** : Using the Random Forest's convenient attribute \"feature_importances\" to calculate and ultimately rank the feature importance.\n\nFinally, with all the points 1 to 5 above, we will combine the results to create our:\n\n**Feature Ranking Matrix** : Matrix of all the features along with the respective model scores which we can use in our ranking.","cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.feature_selection import RFE, f_regression\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"metadata":{"_uuid":"9443b3ea8bebeba0f134529375974b6c63722015","_cell_guid":"0b231163-95bb-e5d3-eda2-dd6703aa8649","collapsed":true},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"5c3eb6eb38eb2cb7a119a52df82845d57d815a98","_cell_guid":"2b932325-d368-ee17-75cb-40feb7e8d1bf"},"outputs":[],"source":"# 1. DATA CLEANSING AND ANALYSIS\n\nLet's first read in the house data as a dataframe \"house\" and inspect the first 5 rows","cell_type":"markdown"},{"source":"# Load Data\ntrain = pd.read_csv('../input/train_2016_v2.csv')\nprop = pd.read_csv('../input/properties_2016.csv')\n\n# Convert to float32\nfor c, dtype in zip(prop.columns, prop.dtypes):\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)\n\n# Merge training dataset with properties dataset\ndf_train = train.merge(prop, how='left', on='parcelid')\n\n# Remove ID columns\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)","execution_count":null,"metadata":{"_uuid":"2fde725223b5232c02aedae8b993d3bb1c3d8eb4","collapsed":false},"outputs":[],"cell_type":"code"},{"source":"x_train.head()","execution_count":null,"metadata":{"_uuid":"ee8644c1339dc96ddffb9dea31086f76624e26eb","_cell_guid":"1ca75509-383d-c6f5-d809-8a0e99b4e844","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"58414416d02484843c6466f0dd754c08877f0e55","_cell_guid":"78ba4e90-1d29-ae62-9fc5-ad0b590a1845"},"outputs":[],"source":"Now its time for some general data inspection. Let's first examine to see if there are any nulls in the dataframe as well as look at the type of the data (i.e whether it is a string or numeric)","cell_type":"markdown"},{"source":"print(x_train.dtypes)","execution_count":null,"metadata":{"_uuid":"c3a61eb6ae120e9326d4814fd8b32fbc63f2d710","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"12fece0d369fe97e6481a50de34b47b6b9bb7341"},"outputs":[],"source":"I'll see what values are in the object type columns -- We will have to either break these features up so that for each option there is just a binary (True / False) indicator, or find a way to make the existing features binary.","cell_type":"markdown"},{"source":"x_train['taxdelinquencyflag'].value_counts()","execution_count":null,"metadata":{"_uuid":"6a5ff24a412f6cb04cc8d2a709931a79d7634c4d","collapsed":false},"outputs":[],"cell_type":"code"},{"source":"x_train['fireplaceflag'].value_counts()","execution_count":null,"metadata":{"_uuid":"30e6bdb7ea9912b17b5503f2a4c98e95c714d928","collapsed":false},"outputs":[],"cell_type":"code"},{"source":"x_train['hashottuborspa'].value_counts()","execution_count":null,"metadata":{"_uuid":"2b6fca3dbe75ce912bdc9ca14f9b2a75fa838342","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"0c0dcc9ba2c0d32b65ca19df68c54168f8e8b7ae"},"outputs":[],"source":"I'll have to convert the 'Y' to a TRUE and otherwise convert any NaN to False's","cell_type":"markdown"},{"source":"x_train['taxdelinquencyflag'] = x_train['taxdelinquencyflag'] == 'Y'","execution_count":null,"metadata":{"_uuid":"dbcc18c46580834dadc919a70ea40733af258dfd","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"for c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)","execution_count":null,"metadata":{"_uuid":"aeb9c2da9b8c904366e0d62e93491d2c24702020","collapsed":false},"outputs":[],"cell_type":"code"},{"source":"# Looking for nulls\nprint(x_train.isnull().sum())","execution_count":null,"metadata":{"_uuid":"d15d597083b8c89ff9999f4fd50a4af64df3e021","_cell_guid":"cb6e65da-ba74-0ffe-828d-ebf7db2b1c07","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"d95e89ee405a134c45c0f36a3c8cfc9b3bec2fc8","_cell_guid":"aa63409f-4adf-d8dc-c781-1217f87710d8"},"outputs":[],"source":"Yikes, the data is pretty sparse.  We're going to have to figure out how to either remove some features or impute values for some of these.\n\nTemporarily I'll be imputing averages for the missing values based on the missing value for said column, but its only a stopgap solution before I can do something else (possibly predict missing values?)","cell_type":"markdown"},{"source":"x_train = x_train.fillna(x_train.mean())","execution_count":null,"metadata":{"_uuid":"ee984e0da95a38641e4a34ece5dfdfe5530ba515","collapsed":true},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"a3668fbd5d02ee623aaf7f2cbc430c8301989db3","_cell_guid":"fc3d8109-ebc5-264c-f964-04f66da0fec0"},"outputs":[],"source":"# 2. Stability Selection via Randomized Lasso\n\nIn a nutshell, this method serves to apply the feature selection on different parts of the data and features repeatedly until the results can be aggregated. Therefore stronger features ( defined as being selected as important) will have greater scores in this method as compared to weaker features. Refer to this paper by Nicolai Meinshausen and Peter Buhlmann for a much greater detail on the method : http://stat.ethz.ch/~nicolai/stability.pdf\n\nIn this notebook, the Stability Selection method is conveniently inbuilt into sklearn's randomized lasso model and therefore this will be implemented as follows:","cell_type":"markdown"},{"source":"# First extract the target variable which is our Log Error\nY = df_train['logerror'].values\nX = x_train.as_matrix()\n\n# Store the column/feature names into a list \"colnames\"\ncolnames = x_train.columns","execution_count":null,"metadata":{"_uuid":"f7b3717f89e357ee844277dde65f298fd2804276","_cell_guid":"45351992-0216-dd3d-76fe-4a658674f0c4","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"1a7926be983626aa6889161a19eee043eef24051","_cell_guid":"1c239bb6-b82e-e35b-7c3c-580c98094cce"},"outputs":[],"source":"Next, we create a function which will be able to conveniently store our feature rankings obtained from the various methods described here into a Python dictionary. In case you are thinking I created this function, no this is not the case. All credit goes to Ando Saabas and I am only trying to apply what he has discussed in this context.","cell_type":"markdown"},{"source":"# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))","execution_count":null,"metadata":{"_uuid":"c92f382f667f648e7679f4b8b48100dd56273119","_cell_guid":"ecc7b17a-b6d5-7d6c-726d-de7b75c3c1eb","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"# Finally let's run our Selection Stability method with Randomized Lasso\nrlasso = RandomizedLasso(alpha=0.04)\nrlasso.fit(X, Y)\nranks[\"rlasso/Stability\"] = ranking(np.abs(rlasso.scores_), colnames)\nprint('finished')","execution_count":null,"metadata":{"_uuid":"53bdbc76d42cb608da895a5f57f50d6a714bbb4d","_cell_guid":"a56e6fbf-1511-2ff7-0693-2f55aa0d21cd","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"e4620d40f9df515483eb62a507425f74345ca99f","_cell_guid":"15244118-31f5-e2df-8ce0-34a6d8f5c729"},"outputs":[],"source":"# 3. Recursive Feature Elimination ( RFE )\n\nNow onto the next method in our feature ranking endeavour. Recursive Feature Elimination or RFE uses a model ( eg. linear Regression or SVM) to select either the best or worst-performing feature, and then excludes the feature. After this, the whole process is iterated until all features in the dataset are used up ( or up to a user-defined limit). Sklearn conveniently possesses a RFE function via the sklearn.feature_selection call and we will use this along with a simple linear regression model for our ranking search as follows:","cell_type":"markdown"},{"source":"# Construct our Linear Regression model\nlr = LinearRegression(normalize=True)\nlr.fit(X,Y)\n#stop the search when only the last feature is left\nrfe = RFE(lr, n_features_to_select=1, verbose =3 )\nrfe.fit(X,Y)\nranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)","execution_count":null,"metadata":{"_uuid":"f4c3fbbc4edeca6db3b183ed14790cb87f638c73","_cell_guid":"e11f8021-2f8b-074b-d0cc-c6d9d6d8ae8e","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"757117f801fb02d422a4e768e40a380e838e57df","_cell_guid":"c6457163-f8c0-85a7-d69f-5ca2a880010f"},"outputs":[],"source":"# 4. Linear Model Feature Ranking\n\nNow let's apply 3 different linear models (Linear, Lasso and Ridge Regression) and how the features are selected and prioritised via these models. To achieve this, I shall use the sklearn implementation of these models and in particular the attribute .coef to return the estimated coefficients for each feature in the linear model.","cell_type":"markdown"},{"source":"# Using Linear Regression\nlr = LinearRegression(normalize=True)\nlr.fit(X,Y)\nranks[\"LinReg\"] = ranking(np.abs(lr.coef_), colnames)\n\n# Using Ridge \nridge = Ridge(alpha = 7)\nridge.fit(X,Y)\nranks['Ridge'] = ranking(np.abs(ridge.coef_), colnames)\n\n# Using Lasso\nlasso = Lasso(alpha=.05)\nlasso.fit(X, Y)\nranks[\"Lasso\"] = ranking(np.abs(lasso.coef_), colnames)","execution_count":null,"metadata":{"_uuid":"7eae3169285b35f76de75292ae4f6091c31960a9","_cell_guid":"64b0534b-c62a-7d85-e181-615b7a3743c9","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"edb4e6e2625bb63f06d53dd9db73402a83575b9a","_cell_guid":"ee729bd0-270e-dc30-8fc8-b2d79873c4e1"},"outputs":[],"source":"# 5. Random Forest feature ranking\n\nSklearn's Random Forest model also comes with it's own inbuilt feature ranking attribute and one can conveniently just call it via \"feature_importances_\". That is what we will be using as follows:","cell_type":"markdown"},{"source":"rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=2)\nrf.fit(X,Y)\nranks[\"RF\"] = ranking(rf.feature_importances_, colnames)","execution_count":null,"metadata":{"_uuid":"f8ab99420883f6e3391bdb97048454c03e9c4f8a","_cell_guid":"f013dad9-8855-530f-f792-48163dab2457","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"37dd6667f94db4526867b21b3375df16954610ef","_cell_guid":"20ed4bd0-29b6-86a8-0bb8-cd34424376d5"},"outputs":[],"source":"# 6. Creating the Feature Ranking Matrix\n\nWe combine the scores from the various methods above and output it in a matrix form for convenient viewing as such:","cell_type":"markdown"},{"source":"# Create empty dictionary to store the mean value calculated from all the scores\nr = {}\nfor name in colnames:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)\n \nmethods = sorted(ranks.keys())\nranks[\"Mean\"] = r\nmethods.append(\"Mean\")\n\nprint(\"\\t%s\" % \"\\t\".join(methods))\nfor name in colnames:\n    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n                         [ranks[method][name] for method in methods]))))","execution_count":null,"metadata":{"_uuid":"e09a8414122d190ae530b4dcc8b59bf7c6a4d82a","_cell_guid":"b6eb5b25-cc7e-f1eb-6a7b-8f591b7e560e","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"4b68741c8ffc59327db34488273b990a5fa6c6aa","_cell_guid":"5d9d44d0-fd8d-894e-657f-81762195499c"},"outputs":[],"source":"Now, with the matrix above, the numbers and layout does not seem very easy or pleasant to the eye. Therefore, let's just collate the mean ranking score attributed to each of the feature and plot that via Seaborn's factorplot.","cell_type":"markdown"},{"source":"# Put the mean scores into a Pandas dataframe\nmeanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n\n# Sort the dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)","execution_count":null,"metadata":{"_uuid":"d798434946ba4507b1138a5ece8647c757b763b9","_cell_guid":"daae301e-e669-b2b6-6978-cbcad579696c","collapsed":false},"outputs":[],"cell_type":"code"},{"source":"# Let's plot the ranking of the features\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", size=10, aspect=1, palette='coolwarm')","execution_count":null,"metadata":{"_uuid":"3f78ab9b8271d36d4405ef83b3fac1609f2c6e2a","_cell_guid":"2bdfa64e-d23d-d042-5e9d-080beb245ece","collapsed":false},"outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"805f7bde7a1120da86f1e0cadd03f276ab7e7f38"},"outputs":[],"source":"# Conclusion\n\nThe top 3 features are \"Three Quarter Bathroom Count\", \"Finished Square Feet\", and \"Tax Delinquincy Flag\".  \"Tax amount\" and \"Has Hot Tub or Spa\" are 4th and 5th.\n\nI'm concerned that the 3/4 bathroom count is listed so highly- that feature is null for about 80% of the scenarios. It also doesn't have an intuitive explanation for why it'd impact the sell price.  On the other hand, square footage, hot tub or spa presence, and tax amount are all very intuitive features that we'd expect to do well.\n\nMy biggest concern is that there are so many nulls in my training set and that simply imputing the average is not a proper solution.  ","cell_type":"markdown"},{"source":"","execution_count":null,"metadata":{"_uuid":"3721ffbc92985a77682779973355c91ad06f6b12","collapsed":true},"outputs":[],"cell_type":"code"}],"metadata":{"_is_fork":false,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"_change_revision":0,"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.1","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}