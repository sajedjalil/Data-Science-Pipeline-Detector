{"nbformat":4,"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"9b42b9efcc543b9b627f8d520e6eb02442c4cc20"},"outputs":[],"source":"## Kaggle Competition Entry: Zillow Home Prices\n\nThis is a walk-through of Anokas's excellent XGB Starter Kernel, found here: https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655/code\n\nA lot of Kaggle kernels lack documentation or any commenting, so before I jump into a Kaggle project I like to first find a popular kernel by a respectable author and get a sense of what has been done and what popular modifications have been done to the kernel before jumping in myself.  Hopefully you will find this useful as well!"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7c548b5f6d2c38e37e65daefcf1a9e4b7781550a","_execution_state":"idle","collapsed":false},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb # xgboost package \nimport gc # to take out da trash [memory management]"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ff3d02b33d7c41423690bf78109fe25b9ff58250","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Lets see whats in our input folder\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ce4426cb3e07eaa90e96945166dbe8d5dda35d8e","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Load Data\ntrain = pd.read_csv('../input/train_2016_v2.csv')\nprop = pd.read_csv('../input/properties_2016.csv')\nsample = pd.read_csv('../input/sample_submission.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2c16cf2cdb78af0b59ddb7a37d4fbbdece5da4d4","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Print Data Shape\n# In order to submit to Kaggle we'll be modifiying the sample dataset with our predictions\nprint (train.shape, prop.shape, sample.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5b84a56bd5e3f17504283f763782519e22cb5f62","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Convert to Float32 \n# This is so that our script can run on Kaggle Kernels\n# Kaggle has a memory limit on the Kernels so this is a necessary step\n# We're turning 64 bit floats into 32 bit floats\n\nfor c, dtype in zip(prop.columns, prop.dtypes):\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5cb1c9c2f50000e37456b1ad719ad90d85b8cfe8","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Merge training dataset with properties dataset\ndf_train = train.merge(prop, how='left', on='parcelid')\n\n# Remove useless columns (anything used for ID purposes, has no variation, or is not suitable for training)\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n\n# Save Train columns\ntrain_columns = x_train.columns\n\n# Train our model to predict log error\ny_train = df_train['logerror'].values\n\n# Binarify our categorical column variables to remove NaN objects\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\n# Delete our old training dataset; take out the trash\ndel df_train; gc.collect()"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bad248a688d17a06771c0a69ec52a8b63dc513d8","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Split dataset at roughly the ~88% mark into training and validation datasets\n# We'll be evaluating the fine tuning of our model by seeing how it runs on the validation dataset\n\nsplit = 80000\nx_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ad02f75a5302a3c8e14ca9d01e37642166df1320","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Split training and validation datasets\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\ndel x_train, x_valid; gc.collect()"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1092e63a505817e70320dd69b9115f038bb3ad79","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Set hyperparameters\n# Only hyperparamater that is relevant for optimizing here (in Anokas's notebook) is max_depth.  \n# \n# When I build my own submission I will try tuning gamma, min_child_weight, subsample, \n# colsample_bytree, as well as the regularization paramaters.\n\nparams = {}\nparams['eta'] = 0.02 # control the learning rate: scale the contribution of each tree by a factor of 0 < eta < 1. Lower is slower but more robust to overfitting.\nparams['objective'] = 'reg:linear' # Default.  Running a regression, since we're predicting values not classes\nparams['eval_metric'] = 'mae' # We're evaluating our models on Mean Average Error.  \nparams['max_depth'] = 4 # Maximum depth of a tree, increase this value will make the model more complex / likely to be overfitting.\nparams['silent'] = 1 # Don't print messages\n\n# Train model\n#\n# 'Watchlist' is an evaluation dataset- We will be tuning our model based on how it does in the validation dataset \nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#\n# Anokas has implemented early stopping.  Once we reach the point where our validation score no \n# longer improves after a set number of iterations (100 in this case) we use the model run that preceeded the \n# chain of 100 un-changed iterations.  This is to prevent additional overfitting where it does not improve the model.\nclf = xgb.train(params, d_train, 10000, watchlist, early_stopping_rounds=100, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"79cca3dce664d813de5e15cf708e8b838cd5a07a","_execution_state":"idle","collapsed":false},"outputs":[],"source":"# Build test set\n\nsample['parcelid'] = sample['ParcelId']\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\n# Memory Management\ndel prop; gc.collect()\n\n# Binarify the data (ie remove NaN, set it to False)\nx_test = df_test[train_columns]\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\n\n# Memory management\ndel df_test, sample; gc.collect()\n\n# Convert table to xgb format\nd_test = xgb.DMatrix(x_test)\n\n# Memory management\ndel x_test; gc.collect()"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ad16e70b3901aaee71eea93c5da9d2de2b92de4d","_execution_state":"idle","collapsed":true},"outputs":[],"source":"# Make predictions on data\np_test = clf.predict(d_test)\n\n# Delete testset; take out trash\ndel d_test; gc.collect()\n\n# Read sample subgmission\nsub = pd.read_csv('../input/sample_submission.csv')\nfor c in sub.columns[sub.columns != 'ParcelId']:\n    sub[c] = p_test\n\n# Write submission\nsub.to_csv('xgb_starter.csv', index=False, float_format='%.4f')"},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"569c4c8d15ef968a8c82e8def97bff5ad23b7ed1","_execution_state":"idle","collapsed":false},"outputs":[],"source":""}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":2},"pygments_lexer":"ipython2","file_extension":".py","nbconvert_exporter":"python","name":"python","mimetype":"text/x-python","version":"2.7.13"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":2}