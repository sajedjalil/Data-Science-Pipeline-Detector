{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{},"source":"This replaces [Part II](https://www.kaggle.com/aharless/my-solution-part-ii-final-xgboost-with-bug) in the 2nd model."},{"source":"AS_DEMO = True\n\nif AS_DEMO:\n    LR = .05\n    NROUNDS = 100\nelse:\n    LR = 0.007\n    NROUNDS = 1800","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"MAKE_SUBMISSION = True          # Generate output file.\nCV_ONLY = False                 # Do validation only; do not generate predicitons.\nFIT_FULL_TRAIN_SET = True       # Fit model to full training set after doing validation.\nFIT_2017_TRAIN_SET = False      # Use 2017 training data for full fit (no leak correction)\nFIT_COMBINED_TRAIN_SET = True  # Fit combined 2016-2017 training set\nUSE_SEASONAL_FEATURES = True\nVAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\nLEARNING_RATE = LR              # shrinkage rate for boosting roudns\nROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\nOPTIMIZE_FUDGE_FACTOR = False   # Optimize factor by which to multiply predictions.\nFUDGE_FACTOR_SCALEDOWN = 0.3    # exponent to reduce optimized fudge factor for prediction","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"47787be314e3cc68e7071ae827aef9710fc8341a","_cell_guid":"f16ac0de-de1c-4b65-a85e-a3227fbe0c47"},"execution_count":1},{"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport datetime as dt\nfrom datetime import datetime\nimport gc\nimport patsy\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.regression.quantile_regression import QuantReg","cell_type":"code","outputs":[],"metadata":{"_uuid":"a6d445499b3664bf165ab992f309a417471707f4","_execution_state":"idle","_cell_guid":"1c1a4c8e-9379-47b8-aa2e-3df8e5d22918"},"execution_count":2},{"source":"properties16 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\nproperties17 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n\n# Number of properties in the zip\nzip_count = properties16['regionidzip'].value_counts().to_dict()\n# Number of properties in the city\ncity_count = properties16['regionidcity'].value_counts().to_dict()\n# Median year of construction by neighborhood\nmedyear = properties16.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n# Mean square feet by neighborhood\nmeanarea = properties16.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n# Neighborhood latitude and longitude\nmedlat = properties16.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\nmedlong = properties16.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\nfor c in properties16.columns:\n    properties16[c]=properties16[c].fillna(-1)\n    if properties16[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties16[c].values))\n        properties16[c] = lbl.transform(list(properties16[c].values))","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"43ea1cf09b04ca53bb99c95d428b4624572ba75a","_execution_state":"idle","_cell_guid":"a6cc2bdd-9d38-4f66-8903-3ce019f7109a"},"execution_count":3},{"source":"train_df = train.merge(properties16, how='left', on='parcelid')\nselect_qtr4 = pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE\nif USE_SEASONAL_FEATURES:\n    basedate = pd.to_datetime('2015-11-15').toordinal()\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"dfe82baffa246f7ebe77d472de49ee6a6d43a419","_cell_guid":"b5752c23-b3d7-4403-ab0b-2813d88e14f5"},"execution_count":4},{"source":"del train\ngc.collect()","cell_type":"code","outputs":[],"metadata":{"_uuid":"95c7a84848e30fa37a75d319b9e7e04deaff9ef9","_cell_guid":"101437b6-c54e-4452-9c81-a824818d651a"},"execution_count":5},{"source":"# Inputs to features that depend on target variable\n# (Ideally these should be recalculated, and the dependent features recalculated,\n#  when fitting to the full training set.  But I haven't implemented that yet.)\n\n# Standard deviation of target value for properties in the city/zip/neighborhood\ncitystd = train_df[~select_qtr4].groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\nzipstd = train_df[~select_qtr4].groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\nhoodstd = train_df[~select_qtr4].groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"9722eb834d3cf02e943b8c85bf99e7a7515173e4","_cell_guid":"d9460c6d-bf39-4bfb-a584-f3aafcabc8fa"},"execution_count":6},{"source":"def calculate_features(df):\n    # Nikunj's features\n    # Number of properties in the zip\n    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n    # Number of properties in the city\n    df['N-city_count'] = df['regionidcity'].map(city_count)\n    # Does property have a garage, pool or hot tub and AC?\n    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n                         (df['pooltypeid10']>0) & \\\n                         (df['airconditioningtypeid']!=5))*1 \n\n    # More features\n    # Mean square feet of neighborhood properties\n    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n    # Median year of construction of neighborhood properties\n    df['med_year'] = df['regionidneighborhood'].map(medyear)\n    # Neighborhood latitude and longitude\n    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n    df['med_long'] = df['regionidneighborhood'].map(medlong)\n\n    df['zip_std'] = df['regionidzip'].map(zipstd)\n    df['city_std'] = df['regionidcity'].map(citystd)\n    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)\n    \n    if USE_SEASONAL_FEATURES:\n        df['cos_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n                             (2*np.pi/365.25) ).apply(np.cos)\n        df['sin_season'] = ( (pd.to_datetime(df['transactiondate']).apply(lambda x: x.toordinal()-basedate)) * \\\n                             (2*np.pi/365.25) ).apply(np.sin)\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"4b9916ad9eb1d2a0b1aab78e71191c34a78a9b77","_cell_guid":"058d3299-005e-4458-a8cd-cd3c4d854190"},"execution_count":7},{"source":"dropvars = ['airconditioningtypeid', 'buildingclasstypeid',\n            'buildingqualitytypeid', 'regionidcity']\ndroptrain = ['parcelid', 'logerror', 'transactiondate']\ndroptest = ['ParcelId']","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"d816890fe0edcab13c415144e76ecd12938ab17e","_cell_guid":"e389a360-b1dc-4257-b3c3-fea69026863d"},"execution_count":8},{"source":"calculate_features(train_df)\n\nx_valid = train_df.drop(dropvars+droptrain, axis=1)[select_qtr4]\ny_valid = train_df[\"logerror\"].values.astype(np.float32)[select_qtr4]\n\nprint('Shape full training set: {}'.format(train_df.shape))\nprint('Dropped vars: {}'.format(len(dropvars+droptrain)))\nprint('Shape valid X: {}'.format(x_valid.shape))\nprint('Shape valid y: {}'.format(y_valid.shape))\n\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nprint('\\nFull training set after removing outliers, before dropping vars:')     \nprint('Shape training set: {}\\n'.format(train_df.shape))\n\nif FIT_FULL_TRAIN_SET:\n    full_train = train_df.copy()\n\ntrain_df=train_df[~select_qtr4]\nx_train=train_df.drop(dropvars+droptrain, axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\nn_train = x_train.shape[0]\nprint('Training subset after removing outliers:')     \nprint('Shape train X: {}'.format(x_train.shape))\nprint('Shape train y: {}'.format(y_train.shape))\n\nif FIT_FULL_TRAIN_SET:\n    x_full = full_train.drop(dropvars+droptrain, axis=1)\n    y_full = full_train[\"logerror\"].values.astype(np.float32)\n    n_full = x_full.shape[0]\n    print('\\nFull trainng set:')     \n    print('Shape train X: {}'.format(x_train.shape))\n    print('Shape train y: {}'.format(y_train.shape))","cell_type":"code","outputs":[],"metadata":{"_uuid":"233443f9b4c2af79e91d235fdc4f660ff1d630bc","_cell_guid":"720a950e-5392-4571-8fd6-e849a9c6967e"},"execution_count":9},{"source":"if not CV_ONLY:\n    # Generate test set data\n    \n    sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n    \n    # Process properties for 2016\n    test_df = pd.merge( sample_submission[['ParcelId']], \n                        properties16.rename(columns = {'parcelid': 'ParcelId'}), \n                        how = 'left', on = 'ParcelId' )\n    if USE_SEASONAL_FEATURES:\n        test_df['transactiondate'] = '2017-10-31'\n        droptest += ['transactiondate']\n    calculate_features(test_df)\n    x_test = test_df.drop(dropvars+droptest, axis=1)\n    print('Shape test: {}'.format(x_test.shape))\n\n    # Process properties for 2017\n    for c in properties17.columns:\n        properties17[c]=properties17[c].fillna(-1)\n        if properties17[c].dtype == 'object':\n            lbl = LabelEncoder()\n            lbl.fit(list(properties17[c].values))\n            properties17[c] = lbl.transform(list(properties17[c].values))\n    zip_count = properties17['regionidzip'].value_counts().to_dict()\n    city_count = properties17['regionidcity'].value_counts().to_dict()\n    medyear = properties17.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n    meanarea = properties17.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n    medlat = properties17.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n    medlong = properties17.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n\n    test_df = pd.merge( sample_submission[['ParcelId']], \n                        properties17.rename(columns = {'parcelid': 'ParcelId'}), \n                        how = 'left', on = 'ParcelId' )\n    if USE_SEASONAL_FEATURES:\n        test_df['transactiondate'] = '2017-10-31'\n    calculate_features(test_df)\n    x_test17 = test_df.drop(dropvars+droptest, axis=1)\n\n    del test_df","cell_type":"code","outputs":[],"metadata":{"_uuid":"f3a9ea8db9161705b9e0d61a82736b44c7dfcb01","_execution_state":"idle","_cell_guid":"d0c5819c-0873-4b49-a8a7-5295632ccbb9"},"execution_count":10},{"source":"del train_df\ndel select_qtr4\ngc.collect()","cell_type":"code","outputs":[],"metadata":{"_uuid":"c6d653da30197972f12f1a7a0ea4d0a36f28c554","_execution_state":"idle","_cell_guid":"03fb6b2f-5166-4646-8445-304765a404ad"},"execution_count":11},{"source":"xgb_params = {  # best as of 2017-09-28 13:20 UTC\n    'eta': LEARNING_RATE,\n    'max_depth': 7, \n    'subsample': 0.6,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 5.0,\n    'alpha': 0.65,\n    'colsample_bytree': 0.5,\n    'base_score': y_mean,'taxdelinquencyyear'\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndvalid_x = xgb.DMatrix(x_valid)\ndvalid_xy = xgb.DMatrix(x_valid, y_valid)\nif not CV_ONLY:\n    dtest = xgb.DMatrix(x_test)\n    dtest17 = xgb.DMatrix(x_test17)\n    del x_test","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"c8c1043d72790dc926ae76f398487e8313caeed1","_execution_state":"idle","_cell_guid":"de59798a-1b8a-4d0e-a92c-846577f4406b"},"execution_count":12},{"source":"del x_train\ngc.collect()","cell_type":"code","outputs":[],"metadata":{"_uuid":"dd0776733ec3d3ac960b9ab9ac6e518a99e012d9","_execution_state":"idle","_cell_guid":"45e61d05-da7f-4841-811a-3a95c86a3306"},"execution_count":13},{"source":"num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\nearly_stopping_rounds = round( num_boost_rounds / 20 )\nprint('Boosting rounds: {}'.format(num_boost_rounds))\nprint('Early stoping rounds: {}'.format(early_stopping_rounds))","cell_type":"code","outputs":[],"metadata":{"scrolled":true,"_uuid":"fe6761fd44aaf7a5c063897c9ca3b90b957b7108","_cell_guid":"de8e97e6-d681-465b-a68a-6178e7593e49"},"execution_count":14},{"source":"evals = [(dtrain,'train'),(dvalid_xy,'eval')]\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n                  evals=evals, early_stopping_rounds=early_stopping_rounds, \n                  verbose_eval=10)","cell_type":"code","outputs":[],"metadata":{"_uuid":"09c99f75bcfb64225f390eee5e5dbf4f47f4103b","_execution_state":"idle","_cell_guid":"95d8ac93-f2ff-4ce3-9cb6-8044d73f8600"},"execution_count":15},{"source":"valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\nprint( \"XGBoost validation set predictions:\" )\nprint( pd.DataFrame(valid_pred).head() )\nprint(\"\\nMean absolute validation error:\")\nmean_absolute_error(y_valid, valid_pred)","cell_type":"code","outputs":[],"metadata":{"_uuid":"02a37067bd8ba72b050d6634e9994b234bdf6389","_execution_state":"idle","_cell_guid":"c241b1e2-d2c6-4936-a6ee-282a4e90ca27"},"execution_count":16},{"cell_type":"markdown","metadata":{},"source":"0.064258203"},{"source":"if OPTIMIZE_FUDGE_FACTOR:\n    mod = QuantReg(y_valid, valid_pred)\n    res = mod.fit(q=.5)\n    print(\"\\nLAD Fit for Fudge Factor:\")\n    print(res.summary())\n\n    fudge = res.params[0]\n    print(\"Optimized fudge factor:\", fudge)\n    print(\"\\nMean absolute validation error with optimized fudge factor: \")\n    print(mean_absolute_error(y_valid, fudge*valid_pred))\n\n    fudge **= FUDGE_FACTOR_SCALEDOWN\n    print(\"Scaled down fudge factor:\", fudge)\n    print(\"\\nMean absolute validation error with scaled down fudge factor: \")\n    print(mean_absolute_error(y_valid, fudge*valid_pred))\nelse:\n    fudge=1.0","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"dbd7ae96da81ab900d6eee1c186b2b56f17e4fc8","_cell_guid":"7dd798fb-9160-4a48-9b52-9bc16f582e0f"},"execution_count":17},{"source":"if FIT_FULL_TRAIN_SET and not CV_ONLY:\n    if FIT_COMBINED_TRAIN_SET:\n        # Merge 2016 and 2017 data sets\n        train16 = pd.read_csv('../input/train_2016_v2.csv')\n        train17 = pd.read_csv('../input/train_2017.csv')\n        train16 = pd.merge(train16, properties16, how = 'left', on = 'parcelid')\n        train17 = pd.merge(train17, properties17, how = 'left', on = 'parcelid')\n        train_df = pd.concat([train16, train17], axis = 0)\n        # Generate features\n        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n        calculate_features(train_df)\n        # Remove outliers\n        train_df=train_df[ train_df.logerror > -0.4 ]\n        train_df=train_df[ train_df.logerror < 0.419 ]\n        # Create final training data sets\n        x_full = train_df.drop(dropvars+droptrain, axis=1)\n        y_full = train_df[\"logerror\"].values.astype(np.float32)\n        n_full = x_full.shape[0]     \n    elif FIT_2017_TRAIN_SET:\n        train = pd.read_csv('../input/train_2017.csv')\n        train_df = train.merge(properties17, how='left', on='parcelid')\n        # Generate features\n        citystd = train_df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n        zipstd = train_df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n        hoodstd = train_df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n        calculate_features(train_df)\n        # Remove outliers\n        train_df=train_df[ train_df.logerror > -0.4 ]\n        train_df=train_df[ train_df.logerror < 0.419 ]\n        # Create final training data sets\n        x_full = train_df.drop(dropvars+droptrain, axis=1)\n        y_full = train_df[\"logerror\"].values.astype(np.float32)\n        n_full = x_full.shape[0]     \n\n    dtrain = xgb.DMatrix(x_full, y_full)\n    num_boost_rounds = int(model.best_ntree_limit*n_full/n_train)","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":18},{"source":"del properties16\ndel properties17\ngc.collect()","cell_type":"code","outputs":[],"metadata":{"_uuid":"a39b9309cbaa1588b0646a079c4bfea929990cc1","_cell_guid":"74ad3d77-75eb-447c-9524-21c225507879"},"execution_count":19},{"source":"optimal_number_of_trees = NROUNDS\nnum_ensembles = 8\npreds = 0.0\npreds17 = 0.0\nfor i in range(num_ensembles):\n    xgb_params['seed'] = i\n    full_model = xgb.train(xgb_params, dtrain, num_boost_round=optimal_number_of_trees, \n                           evals=[(dtrain,'train')], verbose_eval=10)\n    pred = fudge*full_model.predict(dtest)\n    pred17 = fudge*full_model.predict(dtest17)\n    print( \"/ni = \", i)\n    print( \"XGBoost test set predictions for 2016:\" )\n    print( pd.DataFrame(pred).head() )\n    print( \"XGBoost test set predictions for 2017:\" )\n    print( pd.DataFrame(pred17).head() )\n    preds += pred\n    preds17 += pred17\npreds /= num_ensembles\n\n# SHOULD ALSO CONTAIN THE LINE \"preds17 /= num_ensembles\"\n# BUT I NEGLECTED TO ADD IT, SO RESULTS ARE CRAZY\n\nprint( \"/n/nFinal Ensemble\")\nprint( \"XGBoost test set predictions for 2016:\" )\nprint( pd.DataFrame(preds).head() )\nprint( \"XGBoost test set predictions for 2017:\" )\nprint( pd.DataFrame(preds17).head() )\n","cell_type":"code","outputs":[],"metadata":{},"execution_count":20},{"source":"y_pred=[]\ny_pred17=[]\n\nfor i,predict in enumerate(preds):\n       y_pred.append(str(round(predict,4)))\nfor i,predict in enumerate(preds17):\n       y_pred17.append(str(round(predict,4)))\ny_pred=np.array(y_pred)\ny_pred17=np.array(y_pred17)\n\noutput = pd.DataFrame({'ParcelId': sample_submission['ParcelId'].astype(np.int32),\n           '201610': y_pred, '201611': y_pred, '201612': y_pred,\n           '201710': y_pred17, '201711': y_pred17, '201712': y_pred17})\n   # set col 'ParceID' to first col\ncols = output.columns.tolist()\ncols = cols[-1:] + cols[:-1]\noutput = output[cols]\n\noutput.to_csv('finalxgb{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), \n              float_format='%.6f',\n              index=False)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"b85616fbde70ef9d6fe220ef12947fb5d2dd3df6","_execution_state":"idle","_cell_guid":"8d793a38-6df2-45e7-a725-cc2161ed3cee"},"execution_count":21},{"source":"","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"b26353143ba91c98ba970f5051bc9656b39c9bb4","_cell_guid":"b66c3b07-5941-44ab-b2f2-58fc46071055"},"execution_count":null},{"source":"","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"9944aa5d56350e51a0b151af9cf38c7b464dcf0b","_cell_guid":"e5b97a3f-4428-487c-abd4-9507d7850260"},"execution_count":null}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.0","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","name":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1}