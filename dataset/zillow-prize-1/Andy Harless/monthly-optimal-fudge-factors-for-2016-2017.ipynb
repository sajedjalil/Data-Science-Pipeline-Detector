{"cells":[{"metadata":{"_cell_guid":"d4564fe2-06e6-4642-b2a6-a3bd3725c21e","_uuid":"724353a43137568165f0a953ab30f7b13e9881c3"},"cell_type":"markdown","source":"The important thing is that the optimal fudge factors for 2017 turn out to be similar to those for 2016, which suggests that fudge factors derived from probing the public leaderboard will work for the private leaderboard."},{"metadata":{"_cell_guid":"f16ac0de-de1c-4b65-a85e-a3227fbe0c47","_uuid":"47787be314e3cc68e7071ae827aef9710fc8341a","collapsed":true},"execution_count":null,"source":"LEARNING_RATE = 0.02            # shrinkage rate for boosting roudns\nROUNDS_PER_ETA = 20             # maximum number of boosting rounds times learning rate\nVAL_SPLIT_DATE = '2016-09-15'   # First 2016 date not comparable to 2017 training data","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"1c1a4c8e-9379-47b8-aa2e-3df8e5d22918","_execution_state":"idle","_uuid":"a6d445499b3664bf165ab992f309a417471707f4","collapsed":true},"execution_count":null,"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport datetime as dt\nfrom datetime import datetime\nimport gc\nimport patsy\nimport statsmodels.api as smMonthly \nimport statsmodels.formula.api as smf\nfrom statsmodels.regression.quantile_regression import QuantReg","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f288b2d1-010c-4f6b-9401-f849e1433380","_uuid":"6d5c6e151491103206a9c72ab7d4ecd1de50a1ec","collapsed":true},"execution_count":null,"source":"def calculate_aggregates(properties):\n    # Number of properties in the zip\n    zip_count = properties['regionidzip'].value_counts().to_dict()\n    # Number of properties in the city\n    city_count = properties['regionidcity'].value_counts().to_dict()\n    # Median year of construction by neighborhood\n    medyear = properties.groupby('regionidneighborhood')['yearbuilt'].aggregate('median').to_dict()\n    # Mean square feet by neighborhood\n    meanarea = properties.groupby('regionidneighborhood')['calculatedfinishedsquarefeet'].aggregate('mean').to_dict()\n    # Neighborhood latitude and longitude\n    medlat = properties.groupby('regionidneighborhood')['latitude'].aggregate('median').to_dict()\n    medlong = properties.groupby('regionidneighborhood')['longitude'].aggregate('median').to_dict()\n    return( zip_count, city_count, medyear, meanarea, medlat, medlong )\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"10a33853-c574-4346-ad6a-bb7b9ca80e7e","_uuid":"cb614dcab64ab0cec6b11a2c72c7fc7abe56786c","collapsed":true},"execution_count":null,"source":"def munge(properties):\n    for c in properties.columns:\n        properties[c]=properties[c].fillna(-1)\n        if properties[c].dtype == 'object':\n            lbl = LabelEncoder()\n            lbl.fit(list(properties[c].values))\n            properties[c] = lbl.transform(list(properties[c].values))\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"180e44c4-2cff-41a0-a118-181b4a71b23e","_uuid":"9557f96e6cd253ea1ad8e7e631eef334355c8cc2","collapsed":true},"execution_count":null,"source":"def calculate_target_aggreagates(df):\n    # Standard deviation of target value for properties in the city/zip/neighborhood\n    citystd = df.groupby('regionidcity')['logerror'].aggregate(\"std\").to_dict()\n    zipstd = df.groupby('regionidzip')['logerror'].aggregate(\"std\").to_dict()\n    hoodstd = df.groupby('regionidneighborhood')['logerror'].aggregate(\"std\").to_dict()\n    return( citystd, zipstd, hoodstd )","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"058d3299-005e-4458-a8cd-cd3c4d854190","_uuid":"4b9916ad9eb1d2a0b1aab78e71191c34a78a9b77","collapsed":true},"execution_count":null,"source":"def calculate_features(df):\n    # Nikunj's features\n    # Number of properties in the zip\n    df['N-zip_count'] = df['regionidzip'].map(zip_count)\n    # Number of properties in the city\n    df['N-city_count'] = df['regionidcity'].map(city_count)\n    # Does property have a garage, pool or hot tub and AC?\n    df['N-GarPoolAC'] = ((df['garagecarcnt']>0) & \\\n                         (df['pooltypeid10']>0) & \\\n                         (df['airconditioningtypeid']!=5))*1 \n\n    # More features\n    # Mean square feet of neighborhood properties\n    df['mean_area'] = df['regionidneighborhood'].map(meanarea)\n    # Median year of construction of neighborhood properties\n    df['med_year'] = df['regionidneighborhood'].map(medyear)\n    # Neighborhood latitude and longitude\n    df['med_lat'] = df['regionidneighborhood'].map(medlat)\n    df['med_long'] = df['regionidneighborhood'].map(medlong)\n\n    df['zip_std'] = df['regionidzip'].map(zipstd)\n    df['city_std'] = df['regionidcity'].map(citystd)\n    df['hood_std'] = df['regionidneighborhood'].map(hoodstd)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e389a360-b1dc-4257-b3c3-fea69026863d","_uuid":"d816890fe0edcab13c415144e76ecd12938ab17e","collapsed":true},"execution_count":null,"source":"dropvars = ['parcelid', 'airconditioningtypeid', 'buildingclasstypeid',\n            'buildingqualitytypeid', 'regionidcity']\ndroptrain = ['logerror', 'transactiondate']","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9a9fccf6-f446-4480-9e42-470559c3162f","_uuid":"ecbb50d5b99e6db35232e2757f7982da3dfb3d46","collapsed":true},"execution_count":null,"source":"xgb_params = {  # best as of 2017-09-28 13:20 UTC\n    'eta': LEARNING_RATE,\n    'max_depth': 7, \n    'subsample': 0.6,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 5.0,\n    'alpha': 0.65,\n    'colsample_bytree': 0.5,\n    'silent': 1\n}","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"de8e97e6-d681-465b-a68a-6178e7593e49","scrolled":true,"_uuid":"fe6761fd44aaf7a5c063897c9ca3b90b957b7108","collapsed":true},"execution_count":null,"source":"num_boost_rounds = round( ROUNDS_PER_ETA / xgb_params['eta'] )\nearly_stopping_rounds = round( num_boost_rounds / 20 )\nprint('Boosting rounds: {}'.format(num_boost_rounds))\nprint('Early stoping rounds: {}'.format(early_stopping_rounds))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a6cc2bdd-9d38-4f66-8903-3ce019f7109a","_execution_state":"idle","_uuid":"43ea1cf09b04ca53bb99c95d428b4624572ba75a","collapsed":true},"execution_count":null,"source":"properties = pd.read_csv('../input/properties_2016.csv')\naggs = calculate_aggregates(properties)\nzip_count, city_count, medyear, meanarea, medlat, medlong = aggs\nmunge(properties)\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\ntrain_df = train.merge(properties, how='left', on='parcelid')\ndel train\ngc.collect()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"af6766ec-9ad8-463f-bf2f-fd611e86114d","_uuid":"3a8d09d2e92d506d08039a9c98d5719ee40b3013","collapsed":true},"execution_count":null,"source":"for m in range(1,12+1):\n    \n    print( \"\\n\\nFUDGE FACTOR ANALYSIS FOR 2016 MONTH \", m)\n    select_mon = pd.to_datetime(train_df[\"transactiondate\"]).dt.month==m\n\n    select_data = select_mon & (pd.to_datetime(train_df[\"transactiondate\"]) >= VAL_SPLIT_DATE)\n    target_aggs = calculate_target_aggreagates(train_df[~select_data])\n    citystd, zipstd, hoodstd = target_aggs\n\n    train1 = train_df.copy()\n    calculate_features(train1)\n\n    x_valid = train1.drop(dropvars+droptrain, axis=1)[select_mon]\n    y_valid = train1[\"logerror\"].values.astype(np.float32)[select_mon]\n    n_valid = x_valid.shape[0]\n\n    train1=train1[ train1.logerror > -0.4 ]\n    train1=train1[ train1.logerror < 0.419 ]\n\n    train1=train1[~select_mon]\n\n    # Use only training data comparable to what is available for 2017\n    select_qtr4 = pd.to_datetime(train1[\"transactiondate\"]) >= VAL_SPLIT_DATE\n    train1=train1[~select_qtr4]\n    \n    x_train=train1.drop(dropvars+droptrain, axis=1)\n    y_train = train1[\"logerror\"].values.astype(np.float32)\n    y_mean = np.mean(y_train)\n    xgb_params['base_score'] = y_mean\n    \n    n_train = x_train.shape[0]\n\n    print('Fitting model to {} points & using {} to fit fudge factor'.format(n_train, n_valid))\n\n    dtrain = xgb.DMatrix(x_train, y_train)\n    dvalid_x = xgb.DMatrix(x_valid)\n    dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n\n    evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n    model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n                      evals=evals, early_stopping_rounds=early_stopping_rounds,\n                      verbose_eval=False)\n\n    valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n    fudge = QuantReg(y_valid, valid_pred).fit(q=.5).params[0]\n    rawerr = mean_absolute_error(y_valid, valid_pred)\n    fudgerr = mean_absolute_error(y_valid, fudge*valid_pred)\n    print(\"Fudge factor reduces MAE from {0:9.6f} to {1:9.6f}\".format(rawerr, fudgerr))\n    print(\"Optimized fudge factor for month {0}: {1:9.6f}\".format(m, fudge))\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a6cc2bdd-9d38-4f66-8903-3ce019f7109a","_execution_state":"idle","_uuid":"43ea1cf09b04ca53bb99c95d428b4624572ba75a","collapsed":true},"execution_count":null,"source":"properties = pd.read_csv('../input/properties_2017.csv')\naggs = calculate_aggregates(properties)\nzip_count, city_count, medyear, meanarea, medlat, medlong = aggs\nmunge(properties)\ntrain = pd.read_csv(\"../input/train_2017.csv\")\ntrain_df = train.merge(properties, how='left', on='parcelid')\ndel train\ngc.collect()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e7d9595d-13e9-46a8-a22e-51e96e0eaf91","_uuid":"3a95f73a66e5bfdbed8d3e4fb7658a05f85d8fb5","collapsed":true},"execution_count":null,"source":"for m in range(1,9+1):\n    \n    print( \"\\n\\nFUDGE FACTOR ANALYSIS FOR 2017 MONTH \", m)\n    select_mon = pd.to_datetime(train_df[\"transactiondate\"]).dt.month==m\n\n    target_aggs = calculate_target_aggreagates(train_df[~select_mon])\n    citystd, zipstd, hoodstd = target_aggs\n\n    train1 = train_df.copy()\n    calculate_features(train1)\n\n    x_valid = train1.drop(dropvars+droptrain, axis=1)[select_mon]\n    y_valid = train1[\"logerror\"].values.astype(np.float32)[select_mon]\n    n_valid = x_valid.shape[0]\n\n    train1=train1[ train1.logerror > -0.4 ]\n    train1=train1[ train1.logerror < 0.419 ]\n\n    train1=train1[~select_mon]\n    \n    x_train=train1.drop(dropvars+droptrain, axis=1)\n    y_train = train1[\"logerror\"].values.astype(np.float32)\n    y_mean = np.mean(y_train)\n    xgb_params['base_score'] = y_mean\n    \n    n_train = x_train.shape[0]\n\n    print('Fitting model to {} points & using {} to fit fudge factor'.format(n_train, n_valid))\n\n    dtrain = xgb.DMatrix(x_train, y_train)\n    dvalid_x = xgb.DMatrix(x_valid)\n    dvalid_xy = xgb.DMatrix(x_valid, y_valid)\n\n    evals = [(dtrain,'train'),(dvalid_xy,'eval')]\n    model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,\n                      evals=evals, early_stopping_rounds=early_stopping_rounds,\n                      verbose_eval=False)\n\n    valid_pred = model.predict(dvalid_x, ntree_limit=model.best_ntree_limit)\n    fudge = QuantReg(y_valid, valid_pred).fit(q=.5).params[0]\n    rawerr = mean_absolute_error(y_valid, valid_pred)\n    fudgerr = mean_absolute_error(y_valid, fudge*valid_pred)\n    print(\"Fudge factor reduces MAE from {0:9.6f} to {1:9.6f}\".format(rawerr, fudgerr))\n    print(\"Optimized fudge factor for month {0}: {1:9.6f}\".format(m, fudge))\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a9d5754f-03b2-4fb6-9f50-b6e906cd65d3","_uuid":"c6e555efa782a348ba44d9bc35b0ddb19809c0fc","collapsed":true},"execution_count":null,"source":"","cell_type":"code","outputs":[]}],"nbformat":4,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","file_extension":".py","name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1}