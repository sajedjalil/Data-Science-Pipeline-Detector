{"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_is_fork":false,"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.4","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python"},"_change_revision":0},"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"The following notebook introduces ML-Ensemble, a Python library for memory-efficient parallel ensemble learning with a Scikit-learn API. \n\nML-Ensemble also deploys a neural network-like API for building ensembles of several layers, and can accomodate a great variety of ensemble architectures. \n\nFor more information, see [ml-ensemble.com](http://ml-ensemble.com) or visit the [github](https://github.com/flennerhag/mlens) repository.","metadata":{"_uuid":"0711e8229ade0ae8332d9305f5958f75d82d32b5","_cell_guid":"c0c2520a-71c8-c705-b362-7844e4b25b79"}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# Inputs\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Data viz\nfrom mlens.visualization import corr_X_y, corrmat\n\n# Model evaluation\nfrom mlens.metrics import make_scorer\nfrom mlens.model_selection import Evaluator\n\n# Ensemble\nfrom mlens.ensemble import SuperLearner\n\nfrom scipy.stats import uniform, randint\n\nfrom matplotlib.pyplot import show\n%matplotlib inline","execution_count":null,"metadata":{"_uuid":"6648337953f0f09329acf2036bcf8f1875fc6fa6","_cell_guid":"1fed8d6f-4c44-b4c7-363b-2ff9e6a594c3"},"outputs":[]},{"cell_type":"code","source":"SEED = 148\nnp.random.seed(SEED)","execution_count":null,"metadata":{"_uuid":"9a8ca77596c1ac87edc5f3db24a4300009170330","_cell_guid":"0a509d0f-ab3f-5e19-97ad-c88ed75bd393","collapsed":true},"outputs":[]},{"cell_type":"markdown","source":"# 1. Getting a good baseline for ensemble learning\n\nIt's always good to check how inputs play along with the output.\nHere, we highlight one example functionality of the Ml-Ensemble's\nvisualization library.","metadata":{"_uuid":"c002f7e728821c091285df6cd316db9145e390e6","_cell_guid":"082bc8ab-5880-8b96-f57c-1429432be317"}},{"cell_type":"code","source":"def build_train():\n    \"\"\"Read in training data and return input, output, columns tuple.\"\"\"\n\n    # This is a version of Anovas minimally prepared dataset\n    # for the xgbstarter script\n    # https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n\n    df = pd.read_csv('../input/train_2016_v2.csv')\n\n    prop = pd.read_csv('../input/properties_2016.csv')\n    convert = prop.dtypes == 'float64'\n    prop.loc[:, convert] = \\\n        prop.loc[:, convert].apply(lambda x: x.astype(np.float32))\n\n    df = df.merge(prop, how='left', on='parcelid')\n\n    y = df.logerror\n    df = df.drop(['parcelid',\n                  'logerror',\n                  'transactiondate',\n                  'propertyzoningdesc',\n                  'taxdelinquencyflag',\n                  'propertycountylandusecode'], axis=1)\n\n    convert = df.dtypes == 'object'\n    df.loc[:, convert] = \\\n        df.loc[:, convert].apply(lambda x: 1 * (x == True))\n\n    df.fillna(0, inplace=True)\n\n    return df, y, df.columns","execution_count":null,"metadata":{"_uuid":"47e86754ea1fe98665d213367c5a63e445b8da00","_cell_guid":"96d981c5-a6a1-1e62-9bad-9aac5197f769","collapsed":true},"outputs":[]},{"cell_type":"code","source":"xtrain, ytrain, columns = build_train()\nxtrain, xtest, ytrain, ytest = train_test_split(\n    xtrain, ytrain, test_size=0.5, random_state=SEED)","execution_count":null,"metadata":{"_uuid":"f9abed76330ce6cfbaca1a608ba0910374ad18c5","_cell_guid":"9d067d92-4703-d61b-5432-e9e58def803e"},"outputs":[]},{"cell_type":"code","source":"corr_X_y(xtrain, ytrain, figsize=(16, 10), label_rotation=80, hspace=1, fontsize=14)","execution_count":null,"metadata":{"_uuid":"4e20207488c1c2985970342e0afc63ea59aef021","_cell_guid":"b0e7fe89-12b7-eab5-d270-b2128313afa9"},"outputs":[]},{"cell_type":"markdown","source":"A few features seems to be (first-order) uncorrelated with the output, suggesting estimators with inherent\nfeature selection should be preferred.","metadata":{"_uuid":"4866876e67c519a709bc01ae964090c15f6a217e","_cell_guid":"91c81883-5c34-2ca4-afc1-bf95534bae26"}},{"cell_type":"markdown","source":"Now, consider how set of base learners (estimators) perform as they are.","metadata":{"_uuid":"c945fa51ccc5a542005e4d86faf88c795593b4e4","_cell_guid":"278631f6-4a65-dda2-cbf1-82965ea6b893"}},{"cell_type":"code","source":"# We consider the following models (or base learners)\ngb = XGBRegressor(n_jobs=1, random_state=SEED)\nls = Lasso(alpha=1e-6, normalize=True)\nel = ElasticNet(alpha=1e-6, normalize=True)\nrf = RandomForestRegressor(random_state=SEED)\n\nbase_learners = [\n    ('ls', ls), ('el', el), ('rf', rf), ('gb', gb)\n]","execution_count":null,"metadata":{"_uuid":"4766f173ae56fb0a76ee3e5f973a6e99a5d4aebd","_cell_guid":"299d7916-8391-5a21-db3b-ac5014eb925e","collapsed":true},"outputs":[]},{"cell_type":"code","source":"P = np.zeros((xtest.shape[0], len(base_learners)))\nP = pd.DataFrame(P, columns=[e for e, _ in base_learners])\n\nfor est_name, est in base_learners:\n    est.fit(xtrain, ytrain)\n    p = est.predict(xtest)\n    P.loc[:, est_name] = p\n    print(\"%3s : %.4f\" % (est_name, mean_absolute_error(ytest, p)))","execution_count":null,"metadata":{"_uuid":"33d6e7e936b3e8f65c3fd4175056ea0c28664fd0","_cell_guid":"5d46059c-2c3c-b383-dd3f-0b129c4cf1db"},"outputs":[]},{"cell_type":"markdown","source":"So they all score relatively close. However, they seem to capture different aspects of the feature space, as shown by the low correlation of their predictions:","metadata":{"_uuid":"87661df1d856fe08a4d1fb9234307f30b99535bb","_cell_guid":"e180babd-e852-60ec-319d-c704606b4f71"}},{"cell_type":"code","source":"ax = corrmat(P.corr())\nshow()","execution_count":null,"metadata":{"_uuid":"bec7d505a04f83e5b3da5fb37373b057d3a4e339","_cell_guid":"9ecb9ad7-5d71-0d26-3f6c-5b49ebb15cc6"},"outputs":[]},{"cell_type":"markdown","source":"They are in fact not particularly correlated in their scoring (except the linear models), and hence\nan ensemble may be able to outperform any single model by learning to combine their respective strength.","metadata":{"_uuid":"2f260b72c39f3192886f16071b235dd9dbd55d76","_cell_guid":"75fc44d9-540a-d669-4f0f-2307fb1387d6"}},{"cell_type":"markdown","source":"## 2. Comparing base learners\n\n*emphasized text*To facilitate base learner comparison, ML-Ensemble implements a randomized grid search\nclass that allows specification of several estimators (and preprocessing pipelines) in\none grid search.","metadata":{"_uuid":"adb08a29e29267201942ece4b188dfd5b71c26c3","_cell_guid":"a1b054d0-9385-26ad-be38-4f22c89e113a"}},{"cell_type":"code","source":"# Put their parameter dictionaries in a dictionary with the\n# estimator names as keys\nparam_dicts = {\n    'ls':\n    {'alpha': uniform(1e-6, 1e-5)},\n    'el':\n    {'alpha': uniform(1e-6, 1e-5),\n     'l1_ratio': uniform(0, 1)\n    },\n    'gb':\n    {'learning_rate': uniform(0.02, 0.04),\n     'colsample_bytree': uniform(0.55, 0.66),\n     'min_child_weight': randint(30, 60),\n     'max_depth': randint(3, 7),\n     'subsample': uniform(0.4, 0.2),\n     'n_estimators': randint(150, 200),\n     'colsample_bytree': uniform(0.6, 0.4),\n     'reg_lambda': uniform(1, 2),\n     'reg_alpha': uniform(1, 2),\n    },\n    'rf':\n    {'max_depth': randint(2, 5),\n     'min_samples_split': randint(5, 20),\n     'min_samples_leaf': randint(10, 20),\n     'n_estimators': randint(50, 100),\n     'max_features': uniform(0.6, 0.3)\n    }\n}","execution_count":null,"metadata":{"_uuid":"e6effb52441cc66701c26045a3cc48b7c3a26e60","_cell_guid":"3f4fbe54-a250-e0fd-fcde-821228da7c18","collapsed":true},"outputs":[]},{"cell_type":"code","source":"scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n\nevl = Evaluator(\n    scorer,\n    cv=2,\n    random_state=SEED,\n    verbose=5,\n)","execution_count":null,"metadata":{"_uuid":"f40f0ba8ba707e8064b6dc0a0f766cf921fbd92c","_cell_guid":"3fd0c5c9-c9fe-2465-6341-9f54d899346f","collapsed":true},"outputs":[]},{"cell_type":"code","source":"evl.fit(\n    xtrain, ytrain,\n    estimators=base_learners,\n    param_dicts=param_dicts,\n    preprocessing={'sc': [StandardScaler()], 'none': []},\n    n_iter=2  # bump this up to do a larger grid search\n)","execution_count":null,"metadata":{"_uuid":"526deea1d7f9f2bf83a82b750b50abcb53801d93","_cell_guid":"7dfd3238-b31d-6577-0613-0a71abaf95da"},"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(evl.results)","execution_count":null,"metadata":{"_uuid":"ec89cd5aedc99437e4388ea06c79384e9aa1c65c","_cell_guid":"79510c7f-0465-e19f-1c70-6cc3d846f3c2"},"outputs":[]},{"cell_type":"markdown","source":"There you have it, a comparison of tuned models in one grid search!\n\nOptimal parameters are then easily accessed.","metadata":{"_uuid":"f26e58260b2fcd5648f5724413754f5b9c273f1d","_cell_guid":"b1074dc1-70ea-4fee-5746-9d22fab75ffd"}},{"cell_type":"code","source":"evl.results[\"params\"]['sc.gb']","execution_count":null,"metadata":{"_uuid":"930e9e92362d8598ae607da502d5872e805e742a","_cell_guid":"a56f93f3-7f61-e402-dd4e-084e7e542f4d"},"outputs":[]},{"cell_type":"markdown","source":"# 3. Comparing meta learners\n\nRunning an entire ensemble several times just to compare different meta learners can be prohibitvely expensive. ML-Ensemble implements a class that acts as a transformer, allowing you to use ingoing layers as a \"preprocessing\" step, so that you need only evaluate the meta learners iteratively.","metadata":{"_uuid":"8a10ba8da3b7348213b4cb0eebcc1278c6c6313e","_cell_guid":"2b8820aa-85fe-7d5f-9697-9a7ac528007c"}},{"cell_type":"code","source":"for case_name, params in evl.results[\"params\"].items():\n    case, case_est = case_name.split('.')\n    for est_name, est in base_learners:\n        if est_name == case_est:\n            est.set_params(**params)","execution_count":null,"metadata":{"_uuid":"97f39f5f3f0205e7c0c77176e86167fe20348af3","_cell_guid":"38156447-69dd-9bc5-2f68-e9f1f59aebc0"},"outputs":[]},{"cell_type":"code","source":"# We will compare a GBM and an elastic net as the meta learner\n# These are cloned internally so we can go ahead and grab the fitted ones\nmeta_learners = [\n    ('gb', gb), ('el', el)\n]\n\n# Note that when we have a preprocessing pipeline,\n# keys are in the (prep_name, est_name) format\nparam_dicts = {\n    'el':\n    {'alpha': uniform(1e-5, 1),\n     'l1_ratio': uniform(0, 1)\n    },\n    'gb':\n    {'learning_rate': uniform(0.01, 0.2),\n     'subsample': uniform(0.5, 0.5),\n     'reg_lambda': uniform(0.1, 1),\n     'n_estimators': randint(10, 100)\n    },\n}","execution_count":null,"metadata":{"_uuid":"13535ea3d7428624b20bef1e0b377de77fcbc062","_cell_guid":"7b29ad65-eee3-b75c-7865-10bfbda05b54","collapsed":true},"outputs":[]},{"cell_type":"code","source":"# Put the layers you don't want to tune into an ensemble with model selection turned on\n# Just remember to turn it off when you're done!\nin_layer = SuperLearner(model_selection=True)\nin_layer.add(base_learners)\n\npreprocess = [in_layer]","execution_count":null,"metadata":{"_uuid":"96fe7196da7423dd060283e448452d7a3f196512","_cell_guid":"d1230a30-cc93-ffc7-a0e5-2afeacee6577"},"outputs":[]},{"cell_type":"code","source":"evl.fit(\n    xtrain, ytrain,\n    meta_learners,\n    param_dicts,\n    preprocessing={'meta': preprocess},\n    n_iter=4                            # bump this up to do a larger grid search\n)","execution_count":null,"metadata":{"_uuid":"c42ab7099d8325d4c90815545660dcc981397962","_cell_guid":"fdaef90c-8b4e-ec0a-49b1-221ddc026300"},"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(evl.results)","execution_count":null,"metadata":{"_uuid":"4cbb073a96269e001dd215e7b7b991b96b72746f","_cell_guid":"42e4b794-8ecc-4419-abb7-a682c0cf551b"},"outputs":[]},{"cell_type":"markdown","source":"# 4. Ensemble learning\n\nWith these results in mind, we now turn to building an ensemble estimator.\n\nML-Ensemble uses a neural network-like API to specify layers of base learners to be\nfitted sequentially on the previous layer's predictions (or the raw input for the\nfirst layer). An ensemble is built as a Scikit-learn estimator, and can be used as\nany other Scikit-learn class.","metadata":{"_uuid":"794b8592abb7b5fc35104c164f696617db517ad8","_cell_guid":"64e69cf0-aee4-c65a-f9ce-d58b370066c9"}},{"cell_type":"code","source":"# Let's pick the linear meta learner with the above tuned\n# hyper-parameters. Note that ideally, you'd want to tune\n# the ensemble as a whole, not each estimator at a time\nmeta_learner = meta_learners[1][1]\nmeta_learner.set_params(**evl.results[\"params\"][\"meta.el\"])\n\n# We can grab the preprocessing layer and turn model selection off\nens = in_layer\nens.model_selection = False\nens.add_meta(meta_learner)","execution_count":null,"metadata":{"_uuid":"d4c76c85f97ba6d5758df8068bf2eddac8c75db5","_cell_guid":"d7440c1d-f3a5-a001-b4f1-82c20f4229cd"},"outputs":[]},{"cell_type":"markdown","source":"The ensemble we will implement is the Super Learner, also known as a stacking ensemble. There are several alternatives, see the documentation for further info.","metadata":{"_uuid":"6c8b9d6d2a12f5c4700893698fef88b5325bf24d","_cell_guid":"1ff3192d-5cd4-b8fa-8b4f-45b6d2bf4cbe"}},{"cell_type":"markdown","source":"Once instantiated, the ensemble will behave like any other Scikit-learn estimator.","metadata":{"_uuid":"bb61ee8f6f799100bc152afa434975a9155b2f14","_cell_guid":"43b2ba6d-cf90-3de4-be58-a078fb01a1ee"}},{"cell_type":"code","source":"ens.fit(xtrain, ytrain)","execution_count":null,"metadata":{"_uuid":"7c8210cc642b8842b588c349ce0a69aca8a65ab1","_cell_guid":"f0f797a6-d467-5d5f-23a4-badc954e85ba"},"outputs":[]},{"cell_type":"markdown","source":"Predictions are generated as usual:","metadata":{"_uuid":"aec31fb264f325cd0c51b117358365c6c50a9b78","_cell_guid":"032e87cd-d503-03dd-e5e8-bb466d2a3fa4"}},{"cell_type":"code","source":"pred = ens.predict(xtest)","execution_count":null,"metadata":{"_uuid":"5b94791d73be17f279ac50231d257dc1e539fd59","_cell_guid":"3c64a85a-816c-3954-d5e2-c3b3b818bbc9"},"outputs":[]},{"cell_type":"code","source":"print(\"ensemble score: %.4f\" % mean_absolute_error(ytest, pred))","execution_count":null,"metadata":{"_uuid":"066f1a995df166cc6b44dec3606a5a8f0f96f0e5","_cell_guid":"2513f179-ea59-653e-7b4b-84084e64eaf8"},"outputs":[]},{"cell_type":"markdown","source":"And that's it for this tutorial!\n\nYou might have noticed that the ensemble did not achieve an increase in performance. This is partly due to the lack of proper hyper parameter tuning, but more importantly because the base learners are not sufficiently accurate for there to be anything meaningful for the meta learner to learn from (note that predicting the average gets you about 0.07) \n\nIn these cases, unless the meta learner is underfitting, the ensemble will at least be on par with the best base learner.  Good features are always the primary source of predictive power. Once you have them, combining different estimators in an ensemble is a powerful way of learning as much of the signal in the data as possible.\n\nIf you decide to give ML-Ensemble a try, note that the library is in beta testing so you may run into some unexpected behavior or see opportunities for improvements. Feel free to contribute to the project via the [github](https://github.com/flennerhag/mlens) repository! ","metadata":{"_uuid":"3c7724e9fe3562b4963b895f0b9c5a5960959d05","_cell_guid":"3ee96c61-675a-5428-b4b3-19991a8780be"}}]}