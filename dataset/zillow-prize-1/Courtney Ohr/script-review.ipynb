{"nbformat_minor":1,"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"879762d86b27fa2b6fc0fed43fe35e5760fabf01","_cell_guid":"c4f516db-88b9-40f5-9ab4-a58add0f6b32"},"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\nimport gc\n\n#Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy import arange\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\n#Explore training data\ntrain_start= pd.read_csv('../input/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\ntrain_start.head()\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"31fff4e669f284acbdecbec560379c8260c97c0a","_cell_guid":"3df2cd10-7464-4646-aae6-04fed5413034"},"source":"#Explore training data\ntrain_start= pd.read_csv('../input/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\ntrain_start.head()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4e3e8549443019043de2918fc6c1c06928f38a03","_cell_guid":"b14323c5-d6f9-422b-89e1-34a7e1b2ffd6"},"source":"#Explore LogError to check for outliers because improving residual error is a component of the competition\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_start.shape[0]), np.sort(train_start.logerror.values))\nplt.xlabel('index', fontsize=14)\nplt.ylabel('logerror', fontsize=14)\nplt.show()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0c0214c7ec2d3f40d34b5da275d9a5476037e31b","_cell_guid":"36ea34a3-abee-422f-957b-f4aa503230b9"},"source":"#Remove outliers\n#upper_per = np.percentile(train_start.logerror.values, 99)\n#lower_per = np.percentile(train_start.logerror.values, 1)\n#train_start['logerror'].ix[train_start['logerror']>upper_per] = upper_per\n#train_start['logerror'].ix[train_start['logerror']<lower_per] = lower_per\n\n#Histogram of resulting data\n#plt.figure(figsize=(12,8))\n#sns.distplot(train_start.logerror.values, bins=50, kde=False)\n#plt.xlabel('logerror', fontsize=14)\n#plt.show()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b3faae365e030505a5f4845484e6cfdbc566eb8f","_cell_guid":"bedca074-36f4-4c5d-a293-9b8ede2b34b0"},"source":"#Explore the data sales dates i.e. number of transactions in a month\ntrain_start['transaction_month']=train_start['transactiondate'].dt.month\ncounts=train_start['transaction_month'].value_counts()\nplt.figure(figsize=(14,8))\nsns.barplot(counts.index, counts.values, alpha=0.8)\nplt.xticks(rotation='vertical')\nplt.xlabel('Transactions (month)', fontsize=14)\nplt.ylabel('Occurences',fontsize=14)\nplt.show()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"634ebd3643d7e0e6b43bdf246d8c6d549fceaf09","_cell_guid":"66b8b49b-b716-481c-a37c-c9525e0769a5"},"source":"#Datasets\nprop_start = pd.read_csv('../input/properties_2016.csv')\ntest2 = pd.read_csv('../input/sample_submission.csv')\n#Rename test data field ParcelID to match training data\ntest = test2.rename(columns={'ParcelId':'parcelid'})\n#print (test.dtypes)\n#print (train.dtypes)\n#print (props.dtypes)\n\n#Explore Propery data fields\nprop_start.head()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"696563e7bad0502f06c09fcd36a92c0a087ba567","_cell_guid":"c2bffb03-dfef-43f3-98d4-77032a8424af"},"source":"##Change the data types to improve memory usage\nfor column in prop_start.columns:\n    if prop_start[column].dtype==int:\n        prop_start[column]=prop_start[column].astype(np.int32)\n    if prop_start[column].dtype==float:\n        prop_start[column]=prop_start[column].astype(np.float32)\n        \nfor column in test.columns:\n       if test[column].dtype==int:\n           test[column]=test[column].astype(np.int32)\n       if test[column].dtype==float:\n           test[column]=test[column].astype(np.float32)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"dede59823aa611fcf9d5a47055c4881fd36ad478","_cell_guid":"13a2ff9d-493f-4547-bd47-0f6221372dba"},"source":"#Visualize the missing data NaN\nmissing_dv=prop_start.isnull().sum(axis=0).reset_index()\nmissing_dv.columns=['column_name', 'missing']\nmissing_dv = missing_dv.ix[missing_dv['missing']>0]\nmissing_dv = missing_dv.sort_values(by='missing')\nwth=0.9\nindex=np.arange(missing_dv.shape[0])\nfig, ax = plt.subplots(figsize=(14,20))\nrects = ax. barh(index, missing_dv.missing.values, color='blue')\nax.set_yticks(index)\nax.set_yticklabels(missing_dv.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Missing Values (count)\")\nplt.show()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b06ba3ae00ad9dca86e245abd2b121f22353f10c","_cell_guid":"dde64b77-bd6d-42b7-97a3-f49a6e6aa05f"},"source":"#Explore spatial data files\nplt.figure(figsize=(14,14))\nsns.jointplot(x=prop_start.latitude.values, y=prop_start.longitude.values, size=12)\nplt.ylabel('Longitude',fontsize=8)\nplt.xlabel('Latitude', fontsize=8)\nplt.show()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e29527ba2826b48a892786455505c29103702860","_cell_guid":"bfed691e-2a3c-416e-b7aa-4a0367ff2eca"},"source":"###Calculate some properties for estimating home value\n\n#Living area of properties\nprop_start['living_area'] = prop_start['calculatedfinishedsquarefeet']/prop_start['lotsizesquarefeet']\nprop_start['EstRoomSize']= prop_start['calculatedfinishedsquarefeet']/prop_start['roomcnt']\n\n#Ratio of tax per worth of property\nprop_start['tax_ratio']=prop_start['taxvaluedollarcnt']/prop_start['taxamount']\n\n#Proportion of structure value to land\nprop_start['tax_proportion']=prop_start['structuretaxvaluedollarcnt']/prop_start['landtaxvaluedollarcnt']\n\n#Time of unpaid taxes\nprop_start['DeliqCount']= 2018- prop_start['taxdelinquencyyear']\n\n#Simplify Land Uses from 25 to 4 categories: Mixed, Home, Other, Not Built\nprop_start['LandUse'] = prop_start.propertylandusetypeid.replace({31 : \"Mixed\", 46 : \"Other\", \n                                                                  47 : \"Mixed\", 246 : \"Mixed\", \n                                                                  247 : \"Mixed\", 248 : \"Mixed\", \n                                                                  260 : \"Home\", 261 : \"Home\", 262 : \"Home\", \n                                                                  263 : \"Home\", 264 : \"Home\", 265 : \"Home\", \n                                                                  266 : \"Home\", 267 : \"Home\", 268 : \"Home\", \n                                                                  269 : \"Not Built\", 270 : \"Home\", 271 : \"Home\", \n                                                                  273 : \"Home\", 274 : \"Other\", 275 : \"Home\", \n                                                                  276 : \"Home\", 279 : \"Home\", 290 : \"Not Built\", \n                                                                  291 : \"Not Built\" })\nprint('Done')","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"60bc8c7bf5a23b8fad3a2a61c6bdbac1c974b5e0","_cell_guid":"f4f15a36-7462-4f4b-a862-117d9364238b"},"source":"#Add propertiy data to training and testing datasets\nm_train= train_start.merge(prop_start, how='left', on='parcelid')\nm_test= test.merge(prop_start, how='left', on='parcelid')\n\n#Look at heatmap to see if variables are correlated\nvar= ['airconditioningtypeid','architecturalstyletypeid','buildingqualitytypeid',\n            'buildingclasstypeid','decktypeid','fips','hashottuborspa','heatingorsystemtypeid','living_area',\n            'pooltypeid10','pooltypeid2','pooltypeid7','propertycountylandusecode',\n            'propertylandusetypeid','propertyzoningdesc','rawcensustractandblock','regionidcity',\n            'regionidcounty','regionidneighborhood','regionidzip','storytypeid','tax_ratio', 'tax_proportion',\n            'typeconstructiontypeid','yearbuilt','taxdelinquencyflag']\ncall=[i for i in m_train.columns if i not in var]\nplt.figure(figsize=(14,14))\ncmap=sns.diverging_palette(220,20, sep=20, as_cmap=True)\nsns.heatmap(data=m_train[call].corr(),cmap=cmap)\nplt.show()\nplt.gcf().clear()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0bacc774d1074781b19178c9fd84aa914a630689","_cell_guid":"c5166933-b99d-4eb8-a270-287e3c8992d4"},"source":"###More data clean-up, identify and fill-in missing values with label encoder\n#Label Encoding: Encode labels with value between 0 and n_classes-1.\nfrom sklearn.preprocessing import LabelEncoder\nLE=LabelEncoder()\nfor l in m_train.columns:\n    m_train[l]=m_train[l].fillna(0)\n    if m_train[l].dtype=='object':\n        LE.fit(list(m_train[l].values)) #normalize labels\n        m_train[l]=LE.transform(list(m_train[l].values))  #transform non-numerical labels to numerical\n        \nfor l in m_test.columns:\n    m_test[l]=m_test[l].fillna(0)\n    if m_test[l].dtype=='object':\n        LE.fit(list(m_test[l].values)) #normalize labels\n        m_test[l]=LE.transform(list(m_test[l].values)) #transform non-numerical labels to numerical\n        \n#Drop properties we don't need anymore for training: parcelid, log error, sell date, propert zoning description and land use code\nsec_train=m_train.drop(['transaction_month','parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n#Drop values we don't need for validation\nsec_test= m_test.drop(['parcelid','propertyzoningdesc','propertycountylandusecode','201610','201611','201612', '201710', '201711', '201712'], axis=1)\nprint('Done')","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3056a5f068b35e9b068c3a80c5ab175d5bddd8a4","_cell_guid":"9803f543-e4d9-4d27-86de-236d346753a6","scrolled":true},"source":"#Use XGBoost to look more into the variable importance\nX=sec_train.values\nY=m_train['logerror'].values\nxgb_params = {'eta': 0.05,'max_depth': 8,'subsample': 0.7,'colsample_bytree': 0.7, 'objective': 'reg:linear',\n    'silent': 1,'seed' : 0}\ndtrain = xgb.DMatrix(sec_train, Y, feature_names=sec_train.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=150)\n# plot the important features #\nfig, ax = plt.subplots(figsize=(18,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f4f7a58dc43d58f3af546330356942bd4f9de0a4","_cell_guid":"4e59b160-35cb-4bd8-8d76-1966348d0d80"},"source":"train_columns = sec_train.columns\nd_train = lgb.Dataset(sec_train, label=Y)\nparams = {'max_bin': 10,'learning_rate': 0.0021,'boosting_type': 'gbdt','objective': 'regression',\n          'metric': 'l1','sub_feature': 0.345,'bagging_fraction': 0.85,'bagging_freq': 40,\n          'num_leaves':512, 'min_data': 500,'min_hessian': 0.05,'verbose': 0,'feature_fraction_seed':2,\n          'bagging_seed': 3}\n#Light Gradient Boosting Model\nprint('Fitting LightGBM model ...')\nclf = lgb.train(params, d_train, 200)\nprint ('completed')\n","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d97d7d74e278a567bc7074283521732e6692888e","_cell_guid":"6ec46cbe-0563-4332-bf76-eb0cbaf04a4a"},"source":"Xtest = sec_test[train_columns]\np_test = clf.predict(Xtest)\n#print('1')\nYmean = np.mean(Y)\n#Extreme Gradient Boosting Model 1\nprint ('prepping paramaters for Xboost')\nparamsXGB = {'eta': 0.037,'max_depth': 5,'subsample': 0.80,'objective': 'reg:linear','eval_metric': 'mae',\n    'lambda': 0.8,'alpha': 0.4, 'base_score': Ymean,'silent': 1}\ndtrain = xgb.DMatrix(sec_train, Y)\ndtest = xgb.DMatrix(Xtest)\nnum_boost = 250\nprint( 'Training XGBoost')\nmodel = xgb.train(dict(paramsXGB, silent=1), dtrain, num_boost_round=num_boost)\nprint( 'Predicting with XGBoost')\nxgb_pred1 = model.predict(dtest)\nprint( 'First XGBoost predictions:')\nprint( pd.DataFrame(xgb_pred1).head())","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"536227c666da8d4a58037be1f320f3a2d68b098d","_cell_guid":"2794a12d-d1ce-4dde-95df-399b2086560c"},"source":"#Extreme Gradient Boosting Model 2\nparamsXGB2 = {'eta': 0.033,'max_depth': 6,'subsample': 0.80,'objective': 'reg:linear',\n    'eval_metric': 'mae','base_score': Ymean,'silent': 1}\nnum_boost2 = 150\nprint( 'Training XGBoost for second model ...')\nmodel = xgb.train(dict(paramsXGB2, silent=1), dtrain, num_boost_round=num_boost2)\n#print( 'Predicting with XGBoost second time')\nxgb_pred2 = model.predict(dtest)\nprint( 'Second XGBoost predictions:' )\nprint( pd.DataFrame(xgb_pred2).head() )","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3ac1c93dfecbc99a3f602747448265d540b47700","_cell_guid":"4170ddf3-2b39-43f9-bdc8-5bdf02ff9baa"},"source":"XGB1_WEIGHT = 0.8083 # Weight of first in combination of two XGB models\nxgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\nprint( 'Combined XGBoost predictions:' )\nprint( pd.DataFrame(xgb_pred).head() )","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d1398c165cb7f4748a704e5f79fb4f02e3863ea6","_cell_guid":"271854ee-8ea7-4ee2-8975-e61ef79d7760","scrolled":false},"source":"#Combine LGB and Xboost\nXGB_WEIGHT = 0.700 #based on https://www.kaggle.com/hsperr/finding-ensamble-weights and various Kernel results\nBASELINE_WEIGHT = 0.0056 #based on\nOLS_WEIGHT = 0.0620 # based on https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\nXGB1_WEIGHT = 0.8083 # Weight of first in combination of two XGB models\nBASELINE_PRED = 0.0115 # Baseline based on mean of training data\ngc.collect()\nnp.random.seed(17)\nrandom.seed(17)\ntrain_F = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nproperties2 = pd.read_csv(\"../input/properties_2016.csv\")\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\n#print(len(train_F),len(properties2),len(submission))\n##OLS based on https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\ndef get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n#Submission Calculation: logerror=log(Zestimate)âˆ’log(SalePrice)\ndef MAE(y, ypred):\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\ntrain_F = pd.merge(train_F, properties2, how='left', on='parcelid')\nY = train_F['logerror'].values\ntest = pd.merge(submission, properties2, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\nexc = [train_F.columns[c] for c in range(len(train_F.columns)) if train_F.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train_F.columns if c not in exc]\ntrain_F = get_features(train_F[col])\ntest['transactiondate'] = '2016-01-01' \ntest = get_features(test[col])\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train_F, Y); #print('fit...')\n#print(MAE(Y, reg.predict(train_F)))\ntrain_F = [];  Y = [] \ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']\nprint( 'Combining XGBoost, LightGBM, and baseline predicitons' )\nlgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\npred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\nprint( 'Combined XGB/LGB/baseline predictions:' )\nprint( pd.DataFrame(pred0).head() )\nprint( 'Predicting with OLS and combining with XGB/LGB/baseline predicitons:' )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\nprint( 'Final Prediction Log Errors (XGB/LGB/baseline):' )\nfrom datetime import datetime\nprint( 'Writing results to disk')\nsubmission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\nprint( 'Finished')\nLogError=submission['201610'].values\nAvg=np.mean(LogError)\nprint ('Avgerage Log Error: ')\nprint (Avg)\nsubmission","outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","name":"python","file_extension":".py","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4}