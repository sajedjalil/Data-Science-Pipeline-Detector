{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Zillow Prize: Zillow’s Home Value Prediction (Zestimate)\n\n**Zillow** is online real estate marketing company in United States. According to official page of Zillow, **Zestimate** is Zillow’s estimate of a home’s market value. The Zestimate incorporates public and user-submitted data, taking into account home facts, location and market conditions.\n<br>\n\nThis notebook is being written 3 years after competition, just for purpose of studying and sharing ideas! "},{"metadata":{},"cell_type":"markdown","source":"## Contents\n**Part 1**\n1. Load and preview dataset\n2. Data Cleansing\n3. EDA\n\n\n**Part 2**\n1. Feature Engineering\n2. Modeling"},{"metadata":{},"cell_type":"markdown","source":"## 1. Load and preview datasets\n\nNote) <br>\n`train_2016` data was updated - containing data from 2016-01-01 to 2016-12-31 so there's no need to predict log error in 2016. Instead, I'm going to predict log error of 2017 by using given datasets from 2016 and 2017."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_2016 = pd.read_csv(\"../input/zillow-prize-1/train_2016_v2.csv\", header = 0)\ntrain_2017 = pd.read_csv(\"../input/zillow-prize-1/train_2017.csv\", header = 0)\nproperties_2016 = pd.read_csv(\"../input/zillow-prize-1/properties_2016.csv\", header = 0)\nproperties_2017 = pd.read_csv(\"../input/zillow-prize-1/properties_2017.csv\", header = 0)\n\nprint(train_2016.shape)\nprint(train_2017.shape)\nprint(properties_2016.shape)\nprint(properties_2017.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to merge given datasets to conduct analysis and modeling process.\n1. Merge train_2016 and properties_2016, train_2017 and properties_2017, joining on `parcelid`\n2. Concatenate data from 2016 and 2017 and make `data` dataset which is total data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# make 2016 and 2017 merged data\ndata2016 = pd.merge(train_2016, properties_2016, how = 'left', on = 'parcelid')\ndata2017 = pd.merge(train_2017, properties_2017, how = 'left', on = 'parcelid')\n\nprint(data2016.shape)\nprint(data2017.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final merged data\ndata = pd.concat([data2016, data2017], axis = 0)\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Cleansing"},{"metadata":{},"cell_type":"markdown","source":"### (1) Missing values\nCalculate percentage of missing values in each variable. It's quite normal to drop variable with missing percentage more than 90% of total data, but I'll take a look at those data if they have any specialties first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calcuate missing %\nna_ratio = data.isna().sum().sort_values(ascending = False)/len(data)\nna_ratio","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`to_drop` is set of variables which have more than 90% missing values. There are 3 object type variables(categorical), 17 float type variables in `to_drop` variables. \n* float: Those are more reasonable to drop, since it's hard to estimate missing values from existing values in this case\n* object: Object variables are categorical variables. NaN values in those variables might have some meanings, so we have to consider those first."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = na_ratio[na_ratio>0.9].index.tolist()\ndata[to_drop].dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those are categorical variables with missing percentage over 90%: `fireplaceflag`, `hashottuborspa`, `taxdelinquencyflag` <br>\nNA values in each variable means that **there is no fireplace/hot tub(spa) and property taxes for this parcel are not past due as of 2015.** That is, NA is unique category in those variables, so it better not drop `fireplaceflag`, `hashottuborspa`, `taxdelinquencyflag` variables.<br>\n<br>\nTherefore, I'll drop only 17 float variables among `to_drop` variables.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"na_obj = data[to_drop].select_dtypes('object').columns # object variables in to_drop list\ndata[na_obj] = data[na_obj].fillna('None') # value_counts() method does not count NA, so replace it with 'None'\n\nfig, ax = plt.subplots(ncols = 3, nrows = 1, figsize = (9,3))\nfor i, col in enumerate(na_obj):\n    data[col].value_counts().plot.bar(ax = ax[i], color = '#d4dddd')\n    ax[i].set_title(f'{col} distribution', fontsize = 10);\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop 17 float variables in to_drop\nna_float = data[to_drop].select_dtypes('float').columns \ndata.drop(na_float, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dim of data after dropping NA columns\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Take closer look on more NA variables!"},{"metadata":{"trusted":true},"cell_type":"code","source":"na_ratio = data.isna().sum().sort_values(ascending = False)/len(data)\nna_cols = na_ratio[na_ratio>0].index\n\n# ends with 'cnt'\nna_cnt = na_cols[na_cols.str.endswith('cnt')]\nna_others = na_cols[~na_cols.str.endswith('cnt')]\nprint(na_cnt)\nprint('\\n')\nprint(na_others)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# na_cnt\ndata[na_cnt] = data[na_cnt].fillna(data[na_cnt].mean())\ndata[na_others] = data[na_others].fillna(data[na_others].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_ratio = data.isna().sum().sort_values(ascending = False)/len(data)\nna_ratio[na_ratio>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (2) Delete variables with only 1 unique value\nIn our data, there are variables which has only one unique value. I removed those variables since those variables does not affect target variable, logerror.<br>\nThose variables are `poolcnt` and `pooltypeid7`."},{"metadata":{"trusted":true},"cell_type":"code","source":"one_col = data.columns[data.nunique()==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop one_col variables\ndata.drop(one_col, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (3) Datetime format\nChange dtype of `transactiondate` variable to datetime, and then make variables that denote year, month, day, weekday each."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['transactiondate'] = pd.to_datetime(data['transactiondate'], format = '%Y-%m-%d')\n\ndata['year'] = data['transactiondate'].dt.year\ndata['month'] = data['transactiondate'].dt.month\ndata['day'] = data['transactiondate'].dt.day\ndata['weekday'] = data['transactiondate'].dt.weekday\n\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. EDA\nSimple exploration of target and features is included in this part! Also, I tried to try visualize plots neatly as much as I can, thanks to visualizaion kernel\n<https://www.kaggle.com/subinium/awesome-visualization-with-titanic-dataset>"},{"metadata":{},"cell_type":"markdown","source":"### (1) Target variable: Logerror\nTarget variable, `Logerror` ranges from -5 to 5, but mostly distributed around 0. Plus, distribution plots of year 2016 and 2017 are almost same!"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = plt.subplot(1,1,1)\nsns.distplot(data['logerror'], color = '#004c70')\nplt.title('Overall distribution of Logerror', fontsize = 15)\n\nfor s in ['top','left','right']:\n    ax.spines[s].set_visible(False)\nax.grid(axis='y', linestyle='-', alpha=0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(1,2,1)\nsns.distplot(data2016['logerror'], color = '#004c70')\nplt.title('Year 2016')\nplt.subplot(1,2,2)\nsns.distplot(data2017['logerror'], color = '#990000')\nplt.title('Year 2017')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (2) Time values analysis\n1. Transaction Date\n - Compare monthly, daily, weekly overall mean of logerror(transaction date)\n2. Year Built\n"},{"metadata":{},"cell_type":"markdown","source":"#### 1) Transaction Date\n\n**Monthly Analysis**\n\n + The first graph shows monthly transaction amount and second one shows monthly mean of logerror. It's interesting to find that monthly transaction amount and monthly mean of logerror shows opposite trend. **To be more specific, when there is large transaction amount in certain month, logerror mean tends to be lower than other months.** I colored large transaction & lower logerror mean to red, small transaction & higher logerror mean to blue! However, it's hard to conclude that there is correlation between those two yet.\n + More generally, more transactions occur during spring and summer seasons but transaction amount is comparatively lower in winter seasons(11,12,1,2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"color_map = ['#d4dddd' for _ in range(12)]\ncolor_map[10] = color_map[11] = '#004c70'; color_map[4] = color_map[5] = '#990000'\n\n\nplt.figure(figsize = (12,4))\nax1 = plt.subplot(1,2,1)\ndata.groupby('month')['logerror'].count().plot.bar(color = color_map)\nplt.xticks(rotation = 0); plt.xlabel('Month'); plt.ylabel('Amount')\nplt.title('Monthly transaction amount', fontsize = 15)\n\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\nax1.grid(axis='y', linestyle='-', alpha=0.4)\n\n\nax2 = plt.subplot(1,2,2)\ndata.groupby('month')['logerror'].mean().plot.bar(color = color_map)\nplt.axhline(data['logerror'].mean(), linestyle = '--', color = 'black', linewidth = 0.5)\nplt.text(0, 0.0145, 'mean', bbox=dict(facecolor='none', edgecolor='black', boxstyle='round'))\nplt.xticks(rotation = 0); plt.xlabel('Month'); plt.ylabel('Mean Logerror')\nplt.title('Monthly mean of Logerror', fontsize = 15)\n\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax2.spines[s].set_visible(False)\nax2.grid(axis='y', linestyle='-', alpha=0.4)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Weekly Analysis**\n+ Weekly mean logerror comparision plot also shows interesting results. On weekdends(Saturday & Sunday), less transactions occur. It seems that most transactions are done during weekdays.\n+ Weekly mean of logerror on weekends is much lower than other weekdays."},{"metadata":{"trusted":true},"cell_type":"code","source":"color_map = ['#d4dddd' for _ in range(7)]\ncolor_map[5] = color_map[6] = '#004c70'\n\nplt.figure(figsize = (12,4))\nax1 = plt.subplot(1,2,1)\ndata.groupby('weekday')['logerror'].count().plot.bar(color = color_map)\nplt.xticks(range(0,7),['Mon','Tue','Wed','Thu','Fri','Sat','Sun'],rotation = 0); plt.xticks(rotation = 0); plt.xlabel('Weekday'); plt.ylabel('Amount')\nplt.title('Weekly transaction amount', fontsize = 15)\n\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\nax1.grid(axis='y', linestyle='-', alpha=0.4)\n\n\n\nax = plt.subplot(1,2,2)\ndata.groupby('weekday')['logerror'].mean().plot.bar(color = color_map)\nplt.axhline(data['logerror'].mean(), linestyle = '--', color = 'black', linewidth = 0.5)\nplt.text(6, 0.0145, 'mean', bbox=dict(facecolor='none', edgecolor='black', boxstyle='round'))\nplt.xticks(range(0,7),['Mon','Tue','Wed','Thu','Fri','Sat','Sun'],rotation = 0); plt.xlabel('Weekday'); plt.ylabel('Mean Logerror')\nplt.title('Weekly mean of Logerror', fontsize = 15)\n\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax.spines[s].set_visible(False)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2) Year Built\n`yearbuilt` variable denotes 'the year the principal residence was built'. \n* Most of residences were built in 100 years, especially from 1950.\n* Mean logerror is large for residences that were built a long time ago(1824 ~ 1900). Recently built residences have smaller logerrors compared to old ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,4))\nax1 = plt.subplot(1,2,1)\nsns.kdeplot(data['yearbuilt'], color = '#004c70');plt.legend(loc='best'); plt.ylabel('Number of residences')\nplt.title('Residences built year distribution', fontsize = 15)\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\nax1.grid(axis='y', linestyle='-', alpha=0.4)\n\nax2 = plt.subplot(1,2,2)\nyearbuilt = data.groupby(['yearbuilt'])['logerror'].mean().reset_index()\nsns.lineplot(data = yearbuilt, x = 'yearbuilt', y = 'logerror', marker = 'o', markersize = 0.6, color = '#990000')\nplt.xticks([]); plt.xlabel('Year Built'); plt.ylabel('Mean Logerror')\nplt.title('Year Built mean of Logerror', fontsize = 15)\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax2.spines[s].set_visible(False)\nax2.grid(axis='y', linestyle='-', alpha=0.4)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (3) Parcel Locations\nLet's explore location/region related variables! Variables written below are used to plot regions.\n1. longitude & latitude\n2. regions: `regionidcounty`, `regionidcity`, `regionidzip`\n\n"},{"metadata":{},"cell_type":"markdown","source":"I visualized regions included in zillow data using latitude and longitude first! When plotting regions with those two coordinates, longitude corresponds to x-axis, and latitude corresponds to y-axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# rescale location variables\ndata[['latitude','longitude']] = data[['latitude','longitude']]/1000000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall region looks like below! According to Kaggle description, data from Los Angeles, Orange, Ventura are included."},{"metadata":{"trusted":true},"cell_type":"code","source":"# overall region plot\nplt.figure(figsize = (8,4))\nax = plt.subplot(1,1,1)\nplt.plot(data['longitude'], data['latitude'], 'o', markersize = 0.2, color = '#004c70')\nplt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.title('Locations of parcels', fontsize = 15)\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax.spines[s].set_visible(False)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionaly, I plotted 3 counties with different colors. (Colors: Ventura - red, LA - grey, Orange - blue)<br>\nPlus, `county` denotes specific regions, so it's better to convert those variable into categorical variable!\n\n<br>\n\n**Side notes**: `regionidcity` and `regionidneighborhood` has too many unique values, and there is no significant difference in logerror among values in those variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to categorical variable\nnew_cat = []\nfor value in data['regionidcounty'].values:\n    if value == 1286:\n        new_cat.append('Orange')\n    elif value == 2061:\n        new_cat.append('Ventura')\n    else:\n        new_cat.append('LA')\n\ndata['regionidcounty'] = new_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# county\ncolors = ['#d4dddd','#004c70', '#990000'] # originally 3, but includes NA values\n\nplt.figure(figsize = (8,4))\nax = plt.subplot(1,1,1)\nfor i, c in enumerate(data['regionidcounty'].unique()):\n    df = data[data['regionidcounty']==c]\n    plt.plot(df['longitude'], df['latitude'], 'o', markersize = 0.8, color = colors[i], label = c)\nplt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.title('Counties of parcels', fontsize = 15); plt.legend(loc = 'best')\n    \nfor s in [\"top\",\"right\",\"left\"]:\n    ax.spines[s].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (4) Residence components analysis\nIn this section, I'll explore several important columns related to `logerror`. Here, important columns are columns whose correlation coefficient with logerror is relatively bigger than other columns."},{"metadata":{},"cell_type":"markdown","source":"#### Correlation Heatmap\nGraph below shows correlation heatmap based on correlation coefficients with logerror. Overall coeffs are really small, maximum is 0.04. Therefore, I adjusted vmin and vmax to -0.05 and 0.05 each. According to this heatmap, important variables are **finishedsquarefeet12, calcuatedfinishedsquarefeet, calculatedbathnbr, bedroomcnt, fullbathcnt, bathroomcnt..etc**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = pd.DataFrame(data.corr()['logerror'].sort_values(ascending = False)).rename(columns = {'logerror':'correlation'})\n\nplt.figure(figsize = (3,8))\nsns.heatmap(corr, annot = True, fmt = '.2f', vmin = -0.05, vmax = 0.05, cmap = 'YlGnBu')\nplt.title('Correlation heatmap', fontsize = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1) Finished living area related variables\nTwo variables with top 2 correlation coefficients with logerror are `finishedsquarefeet12` and `calculatedfinishedsquarefeet`. Let's take a loot at descriptions first. Descriptions about those two variables are quite similar.\n* `finishedsquarefeet12`: Finished living area\n* `calculatedfinishedsquarefeet`:  Calculated total finished living area of the home \n\n"},{"metadata":{},"cell_type":"markdown","source":"Distribution of `finishedsquarefeet12` and `calculatedfinishedsquarefeet` are very similar, the only difference is that `calculatedfinishedsquarefeet` has some larger values."},{"metadata":{"trusted":true},"cell_type":"code","source":"top2 = ['finishedsquarefeet12','calculatedfinishedsquarefeet']\ncolors = ['#004c70', '#990000']\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,4))\nfor i, col in enumerate(top2):\n    sns.distplot(data[col], color = colors[i], ax = ax[i])\n    ax[i].set_title(f'{col} distribution', fontsize = 15)\n\nfor s in ['top','left','right']:\n    ax[0].spines[s].set_visible(False)\n    ax[1].spines[s].set_visible(False)\nax[0].grid(axis='y', linestyle='-', alpha=0.4); ax[1].grid(axis='y', linestyle='-', alpha=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation coefficient of those two variables is 0.958, which means that those variables are **highly correlated.** "},{"metadata":{"trusted":true},"cell_type":"code","source":"data[top2].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To analyze more specifically, let's take a look at jointplot of two variables. Except some values, those two variables are on function y = x.\n\n<br>\n\nTo sum up, top 2 variables that have high correlation coefficients with logerror are highly correlated and they have similar values and both denote finished living area. For regression, it's always better to delete one of highly correlated variables since they might cause **multicolinearity** issues."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data = data, x = 'finishedsquarefeet12', y = 'calculatedfinishedsquarefeet', kind = 'reg', color = 'purple')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2) Room types related variables\nRoom type variables, `bedroomcnt`, `bathroomcnt`, `fullbathcnt`, `calculatedbathnbr` have relatively high correlation coefficients with logerror. Among those vairables, three of them are related to bathroom. \n<br>\n\n**Description**\n* `bedroomcnt`:  Number of bedrooms in home \n* `bathroomcnt`:  Number of bathrooms in home including fractional bathrooms\n* `fullbathcnt`:  Number of full bathrooms (sink, shower + bathtub, and toilet) present in home\n* `calculatedbathnbr`:  Number of bathrooms in home including fractional bathroom\n"},{"metadata":{},"cell_type":"markdown","source":"`bedroomcnt` represents number of bedrooms in home. This ranges from 0 to 16. \n* Most of residences have 2-4 bedrooms in total\n* As a number of room increases(if there are more than 8 rooms in home), mean logerror was larger than that of small numbe of bedrooms. This is because there are very few residences with more than 8 rooms, so it's hard to estimated house prices with small amount of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# change to integer type\ndata['bedroomcnt'] = data['bedroomcnt'].astype(int)\n\n\nplt.figure(figsize = (12,4))\ncolor_map = ['#d4dddd' for _ in range(len(data['bedroomcnt'].unique()))]\ncolor_map2 = ['#d4dddd' for _ in range(len(data['bedroomcnt'].unique()))]\ncolor_map[3] = '#990000'\n\nax1 = plt.subplot(1,2,1)\ndata.groupby('bedroomcnt')['logerror'].count().plot.bar(color = color_map)\nplt.xticks(rotation = 0); plt.xlabel('Number of bedrooms'); plt.ylabel('Count')\nplt.title('Number of bedrooms distribution', fontsize = 15)\n\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\nax1.grid(axis='y', linestyle='-', alpha=0.4)\n\n\nax2 = plt.subplot(1,2,2)\ndata.groupby('bedroomcnt')['logerror'].mean().plot.bar(color = color_map2)\nplt.axhline(data['logerror'].mean(), linestyle = '--', color = 'black', linewidth = 0.5)\nplt.text(0, 0.05, 'mean', bbox=dict(facecolor='none', edgecolor='black', boxstyle='round'))\nplt.xticks(rotation = 0); plt.xlabel('Number of bedrooms'); plt.ylabel('Logerror')\nplt.title('Logerror distribution with respect to bedrooms counts', fontsize = 13)\n\n\n# axis setting\nfor s in [\"top\",\"right\",\"left\"]:\n    ax2.spines[s].set_visible(False)\nax2.grid(axis='y', linestyle='-', alpha=0.4)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's analyze three bathroom count related variables. First, they are also highly correlated, correlation coefficients are over 0.9."},{"metadata":{"trusted":true},"cell_type":"code","source":"bathroom = ['bathroomcnt','fullbathcnt','calculatedbathnbr']\ndata[bathroom] = data[bathroom].astype(int)\ndata[bathroom].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Number of bathrooms and number of full bathrooms(sink, shower+tub, toilet) are different as we can expect.\n* Distribution of `fullbathcnt` and `calculatedbathnbr` perfectly overlap. (Their correlation coef is 0.99) From this, we know that those two variables both denotes full bathroom + fraction bathroom counts. To avoid multicolinearity problem, I'll remove one of those variables in feature engineering part."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,4))\nax1 = plt.subplot(1,2,1)\ndata.groupby('bathroomcnt')['logerror'].count().plot.bar(color = '#990000', alpha = 0.4, label = 'bathroom')\ndata.groupby('fullbathcnt')['logerror'].count().plot.bar(color = '#004c70', alpha = 0.4, label = 'fullbathroom')\nplt.title('Bathroom count vs Full bathroom count', fontsize =15); plt.xticks(rotation = False); plt.legend()\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax1.spines[s].set_visible(False)\nax1.grid(axis='y', linestyle='-', alpha=0.4)\n\nax2 = plt.subplot(1,2,2)\ndata.groupby('fullbathcnt')['logerror'].count().plot.bar(color = '#004c70', alpha = 0.4, label = 'fullbathroom' )\ndata.groupby('calculatedbathnbr')['logerror'].count().plot.bar(color = 'yellow', alpha = 0.4, label = 'bathroom neighbor')\nplt.title('Full bathroom count vs Calcuated bathroom neighbors', fontsize =15); plt.xticks(rotation = False); plt.legend()\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax2.spines[s].set_visible(False)\nax2.grid(axis='y', linestyle='-', alpha=0.4)\nplt.tight_layout()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}