{"cells":[{"cell_type":"markdown","source":"## This is my first time learning kaggle. Thank you!","metadata":{"_uuid":"dcf74290ba2e39ec6923d213dbfd7cc18dbd91be","_cell_guid":"b2783aac-ccca-46b3-8f71-6176b30c5d52"}},{"source":"# ! _*_ coding:utf-8 _*_\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","cell_type":"code","metadata":{"_uuid":"f21e71e3a4b314f8e09a25585fec26f595feac05","_cell_guid":"87752291-1df5-4c4f-abe1-eddcefdee7f0","collapsed":true},"execution_count":null,"outputs":[]},{"source":"from subprocess import check_output\nprint(check_output([\"ls\",\"../input\"]).decode(\"utf8\"))","cell_type":"code","metadata":{"_uuid":"b0695bc009d5dbf93fad335e0b6f3ceb818c9ea0","_cell_guid":"b4ea3432-eed4-44bd-bc34-96b5237c9fa6"},"execution_count":null,"outputs":[]},{"source":"train_df = pd.read_csv(\"../input/train_2016_v2.csv\",parse_dates=[\"transactiondate\"])\nprint(train_df.shape)\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"4a417f5b75a63384f8c94a2b35f32c156e8cacc7","_cell_guid":"3e93818d-2819-4ad2-806d-5fdb8ab12acc"},"execution_count":null,"outputs":[]},{"source":"plt.scatter(range(train_df.shape[0]),np.sort(train_df.logerror))\nplt.show()","cell_type":"code","metadata":{"_uuid":"81b362d31fc905204292f25f50ec9460d0e50c68","_cell_guid":"5641e9b4-757f-4b13-a407-0622b8486053"},"execution_count":null,"outputs":[]},{"source":"train_df = pd.read_csv(\"../input/properties_2016.csv\")\nprint(train_df.shape)\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"36b079c6024040c7bf3cc86cf31e6915652bc605","_cell_guid":"84446fc1-1938-4a08-8195-d12243399c94"},"execution_count":null,"outputs":[]},{"source":"train_df = pd.read_csv(\"../input/properties_2017.csv\")\nprint(train_df.shape)\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"a62255657588b53c4dcfcf047edcdfcd7f48fd61","_cell_guid":"2c320bc9-565b-416c-af8e-0c71b729bc05"},"execution_count":null,"outputs":[]},{"source":"train_df = pd.read_csv(\"../input/sample_submission.csv\")\nprint(train_df.shape)\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"04a7d006f7c71b3d9782b2f923423ea75d384c47","_cell_guid":"65ee6e90-56fb-456a-9719-280e8a37f146"},"execution_count":null,"outputs":[]},{"source":"train_df = pd.read_csv(\"../input/train_2017.csv\")\nprint(train_df.shape)\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"a1a6277de4584ce8aee5f0bc8c476596cdd083b1","_cell_guid":"9ba5d0f6-3413-486d-bde8-e9148b4f663d"},"execution_count":null,"outputs":[]},{"source":"train_df = pd.read_excel(\"../input/zillow_data_dictionary.xlsx\")\nprint(train_df.shape)\ntrain_df.head()","cell_type":"code","metadata":{"_uuid":"ab796c1feffeb6e692205aaf62eb004dcef527b9","_cell_guid":"9e3ba4f6-e1eb-4cce-8265-ee8c18e939fb"},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"0657c8d8cd95cc67c1dcef90e46ecfaeccdc22e3","_cell_guid":"8c1734b8-c714-4f35-a4c4-8d9b50c470f1","collapsed":true},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"a3e11ee7bd6ba2d9909ec31c859a2f322c224878","_cell_guid":"6ddf525c-a5b0-4aa7-8659-febf6291ae7c","collapsed":true},"execution_count":null,"outputs":[]},{"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport gc\nprint(\"Loading data ...\")","cell_type":"code","metadata":{"_uuid":"d6a6e6c76e6cce27e8bf609730dd328c89f5319f","_cell_guid":"ea00be2d-63fb-4f02-b5ea-ba9f1d33943f"},"execution_count":null,"outputs":[]},{"source":"\"\"\"\nproperties_2016.csv\nproperties_2017.csv\nsample_submission.csv\ntrain_2016_v2.csv\ntrain_2017.csv\nzillow_data_dictionary.xlsx\n\"\"\"\n\n\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\nprop = pd.read_csv(\"../input/properties_2016.csv\")\nsample = pd.read_csv(\"../input/sample_submission.csv\")\nprint(\"Binding to float32\")\nfor c,dtype in zip(prop.columns,prop.dtypes):\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)","cell_type":"code","metadata":{"_uuid":"25ea1f856a56b8b09f33ebfaa22cb95dad7c327f","_cell_guid":"7852f099-0bf6-4f05-9b14-ecad5f5c1ac3"},"execution_count":null,"outputs":[]},{"source":"print(\"Creating training set ...\")\ndf_train = train.merge(prop,how=\"left\",on=\"parcelid\")\nx_train = df_train.drop(['parcelid','logerror','transactiondate','propertyzoningdesc',\n                        'propertycountylandusecode'],axis=1)\ny_train = df_train['logerror'].values\nprint(x_train.shape,y_train.shape)","cell_type":"code","metadata":{"_uuid":"71b99d7190be75cb95734f5632c709d3fc99fd95","_cell_guid":"576327ce-3e16-4541-82be-52e775945387"},"execution_count":null,"outputs":[]},{"source":"train_columns = x_train.columns\nfor c in x_train.dtypes[x_train.dtypes==object].index.values:\n    x_train[c]=(x_train[c]==True)\ndel df_train;gc.collect()\nsplit = 80000\nx_train,y_train,x_valid,y_valid = x_trian[:split],y_train[:split],x_train[split:],y_train[split:]","cell_type":"code","metadata":{"_uuid":"7d7903e1e190ee25e97d4fe3bf0804fb3afeced2","_cell_guid":"72047699-d3ea-48e5-928d-0da52de8f2f3"},"execution_count":null,"outputs":[]},{"source":"print('Nuilding DMatrix ...')\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\ndel x_train,x_valid;gc.collect()","cell_type":"code","metadata":{"_uuid":"4acf930b1e02f639414dec37ad389454df6eabb5","_cell_guid":"96a5a4bb-09bb-4393-ac73-0a505863bc5f","collapsed":true},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"8622173a737a8b5399d84615a2b66aea17ea3307","_cell_guid":"208325c4-96c3-443f-bb69-1de4d084c90c","collapsed":true},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"95bb736f5f2c5166835290fa73bed62e731f4329","_cell_guid":"16c3daee-6c98-417d-b9c4-18233854768c","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Parameters\nXGB_WEIGHT = 0.6500\nBASELINE_WEIGHT = 0.0056\nBASELINE_PRED = 0.0115\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\n##### READ IN RAW DATA\nprint( \"\\nReading data from disk ...\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\")\n##### PROCESS DATA FOR LIGHTGBM\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\t\n        prop[c] = prop[c].astype(np.float32)\ndf_train = train.merge(prop, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\ndel df_train; gc.collect()\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)\n##### RUN LIGHTGBM\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\ndel d_train; gc.collect()\ndel x_train; gc.collect()\nprint(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\nprint(\"   ...\")\ndel sample, prop; gc.collect()\nprint(\"   ...\")\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\nprint(\"\\nStart LightGBM prediction ...\")\n# num_threads > 1 will predict very slow in kernal\nclf.reset_parameter({\"num_threads\":1})\np_test = clf.predict(x_test)\ndel x_test; gc.collect()\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n##### RE-READ PROPERTIES FILE\n##### (I tried keeping a copy, but the program crashed.)\nprint( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('../input/properties_2016.csv')\n##### PROCESS DATA FOR XGBOOST\nprint( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\ntrain_df = train.merge(properties, how='left', on='parcelid')\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)\n# shape        \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n# drop out ouliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.418 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n##### RUN XGBOOST\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n# Enough with the ridiculously overfit parameters.\n# I'm going back to my version 20 instead of copying Jayaraman.\n# I want a num_boost_rounds that's chosen by my CV,\n# not one that's chosen by overfitting the public leaderboard.\n# (There may be underlying differences between the train and test data\n#  that will affect some parameters, but they shouldn't affect that.)\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n# cross-validation\n#print( \"Running XGBoost CV ...\" )\n#cv_result = xgb.cv(xgb_params, \n#                   dtrain, \n#                   nfold=5,\n#                   num_boost_round=350,\n#                   early_stopping_rounds=50,\n#                   verbose_eval=10, \n#                   show_stdv=False\n#                  )\n#num_boost_rounds = len(cv_result)\n# num_boost_rounds = 150\nnum_boost_rounds = 242\nprint(\"\\nXGBoost tuned with CV in:\")\nprint(\"   https://www.kaggle.com/aharless/xgboost-without-outliers-tweak \")\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred = model.predict(dtest)\nprint( \"\\nXGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )\n##### COMBINE PREDICTIONS\nprint( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\nlgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT\npred = XGB_WEIGHT*xgb_pred + BASELINE_WEIGHT*BASELINE_PRED + lgb_weight*p_test\nprint( \"\\nCombined predictions:\" )\nprint( pd.DataFrame(pred).head() )\n##### WRITE THE RESULTS\nprint( \"\\nPreparing results for write ...\" )\ny_pred=[]\nfor i,predict in enumerate(pred):\n    y_pred.append(str(round(predict,4)))\ny_pred=np.array(y_pred)\noutput = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n# set col 'ParceID' to first col\ncols = output.columns.tolist()\ncols = cols[-1:] + cols[:-1]\noutput = output[cols]\nfrom datetime import datetime\nprint( \"\\nWriting results to disk ...\" )\noutput.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\nprint( \"\\nFinished ...\" )","cell_type":"code","metadata":{"_uuid":"e8a1fcc7f8ac269bc6ee57c5c65e934c180cf321","_cell_guid":"223e1e62-3931-438b-a8e4-0965f8c362e7","collapsed":true},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"0e0552a60b1f1eb0eee05fd8edb9e3b343a085a6","_cell_guid":"132e1128-1d8a-496c-97fa-6b658a835251","collapsed":true},"execution_count":null,"outputs":[]},{"source":"\"\"\"\nproperties_2016.csv\nproperties_2017.csv\nsample_submission.csv\ntrain_2016_v2.csv\ntrain_2017.csv\nzillow_data_dictionary.xlsx\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport gc\nprint('Loading data ...')\ntrain = pd.read_csv('../input/train_2016_v2.csv')\nprop = pd.read_csv('../input/properties_2016.csv')\nfor c, dtype in zip(prop.columns, prop.dtypes):\t\n    if dtype == np.float64:\t\t\n        prop[c] = prop[c].astype(np.float32)\ndf_train = train.merge(prop, how='left', on='parcelid')\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\ndel df_train; gc.collect()\nsplit = 90000\nx_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\nx_train = x_train.values.astype(np.float32, copy=False)\nx_valid = x_valid.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)\nd_valid = lgb.Dataset(x_valid, label=y_valid)\nparams = {}\nparams['learning_rate'] = 0.002\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'mae'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 60\nparams['min_data'] = 500\nparams['min_hessian'] = 1\nwatchlist = [d_valid]\nclf = lgb.train(params, d_train, 500, watchlist)\ndel d_train, d_valid; gc.collect()\ndel x_train, x_valid; gc.collect()\nprint(\"Prepare for the prediction ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nsample['parcelid'] = sample['ParcelId']\ndf_test = sample.merge(prop, on='parcelid', how='left')\ndel sample, prop; gc.collect()\nx_test = df_test[train_columns]\ndel df_test; gc.collect()\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nx_test = x_test.values.astype(np.float32, copy=False)\nprint(\"Start prediction ...\")\n# num_threads > 1 will predict very slow in kernal\nclf.reset_parameter({\"num_threads\":1})\np_test = clf.predict(x_test)\ndel x_test; gc.collect()\nprint(\"Start write result ...\")\nsub = pd.read_csv('../input/sample_submission.csv')\nfor c in sub.columns[sub.columns != 'ParcelId']:\n    sub[c] = p_test\nsub.to_csv('lgb_starter.csv', index=False, float_format='%.4f')","cell_type":"code","metadata":{"_uuid":"4713288fc2324ea417586c78d3639a11534a6177","_cell_guid":"159128b2-6bc4-474e-87e8-cad33d7d36de"},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"}}}