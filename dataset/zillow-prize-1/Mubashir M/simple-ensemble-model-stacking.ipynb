{"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7dee1d0e-7c00-4163-a4ce-405ca823d1b5","_uuid":"df085aacf72fb3ab5b8a9bc1ea4ce7e24a719baf"},"source":"This notebook is a very basic and simple introductory primer to the method of ensembling models, in particular the variant of ensembling known as Stacking. In a nutshell stacking uses as a first-level (base), the predictions of a few basic machine learning models (Regressors) and then uses another model at the second-level to predict the output from the earlier first-level predictions.\n\nI myself am quite a newcomer to the Kaggle scene as well and the first proper ensembling/stacking script. The material in this notebook borrows heavily from Faron's script as well as anisotrpoic although ported to factor in ensembles of Regressors whilst those were ensembles of classifiers. Anyway please check out his script here:\n\nStacking Starter : by [Faron](http://)"},{"cell_type":"code","metadata":{"_cell_guid":"89225e1c-4052-4001-9d16-ad6bfcc7d88a","_uuid":"2696045484f19856c3ad59f7ad7005ae3f0ab869"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import KFold;","execution_count":1},{"cell_type":"code","metadata":{"_cell_guid":"55302570-2d4e-493d-9feb-3c488d497e7b","_uuid":"7004eb958bacc7fbe36b77b441d67de85abf3d92"},"outputs":[],"source":"print( \"\\nReading data from disk ...\")\nproperties = pd.read_csv(r\"../input/properties_2016.csv\")\ntrain_df = pd.read_csv(\"../input/train_2016_v2.csv\")\ntest_df = pd.read_csv(\"../input/sample_submission.csv\")\ntest_df = test_df.rename(columns={'ParcelId': 'parcelid'})","execution_count":2},{"cell_type":"code","metadata":{"_cell_guid":"49e2313a-459f-4698-9240-f78ba8fd9301","_uuid":"1b4726a839b5492077c8954b5cf29af8e1e3e765","collapsed":true},"outputs":[],"source":"train = train_df.merge(properties, how = 'left', on = 'parcelid')\ntest = test_df.merge(properties, on='parcelid', how='left')","execution_count":3},{"cell_type":"markdown","metadata":{"_cell_guid":"d4643519-fa4b-4dab-a438-325cdb3fe03e","_uuid":"9e72dd55211f11ca221737223fffbda2531df46d"},"source":"### Encoding the Variables"},{"cell_type":"code","metadata":{"_cell_guid":"47dfcf0f-a59a-4650-833b-b7380975f66b","_uuid":"82dc6d49653e60bb7fd8c42498084cf540da13e5","collapsed":true},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder  \n\nlbl = LabelEncoder()\n\nfor c in train.columns:\n    train[c]=train[c].fillna(0)\n    if train[c].dtype == 'object':\n        lbl.fit(list(train[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n\nfor c in test.columns:\n    test[c]=test[c].fillna(0)\n    if test[c].dtype == 'object':\n        lbl.fit(list(test[c].values))\n        test[c] = lbl.transform(list(test[c].values))     ","execution_count":4},{"cell_type":"markdown","metadata":{"_cell_guid":"a1917dbc-5c67-4e57-acf2-f460f529aaf7","_uuid":"765d7526e4f49e2ea691b2d83568f9bfa46febe0"},"source":"## Pearson Correlation Heatmap\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"7eabe5c7-51be-44e7-afec-50e705fcecf0","_uuid":"aa281ad6d465684f45a46c5619ab05b4046fba1b"},"source":"let us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilise the Seaborn plotting package which allows us to plot heatmaps very conveniently"},{"cell_type":"markdown","metadata":{"_cell_guid":"888fc687-aa56-4a43-9f9d-a5f97ea52c1a","_uuid":"a8af8fb5f15380e80ebefe03d7ee1d1ed1b9b27d"},"source":"### First, Calculate Feature Importance "},{"cell_type":"markdown","metadata":{"_cell_guid":"71fec4bd-0610-460f-aa09-50919dfdbc42","_uuid":"8077c750092cff0d1497866fccdb58753816e0a2"},"source":"We are calculating the feature importance because the variables are just too much, so we only need concern ourselves with the ones that are useful for our analysis"},{"cell_type":"code","metadata":{"_cell_guid":"c8817457-2eb4-4495-8435-3b7ebf730980","_uuid":"ecbbcf49aedfbfa8090632a4f204eb0c555affc4","collapsed":true},"outputs":[],"source":"from sklearn import model_selection, preprocessing\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain_y = train.logerror.values\ntrain_X = train.drop([\"parcelid\", \"transactiondate\", \"logerror\"], axis=1)\n","execution_count":5},{"cell_type":"code","metadata":{"_cell_guid":"5ca6f4ec-6850-453b-8bb9-329e59f357bd","_uuid":"60938ad4937b82144376110ba59acd6180b00832","collapsed":true},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)","execution_count":6},{"cell_type":"code","metadata":{"_cell_guid":"bc7f7905-fdab-4a03-b812-d8ded4c56728","_uuid":"274e0f2f84f18e5c5e52a3a0fd6dce727bce6ecb","collapsed":true},"outputs":[],"source":"featureImportance = model.get_fscore()\nfeatures = pd.DataFrame()\nfeatures['features'] = featureImportance.keys()\nfeatures['importance'] = featureImportance.values()\n","execution_count":7},{"cell_type":"code","metadata":{"_cell_guid":"7fc21571-f892-40e0-b82c-9640a56cab8d","_uuid":"c2d155731f29264d865589a39280e1041e71fcb8"},"outputs":[],"source":"features.sort_values(by=['importance'],ascending=False,inplace=True)\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nplt.xticks(rotation=90)\nsns.barplot(data=features.head(15),x=\"importance\",y=\"features\",ax=ax,orient=\"h\", color = \"#34495e\")","execution_count":8},{"cell_type":"code","metadata":{"_cell_guid":"e026d05a-126b-457d-86f7-4240a0b500d8","_uuid":"ec1a8e187db86cdd28b058b5bcd1021322c0e6ac","collapsed":true},"outputs":[],"source":"topFeatures = features[\"features\"].tolist()[:20]\ncorrMatt = train[topFeatures].corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\n","execution_count":9},{"cell_type":"code","metadata":{"scrolled":false,"_cell_guid":"afa951e8-754c-45b8-bba0-7be0ce2a48b6","_uuid":"6010acffc2b07d9f440706e8f5c31aae84880fff"},"outputs":[],"source":"colormap = plt.cm.viridis\nplt.figure(figsize=(12,20))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(corrMatt,linewidths=0.6,vmax=1.0, mask = mask, square = True, linecolor='white', annot=True)\nplt.show()","execution_count":10},{"cell_type":"markdown","metadata":{"_cell_guid":"dfa12ab6-a310-48a2-adea-f4bdd9b122f2","_uuid":"c46e3a7f4d4c0957a8bb71c6e079ec88049a0de9"},"source":"### Outlier's Check"},{"cell_type":"code","metadata":{"_cell_guid":"0cbe693c-1656-4b5d-bc23-f5fe2149ac27","_uuid":"fbd839116aae43e509370351e8bfccc8e01cd522"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nsns.distplot(train.logerror.values, bins=500, kde=False)\nplt.xlabel('logerror', fontsize=12)\nplt.show()","execution_count":11},{"cell_type":"code","metadata":{"scrolled":true,"_cell_guid":"e0e6d3b3-de93-436e-b4c6-75bcc15bde55","_uuid":"4f6cd60316e3ddb65f31ec6a5a19033465b1a64a"},"outputs":[],"source":"train=train[ train.logerror > -0.40 ]\ntrain=train[ train.logerror < 0.419 ]\n\nplt.figure(figsize=(12,8))\nsns.distplot(train.logerror.values, bins=50, kde=False)\nplt.xlabel('logerror', fontsize=12)\nplt.show()","execution_count":12},{"cell_type":"code","metadata":{"_cell_guid":"41625fb1-d5f3-4aea-b4fb-d5b2118f4f59","_uuid":"770dcbd8807b0231c715c22ab4c5fde42aa84313"},"outputs":[],"source":"test.head(2)","execution_count":13},{"cell_type":"markdown","metadata":{"_cell_guid":"f037f0d6-6906-43aa-b3ef-56b351c2e501","_uuid":"15ec6985072ebdc3617d002d812b95252c75111b"},"source":"### Helpers via Python Classes"},{"cell_type":"markdown","metadata":{"_cell_guid":"8720a0e9-d809-42bd-9687-d931d3f07f21","_uuid":"324e150375e73f75a1f3525ec0d0040c512ff4e5"},"source":"In the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn regressor. Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke four different regressor."},{"cell_type":"code","metadata":{"_cell_guid":"7eb60633-89eb-4767-84f7-936f00ea04db","_uuid":"d039933eaada44139a2d58c50b3c2e6cd23f3c22","collapsed":true},"outputs":[],"source":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn Regressor\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer","execution_count":14},{"cell_type":"markdown","metadata":{"_cell_guid":"383cd745-7ee5-4877-9b2a-61194132a4c8","_uuid":"d7c9a914706a162c4e9cefdf0d4d0697a81b9c8a"},"source":"def **init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (regressor), you have to give it the parameters of clf (what sklearn regressor you want), seed (random seed) and params (parameters for the regressors).\nThe rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn regressors."},{"cell_type":"markdown","metadata":{"_cell_guid":"8df2579e-35e0-4c24-87c2-2aab50d012b0","_uuid":"be5da9c71475fe056d79858c2c6539b24a4820a3"},"source":"### Out-of-Fold Predictions"},{"cell_type":"code","metadata":{"_cell_guid":"2cf1dbfb-50d7-4ee9-8937-b643166ef705","_uuid":"68450731240107d3c61c9d6d6e5ea0964a2c7ae6"},"outputs":[],"source":"print(train.shape)\nprint(test.shape)","execution_count":15},{"cell_type":"markdown","metadata":{"_cell_guid":"22afb8de-4ed8-4b40-9ec5-759a9faf625d","_uuid":"f39b6b517d6917aa3ce0efd2852b6393307064db"},"source":"Now as alluded to above in the introductory section, stacking uses predictions of base regressors as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions."},{"cell_type":"code","metadata":{"_cell_guid":"168fbe75-12b2-4c20-92f7-2254a7a3b220","_uuid":"298179e1946fb8f11665ba3210a31ded32d58ef4","collapsed":true},"outputs":[],"source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":16},{"cell_type":"markdown","metadata":{"_cell_guid":"6512a8bc-236d-46ba-a9a4-c8ccb1174d40","_uuid":"00187ba29843ade4f8d729d27dc3baae1987e795"},"source":"### Generating our Base First-Level Models"},{"cell_type":"markdown","metadata":{"_cell_guid":"471bdc7c-3c06-4532-ad78-b978b95c15fe","_uuid":"4263bf8fd6e4df6c0efb8e333907d7cb08be9b3a"},"source":"So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\nRandom Forest regressor\nExtra Trees regressor\nAdaBoost regressor\nGradient Boosting regressor\n"},{"cell_type":"code","metadata":{"_cell_guid":"0db9a389-4d1b-4e85-9d11-8615f6303f7e","_uuid":"27d719e0af1cd6a7037526a07bab7a7752c731e8","collapsed":true},"outputs":[],"source":"# Put in our parameters for said regressors\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 50,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':50,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 50,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 50,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n","execution_count":17},{"cell_type":"markdown","metadata":{"_cell_guid":"ca92bbe4-0cff-4167-8160-544a4f4d5c48","_uuid":"0aa4288cff45506cf73b08a5866bc63da4f39caa"},"source":"Furthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 4 objects that represent our 4 learning models via our Helper Sklearn Class we defined earlier."},{"cell_type":"code","metadata":{"_cell_guid":"ed9b77f7-72aa-4581-91a6-336e640c701c","_uuid":"12784f83694b1e42e399d9e083feb5fb87f96d72","collapsed":true},"outputs":[],"source":"#Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestRegressor, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesRegressor, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostRegressor, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingRegressor, seed=SEED, params=gb_params)\n#svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","execution_count":18},{"cell_type":"markdown","metadata":{"_cell_guid":"3b47f7c2-f14b-4471-a029-38f3e0beea4f","_uuid":"663b60ea710d36de2ce36ab94acbdefafcf4892a"},"source":"### Features Checking"},{"cell_type":"code","metadata":{"_cell_guid":"b9fb8e1d-99a1-48d2-b06b-5f7e35350ff5","_uuid":"1ba082a7d1f927d22f0a104b50740c68aa76aa31"},"outputs":[],"source":"feature_names = list(train.columns)\nprint(np.setdiff1d(train.columns, test.columns))","execution_count":19},{"cell_type":"code","metadata":{"scrolled":true,"_cell_guid":"e40e0c83-1d3a-4c6f-bbcb-0f3050c32cde","_uuid":"1707a9e5522a3453182fb918a4955ea2216a99e9"},"outputs":[],"source":"do_not_include = ['parcelid', 'logerror', 'transactiondate', 'hashottuborspa',\n 'propertycountylandusecode',\n 'propertyzoningdesc',\n 'fireplaceflag',\n 'taxdelinquencyflag']\n\nfeature_names = [f for f in train.columns if f not in do_not_include]\n\nprint(\"We have %i features.\"% len(feature_names))\ntrain[feature_names].count()","execution_count":20},{"cell_type":"markdown","metadata":{"_cell_guid":"1b99d241-8965-446b-a8c9-f5f90085189c","_uuid":"b6fa7d858aca7d1e8b7ff564ee0708d0d514a35a"},"source":"#### Creating NumPy arrays out of our train and test sets"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ca90bc6-2640-4d1f-8a3b-af0afa2a9b6c","_uuid":"3c24e8120acf240c1b1bbbb1632e68b74aae8480"},"source":"Great. Having prepared our first layer base models as such, we can now ready the training and test test data for input into our regressors by generating NumPy arrays out of their original dataframes as follows:"},{"cell_type":"code","metadata":{"_cell_guid":"392fcaa4-d85f-48eb-a740-a5a2fbd4132f","_uuid":"c4ac5a80eecd539402a239789771a9587ce62d14","collapsed":true},"outputs":[],"source":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['logerror'].ravel()\n#train = train.drop(['logerror', 'parcelid', 'transactiondate'], axis=1)\n","execution_count":21},{"cell_type":"code","metadata":{"_cell_guid":"0241e044-d448-4d2e-96c7-b2f4e78b287e","_uuid":"a566aedc0f739b5f88d946e5aee9fa64eac3dc09","collapsed":true},"outputs":[],"source":"train = train[feature_names]\ntest = test[feature_names]","execution_count":22},{"cell_type":"code","metadata":{"_cell_guid":"0c0ba291-6058-4135-8b8a-e2d99f6c137c","_uuid":"6201904fb3a40eca62ebe64f0eb51623ee6ba206"},"outputs":[],"source":"print(train.shape)\nprint(test.shape)","execution_count":23},{"cell_type":"code","metadata":{"_cell_guid":"df09c168-2133-4f15-a625-86430e13e759","_uuid":"ad90fc6dcfd55e5392230c87040703a4698733a1","collapsed":true},"outputs":[],"source":"x_train = train.values # Creates an array of the train data\nx_test = test.values # Creats an array of the test data","execution_count":24},{"cell_type":"markdown","metadata":{"_cell_guid":"23b99207-9a42-4736-af5f-9969c8d64d0b","_uuid":"4c19a6ebdedb9fc795550f90457e17debf410eb6"},"source":"### Output of the First level Predictions"},{"cell_type":"markdown","metadata":{"_cell_guid":"a4698e3f-1527-4ca0-a42a-0ad200196c62","_uuid":"bb5f68b7571f4b913107ea2cbd2ad775918ed598"},"source":"We now feed the training and test data into our 4 base regressors and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run."},{"cell_type":"code","metadata":{"_cell_guid":"e3cc8e9b-d63b-4505-b58f-1f35f71943b5","_uuid":"23b3b6453342e6e30a715cfa3305394773f57061"},"outputs":[],"source":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n#svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector \n\nprint(\"Training is complete\")","execution_count":25},{"cell_type":"markdown","metadata":{"_cell_guid":"6f70018b-5741-4b69-ab4c-f81e1f0bf553","_uuid":"ce1dd451d3084810a8d030d03531c95ef978691c"},"source":"### Feature importances generated from the different regressors"},{"cell_type":"code","metadata":{"scrolled":true,"_cell_guid":"0d52d1ea-719d-4d33-a0a7-ae6d12572703","_uuid":"4a2c1dba6b61d15d628e5674c93469cfc5b00853"},"outputs":[],"source":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n","execution_count":26},{"cell_type":"markdown","metadata":{"_cell_guid":"5fdede5f-feea-4a6a-be50-60ed5fae9178","_uuid":"f6a03c0788c59e62884abd68088e05cb64481496"},"source":"## Second-Level Predictions from the First-level Output"},{"cell_type":"markdown","metadata":{"_cell_guid":"087ac53f-fbb2-41ba-bd94-555af8f43c84","_uuid":"8e98d8253a92efe53a93a85a38a2287a58570188"},"source":"### First-level output as new features"},{"cell_type":"markdown","metadata":{"_cell_guid":"53c51a6f-3ce0-4bc3-90cc-1385f973c95a","_uuid":"94adc1fb63f25a647c90f853b037442e359accbf"},"source":"Having now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next regressor. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier regressors and we train the next regressor on this."},{"cell_type":"code","metadata":{"_cell_guid":"fd9220f7-78b4-4fb6-981a-e9b5474fe8f3","_uuid":"0598e044eb14ab2ed3f8ab07f4360f406da941fb"},"outputs":[],"source":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","execution_count":32},{"cell_type":"markdown","metadata":{"_cell_guid":"465866f2-ae87-4009-9b2e-2eb7c1fd2623","_uuid":"d9504ce4fe685de0439b5ab7ccc16bee3334aae5"},"source":"### Correlation Heatmap of the Second Level Training set"},{"cell_type":"code","metadata":{"_cell_guid":"2cbf5526-211b-4443-972f-ce11fecebdda","_uuid":"31d9b1eec762803ed93952af026f034381edb652"},"outputs":[],"source":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Portland',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","execution_count":33},{"cell_type":"markdown","metadata":{"_cell_guid":"afea80c2-7ab0-473c-a2aa-f5007adc1ebd","_uuid":"3e38b492d2b44b164248d0c94972769fcf6250c6"},"source":" ### Making the New Training & Testing Sets"},{"cell_type":"code","metadata":{"_cell_guid":"181f9ee6-52c4-470f-8fb4-d31e70a032c2","_uuid":"44f8a7159d77ee1d12a4190d090bc5930b618c4c","collapsed":true},"outputs":[],"source":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)","execution_count":34},{"cell_type":"markdown","metadata":{"_cell_guid":"284665bc-8b31-40c5-8073-72418026a158","_uuid":"60d9fc85810081bd6ff71f081f3ac365b455ebc2"},"source":"### Second level learning model via XGBoost"},{"cell_type":"markdown","metadata":{"_cell_guid":"1909f751-0b57-48db-a898-91a2f7171193","_uuid":"1fca1f7774cc2657bb955028fd6d33b04525e97b"},"source":"Here we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the official documentation.\nAnyways, we call an XGBoost and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"},{"cell_type":"markdown","metadata":{"_cell_guid":"342434d0-a809-4513-8b0e-630e3ceafde3","_uuid":"95a565ed3f3d6c287f1cf04fd077860074a83c25"},"source":"### Assignment of Variable"},{"cell_type":"code","metadata":{"_cell_guid":"f626e437-9810-4693-a80d-0e5125de8494","_uuid":"a715727948368e553f74ea7852c4395e9ec4734e","collapsed":true},"outputs":[],"source":"X = x_train\ny = y_train\ny_mean = np.mean(y_train)","execution_count":35},{"cell_type":"code","metadata":{"_cell_guid":"342be2d3-6a03-41bb-8296-5e79f1acb364","_uuid":"ca7d5e915da865b8d492c412ed335d35a2df740e","collapsed":true},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nXtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.2, random_state=2000)\n\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\n\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters! My favorite is random search :)\nxgb_params = {\n    'eta': 0.025,\n    'max_depth': 7,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n","execution_count":36},{"cell_type":"code","metadata":{"_cell_guid":"66d2979c-3db9-47e4-a9c2-370dee9cc43a","_uuid":"efff35850045e78c6ebe52600e15de566d0acfe2"},"outputs":[],"source":"model_xgb = xgb.train(xgb_params, dtrain, 2000, watchlist, early_stopping_rounds=300,\n                  maximize=False, verbose_eval=15)","execution_count":37},{"cell_type":"code","metadata":{"_cell_guid":"d7686d72-d228-45b0-ae43-eb8ace4f97ff","_uuid":"5fbe7da186533914da9d4d2f1328de038ecc21e1","collapsed":true},"outputs":[],"source":"dtest = xgb.DMatrix(x_test)\npredicted_test_xgb = model_xgb.predict(dtest)","execution_count":38},{"cell_type":"markdown","metadata":{"_cell_guid":"84768bc7-fb7e-49df-9d35-bee325f9ad53","_uuid":"7dff0560fc86e6a89d15924e97f70022847a95fd"},"source":"### Producing the Submission file"},{"cell_type":"code","metadata":{"_cell_guid":"0ee2455f-7855-42b2-815f-80ff38713f41","_uuid":"867613042f1d4720ba0112ea2713309f695a82c7","collapsed":true},"outputs":[],"source":"sub = pd.read_csv('../input/sample_submission.csv')\nfor c in sub.columns[sub.columns != 'ParcelId']:\n    sub[c] = predicted_test_xgb\n\nprint('Writing csv ...')\nsub.to_csv('xgb_stacked.csv', index=False, float_format='%.4f')","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"d49ed9f4-f999-4fe0-848b-dd14722b2354","_uuid":"2a5b408a5be3e07f9ed33dc8c3a7b11290248fbc"},"source":"### Steps for Further Improvement"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee3d3b82-f7b0-4603-82a7-0be35db19065","_uuid":"4a66fdb197ea78c6485bcdd9c455849cc25ba88c"},"source":"\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers/regressors as well as levels of stacking which go to more than 2 levels. \n\nThe base models in here are not optimized, instead reduced number of estimators have been taken or otherwise it consumes a lot of time. I encourage participants to fork the script on their host machine and run it, make the changes as necessary. I think it has somewhere around 0.65 on the public Leaderboard. But it can be used as a reference to make ones own models. \n\nSome additional steps that may be taken to improve one's score could be:\n1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n2. Introduce a greater variety of base models for learning. \n3. Optimizing the parameters of base learning models\n\nThe more uncorrelated the results, the better the final score."},{"cell_type":"code","metadata":{"_cell_guid":"deba0597-a7ba-45c2-a601-5db15775db68","_uuid":"83d18add8767c54aa27dd69e34bb27c58fd1a841","collapsed":true},"outputs":[],"source":"","execution_count":null}],"nbformat_minor":1}