{"cells":[{"cell_type":"markdown","source":"# Exploration of Zillow Data\n### This is a work in progress and is still being updated.","metadata":{"_uuid":"ecf1cab7a5f16142deac740de61610b9c9b1a159","_cell_guid":"268b4f8f-61b0-4da6-9e6e-560f2c3d8392"}},{"source":"# Load in libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","cell_type":"code","metadata":{"_uuid":"2c6e9be963ae6d379d5f67f20a09f041ee99cb33","_cell_guid":"9951a7ad-4288-4dc8-87b1-a48de752b458","_execution_state":"idle"},"execution_count":null,"outputs":[]},{"source":"# Read in datasets / information \ny_train = pd.read_csv('../input/train_2016_v2.csv', index_col='parcelid', low_memory=False)\nx_train = pd.read_csv('../input/properties_2016.csv', index_col='parcelid', low_memory=False)\ndatadict = pd.read_excel('../input/zillow_data_dictionary.xlsx', index_col='Feature')\ndatadict.index = datadict.index.str.replace('\\'', '')","cell_type":"code","metadata":{"_uuid":"76095c408077735649158a84b689f3fbec12169f","_cell_guid":"26154972-00f4-468d-8b74-817dcfe906f1","_execution_state":"busy"},"execution_count":null,"outputs":[]},{"source":"# Let's merge the x and y dataframes -- this will give us a complete set\ntrain = pd.merge(x_train, y_train, left_index=True, right_index=True, how='right')\nprint(train.shape)","cell_type":"code","metadata":{"_uuid":"32acae5ed8bf483542706b63e81581789a08b732","_cell_guid":"46ac61b9-bf23-48a6-b6c9-57b76da46505"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's brainstorm a list of things that are going to play into the home price. But we are not just interested in the price -- we're interested in the *error* in predicting home price. So this may be things that Zillow is not taking into account under normal circumstances.\n\n- Location, incl. state, county and neighborhood\n- Public services available, e.g. schools\n- Timing, incl. season and month\n- Remodeled home\n- Other flipping homes\n- Gentrification booms\n\nNow, let's start to look at the actual data. First, let's look at which fields have a lot of NULL data, which might influence the interpretation of our summary statistics.'","metadata":{"_uuid":"ee3d0c30963ee4df998e1b456004af0d9ff05373","_cell_guid":"a6be4424-0574-4eb2-834a-bc9565b73f39"}},{"source":"# Let's find the values that are most present\n%matplotlib inline\nfig, ax = plt.subplots(1,1, figsize=(8,6))\nnumnulls = x_train.notnull().sum()\nnumnulls.sort_values().plot(kind='barh', figsize=(4,8), ax=ax)\nax.set_title('Number of Valid Values per Column')\nplt.show()","cell_type":"code","metadata":{"_uuid":"851cfd67f3f41277b418567d00a3bb0c63a9d6db","_cell_guid":"e8ef39b3-35c8-488a-b1f8-b30d06f761d7","_execution_state":"idle"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column Inter-Correlations\nWe have different types of data here: \n- Continuous variables (floor size, year built)\n- Grouping variables (zip code, architecture type)\n\nIt also looks like there are some redundancies in here. For example, \"tax amount\", \"taxvaluedollarcnt\" and \"structuretaxvaluedollarcnt\" are probably all correlated. However, if an entry is missing one of these features (or more), the algorithm may under/over-weight them. \n\nSo, let's check out whether a few sets of columns are correlated or not using seaborn's \"pair plot\"","metadata":{"_uuid":"8e6bc68fec8812f6c82f41985cdf366cfb085efa","_cell_guid":"bfbb9125-4a1f-4d52-9960-6907231dfa55"}},{"source":"# Different columns that seem like they might be related\ng1 = ['taxamount', 'taxvaluedollarcnt', 'structuretaxvaluedollarcnt','landtaxvaluedollarcnt'] \n#g2 = ['assessmentyear', 'yearbuilt']\n#g3 = ['bedroomcnt', 'bathroomcnt', 'calculatedbathnbr', 'fullbathcnt', 'roomcnt']\n#g4 = ['calculatedfinishedsquarefeet','finishedsquarefeet12', 'lotsizesquarefeet']\n\ngp = g1\n# The pairplot creates a matrix of plots: on the diagonal is a histogram/density plot for \n# a single column. On the off-diagonals are bivariate correlation plots, which help us get a sense\n# of how much independence there is between columns.\naxes = sns.pairplot(train[gp].dropna(), diag_kind='kde',\n                    markers='.', dropna=True)\n\nplt.suptitle('Correlations Among Tax-Related Columns', fontsize=18, y=1.05)\nplt.show()","cell_type":"code","metadata":{"_uuid":"a6bb7f4b333fac8944a35f48e212328d84fb7c5f","_cell_guid":"75e3d2b1-8631-45e9-9bbd-cdb40014a942"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Independent Component Analysis\nOne way of handling these inter-correlations is to \"decompose\" the dataframe into smaller units. We could do this for small groups of values to get a stronger \"tax\" feature, for example. Or, we can do it to the entire dataframe. \n\nLet's use Independent Components Analysis to see what we can find from all the columns in the dataset. ","metadata":{"_uuid":"4aa932c7361ab3c4166f85a79262b348fd1a3be8","_cell_guid":"05a93947-d6a8-4464-bda4-aa10226fe93f"}},{"source":"# Since there are a lot of NaN values, we need to fill them in, or else the algorithm\n# has to drop the data record entirely.\nfrom sklearn.preprocessing import Imputer\n# We only want to compute components for non-ID-like, continuous variables\nica_cols = [col for col in train.columns if (train[col].dtype == 'float64') & (not 'id' in col)]\nica_cols.remove('logerror')\nimp = Imputer(missing_values='NaN', strategy='median')\nimp_qcols = imp.fit_transform(X=train[ica_cols])\n\n\n# Now, let's import FastICA and set our number of components. This is somewhat arbitrary, and\n# the number chosen *will* affect the components output. Let's try 5.\nfrom sklearn.decomposition import FastICA\nncomp = 5\nica = FastICA(n_components=ncomp)\nica_data = ica.fit_transform(imp_qcols)\n\n# Now, let's re-format the data for viewing\ncomp_names = ['component_{}'.format(x+1) for x in np.arange(ncomp)]\nica_dx = pd.DataFrame(data = ica.mixing_, \n                      columns=comp_names, \n                      index=ica_cols)\nica_df = pd.DataFrame(ica_data, \n                      columns=comp_names, \n                      index=train.index)\n\n# Let's look at the relative weights of each column for each component\nica_dx.astype(int)","cell_type":"code","metadata":{"_uuid":"2dbb9992ef6e575ae07ec030cb27b1110754d2b7","_cell_guid":"8c39d750-0eb7-41fb-aab3-fdf5e1db915b","_execution_state":"idle"},"execution_count":null,"outputs":[]},{"source":"# Now let's check whether these columns are truly \"independent\" -- and whether they predict logerror\nsns.pairplot(ica_df.join(train['logerror'], how='left'),\n             diag_kind='kde', markers='.')\nplt.show()","cell_type":"code","metadata":{"_uuid":"04a8b034cbdc17e65db7813337cd26715e2a21c7","_cell_guid":"051ef56f-3077-4a2f-852b-f47c319257e6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## See What Columns Predict Membership in \"Outlier\" Class \n\nAnother way we can approach this problem is to to try and target and reduce \"outlier\" properties, since these are disproportionately going to throw off the Zestimate. \n\nWhat we're going to do below is:\n- Define an outlier group (cut-off of abs(logerror) > 0.25)\n- For each float-like column, we'll test whether there is a difference in the mean of the distribution between the outlier and insider groups.","metadata":{"_uuid":"94707fa28d33a78a6325a08899348a8a3dfc8ba9","_cell_guid":"984a4cba-b09c-4a82-9b51-e3c49f539477"}},{"source":"from scipy.stats import ttest_ind\n\n# Distinguish outliers from other entries\nouts = train.logerror.abs() > 0.25\ninside = train.loc[outs==False]\noutside = train.loc[outs]\n\n# Initialize results data frame\ninsideout = pd.DataFrame(columns=['tstat', 'pval', 'n_inside', 'n_outside'])\n\n# Loop through cols and get difference stats for in / out\nfloat_cols = [col for col in train.columns if (train[col].dtype == 'float64') & (not 'id' in col)]\nfor col in float_cols:\n    try:\n        s = pd.Series(name=col)\n        s['tstat'], s['pval'] = ttest_ind(inside[col], outside[col], nan_policy='omit')\n        s['n_inside'] = inside[col].notnull().sum()\n        s['n_outside'] = outside[col].notnull().sum()\n        insideout = insideout.append(s)\n    except TypeError as exc:\n        print('{}: {}'.format(col, exc))\n        \n# Let's look at columns that seem to be different, but only those that are populated by outlier data\n(insideout.loc[(insideout.tstat.abs() > 5) & (insideout.n_outside > 3000)]\n    .sort_values(by='tstat'))","cell_type":"code","metadata":{"_uuid":"43751887b6c51dc66bf1cbed23ba30545faf3398","_cell_guid":"7a3319d0-ed70-4df9-b8e2-2eb2896aa3a0"},"execution_count":null,"outputs":[]},{"source":"# Let's take a look at a few of these:\nmetric = 'yearbuilt'\n\ndef plot_boxplot_column(col):    \n    fig, ax = plt.subplots(1,1)\n    train.boxplot(column=col, by=outs.values, ax=ax)\n\n    # Have to play around a bit with the legend and title...\n    fig.suptitle('{} by Outlier Designation'.format(metric))\n    ax.set_title('')\n    ax.set_xlabel('Outlier?')\n    ax.set_ylabel(metric)\n    #ax.legend().set_visible(False)    \n    \n    return  ax \n\nax = plot_boxplot_column(metric)\n#ax.set_ylim([0, 10])\nplt.show()","cell_type":"code","metadata":{"_uuid":"672ca90607b431753d396605da702b6292d5ad47","_cell_guid":"539b6b1a-cd05-428a-8700-8c99717d610b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finally, let's bring this home.","metadata":{"_uuid":"079af0883e6d61029a4129cbc89947c238510468","_cell_guid":"9bc4ff7c-0dcc-4fdf-a5d3-d5b6fc6f1c9b"}},{"source":"# Set up our training data\ncat_cols = [col for col in train.columns if ('id' in col) or (col in ['fips'])]\nfloat_cols = [col for col in train.columns if (train[col].dtype == 'float64') \n                                              & (not col in cat_cols)\n                                              & (not col in ['latitude', 'longitude', \n                                                             'censustractandblock', 'logerror'])]\nX_train = train[float_cols].fillna(train.median()).values\ny_train = train['logerror'].values\nfeature_names = train[float_cols].columns.values\n\nfrom sklearn.svm import SVR\nmodel = SVR(C=1)\nmodel.fit(X_train, y_train)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"predictions = model.predict(X_train)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"# Get a baseline, no seasonal differnces\npredictions = pd.Series(model.predict(X_train), index=train.index)\n\n# The output file should have 18232 prediction rows, with six predicted values each\npredict_df = pd.DataFrame(data=s, columns=['201610'])\nfor col in ['201611','201612','201710','201711','201712']:\n    predict_df[col] = predict_df['201610']\n    \npredict_df.to_csv('submission.csv')","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"source":"import os\nos.listdir()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"\n\n# Import Random Forest Regressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor(n_estimators=10, max_depth=5, \n                            max_features=0.3, criterion='mae', \n                            n_jobs=-1, random_state=0)\nmodel.fit(X_train, y_train)\n\n## Get importance of each feature, and order it\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20] # [::-1] reverses the order, [:20] takes top 20\n\n# Plot it!\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feature_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()\nimport xgboost as xgb\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\ndtrain = xgb.DMatrix(X_train, y_train, feature_names=feature_names)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","cell_type":"code","metadata":{"_uuid":"1db95c4582b49b7d282b072b7a414292dd61cc82","_cell_guid":"22d1ab63-2de0-45d0-a099-a8f921cf987d","collapsed":true},"execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"}}}