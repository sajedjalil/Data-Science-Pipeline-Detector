{"cells":[{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"59359d0c-e3ed-4d26-a5bd-fe2817ba428b","_uuid":"c7ba28f460b0218169207dbccf3d9d43ee3382f1"},"source":"# Parameters\nFUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n\nXGB_WEIGHT = 0.6200\nBASELINE_WEIGHT = 0.0100\nOLS_WEIGHT = 0.0620\nNN_WEIGHT = 0.0800\n\nXGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"089dd815-a561-4638-aafc-acbfc24eab33","_kg_hide-input":false,"_uuid":"b6e0e6cd1e125f5b42a5804af49857475be2c313","_kg_hide-output":false},"source":"#Import Lib#\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as  xgb \nimport random \nimport lightgbm as lgb\nimport datetime as dt \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"000dc011-ac3e-4e00-bac8-a747459cad03","_uuid":"51231327470cf646b219212a2c7c86431a164459"},"source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"122fd6ec-bdd8-4cb3-8fe6-3591af1e63e0","_uuid":"67e139fb63551317191e820c5dd11147713ceb9e"},"source":"#Load data#\ntrain_2017 = pd.read_csv('../input/train_2017.csv',parse_dates = [\"transactiondate\"])\ntrain_2016 = pd.read_csv('../input/train_2016_v2.csv',parse_dates = [\"transactiondate\"])\nframes = [train_2016,train_2017]\ntrain = pd.concat(frames)\nproperties = pd.read_csv('../input/properties_2017.csv')\ntest = pd.read_csv('../input/sample_submission.csv') \ntest = test.rename(columns = {'ParcelId': 'parcelid'})","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"615f3565-2e59-48a4-a827-4e75037139e6","_uuid":"244b955abfc8800b03221a317a6d2d8f63aaa128"},"source":"\nprint(\"Training Size:\" + str(train.shape))\nprint(\"Property Size:\" + str(properties.shape))\nprint(\"Sample Size:\" + str(test.shape))\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"a92d8e71-a68f-4c28-961e-282913b9c1b1","_uuid":"56a90c695cb156768db366e73fd420dd677510e1"},"source":"################\n################\n##  LightGBM  ##\n################\n################\n\n# This section is (I think) originally derived from SIDHARTH's script:\n#   https://www.kaggle.com/sidharthkumar/trying-lightgbm\n# which was forked and tuned by Yuqing Xue:\n#   https://www.kaggle.com/yuqingxue/lightgbm-85-97\n# and updated by Andy Harless:\n#   https://www.kaggle.com/aharless/lightgbm-with-outliers-remaining\n# and a lot of additional changes have happened since then","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9bfab682-16ed-45ce-9740-3b3a7fdd5853","_uuid":"15e5c227e2b5445e6f0cd4a48a99e237ad7ebae7"},"source":"##### PROCESS DATA FOR LIGHTGBM\nprint( \"\\nProcessing data for LightGBM ...\" )\nfor c, dtype in zip(properties.columns, properties.dtypes):\n    if dtype == np.float64:\n        properties[c] = properties[c].astype(np.float32)\n        \ndf_train = train.merge(properties, how='left', on='parcelid')\ndf_train.fillna(df_train.median(),inplace = True)\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3b338de8-26f6-4f85-997f-fb1586ee85bb","_uuid":"ace81299ffd375170aa55e9a2baf604a5c802b9d"},"source":"train_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n\ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label=y_train)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"dbb7fa3e-9a18-4fb5-95bd-0450cb349546","_uuid":"28b136e8e2bbdfdb11e46e46c8c70e9cbadbd2a7"},"source":"##### RUN LIGHTGBM\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"ab87ad3e-2f16-48a4-8ffb-15987d057299","_uuid":"19de39c267a3f9750e5e797a27f8c6570912c8e9"},"source":"print(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"b0ba347a-57ea-4846-935a-16e7b264454a","_uuid":"87b255f4ed0782ea79c3919a3524a2302f4cbcaf"},"source":"del d_train; gc.collect()\ndel x_train; gc.collect()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"8cbbefed-81a0-44d7-b8e3-f3b1b8b32f24","_uuid":"314cb3cdd7cefe40d1808adc0cedee41066f029c"},"source":"print(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('../input/sample_submission.csv')\nprint(\"   ...\")\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\ndf_test = sample.merge(properties, on='parcelid', how='left')\nprint(\"   ...\")\ndel sample, properties; gc.collect()\nprint(\"   ...\")\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"5d6c0c7e-fd83-4db7-930f-92ed73e73175","_uuid":"00dadb3fdb3ccca036e203d124c238964e88c97b"},"source":"################\n################\n##  XGBoost   ##\n################\n################\n\n# This section is (I think) originally derived from Infinite Wing's script:\n#   https://www.kaggle.com/infinitewing/xgboost-without-outliers-lb-0-06463\n# inspired by this thread:\n#   https://www.kaggle.com/c/zillow-prize-1/discussion/33710\n# but the code has gone through a lot of changes since then","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"69b8be8a-0eb6-4f72-8821-e7f0804d0d75","_uuid":"37393d526e1960165deacdce966a0e4b1307b4ed"},"source":"print( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('../input/properties_2017.csv')","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"a4691c8e-2504-4c8e-9d6d-ce136135c698","_uuid":"5eacc94634fd94749267c2cf610fbe6baaa271d5"},"source":"##### PROCESS DATA FOR XGBOOST\n\nprint( \"\\nProcessing data for XGBoost ...\")\nfor c in properties.columns:\n    properties[c]=properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n\ntrain_df = train.merge(properties, how='left', on='parcelid')\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis=1)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"e51653bd-6a34-40da-bf0d-57285ef3bd4c","_uuid":"d99966f9fa7b583c097701fc2d3e2997a33abad8"},"source":"print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"75f291f6-4aec-4b15-9580-5d094a65c4cf","_uuid":"7ec10d397fba70b0abca77439e0bf96c5d536c3f"},"source":"# drop out ouliers\ntrain_df=train_df[ train_df.logerror > -0.4 ]\ntrain_df=train_df[ train_df.logerror < 0.419 ]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"7b4c743d-2ad8-4e15-aed4-2e97d1a7fff0","_uuid":"bf03461e5720946c0923279c214d0917a9721abf"},"source":"##### RUN XGBOOST\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"420e4927-80a6-4404-b523-b644508e4ce9","_uuid":"fa86ec195132779df44b228ca63a3f8f051d1910"},"source":"dtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 250\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\n# train model\nprint( \"\\nTraining XGBoost ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost ...\")\nxgb_pred1 = model.predict(dtest)\n\nprint( \"\\nFirst XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred1).head() )","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"f855ddf1-47ae-4469-bc76-e756f2f50b65","_uuid":"563840510539bf1118d65537f7b5edad64fcbb90"},"source":"print(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\n\nnum_boost_rounds = 150\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head() )","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"72ee3a29-bfa9-4b82-91f6-cc312974f256","_uuid":"951e7cefc58a4d0d3cd90c5cd831c942bc68a784"},"source":"##### COMBINE XGBOOST RESULTS\nxgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n#xgb_pred = xgb_pred1\n\nprint( \"\\nCombined XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred).head() )\n\ndel train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"4235c674-f321-489d-9272-655e63e1b387","_uuid":"cf49da3a167923796bb34ca0e4b5f46b1e8815c3"},"source":"######################\n######################\n##  Neural Network  ##\n######################\n######################\n\n# Neural network copied from this script:\n#   https://www.kaggle.com/aharless/keras-neural-network-lb-06492 (version 20)\n# which was built on the skeleton in this notebook:\n#   https://www.kaggle.com/prasunmishra/ann-using-keras","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"d9190be8-66fb-4399-b5c1-52075b7a8f82","_uuid":"f4aa155f4370419dfbaf51e551ccd387f1f825b6"},"source":"# Read in data for neural network\nprint( \"\\n\\nProcessing data for Neural Network ...\")\nprint('\\nLoading train, prop and sample data...')\ntrain_2017 = pd.read_csv('../input/train_2017.csv',parse_dates = [\"transactiondate\"])\ntrain_2016 = pd.read_csv('../input/train_2016_v2.csv',parse_dates = [\"transactiondate\"])\nframes = [train_2016,train_2017]\ntrain = pd.concat(frames)\nprop = pd.read_csv('../input/properties_2017.csv')\nsample = pd.read_csv('../input/sample_submission.csv')","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"cc6894a2-3f7f-42fa-afd1-380e54806138","_uuid":"8a872ef02a159d31f5749f0c6ed8691666895c9b"},"source":"print('Fitting Label Encoder on properties...')\nfor c in prop.columns:\n    prop[c]=prop[c].fillna(-1)\n    if prop[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(prop[c].values))\n        prop[c] = lbl.transform(list(prop[c].values))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"fc5753e9-ecb9-4a31-b1ed-75ce26888c50","_uuid":"0e5eec62940c8a2f204d8f955f928466ae041156"},"source":"print('Creating training set...')\ndf_train = train.merge(prop, how='left', on='parcelid')\n\ndf_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\ndf_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\ndf_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\ndf_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"704a6ca7-d588-46e3-a811-821a78f61506","_uuid":"8347b3f8fbf17f770377376478475ad389ab696c"},"source":"print('Filling NA/NaN values...' )\ndf_train.fillna(-1.0)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"d7dec468-bdd2-45d8-88c2-c5f1c1887f24","_uuid":"9332ff5fe65007982ab753e4daa79494814833e6"},"source":"print('Creating x_train and y_train from df_train...' )\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train[\"logerror\"]","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"1c43e8c3-a1ff-4d4d-9b54-d69b20e060ba","_uuid":"c60456919dc9b6eaf6a61405c118bdc68062bd95"},"source":"y_mean = np.mean(y_train)\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c38d7149-a340-41ca-ab37-86b767fa8b42","_uuid":"abc640f85773db8dc72fd5b2a974edaa23e0c3a6"},"source":"print('Creating df_test...')\nsample['parcelid'] = sample['ParcelId']\n\nprint(\"Merging Sample with property data...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\ndf_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\ndf_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\ndf_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\ndf_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\ndf_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \nx_test = df_test[train_columns]","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"948ca1df-0124-4da3-8214-aeb5e9469955","_uuid":"60d9d58a1e733a2b0b55702236deb1efe2ba4bab"},"source":"print('Shape of x_test:', x_test.shape)\nprint(\"Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"168971f7-8743-488b-a401-f21b8b8f00aa","_uuid":"3b10a13f8af48edb7fb1ee05c7418f1ac85bb6ed"},"source":"## Preprocessing\nprint(\"\\nPreprocessing neural network data...\")\nimputer= Imputer()\nimputer.fit(x_train.iloc[:, :])\nx_train = imputer.transform(x_train.iloc[:, :])\nimputer.fit(x_test.iloc[:, :])\nx_test = imputer.transform(x_test.iloc[:, :])\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nlen_x=int(x_train.shape[1])\nprint(\"len_x is:\",len_x)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"d642fa8c-d96d-4190-83ea-cb11b8054f36","_uuid":"56a6b773a1d7268427dc2580967e63317954f5a1"},"source":"print(\"\\nSetting up neural network model...\")\nnn = Sequential()\nnn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = len_x))\nnn.add(PReLU())\nnn.add(Dropout(.4))\nnn.add(Dense(units = 160 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(units = 64 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.5))\nnn.add(Dense(units = 26, kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(1, kernel_initializer='normal'))\nnn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"7b85de4f-dcbf-4307-b62e-8f442eb6c957","_uuid":"9b372280c49a20e7a893de7f73571a999551fc3d"},"source":"print(\"\\nFitting neural network model...\")\nnn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 70, verbose=2)\n\nprint(\"\\nPredicting with neural network model...\")\n#print(\"x_test.shape:\",x_test.shape)\ny_pred_ann = nn.predict(x_test)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"a2d55401-f178-435f-96d8-168d15ede8c2","_uuid":"2bf3a3eef0732f0dcf9aaa803aa1ffbe32e5bf9d"},"source":"#transaction dateprint( \"\\nPreparing results for write...\" )\nnn_pred = y_pred_ann.flatten()\nprint( \"Type of nn_pred is \", type(nn_pred) )\nprint( \"Shape of nn_pred is \", nn_pred.shape )","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"97a00bb8-b336-4297-b3ba-0d7fee36a004","_uuid":"65a03f030ec4e9bf410d69cb22313ba6a40af316"},"source":"# Cleanup\ndel train\ndel prop\ndel sample\ndel x_train\ndel x_test\ndel df_train\ndel df_test\ndel y_pred_ann\ngc.collect()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"78678ebb-8cdf-4446-8532-04fb2080c59e","_uuid":"7fa6b148277b981ddeb6e5b1e5036d4fc065460b"},"source":"################\n################\n##    OLS     ##\n################\n################\n\n# This section is derived from the1owl's notebook:\n#    https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n# which I (Andy Harless) updated and made into a script:\n#    https://www.kaggle.com/aharless/updated-script-version-of-the1owl-s-basic-ols","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"bf58223c-cd65-4a59-a9a7-b190e93b6c60","_uuid":"2f28ac1a7228c79d4c9db47e81036040ecc46f88"},"source":"np.random.seed(17)\nrandom.seed(17)\n\nprint( \"\\n\\nProcessing data for OLS ...\")\n\ntrain_2017 = pd.read_csv('../input/train_2017.csv',parse_dates = [\"transactiondate\"])\ntrain_2016 = pd.read_csv('../input/train_2016_v2.csv',parse_dates = [\"transactiondate\"])\nframes = [train_2016,train_2017]\ntrain = pd.concat(frames)\nproperties = pd.read_csv(\"../input/properties_2017.csv\")\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nprint(len(train),len(properties),len(submission))\n\ndef get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"31eeaa32-8df8-47e4-9ad2-f1688fef4e32","_uuid":"95a1cfe6f309baee38e3958d39ec44b8e78d98ec"},"source":"def MAE(y, ypred):\n    #logerror=log(Zestimate)âˆ’log(SalePrice)\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"5315a8cf-62b1-40d6-9ffa-2fc02dbad6fa","_uuid":"d30170e249cbb5dba5aa7180fc9b69fa3c60b079"},"source":"train = pd.merge(train, properties, how='left', on='parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3cc5d446-7abd-4f30-a5f2-d6bb65b97085","_uuid":"0f7e1130bb5d10a4f87e5df6f3af0117096c538e"},"source":"train = get_features(train[col])\ntest['transactiondate'] = '2016-01-01' #should use the most common training date\ntest = get_features(test[col])","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"cd6ef9d1-8ad6-4b8a-959f-6944d85ccaba","_uuid":"4c45d62200d79fafefa3a8e1a984e92ca93384b1"},"source":"print(\"\\nFitting OLS...\")\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"594423fe-364b-42eb-9901-0b5bf20cb65e","_uuid":"be59f52eda04e92a8a1d112d75e586cf6210f89e"},"source":"########################\n########################\n##  Combine and Save  ##\n########################\n########################\n\n\n##### COMBINE PREDICTIONS\n\nprint( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\nlgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \nlgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\nnn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\npred0 = 0\npred0 += xgb_weight0*xgb_pred\npred0 += baseline_weight0*BASELINE_PRED\npred0 += lgb_weight0*p_test\npred0 += nn_weight0*nn_pred","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"922cd2ae-d183-4c67-99ae-f2a953bc0a73","_uuid":"191e7225e95be0d4821eb44203d95c110a76353c"},"source":"print( \"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n    \nprint( \"\\nWriting results to disk ...\" )\nsubmission.to_csv('1003_try.csv', index=False , float_format='%.4f')\nprint( \"\\nFinished ...\")","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9549843d-71db-43b2-ba42-26f0b49adeaa","_uuid":"cc4bd29919e6abbee25b8e1393b06c0674c96553"},"source":"train_c = train.copy()\ntrain_c['trans_month'] = train_c['transactiondate'].dt.month\ncnt_srs = train_c['trans_month'].value_counts()\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values,alpha = 0.8)\nplt.xticks(rotation='vertical')\nplt.xlabel('Month of transaction', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c541f93b-31bd-48c1-8ef0-3a1427e38655","_uuid":"097d7ce2dc29636a6523fb4e4f758a02f1581ff4"},"source":"#check number of Nulls in property dataset\nmissing_df = properties.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name','missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] / properties.shape[0]\nmissing_filtered = missing_df.loc[missing_df['missing_ratio']>0.99]\n#eliminate columns that has too many null\neliminate_list = missing_filtered['column_name'].values\nproperties = properties.drop(eliminate_list,axis=1)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"adec7fbd-5c83-4b26-8d4d-da0620134146","_uuid":"a8d758440ba11cfee982dca1b54c6a7946e9a8cc"},"source":"# for c in properties.columns.values:\n#     plt.figure(figsize=(12,6))\n#     sns.countplot(x=c, data=properties)\n#     plt.ylabel('Count', fontsize=12)\n#     plt.xlabel(c, fontsize=12)\n#     plt.xticks(rotation='vertical')\n#     plt.show()\nplt.figure(figsize=(12,6))\nsns.countplot(x='finishedsquarefeet12', data=properties)\nplt.ylabel('Count', fontsize=12)\nplt.xlabel(\"finishedsquarefeet12\", fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"430f81c5-97b4-470b-ae16-822c92b80a6c","_uuid":"676adb9b7235f4070a0c04bed2d455a345ba447c"},"source":"print('t')","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"845c41db-b3dc-4ed5-8387-ab93317af0c0","_uuid":"660dd272479132bb395cb9edb7abaf143291ce26"},"source":"#convert datatype\ndef convert_datatype(dataframe):\n    for c, dtype in zip(dataframe.columns, dataframe.dtypes):\n        if dtype == np.float64:\n            dataframe[c] = dataframe[c].astype(np.float32)\n        if dtype == np.int64:\n            dataframe[c] = dataframe[c].astype(np.int32)\n            \nconvert_datatype(properties)\nconvert_datatype(test)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"ed303344-6616-462e-aaba-d05113f51091","_uuid":"0d05968943886e43d46dff16e4630a073db3a521"},"source":"#living area proportions \nproperties['living_area_prop'] = properties['calculatedfinishedsquarefeet'] / \\\nproperties['lotsizesquarefeet']\n#tax value ratio\nproperties['value_ratio'] = properties['taxvaluedollarcnt'] / properties['taxamount']\n#tax value proportions\nproperties['value_prop'] = properties['structuretaxvaluedollarcnt'] /\\\nproperties['landtaxvaluedollarcnt']","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"10ae842e-8edf-4710-b1c7-10eab0b0e8b8","_uuid":"c64b9bec5ec0c274ddccc0232ab7cb7a4578d9f7"},"source":"#mergeing datasets\ndf_train = train.merge(properties, how='left', on='parcelid')\ndf_test = test.merge(properties, how='left', on='parcelid')","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"86c0e104-1f60-4cdd-bff1-db0c3b4c92a9","_uuid":"3447824bed10701c5d6b78bb4c5807eb911224c2"},"source":"print(df_train.shape,train.shape,properties.shape)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"3c8c8fdb-98d5-4828-bfdb-c0cf22882de4","_uuid":"07e3cfd2517060eac60b6c5d04f308fb5a2265ca"},"source":"#change missing values into 0\n#change categorical into to numerical\n\ndef convert_label(dataframe):\n    lbI = LabelEncoder()\n    for c in dataframe.columns:\n        dataframe[c] = dataframe[c].fillna(0)\n        if dataframe[c].dtype == 'object':\n            lbI.fit(list(dataframe[c].values))\n            dataframe[c] = lbI.transform(list(dataframe[c].values))\n\nconvert_label(df_train)\nconvert_label(df_test)\n\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"679fb70b-88f5-4e24-a0a8-27f4fc1a99cb","_uuid":"c6a75c526a242cd341267a7e8d5f59dc50b8c801"},"source":"#re-arranging \nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', ], axis=1)\nx_test = df_test.drop(['parcelid', 'propertyzoningdesc',\n                       'propertycountylandusecode', '201610', '201611', \n                       '201612', '201710', '201711', '201712'], axis = 1) \nx_train = x_train.values\ny_train = df_train['logerror'].values","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"745f394a-1643-41c3-85d5-57d631b273cc","_uuid":"40b057a28ac06d339c44e47a5b93df11ab330826"},"source":"from datetime import datetime\n\n","outputs":[]}],"metadata":{"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py","name":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"nbformat":4}