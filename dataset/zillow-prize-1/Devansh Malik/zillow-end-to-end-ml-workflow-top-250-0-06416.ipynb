{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n**If you use parts of this notebook in your scripts/notebooks, giving some kind of credit would be highly appreciated :) You can for instance link back to this notebook or upvote the notebook. Thanks!**\n","metadata":{}},{"cell_type":"markdown","source":"\n\nThe goal of this notebook is to create a full machine learning pipeline including exploratory data analysis on the Zillow housing dataset. This includes initial data exploration, handling missing data, feature engineering, picking & tuning an algorithm and any other steps in the data science workflow. \n\n**The project aims to create a fully automated ML workflow with production level modular code including the use of [Scikit-Learn's `Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).** This approach has the following benefits: \n- Makes workflow much easier to read and understand \n- Enforces cleaner, modular code that is production ready \n- Easily test models with different preprocessing steps and parameters\n- Allows reuse of code in future projects \n\n\nThe work in this notebook is inspired by other popular Kaggle notebooks including: \n- [Titantic Data Science Solutions Notebook](https://www.kaggle.com/startupsci/titanic-data-science-solutions) by **Manav Sehgal**.\n- [A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset) by **Julien Cohen-Solal** \n- [Great feature engineering in a python script](https://www.kaggle.com/humananalog/xgboost-lasso) by **Human Analog**\n- [Stacked Regressions to predict House Prices](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by **Serigne**\n- [Great Feature Engineering on Zillow Dataset](https://www.kaggle.com/deepakk92/notebook211fdc91df/notebook) by **Deepak**\n- [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.ca/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291) by **Aurélien Géron**\n- [Gradient Boosting and Inference Tutorials](https://github.com/numeristical/resources/tree/master/GBIP) by **Brian Lucena**\n\n**Note: This notebook is EXTREMELY long and thus, feel free to jump to sections / models of your interest if you're looking for specific topics.**","metadata":{}},{"cell_type":"markdown","source":"#### **The best performing model from this analysis yielded a Public Score of 0.06416 and a Private Score of 0.07497 which are within Top 250 and Top 60 respectively in the world out of over 3700 submissions.** \n\nUnfortunately, since the competition ended a few years ago (I'm writing this notebook in 2022), no models could be submitted towards the final leaderboard score. ","metadata":{}},{"cell_type":"code","source":"# Import necessary librairies\nimport gc \nimport numpy as np # linear algebra\nfrom numpy import hstack\nfrom numpy import array\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n# Definitions\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\npd.set_option('display.max_columns', 100)    #Display upto 100 columns \npd.set_option('display.max_rows', 100) \n%matplotlib inline\n\n# Check the files available in the directory\nimport os\nfor dirname, _, filenames in os.walk('../input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')        ","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:37:28.685753Z","iopub.execute_input":"2022-02-15T20:37:28.686164Z","iopub.status.idle":"2022-02-15T20:37:28.702408Z","shell.execute_reply.started":"2022-02-15T20:37:28.686129Z","shell.execute_reply":"2022-02-15T20:37:28.701667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scikit-Learn 1.0 or higher is required for this project due to the introduction of Feature Names Support among other [updates](https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_0_0.html). These updates allow easier use of Sklearn `Pipeline` when creating an end-to-end machine learning pipeline. ","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn==1.0.0\n\n# Confirm sklearn version\nfrom sklearn import __version__\n__version__","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:37:28.703587Z","iopub.execute_input":"2022-02-15T20:37:28.703987Z","iopub.status.idle":"2022-02-15T20:37:35.908467Z","shell.execute_reply.started":"2022-02-15T20:37:28.703955Z","shell.execute_reply":"2022-02-15T20:37:35.907643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Zillow Datasets\n\nDue to the setup of the Zillow competition, the training dataset is present in two different CSVs, one for each of 2016 and 2017 properties along with a corresponding CSV for each that contains the target variable. \n\nZillow is asking us to predict the log-error between their Zestimate and the actual sale price, given all the features of a home. The log error is defined as\n`logerror = log(Zestimate) - log(SalePrice)` ","metadata":{}},{"cell_type":"code","source":"import os\n\nHOUSING_PATH = os.path.join(\"..\", \"input\", \"zillow-prize-1\")\nPROPERTIES_2016 = 'properties_2016.csv'\nPROPERTIES_2017 = 'properties_2017.csv'\nTRAIN_2016 = 'train_2016_v2.csv'\nTRAIN_2017 = 'train_2017.csv'\n    \ndef load_housing_data(housing_path=HOUSING_PATH):\n    properties_2016 = pd.read_csv(os.path.join(housing_path, PROPERTIES_2016))\n    properties_2017 = pd.read_csv(os.path.join(housing_path, PROPERTIES_2017))\n    train_2016 = pd.read_csv(os.path.join(housing_path, TRAIN_2016))\n    train_2017 = pd.read_csv(os.path.join(housing_path, TRAIN_2017))\n\n    # Left join will ignore all properties that do not have a logerror (target variable) associated with them\n    train_2016 = pd.merge(train_2016, properties_2016, how = 'left', on = 'parcelid')\n    train_2017 = pd.merge(train_2017, properties_2017, how = 'left', on = 'parcelid')\n    \n    # Union data for 2016 and 2017 into one dataframe\n    all_properties = pd.concat([properties_2016, properties_2017], ignore_index=True)\n    all_training = pd.concat([train_2016, train_2017], ignore_index=True)\n    return all_properties, all_training","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:34:57.035364Z","iopub.execute_input":"2022-02-16T04:34:57.03582Z","iopub.status.idle":"2022-02-16T04:34:57.045212Z","shell.execute_reply.started":"2022-02-16T04:34:57.035783Z","shell.execute_reply":"2022-02-16T04:34:57.044145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_properties, housing = load_housing_data()\nhousing.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:46:27.224593Z","iopub.execute_input":"2022-02-16T04:46:27.225142Z","iopub.status.idle":"2022-02-16T04:47:07.29054Z","shell.execute_reply.started":"2022-02-16T04:46:27.225108Z","shell.execute_reply":"2022-02-16T04:47:07.289793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total Properties Shape: {}\".format(all_properties.shape))\nprint(\"-\"*50)\nhousing.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:14.617197Z","iopub.execute_input":"2022-02-15T20:38:14.617551Z","iopub.status.idle":"2022-02-15T20:38:14.73463Z","shell.execute_reply.started":"2022-02-15T20:38:14.617518Z","shell.execute_reply":"2022-02-15T20:38:14.733627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations based on the information above:** \n- Total number of properties is 5.97 million, much smaller than the training dataset with transactions information for 335k entries\n- Variable types distribution: 53 Float64, 6 Object (Categorical) and 1 Integer (`ParcelID`)\n- Lots of variables contain missing data - these features are candidates for **Imputation or Dropping**. ","metadata":{}},{"cell_type":"code","source":"# Check for and drop duplicates in training dataset\ndef check_duplicates(housing): \n    idsUnique = len(housing[['parcelid', 'transactiondate']].value_counts())\n    idsTotal = housing.shape[0]\n    idsDupli = idsTotal - idsUnique\n    print(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")\n    \ndef drop_duplicates(housing):\n    # Drop all duplicate entries which have the same parcelID and Transaction Date\n    print(\"Dropping all duplicates based on parcelid and transactiondate...\")\n    return housing.drop_duplicates(subset=['parcelid', 'transactiondate'], keep='last', ignore_index=True) ","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:14.736239Z","iopub.execute_input":"2022-02-15T20:38:14.736694Z","iopub.status.idle":"2022-02-15T20:38:14.74384Z","shell.execute_reply.started":"2022-02-15T20:38:14.736647Z","shell.execute_reply":"2022-02-15T20:38:14.743058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for and drop duplicates\ncheck_duplicates(housing)\nhousing = drop_duplicates(housing)\n\n# Validate \ncheck_duplicates(housing)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:14.745181Z","iopub.execute_input":"2022-02-15T20:38:14.74552Z","iopub.status.idle":"2022-02-15T20:38:15.092962Z","shell.execute_reply.started":"2022-02-15T20:38:14.745491Z","shell.execute_reply":"2022-02-15T20:38:15.091894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target Variable \n\n`logerror` is the variable we need to predict. So let's do some analysis on this variable first. ","metadata":{}},{"cell_type":"code","source":"y = housing.logerror","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:15.095358Z","iopub.execute_input":"2022-02-15T20:38:15.095655Z","iopub.status.idle":"2022-02-15T20:38:15.100446Z","shell.execute_reply.started":"2022-02-15T20:38:15.095608Z","shell.execute_reply":"2022-02-15T20:38:15.099217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.hist(bins=100, figsize=(8,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:15.102414Z","iopub.execute_input":"2022-02-15T20:38:15.102845Z","iopub.status.idle":"2022-02-15T20:38:15.473205Z","shell.execute_reply.started":"2022-02-15T20:38:15.102802Z","shell.execute_reply":"2022-02-15T20:38:15.472367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:15.474472Z","iopub.execute_input":"2022-02-15T20:38:15.47508Z","iopub.status.idle":"2022-02-15T20:38:15.491523Z","shell.execute_reply.started":"2022-02-15T20:38:15.475034Z","shell.execute_reply":"2022-02-15T20:38:15.490526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('LogError distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:15.492885Z","iopub.execute_input":"2022-02-15T20:38:15.493303Z","iopub.status.idle":"2022-02-15T20:38:17.16696Z","shell.execute_reply.started":"2022-02-15T20:38:15.49326Z","shell.execute_reply":"2022-02-15T20:38:17.166233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable is skewed and contains outliers. There are two options to treat this: \n- For most cases, since (linear) models love normally distributed data, we would have transformed this variable through a log transformation to make it more normally distributed. \n- **However, given the unique nature of this specific problem where we are trying to predict the errors that the Zillow model makes when estimating house values, it makes more sense to ignore the cases where the Zillow model is horribly wrong - rather than try to learn the unique cases where it is way off.**\n\n","metadata":{}},{"cell_type":"markdown","source":"### Dropping Outliers \n\nSince the data is mostly normally distributed outside of the outliers, we will drop all values that are more than 2.5 standard deviations away from the mean. For reference, for the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean.","metadata":{}},{"cell_type":"code","source":"highest_thres = y.mean() + 2.5*y.std()\nlowest_thres = y.mean() - 2.5*y.std()\nprint(\"Highest allowed\",highest_thres)\nprint(\"Lowest allowed\", lowest_thres)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:17.168064Z","iopub.execute_input":"2022-02-15T20:38:17.168465Z","iopub.status.idle":"2022-02-15T20:38:17.1752Z","shell.execute_reply.started":"2022-02-15T20:38:17.168435Z","shell.execute_reply":"2022-02-15T20:38:17.174531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only the training set outliers will be dropped (not validation or testing set to ensure model performs well on outliers too)\ny = y[y > lowest_thres]\ny = y[y < highest_thres]\n\n# Update original Housing dataframe \nhousing = housing[housing.logerror > lowest_thres]\nhousing = housing[housing.logerror < highest_thres]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:17.176144Z","iopub.execute_input":"2022-02-15T20:38:17.176537Z","iopub.status.idle":"2022-02-15T20:38:17.260273Z","shell.execute_reply.started":"2022-02-15T20:38:17.176508Z","shell.execute_reply":"2022-02-15T20:38:17.259505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop rows containing either 75% or more NaN Values\npercent = 75.0 \nmin_count =  int(((100-percent)/100)*housing.shape[1] + 1)\nhousing = housing.dropna(axis=0, thresh=min_count)\nhousing.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:17.261346Z","iopub.execute_input":"2022-02-15T20:38:17.261931Z","iopub.status.idle":"2022-02-15T20:38:17.396425Z","shell.execute_reply.started":"2022-02-15T20:38:17.261879Z","shell.execute_reply":"2022-02-15T20:38:17.395585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.hist(bins=100, figsize=(8,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:17.397566Z","iopub.execute_input":"2022-02-15T20:38:17.397849Z","iopub.status.idle":"2022-02-15T20:38:17.769505Z","shell.execute_reply.started":"2022-02-15T20:38:17.397822Z","shell.execute_reply":"2022-02-15T20:38:17.768415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the new distribution \nsns.distplot(y , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('LogError distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:38:17.771074Z","iopub.execute_input":"2022-02-15T20:38:17.771385Z","iopub.status.idle":"2022-02-15T20:38:19.596826Z","shell.execute_reply.started":"2022-02-15T20:38:17.771353Z","shell.execute_reply":"2022-02-15T20:38:19.59588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The skew seems now corrected and the data appears better normally distributed.","metadata":{}},{"cell_type":"markdown","source":"# Create Test / Train Datasets \n\nTo avoid **data snooping** bias by overfitting to the test set, we need to create a test set, put it aside and never look at it. The most basic solution is to pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside. \n\n- **Issue with approach**: If you run the program again, it will generate a different test set! Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid.\n- **Possible Solution**: To have a stable train/test split even after updating the dataset, a common solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier).\n    - For example, you could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.\n\n**Since `parcelid` is a unique identifier for each instance in the housing dataset, we can use it to implement the hash strategy outlined above.**\n","metadata":{}},{"cell_type":"code","source":"from zlib import crc32\nfrom sklearn.model_selection import train_test_split \n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    \n    train_set = data.loc[~in_test_set]\n    test_set = data.loc[in_test_set]\n    \n    X_train = train_set.drop(\"logerror\", axis=1)\n    y_train = train_set[\"logerror\"].copy()\n    X_test = test_set.drop(\"logerror\", axis=1)\n    y_test = test_set[\"logerror\"].copy()\n    return X_train, X_test, y_train, y_test\n\nX_other, X_test, y_other, y_test = split_train_test_by_id(housing, 0.1, \"parcelid\")\nprint(f\"Other Dataset Shape: {X_other.shape}; Test Dataset Shape: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:47:54.819025Z","iopub.execute_input":"2022-02-16T04:47:54.819632Z","iopub.status.idle":"2022-02-16T04:47:55.383971Z","shell.execute_reply.started":"2022-02-16T04:47:54.819579Z","shell.execute_reply":"2022-02-16T04:47:55.383207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next step is to further split `X_train, y_train` into Training and Validation Datasets. This is because:\n- The models will be trained on the Training Set and compared to other models using the Validation Set. The Validation Set is used during the iterative process of model creation to validate the performance of the models being created. \n- On the other hand, the test set is kept hidden until the **final** step when the fully tuned machine learning algorithm is ready for deployment. ","metadata":{}},{"cell_type":"code","source":"# Split X_other into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_other, y_other, train_size=0.9, random_state=42)\nprint(f\"Training Dataset Shape: {X_train.shape}\")    # 81% of instances are in training \nprint(f\"Test Dataset Shape: {X_test.shape}\")         # 10% of instances are in test \nprint(f\"Validation Dataset Shape: {X_val.shape}\")    # 9% of instances are in validation ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:47:58.066669Z","iopub.execute_input":"2022-02-16T04:47:58.067191Z","iopub.status.idle":"2022-02-16T04:47:58.155502Z","shell.execute_reply.started":"2022-02-16T04:47:58.067156Z","shell.execute_reply":"2022-02-16T04:47:58.154393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear memory\ndel all_properties, housing; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:49:03.523957Z","iopub.execute_input":"2022-02-16T04:49:03.524517Z","iopub.status.idle":"2022-02-16T04:49:03.821083Z","shell.execute_reply.started":"2022-02-16T04:49:03.52448Z","shell.execute_reply":"2022-02-16T04:49:03.820239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing Pipelines\n\nAs observed earlier, the final training dataset is missing values for many variables. The next step is to proceed sequentially through features with missing values and **either drop or impute values** based on the information available followed by performing **feature engineering**.\n\n**Note: This is the LONGEST and MOST TEDIOUS section of the notebook where we create a Scikit-Learn Pipeline for every single preprocessing done on the data. Despite the tediousness in creating these individual pipelines, the final product pays dividends instantly as:** \n- You can reuse the modular code in future projects for preprocessing. \n- You can test different preprocessing combinations for each model easily to find the most optimized result\n- Your code is easier to understand for others and is easier to deploy in production setting\n- Finally, it forces you to stick to best coding practices in your data science projects\n\n### Feel free to skip to the next section (Full Data Preparation Pipeline) without delving into the details of the implementation for each pipeline as you will be able to observe their usage in later sections regardless.\n","metadata":{}},{"cell_type":"markdown","source":"### Missing Data","metadata":{"execution":{"iopub.status.busy":"2022-02-11T20:06:57.871659Z","iopub.execute_input":"2022-02-11T20:06:57.872375Z","iopub.status.idle":"2022-02-11T20:06:57.876913Z","shell.execute_reply.started":"2022-02-11T20:06:57.872302Z","shell.execute_reply":"2022-02-11T20:06:57.876065Z"}}},{"cell_type":"code","source":"all_data_na = (X_train.isnull().sum() / len(X_train)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data[:35]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-15T20:39:07.806024Z","iopub.execute_input":"2022-02-15T20:39:07.806386Z","iopub.status.idle":"2022-02-15T20:39:07.906863Z","shell.execute_reply.started":"2022-02-15T20:39:07.806353Z","shell.execute_reply":"2022-02-15T20:39:07.905893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Features with one unique value!!\")\nexclude_unique = []\nfor c in X_train.columns:\n    num_uniques = len(X_train[c].unique())\n    if X_train[c].isnull().sum() != 0:\n        num_uniques -= 1\n    if num_uniques == 1:\n        exclude_unique.append(c)\nprint(exclude_unique)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:07.908284Z","iopub.execute_input":"2022-02-15T20:39:07.908592Z","iopub.status.idle":"2022-02-15T20:39:08.255316Z","shell.execute_reply.started":"2022-02-15T20:39:07.908563Z","shell.execute_reply":"2022-02-15T20:39:08.254357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop Candidates \n\n\n\nThe following variables are dropped instead of being imputed due to extremely high proportion of missing values making it impractical to impute based on such a small smaple. \n\n- **finishedsquarefeet13**: Data description says 'Perimeter Living Area'. Over 99% of the dataset is missing values.\n- **finishedsquarefeet15**: Data description says 'Total area'. Over 96% of the dataset is missing values.\n- **finishedfloor1squarefeet**: Over 92% of the dataset is missing values.\n- **finishedsquarefeet50**: Over 92% of the dataset is missing values.\n- **storytypeid**: Data description states 35 possible values for the Story Type. Over 99% of the dataset is missing values.\n- **buildingclasstypeid**: With over 99.8% of the dataset missing, imputing won't contribute any new information. \n- **architecturalstyletypeid**: Data description states 27 possible values. Over 99% of the dataset is missing values.\n- **typeconstructiontypeid**: Data description states 18 possible values. Over 99% of the dataset is missing values.\n- **finishedsquarefeet6**: Data description states 'Base unfinished and finished area'. However, with over 99% of data missing, feature is dropped. \n- **numberofstories**: With over 77% of the missing values and no one value dominating the variable distribution, it is impractical to impute and introduce incorrect information. \n- **rawcensustractandblock, censustractandblock**: With almost 58k unique values, these features are being dropped until further research can be done into incorporating information from them in the model algorithm. \n- **assessmentyear**: Dropping as the variable has only one unique value in the dataset.\n- **parcelid, transactiondate**: This variable is not available when trying to predict target variable `logerror` \n- **basementsqft, yardbuildingsqft26**: Dropping due to over 96% of data missing.  \n\n<br>\n\n#### Duplicate Features \n\nFollowing features are dropped as they represent information already available from another variable in the dataset. \n- **pooltypeid10, hashottuborspa**: Duplicate information is present in `pooltypeid2` feature.\n- **pooltypeid7**: Data description states 'Pool without Hot Tub', duplicate information is present in `pooltypeid2` feature which is boolean for 'Pool with Spa/Hot Tub'\n- **fireplaceflag**: Duplicate information is present in `fireplacecnt` feature with `fireplaceflag` having a higher missing ratio. \n- **threequarterbathnbr, calculatedbathnbr, fullbathcnt**: Duplicate information is present in `bathroomcnt` which has a lower missing ratio\n- **finishedsquarefeet12**: Duplicate information in `calculatedfinishedsquarefeet`.\n- **taxvaluedollarcnt**: Duplicate information from the sum of `structuretaxvaluedollarcnt` and `landtaxvaluedollarcnt`\n- **roomcnt**: Duplicate information from `bedroomcnt` and `roomcnt` shows inconsistent data with majority of properties having 0 rooms in the principal residence\n- **propertyzoningdesc, propertycountylandusecode**: `propertylandusetypeid` already provides similar information. In addition, this variable has over 2300 unique values - drastically increasing dataset cardinality. \n- **regionidneighborhood, regionidzip, regionidcity**: With over 60% of the dataset missing values and 500+ unique values, there are better region based features such as `regionidcounty, latitude, longitude` to obtain this information\n- **taxdelinquencyyear**: Given the high proportion of missing data (over 97%) and **taxdelinquencyflag** providing similar information, the information gain from years is low. \n\n\n**Note: Since we're creating modular pipelines, these variables will not be dropped for every machine learning algorithm. For example, Gradient Boosting Machines are designed to natively handle missing values and extracting information from them - thus, dropping as many variables will not be necessary for more advanced algorithms unlike simpler ones such as Linear Regression.**","metadata":{}},{"cell_type":"code","source":"# Training data copy to test individual pipelines \nX_temp = X_train.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:08.256871Z","iopub.execute_input":"2022-02-15T20:39:08.257167Z","iopub.status.idle":"2022-02-15T20:39:08.282072Z","shell.execute_reply.started":"2022-02-15T20:39:08.257137Z","shell.execute_reply":"2022-02-15T20:39:08.281033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropping Features Pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass FeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, features_to_drop):\n        self.features_to_drop = features_to_drop\n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    def transform(self, X): \n        updated_X = X.drop(self.features_to_drop, axis=1)\n        return updated_X\n    \n# Drop features\nlin_reg_drop_vars = [\"finishedsquarefeet13\", \"finishedsquarefeet15\", \"finishedfloor1squarefeet\", \"finishedsquarefeet50\",\n             \"storytypeid\", \"architecturalstyletypeid\", \"buildingclasstypeid\", \"typeconstructiontypeid\", \"finishedsquarefeet6\",\n             \"pooltypeid10\", \"pooltypeid7\", \"hashottuborspa\", \"fireplaceflag\", \"threequarterbathnbr\", \"calculatedbathnbr\",\n             \"fullbathcnt\", \"numberofstories\", \"rawcensustractandblock\", \"censustractandblock\",\n             \"finishedsquarefeet12\", \"taxvaluedollarcnt\", \"taxamount\", \"assessmentyear\", \"roomcnt\",\n             \"propertyzoningdesc\", \"regionidneighborhood\", \"regionidzip\", \"taxdelinquencyyear\",\n             \"propertycountylandusecode\", \"regionidcity\", \"parcelid\", \"basementsqft\", \"yardbuildingsqft26\", \"transactiondate\"\n            ]\n\n# Sample code to test pipeline \nfeat_dropper = FeatureDropper(features_to_drop=lin_reg_drop_vars)\nX_temp = feat_dropper.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:08.283695Z","iopub.execute_input":"2022-02-15T20:39:08.284164Z","iopub.status.idle":"2022-02-15T20:39:08.310919Z","shell.execute_reply.started":"2022-02-15T20:39:08.284125Z","shell.execute_reply":"2022-02-15T20:39:08.309886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Imputation Pipeline\n\nThere are two main types of data imputation:\n1) **Univariate Imputation**: Impute values in a feature using only non-missing values in that feature only. Examples include calculating the mean/median/mode of a specific variable such as `lot_size` from all non-missing values and imputing the calculated value for missing `lot_size` rows. \n\n2) **Multivariate Imputation**: Multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer). Basically, the purpose of multivariate imputation is to use other features (columns) in the dataset to predict the missing value(s) in the current feature.\n\n**In this notebook, we will create a pipeline for each type of imputation which can easily be used later to compare the performance of models using univariate vs multivariate imputation.**","metadata":{}},{"cell_type":"markdown","source":"## Univariate Imputation\n\n### 0/None Imputation Features\nBased on the data descriptions of the following features, missing values most likely indicate the property missing the specific feature (eg: no basement, no shed, no pool, etc.). Thus, the imputed values in either `0`, `None`, etc. depending on the variable type. \n\n**0 Imputation**\n\n- **yardbuildingsqft17**: Data description says 'Patio in yard'. Missing values are likely zero for having no patio in yard.\n- **fireplacecnt**: Missing values must indicate no fireplace present in the unit, thus imputing `0` for all missing values. \n- **poolcnt**: All values are `1` in data indicating one pool is present, missing values must be `0` indicating otherwise. \n- **poolsizesum**: All missing values indicate no pool on property, thus impute `0` for sqaure footage of all pools. \n- **pooltypeid2**: All values are `1` in data indicating a pool with spa/hot tub is present, missing values must be `0` indicating otherwise. \n- **pooltypeid7**: All values are `1` in data indicating a pool without hot tub is present, missing values must be `0` indicating otherwise. \n- **hashottuborspa**: All values are `1` in data indicating a spa/hot tub is present, missing values must be `0` indicating no hot tub/spa. \n- **decktypeid**: Same value in data indicating a deck is present, missing values must be `0` indicating no deck. \n- **taxdelinquencyflag**: All values are `Y` in data for properties that are tax delinquent implying all missing values must be `N` or `0` for easier handling. \n- **garagecarcnt**: No properties have `0` as the garage car count, indicating the missing values are all properties without a garage. \n- **garagetotalsqft**: All properties with missing `garagecarcnt` are also missing the square feet, indicating the properties do not have a garage. \n\n### Mode Imputation Features\n\nThe following features are imputed with the most frequent value (mode) due to the majority of the dataset having that specific value. \n- **airconditioningtypeid**: Majority of the properties have a Central air conditioning type. \n- **heatingorsystemtypeid**: Majority of the properties have a Central heating system. \n- **unitcnt**: Majority of the property are built into 1 unit.\n- **fips, propertylandusetypeid, regionidcounty**: With only 0.4% of the dataset missing values, imputing the mode is acceptable. \n- **yearbuilt**: Since missing rows is extremely low, mode suffices\n\n### Median Imputation Features\n\nThe following features are imputing with the 50th percentile value (median) to best represent the numerical distributions represented by each features. \n- **buildingqualitytypeid**: Overall condition of the condition from best (lowest) to worst (highest). Thus, imputing the median for missing values. \n- **lotsizesquarefeet**: Continuous numerical variable for area of the lot\n- **bathroomcnt, bedroomcnt, calculatedfinishedsquarefeet**\n- **structuretaxvaluedollarcnt, landtaxvaluedollarcnt, latitude, longitude**","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:08.625025Z","iopub.execute_input":"2022-02-15T20:39:08.625392Z","iopub.status.idle":"2022-02-15T20:39:08.630663Z","shell.execute_reply.started":"2022-02-15T20:39:08.625361Z","shell.execute_reply":"2022-02-15T20:39:08.629786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute 0\nimpute_0_vars = [\"yardbuildingsqft17\", \"fireplacecnt\", \"poolcnt\", \"garagecarcnt\", \"garagetotalsqft\",\n                 \"pooltypeid2\", \"poolsizesum\", \"decktypeid\", \"taxdelinquencyflag\"]\n\n# Impute mode\nimpute_mode_vars = [\"airconditioningtypeid\", \"heatingorsystemtypeid\", \"unitcnt\", \"fips\", \n                    \"propertylandusetypeid\", \"regionidcounty\", \"yearbuilt\"] \n\n# Impute median \nimpute_median_vars = [\"buildingqualitytypeid\", \"lotsizesquarefeet\", \"bathroomcnt\", \"bedroomcnt\", \"calculatedfinishedsquarefeet\",\n                      \"structuretaxvaluedollarcnt\", \"landtaxvaluedollarcnt\", \"latitude\", \"longitude\"]\n\nunivariate_impute_pipe = ColumnTransformer([\n        (\"impute_0\", SimpleImputer(strategy=\"constant\", fill_value=0), impute_0_vars),\n        (\"impute_mode\", SimpleImputer(strategy=\"most_frequent\"), impute_mode_vars),\n        (\"impute_median\", SimpleImputer(strategy=\"median\"), impute_median_vars),\n    ],\n    remainder='passthrough'\n)\n\n# Sample code to test pipeline [ONLY RUN ONE OF UNIVARIATE OR MULTIVARIATE PIPELINES]\nX_temp = univariate_impute_pipe.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:08.632068Z","iopub.execute_input":"2022-02-15T20:39:08.632603Z","iopub.status.idle":"2022-02-15T20:39:09.624107Z","shell.execute_reply.started":"2022-02-15T20:39:08.632561Z","shell.execute_reply":"2022-02-15T20:39:09.622935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multivariate Imputation Pipeline\n\nWe will be using a `RandomForestRegressor` as the estimator to predict the missing values in each feature. For details on `Random Forest` algorithm and tuning its hyperparameters, refer to modeling section later. \n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n\ncat_impute_vars = [\"airconditioningtypeid\", \"heatingorsystemtypeid\", \"fips\", \"propertylandusetypeid\", \"regionidcounty\", \"pooltypeid2\", \"decktypeid\", \"taxdelinquencyflag\"] \nnumeric_impute_vars = [\"bathroomcnt\", \"bedroomcnt\", \"buildingqualitytypeid\", \"calculatedfinishedsquarefeet\",\n                  \"fireplacecnt\", \"garagecarcnt\", \"garagetotalsqft\", \"latitude\", \"longitude\", \"lotsizesquarefeet\", \"poolcnt\",\n                  \"poolsizesum\", \"unitcnt\", \"yardbuildingsqft17\", \"yearbuilt\", \"structuretaxvaluedollarcnt\", \"landtaxvaluedollarcnt\"]\n\nmultivariate_impute_pipe = ColumnTransformer([\n        (\"impute_cats\", SimpleImputer(strategy=\"constant\", fill_value='missing'), cat_impute_vars),\n        (\"impute_num\", IterativeImputer(estimator=RandomForestRegressor(n_estimators=1, max_depth=30, min_samples_leaf=32), random_state=0, max_iter=1), numeric_impute_vars),\n    ],\n    remainder='passthrough'\n)\n\n# Sample code to test pipeline [ONLY RUN ONE OF UNIVARIATE OR MULTIVARIATE PIPELINES]\n# X_temp = multivariate_impute_pipe.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:09.628275Z","iopub.execute_input":"2022-02-15T20:39:09.62864Z","iopub.status.idle":"2022-02-15T20:39:09.635991Z","shell.execute_reply.started":"2022-02-15T20:39:09.628592Z","shell.execute_reply":"2022-02-15T20:39:09.635133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Column Names Appender Pipeline\n\nThe output of the imputation pipeline is not a Pandas DataFrame but a NumPy Array. The following pipeline takes as an input the imputation pipeline and creates a DataFrame from the Numpy Array input. ","metadata":{}},{"cell_type":"code","source":"class ColumnNamesAppender(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Takes a Column Transformer pipeline as an input along with the Numpy Array \n    to output the DataFrame with column names appended to it. \n    \"\"\"\n    def __init__(self, column_transformer, orig_columns, num_transformers):\n        self.column_transformer = column_transformer\n        self.orig_columns = orig_columns\n        self.num_transformers = num_transformers\n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    def transform(self, X): \n        X_column_names = self.get_columns_from_transformer(self.column_transformer, self.orig_columns, self.num_transformers)\n        \n        # Create dataframe from numpy array and column names \n        X = pd.DataFrame(X, columns=X_column_names)\n        return X \n    \n    @staticmethod\n    def get_columns_from_transformer(column_transformer, input_colums, num_transformers):    \n        col_name = []\n        \n        for transformer_in_columns in column_transformer.transformers_: #the last transformer is ColumnTransformer's 'remainder'\n            raw_col_name = transformer_in_columns[2]\n            if isinstance(transformer_in_columns[1],Pipeline): \n                transformer = transformer_in_columns[1].steps[-1][1]\n            else:\n                transformer = transformer_in_columns[1]\n            try:\n                names = transformer.get_feature_names([raw_col_name])\n            except AttributeError: # if no 'get_feature_names' function, use raw column name\n                names = raw_col_name\n            if isinstance(names,np.ndarray): \n                col_name += names.tolist()\n            elif isinstance(names,list):\n                col_name += names    \n            elif isinstance(names,str):\n                col_name.append(names)\n\n        return col_name\n\n\n# Code to test pipeline (using imputation pipeline as sample ColumnTransformer pipeline)\n# TODO: Currently unable to handle columns that are passed through remainder \ncolumn_appender = ColumnNamesAppender(univariate_impute_pipe, orig_columns=X_train.columns, num_transformers=3)\nX_temp = column_appender.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:09.637397Z","iopub.execute_input":"2022-02-15T20:39:09.637941Z","iopub.status.idle":"2022-02-15T20:39:09.741679Z","shell.execute_reply.started":"2022-02-15T20:39:09.637903Z","shell.execute_reply":"2022-02-15T20:39:09.740858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if still missing values\nall_data_na = (X_temp.isnull().sum() / len(X_temp)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:09.743002Z","iopub.execute_input":"2022-02-15T20:39:09.743483Z","iopub.status.idle":"2022-02-15T20:39:10.001697Z","shell.execute_reply.started":"2022-02-15T20:39:09.743449Z","shell.execute_reply":"2022-02-15T20:39:10.000699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert Variables Types Pipeline\n\nThis section changes the feature data type to a more appropriate type. This is because many categorical and boolean variables are currently encoded as floats.","metadata":{}},{"cell_type":"code","source":"convert_to_int = [\"yearbuilt\"] \nconvert_to_string= [\"airconditioningtypeid\", \"heatingorsystemtypeid\", \"fips\", \"propertylandusetypeid\", \"regionidcounty\", \"pooltypeid2\", \"decktypeid\", \"taxdelinquencyflag\"] \nconvert_to_float= [\"bathroomcnt\", \"bedroomcnt\", \"buildingqualitytypeid\", \"calculatedfinishedsquarefeet\",\n                  \"fireplacecnt\", \"garagecarcnt\", \"garagetotalsqft\", \"latitude\", \"longitude\", \"lotsizesquarefeet\", \"poolcnt\",\n                  \"poolsizesum\", \"unitcnt\", \"yardbuildingsqft17\",\n                  \"structuretaxvaluedollarcnt\", \"landtaxvaluedollarcnt\"]\n\nclass ConvertFeatureType(BaseEstimator, TransformerMixin): \n    def __init__(self, convert_to_int=[], convert_to_bool=[], convert_to_string=[], convert_to_float=[]):\n        self.convert_to_int = convert_to_int\n        self.convert_to_bool = convert_to_bool\n        self.convert_to_string = convert_to_string\n        self.convert_to_float = convert_to_float\n        self.features = {\"int\": convert_to_int, \"float\": convert_to_float, \"boolean\": convert_to_bool, \"str\": convert_to_string}\n        \n    def fit(self, X, y=None): \n        return self  # Nothing else to do \n    \n    def transform(self, X): \n        self.map_bool_features(X)        \n        for data_type in self.features.keys(): \n            X = self.convert_feature_types(X, data_type)\n        return X \n    \n    def map_bool_features(self, X): \n        \"\"\"Convert all non null values to True in bool features prior to changing type to Boolean.\"\"\"\n        for var in self.convert_to_bool:\n            X[var][X[var].notnull()] = True\n    \n    def convert_feature_types(self, X, data_type): \n        for var in self.features[data_type]: \n            X[var] = X[var].astype(data_type) \n        return X\n    \n# # Code to test pipeline\nfeature_type_changer = ConvertFeatureType(convert_to_int=convert_to_int, convert_to_string=convert_to_string, convert_to_float=convert_to_float)\nX_temp = feature_type_changer.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:10.003136Z","iopub.execute_input":"2022-02-15T20:39:10.003758Z","iopub.status.idle":"2022-02-15T20:39:11.318496Z","shell.execute_reply.started":"2022-02-15T20:39:10.003709Z","shell.execute_reply":"2022-02-15T20:39:11.317407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvertToType(BaseEstimator, TransformerMixin): \n    '''\n    Variation of pipeline above to convert ALL colummns to specific type.\n    Handy for algorithms such as XGBoost which expect all features to not be object type. \n    '''\n    def __init__(self, var_type, vars_to_convert=None):\n        self.var_type = var_type\n        self.vars_to_convert = vars_to_convert\n        \n    def fit(self, X, y=None): \n        return self  # Nothing else to do \n    \n    def transform(self, X): \n        if self.vars_to_convert: \n            for col in self.vars_to_convert: \n                X[col] = X[col].astype(self.var_type) \n        else: \n            for col in X.columns: \n                X[col] = X[col].astype(self.var_type)     \n        return X\n      \n# Code to test pipeline\n# convert_to_float = ConvertToType(var_type='float', vars_to_convert=sample_vars)\n# X_temp = convert_to_float.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:11.319864Z","iopub.execute_input":"2022-02-15T20:39:11.320184Z","iopub.status.idle":"2022-02-15T20:39:11.328086Z","shell.execute_reply.started":"2022-02-15T20:39:11.320148Z","shell.execute_reply":"2022-02-15T20:39:11.326886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Feature Creator Pipeline \n\nThis pipeline creates simple date features by extracting the information from `transactiondate` ","metadata":{}},{"cell_type":"code","source":"class CreateDateFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Creates simple date features by extracting the information from `transactiondate` \n    \"\"\"\n    def __init__(self):\n        pass\n        \n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    \n    def transform(self, X): \n        dt = pd.to_datetime(X['transactiondate']).dt\n        X['transaction_year'] = (dt.year).astype('category')\n        X['transaction_month'] = ((dt.year - 2016)*12 + dt.month).astype('category')\n        X['transaction_day'] = dt.day\n        X['transaction_quarter'] = ((dt.year - 2016)*4 + dt.quarter).astype('category')\n        X = X.drop(['transactiondate'], axis=1)\n    \n        return X\n    \n# Code to test pipeline\n# date_feat_creator = CreateDateFeatures()\n# X_temp = date_feat_creator.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:11.346335Z","iopub.execute_input":"2022-02-15T20:39:11.346816Z","iopub.status.idle":"2022-02-15T20:39:11.357728Z","shell.execute_reply.started":"2022-02-15T20:39:11.346782Z","shell.execute_reply":"2022-02-15T20:39:11.356802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Year Feature Creation Pipeline\n\nThe following transformer takes datetime features as input skewed_featsand converts them into years from present. For example: `1989` is converted into `present_year(2021) - 1989 = 32` ","metadata":{}},{"cell_type":"code","source":"from datetime import date\n\nclass CreateYearFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Creates new features converting dates into years from present. \n    Eg: 1989 is converted into present_year(2021) - 1989 which is 32. \n    \"\"\"\n    def __init__(self, date_features):\n        self.date_features = date_features\n        self.current_year = date.today().year\n    \n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    \n    def transform(self, X): \n        for var in self.date_features.keys(): \n            new_var_name = self.date_features[var]\n            X[new_var_name] = self.current_year - X[var]\n            X[new_var_name] = X[new_var_name].astype('float') \n            \n            # Drop old feature\n            X.drop(var, axis=1, inplace=True)\n        return X\n    \n# Date features\ndate_features = {\"yearbuilt\": \"house_age\"}\n\n# Code to test pipeline\nyear_feat_creator = CreateYearFeatures(date_features=date_features)\nX_temp = year_feat_creator.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:11.359067Z","iopub.execute_input":"2022-02-15T20:39:11.359505Z","iopub.status.idle":"2022-02-15T20:39:11.417379Z","shell.execute_reply.started":"2022-02-15T20:39:11.359473Z","shell.execute_reply":"2022-02-15T20:39:11.416634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combining Existing Features Pipeline\n\nThe following transformer creates new derived variables by combining existing variables ","metadata":{}},{"cell_type":"code","source":"class CreateDerivedFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Creates new features by combining existing variables \n    \"\"\"\n    def __init__(self):\n        return None # nothing else to do \n    \n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    \n    def transform(self, X): \n        # Average Size Features \n        X['avg_garage_size'] = X['garagetotalsqft'] / X['garagecarcnt']\n        X['property_tax_per_sqft'] = X['taxamount'] / X['calculatedfinishedsquarefeet']\n        \n        # Average area in sqft per room\n        mask = (X.roomcnt >= 1)  # avoid dividing by zero\n        X.loc[mask, 'avg_area_per_room'] = X.loc[mask, 'calculatedfinishedsquarefeet'] / X.loc[mask, 'roomcnt']\n        \n        # Derived Room Count\n        X['derived_room_cnt'] = X['bedroomcnt'] + X['bathroomcnt']\n        \n        # Use the derived room_cnt to calculate the avg area again\n        mask = (X.derived_room_cnt >= 1)\n        X.loc[mask,'derived_avg_area_per_room'] = X.loc[mask,'calculatedfinishedsquarefeet'] / X.loc[mask,'derived_room_cnt']\n        \n        # Rotated Coordinates\n        X['location_1'] = X['latitude'] + X['longitude']\n        X['location_2'] = X['latitude'] - X['longitude']\n        X['location_3'] = X['latitude'] + 0.5 * X['longitude']\n        X['location_4'] = X['latitude'] - 0.5 * X['longitude']\n        \n        # 'finished_area_sqft' and 'total_area' cover only a strict subset of 'finished_area_sqft_calc' in terms of \n        # non-missing values. Also, when both fields are not null, the values are always the same.\n        # So we can probably drop 'finished_area_sqft' and 'total_area' since they are redundant\n        # If there're some patterns in when the values are missing, we can add two isMissing binary features\n        X['missing_finished_area'] = X['finishedsquarefeet12'].isnull().astype(float)\n        X['missing_total_area'] = X['finishedsquarefeet15'].isnull().astype(float)\n        X = X.drop(['finishedsquarefeet12', 'finishedsquarefeet15'], axis=1)\n        X['missing_bathroom_cnt_calc'] = X['calculatedbathnbr'].isnull().astype(float)\n        X = X.drop(['calculatedbathnbr'], axis=1)\n        \n        return X\n    \n# Code to test pipeline\n# derived_feat_creator = CreateDerivedFeatures()\n# X_temp = derived_feat_creator.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:11.418696Z","iopub.execute_input":"2022-02-15T20:39:11.419122Z","iopub.status.idle":"2022-02-15T20:39:11.430994Z","shell.execute_reply.started":"2022-02-15T20:39:11.41909Z","shell.execute_reply":"2022-02-15T20:39:11.43018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Aggregated Features Pipeline\n\nThe following pipeline creates aggregated features such as aggregations for a specific zip code. ","metadata":{}},{"cell_type":"code","source":"class CreateAggregatedFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Creates new features by combining existing variables \n    \"\"\"\n    def __init__(self, group_col, agg_cols):\n        self.group_col = group_col\n        self.agg_cols = agg_cols\n    \n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    \n    def transform(self, X): \n        group_col = self.group_col\n        X[group_col + '-groupcnt'] = X[group_col].map(X[group_col].value_counts())\n        \n        new_columns = []  # New feature columns added to the DataFrame\n        for col in self.agg_cols:\n            aggregates = X.groupby(group_col, as_index=False)[col].agg([np.mean])\n            aggregates.columns = [group_col + '-' + col + '-' + s for s in ['mean']]\n            new_columns += list(aggregates.columns)\n            X = X.merge(how='left', right=aggregates, on=group_col)\n\n        for col in self.agg_cols:\n            mean = X[group_col + '-' + col + '-mean']\n            diff = X[col] - mean\n\n            X[group_col + '-' + col + '-' + 'diff'] = diff\n            if col != 'yearbuilt':\n                X[group_col + '-' + col + '-' + 'percent'] = diff / mean\n\n        # Set the values of the new features to NaN if the groupcnt is too small (prevent overfitting)\n        threshold = 100\n        X[new_columns] = X.loc[X[group_col + '-groupcnt'] < threshold, new_columns] = np.nan\n\n        # Drop the mean features which are not as useful\n        X = X.drop([group_col+'-'+col+'-mean' for col in self.agg_cols], axis=1)\n\n        return X\n    \n# Code to test pipeline\n# aggregated_feat_creator = CreateAggregatedFeatures(group_col=group_col, agg_cols=agg_cols)\n# X_temp = aggregated_feat_creator.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:11.432017Z","iopub.execute_input":"2022-02-15T20:39:11.43248Z","iopub.status.idle":"2022-02-15T20:39:11.446557Z","shell.execute_reply.started":"2022-02-15T20:39:11.432449Z","shell.execute_reply":"2022-02-15T20:39:11.445555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encoding Categorical Variables + Standardizing Numerical Variables \nThe following custom transformer uses `ColumnTransformer` to perform One-Hot Encoding on Categorical Features and Robust Scaler on Numerical Features.","metadata":{}},{"cell_type":"code","source":"class FeatureEncoderAndScaler(BaseEstimator, TransformerMixin):\n    def __init__(self, features_to_encode=None, features_to_scale=None, numeric_types=[\"float\"]):\n        self.features_to_encode = features_to_encode\n        self.features_to_scale = features_to_scale\n        self.numeric_types = numeric_types\n        self.feature_encoder_and_scaler = None\n        \n    def fit(self, X, y=None):\n        if not self.features_to_encode:\n            self.features_to_encode = X.select_dtypes(include = [\"object\"]).columns \n        if not self.features_to_scale:\n            self.features_to_scale = X.select_dtypes(include = self.numeric_types).columns   \n        \n        feature_encoder_scaler = ColumnTransformer([\n            (\"ohe_cats\", OneHotEncoder(handle_unknown='ignore', sparse=False), self.features_to_encode),\n            (\"num_scaler\", RobustScaler(), self.features_to_scale),\n        ],\n            remainder='passthrough',\n#             verbose_feature_names_out='False'    # To turn off prefixing of transformer name to feature\n        )\n                    \n        self.feature_encoder_scaler = feature_encoder_scaler.fit(X)\n        return self   \n    \n    def transform(self, X): \n        # OneHotEncoder returns numpy array which is converted to dataframe\n        X_np = self.feature_encoder_scaler.transform(X)\n        X = pd.DataFrame(\n            X_np, \n            columns=self.feature_encoder_scaler.get_feature_names_out()\n        )\n        X = self.convert_feature_types(X)\n        \n        return X\n    \n    def convert_feature_types(self, X):\n        \"\"\"Convert feature types to object, float, bool based on the column name. \n        Columns with `ohe_cats` are object, `num_scaler` are float, `remainder` are bool\"\"\"\n        for column in X:\n            if 'ohe_cats' in column:\n                X[column] = X[column].astype(\"object\") \n            elif 'num_scaler' in column: \n                X[column] = X[column].astype(\"float\") \n            elif 'remainder' in column: \n                X[column] = X[column].astype(\"boolean\") \n        return X \n        \n\n# Code to test pipeline\n# cat_features = X_test.select_dtypes(include = [\"object\"]).columns            # optional as encoder automatically detects relevant features     \n# feature_encoder_scaler = FeatureEncoderAndScaler(features_to_encode=cat_features)\nfeature_encoder_scaler = FeatureEncoderAndScaler()\nX_temp = feature_encoder_scaler.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:11.447982Z","iopub.execute_input":"2022-02-15T20:39:11.448287Z","iopub.status.idle":"2022-02-15T20:39:13.929249Z","shell.execute_reply.started":"2022-02-15T20:39:11.448256Z","shell.execute_reply":"2022-02-15T20:39:13.928208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample plot histogram to check standardized variable\nX_temp.num_scaler__calculatedfinishedsquarefeet.hist(bins=50, figsize=(8,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:13.930662Z","iopub.execute_input":"2022-02-15T20:39:13.930993Z","iopub.status.idle":"2022-02-15T20:39:14.233727Z","shell.execute_reply.started":"2022-02-15T20:39:13.930962Z","shell.execute_reply":"2022-02-15T20:39:14.232429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Sanity Check \n\nPrior to creating new features using the chosen subset of variables from the original dataset, it is important to perform a quick correlation analysis between the variables to drop any highly correlated variables that were missed in the earlier analysis","metadata":{}},{"cell_type":"code","source":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = X_temp.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:14.234937Z","iopub.execute_input":"2022-02-15T20:39:14.235222Z","iopub.status.idle":"2022-02-15T20:39:15.130793Z","shell.execute_reply.started":"2022-02-15T20:39:14.235195Z","shell.execute_reply":"2022-02-15T20:39:15.129664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`TaxAmount` is highly correlated with `structuretaxvaluedollarcnt` and `landtaxvaluedollarcnt` which makes sense given its the sum of the two variables. Thus, it is dropped from the analysis.\n\n**Note**: After the initial exploratory analysis, the finding that `TaxAmount` is correlated with other variables and needs to be droppped was incorporated in `FeatureDropper` pipeline earlier to avoid repeated code later. \n\nSample Code to drop variable: \n`df_train = df_train.drop(\"taxamount\", axis=1)`","metadata":{}},{"cell_type":"markdown","source":"## Create Polynomial Features Pipeline\n\nTo better detect higher degree relations between the target variable and predictors, we'll find the most important features relative to the target and create 3 new polynomial variables for each of the top 8 existing features (out of the 15 numerical features). The three new polynomial features are: \n- `feature^2`\n- `feature^3`\n- `sqrt(feature)` \n\nThis transformation is useful for models such as linear regression compared to more complex models such as Random Forest, GBMs that can detect non-linear patters within the data without such feature engineering.","metadata":{}},{"cell_type":"code","source":"# Find most important features relative to target (Take absolute value)\nprint(\"Find most important features relative to target\")\ncorr_df = X_temp.copy()\ncorr_df[\"logerror\"] = y_train.values\ncorr = corr_df.corr()\nmost_corr_feat = corr.logerror.abs().sort_values(ascending=False)[1:9].index\nmost_corr_feat","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:15.132376Z","iopub.execute_input":"2022-02-15T20:39:15.132831Z","iopub.status.idle":"2022-02-15T20:39:15.4592Z","shell.execute_reply.started":"2022-02-15T20:39:15.132774Z","shell.execute_reply":"2022-02-15T20:39:15.458163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CreatePolynomialFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Creates new polynomial features using the 10 most important features relative to the target. \n    3 new polynomial variables for each of the existing features: squared, cubed, sqrt. \n    \"\"\"\n    def __init__(self, most_imp_feat):\n        self.most_imp_feat = most_imp_feat\n    \n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    \n    def transform(self, X): \n        \n        for var in self.most_imp_feat: \n            # New var names \n            s2_var_name = var + '-s2'\n            s3_var_name = var + '-s3'\n            sq_var_name = var + '-sqrt'\n            \n            # Create features \n            X[s2_var_name] = X[var] ** 2 \n            X[s3_var_name] = X[var] ** 3 \n            X[sq_var_name] = np.sqrt(X[var] + abs(min(X[var])))  # Translate feature to ensure min value is 0 before sqrt \n            \n        return X\n    \n# Code to test pipeline\npoly_feat_creator = CreatePolynomialFeatures(most_corr_feat)\nX_temp = poly_feat_creator.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:15.460474Z","iopub.execute_input":"2022-02-15T20:39:15.460778Z","iopub.status.idle":"2022-02-15T20:39:15.744705Z","shell.execute_reply.started":"2022-02-15T20:39:15.460749Z","shell.execute_reply":"2022-02-15T20:39:15.743806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Skewed Features Pipeline\nNext transformation is applied to numerical features that are highly skewed (all variables with skewness above the threshold value of 0.75). We will be performing the **Box Cox Transformation** using scipy function `boxcox1p`. \n\nMore details on Box Cox Transformation can be found [here](https://onlinestatbook.com/2/transformations/box-cox.html) along with Scipy official documentation for `boxcox1p` [here](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.special.boxcox1p.html). ","metadata":{}},{"cell_type":"code","source":"# Code to detect any skewed features \nnumeric_feats = X_temp.dtypes[X_temp.dtypes == 'float'].index\nskewed_feats = X_temp[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewed_feats[:20]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:15.74722Z","iopub.execute_input":"2022-02-15T20:39:15.747518Z","iopub.status.idle":"2022-02-15T20:39:16.037758Z","shell.execute_reply.started":"2022-02-15T20:39:15.747488Z","shell.execute_reply":"2022-02-15T20:39:16.036728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_temp.num_scaler__lotsizesquarefeet.hist(bins=10, figsize=(8,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:16.039186Z","iopub.execute_input":"2022-02-15T20:39:16.039464Z","iopub.status.idle":"2022-02-15T20:39:16.276534Z","shell.execute_reply.started":"2022-02-15T20:39:16.039436Z","shell.execute_reply":"2022-02-15T20:39:16.275587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import date\nfrom scipy.special import boxcox1p\n\nclass BoxCoxSkewedFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Performs Box-Cox tranformation on all numerical variables with skewness\n    above a certain threshold. \n    \"\"\"\n    def __init__(self, skewness_thres=0.75):\n        self.skewness_thres = skewness_thres\n    def fit(self, X, y=None):\n        return self  # nothing else to do \n    def transform(self, X): \n        numeric_feats = X.dtypes[X.dtypes == 'float'].index\n        skewed_feats = X[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n        \n        skewed_feats = skewed_feats[abs(skewed_feats) > self.skewness_thres].index\n        \n        # Apply box-cox to each variable \n        lam = 0.18\n        for feat in skewed_feats:\n            X[feat] = X[feat] + abs(min(X[feat]))       # Translate feature to ensure minimum value is 0 \n            X[feat] = boxcox1p(X[feat], lam)\n        return X\n    \n\n# # Code to test pipeline\nskew_transformer = BoxCoxSkewedFeatures()\nX_temp = skew_transformer.transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:16.277805Z","iopub.execute_input":"2022-02-15T20:39:16.278088Z","iopub.status.idle":"2022-02-15T20:39:17.444006Z","shell.execute_reply.started":"2022-02-15T20:39:16.27806Z","shell.execute_reply":"2022-02-15T20:39:17.443025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check skewed features after box-cox transformation\nnumeric_feats = X_temp.dtypes[X_temp.dtypes == 'float'].index\nskewed_feats = X_temp[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewed_feats[:20]","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:17.445192Z","iopub.execute_input":"2022-02-15T20:39:17.44548Z","iopub.status.idle":"2022-02-15T20:39:17.578585Z","shell.execute_reply.started":"2022-02-15T20:39:17.445451Z","shell.execute_reply":"2022-02-15T20:39:17.577524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_temp.num_scaler__lotsizesquarefeet.hist(bins=10, figsize=(8,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:17.650234Z","iopub.execute_input":"2022-02-15T20:39:17.650643Z","iopub.status.idle":"2022-02-15T20:39:17.881592Z","shell.execute_reply.started":"2022-02-15T20:39:17.650594Z","shell.execute_reply":"2022-02-15T20:39:17.880455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_temp.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:17.580712Z","iopub.execute_input":"2022-02-15T20:39:17.581151Z","iopub.status.idle":"2022-02-15T20:39:17.648682Z","shell.execute_reply.started":"2022-02-15T20:39:17.581103Z","shell.execute_reply":"2022-02-15T20:39:17.64768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ordinal Encoding Categorical Variables \n\nThis is done to categorical variables that may contain information in their ordering set. Unfortunately, there are no features in this dataset that require ordinal encoding. For any pipelines that require ordinal encoding, it is suggested to use `OrdinalEncoder` instead of `LabelEncoder` as they both perform the exact same operation however, `OrdinalEncoder` can handle multiple columns while `LabelEncoder` needs to be called on columns one at a time. ","metadata":{}},{"cell_type":"code","source":"# Clear memory\ndel X_temp; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:17.883734Z","iopub.execute_input":"2022-02-15T20:39:17.884035Z","iopub.status.idle":"2022-02-15T20:39:18.054065Z","shell.execute_reply.started":"2022-02-15T20:39:17.884006Z","shell.execute_reply":"2022-02-15T20:39:18.052937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Full Data Preparation Pipeline \n\nAll the hardwork in creating the individual pipelines above can now be easily used by creating custom Pipelines as shown in cells below. This allows us to easily experiment with different preprocessing steps for different models.","metadata":{}},{"cell_type":"code","source":"X_prepared = X_train.copy()\nX_prepared_val = X_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:30:56.79271Z","iopub.execute_input":"2022-02-15T19:30:56.793055Z","iopub.status.idle":"2022-02-15T19:30:56.816513Z","shell.execute_reply.started":"2022-02-15T19:30:56.793024Z","shell.execute_reply":"2022-02-15T19:30:56.815738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Dropper Pipeline\nfeature_dropper = FeatureDropper(features_to_drop=lin_reg_drop_vars)\n\n# Convert Date Features Pipeline\nyear_feat_creator = CreateYearFeatures(date_features=date_features)\n\n# Feature Encoding and Scaling Pipeline\nfeature_encoder_scaler = FeatureEncoderAndScaler()\n\n# Transform Skewed Numerical Features Pipeline\nskew_transformer = BoxCoxSkewedFeatures()\n\n# Two versions below: Univariate Imputation and Multivariate Imputation - ONLY UNCOMMENT ONE SECTION\n########################################### \n# 1) Univariate Imputation Pipeline\n\nconvert_to_bool = [\"pooltypeid2\", \"decktypeid\", \"taxdelinquencyflag\"] \nconvert_to_string= [\"airconditioningtypeid\", \"heatingorsystemtypeid\", \"fips\", \"propertylandusetypeid\", \"regionidcounty\"]\nunivariate_impute_pipe = ColumnTransformer([\n        (\"impute_0\", SimpleImputer(strategy=\"constant\", fill_value=0), impute_0_vars),\n        (\"impute_mode\", SimpleImputer(strategy=\"most_frequent\"), impute_mode_vars),\n        (\"impute_median\", SimpleImputer(strategy=\"median\"), impute_median_vars),\n    ],\n    remainder='passthrough'\n)\ncol_name_appender = ColumnNamesAppender(univariate_impute_pipe, X_train.columns, num_transformers=3)\nfeature_type_changer = ConvertFeatureType(convert_to_int=convert_to_int, convert_to_string=convert_to_string, \n                                          convert_to_float=convert_to_float, convert_to_bool=convert_to_bool)\npoly_feat_creator = CreatePolynomialFeatures(most_corr_feat)\n\n\n############################################\n# 2) Multivariate Version\n\n# convert_to_string= [\"airconditioningtypeid\", \"heatingorsystemtypeid\", \"fips\", \"propertylandusetypeid\", \"regionidcounty\", \"pooltypeid2\", \"decktypeid\", \"taxdelinquencyflag\"]\n# multivariate_impute_pipe = ColumnTransformer([\n#         (\"impute_cats\", SimpleImputer(strategy=\"constant\", fill_value='missing'), cat_impute_vars),\n#         (\"impute_num\", IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, max_depth=30, min_samples_leaf=32), random_state=0, max_iter=1), numeric_impute_vars),\n#     ],\n#     remainder='passthrough'\n# )\n# col_name_appender = ColumnNamesAppender(multivariate_impute_pipe, X_train.columns, num_transformers=3)\n# feature_type_changer = ConvertFeatureType(convert_to_int=convert_to_int, convert_to_string=convert_to_string, convert_to_float=convert_to_float)\n# # most_corr_feat = list(map(lambda x: x.replace('num_scaler__',''), most_corr_feat))     # Clean up feature names by removing `num_scaler__`\n# poly_feat_creator = CreatePolynomialFeatures(most_corr_feat)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:30:58.995008Z","iopub.execute_input":"2022-02-15T19:30:58.99598Z","iopub.status.idle":"2022-02-15T19:30:59.009829Z","shell.execute_reply.started":"2022-02-15T19:30:58.995925Z","shell.execute_reply":"2022-02-15T19:30:59.008733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin_reg_preprocessor = Pipeline([\n        ('feature_dropper', feature_dropper),\n        ('univariate_impute_pipe', univariate_impute_pipe),\n#         ('multivariate_impute_pipe', multivariate_impute_pipe),\n        ('col_name_appender', col_name_appender),\n        ('feature_type_changer', feature_type_changer),\n        ('year_feat_creator', year_feat_creator),\n        ('feature_encoder_scaler', feature_encoder_scaler),\n        ('poly_feat_creator', poly_feat_creator),\n        ('skew_transformer', skew_transformer),\n    ])\n\ndata_prep_pipe = lin_reg_preprocessor.fit(X_prepared)\nX_prepared = lin_reg_preprocessor.transform(X_prepared)\nX_prepared_val = lin_reg_preprocessor.transform(X_prepared_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:31:07.229698Z","iopub.execute_input":"2022-02-15T19:31:07.230416Z","iopub.status.idle":"2022-02-15T19:31:28.645077Z","shell.execute_reply.started":"2022-02-15T19:31:07.230365Z","shell.execute_reply":"2022-02-15T19:31:28.64396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import set_config\nset_config(display='diagram')\nlin_reg_preprocessor","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:31:28.646505Z","iopub.execute_input":"2022-02-15T19:31:28.647019Z","iopub.status.idle":"2022-02-15T19:31:28.789784Z","shell.execute_reply.started":"2022-02-15T19:31:28.646982Z","shell.execute_reply":"2022-02-15T19:31:28.788764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select and Train Models\n\n**As this project is primarily for learning purposes, candidate machine learning algorithms will be analyzed in greater detail before being rejected to better understand the algorithms for future usage.**\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:29.842568Z","iopub.execute_input":"2022-02-15T20:39:29.843041Z","iopub.status.idle":"2022-02-15T20:39:29.848472Z","shell.execute_reply.started":"2022-02-15T20:39:29.843005Z","shell.execute_reply":"2022-02-15T20:39:29.847364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin_reg = LinearRegression()\nlin_reg.fit(X_prepared, y_train)\n\n# let's try the full preprocessing pipeline on a few training instances\nsome_data = X_train.iloc[:5]\nsome_labels = y_train.iloc[:5]\nsome_data_prepared = data_prep_pipe.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:32:02.214567Z","iopub.execute_input":"2022-02-15T19:32:02.21492Z","iopub.status.idle":"2022-02-15T19:32:04.236663Z","shell.execute_reply.started":"2022-02-15T19:32:02.214887Z","shell.execute_reply":"2022-02-15T19:32:04.235652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation \n\n### Baseline Metrics \n\nIt is important to set a baseline for the model's performance to compare different algorithms. For regression problems, the baseline metrics are calculated by replacing $y'$ with $\\bar{y}$. Using this, the different baseline regression metrics are: \n\n- **MSE Baseline**: Variance of the target variable\n- **RMSE Baseline**: Standard Deviation of the target variable\n- **MAE Baseline**: Average Abolsute Deviation of the target variable\n- **R2 Baseline**: 0\n\nFor this regression problem, we will use the models' **Mean Absolute Error** and **RMSE (Root Mean Squared Error)** to compare the different algorithms which have **baseline values of 0.533 and 0.0837** respectively.\n\nWe will also observe the RMSE as another evaluation metric which punishes more for outliers than MAE. ","metadata":{}},{"cell_type":"code","source":"# Baseline for RMSE\nprint(f\"MAE Baseline: {y_train.mad()}\")\nprint(f\"RMSE Baseline: {y_train.std()}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:31.891931Z","iopub.execute_input":"2022-02-15T20:39:31.892281Z","iopub.status.idle":"2022-02-15T20:39:31.904757Z","shell.execute_reply.started":"2022-02-15T20:39:31.892249Z","shell.execute_reply":"2022-02-15T20:39:31.903748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MAE Evaluation \n\nTo evaluate and short list the most promising models, we will use the models' **MAE** in two different ways: \n\n1) **MAE on Validation Set**: Calculates the MAE on the validation set which is quicker to calculate than evaluation using Cross-Validation. However, it is possible the MAE obtained is skewed depending on the instances sampled in the validation set. \n\n2) A great alternative is to use **K-Fold Cross-Validation** where the training set is randomly split into `n` subsets (for example 10 subsets) called *folds*. It trains and evaluates the model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. Result is an array containing the 10 evaluation scores. Takes longer to evaluate but provides a more accurate measure of the model's performance.\n\n","metadata":{}},{"cell_type":"code","source":"def get_eval_metrics(models, X, y_true): \n    \"\"\"\n    Calculates MAE (Mean Absoulate Error) and RMSE (Root Mean Squared Error) on the data set for input models. \n    `models`: list of fit models \n    \"\"\"\n    for model in models: \n        y_pred= model.predict(X)\n        rmse = mean_squared_error(y_true, y_pred, squared=False)\n        mae = mean_absolute_error(y_true, y_pred)\n        print(f\"Model: {model}\")\n        print(f\"MAE: {mae}, RMSE: {rmse}\")\n\n# Test usage of RMSE function\n# get_eval_metrics([lin_reg, ridge_reg, lasso_reg], X_prepared_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:32.644569Z","iopub.execute_input":"2022-02-15T20:39:32.645167Z","iopub.status.idle":"2022-02-15T20:39:32.650987Z","shell.execute_reply.started":"2022-02-15T20:39:32.64513Z","shell.execute_reply":"2022-02-15T20:39:32.649712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_scores(model, scores):\n    print(\"-\"*50)\n    print(\"Model:\", model)\n    print(\"\\nScores:\", scores)\n    print(\"\\nMean:\", scores.mean())\n    print(\"\\nStandard deviation:\", scores.std())\n    \ndef get_cross_val_scores(models, X, y, cv=10, fit_params=None):\n    \"\"\"\n    Performs k-fold cross validation and calculates MAE for each fold for all input models. \n    `models`: list of fit models \n    \"\"\"    \n    for model in models: \n        mae = -cross_val_score(model, X, y, scoring=\"neg_mean_absolute_error\", cv=cv, fit_params=fit_params)\n        display_scores(model, mae) \n\n# Test usage of cross val function\n# get_cross_val_scores([lin_reg, ridge_reg], X_prepared, y_train, cv=5)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:33.390207Z","iopub.execute_input":"2022-02-15T20:39:33.390813Z","iopub.status.idle":"2022-02-15T20:39:33.397656Z","shell.execute_reply.started":"2022-02-15T20:39:33.390766Z","shell.execute_reply":"2022-02-15T20:39:33.396553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Multiple Models \n\nNow that we've tested our data preparation pipeline with a sample model, the next step is to train the data on different regression algorithms to shortlist the most promising algorithms for our problem. \n\nAlgorithms to test with include: \n- **Multiple Linear Regression**: Simple algorithm to implement but can over-simplify real-world problems by assuming a linear relationship among the variables. \n- **Linear Regression with Regularization (Lasso, Ridge, ElasticNet)**: Techniques that penalize the linear regression model to prevent overfitting.\n- **Support Vector Regression**: Uses hyperplanes to segregate the data. \n- **Decision Tree**: Powerful model capable of finding complex nonlinear relationships in the data.\n- **Random Forest**: Train many Decision Tress on random subsets of the features (*Ensemble Learning*).\n- **Gradient Boosting Machines**: Powerful and versatile algorithms (mostly using trees) that sequentially train weak learners to improve after each training iteration. \n- **Stacking Models**: Stacking involves training a new learning algorithm to combine the predictions of several base learners","metadata":{}},{"cell_type":"markdown","source":"# 1. Linear Regression Models \n\nBoth plain Linear Regression and Regularized Linear Regression algorithms will be tested. \nBoth **Simple Multiple Linear Regression** and **Regularized Linear Regression (Ridge, Lasso, ElasticNet)** will be analyzed. Specifically, the algorithms tested are: \n\n- **Simple Multiple Linear Regression**: Plain linear regression that minimizes the Mean Squared Error(MSE) cost function. \n\n### Regularized Models\nA good way to reduce overfitting is to regularize the model (i.e. to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For a linear model, regularization is typically acheived by constraining the weights of the model - this forced the learning algorithm to not only fit the data but also keep the model weights as small as possible. \n- **Ridge Regression (L2 norm)**: A regularization term equal to $\\alpha*\\Theta_{j}^2$ is added to the cost function where $\\Theta_{j}$ are the model weights and $\\alpha$ is a hyperparameter that controls how much the model is regularized. \n    - If $\\alpha=0$, then Ridge Regression is just Simple Linear Regression. If $\\alpha$ is very large, then all weights end up very close to zero (**but NOT zero**) and the result is a flat line going through the data's mean.\n- **Lasso Regression (L1 norm)**: A regularization term equal to $\\alpha*\\lvert \\Theta_{j} \\rvert$ is added to the cost function where $\\Theta_{j}$ are the model weights and $\\alpha$ is a hyperparameter that controls how much the model is regularized. \n    - Lasso Regression tends to eliminate the weights of the least important features (i.e. set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model with few nonzero feature weights. \n- **ElasticNet Regression (L1 and L2 norm)**: Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso's regularization terms, and you can control the mix ratio $r$. \n    - When $r=0$, Elastic Net is equivalent to Ridge Regression, and when $r=1$, it is equivalent to Lasso Regression. \n\n### Which Model to Use When?\n\n- **Simple Linear Regression**: It is almost always preferable to have at least a bit of regularization, so generally you should avoid plain Linear Regression. \n- **Ridge Regression**: Ridge is a good default when most features are useful, but if you suspect that only a few features are useful, you should prefer Lasso or Elastic Net because they can exclude the useless features from the equations (Ridge never makes the weights zero, can only be very close to zero). \n- **Elastic-Net / Lasso**: In general, Elastic Net is preferred over Lasso because Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correalted. \n    - Especially good at dealing with situations when there are correlations between parameters as **Elastic Net groups and shrinks associated with the correlated variables and leaves them in equation or removes them all at once.**\n\nCross Validation is used to find the optimal values for each of the Ridge and Lasso parameters ($\\alpha_{1}, \\alpha_{2}$). \n\n**TODO**: Add residual plot and real vs predicted plots. Look at notebook here for [reference](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset)","metadata":{}},{"cell_type":"markdown","source":"## 1a) Simple Linear Regression\n\n**Things of note:**\n- The model RMSE is significantly higher than MAE which suggests that the outliers are affecting the model's performance as RMSE punishes the model more for mispredicting outliers. \n- The K-Fold Cross Validation shows that the model's performance is highly volatile","metadata":{}},{"cell_type":"code","source":"# Linear Regression \nlin_reg = LinearRegression()\nlin_reg.fit(X_prepared, y_train)\n\nget_eval_metrics([lin_reg], X_prepared_val, y_val)\nget_cross_val_scores([lin_reg], X_prepared, y_train, cv=5)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:32:30.511224Z","iopub.execute_input":"2022-02-15T19:32:30.51159Z","iopub.status.idle":"2022-02-15T19:32:42.096845Z","shell.execute_reply.started":"2022-02-15T19:32:30.511554Z","shell.execute_reply":"2022-02-15T19:32:42.092812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Curves \n\nTo detect where the model is under or overfitting, we can look at the *learning curves*. **Learning Curves** are plots of the model’s performance on the training set and the validation set as a function of the training set size (or the training iteration). To generate the plots, train the model several times on different sized subsets of the training set.\n ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(model, X_train, y_train, X_val, y_val):\n    \"\"\"\n    Train the input model on different sized subsets and test on validation set. \n    Output a plot of training and validation error for the different sized subsets. \n    \"\"\"\n    train_errors, val_errors = [], []\n    num_instances = np.linspace(1, len(X_train), num=15).astype(int)\n    \n    for m in num_instances:\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_absolute_error(y_train[:m], y_train_predict))\n        val_errors.append(mean_absolute_error(y_val, y_val_predict))\n    plt.plot(num_instances, train_errors, \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(num_instances, val_errors, \"b-\", linewidth=3, label=\"val\")\n    plt.legend(loc='best')\n    plt.title(model)\n    return plt","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:32:42.099681Z","iopub.execute_input":"2022-02-15T19:32:42.099973Z","iopub.status.idle":"2022-02-15T19:32:42.108949Z","shell.execute_reply.started":"2022-02-15T19:32:42.099941Z","shell.execute_reply":"2022-02-15T19:32:42.107966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin_reg = LinearRegression()\nplt = plot_learning_curves(lin_reg, X_prepared, y_train, X_prepared_val, y_val)\nplt.ylim(0, 0.25) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:34:50.05774Z","iopub.execute_input":"2022-02-15T19:34:50.058468Z","iopub.status.idle":"2022-02-15T19:35:17.393789Z","shell.execute_reply.started":"2022-02-15T19:34:50.058426Z","shell.execute_reply":"2022-02-15T19:35:17.392819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Learning Curve Conclusion for Linear Regression \n\n- `train` curve starts with really low RMSE due to the model overfitting on the small number of instances. Conversely, `val` curve (highest value cut off) starts with an extremely high RMSE which is expected since the model is overfit on the small number of training instances. \n- The Linear Regression model is underfitting as increasing the number of instances does not improve the model's performance much on either dataset. \n- Furthermore, the model is not overfitting as the MAE error observed for both the validation and training sets is extremely close (errors would be different between `train` and `val` datasets if the model were overfitting with the `train` dataset, thus showing lower MAE values.  \n\n**As expected, Simple Linear Regression is unable to handle the intricacies of the training data and thus is a high bias model which is underfitting.**","metadata":{}},{"cell_type":"markdown","source":"## 1b) Ridge and Lasso Regression \n\n- **Ridge Regression (L2 norm)**: A regularization term equal to $\\alpha*\\Theta_{j}^2$ is added to the cost function where $\\Theta_{j}$ are the model weights and $\\alpha$ is a hyperparameter that controls how much the model is regularized. \n- **Lasso Regression (L1 norm)**: A regularization term equal to $\\alpha*\\lvert \\Theta_{j} \\rvert$ is added to the cost function where $\\Theta_{j}$ are the model weights and $\\alpha$ is a hyperparameter that controls how much the model is regularized. \n\nSince the Ridge and Lasso models' performance is highly affected by the chosen `alpha` value, we need to tune its value to find the optimal `alpha` for the given problem. \n\n**NOTE**: **In this case, any of the Regularized Linear Models will NOT perform any better than the Simple Linear Regression. Regularized models help reduce overfitting and we have already established that our current Linear Model is underfitting. Thus, the following models are being explored merely for LEARNING purposes.**","metadata":{}},{"cell_type":"code","source":"# Set of alpha values to test\nalphas = np.logspace(0,2,20)\nprint(f\"Testing with alphas={alphas}\")\n\n# Tune Ridge Regression\nridgecv = RidgeCV(alphas=alphas)\nridgecv.fit(X_prepared, y_train)\nprint(f\"Best Ridge Alpha: {ridgecv.alpha_}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:33:09.843725Z","iopub.execute_input":"2022-02-15T19:33:09.84434Z","iopub.status.idle":"2022-02-15T19:33:13.16208Z","shell.execute_reply.started":"2022-02-15T19:33:09.844293Z","shell.execute_reply":"2022-02-15T19:33:13.160959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set of alpha values to test\nalphas = np.logspace(1,5,20)\nprint(f\"Testing with alphas={alphas}\")\n\n# Tune Lasso Regression\nlassocv = LassoCV(alphas=alphas)\nlassocv.fit(X_prepared, y_train)\nprint(f\"Best Lasso Alpha: {lassocv.alpha_}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:33:13.163695Z","iopub.execute_input":"2022-02-15T19:33:13.164335Z","iopub.status.idle":"2022-02-15T19:33:17.250257Z","shell.execute_reply.started":"2022-02-15T19:33:13.164285Z","shell.execute_reply":"2022-02-15T19:33:17.249388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The extremely high value of `alpha` for Lasso regression above will regularize the model heavily and push coefficients to zero.**","metadata":{}},{"cell_type":"code","source":"# Fit using optimal alpha\nridge = Ridge(alpha=ridgecv.alpha_)\nridge.fit(X_prepared, y_train)\n\nlasso = Lasso(alpha=lassocv.alpha_)\nlasso.fit(X_prepared, y_train)\n\n# get_eval_metrics([ridge, lasso], X_prepared_val, y_val)\nget_cross_val_scores([ridge, lasso], X_prepared, y_train, cv=5)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:33:17.251687Z","iopub.execute_input":"2022-02-15T19:33:17.252101Z","iopub.status.idle":"2022-02-15T19:33:43.27882Z","shell.execute_reply.started":"2022-02-15T19:33:17.252056Z","shell.execute_reply":"2022-02-15T19:33:43.277641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning Curves for Ridge and Lasso\nridge = Ridge(alpha=ridgecv.alpha_)\nplt = plot_learning_curves(ridge, X_prepared, y_train, X_prepared_val, y_val)\nplt.show()\n\nlasso = Lasso(alpha=lassocv.alpha_)\nplt = plot_learning_curves(lasso, X_prepared, y_train, X_prepared_val, y_val)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:33:43.280814Z","iopub.execute_input":"2022-02-15T19:33:43.281593Z","iopub.status.idle":"2022-02-15T19:34:43.251473Z","shell.execute_reply.started":"2022-02-15T19:33:43.281538Z","shell.execute_reply":"2022-02-15T19:34:43.250585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As expected, even the regularized Linear Models do not perform any better. Thus, we need to consider more complex algorithms which make fewer assumptions to try and understand the relationship between the features and target variable.** ","metadata":{}},{"cell_type":"markdown","source":"# 2) Decision Trees & Random Forest Models\n\nNext, we'll consider the use of tree-based algorithms. Two of the popular tree-based algorithms are: \n- **Decision Tree**: Powerful model capable of finding complex nonlinear relationships in the data.\n- **Random Forest**: Train many Decision Tress on random subsets of the features via the bagging method (Ensemble Learning).\n \nTo save time, `DecisionTreeRegressor` will be skipped as a candidate due to its tendency to easily overfit and yield a model with high variance despite low bias. **Instead, `RandomForestRegressor` is a better option as it is an ensemble of Decision Trees, generally trained via the bagging method (sampling the dataset with replacement).** \n\n**Random Forest introduces extra randomness when growing trees by searching for the best feature among a *random subset of features* instead of *the very best feature at every node*. \nThis yields in greater tree diversity, which trades a higher bias for a lower variance, generally resulting in a better overall model.**","metadata":{}},{"cell_type":"code","source":"X_prepared = X_train.copy()\nX_prepared_val = X_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:07:32.170886Z","iopub.execute_input":"2022-02-16T04:07:32.171176Z","iopub.status.idle":"2022-02-16T04:07:32.232879Z","shell.execute_reply.started":"2022-02-16T04:07:32.171146Z","shell.execute_reply":"2022-02-16T04:07:32.231692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Dropper Pipeline\nrf_drop_vars = ['parcelid']\nfeature_dropper = FeatureDropper(features_to_drop=rf_drop_vars)\n\n# Create Date Features\ndate_feat_creator = CreateDateFeatures()\nfinal_column_names = list(X_train.columns) + ['transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter']\n\n# Univariate Impute Pipeline\nrf_impute_median_vars = [\n    \"lotsizesquarefeet\", \"bathroomcnt\", \"bedroomcnt\", \"calculatedfinishedsquarefeet\", \"structuretaxvaluedollarcnt\", \n    \"landtaxvaluedollarcnt\", \"latitude\", \"longitude\", \"finishedsquarefeet13\", \"finishedsquarefeet15\", \n    \"finishedfloor1squarefeet\", \"finishedsquarefeet50\", \"finishedsquarefeet6\", \"threequarterbathnbr\", \n    \"calculatedbathnbr\", \"fullbathcnt\", \"numberofstories\", \"finishedsquarefeet12\", \"taxvaluedollarcnt\", \n    \"taxamount\", \"roomcnt\", \"basementsqft\", \"yardbuildingsqft26\", \"unitcnt\"\n]\nrf_impute_0_vars = [\"yardbuildingsqft17\", \"fireplacecnt\", \"poolcnt\", \"garagecarcnt\", \"garagetotalsqft\", \"poolsizesum\"]\nrf_cat_vars = [\n    'airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid', \n    'decktypeid', 'fips',  'hashottuborspa', 'heatingorsystemtypeid',  'pooltypeid10', 'pooltypeid2', \n    'pooltypeid7', 'propertycountylandusecode', 'propertylandusetypeid', 'propertyzoningdesc', \n    'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', \n    'storytypeid',  'typeconstructiontypeid', 'fireplaceflag',  'assessmentyear', \n    'taxdelinquencyflag', 'taxdelinquencyyear', 'censustractandblock', 'yearbuilt',\n    'transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter',\n]\n\nunivariate_impute_pipe = ColumnTransformer([\n    (\"rf_impute_cats\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\"), rf_cat_vars),\n    (\"rf_impute_0_vars\", SimpleImputer(strategy=\"constant\", fill_value=0), rf_impute_0_vars),\n    (\"rf_impute_median\", SimpleImputer(strategy=\"median\"), rf_impute_median_vars),\n],\n    remainder='passthrough'\n)\n\n# Column Names Appender \ncol_name_appender = ColumnNamesAppender(univariate_impute_pipe, final_column_names, num_transformers=3)\n\n# Convert to Cat + One-Hot Encoding\nconvert_to_cat = ConvertToType(var_type='str', vars_to_convert=rf_cat_vars)\nfeature_encoder = ColumnTransformer([\n    (\"ohe_cats\", OneHotEncoder(handle_unknown='ignore'), rf_cat_vars)\n],\n    remainder='passthrough'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:07:32.234546Z","iopub.execute_input":"2022-02-16T04:07:32.234891Z","iopub.status.idle":"2022-02-16T04:07:32.247201Z","shell.execute_reply.started":"2022-02-16T04:07:32.234857Z","shell.execute_reply":"2022-02-16T04:07:32.24617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_preprocessor = Pipeline([\n    ('date_feat_creator', date_feat_creator),\n    ('feature_dropper', feature_dropper),\n    ('univariate_impute_pipe', univariate_impute_pipe),\n    ('col_name_appender', col_name_appender),\n    ('convert_to_cat', convert_to_cat),\n    ('feature_encoder', feature_encoder),\n])\n\ndata_prep_pipe = rf_preprocessor.fit(X_prepared)\nX_prepared = rf_preprocessor.transform(X_prepared)\nX_prepared_val = rf_preprocessor.transform(X_prepared_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:07:32.248898Z","iopub.execute_input":"2022-02-16T04:07:32.249326Z","iopub.status.idle":"2022-02-16T04:07:48.249517Z","shell.execute_reply.started":"2022-02-16T04:07:32.249284Z","shell.execute_reply":"2022-02-16T04:07:48.248451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_rf = X_prepared.copy()\nX_rf_val = X_prepared_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:57.626695Z","iopub.execute_input":"2022-02-15T20:39:57.627006Z","iopub.status.idle":"2022-02-15T20:39:57.65362Z","shell.execute_reply.started":"2022-02-15T20:39:57.626975Z","shell.execute_reply":"2022-02-15T20:39:57.652626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning\n\nFor random forests and tree-based models in general, overfitting on the training set happens very easily given the very few assumptions Decision Trees make about the training data. Thus, such models are often called a **non-parametric model** because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a **parametric model**, such as a linear model, has a predetermined number of parameters, limiting its degrees of freedome and reducing the risk of overfiting. \n\nThus, to avoid overfitting the training data, we will be looking at the following hyperparameters: \n\n- ``max_depth``: To restrict the maximum number of levels of the Decision Tree (unlimited depth by default). \n- ``max_features``: Number of features to consider at every split (default is all features)\n- ``min_samples_split``: The minimum number of samples a node must have before it can be split (2 by default)\n- ``min_samples_leaf``: The minimum number of samples a leaf node must have (1 by default)\n\n**Increasing ``min_*`` hyperparameters or reducing ``max_*`` hyperparameters will regularize the model.**\n\nTo reduce the overall runtime of training a random forest, we will be looking at the following hyperparameters: \n- ``max_samples``: Fraction of original dataset given to any individual tree. Given the massive training dataset in our case, the full dataset is overkill when training each tree (default is full dataset).\n- ``n_estimators``: Number of trees in random forest (default=100)","metadata":{}},{"cell_type":"markdown","source":"### Guide to Tuning Random Forests\n\nWe will follow the following strategy to tune random forest given the high training time due to large sample size. Luckily, Random Forests perform fairly well out of the box and do not require a large amount of hyperparameter tuning unlike XGBoost for example. \n\n1. Use the default ``n_estimators`` and low ``max_samples`` until all other hyperparamters are tuned. \n    - This helps reduce the model training time until the final step at which point both hyperparameters can be increased to test any performance improvement. \n    - For this problem, we'll be using the default ``n_estimators`` (100) and ``max_samples=0.1`` i.e. 10 per cent of the data samples given the extremely large data size. \n2. Focus on tuning ``max_depth`` and ``max_features`` \n   - ``max_depth`` helps regularize the model to prevent overfitting and ``max_features`` leads to trees that are more random by picking a random subset of features at every split.\n3. Finally, briefly tune ``min_samples_split`` or ``min_samples_leaf`` which can provide a minor boost in performance through greater regularization\n4. After tuning the hyperparameters in steps 2 and 3, test increasing both ``n_estimators`` and ``max_samples`` to find values at which the model performance plateaus. ","metadata":{}},{"cell_type":"markdown","source":"### Random Search + Grid Search Results \n\nBelow are the results from using Random Search in combination with Grid Search to arrive at the optimized hyperparameters. Sample code for each method is shown in cells below. \n\n1) ``Random Search with max_samples=0.1``\n\n        Random Grid with 20 different combinations = \n        'max_depth': [5, 16, 27, 38, 50, None],\n        'max_features': ['auto', 0.6, 0.7, 0.8],\n        'min_samples_leaf': [32, 64, 128, 256],\n        \n        Best Params = \n        {'max_depth': 50,\n         'max_features': 0.6,\n         'min_samples_leaf': 32,}\n\n2) ``Grid Search with max_samples=0.1``\n\n        Grid = \n        {'max_features': ['auto', 0.5, 0.6, 0.8, 0.9],\n         'max_depth': [10, 40, 70, 100, None],\n         'min_samples_leaf': [16, 32, 64],}\n\n        Best Params: \n        {'max_depth': 70,\n         'max_features': 0.6,\n         'min_samples_leaf': 32,}\n\n3) ``Final Grid Search``: Tune ``n_estimators`` and ``max_samples``\n\n        Grid = \n        {'max_features': [0.6],\n         'max_depth': [70],\n         'min_samples_leaf': [32]\n         'max_samples': [0.1, 0.3, 0.5],\n         'n_estimators': [100, 400, 600]}\n\n        Best Params: \n        {'n_estimators': 400,\n         'max_samples': 0.3,\n         'max_depth': 70,\n         'max_features': 0.6,\n         'min_samples_leaf': 32,}\n         \n**Given that Randon Forest are already great out-of-the-box and the increasingly long training times, there is no benefit in further optimizing the hyperparameters for the Random Forest Regressor.**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Fraction of original dataset given to any individual tree\nmax_samples = [0.1, 0.2]\n# Number of features to consider at every split\nmax_features = ['auto', 0.4, 0.6, 0.8, 0.9]\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(50, 100, num = 4)]\n# max_depth.append(None)\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [16, 32, 48]\n\n# Create the tuning grid\nparam_grid = {\n    'max_samples': max_samples,\n    'max_features': max_features,\n    'max_depth': max_depth,\n    'min_samples_leaf': min_samples_leaf,\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:39:57.655353Z","iopub.execute_input":"2022-02-15T20:39:57.655675Z","iopub.status.idle":"2022-02-15T20:39:57.662364Z","shell.execute_reply.started":"2022-02-15T20:39:57.655641Z","shell.execute_reply":"2022-02-15T20:39:57.661461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Random Grid Sample Code #######\n# Use the random grid to narrow down best hyperparameters\n\n# First create the base model to tune\nrf = RandomForestRegressor(max_samples=0.1)\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 20 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=20, cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n# Fit the random search model\nrf_random.fit(X_prepared, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###### Grid Search Sample Code #######\n# Use the Grid search to further narrown down best hyperparameters\n\n# First create the base model to tune\nrf = RandomForestRegressor()\n\n# Grid search of parameters, using 3 fold cross validation\nrf_grid = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n\n# Fit the random search model\nrf_grid.fit(X_prepared, y_train)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output the best estimator from grid search and it's hyperparameters \nbest_rf = rf_grid.best_estimator_\nbest_rf.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-01-10T22:26:00.603215Z","iopub.execute_input":"2022-01-10T22:26:00.603511Z","iopub.status.idle":"2022-01-10T22:26:00.611652Z","shell.execute_reply.started":"2022-01-10T22:26:00.603481Z","shell.execute_reply":"2022-01-10T22:26:00.609897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Random Forest Model\n\nBased on the iterative hyperparameter tuning earlier, the following Random Forest Regressor is the optimized model. Furthermore, Random Forests have a great quality to measure the relative importance of each feature which is discussed in greater detail in the **Model Interpretability** section later in the notebook. ","metadata":{}},{"cell_type":"code","source":"# BEWARE: With high max_samples and 400 n_estimators, this code takes almost hour to run\nbest_params = {\n    'n_estimators': 400,\n    'max_samples': 0.3,\n    'max_depth': 70,\n    'max_features': 0.6,\n    'min_samples_leaf': 32,    \n    'random_state': 42, \n}\n\nran_forest = RandomForestRegressor(**best_params)\nran_forest.fit(X_prepared, y_train)\n\nget_eval_metrics([ran_forest], X_prepared_val, y_val)\n# get_cross_val_scores([ran_forest], X_prepared, y_train, cv=3)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:07:48.25092Z","iopub.execute_input":"2022-02-16T04:07:48.251236Z","iopub.status.idle":"2022-02-16T04:07:49.231951Z","shell.execute_reply.started":"2022-02-16T04:07:48.251202Z","shell.execute_reply":"2022-02-16T04:07:49.231148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extra Trees Regressor\n \nRandom Forest searches for the best possible threholds when splitting at each node. However, it is possible to make trees even more random by also using random thresholds for each feature instead of best possible threshold. \n\n**A forest of such extremely random trees is called an Extremely Randomized Trees ensemble (or ``Extra-Trees``). This technique trades more bias for a lower variance and also makes Extra-Trees faster to train than regular Random Forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks.**\n\nIn this case, `Extra-Trees` did not perform much better than `Random Forest` and the only way to know which one is better for any problem is to test both algorithms. ","metadata":{}},{"cell_type":"code","source":"# WARNING: With high max_samples and high n_estimators, this code takes almost 2 hours to run\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n# Using the best parameters from Random Forest Regressor\nbest_params = {\n    'n_estimators': 100,\n    'max_samples': 0.3,\n    'max_depth': 70,\n    'max_features': 0.6,\n    'min_samples_leaf': 32,    \n    'random_state': 42, \n}\n\nextra_trees_reg = ExtraTreesRegressor(**best_params)\nextra_trees_reg.fit(X_prepared, y_train)\n# get_cross_val_scores([extra_trees_reg], X_prepared, y_train, cv=3)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T21:29:33.166051Z","iopub.execute_input":"2022-02-06T21:29:33.166351Z","iopub.status.idle":"2022-02-07T00:11:12.881295Z","shell.execute_reply.started":"2022-02-06T21:29:33.166323Z","shell.execute_reply":"2022-02-07T00:11:12.879011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Support Vector Machines (SVM)\n\nThe next algorithm to consider is Support Vector Machines which can be used for both classification and regression problems. \n\nIn a classification problem, the objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points. Conversely, in a regression problem, the objective is reversed: trying to find a hyperplane that fits as many instances as possible while limiting margin violations. \n\n**If you're new to SVM and wish to understand it better, I would highly recommend this [video by StatQuest](https://www.youtube.com/watch?v=efR1C6CvhmE) to understand the main ideas of the algorithm.** \n\n### SVM Advantages\n \n- Very versatile technique with linear and various non-linear kernels \n- Prediction is a SMOOTHLY varying function of the features unlike tree-based algorithms that are step-wise\n- Works best on small to medium-sized datasets and even effective in cases where number of features is greater than the number of samples.\n\n### SVM Disadvantages \n\n**Despite the advantages highlighted earlier, SVM is not a good choice for this Zillow Regression problem because:**\n- SVM's training time increases non-linearly with the number of instances with a time complexity of $O(m^2*n)$ to $O(m^3* n)$ where `m` is the number of instances and `n` is the number of features. This is because SVMs have to estimate at least one parameter for each row in the training data! \n- **Given that the Zillow training data is more than 1e5 points, the training time is extremely long and the results of the base model are much worse than other tested models. Even if there is scope of improvement with hyperparameter tuning through grid search, the amount of training time is unjustified.** \n\nSample code for the base SVM models with the different tested kernels is available in cells before for reference. ","metadata":{}},{"cell_type":"code","source":"# Linear SVM Regression \n# Trains extremely quickly but not useful as the Zillow dataset is not linearly separable\nfrom sklearn.svm import LinearSVR\n\nsvm_reg_linear = LinearSVR(random_state=42)\nsvm_reg_linear.fit(X_prepared, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T23:33:53.254185Z","iopub.execute_input":"2022-01-15T23:33:53.255042Z","iopub.status.idle":"2022-01-15T23:35:09.692567Z","shell.execute_reply.started":"2022-01-15T23:33:53.255002Z","shell.execute_reply":"2022-01-15T23:35:09.691629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVM with Polynomial Kernel - 'transforms' data points to polynomial space using 'kernel trick'\nfrom sklearn.svm import SVR\nsvm_poly_reg = SVR(kernel=\"poly\")\nsvm_poly_reg.fit(X_prepared, y_train)\n\n# SVM with Radial Basis Function (RBF) Kernel \nrbf_svm = SVR(kernel = 'rbf')\nrbf_svm.fit(X_prepared, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T23:35:09.694519Z","iopub.execute_input":"2022-01-15T23:35:09.695381Z","iopub.status.idle":"2022-01-15T23:36:55.985648Z","shell.execute_reply.started":"2022-01-15T23:35:09.695327Z","shell.execute_reply":"2022-01-15T23:36:55.984728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_eval_metrics([svm_reg_linear, svm_poly_reg, rbf_svm], X_prepared_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T23:36:55.987595Z","iopub.execute_input":"2022-01-15T23:36:55.988207Z","iopub.status.idle":"2022-01-15T23:37:02.958259Z","shell.execute_reply.started":"2022-01-15T23:36:55.988164Z","shell.execute_reply":"2022-01-15T23:37:02.957334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Gradient Boosting Machines","metadata":{}},{"cell_type":"markdown","source":"The final model we'll be exploring is Gradient Boosting Machines (GBMs) which are extremely powerful algorithms for both regression and classification problems. \n\n**Whereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow trees in sequence with each tree learning and improving on the previous one. Although shallow trees by themselves are rather weak predictive models, they can be “boosted” to produce a powerful “committee” that, when appropriately tuned, is often hard to beat with other algorithms.**\n\nIn this section, we will be working with a few different implementations of Gradient Boost: \n- **XGBoost**: Stands for \"eXtreme Gradient Boosting\" and came out in 2015. It implemented a number of theoretical and practical improvements, many of which have become standard in subsequent packages. \n- **LightGBM**: Written by researchers at Microsoft Research and released in 2017. Major advances included smarter techniques (by default) to avoid searching all splits and speeding things up along with other improvements in handling categorical variables. \n- **CatBoost**: Written by group of researchers at Yandex and its major advance was allowing native handling of categorical variables (as strings) by doing on-the-fly numerical encoding of categorical variables, with a clever encoding scheme to reduce overfitting. \n\n**If you're unfamiliar with how Gradient Boost works, I highly recommend the following:**\n- StatQuest's [Gradient Boost youtube playlist](https://www.youtube.com/playlist?list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6).\n- StatQuest's [XGBoost youtube playlist](https://www.youtube.com/playlist?list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ).\n\n### Gradient Boosting vs Random Forest\n- Typically, Gradient Boosting can outperform Random Forests by a small, but not insignificant amount.\n- However, Gradient Boosting requires much more \"parameter tuning\" to get the best performance\n- For Random Forest, can usually use 1K or 2K trees and do well.  The only \"major\" parameter is the `max_features` and the default is usually reasonable.\n- For Gradient Boosting, parameter tuning is considerably more complicated.","metadata":{}},{"cell_type":"markdown","source":"# 4-a) XGBoost \n\nEven though Scikit-Learn has its own implementation of Gradient Boost, it lacks in both performance and speed compared to other implementations. Thus, we will skip Scikit-Learn's implementation in favor of XGBoost, LightGBM, CatBoost.\n\n### Missing Data\n\nOne of most powerful capabilities of Gradient Boosting machines is their ability to handle missing data. They do not require data imputation and are designed to extract as much information as possible from the rows with missing data. ","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:53:17.026593Z","iopub.execute_input":"2022-02-16T02:53:17.027048Z","iopub.status.idle":"2022-02-16T02:53:17.032241Z","shell.execute_reply.started":"2022-02-16T02:53:17.027006Z","shell.execute_reply":"2022-02-16T02:53:17.03124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_prepared = X_train.copy()\nX_prepared_val = X_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:53:17.430552Z","iopub.execute_input":"2022-02-16T02:53:17.430998Z","iopub.status.idle":"2022-02-16T02:53:18.209471Z","shell.execute_reply.started":"2022-02-16T02:53:17.430962Z","shell.execute_reply":"2022-02-16T02:53:18.20843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Prep for XGBoost with Original Data \n\n# Feature Dropper Pipeline\nxgb_drop = ['parcelid']\nfeature_dropper = FeatureDropper(features_to_drop=xgb_drop)\n\n# Convert Year Features Pipeline\nyear_feat_creator = CreateYearFeatures(date_features=date_features)\n\n# Date Feature Creator\ndate_feat_creator = CreateDateFeatures()\n\n# Feature Encoding Pipeline\ncat_vars = ['transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter', \n            'airconditioningtypeid', 'architecturalstyletypeid', 'buildingclasstypeid', 'buildingqualitytypeid', \n            'decktypeid', 'fips',  'hashottuborspa', 'heatingorsystemtypeid',  'pooltypeid10', 'pooltypeid2', \n            'pooltypeid7', 'propertycountylandusecode', 'propertylandusetypeid', 'propertyzoningdesc', \n            'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', \n            'storytypeid',  'typeconstructiontypeid', 'fireplaceflag',  'assessmentyear', \n            'taxdelinquencyflag', 'taxdelinquencyyear', 'censustractandblock',\n           ]\nfeature_encoder = ColumnTransformer([\n    (\"ohe_cats\", OneHotEncoder(handle_unknown='ignore'), cat_vars)\n],\n    remainder='passthrough'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:53:18.2114Z","iopub.execute_input":"2022-02-16T02:53:18.211824Z","iopub.status.idle":"2022-02-16T02:53:18.219347Z","shell.execute_reply.started":"2022-02-16T02:53:18.211778Z","shell.execute_reply":"2022-02-16T02:53:18.218681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_preprocessor = Pipeline([\n    ('feature_dropper', feature_dropper),\n    ('date_feat_creator', date_feat_creator),\n    ('year_feat_creator', year_feat_creator),\n    ('feature_encoder', feature_encoder),\n])\n\ndata_prep_pipe = xgb_preprocessor.fit(X_prepared)\nX_prepared = xgb_preprocessor.transform(X_prepared)\nX_prepared_val = xgb_preprocessor.transform(X_prepared_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:53:19.859128Z","iopub.execute_input":"2022-02-16T02:53:19.85967Z","iopub.status.idle":"2022-02-16T02:53:23.634017Z","shell.execute_reply.started":"2022-02-16T02:53:19.859608Z","shell.execute_reply":"2022-02-16T02:53:23.632924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_xgb = X_prepared.copy()\nX_xgb_val = X_prepared_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:42:58.594906Z","iopub.execute_input":"2022-02-15T21:42:58.59527Z","iopub.status.idle":"2022-02-15T21:42:58.630556Z","shell.execute_reply.started":"2022-02-15T21:42:58.59523Z","shell.execute_reply":"2022-02-15T21:42:58.629635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize XGBoost Model \nparams = {\n    'learning_rate': 0.3,\n    'n_estimators': 10000,\n    'random_state': 42,\n}\n\nxgb_base = xgb.XGBRegressor(**params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T02:53:23.998193Z","iopub.execute_input":"2022-02-16T02:53:23.99864Z","iopub.status.idle":"2022-02-16T02:53:24.004112Z","shell.execute_reply.started":"2022-02-16T02:53:23.998592Z","shell.execute_reply":"2022-02-16T02:53:24.003068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Major Tuning Parameters \n\nThe most importnant parameters to tune for Gradient Boosting are: \n- `max_depth`: How deep to build the trees.  This is a very important parameter and performance may change dramatically for different values.  Larger values are more likely to overfit, smaller values more likely to underfit.  \n\n- `learning_rate`, `n_estimators`: These parameters are also very important and interact highly with one another (and with `max_depth`).  Typically, the smaller your \"step_size\" (learning_rate), the more steps you will need to take to reach maximum performance. However, unlike random forests, if you continue to build trees in boosting, you will start overfitting, and performance (measured on the test set) will get worse.\n\nThe best way to handle this is:\n- Set aside a validation set for early stopping\n- Use a low `learning rate`\n- Use a high `n_estimators` (we will early stop)\n- Stop when performance on the validation set begins to degrade\n    \nXGBoost makes it easy to implement early stopping, as demonstrated below through the use of ``early_stopping_rounds`` and ``eval_set``. ","metadata":{}},{"cell_type":"code","source":"# Fit model using early stopping and validation set \nfit_params={'early_stopping_rounds': 10, \n            'eval_metric': 'mae',\n            'eval_set': [[X_prepared_val, y_val]]}\n\nxgb_base.fit(X_prepared, y_train, **fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T02:53:27.966989Z","iopub.execute_input":"2022-02-16T02:53:27.96758Z","iopub.status.idle":"2022-02-16T02:53:52.710348Z","shell.execute_reply.started":"2022-02-16T02:53:27.967531Z","shell.execute_reply":"2022-02-16T02:53:52.709354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit_params={'early_stopping_rounds': 10, \n            'eval_metric': 'mae',\n            'verbose': False,\n            'eval_set': [[X_prepared_val, y_val]]}\n\n# get_eval_metrics([xgb_base], X_prepared_val, y_val)\nget_cross_val_scores([xgb_base], X_prepared, y_train, cv=3, fit_params=fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T02:53:52.712274Z","iopub.execute_input":"2022-02-16T02:53:52.712724Z","iopub.status.idle":"2022-02-16T02:54:43.189567Z","shell.execute_reply.started":"2022-02-16T02:53:52.712678Z","shell.execute_reply":"2022-02-16T02:54:43.188725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Max Depth Tuning\n\nWith `n_estimators` and `learning_rate` taken care through the use of early stopping and validation, the next main hyperparameter to tune is ``max_depth``. \n\nThe following code tests the impact of different max depth values and the performance seems to be best around a depth of 3.","metadata":{}},{"cell_type":"code","source":"fit_params={\n    'early_stopping_rounds': 10, \n    'eval_metric': 'mae',\n    'verbose': False,            \n    'eval_set': [[X_prepared_val, y_val]],\n}\n\nother_params = {\n    'learning_rate': 0.2,\n    'n_estimators': 10000,\n    'random_state': 42,\n}\n\ndef param_exploration(param_name, param_vals, other_params=other_params, fit_params=fit_params, cross_val=False):\n    \"\"\"\n    Function to explore the impact of different values for ONE parameter on MAE.\n    Tested using either single validation test set or cross validation (cross_val=True)\n    \"\"\"\n    mae_vec = np.zeros(len(param_vals))\n\n    for i, val in enumerate(param_vals):\n        params[param_name] = val\n        xgb_temp = xgb.XGBRegressor(**params)\n        \n        if cross_val:\n            # TESTING WITH CROSS VALIDATION\n            mae = -cross_val_score(xgb_temp, X_prepared, y_train, scoring=\"neg_mean_absolute_error\", cv=3, fit_params=fit_params)\n            mae_vec[i] = mae.mean()\n        else: \n            # TESTING WITH SINGLE VALIDATION SET\n            xgb_temp.fit(X_prepared, y_train, **fit_params)\n            preds = xgb_temp.predict(X_prepared_val)\n            mae_vec[i] = mean_absolute_error(y_val, preds)    \n    \n    plt.plot(param_vals, mae_vec)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:54:43.193779Z","iopub.execute_input":"2022-02-16T02:54:43.194354Z","iopub.status.idle":"2022-02-16T02:54:43.203862Z","shell.execute_reply.started":"2022-02-16T02:54:43.194302Z","shell.execute_reply":"2022-02-16T02:54:43.202735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Max Depth Tuning\n# From initial testing, cross validation was necessary when testing the different values as the model was starting to overfit on the single validation set. \n# BEWARE - TAKES 5-10 MINUTES TO RUN\nmax_depth_vals_vec=list(range(1,7))\nparam_exploration(param_name=\"max_depth\", param_vals=max_depth_vals_vec, cross_val=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:54:43.205198Z","iopub.execute_input":"2022-02-16T02:54:43.205464Z","iopub.status.idle":"2022-02-16T02:58:08.328406Z","shell.execute_reply.started":"2022-02-16T02:54:43.205434Z","shell.execute_reply":"2022-02-16T02:58:08.3274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Other boosting parameters\nThere are other parameters that can yield improvements if tuned appropriately which fall into several groups, and different packages handle them (and name them) differently.\n\n\n#### Data (row) sampling parameters\nHow to choose the data for each tree.  Unlike Random Forest, \"Standard\" boosting just uses the whole data set for each tree.  However, you can choose to sample a fraction of the data to add regularization.\n- subsample, replacement, etc.\n\n#### Feature sampling parameters\nUnlike Random Forest, the default in gradient boosting is to check every feature at every node.  Since boosting is not relying on having uncorrelated trees, it can still give good results without this randomization.  However, randomly sampling features can help regularize, in addition to speeding up training (it is faster to train if you don't check every column every time). XGBoost permits subsampling features per tree, level, *and* node.\n- Examples: colsample_bynode, colsample_bytree, colsample_bylevel\n\n#### Split sampling parameters\nFor each feature, should we check all possible splits, or reduce the number through approximation\n- Examples: tree_method, sketch_eps in XGBoost\n\n#### Regularization parameters\nOther regularization parameters include:\n- minimum \"improvement\" required to make a split\n- \"Shrinkage\" of the leaf values of the trees (as in LASSO / Ridge regression)\n- gamma, reg_lambda, reg_alpha\n\n","metadata":{}},{"cell_type":"markdown","source":"## Manually Exploring Parameters (One by One) \n\n**Warning**: Each of the plots generated below take a few minutes to run. ","metadata":{}},{"cell_type":"code","source":"# Subsample: Fraction of dataset to use to build each tree\nparam_vals_vec=[0.3, 0.5, 0.6, 0.7, 0.8, 0.9, .95, 1]\nparam_exploration(param_name=\"subsample\", param_vals=param_vals_vec, cross_val=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:58:08.330604Z","iopub.execute_input":"2022-02-16T02:58:08.331045Z","iopub.status.idle":"2022-02-16T03:01:20.086378Z","shell.execute_reply.started":"2022-02-16T02:58:08.330998Z","shell.execute_reply":"2022-02-16T03:01:20.085461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ColSample By Tree: Ration of columns chosen when constructing each tree\nparam_vals_vec=[0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nparam_exploration(param_name=\"colsample_bytree\", param_vals=param_vals_vec, cross_val=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:01:20.088015Z","iopub.execute_input":"2022-02-16T03:01:20.088429Z","iopub.status.idle":"2022-02-16T03:03:42.949219Z","shell.execute_reply.started":"2022-02-16T03:01:20.088382Z","shell.execute_reply":"2022-02-16T03:03:42.948228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ColSample By Node: Ratio of columns chosen at each node (split)\nparam_vals_vec=[0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nparam_exploration(param_name=\"colsample_bynode\", param_vals=param_vals_vec, cross_val=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:03:42.952127Z","iopub.execute_input":"2022-02-16T03:03:42.952412Z","iopub.status.idle":"2022-02-16T03:06:14.272576Z","shell.execute_reply.started":"2022-02-16T03:03:42.952383Z","shell.execute_reply":"2022-02-16T03:06:14.271575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reg Lambda: Regularization parameter from Ridge Regression, the larger the value, the more conservative the algorithm\nparam_vals_vec=[1, 2, 3, 5, 10, 20]\nparam_exploration(param_name=\"reg_lambda\", param_vals=param_vals_vec, cross_val=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:06:14.274384Z","iopub.execute_input":"2022-02-16T03:06:14.27472Z","iopub.status.idle":"2022-02-16T03:09:41.006511Z","shell.execute_reply.started":"2022-02-16T03:06:14.274686Z","shell.execute_reply":"2022-02-16T03:09:41.005364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reg Alpha: Regularization parameter from Lasso Regression, the larger the value, the more conservative the algorithm\nparam_vals_vec=[1, 2, 3, 5, 10, 20]\nparam_exploration(param_name=\"reg_alpha\", param_vals=param_vals_vec, cross_val=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:09:41.008321Z","iopub.execute_input":"2022-02-16T03:09:41.008761Z","iopub.status.idle":"2022-02-16T03:14:17.170425Z","shell.execute_reply.started":"2022-02-16T03:09:41.008715Z","shell.execute_reply":"2022-02-16T03:14:17.1694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gamma: Another regularization paramter - the larger the value, the more conservative the algorithm\nparam_vals_vec=[0.1, 0.3, 0.5, 0.7, 1, 2]\nparam_exploration(param_name=\"gamma\", param_vals=param_vals_vec, cross_val=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:14:17.172068Z","iopub.execute_input":"2022-02-16T03:14:17.17246Z","iopub.status.idle":"2022-02-16T03:17:07.394707Z","shell.execute_reply.started":"2022-02-16T03:14:17.172418Z","shell.execute_reply":"2022-02-16T03:17:07.393563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting Parameters in Gradient Boosting\nThere are several approaches to finding the best parameters for your Gradient Boosting Model\n1. Do a massive \"grid search\"\n2. Just play around manually\n3. \"Smart\" parameter search that tries to search promising combinations\n\nThere are drawbacks to all of these:\n- Grid search is extremely time consuming\n- It is difficult to know if you are choosing appropriate parameters and ranges\n- There may be other considerations than just metric performance (model size, coherence, training time)\n- Manual approaches are haphazard, attention consuming\n- Smart approaches are not always as smart as they could be","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:16:19.112038Z","iopub.status.idle":"2022-01-25T03:16:19.112461Z"}}},{"cell_type":"markdown","source":"### Suggested hybrid approach\n1. Use early stopping, low learning rate, high number of trees (to remove learning rate and number of estimators from the search)\n1. Run hyperopt with a few parameters (including max_depth) to \"understand the landscape\"\n1. Iteratively refine your ranges and add more parameters to the search\n","metadata":{}},{"cell_type":"markdown","source":"## Hyperopt Package\n\nInstead of exhaustively searching a huge parameter space, focus the search on \"promising\" areas of the search space (based on findings seen so far).\n\nThe `hyperopt` package is a python library for search spaces optimizing and overall does a reasonably good job of seraching the parameter space. For a detailed tutorial on Hyperopt usage on different Gradient Boost algorithm, refer to this [medium article](https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e). \n","metadata":{}},{"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:17:07.396044Z","iopub.execute_input":"2022-02-16T03:17:07.396334Z","iopub.status.idle":"2022-02-16T03:17:07.85702Z","shell.execute_reply.started":"2022-02-16T03:17:07.396304Z","shell.execute_reply":"2022-02-16T03:17:07.855925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Must define the \"loss function\" which takes a set of parameters and output the loss value","metadata":{}},{"cell_type":"code","source":"def eval_model(params):\n    xgb_base = xgb.XGBRegressor()\n    xgb_base.set_params(**params)\n    \n    # TESTING WITH SINGLE VALIDATION SET\n#     xgb_base.fit(X_prepared, y_train, eval_set=[(X_prepared_val, y_val)], early_stopping_rounds=20,\n#                 eval_metric='mae', verbose=False)\n#     preds = xgb_base.predict(X_prepared_val)\n#     mae = mean_absolute_error(y_val, preds)\n    \n    # TESTING WITH CROSS VALIDATION\n    fit_params={'early_stopping_rounds': 15, \n            'eval_metric': 'mae',\n            'verbose': False,\n            'eval_set': [[X_prepared_val, y_val]]}\n    mae = -cross_val_score(xgb_base, X_prepared, y_train, scoring=\"neg_mean_absolute_error\", cv=3, fit_params=fit_params)\n    mae = mae.mean()\n    \n    return(mae)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:17:07.85836Z","iopub.execute_input":"2022-02-16T03:17:07.858685Z","iopub.status.idle":"2022-02-16T03:17:07.864912Z","shell.execute_reply.started":"2022-02-16T03:17:07.85865Z","shell.execute_reply":"2022-02-16T03:17:07.864129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Next, we define the parameter space.  \n- There are several options to define ranges.\n    - `randint`: chooses a random integer between 0 and an upper bound \n    - `choice`: choose from a set of specific values\n    - `uniform`: choose a (float) in the range\n    - `quniform`: choose a discrete range of floats\n    \nSee more at hyperopt documentation:\nhttp://hyperopt.github.io/hyperopt/getting-started/search_spaces/","metadata":{"execution":{"iopub.status.busy":"2022-01-25T03:22:02.294896Z","iopub.execute_input":"2022-01-25T03:22:02.295257Z","iopub.status.idle":"2022-01-25T03:22:02.302203Z","shell.execute_reply.started":"2022-01-25T03:22:02.295227Z","shell.execute_reply":"2022-01-25T03:22:02.301352Z"}}},{"cell_type":"code","source":"fspace1 = {\n    'max_depth':hp.randint('max_depth', 7), # goes up to 6-1\n    'colsample_bynode': hp.uniform('colsample_bynode',.3,1),\n    'subsample': hp.quniform('subsample',.3, 1,.05),\n    'gamma':hp.uniform('gamma',.2, 5),\n    'tree_method': hp.choice('tree_method', ['approx', 'exact']),\n    'n_estimators': 10000,\n    'learning_rate': 0.15,\n    'random_state': 42,\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:17:07.86614Z","iopub.execute_input":"2022-02-16T03:17:07.866651Z","iopub.status.idle":"2022-02-16T03:17:07.87982Z","shell.execute_reply.started":"2022-02-16T03:17:07.866595Z","shell.execute_reply":"2022-02-16T03:17:07.878836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finally, we use the `fmin` function and record our trials in a `Trials` object\n# BEWARE: THIS CODE TOOK 7+ HOURS TO RUN WHEN PERFORMING 1200 TRIALS\ntrials1 = Trials()\nbest_param = fmin(fn=eval_model, space=fspace1, algo=tpe.suggest, max_evals=10, trials=trials1)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:17:07.88123Z","iopub.execute_input":"2022-02-16T03:17:07.88182Z","iopub.status.idle":"2022-02-16T03:25:31.53747Z","shell.execute_reply.started":"2022-02-16T03:17:07.881768Z","shell.execute_reply":"2022-02-16T03:25:31.536653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_param","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:25:31.538984Z","iopub.execute_input":"2022-02-16T03:25:31.539564Z","iopub.status.idle":"2022-02-16T03:25:31.545821Z","shell.execute_reply.started":"2022-02-16T03:25:31.539522Z","shell.execute_reply":"2022-02-16T03:25:31.545059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The individual trials can be explored to detect trends within specific parameters \n","metadata":{}},{"cell_type":"code","source":"# Extract data from hyperopt trials \nmax_depth_trial_vals = np.array([t['misc']['vals']['max_depth'] for t in trials1.trials])\nss_trial_vals = np.array([t['misc']['vals']['subsample'] for t in trials1.trials])\ngamma_trial_vals = np.array([t['misc']['vals']['gamma'] for t in trials1.trials])\nloss_trial_vals = np.array([t['result']['loss'] for t in trials1.trials])\ntrial_nums = np.array([t['tid'] for t in trials1.trials])","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:25:31.547087Z","iopub.execute_input":"2022-02-16T03:25:31.547601Z","iopub.status.idle":"2022-02-16T03:25:31.557965Z","shell.execute_reply.started":"2022-02-16T03:25:31.547566Z","shell.execute_reply":"2022-02-16T03:25:31.557191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot trends between different parameter values and corresponding losses. \n\nFor example, the ``max_depth`` plot shows decreasing losses with an increase in depth which plateaus are a certain value - due to Kaggle notebook refreshing at startup, the plot for 1200 trials experiment in unavailable anymore :(\n","metadata":{}},{"cell_type":"code","source":"# Plot trends between different parameter values and corresponding losses \nplt.scatter(max_depth_trial_vals, loss_trial_vals, alpha=.2)\n# plt.scatter(ss_trial_vals, loss_trial_vals, alpha=.2)\n# plt.scatter(gamma_trial_vals, loss_trial_vals, alpha=.2)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:25:31.559204Z","iopub.execute_input":"2022-02-16T03:25:31.559603Z","iopub.status.idle":"2022-02-16T03:25:31.845939Z","shell.execute_reply.started":"2022-02-16T03:25:31.559572Z","shell.execute_reply":"2022-02-16T03:25:31.844761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Params Model \n\nBased on the 1200 ``hyperopt`` trials, the following hyperparameter values had the best loss: \n```\n{'colsample_bynode': 0.382,\n 'gamma': 0.201,\n 'max_depth': 5,\n 'subsample': 0.95,\n 'tree_method': 'exact'}\n```\n\nThe other important parameters (``learning_rate``, ``n_estimators``) were already set. ","metadata":{}},{"cell_type":"code","source":"fit_params={\n    'early_stopping_rounds': 15, \n    'eval_metric': 'mae',\n    'verbose': False,\n    'eval_set': [[X_prepared_val, y_val]],\n}\n\nbest_params = {\n    'colsample_bynode': 0.75,   \n    'gamma': 0.201,\n    'max_depth': 5,   \n    'subsample': 0.95, \n    'tree_method': 'exact',\n    'learning_rate': 0.01,\n    'n_estimators': 10000,\n    'random_state': 42,\n}\n\nxgb_base = xgb.XGBRegressor(**best_params)\nxgb_base.fit(X_prepared, y_train, **fit_params)\nget_cross_val_scores([xgb_base], X_prepared, y_train, cv=3, fit_params=fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T03:25:31.847745Z","iopub.execute_input":"2022-02-16T03:25:31.848197Z","iopub.status.idle":"2022-02-16T03:45:26.262281Z","shell.execute_reply.started":"2022-02-16T03:25:31.848148Z","shell.execute_reply":"2022-02-16T03:45:26.26132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4-b) LightGBM\n\nLightGBM came out around 2017 and is written by researchers at Microsoft Research. Its major advances include: \n\n- Smarter techniques to avoid searching all splits and speeding things up (ignoring small gradients to reduce data at each iteration)\n- \"looks for\" mutually exclusive features to handle them better\n- Instead of building decision trees one level at a time like XGBoost, LightGBM uses a leaf-wise tree growth approach which results in fewer unnecesary nodes. \n\nGiven the already long length of this notebook, I will not go into too many detailed explanations about LightGBM. **However, from my limited time using LightGBM, I am already enjoying working with it more than XGBoost - primarily due to the performance boost when training which feels MAGNITUDES faster than XGBoost. The performance boost does NOT come with a tradeoff in model accuray - LightGBM basic model is performing better than the fully tuned XGBoost model.** \n\n### LightGBM Resources: \n- [Intro to LightGBM and Performance vs XGBoost](https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997)\n- [Understanding LightGBM Parameters - Detailed Guide](https://neptune.ai/blog/lightgbm-parameters-guide) \n- [Guide to tuning LightGBM using Optuna](https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)\n","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgbm\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\nimport optuna  # !pip install optuna\nfrom optuna.integration import LightGBMPruningCallback","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:45:26.267687Z","iopub.execute_input":"2022-02-16T03:45:26.268025Z","iopub.status.idle":"2022-02-16T03:45:26.27319Z","shell.execute_reply.started":"2022-02-16T03:45:26.26799Z","shell.execute_reply":"2022-02-16T03:45:26.272157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_prepared = X_train.copy()\nX_prepared_val = X_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:45:26.275343Z","iopub.execute_input":"2022-02-16T03:45:26.275981Z","iopub.status.idle":"2022-02-16T03:45:26.314662Z","shell.execute_reply.started":"2022-02-16T03:45:26.275933Z","shell.execute_reply":"2022-02-16T03:45:26.313559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvertToCategorical(BaseEstimator, TransformerMixin): \n    '''\n    Pipeline to convert categorical variables to Pandas Category type. \n    This pipeline is specific to LGBM which handles categorical vars differently.\n    '''\n    def __init__(self, cat_vars):\n        self.cat_vars = cat_vars\n        \n    def fit(self, X, y=None): \n        return self  # Nothing else to do \n    \n    def transform(self, X): \n        for col in self.cat_vars: \n            X[col] = pd.Categorical(X[col])\n        return X\n      \n# Code to test pipeline\n# convert_to_cat = ConvertToCategorical(cat_vars=cat_vars)\n# X_temp = convert_to_cat.fit_transform(X_temp)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:45:26.316311Z","iopub.execute_input":"2022-02-16T03:45:26.316758Z","iopub.status.idle":"2022-02-16T03:45:26.324653Z","shell.execute_reply.started":"2022-02-16T03:45:26.316707Z","shell.execute_reply":"2022-02-16T03:45:26.323412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Dropper Pipeline \nvar_drop = ['parcelid',]\nfeature_dropper = FeatureDropper(features_to_drop=var_drop)\n\n# Convert Date Features Pipeline\nyear_feat_creator = CreateYearFeatures(date_features=date_features)\n\n# Date Feature Creator\ndate_feat_creator = CreateDateFeatures()\n\n# Derived Features Creator\nderived_feat_creator = CreateDerivedFeatures()\n\n# Feature Encoding Pipeline\ncat_vars = ['transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', \n            'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', \n            'regionidneighborhood', 'regionidzip', 'assessmentyear', 'typeconstructiontypeid', 'architecturalstyletypeid', \n            'buildingclasstypeid', 'pooltypeid2', 'pooltypeid7', 'storytypeid',  'hashottuborspa', 'pooltypeid2', \n            'taxdelinquencyyear',  'taxdelinquencyflag', 'fireplaceflag', 'decktypeid', 'pooltypeid10', \n            'propertycountylandusecode', 'propertyzoningdesc', 'rawcensustractandblock', 'censustractandblock'\n           ]\n# Feature Encoding Pipeline\nfeature_encoder = ColumnTransformer([\n    (\"ohe_cats\", OneHotEncoder(handle_unknown='ignore'), cat_vars)],\n    remainder='passthrough'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:45:26.326349Z","iopub.execute_input":"2022-02-16T03:45:26.326828Z","iopub.status.idle":"2022-02-16T03:45:26.337497Z","shell.execute_reply.started":"2022-02-16T03:45:26.326777Z","shell.execute_reply":"2022-02-16T03:45:26.336386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LightGBM Categorical Handling \n\nLightGBM handles categorical variables different from other models which mainly use One-Hot Encoding. It handles categorical variables by ranking their marginal target value in each node. \n\nFor this problem, we tested using both One-Hot Encoding and LightGBM's approach and decided to use OHE which performed much better. \n","metadata":{}},{"cell_type":"code","source":"lgbm_preprocessor = Pipeline([\n    ('derived_feat_creator', derived_feat_creator),\n    ('feature_dropper', feature_dropper),\n    ('date_feat_creator', date_feat_creator),\n    ('year_feat_creator', year_feat_creator),\n    ('feature_encoder', feature_encoder),\n])\n\ndata_prep_pipe = lgbm_preprocessor.fit(X_prepared)\nX_prepared = lgbm_preprocessor.transform(X_prepared)\nX_prepared_val = lgbm_preprocessor.transform(X_prepared_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:45:26.338983Z","iopub.execute_input":"2022-02-16T03:45:26.339299Z","iopub.status.idle":"2022-02-16T03:45:30.394139Z","shell.execute_reply.started":"2022-02-16T03:45:26.339266Z","shell.execute_reply":"2022-02-16T03:45:30.392769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_lgbm = X_prepared.copy()\nX_lgbm_val = X_prepared_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:50:48.345227Z","iopub.execute_input":"2022-02-15T21:50:48.345722Z","iopub.status.idle":"2022-02-15T21:50:48.386319Z","shell.execute_reply.started":"2022-02-15T21:50:48.34567Z","shell.execute_reply":"2022-02-15T21:50:48.385094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize LightGBM Model \nparams = {\n    'n_estimators': 10000,\n    'random_state': 42\n}\n\n# Fit model using early stopping and validation set \nfit_params={'early_stopping_rounds': 30, \n            'eval_metric': 'mae',\n            'verbose': False,\n            'eval_set': [[X_prepared_val, y_val]]\n           }\n\nlgbm_base = lgbm.LGBMRegressor(**params)\nlgbm_base.fit(X_prepared, y_train, **fit_params)\nget_cross_val_scores([lgbm_base], X_prepared, y_train, cv=3, fit_params=fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T03:45:30.395735Z","iopub.execute_input":"2022-02-16T03:45:30.396201Z","iopub.status.idle":"2022-02-16T03:45:46.522468Z","shell.execute_reply.started":"2022-02-16T03:45:30.396154Z","shell.execute_reply":"2022-02-16T03:45:46.521374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM Hyperparameter Tuning (Using Optuna)\n\nBy the time I reached LightGBM for this project, I encountered another hyperparameter tuning package called ``Optuna`` which is much newer and more maintained than ``hyperopt``. Thus, I decided to use Optuna for LightGBM tuning. Optuna claims to perform better tuning than TPE (Tree-structured Parzem Estimator) algorithms which is used by ``hyperopt`` - I am unable to speak to it's comparative performance but did find the package usage and documentation better than hyperopt. \n\n### Resources Used: \n- [Guide to tuning LightGBM using Optuna](https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)\n- [LightGBM Tuner: New Optuna Integration for Hyperparameter Optimization](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258)\n","metadata":{}},{"cell_type":"code","source":"def objective(trial, X, y):\n    param_grid = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95, step=0.1),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95, step=0.1),\n    }\n    \n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = lgbm.LGBMRegressor(**param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"mae\",\n            verbose=False,\n            early_stopping_rounds=25,\n            callbacks=[\n                LightGBMPruningCallback(trial, \"l1\")\n            ],  # Add a pruning callback\n        )\n        preds = model.predict(X_test)\n        cv_scores[idx] = mean_absolute_error(y_test, preds)\n\n    return np.mean(cv_scores)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T03:31:48.824529Z","iopub.execute_input":"2022-01-27T03:31:48.824935Z","iopub.status.idle":"2022-01-27T03:31:48.838286Z","shell.execute_reply.started":"2022-01-27T03:31:48.824904Z","shell.execute_reply":"2022-01-27T03:31:48.837163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similar to Hyperopt - this code creates an experiment using the objective function and parameter grid above\nstudy = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Regressor\")\nfunc = lambda trial: objective(trial, X_prepared, y_train.to_numpy())\nstudy.optimize(func, n_trials=10000)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\tBest value (mae): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T08:06:55.701606Z","iopub.execute_input":"2022-01-27T08:06:55.702087Z","iopub.status.idle":"2022-01-27T08:06:55.713737Z","shell.execute_reply.started":"2022-01-27T08:06:55.702033Z","shell.execute_reply":"2022-01-27T08:06:55.712524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Params Model \n\nBased on the 10000 ``Optuna`` trials, the following hyperparameter values had the best loss: \n```\nbest_params = {\n    'num_leaves': 512,\n    'max_depth': 9,\n    'lambda_l2': 15,\n    'bagging_fraction': 0.9,\n    'feature_fraction': 0.6,\n    'learning_rate': 0.01,\n    'n_estimators': 10000,\n    'random_state': 42,\n}\n\n```\n\nThe other important parameters (``learning_rate``, ``n_estimators``) were already set.","metadata":{}},{"cell_type":"code","source":"fit_params={\n    'early_stopping_rounds': 30, \n    'eval_metric': 'mae',\n    'verbose': False,\n    'eval_set': [[X_prepared_val, y_val]]\n}\n\nbest_params = {\n    'num_leaves': 512,\n    'max_depth': 9,\n    'lambda_l2': 15,\n    'bagging_fraction': 0.9,\n    'feature_fraction': 0.6,\n    'learning_rate': 0.01,\n    'n_estimators': 10000,\n    'random_state': 42,\n}\n\nlgbm_base = lgbm.LGBMRegressor(**best_params)\nlgbm_base.fit(X_prepared, y_train, **fit_params)\nget_cross_val_scores([lgbm_base], X_prepared, y_train, cv=3, fit_params=fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T03:45:46.524093Z","iopub.execute_input":"2022-02-16T03:45:46.52471Z","iopub.status.idle":"2022-02-16T03:47:19.590678Z","shell.execute_reply.started":"2022-02-16T03:45:46.524661Z","shell.execute_reply":"2022-02-16T03:47:19.589631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4-c) CatBoost\n\nCatboost came out around 2017-2018 (paper in NeurIPS 2018), written by a group of researchers at Yandex.  Its major advances include: \n\n- Allows native handling of categorical variables (as strings).  Specifically, it performs on-the-fly *numerical encoding* of categorical variables, with a clever encoding scheme to reduce overfitting (their paper argues that the standard approach of LightGBM overfits).  \n- Similar to LightGRM, it is also highly optimized and very fast.\n\nSimilar to LightGTM, I will not go into too many detailed explanations given the already long length of the notebook. **However, from my limited time using CatBoost, I find it to be faster than XGBoost but slower than LightGBM. In terms of model accuracy, CatBoost's default parameters model performs better than the fully tuned XGBoost and LightGBM.**\n\n### CatBoost Resources: \n- [CatBoost Hyper-Parameter Optimization with Optuna](https://towardsdatascience.com/hyper-parameter-optimization-with-optuna-4920d5732edf)\n- [CatBoost HyperParameter Tuning with Optuna!](https://www.kaggle.com/saurabhshahane/catboost-hyperparameter-tuning-with-optuna)\n","metadata":{}},{"cell_type":"code","source":"import catboost as cb\nimport optuna \nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:47:19.591904Z","iopub.execute_input":"2022-02-16T03:47:19.592309Z","iopub.status.idle":"2022-02-16T03:47:19.596403Z","shell.execute_reply.started":"2022-02-16T03:47:19.592277Z","shell.execute_reply":"2022-02-16T03:47:19.595688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_prepared = X_train.copy()\nX_prepared_val = X_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:47:19.597338Z","iopub.execute_input":"2022-02-16T03:47:19.597767Z","iopub.status.idle":"2022-02-16T03:47:19.635838Z","shell.execute_reply.started":"2022-02-16T03:47:19.597736Z","shell.execute_reply":"2022-02-16T03:47:19.634843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Dropper Pipeline\nvar_drop = ['parcelid',]\nfeature_dropper = FeatureDropper(features_to_drop=var_drop)\n\n# Convert Date Features Pipeline\nyear_feat_creator = CreateYearFeatures(date_features=date_features)\n\n# Feature Encoding Pipeline\ncat_vars = ['transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', \n            'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', \n            'regionidneighborhood', 'regionidzip', 'assessmentyear', 'typeconstructiontypeid', 'architecturalstyletypeid', \n            'buildingclasstypeid', 'pooltypeid2', 'pooltypeid7', 'storytypeid',  'hashottuborspa', 'pooltypeid2', \n            'taxdelinquencyyear',  'taxdelinquencyflag', 'fireplaceflag', 'decktypeid', 'pooltypeid10', \n            'propertycountylandusecode', 'propertyzoningdesc', 'rawcensustractandblock', 'censustractandblock'\n           ]\nconvert_to_cat = ConvertToType(var_type='str', vars_to_convert=cat_vars)\n\n# Date Feature Creator\ndate_feat_creator = CreateDateFeatures()\n\n# Derived Features Creator\nderived_feat_creator = CreateDerivedFeatures()\n\n# Aggregated Feature Creator\ngroup_col = 'regionidcity'\nagg_cols = ['lotsizesquarefeet', 'yearbuilt', 'calculatedfinishedsquarefeet',\n            'structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount', 'property_tax_per_sqft']\naggregated_feat_creator = CreateAggregatedFeatures(group_col=group_col, agg_cols=agg_cols)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:47:19.637106Z","iopub.execute_input":"2022-02-16T03:47:19.637442Z","iopub.status.idle":"2022-02-16T03:47:19.646289Z","shell.execute_reply.started":"2022-02-16T03:47:19.637407Z","shell.execute_reply":"2022-02-16T03:47:19.644911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost Categorical Handling \n\nCatBoost handles categorical variables natively which is different from other models that use One-Hot Encoding. It handles categorical variables by ranking their marginal target value in each node. \n\nFor this problem, we tested using both One-Hot Encoding and CatBoost's on-the-fly numerical encoding approach and decided to use CatBoost's built-in categorical handling which performed much better. \n","metadata":{}},{"cell_type":"code","source":"cb_preprocessor = Pipeline([\n    ('derived_feat_creator', derived_feat_creator),\n#     ('aggregated_feat_creator', aggregated_feat_creator),\n    ('feature_dropper', feature_dropper),\n    ('date_feat_creator', date_feat_creator),\n    ('year_feat_creator', year_feat_creator),\n    ('convert_to_cat', convert_to_cat),\n])\n\ndata_prep_pipe = cb_preprocessor.fit(X_prepared)\nX_prepared = cb_preprocessor.transform(X_prepared)\nX_prepared_val = cb_preprocessor.transform(X_prepared_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:47:19.647694Z","iopub.execute_input":"2022-02-16T03:47:19.648208Z","iopub.status.idle":"2022-02-16T03:47:24.510035Z","shell.execute_reply.started":"2022-02-16T03:47:19.648172Z","shell.execute_reply":"2022-02-16T03:47:24.508983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_cb = X_prepared.copy()\nX_cb_val = X_prepared_val.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T03:47:24.511554Z","iopub.execute_input":"2022-02-16T03:47:24.511992Z","iopub.status.idle":"2022-02-16T03:47:24.954529Z","shell.execute_reply.started":"2022-02-16T03:47:24.511949Z","shell.execute_reply":"2022-02-16T03:47:24.953337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize CatBoost Model \nparams = {\n    'n_estimators': 10000,\n    'random_state': 42,\n    'eval_metric': 'MAE',\n}\n\n# Fit model using early stopping and validation set \nfit_params={\n    'eval_set': (X_prepared_val, y_val),\n    'cat_features': cat_vars,\n    'early_stopping_rounds': 10, \n    'verbose': False,\n}\n\ncb_base = cb.CatBoostRegressor(**params)\ncb_base.fit(X_prepared, y_train, **fit_params)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_eval_metrics([cb_base], X_prepared_val, y_val)\n# get_cross_val_scores([cb_base], X_prepared, y_train, cv=3, fit_params=fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T03:50:26.575769Z","iopub.execute_input":"2022-02-16T03:50:26.576048Z","iopub.status.idle":"2022-02-16T03:50:26.759811Z","shell.execute_reply.started":"2022-02-16T03:50:26.57602Z","shell.execute_reply":"2022-02-16T03:50:26.758723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost Hyperparameter Tuning (Using Optuna)\n\nGiven the better documentation for Optuna compared to ``hyperopt``, I decided to continue using it for CatBoost Hyperparameter tuning. ","metadata":{}},{"cell_type":"code","source":"def objective(trial, X, y):    \n    param_grid = {\n        # Fixed Params\n        \"eval_metric\": \"MAE\",\n        \"n_estimators\": 10000,\n        \"random_state\": 42,\n    \n        # Tuned Params\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.05, 0.3),\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e1),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 20),\n        \"one_hot_max_size\": trial.suggest_int(\"one_hot_max_size\", 2, 20),  \n    }\n    # Conditional Hyper-Parameters\n    if param_grid[\"bootstrap_type\"] == \"Bayesian\":\n        param_grid[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param_grid[\"bootstrap_type\"] == \"Bernoulli\":\n        param_grid[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n    \n    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    cv_scores = np.empty(3)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        model = cb.CatBoostRegressor(**param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            cat_features=cat_vars,\n            early_stopping_rounds=20,\n            verbose=False\n        )\n        preds = model.predict(X_test)\n        cv_scores[idx] = mean_absolute_error(y_test, preds)\n\n    return np.mean(cv_scores)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:52:50.34179Z","iopub.execute_input":"2022-01-31T02:52:50.342123Z","iopub.status.idle":"2022-01-31T02:52:50.357636Z","shell.execute_reply.started":"2022-01-31T02:52:50.342095Z","shell.execute_reply":"2022-01-31T02:52:50.356869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similar to Hyperopt - this code creates an experiment using the objective function and parameter grid above\nstudy = optuna.create_study(direction=\"minimize\", study_name=\"CatBoost Regressor\")\nfunc = lambda trial: objective(trial, X_prepared, y_train.to_numpy())\nstudy.optimize(func, n_trials=400)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-31T02:53:00.622847Z","iopub.execute_input":"2022-01-31T02:53:00.623192Z","iopub.status.idle":"2022-01-31T08:06:49.766639Z","shell.execute_reply.started":"2022-01-31T02:53:00.623163Z","shell.execute_reply":"2022-01-31T08:06:49.76388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\\tBest value (mae): {study.best_value:.5f}\")\nprint(f\"\\tBest params:\")\n\nfor key, value in study.best_params.items():\n    print(f\"\\t\\t{key}: {value}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:06:49.768255Z","iopub.execute_input":"2022-01-31T08:06:49.768678Z","iopub.status.idle":"2022-01-31T08:06:49.779044Z","shell.execute_reply.started":"2022-01-31T08:06:49.768647Z","shell.execute_reply":"2022-01-31T08:06:49.777729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Params Model \n\nBased on the 400 ``Optuna`` trials, the following hyperparameter values had the best loss of MAE: 0.05265\n```\nbest_params = {\n    'learning_rate': 0.01,\n    'l2_leaf_reg': 3,\n    'depth': 6,\n    'n_estimators': 900,\n    'random_state': 42,\n    'eval_metric': 'MAE',\n    'loss_function': 'MAE'\n}\n```","metadata":{}},{"cell_type":"code","source":"best_params = {\n    'learning_rate': 0.03,\n    'l2_leaf_reg': 3,\n    'depth': 6,\n    'n_estimators': 900,\n    'random_state': 42,\n    'eval_metric': \"MAE\",\n    'loss_function': 'MAE',\n}\n\nfit_params={\n    'cat_features': cat_vars,\n    'verbose': False,\n}\n\ncb_base = cb.CatBoostRegressor(**best_params)\ncb_base.fit(X_prepared, y_train, **fit_params)\n\nget_eval_metrics([cb_base], X_prepared_val, y_val)\n# get_cross_val_scores([cb_base], X_prepared, y_train, cv=3, fit_params=fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T04:09:13.778098Z","iopub.execute_input":"2022-02-16T04:09:13.778756Z","iopub.status.idle":"2022-02-16T04:09:14.01408Z","shell.execute_reply.started":"2022-02-16T04:09:13.778711Z","shell.execute_reply":"2022-02-16T04:09:14.013221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Interpretability \n\nAn important aspect of machine learning model building is to be able to understand and evaluate the models **beyond** simple metrics on their test set performance to be able to trust the model. **This interpretation is extremely cruicial for complex models such as gradient boosting machines which usually perform better than easy to explain models such as Linear Regression, and thus require greater care when being used.** \n\nInterpretability can include answering questions such as: \n\n1. Which variables are most important to my model *in general*? \n1. What is the nature of the relationship between the predictors and the target? \n1. Are there significant interaction effects?\n1. For a specific prediction, what were the most important reasons leading to that prediction? \n\n**Generally, we will be exploring two different types of feature importances as we look to better interpret our earlier models:** \n\n1. **Global Feature Importance**: Does the model make predictions based on reasonable features? \n2. **Local Feature Importance**: Can we trust the model's prediction for one specific data point? ","metadata":{}},{"cell_type":"markdown","source":"## 1. Global Feature Importance\n\nWe have already briefly explored global feature importances earlier when using `feature_importances_` parameter of the Random Forest Regressor. These values are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree. \n\nSimilar functions are avaiable for **XGBoost except the feature importance values can be different based on the metric used for calculations. There are five different importance types:**\n\n- ``weight``: the number of times a feature is used to split the data across all trees.\n- ``gain``: the average gain across all splits the feature is used in.\n- ``cover``: the average coverage across all splits the feature is used in.\n- ``total_gain``: the total gain across all splits the feature is used in.\n- ``total_cover``: the total coverage across all splits the feature is used in.\n\nThe code sample below shows feature importances for the different gradient boosting models explored with a mini deep dive into CatBoost global feature importances. \n```\nxgb_base.get_booster().get_score(importance_type='gain')\nlgbm_base.feature_importances_\ncb_base.get_feature_importance()\n```","metadata":{}},{"cell_type":"code","source":"cb_importances = cb_base.get_feature_importance()\ncb_importances = pd.DataFrame(sorted(zip(cb_importances, X_cb.columns)), columns=['Value','Feature'])\n\n# Select 15 most important features\ncb_importances = cb_importances.sort_values(by='Value', ascending=False)[:20]\n\n# Plot importances \nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=cb_importances.sort_values(by=\"Value\", ascending=False))\nplt.title('CatBoost Global Feature Importances')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:10:07.424782Z","iopub.execute_input":"2022-02-16T04:10:07.425192Z","iopub.status.idle":"2022-02-16T04:10:07.908039Z","shell.execute_reply.started":"2022-02-16T04:10:07.425157Z","shell.execute_reply":"2022-02-16T04:10:07.907023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the plot above shows how the location related features (``propertycountylandusecode``, ``regionidzip``) are some of the most important followed by the age of house and ``calculatedfinishedsquarefeet``. ","metadata":{}},{"cell_type":"markdown","source":"## 2. Local Feature Importance \n\nLocal importance tries to help explain how different feature might be interaction and how the model makes a **specific** prediction. We will be highlighting two different tools to help with this: \n\n1. **ICE (Individual Conditional Expectation) Plots**: Excellent tool to understand \"what the model is thinking\" (examples below will clarify how they do this). \n2. **SHAP**: Motivated by \"Shapley Value\" in Game Theory, this aims to explain why a particular instance is \"different\" from average and which features contribute to the *specific* prediction. ","metadata":{}},{"cell_type":"markdown","source":"### ICE Plots \n \nIndividual Conditional Expectation (ICE) plots display one line per instance that shows how the instance’s prediction changes when a feature changes. The values for a line (and one instance) can be computed by keeping all other features the same, creating variants of this instance by replacing the feature’s value with values from a grid and making predictions with the black box model for these newly created instances. The result is a set of points for an instance with the feature value from the grid and the respective predictions.\n\nHow do ICE plots work?\n- Take an actual data point and see what the model predicts.\n- Pick a variable and change the value of that variable (over some range of values).\n- Plot the model prediction as a function of the \"altered\" value\n- Do this for multiple points, and plot on the same graph\n- Do this for all variables of interest","metadata":{}},{"cell_type":"code","source":"# !pip install ml_insights  #if you don't have it installed\nimport ml_insights as mli","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T04:10:46.197815Z","iopub.execute_input":"2022-02-16T04:10:46.198203Z","iopub.status.idle":"2022-02-16T04:10:46.214503Z","shell.execute_reply.started":"2022-02-16T04:10:46.198167Z","shell.execute_reply":"2022-02-16T04:10:46.213214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pts = mli.get_range_dict(X_cb)\ntest_pts = X_cb_val.sample(5, random_state=40)\nimp_num_feat = [\n    'calculatedfinishedsquarefeet', 'house_age',\n    'taxamount', 'structuretaxvaluedollarcnt', 'latitude', \n]","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:18:15.706823Z","iopub.execute_input":"2022-02-16T04:18:15.707236Z","iopub.status.idle":"2022-02-16T04:18:17.378798Z","shell.execute_reply.started":"2022-02-16T04:18:15.707188Z","shell.execute_reply":"2022-02-16T04:18:17.37771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ranges of values used for x-axis need to be updated to remove outliers to better interpret the ICE-plot.","metadata":{}},{"cell_type":"code","source":"# Updating the ranges of features on x-axis for better interpretation \nrange_pts['calculatedfinishedsquarefeet'] = np.linspace(0, 6000, 200)\nrange_pts['house_age'] = np.linspace(0, 130, 65)\nrange_pts['taxamount'] = np.linspace(0, 25000, 200)\nrange_pts['structuretaxvaluedollarcnt'] = np.linspace(0, 300000, 200)\nrange_pts['latitude'] = np.linspace(3.35e+07, 3.46e+07, 200)\n\nmli.ice_plot(cb_base, test_pts, imp_num_feat, range_pts=range_pts, pred_fn='predict', figsize=(15,10))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:18:17.500702Z","iopub.execute_input":"2022-02-16T04:18:17.501097Z","iopub.status.idle":"2022-02-16T04:18:30.569403Z","shell.execute_reply.started":"2022-02-16T04:18:17.501062Z","shell.execute_reply":"2022-02-16T04:18:30.568338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interpreting ICE plots\n\nThings to look for: \n- When the line is flat, that means the variable is unimportant in that range (for that particular data point)\n- When the line is steep, that means the variable has a strong effect on the target in that range.\n- If the line is very wiggly, this often betrays some degree of \"overfitting\" in the model.  Interestingly, this is often *not* reflected in the test set metrics.  However, it is reflective of the \"coherence\" of the model.  It also may reflect a paucity of training data in that region\n- If all the lines show the same basic effect, this suggests that there is little interaction.\n- If some lines have very different trajectories, this indicates a high degree of interactivity.\n\n\n**NOTE: ICE-plots tell us what the *model* thinks.  To the extent that the model is reflective of the \"real world\" that generated the data, it may be useful in understanding the real world.  However, it may also be demonstrating places where the model is wrong, has little data, or is displaying artifacts of the way it was trained.  This is where our \"human\" thinking can be combined with the model to try and understand the world better.**","metadata":{}},{"cell_type":"markdown","source":"## SHAP: Explaining Individual Predictions\n\nGoal of SHAP values is to: \n- Explain why a particular instance is \"different\" from average.\n- Which features / concepts contributed most to its \"distinctiveness\"?\n- Can we attribute the \"distance from average\" of a particular case to the individual features?\n\n\n**Note: If you're unfamiliar with SHAP Values completely, we will only be explaining it briefly here due the notebook's already long length. I highly recommend this [insightful article on SHAP for Interpretability](https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/) to better understand how they work.**","metadata":{}},{"cell_type":"code","source":"# !pip install shap #if you don't have it installed\nimport shap\nshap.initjs()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:19:02.778502Z","iopub.execute_input":"2022-02-16T04:19:02.778903Z","iopub.status.idle":"2022-02-16T04:19:10.957368Z","shell.execute_reply.started":"2022-02-16T04:19:02.778866Z","shell.execute_reply":"2022-02-16T04:19:10.952511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.Explainer(cb_base)\nshap_values = explainer(X_cb)\n\n# The SHAP expected value is the median of the target variable \nexplainer.expected_value, np.mean(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:20:27.240517Z","iopub.execute_input":"2022-02-16T04:20:27.240927Z","iopub.status.idle":"2022-02-16T04:21:13.32753Z","shell.execute_reply.started":"2022-02-16T04:20:27.240881Z","shell.execute_reply":"2022-02-16T04:21:13.326419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Waterfall Plots\n\nSHAP explains how individual predictions are arrived at in terms of the contributions from each of the model's input variables. This is a highly intuitive approach that produces simple but informative outputs. \n\nWaterfall plots the most complete display of a single prediction. The waterfall structure emphasizes the additive nature of positive and negative contributors, and how they build on the *base value* to yield the model's prediction, $f(x)$\n\n**Note: Unfortunately, the target feature values in our Zillow project are extremely tiny (mean of 0.008) and thus, the contributions shown in most charts below will be zero due to rounding. HOWEVER, the positive and negative contributions can still be gauged based on the magntitude of the various chart elements**\n","metadata":{}},{"cell_type":"code","source":"i_med = np.argsort(y_train)[len(y_train)//2 + 2]       # Median (offset by 2 due to missing index error)\ni_71 = np.argsort(y_train)[int(len(y_train)*0.75)] # 75th percentile\ni_20 = np.argsort(y_train)[int(len(y_train)*0.20)]  # 20th percentile\n\nshap.plots.waterfall(shap_values[i_71], max_display=15)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:22:11.96736Z","iopub.execute_input":"2022-02-16T04:22:11.967906Z","iopub.status.idle":"2022-02-16T04:22:22.619886Z","shell.execute_reply.started":"2022-02-16T04:22:11.96786Z","shell.execute_reply":"2022-02-16T04:22:22.618713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Force Plots \n\nForce plots are equivalent representations as Waterfall plots that display the key information is a more condensed format. ","metadata":{}},{"cell_type":"code","source":"# Force plot for median y_train value\nshap.plots.force(shap_values[i_med])","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:22:50.764535Z","iopub.execute_input":"2022-02-16T04:22:50.765217Z","iopub.status.idle":"2022-02-16T04:23:00.178504Z","shell.execute_reply.started":"2022-02-16T04:22:50.765178Z","shell.execute_reply":"2022-02-16T04:23:00.177438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Force plot for 20th percentile y_train value\nshap.plots.force(shap_values[i_20])","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:23:00.180207Z","iopub.execute_input":"2022-02-16T04:23:00.180577Z","iopub.status.idle":"2022-02-16T04:23:09.542759Z","shell.execute_reply.started":"2022-02-16T04:23:00.180542Z","shell.execute_reply":"2022-02-16T04:23:09.541729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHAP for Global Interpretability \n\nThanks to its versatility, SHAP values can also be used to get a sense of how \"globally\" important a feature is by aggregating the (absolute value of) the local feature values. These numbers can be thought of as the \"average absolute impact\" that a variable has on the prediction.","metadata":{}},{"cell_type":"code","source":"shap.plots.bar(shap_values, max_display=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T04:23:09.756633Z","iopub.execute_input":"2022-02-16T04:23:09.757167Z","iopub.status.idle":"2022-02-16T04:23:20.292627Z","shell.execute_reply.started":"2022-02-16T04:23:09.757117Z","shell.execute_reply":"2022-02-16T04:23:20.291594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The bar plot of the absolute SHAP values paints an interesting and *slightly* different picture than the ``feature_importances_`` function earlier which said ``house_age`` was one of the most important feature. However, according to the SHAP bar plot above, there are many other variables that contribute greater to the final predicted value on average.**\n\n**Furthermore, ``rawcensustractandblock`` and ``poolcnt`` are within top 7 contributors according to SHAP values compared to not even being in top 15 according to ``feature_importance_``. This can be a clue to further dig into these variables' impact and analyze whether their affects are postively or negatively affecting the model.** ","metadata":{}},{"cell_type":"markdown","source":"# Stacking Models \n\nUntil now, we have been training individual learners, which will be referred to as **base learners** going forward. **Stacking** (sometimes called \"stacked generalization\") involves training a new algorithm  to combine the predictions of several base learners.\n\nFirst, the base learners are trained using the available training data, then a combiner or meta algorithm, referred to as **meta learner or super learner**, is trained to make a final prediction based on the predictions of the base learners. Such stacked ensembles tend to outperform any of the individual base learners (eg. a single RF or GBM) and have been shown to represent an asymptotitcally optimal system for learning. \n\nThis [diagram](https://media.geeksforgeeks.org/wp-content/uploads/20190515104518/stacking.png) visually shows the process of stacking models where 'Second level model' in the image refers to the **Meta Learner**. ","metadata":{}},{"cell_type":"markdown","source":"## Create Training Data For Meta Learners\n\nThere are two ways to create a training dataset for the Meta Learner: \n1. **Split training set into two subsets**:\n    - The first subset is used to train the individual base learners (such as we have already done in earlier sections) \n    - The first layer's base learners are then used to make predictions on the second (held-out) set which ensures that the predictions are \"clean\", since the predictors never saw these instances during training. \n    - **Pros & Cons**: The biggest benefit is that this approach is simple and easy to implement. However, the major drawback is that we lose the size of training sets used to train both base learners and meta learner and are forced to compromise on the dataset sizes. \n2. **Out-of-Fold Predictions Dataset**: \n   - This approach uses K-Fold Cross Validation to train each individual base learner on K-1 folds and uses the Kth fold to generate out-of-fold predictions (i.e. the base learner has previously not seen the data in the Kth fold)\n   - This approach is repeated K times for each base learner - every time the base learner is trained on K-1 folds and generates predictions on the Kth fold. \n   - **Pros**: The biggest benefit is that we're able to generate a full size training set for the Meta Learner by combining all the out-of-fold predictions from each base learner. \n   - **Cons**: This approach is more complex to implement and takes significantly longer to run as each base learner is trained K-times to generate the final training set for the Meta Learner. \n   \n**This notebook chose the second approach and the cells below create individual functions to train each base learner followed by running K-fold training to generate the final training set for the Meta Learner.**","metadata":{}},{"cell_type":"code","source":"def fit_rf(X_prepared, y_train, random_state=42): \n    best_params = {\n        'n_estimators': 400,\n        'max_samples': 0.3,\n        'max_depth': 70,\n        'max_features': 0.6,\n        'min_samples_leaf': 32,    \n        'random_state': random_state, \n    }\n\n    ran_forest = RandomForestRegressor(**best_params)\n    ran_forest.fit(X_prepared, y_train)\n    return ran_forest","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:58:08.096238Z","iopub.execute_input":"2022-02-15T21:58:08.096675Z","iopub.status.idle":"2022-02-15T21:58:08.105Z","shell.execute_reply.started":"2022-02-15T21:58:08.096625Z","shell.execute_reply":"2022-02-15T21:58:08.103472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_xgb(X_prepared, y_train, random_state=42): \n    fit_params={\n        'early_stopping_rounds': 15, \n        'eval_metric': 'mae',\n        'verbose': False,\n        'eval_set': [[X_xgb_val, y_val]],\n    }\n\n    best_params = {\n        'colsample_bynode': 0.75,   \n        'gamma': 0.201,\n        'max_depth': 5,   \n        'subsample': 0.95, \n        'tree_method': 'exact',\n        'learning_rate': 0.01,\n        'n_estimators': 10000,\n        'random_state': random_state,\n    }\n\n    xgb_base = xgb.XGBRegressor(**best_params)\n    xgb_base.fit(X_prepared, y_train, **fit_params)\n    return xgb_base","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:58:08.10691Z","iopub.execute_input":"2022-02-15T21:58:08.107497Z","iopub.status.idle":"2022-02-15T21:58:08.12549Z","shell.execute_reply.started":"2022-02-15T21:58:08.107454Z","shell.execute_reply":"2022-02-15T21:58:08.124222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_lgbm(X_prepared, y_train, random_state=42): \n    fit_params={\n        'early_stopping_rounds': 30, \n        'eval_metric': 'mae',\n        'verbose': False,\n        'eval_set': [[X_lgbm_val, y_val]]\n    }\n    \n    best_params = {\n        'num_leaves': 512,\n        'max_depth': 9,\n        'lambda_l2': 15,\n        'bagging_fraction': 0.9,\n        'feature_fraction': 0.6,\n        'learning_rate': 0.01,\n        'n_estimators': 10000,\n        'random_state': random_state,\n    }\n\n    lgbm_base = lgbm.LGBMRegressor(**best_params)\n    lgbm_base.fit(X_prepared, y_train, **fit_params)\n    return lgbm_base","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:58:08.131118Z","iopub.execute_input":"2022-02-15T21:58:08.13183Z","iopub.status.idle":"2022-02-15T21:58:08.141365Z","shell.execute_reply.started":"2022-02-15T21:58:08.131751Z","shell.execute_reply":"2022-02-15T21:58:08.140163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_catboost(X_prepared, y_train, random_state=42):    \n    best_params = {\n        'learning_rate': 0.03,\n        'l2_leaf_reg': 3,\n        'depth': 6,\n        'n_estimators': 900,\n        'random_state': random_state,\n        'eval_metric': \"MAE\",\n        'loss_function': 'MAE',\n    }\n\n    fit_params={\n        'cat_features': cat_vars,\n        'verbose': False,\n    }\n\n    cb_base = cb.CatBoostRegressor(**best_params)\n    cb_base.fit(X_prepared, y_train, **fit_params)\n    return cb_base","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:58:08.143702Z","iopub.execute_input":"2022-02-15T21:58:08.144361Z","iopub.status.idle":"2022-02-15T21:58:08.170214Z","shell.execute_reply.started":"2022-02-15T21:58:08.144311Z","shell.execute_reply":"2022-02-15T21:58:08.169072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold \n\ndef get_out_of_fold_preds(X_prepared, y_train, model_fcn, random_state=42):\n    \"\"\"\n    Use K-Fold Cross Validation to create out-of-fold predictions for supplied model. \n    The out-of-fold predictions can then be used to train a Meta Learner to further\n    improve the model's performance.\n    \"\"\"\n    model_preds = list()\n    meta_y = list()\n    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n    for train_ix, test_ix in kfold.split(X_prepared):\n        # Depending on whether X_prepared is a dataframe or numpy array, uncomment one line below\n#         train_X, test_X = X_prepared.iloc[train_ix], X_prepared.iloc[test_ix]\n        train_X, test_X = X_prepared[train_ix], X_prepared[test_ix]\n    \n        train_y, test_y = y_train.iloc[train_ix], y_train.iloc[test_ix]\n        \n        model = model_fcn(train_X, train_y, random_state=random_state)\n        preds = model.predict(test_X)\n        meta_y.extend(test_y)\n        model_preds.extend(preds)\n\n    return model_preds, meta_y","metadata":{"execution":{"iopub.status.busy":"2022-02-15T21:58:08.172564Z","iopub.execute_input":"2022-02-15T21:58:08.174402Z","iopub.status.idle":"2022-02-15T21:58:08.192629Z","shell.execute_reply.started":"2022-02-15T21:58:08.174319Z","shell.execute_reply":"2022-02-15T21:58:08.190802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get Out-of-fold predictions for each base learner\n# WARNING: This cell takes multiple hours to run (remove Random Forest regressor to save time which takes significantly longer than others)\nxgb_preds, meta_y = get_out_of_fold_preds(X_xgb, y_train, fit_xgb)\nlgbm_preds, meta_y = get_out_of_fold_preds(X_lgbm, y_train, fit_lgbm)\nrf_preds, meta_y = get_out_of_fold_preds(X_rf, y_train, fit_rf)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use .iloc line for split in get_out_of_fold_preds function\ncb_preds, meta_y = get_out_of_fold_preds(X_cb, y_train, fit_catboost)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create meta_X training set by combining out-of-fold predictions from individual learners \nyhat1 = array(rf_preds).reshape((len(rf_preds), 1))\nyhat2 = array(xgb_preds).reshape((len(xgb_preds), 1))\nyhat3 = array(lgbm_preds).reshape((len(lgbm_preds), 1))\nyhat4 = array(cb_preds).reshape((len(cb_preds), 1))\n\nmeta_X = hstack((yhat1, yhat2, yhat3, yhat4))\nmeta_X","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:13:01.615127Z","iopub.execute_input":"2022-02-16T01:13:01.615791Z","iopub.status.idle":"2022-02-16T01:13:01.787645Z","shell.execute_reply.started":"2022-02-16T01:13:01.615743Z","shell.execute_reply":"2022-02-16T01:13:01.786603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create validation datset for testing Meta Learner \n\n# Make predictions on original validation sets using originally trained base learners\nxgb_val_preds = xgb_base.predict(X_xgb_val)\nlgbm_val_preds = lgbm_base.predict(X_lgbm_val)\ncb_val_preds = cb_base.predict(X_cb_val)\nrf_val_preds = ran_forest.predict(X_rf_val)\n\n# Create validation dataset\nyhat1 = array(rf_val_preds).reshape((len(rf_val_preds), 1))\nyhat2 = array(xgb_val_preds).reshape((len(xgb_val_preds), 1))\nyhat3 = array(lgbm_val_preds).reshape((len(lgbm_val_preds), 1))\nyhat4 = array(cb_val_preds).reshape((len(cb_val_preds), 1))\nmeta_X_val = hstack((yhat1, yhat2, yhat3, yhat4))\nmeta_X_val","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:15:39.955293Z","iopub.execute_input":"2022-02-16T01:15:39.955946Z","iopub.status.idle":"2022-02-16T01:15:43.156393Z","shell.execute_reply.started":"2022-02-16T01:15:39.955893Z","shell.execute_reply":"2022-02-16T01:15:43.155378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_X.shape, meta_X_val.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:16:00.833195Z","iopub.execute_input":"2022-02-16T01:16:00.833675Z","iopub.status.idle":"2022-02-16T01:16:00.840122Z","shell.execute_reply.started":"2022-02-16T01:16:00.833619Z","shell.execute_reply":"2022-02-16T01:16:00.839097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta Learners\n\nTrain a Meta Learner using training set created from out-of-fold predictions of individual learners ","metadata":{}},{"cell_type":"code","source":"# Initialize LightGBM Model \nbest_params = {\n    'num_leaves': 128,\n    'max_depth': 6,\n    'lambda_l2': 15,\n    'bagging_fraction': 0.6,\n    'learning_rate': 0.01,\n    'n_estimators': 10000,\n    'random_state': 42,\n}\n\n# Fit model using early stopping and validation set \nfit_params={'early_stopping_rounds': 30, \n            'eval_metric': 'mae',\n            'verbose': False,\n            'eval_set': [[meta_X_val, y_val]]\n           }\n\nlgbm_stacked = lgbm.LGBMRegressor(**best_params)\nlgbm_stacked.fit(meta_X, meta_y, **fit_params)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-16T01:16:11.143968Z","iopub.execute_input":"2022-02-16T01:16:11.144417Z","iopub.status.idle":"2022-02-16T01:16:12.81769Z","shell.execute_reply.started":"2022-02-16T01:16:11.144377Z","shell.execute_reply":"2022-02-16T01:16:12.816744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CatBoost Meta Learner\nbest_params = {\n    'learning_rate': 0.01,\n    'l2_leaf_reg': 15,\n    'depth': 6,\n    'min_data_in_leaf': 32,\n    'subsample': 0.6,\n    'n_estimators': 10000,\n    'random_state': 42,\n    'eval_metric': \"MAE\",\n}\n\nfit_params={\n    'eval_set': (meta_X_val, y_val),\n    'early_stopping_rounds': 30, \n    'verbose': False,\n}\n\ncb_stacked = cb.CatBoostRegressor(**best_params)\ncb_stacked.fit(meta_X, meta_y, **fit_params)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:16:12.821803Z","iopub.execute_input":"2022-02-16T01:16:12.822516Z","iopub.status.idle":"2022-02-16T01:16:18.262791Z","shell.execute_reply.started":"2022-02-16T01:16:12.822459Z","shell.execute_reply":"2022-02-16T01:16:18.261597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Meta Model MAE on Validation Set\ny_preds = lgbm_stacked.predict(meta_X_val)\n# y_preds = cb_stacked.predict(meta_X_val)\n\nmae = mean_absolute_error(y_val, y_preds)\nrmse = mean_squared_error(y_val, y_preds, squared=False)\nprint(f\"Stacked Model Evaluation\")\nprint(f\"MAE: {mae}, RMSE: {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:16:23.347091Z","iopub.execute_input":"2022-02-16T01:16:23.347537Z","iopub.status.idle":"2022-02-16T01:16:23.437494Z","shell.execute_reply.started":"2022-02-16T01:16:23.347491Z","shell.execute_reply":"2022-02-16T01:16:23.436551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining Multiple Models \n\nIn this section, we will not be using a new machine learning algorithm but rather use ensemble learning (similar to how Random Forest and GBMs do) to combine different learners that we have already trained. \n\n**By combining different learners trained earlier (Ridge / Lasso Regression, Random Forest, GBMs) in an ensemble, it can provide a final model that is more diverse than any of the individual models. This is because by combining diverse classifiers, it increases the chance that they will make very different types of errors, improving the ensemble's accuracy.**\n\n**We can even combine the Stacking Algorithm with the individual base learners using different weights for each model to make the final predictions even more diverse.**","metadata":{}},{"cell_type":"code","source":"def create_model_preds(all_models, X):\n    \"\"\"\n    Combine model predictions from different models using the input weights.\n    \n    all_models: List[Tuple(model, preprocessor)]\n        List of a tuple of model info where each tuple has fitted model and data preprocessor \n    weights: List[]\n        Weights for each model in all_models (must be same length as all_models)\n    X: \n        Data to predict on\n    \"\"\"\n    combined_preds = []\n    for model_info in all_models: \n        model, preprocessor = model_info\n        print(f\"Starting preds for {model}\")\n        # Note: function below is defined in later section (Submission CSV Generation)\n        model_pred = predict_in_chunks(model, preprocessor, X) \n        combined_preds.append(model_pred)\n    return combined_preds\n    \ndef combine_model_preds(combined_preds, weights, X):\n    # Combine based on weights\n    final_pred = np.empty(len(X))\n    print(f\"Combining with weights: {weights}\")\n    for idx, weight in enumerate(weights):\n        final_pred += combined_preds[idx] * weight\n    \n    return final_pred","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:16:30.323185Z","iopub.execute_input":"2022-02-16T01:16:30.323636Z","iopub.status.idle":"2022-02-16T01:16:30.33311Z","shell.execute_reply.started":"2022-02-16T01:16:30.323583Z","shell.execute_reply":"2022-02-16T01:16:30.331732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Models to generate predictions for\nall_models = [\n    (ran_forest, rf_preprocessor),\n    (xgb_base, xgb_preprocessor),\n    (lgbm_base, lgbm_preprocessor),\n    (cb_base, cb_preprocessor),\n] \n\n# Creating Predictions for each model \ncombined_preds = create_model_preds(all_models, df_test) # df_test is defined in next section (Submission CSV Generation)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:27:37.691236Z","iopub.execute_input":"2022-02-16T01:27:37.691702Z","iopub.status.idle":"2022-02-16T01:47:40.901448Z","shell.execute_reply.started":"2022-02-16T01:27:37.691659Z","shell.execute_reply":"2022-02-16T01:47:40.900533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Meta_X from base learner predictions \nyhat1 = combined_preds[0].reshape((len(df_test), 1))\nyhat2 = combined_preds[1].reshape((len(df_test), 1))\nyhat3 = combined_preds[2].reshape((len(df_test), 1))\nyhat4 = combined_preds[3].reshape((len(df_test), 1))\n\nmeta_X_submission = hstack((yhat1, yhat2, yhat3, yhat4))\nmeta_X_submission.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:47:40.902985Z","iopub.execute_input":"2022-02-16T01:47:40.903393Z","iopub.status.idle":"2022-02-16T01:47:40.961458Z","shell.execute_reply.started":"2022-02-16T01:47:40.90335Z","shell.execute_reply":"2022-02-16T01:47:40.960452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create predictions from Meta Learners using outputs from individual learners\nlgbm_stacked_pred = lgbm_stacked.predict(meta_X_submission)\ncb_stacked_pred = cb_stacked.predict(meta_X_submission)\n\nmeta_combined_preds = [lgbm_stacked_pred, cb_stacked_pred]","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:47:40.962986Z","iopub.execute_input":"2022-02-16T01:47:40.963265Z","iopub.status.idle":"2022-02-16T01:48:15.835761Z","shell.execute_reply.started":"2022-02-16T01:47:40.963237Z","shell.execute_reply":"2022-02-16T01:48:15.834752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Combine Models Based on Weights\n\nIn this section, we can combine the predictions from the base learners and meta learners in any ratios that add up to 1. Testing with different weights can lead to a more diverse model with lower variance than any of the individual learners in most cases. \n\n**The best performing model combined predictions from ``LightGBM, CatBoost, Stacked_LightGBM`` with weights ``[0.20, 0.55, 0.25]`` respectively.** \n\n","metadata":{}},{"cell_type":"markdown","source":"#### **The final model yielded in a Public Score of 0.06416 and a private score of 0.07497 which are within Top 250 and Top 60 respectively in the world out of over 3700 submissions.** ","metadata":{}},{"cell_type":"code","source":"# Models = [RF, XGB, LGBM, CB]\nweights = [0.0, 0.0, 0.20, 0.55]\nfinal_pred = combine_model_preds(combined_preds, weights, df_test)\n\n# Models: [Stacked_LGBM, Stacked_CB]\nweights = [0.25, 0.00]\nprint(f\"Combining with weights: {weights}\")\nfor idx, weight in enumerate(weights):\n    final_pred += meta_combined_preds[idx] * weight","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:39:34.756819Z","iopub.execute_input":"2022-02-16T02:39:34.757258Z","iopub.status.idle":"2022-02-16T02:39:34.843508Z","shell.execute_reply.started":"2022-02-16T02:39:34.757218Z","shell.execute_reply":"2022-02-16T02:39:34.842397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create final submission CSV using final test predictions \nsub = pd.read_csv('../input/zillow-prize-1/sample_submission.csv')\nfor c in sub.columns[sub.columns != 'ParcelId']:\n    sub[c] = final_pred\n\nprint('Writing csv ...')\nsub.to_csv('_9_Combined_plus_Stacked.csv', index=False, float_format='%.4f') ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T02:39:39.122399Z","iopub.execute_input":"2022-02-16T02:39:39.123125Z","iopub.status.idle":"2022-02-16T02:40:32.587071Z","shell.execute_reply.started":"2022-02-16T02:39:39.123068Z","shell.execute_reply":"2022-02-16T02:40:32.585914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission CSV Generation\n\nThe following code runs a chosen model along with the Scikit-Learn Pipeline preprocessor to generate the final submission CSV using the Kaggle Test data. ","metadata":{}},{"cell_type":"code","source":"print('Loading Properties ...')\nproperties_2016 = pd.read_csv('../input/zillow-prize-1/properties_2016.csv', low_memory = False)\n\nprint('Loading Sample ...')\nsample_submission = pd.read_csv('../input/zillow-prize-1/sample_submission.csv', low_memory = False)\nsample_submission['parcelid'] = sample_submission['ParcelId']\n\nprint('Concat Train 2016 & 2017 ...')\ndf_test = pd.merge(sample_submission[['parcelid']], properties_2016, how = 'left', on = 'parcelid')\ndf_test['transactiondate'] = pd.Timestamp('2016-12-01') \ndf_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:16:39.614125Z","iopub.execute_input":"2022-02-16T01:16:39.614557Z","iopub.status.idle":"2022-02-16T01:17:21.031459Z","shell.execute_reply.started":"2022-02-16T01:16:39.61452Z","shell.execute_reply":"2022-02-16T01:17:21.030378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del properties_2016, sample_submission; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunker(seq, size):\n    \"\"\"Function to iterate over dataframe in chunks.\"\"\"\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\ndef predict_in_chunks(model, preprocessor, df_test): \n    final_pred = []\n\n    for group in chunker(df_test, 300000):\n        group = preprocessor.transform(group)\n        pred = model.predict(group)\n        final_pred.append(pred)\n        del group; gc.collect()\n        del pred; gc.collect()\n    return np.concatenate(final_pred).ravel()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T01:17:21.256734Z","iopub.execute_input":"2022-02-16T01:17:21.257041Z","iopub.status.idle":"2022-02-16T01:17:21.270105Z","shell.execute_reply.started":"2022-02-16T01:17:21.257008Z","shell.execute_reply":"2022-02-16T01:17:21.269179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run the chosen model to generate predictions\nmodel = cb_base\nfinal_pred = predict_in_chunks(model, cb_preprocessor, df_test)\nlen(final_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:10:58.415385Z","iopub.execute_input":"2022-02-11T06:10:58.415673Z","iopub.status.idle":"2022-02-11T06:12:08.39486Z","shell.execute_reply.started":"2022-02-11T06:10:58.41564Z","shell.execute_reply":"2022-02-11T06:12:08.393941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create final submission CSV using final test predictions \nsub = pd.read_csv('../input/zillow-prize-1/sample_submission.csv')\nfor c in sub.columns[sub.columns != 'ParcelId']:\n    sub[c] = final_pred\n\nprint('Writing csv ...')\nsub.to_csv('2_CatBoost_smaller_feat_set_best_params.csv', index=False, float_format='%.4f') ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:13:14.801768Z","iopub.execute_input":"2022-02-11T06:13:14.802457Z","iopub.status.idle":"2022-02-11T06:14:11.559088Z","shell.execute_reply.started":"2022-02-11T06:13:14.802407Z","shell.execute_reply":"2022-02-11T06:14:11.557988Z"},"trusted":true},"execution_count":null,"outputs":[]}]}