{"cells":[{"metadata":{"_cell_guid":"a5a7cd57-2d5f-48f0-b1ef-00969ff886b2","_uuid":"4d90a5748f6c6b944c3aebaa90a7294677d1d20c"},"cell_type":"markdown","source":"## Introduction\n\nThe objective of this notebook is to show how to use pytorch as a scikit learn regressor to make predictions. The advantage of using it in this way, is that you can tune the hyperparameters for the model using sklearn grid or random parameter search.\n\nThe cleaning and formatting of the data is done using sklearn pipelines. The idea behind this is that it makes it easy to construct different features for different models and the various forms of feature engineering can be included in the gridsearch. In this notebook however, we only use one pipeline so hopefully I'll have time to demonstrate this at a later date.\n\nOk, lets get started by importing what we need."},{"metadata":{"_cell_guid":"19a54d50-0f9a-4f86-b245-96f3ecc74fe6","_uuid":"faadfb4caeb3c4e20ee030111661b4afdbc674d4","collapsed":true,"trusted":false},"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport torch.utils.data as data_utils\nimport torch.nn.init as init\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.externals.joblib import Memory\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom tempfile import mkdtemp\nimport datetime\nfrom dateutil.parser import parse\nimport inspect\nfrom numbers import Number\nimport math\n\nimport zlib\nimport zipfile","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2402d2f5-b52b-471d-806f-97e0388f4719","_uuid":"a88a4a4400690ea2ae347b30a3e4b7c61b4a6112"},"cell_type":"markdown","source":"## Pytorch regressor\n\nSo first up, let's construct scikit learn regressor containing a pytorch model. In case you want to try this outside of kaggle kernels, I've included the code to run the model on the GPU.\n\nThis regressor is currently missing the ability to pass in a test set and I haven't included dropout in the model. You can add this yourself, or I'll include the code if anyone mentions it would be useful in the comments."},{"metadata":{"_cell_guid":"edd7f0fd-1f63-4c16-921b-c77a59da87a0","_uuid":"a6e97f2114448d1cdbe3db2940f314a1b63ae737","collapsed":true,"trusted":false},"cell_type":"code","source":"class PytorchRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"A pytorch regressor\"\"\"\n\n    def __init__(self, output_dim=1, input_dim=100, hidden_layer_dims=[100, 100],\n                 num_epochs=1, learning_rate=0.01, batch_size=128, shuffle=False,\n                 callbacks=[], use_gpu=True, verbose=1):\n        \"\"\"\n        Called when initializing the regressor\n        \"\"\"\n        self._history = None\n        self._model = None\n        self._gpu = use_gpu and torch.cuda.is_available()\n\n        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n        values.pop(\"self\")\n\n        for arg, val in values.items():\n            setattr(self, arg, val)\n\n    def _build_model(self):\n        self._layer_dims = [self.input_dim] + \\\n            self.hidden_layer_dims + [self.output_dim]\n\n        self._model = torch.nn.Sequential()\n\n        # Loop through the layer dimensions and create an input layer, then\n        # create each hidden layer with relu activation.\n        for idx, dim in enumerate(self._layer_dims):\n            if (idx < len(self._layer_dims) - 1):\n                module = torch.nn.Linear(dim, self._layer_dims[idx + 1])\n                init.xavier_uniform(module.weight)\n                self._model.add_module(\"linear\" + str(idx), module)\n\n            if (idx < len(self._layer_dims) - 2):\n                self._model.add_module(\"relu\" + str(idx), torch.nn.ReLU())\n\n        if self._gpu:\n            self._model = self._model.cuda()\n\n    def _train_model(self, X, y):\n        torch_x = torch.from_numpy(X).float()\n        torch_y = torch.from_numpy(y).float()\n        if self._gpu:\n            torch_x = torch_x.cuda()\n            torch_y = torch_y.cuda()\n\n        train = data_utils.TensorDataset(torch_x, torch_y)\n        train_loader = data_utils.DataLoader(train, batch_size=self.batch_size,\n                                             shuffle=self.shuffle)\n\n        loss_fn = torch.nn.MSELoss(size_average=False)\n\n        optimizer = torch.optim.Adam(\n            self._model.parameters(), lr=self.learning_rate)\n\n        self._history = {\"loss\": [], \"val_loss\": [], \"mse_loss\": []}\n\n        finish = False\n        for epoch in range(self.num_epochs):\n            if finish:\n                break\n\n            loss = None\n            idx = 0\n            for idx, (minibatch, target) in enumerate(train_loader):\n                y_pred = self._model(Variable(minibatch))\n\n                loss = loss_fn(y_pred, Variable(\n                    target.cuda().float() if self._gpu else target.float()))\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            y_labels = target.cpu().numpy() if self._gpu else target.numpy()\n            y_pred_results = y_pred.cpu().data.numpy() if self._gpu else y_pred.data.numpy()\n\n            error = mean_absolute_error(y_labels, y_pred_results)\n\n            self._history[\"mse_loss\"].append(loss.data[0])\n            self._history[\"loss\"].append(error)\n\n            if self.verbose > 0:\n                print(\"Results for epoch {}, loss {}, mse_loss {}\".format(epoch + 1,\n                                                                          error, loss.data[0]))\n            for callback in self.callbacks:\n                callback.call(self._model, self._history)\n                if callback.finish:\n                    finish = True\n                    break\n\n    def fit(self, X, y):\n        \"\"\"\n        Trains the pytorch regressor.\n        \"\"\"\n\n        assert (type(self.input_dim) ==\n                int), \"input_dim parameter must be defined\"\n        assert (type(self.output_dim) == int), \"output_dim must be defined\"\n\n        self._build_model()\n        self._train_model(X, y)\n\n        return self\n\n    def predict(self, X, y=None):\n        \"\"\"\n        Makes a prediction using the trained pytorch model\n        \"\"\"\n        if self._history == None:\n            raise RuntimeError(\"Regressor has not been fit\")\n\n        results = []\n        split_size = math.ceil(len(X) / self.batch_size)\n\n        # In case the requested size of prediction is too large for memory (especially gpu)\n        # split into batchs, roughly similar to the original training batch size. Not\n        # particularly scientific but should always be small enough.\n        for batch in np.array_split(X, split_size):\n            x_pred = Variable(torch.from_numpy(batch).float())\n            y_pred = self._model(x_pred.cuda() if self._gpu else x_pred)\n            y_pred_formatted = y_pred.cpu().data.numpy() if self._gpu else y_pred.data.numpy()\n            results = np.append(results, y_pred_formatted)\n\n        return results\n\n    def score(self, X, y, sample_weight=None):\n        \"\"\"\n        Scores the data using the trained pytorch model. Under current implementation\n        returns negative mae.\n        \"\"\"\n        y_pred = self.predict(X, y)\n        return mean_absolute_error(y, y_pred) * -1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b028791-44ae-4441-93ee-8e725b18701a","_uuid":"8d2a31b68d8c8120a4519d06a492b7cf295f4fe4"},"cell_type":"markdown","source":"## Data pipeline\nNow lets build the various components that will later make up our data pipeline. This will do things such as removing outliers, filling missing fields and label encoding. These can be combined with the build in scikit learn transformers (PCA etc) to create the final input data."},{"metadata":{"_cell_guid":"3c505e7f-d61b-4026-aa15-f632ebed2f90","_uuid":"017b4b9ecdf1a8ea81e49c986d6e85cec2fbdc65","collapsed":true,"trusted":false},"cell_type":"code","source":"class OutlierRemover(BaseEstimator, TransformerMixin):\n    def __init__(self, field=\"logerror\", min_val=-0.4, max_val=0.4):\n        self.min_val = min_val\n        self.max_val = max_val\n        self.field = field\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        return data.query(\n            \"{field} > {min_val} and {field} < {max_val}\".format(\n                field=self.field,\n                min_val=self.min_val,\n                max_val=self.max_val))\n\nclass DateEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.base_date = datetime.datetime(1600, 1, 1, 0, 0)\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        data[\"transactiondate\"] = data[\"transactiondate\"].apply(\n            lambda date: date if isinstance(date, Number) else (\n                parse(str(date)) - self.base_date).days)\n        return data\n\n\nclass LabelEncodeObjects(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        for c in data.columns:\n            if data[c].dtype == 'object':\n                lbl = LabelEncoder()\n                lbl.fit(list(data[c].values))\n                data[c] = lbl.transform(list(data[c].values))\n        return data\n\n\nclass LabelEncodeCols(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=[]):\n        self.cols = cols\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        for col in self.cols:\n            lbl = LabelEncoder()\n            lbl.fit(list(data[col].values))\n            data[col] = lbl.transform(list(data[col].values))\n        return data\n\n\nclass NaFiller(BaseEstimator, TransformerMixin):\n    def __init__(self, fill_val=-1):\n        self.fill_val = fill_val\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        return data.fillna(self.fill_val)\n\n\nclass NaColFiller(BaseEstimator, TransformerMixin):\n    def __init(self, fill_val=-1, cols=[]):\n        self.fill_val = fill_val\n        self.cols = cols\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        for col in cols:\n            data[col] = data[col].fillna(self.fill_val)\n        return data\n\n\nclass NaColMeanFiller(BaseEstimator, TransformerMixin):\n    def __init(self, cols=[]):\n        self.cols = cols\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        for col in cols:\n            data[col] = data[col].fillna(data[col].mean)\n        return data\n\n\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=[]):\n        self.cols = cols\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        return data.drop(self.cols, axis=1)\n\n\nclass Cloner(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        return data.copy()\n\n\nclass ColumnOrderer(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data):\n        return data.sort_index(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"25c3d9ba-4de0-45bc-a82d-e15ea151b4aa","_uuid":"f7fea484455ec872a6e4653630a9f53ac337f4cc"},"cell_type":"markdown","source":"Now it's time to read in the data "},{"metadata":{"_cell_guid":"205f71e2-45c0-4ca6-8797-cdc571cc94fe","_uuid":"02aab99a74ced014e17263c46ba963c897fe3d8e","trusted":false},"cell_type":"code","source":"prop = pd.read_csv('../input/properties_2016.csv')\ntrain = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n\ndf_train = train.merge(prop, how='left', on='parcelid')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4348a097-ea69-454b-bfa6-28a7377c2876","_uuid":"83358cc9d3ccf996bc34db867e9ecf0348da7232"},"cell_type":"markdown","source":"Lets drop the outliers and split into x and y"},{"metadata":{"_cell_guid":"eddac068-9aa1-4ee5-ae30-6892d69646f4","_uuid":"30c97a726e62a9ec772a37ae235e8105c4433c1d","collapsed":true,"trusted":false},"cell_type":"code","source":"def make_train_set():\n    reduced = OutlierRemover().transform(df_train)\n    x_train = reduced.drop([\"logerror\"], axis = 1)\n    y_train = reduced[\"logerror\"]\n    return (x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"962bc4f7-bdbe-4d38-a612-3449b285f119","_uuid":"6a9fefb2f17ea08d7242c3f6e8021ac002f9325e"},"cell_type":"markdown","source":"Just a technical point, add caching for the data pipeline"},{"metadata":{"_cell_guid":"20280d3f-bcf2-462a-968b-a4f87a36247e","_uuid":"2b5e206e20d71830a871b2ad84a1f6589c565367","collapsed":true,"trusted":false},"cell_type":"code","source":"cachedir = mkdtemp()\nmemory = Memory(cachedir=cachedir, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26788ca5-a60b-48d4-97a6-e84990dff18e","_uuid":"4ca80ecdd991ab92a08bcffd5c7eeb2bd2db08d4"},"cell_type":"markdown","source":"Add a list of columns to be dropped and id columns to be used by our data processing pipeline."},{"metadata":{"_cell_guid":"9a0d5060-6e6d-40df-8555-23acac95874f","_uuid":"bba4194379e602e44d0ff4c942899bad80ac6cf9","collapsed":true,"trusted":false},"cell_type":"code","source":"drop_cols = ['finishedsquarefeet6', 'finishedsquarefeet12', 'finishedsquarefeet13',\n             'finishedsquarefeet15', 'parcelid']\nid_cols = ['heatingorsystemtypeid', 'propertylandusetypeid', 'storytypeid', \n           'airconditioningtypeid','architecturalstyletypeid', 'buildingclasstypeid', \n           'buildingqualitytypeid', 'typeconstructiontypeid']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f71de58e-9ca9-4cb0-a59b-ff4cebadf450","_uuid":"b20974e07b4743dbce641b22f0554bdeeb95a3f2"},"cell_type":"markdown","source":"Now we can construct the pipeline and transform the training data ready to train the model."},{"metadata":{"_cell_guid":"b40fbf94-b247-4ca4-b859-af064cf2ffb1","_uuid":"726024fa26d31920c69b89a560438bddfc15c84e","collapsed":true,"trusted":false},"cell_type":"code","source":"dp = make_pipeline(Cloner(), DateEncoder(), ColumnDropper(cols=drop_cols),\n                   NaFiller(), LabelEncodeObjects(), LabelEncodeCols(cols=id_cols), ColumnOrderer(),\n                   StandardScaler())\n\nx_train_df, y_train = make_train_set()\nx_train = dp.fit_transform(x_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a7ba85aa-3cb4-48c3-b513-ae322fe84b1e","_uuid":"11c95c9e476fbb0c0b89dd02e6c576baef8606b4"},"cell_type":"markdown","source":"Now we are ready to build and train out pytorch models. As an example of what can be done, I have selected and arbitrary set of different hidden layers and made 4 different models. \n\nAs mentioned in the introduction, to optimise this, you could use a grid or random search to find the optimal network structure and hyperparameters. But for now, this should show what can be done."},{"metadata":{"_cell_guid":"ee885fab-422c-48bd-9756-30a3805c4d3f","_uuid":"c34797d3a262e6b54ca5c3d0ab04f25e70ab210d","collapsed":true,"trusted":false},"cell_type":"code","source":"num_features = 54\nbatch_size = 4096\nlearning_rate = 0.0007\n\nclf1 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[1200, 500, 100, 10],\n                        learning_rate=0.0005, batch_size=batch_size, num_epochs=10)\nclf2 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[500, 500],\n                        learning_rate=learning_rate, batch_size=batch_size, num_epochs=10)\nclf3 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[1500, 500, 10],\n                        learning_rate=learning_rate, batch_size=batch_size, num_epochs=10)\nclf4 = PytorchRegressor(input_dim=num_features, hidden_layer_dims=[1000, 800, 500, 200, 100, 10],\n                        learning_rate=learning_rate, batch_size=batch_size, num_epochs=10)\n\nestimators = [clf1, clf2, clf3, clf4]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d1418f3-d8db-4fc5-91f0-80ffc9ea31b0","_uuid":"1646546306e4926a4629d5247ad8b523bc5471c6"},"cell_type":"markdown","source":"We are now ready to train the various classifiers. I am going to use the same set of training data for each, but for diversity, you could create different pipelines that are optimal for different models."},{"metadata":{"_cell_guid":"d56e91da-d68b-487f-a211-1465afe86a57","_uuid":"5fcf0a0f32da9e0b024968ac914163eaa813704c","trusted":false},"cell_type":"code","source":"for idx, estimator in enumerate(estimators):\n    print(\"Fitting\", idx + 1)\n    estimator.fit(x_train, y_train.as_matrix())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a03a7c6c-f7a3-4110-9aba-82d09cf0ec0a","_uuid":"f192ca9ca54fad5731501613403cae17177c30f6"},"cell_type":"markdown","source":"Just to sanity check, let's have a look at the predictions on the training data to check they look ok and are approximately on the right scale."},{"metadata":{"_cell_guid":"b9655f51-7bb8-4d3d-8b71-c1dacb55b63b","_uuid":"50215bf3361d7ced745e9fff16cc8fd23f6dab3a","trusted":false},"cell_type":"code","source":"print(\"Classifier 1\")\nprint(clf1.predict(x_train))\nprint(\"Classifier 2\")\nprint(clf2.predict(x_train))\nprint(\"Classifier 3\")\nprint(clf3.predict(x_train))\nprint(\"Classifier 4\")\nprint(clf4.predict(x_train))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff72810d-1bcd-4465-9cfd-9364c6e65189","_uuid":"69d8806d856715aadd929714bf0d482c5b553232"},"cell_type":"markdown","source":"Yep, looks ok. So lets move on to the final predictions. Lets run the full set of properties through the pipeline and make predictions on it.\n\nI'll encode the date up front, but this is really just an optimisation to make the pipeline quicker so this code can be safely ignored.\n\nAfter calculating the predictions, we'll add them all to the submission file in the correct format."},{"metadata":{"_cell_guid":"d0fad2c5-d28d-48e4-b652-ac19029b92a7","_uuid":"054aa47fe32c4606dbb68d8f43001355ec03ff97","collapsed":true,"trusted":false},"cell_type":"code","source":"def encode_date(date):\n    base_date = datetime.datetime(1600, 1, 1, 0, 0)\n    return (parse(str(date)) - base_date).days\n\nfull_predict = prop\nfull_predict[\"transactiondate\"] = encode_date(\"2016-10-1\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e4963b4-2856-4512-9e1b-3bdd390c3f39","_uuid":"2a350b4b90b67eb3dc383349af5cd6dc7c62a7fb","trusted":false},"cell_type":"code","source":"predict_dates= [\"2016-10-1\", \"2016-11-1\", \"2016-12-1\", \"2017-10-1\", \"2017-11-1\", \"2017-12-1\"]\nfull_results = { \"ParcelId\" : full_predict[\"parcelid\"]}\n\nfor idx, date in enumerate(predict_dates):\n    print(\"Predicting\", date)\n    full_predict[\"transactiondate\"] = encode_date(date)\n    x_pred = dp.fit_transform(full_predict)\n    \n    pred1 = clf1.predict(x_pred)\n    pred2 = clf2.predict(x_pred)\n    pred3 = clf3.predict(x_pred)\n    pred4 = clf4.predict(x_pred)\n    pred_all = 0.25 * pred1 + 0.25 * pred2 + 0.25 * pred3 + 0.25 * pred4\n    \n    date_header = date.replace(\"-\", \"\")[:-1]\n    full_results[date_header] = pred_all\n\n## Reorder dataframe to ensure ParcelId is at the start\nsubmission = pd.DataFrame(full_results)\npid = submission['ParcelId']\nsubmission.drop(labels=['ParcelId'], axis=1,inplace = True)\nsubmission.insert(0, 'ParcelId', pid)\n\n## Show the head of the submission to see that all is in order\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef09db37-cf23-45fa-b518-da7bd061a5cc","_uuid":"4c738bf8d4bb812772a0946468e8875fdfcf77f1"},"cell_type":"markdown","source":"Finally, we can save the predictions to disk and zip them ready for submission. Or the unzipped version can be combined with other models."},{"metadata":{"_cell_guid":"5d7efe34-a227-4289-914f-aa21dbdc5c59","_uuid":"892cb12ad193a84463727f8af8dfb5983362fc34","collapsed":true,"trusted":false},"cell_type":"code","source":"sub_file_name = 'pytorch_predictions.csv'\nsubmission.to_csv(sub_file_name, index=False, float_format='%.4f')\n\nzf = zipfile.ZipFile(sub_file_name + '.zip', mode='w')\n\nprint('Creating zip')\nzf.write(sub_file_name, compress_type=zipfile.ZIP_DEFLATED)\nzf.close()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4edd76b8-1f85-4602-a5e6-5b15382f9a77","_uuid":"c690335fc64285f21c96428571e23c862d6aa841","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","file_extension":".py","name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}