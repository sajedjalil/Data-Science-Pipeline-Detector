{"cells":[{"metadata":{"_uuid":"096015121b0961197c6f6f6ede421e3b0704c13d"},"cell_type":"markdown","source":"# Zillow Prize: Zillowâ€™s Home Value Prediction (Zestimate)\n## Can you improve the algorithm that changed the world of real estate?\n## Josh Wilson, Keane Johnson, Carlos Sancini"},{"metadata":{"_uuid":"9d3ff7967282070422f019352d8c39d05ad10fde"},"cell_type":"markdown","source":"## Description\nZillow's \"Zestimates\" are estimated home values based on numerous categorical and numerical features of a given property. Zillow has been able to improve the median margin of error from 14% at the Zestimate's release to 5% as of 2017. However, they have introduced a competition to Data Scientists to further improve the accuracy of the Zestimate. The goal of this project is to produce a model that  accurately predicts the logerror produced by the Zestimate algorithm's attempt to predict the price of a house. Success is measured by minimizing the mean absolute error between our prediction and the Zestimate log-error. The log-error is defined as:\n  \n           logerror = log(Zestimate) - log(SalePrice)\n           \n           MAE = SIGMA |predicted logerror - logerror| / n\n\nWe have been provided data on a variety of different features of Los Angeles-area homes sold in the year 2016. Some of these features are numerical and some are categorical. We will be using this data to build our model."},{"metadata":{"_uuid":"e179eb58d02c556b020db853798b6522bc5dbd7f"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_uuid":"28ac87aa03d8e7874c407ce951c039a84769fa09"},"cell_type":"markdown","source":"> ### Import Libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":true},"cell_type":"code","source":"# Packages for manipulating data\nimport numpy as np \nimport pandas as pd\nfrom tabulate import tabulate\n\n# Packages for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\n\n# Stats et al.\nimport scipy.stats as st\nfrom scipy.spatial.distance import cdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Model packages\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.cluster import KMeans\n\n# Support vector machine \nfrom sklearn.svm import SVR\n\n# Various Tree Based Regressors\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n# Packages for measuring model performance\nfrom sklearn.externals import joblib\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_absolute_error\n\n# general packages\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Setting the seed for reproducibility of results\nnp.random.seed(27)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b284dc717643cfda0ec779e143934227471f3a4"},"cell_type":"markdown","source":"> ### Load and Examine the Data"},{"metadata":{"_uuid":"58832dd822c8a050926374c442862d78f3d6de44","trusted":true},"cell_type":"code","source":"# read in training and test data and labels\n\ntrain_data = pd.read_csv('../input/properties_2016.csv')\ntrain_labels = pd.read_csv('../input/train_2016_v2.csv')\n#test_data = pd.read_csv('properties_2017.csv')\n#test_labels = pd.read_csv('train_2017.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"508ef3df87c3266dd22b36e8d61be09f63c10bff"},"cell_type":"markdown","source":"The first step in our exploratory data analysis is to get a basic understanding of our dataset through a quick summary of what it contains."},{"metadata":{"_uuid":"8444aa20536dc1c49474d33816d623e3077c550e","trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55b940cd78346b7763b685957279c7cfeafc954d"},"cell_type":"markdown","source":"This summary shows that there are 53 different variables provided in our dataset. The first thing that stands out is the first row of the table returned by the describe() function - *count*. This refers to the number of values in the dataset for that respective column. There are just under 3 million parcelid's, which uniquely identify properties (our observations). However, most of the features of our properties do not have a similar number for *count*. This means that we are missing data for many different features for many different observations. We will take a closer examination of the missing data later on.\n\nThe second thing that stands out is that there are numeric values representing categorical features. For example, the column *decktypeid* represents the type of deck of a property. The number assigned to the type of deck does not measure magnitude and could have been arbitrary. \n\nFinally, we should note that the mean of the majority of the truly numeric (and not representing categorical) features is greater than the median. This means that our features have a positive skew.\n\nNow that we have some general context to our dataset, we can look at the data itself."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e111772f4012199e369354ed3ca3fae32cf755e9"},"cell_type":"markdown","source":"The training labels dataset consists of three different columns. *parcelids* identifies individual homes. *logerror* is the difference between the log of Zillow's estimated selling price and the log of the home's actual selling price. And *transactiondate* is the date the home sold."},{"metadata":{"_uuid":"4fef91acc55a417f0ba7867907865545da6c6550","scrolled":true,"trusted":false},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04835ee9a14a4960df2583767e33ab7759746f51"},"cell_type":"code","source":"## Distribution of Target Variable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c795f28cc22f0daa0fce443322828c491a02e694"},"cell_type":"markdown","source":"The below graphs of the target variable reveal three important aspects about the variable we are modeling:\n* We are dealing with a relatively normal distribution. Remember that these are essentially results from a model. That means the errors are conditionally normal and we might be able to leverage this fact in our own modeling endeavor.\n* The peak around 0 is relatively higher than what we would expect from a normal distribution. \n* Most importantly, the tails of this normal distribution are rather long, meaning we might have to cope with some outliers in our own modeling."},{"metadata":{"trusted":true,"_uuid":"e678077e0d7b472a81cf734df382f6747cce7412"},"cell_type":"code","source":"# Investigating the distribution of the logerrors\n#train_labels.logerror.describe())\n\n# plotting the log error graphs side-by-side, one with the full range of values and one with a trimmed range\nplt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.distplot(train_labels['logerror'], bins = 50, kde = False)\nplt.title('Histogram of logerror raw')\n\nulimit = np.percentile(train_labels.logerror.values, 99)\nllimit = np.percentile(train_labels.logerror.values, 1)\n\n# creating an array with limited logerror values\nlogerror = train_labels.logerror.values\nlogerror = logerror[np.where(logerror < ulimit)]\nlogerror = logerror[np.where(logerror > llimit)]\n\n\nplt.subplot(1, 2, 2)\nsns.distplot(logerror, bins = 50, kde = False)\nplt.title('Histogram of logerror trimmed')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"250f5af60d00265a20ed1d8007f1712b90307bb9"},"cell_type":"markdown","source":"> ### Examine Missing Data"},{"metadata":{"_uuid":"48c5dc4d5a8d45061f497ea295cec94170bc7423"},"cell_type":"markdown","source":"By looking at some example observations of our training data, we can confirm that lots of observations lack complete data. We can further understand the extent of these missing values through a visualization."},{"metadata":{"_uuid":"ad1e378b1fdcd9ae38a785437d5068b775706211","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\n\nsns.heatmap(\n    train_data.isnull(),\n    cbar=False,\n    yticklabels=False,\n    cmap = 'viridis'\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764296983df9fcd310fdd8546a643e00a2cc846f"},"cell_type":"markdown","source":"This heatmap visualizes whether the training data has missing values. The yellow sections of the chart indicate missing data. This confirms our impression that there is missing data for many of the features. To further understand the extent of missing data, we can calculate the percentage of missing observations by feature."},{"metadata":{"_uuid":"6b13b4b69c1b32499d800913192166512e7fdd23","trusted":true},"cell_type":"code","source":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = round((train_data.isnull().sum()/train_data.isnull().count()).sort_values(ascending=False), 4)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Total Missing Count', '% of Total Observations'])\nmissing_data.index.name = 'Feature'\n\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25bcc3be2dad7ce43b5e4f545d8d59fc170a5dd6"},"cell_type":"markdown","source":"Unsurprisingly, the variables for which the data are most complete revolve around location, bedroom, bathroom, and tax values. For many people, these are the most important factors in choosing where to live: commute to work/school district, the ability to house a family, and the yearly cost of living in the house. There is then a fairly precipitous dropoff for the next tier of variables and the degree of completeness (e.g. heating type and building type); there does not appear to be any logical explanation, available to us, for why these variable might be more or less complete than others. Lastly, we are missing data for almost all observations for seventeen features. This is unfortunate because some of these variables contain information about potentially impactful features of the value of properties, such as the architecture style, and the size of the basement and pool. Our hope is that these sparse variables can be leveraged in some way to help inform our model."},{"metadata":{"_uuid":"2f43165a2b217e8eef56ce45a1eb4edade287895"},"cell_type":"markdown","source":"> ### Correlations Between Dependent and Independent Variables"},{"metadata":{"_uuid":"1ec62cf105b62e83f01cc5cc044c392cb14033eb"},"cell_type":"markdown","source":"The next step of our Exploratory Data Analysis is to find correlations between our dependent and independent variables. This will help us identify instances of potential multicollinearity within our independent variables. It will also help us identify features that change with logerror."},{"metadata":{"_uuid":"605b4ddd3dd5d779dad62f829ed4f435dd7bc0e9","trusted":true},"cell_type":"code","source":"train_data_labels = pd.merge(train_data, train_labels, on='parcelid')\n\ncorrelation = train_data_labels.corr()\n\nplt.figure(figsize=(20,20))\nplt.title('Correlation of Features with logerror',y=1,size=20)\n\nsns.heatmap(\n    correlation,\n    square=True,\n    vmax=0.8\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38f62526c97f2435f44fc5982a1ca57dd275e12c"},"cell_type":"markdown","source":"We consider the investigation of this correlation matrix heatmap to be a great exercise in building intution around the variables and their relationships to each other. Our discussion of the variables below creates some hypotheses for what we expect a machine learning model to uncover.\n\nIt is tough to ignore the very white lines that segment this heatmap. They are the result of sparse data and are not perfectly correlated variables. For example, there is no logical reason that *assessmentyear* and *decktypeid* ought to be correlated. Instead, these are variables with low mass.\n\nLighter squares on this heat map indicate that the variables are correlated. There are five squares that stick out. The first is between *calculatedfinishedsquarefeet* and *finishedsquarefeet12*. Both of these variables actually capture the same features of a home - the finished living area. \n\nOther lighter squares are *structuretaxvaluedollarcnt*, which measures the value of the structures on the property, *taxvaluedollarcnt*, which measures the value of the property, *landtaxvaluedollarcnt*, which measures the value of the land on the property, and *taxamount*, which is the total tax assessed. It is unsurprising that these variables are highly related to each other. Regardless of their similarity, Zillow decided to include *all of them* in the data provided to contestants. There are a couple of reasons why these variables might prove to be useful. First, the completeness of data collection for each variable might differ, so there might be value in the presence of the variable. And second, they measure slightly different things and variation across different tax areas in L.A. might be meaningful for the purchase of a home.\n\nThere are some surprisingly strong connections between variables one might not have thought would display such strong correlations. For example,  the *regioncountyid* and *airconditioningtypeid* display a very strong negative correlation. One could imagine that newer houses might be equipped with central air conditioning while older houses may have window units. However, the connection to region county seems tenuous unless there is some regulation or code in place that requires certain types of air condition vs. others.\n\nAnother interesting strongly negative correlation arises between the *propertylandusetypeid* variable and *garagetotalsqft*. We wouldn't expect an apartment to have a very large garage (or maybe we should depending on how this variable is measured) while a mansion in Bel Air could have a garage the size of some other homes. Lack of transparency on variable measurement degrades trust in the modeling exercise.\n\nFinally, we can visualize how a sample of these variables are correlated with *logerror* through pair plots. We are choosing to visualize features that are present for more than 90% of observations and are not some sort of numeric identifier. For example, although most properties contain *censustractandblock*, we are not including it in this visualization because it is census ID. This 90% cutoff is being used for this visualization but will not necessarily be adhered to for our model-building.\n\nWhat we care most about in this display are the bivariate plots with logerror.  Remember from the above graph of the logerror that there is a strong presence of outliers. We can use the below graph to see if the logerror outliers are associated with any variables in particular. There are several bivariate plots that have a reverse funnel shape, meaning that the outliers for logerror are located at the bottom and that prediction for the houses with features higher up the y - axis (if you look in the first column). Logerror's relationships with *roomcnt* is particularly interesting, because it seems to suggest that Zillow's algorithm does not make accurate predictions for housing with lower *roomcnt*, and there is a decent spread as the *roomcnt* grows until the accuracy increases rather drastically. In many other cases, the larger the house or the more amount of tax dollars associated with the house, the more accurate prediction. Perhaps there are some mediating variables that make the prediction better for these houses, such as recency of the previous purchase or perhaps recent construction. "},{"metadata":{"_uuid":"fa323e37a6c660d7b27a4c9be74eed47ffd52ede","trusted":true},"cell_type":"code","source":"sns.set()\n\ncolumns = [\n    'logerror', 'lotsizesquarefeet', 'finishedsquarefeet12','landtaxvaluedollarcnt', \n    'calculatedfinishedsquarefeet', 'taxamount', 'roomcnt', 'bathroomcnt', 'bedroomcnt'\n]\n\nsns.pairplot(\n    train_data_labels[columns], \n    size=2, \n    kind ='scatter', \n    diag_kind='kde'\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cf99efc381d464da210b04395acd4bbde5be83c"},"cell_type":"markdown","source":"## Preprocessing and feature engineering"},{"metadata":{"_uuid":"5825504d0a11f38235e3f6ea38faa7c4571f87c6"},"cell_type":"markdown","source":"As shown before, our dataset has several features that have a high percentage of missing values. To create our model, it will be considered only features that have less than 10% missing values. Exceptions to this rule were applied when  based on descriptions of the data dictionary and filled values:\n\n- *poolcnt*: the data dictionary provided by Kaggle mentions that this feature related to the number of pools on the lot (if any). This feature will also be combined with other pool related features to create derived variable that indicates the presence of a pool. Missing values will be filled with zeros (absence of a pool).\n\n- *taxdelinquencyflag*: the data dictionary mentions \"Property taxes for this parcel are past due as of 2015\" and there are only 'Y' values filled. Missing values will be filled with zeros ('N' not delinquent).\n\n- *taxdelinquencyyear*: the data dictionary mentions \"year for which the unpaid property taxes were due\"  and only 'Y'. The feature will be transformed to the total number of years of delinquency. Missing values will be filled with zeros (not delinquent).\n\n- *buildingqualitytypeid*: the data dictionary mentions \"overall assessment of condition of the building from best (lowest) to worst (highest)\" and, although this feature has 35% of NAs, its an important feature and missing values will be filled with mean values.\n\n- *heatingorsystemtypeid*: the data dictionary mentions \"type of home heating system\" and 39% of values are missing. An \"other\" value will be considered for this feature as 95% of households in US have a heating system (According to the 2015 report from U.S. Energy Information Administration)\n\nThe mean value assignments for missing values will be done later on based on a clustering strategy, i.e., the mean value considered will be the one of the cluster that the observation belongs to. "},{"metadata":{"_uuid":"ed4f2c66c8f09e5396dec262dede37a1b71871dd","trusted":true},"cell_type":"code","source":"# pool treatment\ntrain_data['has_pool'] = ((train_data.poolcnt > 0) | (train_data.poolsizesum > 0) | (train_data.pooltypeid10 == 1) | \n                          (train_data.pooltypeid2 == 1) | (train_data.pooltypeid7 == 1)).astype(int)\n\n# tax delinquency treatment\ntrain_data['is_delinquent'] = (train_data.taxdelinquencyflag == 'Y').astype(int)\ndef conditions(x): \n    v = 0\n    if np.isnan(x): \n        return 0\n    if x > 15:\n        return int(2016 - (x + 1900))\n    else:\n        return int(2016 - (x + 2000))\ntrain_data['years_of_delinquency'] = train_data['taxdelinquencyyear'].apply(conditions)\n\n### features with less 10% NAs\n\n# characteristics of property\ncols_property_characteristics = [\n    'roomcnt', \n    'bedroomcnt',\n    'bathroomcnt',\n    'fullbathcnt',\n    'calculatedbathnbr',\n    'calculatedfinishedsquarefeet',\n    'finishedsquarefeet12',\n    'lotsizesquarefeet',\n    'yearbuilt',\n    'assessmentyear',\n    'has_pool'] \n\n# tax information about property (proxy for value)\ncols_tax_info = ['taxamount', 'taxvaluedollarcnt', 'structuretaxvaluedollarcnt',\n                 'landtaxvaluedollarcnt', 'is_delinquent', 'years_of_delinquency']\n\n# use of property (categorical)\ncols_property_use = ['propertylandusetypeid', 'propertycountylandusecode']\n\n# locality (categorical)\ncols_locality = ['regionidcounty', 'regionidcity', 'regionidzip'] \n# other locality (no NAs but possibly redundants with locality) \n# cols_other_locality = ['censustractandblock', 'rawcensustractandblock', 'fips', 'longitude', 'latitude']\n\n# sets a value for NAs\ntrain_data[cols_property_use + cols_locality] = train_data[cols_property_use + cols_locality].fillna('unknown')\n\n# drop NA values\nselected_features = cols_locality + cols_property_characteristics + cols_property_use + cols_tax_info\ntrain_data_no_NAs = train_data[selected_features].dropna()\n\nprint(\"Original dataset:\", train_data.shape)\nprint(\"Dataset without NAs:\", train_data_no_NAs.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa9706d18b7fe8b26ab7792086e0f5ad726213ae"},"cell_type":"markdown","source":"A data standardization is applied to selected features to prepare them for clustering. Leaving variances unequal will lead the KMeans algorithm to put more weight on variables with smaller variance. The features selected for clustering are related to property characteristics and value (proxied by tax value). Categorical variables were dropped from the analysis due to potential inconsistencies that these variables could cause (Euclidean distances usually do not make sense for categorical data). Besides filling NAs values, the clusters will also be used as new features for the modeling step as they could encode non-linarities that the linear regression would not identify. The clusters will also be useful to analize the centroid values and understand the housing market.  "},{"metadata":{"_uuid":"8b461c3c82561fd609cec61268478bb2e94f5527","trusted":true},"cell_type":"markdown","source":"# standardizes features\ndf_to_scale = train_data_no_NAs[cols_property_characteristics + cols_tax_info]\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df_to_scale.values)\nscaled_data = pd.DataFrame(scaled_data, columns=df_to_scale.columns, index=df_to_scale.index)\nscaled_data.shape"},{"metadata":{"trusted":true,"_uuid":"3e4930482de1c7e087eb432d257d4bfe79c94577"},"cell_type":"markdown","source":"scaled_data[sorted(scaled_data.columns)].head()"},{"metadata":{"_uuid":"f8444b8b3f76e373c07e4a4e7b42fe51849cc337"},"cell_type":"markdown","source":"To identify the optimal *K*, the Elbow method is used. The Kmeans clustering is executed multiple times with different values of k and with a smaller number of observations, which were sampled from original dataset. This choice was made due to performance issues. The result is show in the plot below.   "},{"metadata":{"_uuid":"318f677ddce6cfba758b198a8bcb7fbd92ba146c","trusted":true},"cell_type":"markdown","source":"# run clustering with less data to find optimal k\nclustering_data = scaled_data.sample(n=50000)\n\ndistortions = []\nK = range(1,50)\nfor k in K:\n    km = KMeans(n_clusters=k, init='k-means++', n_jobs=-1).fit(clustering_data)\n    distortions.append(sum(np.min(cdist(clustering_data, km.cluster_centers_, 'euclidean'), axis=1)) / clustering_data.shape[0])"},{"metadata":{"trusted":false,"_uuid":"21d868c2dd026944132beddfb8de26e01e589fea"},"cell_type":"markdown","source":"# Plot the elbow\nplt.figure(figsize=(10,10))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow method showing the optimal k')\nplt.show()"},{"metadata":{"_uuid":"7c1a663f50a212a1fa9498156b0411760f87b551"},"cell_type":"markdown","source":"By analysing the plot, it can be seems that the elbow is achieved when k is 13. The algorithm is executed once more with all data and the optimal k. "},{"metadata":{"_uuid":"7f874fff6f8e93607f375301fa328d7af9e2ac1e","trusted":true},"cell_type":"markdown","source":"# Runs the clustering with k=13 based on elbow analysis\noptimal_k = 13\nkm = KMeans(n_clusters=optimal_k, init='k-means++', n_jobs=-1).fit(scaled_data)"},{"metadata":{"_uuid":"cc04ef26f30ff7085a1e8cbf7fb72dad95025a79","trusted":true},"cell_type":"markdown","source":"# scale back and presents typical values for each cluster\ncentroids = scaler.inverse_transform(km.cluster_centers_)\ncentroids = pd.DataFrame(centroids, columns=scaled_data.columns)\ncentroids.round(0)"},{"metadata":{"_uuid":"194aa7957971d0621eadeca6e071b130f8229194"},"cell_type":"markdown","source":"An iterative EM-type algorithm will be used to fill NAs based on the following steps: \n\n1) Initialize missing values to their column means\n\n2) Repeat until convergence:\n\n- Perform K-means clustering on the filled-in data\n\n- Set the missing values to the centroid coordinates of the clusters to which they were assigned\n\nBy this method we will achieve aproximate mean values for missing data. The mean value for each row/feature will not be the mean value for the entire dataset then, but the mean value of its corresponding cluster."},{"metadata":{"trusted":true,"_uuid":"09cf652621c2bf06e60c7204a2f3a00fd41ded79"},"cell_type":"markdown","source":"def kmeans_missing(X, n_clusters, max_iter=20):\n    \"\"\"Perform K-Means clustering on data with missing values.\n\n    Args:\n      X: An [n_samples, n_features] array of data to cluster.\n      n_clusters: Number of clusters to form.\n      initial_centroids\n      max_iter: Maximum number of EM iterations to perform.\n\n    Returns:\n      labels: An [n_samples] vector of integer labels.\n      centroids: An [n_clusters, n_features] array of cluster centroids.\n      X_hat: Copy of X with the missing values filled in.\n    \"\"\"\n\n    # Initialize missing values to their column means\n    missing = ~np.isfinite(X)\n    mu = np.nanmean(X, 0, keepdims=1)\n    X_hat = np.where(missing, mu, X)\n    \n    # Standardizes features\n    scaler = StandardScaler()\n    X_hat = scaler.fit_transform(X_hat)\n    \n    for i in range(max_iter):\n        \n        if (i % 10) == 0:\n            print(\"Iteration:\", i)\n        \n        if i > 0:\n            # initialize KMeans with the previous set of centroids. \n            cls = KMeans(n_clusters, init=prev_centroids, n_jobs=-1)\n        else:\n            cls = KMeans(n_clusters, n_jobs=-1)\n\n        # perform clustering on the filled-in data\n        predicted_labels = cls.fit_predict(X_hat)\n        predicted_centroids = cls.cluster_centers_\n\n        # fill in the missing values based on their cluster centroids\n        X_hat[missing] = predicted_centroids[predicted_labels][missing]\n\n        # when the labels have stopped changing then we have converged\n        if i > 0 and np.all(predicted_labels == prev_labels):\n            print(\"Clusters converged after\", i, \"iterations\")\n            break\n\n        prev_labels = predicted_labels\n        prev_centroids = predicted_centroids \n\n    # Scale back values\n    new_centroids = pd.DataFrame(scaler.inverse_transform(predicted_centroids), columns=X.columns)\n    X_hat = pd.DataFrame(scaler.inverse_transform(X_hat), columns=X.columns, index=X.index)\n    \n    return predicted_labels, new_centroids, X_hat, missing, cls, scaler\n\n# obtain dataset with NAs filled based on clusters means\nlabels, new_centroids, train_data_no_NA, missing, cls, scaler = kmeans_missing(\n    train_data[cols_property_characteristics + cols_tax_info], optimal_k)"},{"metadata":{"_uuid":"9e968ec730e43893b824bc28d44b4ad9691d9e30"},"cell_type":"markdown","source":"The table below shows the new clusters obtained after several iterations."},{"metadata":{"trusted":false,"_uuid":"e341867d61a312d6a06ee2b49070d4e85e3e4df0"},"cell_type":"markdown","source":"new_centroids.round(0)"},{"metadata":{"_uuid":"eab36e55df35798a1926a89b68c3c5d2081ca372"},"cell_type":"markdown","source":"The obtained clusters are added as a new feature to the dataset, all numerical types are converted to integers and all categorical type are converted to panda's category data type."},{"metadata":{"trusted":false,"_uuid":"b3ab933fbd37a9526a366aef5f1710609b50cf3f"},"cell_type":"markdown","source":"# adding clusters as a variable\ntrain_data_no_NA['cluster'] = labels\n\n# converting data types and joining categorical and non catgorical data in the final dataset\nprepared_data = pd.concat(\n    [train_data_no_NA.round(0).astype('int64'), train_data[cols_locality + cols_property_use]], \n    axis=1, \n    sort=False)\ncols = cols_locality + cols_property_use + ['cluster', 'is_delinquent', 'has_pool']\nprepared_data[cols] = prepared_data[cols].astype('category')\n\nprint(prepared_data.shape)\nprint(prepared_data.dtypes)"},{"metadata":{"trusted":false,"_uuid":"136f9cd3b55b50c29f69f952b26ed53139a668b8"},"cell_type":"markdown","source":"pd.set_option('display.max_columns', 25)\nprepared_data[sorted(prepared_data.columns)].head()"},{"metadata":{"_uuid":"85f940019f8ee4fffbb9c5a8502a839eca8671dd"},"cell_type":"markdown","source":"The prepared dataset, the kmeans model and the scaler model are then persisted. "},{"metadata":{"_uuid":"ac3853192323c55f07a6c3956e5333f89313a6af","trusted":true},"cell_type":"markdown","source":"# persists the prepared dataset\nprepared_data.to_pickle('./prepared_data.pkl')\n\n# load the persisted dataset \n# prepared_data = pd.read_pickle(\"./prepared_data.pkl\")\n\n# persists the kmeans model\njoblib.dump(cls, './kmeans_model.joblib') \n\n# load the kmeans  model\n# cls = joblib.load('kmeans_model.joblib') \n\n# persists the scaler\njoblib.dump(scaler, './scaler.joblib') \n\n# load the scaler\n# scaler = joblib.load('scaler.joblib') "},{"metadata":{"_uuid":"ee97995dbe9996c5239fd47d826daf5214fd12b9"},"cell_type":"markdown","source":"## Training Data Set for Baseline Model"},{"metadata":{"_uuid":"02bafc6503abf2589bf4080de06122bec127ed2c"},"cell_type":"markdown","source":"For training the baseline model, we ignore any of the feature engineering that we've done previously and any missing value imputation. In this first pass, we want to use all of the variables to see what pops as important and what doesn't. We use a simple measure to fill in the missing data with 0s knowing that imputed values potentially offer us some low hanging fruit for improving the model."},{"metadata":{"trusted":true,"_uuid":"9f97f9b780bd9345665bc33a8282db7abfa87afd"},"cell_type":"code","source":"# transforming data type in place\nfor c, dtype in zip(train_data.columns, train_data.dtypes):\n    if dtype == np.float64:\n        train_data[c] = train_data[c].astype(np.float32)\n  \n# First we are going to make sure the training data only has attributes for the training labels\ntrain_df = train_data.merge(train_labels, how = 'inner', left_on = 'parcelid', right_on = 'parcelid')\nprint(train_df.shape)\ny = train_df['logerror']\n\n# creating train and validation sets\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2)\n\n# remove unnecessary columns from train_df\nX_train = X_train.drop(['parcelid', 'logerror','transactiondate', 'propertyzoningdesc',\n                  'taxdelinquencyflag', 'propertycountylandusecode','hashottuborspa','fireplaceflag'], axis=1)\nX_test = X_test.drop(['parcelid', 'logerror','transactiondate', 'propertyzoningdesc',\n                  'taxdelinquencyflag', 'propertycountylandusecode','hashottuborspa', 'fireplaceflag'], axis=1)\n\n# filling in missing values with a zero...stop gap solution\nX_train.fillna(0, inplace= True)\nX_test.fillna(0, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ec67b0dfe66e865fcfece6ad920a54706b1ddb3"},"cell_type":"markdown","source":"## Baseline Models"},{"metadata":{"_uuid":"291afda61649457de51e76c79499005cee385180"},"cell_type":"markdown","source":"To start, we use 5 different machine learning models: Lasso, Random Forest, Decision Trees, Xtreme Gradient Boosted Machine, and Light GBM. The hope is that these models will vary in performance, suggesting a model that we could potentially grid search and refine for the best performance. We will also be able to investigate how the different tree models weight the importance of the variables, which we can use to inform further feature creation and possibly model selection. Lastly, we can look to see where these baseline models perform worse to understand if there is any way to improve performance."},{"metadata":{"trusted":true,"_uuid":"880f95f810dfa3e6bdb0c3bb90063b57614f1f04","scrolled":false},"cell_type":"code","source":"# storing model object with basic parameters, we didn't standardize prior, so we normalize here\n# alpha set to near 0 is essentially a linear regression without penalties\nls = Lasso(alpha=1e-6, normalize=True)\n\n# Various Tree Models\nrf = RandomForestRegressor(n_estimators = 12, max_depth = 5)\ndt = DecisionTreeRegressor(max_depth = 4)\nxg = XGBRegressor(n_jobs = 1)\nlg = lgb.LGBMRegressor(num_leaves = 31,\n                        learning_rate = 0.05,\n                        n_estimators = 20)\n\n# Other models that were compared\n#sv = SVR(kernel='linear', C=1e3)\n#ada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=300, random_state=seed)\n#cb = CatBoostRegressor(iterations=300, learning_rate=0.01,\n#                        depth=4, l2_leaf_reg=3,\n#                        loss_function='MAE',\n#                        eval_metric='MAE',\n#                        random_seed=seed)\n\n## Feeding a bunch of baseline model objects with set parameters into a function and have it return\n## various details\n\n# Storing model names and their associated objects\nmodel_names = ['Lasso','DecisionTrees', 'Random Forest', 'Xtreme Gradient Boosted', 'Light GBM']\nmodel_objects = [ls, dt, rf, xg, lg]\n\n# Creating a dictionary for storing different model results\nmodel_output = {\"model_names\": model_names, \"model_objects\": model_objects, \"mean_abs_error\": [], \n                \"predictions\": [], \"variable_importance\": []}\n\n# Creating empty lists for storing model outputs for the dictionary\nmae = []\nvariable_importance = []\npredictions = []\n\n# Function to run through models \ndef run_models(model_objects, X_train, y_train, X_test, y_test):\n    for model in model_objects:\n        model.fit(X_train, y_train)\n        p = model.predict(X_test)\n        mae.append(mean_absolute_error(y_test, p))\n        predictions.append(p)\n        try:\n            variable_importance.append(model.feature_importances_)\n        except: \n            variable_importance.append(\"no such attribute\")\n    model_output[\"mean_abs_error\"] = mae\n    model_output[\"predictions\"] = predictions\n    model_output[\"variable_importance\"] = variable_importance\n    return(model_output)\n\n# fit and predict all of the models using the function above\nrun_models(model_objects, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b68779fea0a772bc9cc5fa5ae0d2e6dedffa313d"},"cell_type":"code","source":"# A Nicer Table of the Baseline Models\nfrom tabulate import tabulate\n['Lasso', 'DecisionTrees', 'Random Forest', 'Xtreme Gradient Boosted', 'Light GBM']\nprint(tabulate([['Lasso', model_output[\"mean_abs_error\"][0]], \n                ['DecisionTrees', model_output[\"mean_abs_error\"][1]],\n                ['Random Forest', model_output[\"mean_abs_error\"][2]],\n                ['Xtreme Gradient Boosted', model_output[\"mean_abs_error\"][3]],\n                ['Light GBM', model_output[\"mean_abs_error\"][4]]\n               ], headers=['Model Name', 'MAE'], tablefmt='orgtbl'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bf87af7bcfaf27195b3109bb76a042c3fb4dea3"},"cell_type":"markdown","source":"All of the models chosen are in within very close proximity with regards to performance. The Light GBM has shown the best performance of all of the models and we will be moving forward with it for our grid search."},{"metadata":{"_uuid":"156f7711cfb6e42f3fcdc524452fcc8e24654e53"},"cell_type":"markdown","source":"## Feature Importance for Models"},{"metadata":{"trusted":true,"_uuid":"117bdfcf760be23c6cc5305ffc4d53c7a8abea21"},"cell_type":"code","source":"# plotting feature importances\ndef plot_importances(model_name, i, model_output, top_n):\n    # we only want to plot the variable importance for those models with that attribute\n    if model_output[\"variable_importance\"][i] != \"no such attribute\":\n        # extract from model output dictionary and store as a dataframe\n        feat_imp = pd.DataFrame({'importance': model_output[\"variable_importance\"][i]})    \n        \n        # grab the column names then sort in place\n        feat_imp['feature'] = X_train.columns\n        feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n        \n        # grabbing only the top n features according to their weight\n        feat_imp = feat_imp.iloc[:top_n]\n        feat_imp.sort_values(by='importance', inplace=True)\n        feat_imp = feat_imp.set_index('feature', drop=True)\n        \n        # plotting components\n        feat_imp.plot.barh(title=\"Variable Importance {}\".format(model_name), figsize=(10,10))\n        plt.xlabel('Feature Importance Score')\n        plt.show()\n    else: print(\"no variable importance attribute for {}\".format(model_name))\n\n# loop through models and plot the various variable importances\nj = 0\nfor model_name in model_names:\n    plot_importances(model_name, j, model_output, top_n = 15)\n    j = j + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e41b2d257941007b6de934db0b1209ca1f8cd45"},"cell_type":"markdown","source":"There are remarkable differences on variable importance across the models. Despite having the fewest variables in the graph, decision trees still perform remarkable well compared to the other models. The fact that latitude matters a lot across the different models comes as no surprise. The farther West one goes in L.A. the closer to the water one gets. That is likely to come at a premium and there is likely to be some turnover at these kinds of locations. Random forest and Xtreme gradient boosting both appear to prioritize tax money and size of the building before getting to variables regarding location. While light gbm contains a different set of variables, placing a lot of importance on the zip code and finished square feet. Bedroom and bathroom counts, something that most of us consider to be very important when buying a house (I would know, I just bought one this month!), rarely show up and show up with no consistency."},{"metadata":{"_uuid":"1e40d24a38ae7b605dfc804852f6114fc39e2b7b"},"cell_type":"markdown","source":"## Correlation between model predictions"},{"metadata":{"trusted":true,"_uuid":"6396cf7fd2d7329e63382e38e6eb896a39e20f41"},"cell_type":"code","source":"# Determining if the model predictions are highly related across models\n\n# create a numpy array that contains all of the prediction for each model as column vectors\nmerged_array = model_output['predictions'][0]\n\nk = 1\nfor k in range(1, len(model_names)):\n merged_array = np.vstack((merged_array, model_output['predictions'][k]))\nmerged_array = merged_array.T\ndataset = pd.DataFrame({'Lasso':merged_array[:,0],'DecisionTrees':merged_array[:,1], \n                        'Random Forest':merged_array[:,2], 'Xtreme Gradient Boosted':merged_array[:,3], \n                        'Light GBM': merged_array[:,4]})\n\n# Calculate the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f5762a0bed13f4863cc4f595e207bf30d5e283e"},"cell_type":"markdown","source":"We just witnessed the variability in the variables used to construct trees. We were curious how related the model predictions would be correlated across the baseline models. It turns out that they are not very correlated. This suggests that it might be a worthwhile exercise to combine these models via an ensemble method. That's beyond the scope of this notebook but definitely a worthy avenue to pursue."},{"metadata":{"_uuid":"061566b21f6cfb133b0d716ed63c88a55552ffd8"},"cell_type":"markdown","source":"## Error Analysis"},{"metadata":{"trusted":true,"_uuid":"bc87a8e89d22322c5573da1eebbebde9ee5021fc"},"cell_type":"code","source":"# finding the largest errors made by a model\ni = 0\ndef find_errors(model_output, y_test, i, number):\n    \n    # extract the predictions from the dictionary, note that this is a series object\n    distances = abs(model_output[\"predictions\"][i] - y_test)\n    distances_srt = distances.sort_values(ascending = False)\n    # extract top 100 largest values and return their indices\n    return(distances_srt.keys()[range(0,number)])\n\n# Choose model and inputs\ni = 2\nerrors = find_errors(model_output, y_test,i,50)\n\n# Let's take a look at the target values\ny_test[y_test.index.isin(errors)]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ea95d04bc8e7c8fd29808f4a46333e68b51cf2e"},"cell_type":"code","source":"X_test[X_test.index.isin(errors)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07df5dc58de0906bc3719bbfe1ec75d39058748"},"cell_type":"markdown","source":"A couple of items are relatively obvious after looking at the data for which our baseline models have made the largest errors. First, we have a problem being able to accurately predict the outliers in the data, as seen by the very large test values. Second, there is no obvious pattern for why these predictions are being missed. Property land use, region, and tax amounts vary across the largest errors. The one variable that does stand out on investigation is the yearbuilt. It looks as though most of the error for the random forest model are for those houses built before 2000. Perhaps, a binary variable for the year built will help with the outliers."},{"metadata":{"trusted":true,"_uuid":"c5893e7bd18f599db4eb0b1f74fd80f39fb4680b"},"cell_type":"code","source":"def plot_error_location(errors, X_test):\n    plot_df = X_test[X_test.index.isin(errors)]\n    plt.figure(figsize = (12,12))\n    sns.jointplot(x = plot_df.latitude.values, y = plot_df.longitude.values, size=10)\n    plt.ylabel('Longitude', fontsize =  12)\n    plt.xlabel('Latitude', fontsize = 12)\n    plt.show()\n    \nplot_error_location(errors, X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e68e61fbe4d134fd46774b9cdf3f3e51ccbdc502"},"cell_type":"markdown","source":"The above graph gives some perspective on the location of our largest errors. A more rigorous feature set might contain new ways of dividing up the map. It seems that the no location variable does a sufficiently good job on its own."},{"metadata":{"_uuid":"ef213449da2c41ea6e5910a0e22fe9e9b3edb517"},"cell_type":"markdown","source":"## Grid Search of LGM"},{"metadata":{"_uuid":"dee37794dee87498dd680914a6373afd79a0e5db"},"cell_type":"markdown","source":"We've chosen to grid search the LGM model based on the preliminarily better results that were achieved above. The LGM model also performs well with regards to speed, which matters in this case because the grid search is running on a relatively small compute node.  For example, a relatively small grid search, such as the one below, could take nearly 30 minutes to run. \n\nIn the grid search below and other grid searches that we ran, we varied the learning rate, estimators, and number of leaves. Grid searching the learning rate hyperparameter gives us the best shot at finding a true minimum, hopefully without getting stuck at a local minimum. The number of leaves will control the complexity of the tree, with more leaves representing a more complex tree. And lastly the \n\nGenerally, we found that the smaller learning rate increases our predictive ability, but only marginally and at the cost of the training taking more time. Accuracy also increased with the number of leaves until the complexity became too great and we started overfitting the model. This is when we started including some of the other hyperparameters that control resampling. These were implemented to combat overfitting as we increased the complexity of the model."},{"metadata":{"trusted":true,"_uuid":"7b722586988d3106e43a7127146656a62cd82ec9"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n# Create parameters to search\nparam_grid = {\n    'learning_rate': [0.01, 0.1], # size of the steps we take on the search\n    'n_estimators': [20,40,60],\n    'num_leaves': [6,8,12,16],\n    'boosting_type' : ['gbdt'], # default setting for gradient boosted trees\n    'objective' : ['regression_l1'], # this is the equivalent to mean absolute error\n    'colsample_bytree' : [0.65, 0.66], # helps us with overfitting\n    'subsample' : [0.7,0.75],# how we resample from the data\n    'lambda_l1' : [1,1.2], # regularization parameter\n    'lambda_l2' : [1,1.2,1.4],# regularization parameter\n    }\n\n# Create regressor to use. with default parameters as input\nmdl = lgb.LGBMRegressor(num_leaves = 31)\n\n# Create the grid\ngrid = GridSearchCV(mdl, param_grid,\n                    verbose=0,\n                    cv=4,\n                    n_jobs=2)\n# Run the grid\ngrid.fit(X_train, y_train)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40156b7bfd831144d99a101386c8e148c9a67ecb"},"cell_type":"code","source":"# Setting parameters based on grid search...don't want to keep running as it takes 20-30 minutes\nbest_fit = lgb.LGBMRegressor(objective = 'regression_l1',\n          n_estimators = 60,\n          learning_rate = 0.1,\n          subsample = 0.7,\n          colsample_bytree = 0.66,\n          lambda_l1 = 1,\n          lambda_l2 = 1,\n          num_leaves = 40)\n\n# fitting the model and seeing how we do on the eval set\nbest_fit.fit(X_train,y_train,\n            eval_set = [(X_test,y_test)],\n            eval_metric = 'l1',\n            early_stopping_rounds = 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99b58bee0d7ac57ac4bbcca7ffd9dd838226ec89"},"cell_type":"markdown","source":"## Variable Importance and Error Analysis"},{"metadata":{"_uuid":"0558e8b886a3d6cf6df110a17e2a9b3872f18e45"},"cell_type":"markdown","source":"Using the same methods created above, we can investigate the importance certain variables in the model. Again, we see that the size of the house and the tax amounts are the most important variables, but not the next set of variables are very different. When the house was built and where it was built end up being very influential. This is logical, because older houses tend to need more work done in order to make them livable. The end result being that the price of the house will decrease with the amount of expected work that is needed for updates. And second, locaition, and specifically lat and lon are a very precise way to determine where some house is on the map. Proxies like zip code and region potentially group together houses that are vastly different."},{"metadata":{"trusted":true,"_uuid":"8578dc691b5c7801650d2b51b167f271c1971bc4"},"cell_type":"code","source":"# Looking at variable importance\nmodel_output_bf = {\"model_names\": ['lgb_bestfit'], \"model_objects\": best_fit,\n                   \"mean_abs_error\": mean_absolute_error(y_test, best_fit.predict(X_test)), \n                \"predictions\": best_fit.predict(X_test), \n                   \"variable_importance\": [best_fit.feature_importances_]}\nj = 0\nplot_importances('lgb_bestfit', j, model_output_bf, top_n = 15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d580215a20a795d6ddcf935e97677c117e9731d5"},"cell_type":"markdown","source":"Unfortunately, we see that the most common errors are still being made on outliers. It does not appear that tuning the model helps us in becoming better predictors of outliers."},{"metadata":{"trusted":true,"_uuid":"ead42a4d972def2c83906ef2abb24c1575eb9b7e"},"cell_type":"code","source":"# using the function created above\ni = 0\nerrors = find_errors(model_output_bf, y_test,i,50)\n\n# Let's take a look at the target values\ny_test[y_test.index.isin(errors)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d64adf6f3ab1b51fd7e6a44aa9685dc79797b1c"},"cell_type":"code","source":"X_test[X_test.index.isin(errors)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7559d9b5098d80f199936201b0dd3cab77f4f033"},"cell_type":"markdown","source":"## Next Steps"},{"metadata":{"_uuid":"2d15826786497194d97d14935db9bfa80b2545f7"},"cell_type":"markdown","source":"We believe there are two fruitful avenues we could pursue to make the model more accurate. First, we might want to take the outliers out of the training set and determine if there are any improvements in our accuracy. Second, we could look for a model that does a better job with predicting the outliers and try a stacking approach in which we combine model predictions to make a better prediction overall."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}