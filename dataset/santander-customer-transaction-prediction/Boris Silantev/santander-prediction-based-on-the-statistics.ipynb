{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Santander prediction based on the statistics</h1>\nHello everyone.<br>\nThis kernel predicts the probabilities in Santander competition based on the statistics.<br>\nFirst of all I show that the correlation between the features as well as between features give target=1 is very weak that lets me to suppose that the features are independent. In this case following reasoning can be applied:<br>\nVij – the event that var_i is equal to j<br>\nT – the event that target is equal to 1<br>\nP(T|⋂i(Vij)) = P(T⋂(⋂i(Vij)) / P(⋂i(Vij)) = P(⋂i(Vij)|T) \\* P(T) / P(⋂i(Vij)) = ∏i(P(Vij|T) \\* P(T) / ∏i(P(Vij)<br>\nSo the main function has several parameters, you can try to change them or modify the function"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport copy\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\nx_data = df.drop(['ID_code','target'],axis=1).values\ny_data = df['target'].values\ndf_test = pd.read_csv('../input/test.csv')\nx_test = df_test.drop('ID_code',axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(np.triu(np.corrcoef(x_data,rowvar=False),k=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.max(np.triu(np.corrcoef(x_data[y_data==1],rowvar=False),k=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GetAucRoc(x_train, x_test, y_train, y_test,\n              train_test_merger = False,\n              n_bins = 23,\n              range_limit = 3,\n              bins_finding_method = 'Equal steps',\n              n_vars = 200,\n              smoothing_type=None,\n              smoothing_epsilon=0):\n    \"\"\"\n    This function returns ROC_AUC metrics of the submission provided by GetSubmission()\n    function in the case if y_test is provided:\n    y_test: test set of the targets (will be applied for measure the quality of submission)\n    For more details see the GetSubmission() function\n    \"\"\"\n    y_hat, y_hat_test = GetSubmission(x_train, x_test, y_train,\n                                      train_test_merger = train_test_merger,\n                                      n_bins = n_bins,\n                                      range_limit = range_limit,\n                                      bins_finding_method = bins_finding_method,\n                                      n_vars = n_vars,\n                                      smoothing_type = smoothing_type,\n                                      smoothing_epsilon = smoothing_epsilon)\n    res = list()\n    res.append(roc_auc_score(y_train,y_hat))\n    res.append(roc_auc_score(y_test,y_hat_test))\n    return res\n\ndef GetSubmission(x_train, x_test, y_train,\n              train_test_merger = False,\n              n_bins = 23,\n              range_limit = 3,\n              bins_finding_method = 'Equal steps',\n              n_vars = 200,\n              smoothing_type = None,\n              smoothing_epsilon = 0):\n    \"\"\"\n    This function gets several parameters described below and returns the submission\n    for Santander prediction competition based on the statisctical methods.\n    x_train: train set of features\n    x_test: test set of of features (that will be the base for prediction)\n    y_train: train set of targets\n    train_test_merger: if True the probability of feature being within the range will be\n                       evaluated based on the data in both x_train and x_test, otherwise only\n                       based ob the data in x_train\n    n_bins: in order to calculate the probabilities the function divides the range of the feature\n            values into segments, n_bins corresponds to number of such segments. Increasing of \n            the n_bins let make the prediction more accurate but with too big n_bins the \n            prediction becomes overfitted. If smoothing is applied the overfitting doesn't appear\n            but there is no additional benefits from n_bins increasing\n    range_limit: in order to calculate the probabilities the function divides the range of the\n                 feature values into segments but the function also uses 2 open segments\n                 (-infinity, m(var) - range_limit * std(var)) and \n                 (m(var) + range_limit * std(var), +infinity)\n                 As a default the value 3 is applied that corresponds to 0,28% of the feature\n                 values in this 2 open segments\n    bins_finding_method: in order to divide the range of the feature values different methods can\n                         be applied. In this function following of them are realized:\n                         - \"Equal normal probability\" means that the range is divided into\n                           segments so that if the feature is distributed normally than the \n                           probabilities that the feature value is within the different segments \n                           are equal\n                         - \"Linearly changing probability\" means that the range is divided into\n                           segments so that if the feature is distributed normally than the\n                           probabilities that the feature value is within the different segments\n                           increase linearly till the mean of distribution and than decrease \n                           linearly\n                         - \"Equal a posteriori probability\" means that the range is divided into\n                           segments so that each segments contains equal number of values from \n                           train set\n                         - \"Equal steps\" is used as default method and means that the segments \n                           are of the equal length\n                         Actually the difference between the methods is almost invisible \n                         especially if the smoothing is applied\n    n_vars: number of vars, 200 for Santander competition\n    smoothing_type: to avoid overfitting the smoothing can be applied. Smoothing can be realized \n                    in many different manners but only one type is realized in this function:\n                    - 'plus_minus_epsilon' changes the calculation of probabilies. Without \n                      smoothing the probabilities are calculated as a frequency of the feature \n                      hitting within the range defined by bins, in the case of \n                      'plus_minus_epsilon' the range is enlarged by smoothing_epsilon from both \n                      sides\n    smoothing_epsilon: see smoothing_type description\n    \"\"\"\n    \n    Scaler = StandardScaler()\n    if train_test_merger:\n        x_data = np.vstack((x_train,x_test))\n    else:\n        x_data = x_train.copy()\n    x_data_scaled = Scaler.fit_transform(x_data)\n    x_train_scaled = Scaler.transform(x_train)\n    x_test_scaled = Scaler.transform(x_test)\n    \n    bins = list()\n    if bins_finding_method == \"Equal normal probability\":\n        for i in range(n_vars):\n            norm_limit = norm.cdf(range_limit,loc=0,scale=1)\n            bins.append(norm.isf(np.linspace(norm_limit, 1-norm_limit, n_bins+1),loc=0,scale=1))\n    elif bins_finding_method == \"Linearly changing probability\":\n        for i in range(n_vars):\n            norm_limit = norm.cdf(range_limit,loc=0,scale=1)\n            u = np.arange(1,n_bins//2+n_bins%2)\n            u = np.hstack((u,np.flip(u)[n_bins%2:]))\n            u_cum = np.cumsum(u)\n            u_sum = u.sum()\n            v = (2*norm_limit-1)*(1-np.r_[0,u_cum/u_sum])+(1-norm_limit)\n            bins.append(norm.isf(v,loc=0,scale=1)) \n    elif bins_finding_method == \"Equal a posteriori probability\":\n        for i in range(n_vars):\n            r = np.zeros(n_bins+1)\n            r[0] = - range_limit\n            r[-1] = range_limit\n            u = (np.floor(np.linspace(0,x_data_scaled[:,i].size,n_bins+1))[1:-1]).astype(int)\n            v = x_data_scaled[:,i].copy()\n            v.sort()\n            r[1:-1] = np.minimum(np.maximum((v[u] + v[u+1]) / 2,-range_limit),range_limit)\n            bins.append(r)\n    else:\n        for i in range(n_vars):\n            bins.append(np.linspace(-range_limit, range_limit, n_bins+1))\n            \n    P_Vij = list()\n    for i in range(n_vars):\n        a0 = (x_data_scaled[:,i] < bins[i][0]).sum()\n        if smoothing_type is None:\n            a10 = (x_data_scaled[:,i].reshape(x_data_scaled[:,i].shape[0],1) >= bins[i][:-1])\n            a11 = (x_data_scaled[:,i].reshape(x_data_scaled[:,i].shape[0],1) < bins[i][1:])\n            a1 = (a10 & a11).sum(axis=0)\n        elif smoothing_type == 'plus_minus_epsilon':\n            a10 = (x_data_scaled[:,i].reshape(x_data_scaled[:,i].shape[0],1) >= bins[i][:-1]-smoothing_epsilon)\n            a11 = (x_data_scaled[:,i].reshape(x_data_scaled[:,i].shape[0],1) < bins[i][1:]+smoothing_epsilon)\n            a1 = (a10 & a11).sum(axis=0) * (bins[i][1:]-bins[i][:-1]) / (bins[i][1:]-bins[i][:-1]+2*smoothing_epsilon)            \n        a2 = (x_data_scaled[:,i] >= bins[i][n_bins]).sum()\n        P_Vij.append(np.r_[a0,a1,a2]/x_data_scaled[:,i].size)\n        \n    P_Vij_given_T = list()\n    for i in range(n_vars):\n        a0 = ((x_train_scaled[:,i] < bins[i][0]) * y_train).sum()\n        if smoothing_type is None:\n            a10 = (x_train_scaled[:,i].reshape(x_train_scaled[:,i].shape[0],1) >= bins[i][:-1])\n            a11 = (x_train_scaled[:,i].reshape(x_train_scaled[:,i].shape[0],1) < bins[i][1:])\n            a1 = ((a10 & a11)*y_train.reshape(y_train.shape[0],1)).sum(axis=0)\n        elif smoothing_type == 'plus_minus_epsilon':\n            a10 = (x_train_scaled[:,i].reshape(x_train_scaled[:,i].shape[0],1) >= bins[i][:-1]-smoothing_epsilon)\n            a11 = (x_train_scaled[:,i].reshape(x_train_scaled[:,i].shape[0],1) < bins[i][1:]+smoothing_epsilon)\n            a1 = ((a10 & a11)*y_train.reshape(y_train.shape[0],1)).sum(axis=0) * (bins[i][1:]-bins[i][:-1]) / (bins[i][1:]-bins[i][:-1]+2*smoothing_epsilon)            \n        a2 = ((x_train_scaled[:,i] >= bins[i][n_bins]) * y_train).sum()\n        P_Vij_given_T.append(np.r_[a0,a1,a2]/x_train_scaled[(y_train==1),i].size)\n        \n    P_T = y_train.sum() / y_train.size\n    P_T_given_Vij = list()\n    for i in range(n_vars):\n        P_T_given_Vij.append(np.nan_to_num(P_Vij_given_T[i] * P_T / P_Vij[i]))\n        \n    y_hat = np.ones((x_train_scaled.shape[0],),dtype=float)\n    y_hat = y_hat * P_T\n    for i in range(n_vars):\n        in_bin = n_bins + 1 - (x_train_scaled[:,i].reshape((x_train_scaled[:,i].shape[0],1)) < bins[i]).sum(axis=1)\n        y_hat = y_hat * P_T_given_Vij[i][in_bin] / P_T\n    y_hat_test = np.ones((x_test_scaled.shape[0],),dtype=float)\n    y_hat_test = y_hat_test * P_T\n    for i in range(n_vars):\n        in_bin = n_bins + 1 - (x_test_scaled[:,i].reshape((x_test_scaled[:,i].shape[0],1)) < bins[i]).sum(axis=1)\n        y_hat_test = y_hat_test * P_T_given_Vij[i][in_bin] / P_T\n    res = list()\n    res.append(y_hat)\n    res.append(y_hat_test)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = GetSubmission(x_data, x_test, y_data,\n                    train_test_merger=False,\n                    n_bins=1000,\n                    bins_finding_method='Equal steps',\n                    smoothing_type = 'plus_minus_epsilon',\n                    smoothing_epsilon = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = pd.DataFrame({'ID_code':df_test['ID_code'], 'target':pd.Series(subm[1])})\nsubm.to_csv('submissionS.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}