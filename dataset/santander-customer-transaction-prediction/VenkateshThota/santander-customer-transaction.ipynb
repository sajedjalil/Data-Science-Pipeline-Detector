{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport lightgbm as lgb\nimport eli5 \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pdpbox import pdp, get_dataset, info_plots\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve,classification_report,roc_curve,auc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing train Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\npd.options.display.max_columns = None ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of the dataset \ndf_train.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary of the dataset \ndf_train.describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target Class Count \ntarget_class=df_train['target'].value_counts()\nprint('Count of the target class :\\n',target_class) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Percentage of target class count\nper_target_class=df_train['target'].value_counts()/len(df_train)*100\nprint('Percentage of target class count :\\n',per_target_class) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory data analysis is one of the most important steps in data mining in order to know features of data. It involves the loading dataset, target classes count, data cleaning,typecasting of attributes, missing value analysis, Attributes distributions and trends. So,we must clean the data otherwise it will affect on performance of the model. Now we are going to explain one by one as follows. In this EDA I explained with seaborn visualizations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count plot & violin plot for target class \nfig,ax=plt.subplots(1,2,figsize=(20,5))\nsns.countplot(df_train.target.values,ax=ax[0],palette='spring')\nsns.violinplot(x=df_train.target.values,y=df_train.index.values,ax=ax[1],palette='spring')\nsns.stripplot(x=df_train.target.values,y=df_train.index.values,jitter=True,color='black',linewidth=0.5,size=0.5,alpha=0.5,ax=ax[1],palette='spring')\nax[0].set_xlabel('Target')\nax[1].set_xlabel('Target')\nax[1].set_ylabel('Index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We are having a unbalanced data, where 90% of the data is no. of customers who will not make a transaction & 10 % of the data are those who will make a transaction. \n- From the violin plots, it seems that there is no relationship between the target and index of the data frame, it is more dominated by zero compare to one's.\n- From the jitter plots with violin plots, we can observe that target looks uniformly distributed over the indexes of the data frame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of train attributes\ndef plot_train_attribute_distribution(t0,t1,label1,label2,train_attributes):\n    i=0\n    sns.set_style('darkgrid')\n    fig=plt.figure()\n    ax=plt.subplots(10,10,figsize=(22,18))\n    for attribute in train_attributes:\n        i+=1\n        plt.subplot(10,10,i)\n        sns.distplot(t0[attribute],hist=False,label=label1)\n        sns.distplot(t1[attribute],hist=False,label=label2)\n        plt.legend()\n        plt.xlabel('Attribute',)\n        sns.set_style(\"ticks\",{\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observing train attributes \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Corresponding to negative class\nt0=df_train[df_train.target.values==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Corresponding to possitive class\nt1=df_train[df_train.target.values==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train attributes from 2 to 102\ntrain_attributes=df_train.columns.values[2:102]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of train attributes from var_0 to var_99","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot distribution of train attributes\nplot_train_attribute_distribution(t0,t1,'0','1',train_attributes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train attributes from 102 to 202\ntrain_attributes=df_train.columns.values[102:202]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot distribution of train attributes\nplot_train_attribute_distribution(t0,t1,'0','1',train_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can observed that there is a considerable number of features which are significantly have different distributions for two target variables. For example like var_0,var_1,var_9,var_19,var_18 etc.\n- We can observed that there is a considerable number of features which are significantly have same distributions for two target variables. For example like var_3,var_7,var_10,var_17,var_35 etc. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing the test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_test=pd.read_csv(\"../input/santander-customer-transaction-prediction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of test attributes\ndef plot_test_attribute_distribution(test_attributes):\n    i=0\n    sns.set_style('darkgrid')\n    fig=plt.figure()\n    ax=plt.subplots(10,10,figsize=(22,18))\n    for attribute in test_attributes:\n        i+=1\n        plt.subplot(10,10,i)\n        sns.distplot(df_test[attribute],hist=False)\n        plt.xlabel('Attribute',)\n        sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n    plt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test attribiutes from 1 to 101 \ntest_attributes=df_test.columns.values[1:101]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of test attributes from var_0 to var_99 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot distribution of test attributes\nplot_test_attribute_distribution(test_attributes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test attributes from 101 to 202\ntest_attributes=df_test.columns.values[101:202]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of test attributes from var_100 to var_199 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the distribution of test attributes\nplot_test_attribute_distribution(test_attributes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of mean values per column in train and test dataset \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of Mean Values per column in train & test dataset\nplt.figure(figsize=(16,8))\ntrain_attributes=df_train.columns.values[2:202]\ntest_attributes=df_test.columns.values[1:201]\n#Distribution plot for mean values per column in train attributes: \nsns.distplot(df_train[train_attributes].mean(axis=0),color='red',kde=True,bins=150,label='train')\n#Distribution plot for mean values per column in test attributes: \nsns.distplot(df_test[test_attributes].mean(axis=0),color='blue',kde=True,bins=150,label='test') \nplt.title('Distribution of Mean Values per column in train & test dataset')\nplt.legend()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of mean values per row in train and test dataset:- ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.distplot(df_train[train_attributes].mean(axis=1),color='red',kde=True,bins=150,label='train') \nsns.distplot(df_test[test_attributes].mean(axis=1),color='blue',kde=True,bins=150,label='test') \nplt.title('Distribution of Mean Values per row in train & test dataset')\nplt.legend()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of standard deviation (std) values per column in train and test\ndataset :- \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\ntrain_attributes=df_train.columns.values[2:202]\ntest_attributes=df_test.columns.values[1:201] \nsns.distplot(df_train[train_attributes].std(axis=0),color='blue',kde=True,bins=150,label='train')\nsns.distplot(df_test[test_attributes].std(axis=0),color='green',kde=True,bins=150,label='test') \nplt.title('Distribution of S.D Values per column in train & test dataset')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of standard deviation (std) values per row in train and test\ndataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.distplot(df_train[train_attributes].std(axis=1),color='blue',kde=True,bins=150,label='train') \nsns.distplot(df_test[test_attributes].std(axis=1),color='green',kde=True,bins=150,label='test') \nplt.title('Distribution of S.D Values per row in train & test dataset')\nplt.legend()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of skewness values per column in train and test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\ntrain_attributes=df_train.columns.values[2:202]\ntest_attributes=df_test.columns.values[1:201] \nsns.distplot(df_train[train_attributes].skew(axis=0),color='red',kde=True,bins=150,label='train')\nsns.distplot(df_test[test_attributes].skew(axis=0),color='green',kde=True,bins=150,label='test') \nplt.title('Distribution of skewness Values per column in train & test dataset')\nplt.legend()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of skewness values per column in train and test dataset:- ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8)) \nsns.distplot(df_train[train_attributes].skew(axis=1),color='red',kde=True,bins=150,label='train') \nsns.distplot(df_test[test_attributes].skew(axis=1),color='green',kde=True,bins=150,label='test')\nplt.title('Distribution of skewness Values per row in train & test dataset')\nplt.legend()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of kurtosis values per column in train and test dataset:- ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\ntrain_attributes=df_train.columns.values[2:202]\ntest_attributes=df_test.columns.values[1:201] \nsns.distplot(df_train[train_attributes].kurtosis(axis=0),color='red',kde=True,bins=150,label='train') \nsns.distplot(df_test[test_attributes].kurtosis(axis=0),color='blue',kde=True,bins=150,label='test')\nplt.title('Distribution of kurtosis Values per column in train & test dataset')\nplt.legend()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look distribution of kurtosis values per row in train and test dataset:- ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of kurtosis Values per column in train & test dataset:-\nplt.figure(figsize=(16,8))\n#Distribution plot for kurtosis values per rows in train attributes:\nsns.distplot(df_train[train_attributes].kurtosis(axis=1),color='red',kde=True,bins=150,label='train') \n#Distribution plot for kurtosis values per rows in test attributes:\nsns.distplot(df_test[test_attributes].kurtosis(axis=1),color='green',kde=True,bins=150,label='test')\nplt.title('Distribution of kurtosis Values per row in train & test dataset')\nplt.legend()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the missing values in train & test dataset:- \ntrain_missing=df_train.isnull().sum().sum()\ntest_missing=df_test.isnull().sum().sum() \nprint('Missing values in train data:',train_missing)\nprint('Missing values in test data:',test_missing) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation in train attiributes\ntrain_attributes=df_train.columns.values[2:202]\ntrain_correlation=df_train[train_attributes].corr().abs().unstack().sort_values(kind='quicksort').reset_index() \ntrain_correlation=train_correlation[train_correlation['level_0']!=train_correlation['level_1']]\nprint(train_correlation.head(10))\nprint(train_correlation.tail(10)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation in test attiributes\ntest_attributes=df_test.columns.values[1:201]\ntest_correlation=df_test[train_attributes].corr().abs().unstack().sort_values(kind='quicksort').reset_index() \ntest_correlation=test_correlation[test_correlation['level_0']!=test_correlation['level_1']]\nprint(test_correlation.head(10))\nprint(test_correlation.tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation plot for train and test data: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_correlation=df_train[train_attributes].corr()\ntrain_correlation=train_correlation.values.flatten()\ntrain_correlation=train_correlation[train_correlation!=1]\ntest_correlation=df_test[test_attributes].corr()\ntest_correlation=test_correlation.values.flatten()\ntest_correlation=test_correlation[test_correlation!=1]\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlation,color=\"blue\",label=\"train\")\nsns.distplot(test_correlation,color=\"red\",label=\"test\")\nplt.xlabel(\"Correlation values found in train & test data\")\nplt.ylabel(\"Density\")\nplt.title (\"Correlation values in train & test data\")\nplt.legend() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation matrix, it tells about linear relationship between attributes and help us to\nbuild better models.\nFrom correlation distribution plot, we can observed that correlation between both train\nand test attributes are very small. It means that all both train and test attributes are independent to each other. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Performing feature engineering by using\n- Permutation Importance\n- Partial dependence plots \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Permutation importance:-\nPermutation variable importance measure in a random forest for classification and regression. The variables which are mostly contributed to predict the model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training & testing data: \nX=df_train.drop(columns=['ID_code','target'],axis=1)\ntest=df_test.drop(columns=['ID_code'],axis=1)\ny=df_train['target'] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building a simple model to find the features which are more important: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the train data:\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classifier:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model=RandomForestClassifier(n_estimators=10,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model\nrf_model.fit(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Permutation Importance:-\nfrom eli5.sklearn import PermutationImportance\nperm_imp=PermutationImportance(rf_model,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting the model:-\nperm_imp.fit(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important Features:-\neli5.show_weights(perm_imp,feature_names=X_test.columns.tolist(),top=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partial Dependence plots:\nPartial dependence plot gives a graphical depiction of the marginal effect of a variable on\nthe class probability or classification. While feature importance shows what variables\nmost affect predictions, but partial dependence plots show how a feature affects\npredictions. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Calculation of partial dependence plots on random forest:- \n\nWe are observing impact of main features which are discovered in previous section by using PDP Plot. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features=[v for v in X_test.columns if v not in ['ID_code','target']]\npdp_data=pdp.pdp_isolate(rf_model, dataset=X_test, model_features=features,feature='var_6') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot feature for var_6:-\npdp.pdp_plot(pdp_data,'var_6')\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdp_data=pdp.pdp_isolate(rf_model, dataset=X_test, model_features=features,\nfeature='var_53')\npdp.pdp_plot(pdp_data,'var_53')\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting the data via Sratified KFold Cross Validator:-\n#Training Data:\nX=df_train.drop(['ID_code','target'],axis=1)\nY=df_train['target']\n#Stratified KFold Cross Validator:-\nskf=StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\nfor train_index, valid_index in skf.split(X,Y):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = Y.iloc[train_index], Y.iloc[valid_index] \nprint('Shape of X_train :',X_train.shape)\nprint('Shape of X_valid :',X_valid.shape)\nprint('Shape of y_train :',y_train.shape)\nprint('Shape of y_valid :',y_valid.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model=LogisticRegression(random_state=42)\n#fitting the model\nlr_model.fit(X_train,y_train) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy\nlr_score=lr_model.score(X_train,y_train)\nprint('Accuracy of lr_model :',lr_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation prediction of lr_model\ncv_predict=cross_val_predict(lr_model,X_valid,y_valid,cv=5) \n#Cross validation score\ncv_score=cross_val_score(lr_model,X_valid,y_valid,cv=5)\nprint('cross val score :',np.average(cv_score)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix:-\ncm=confusion_matrix(y_valid,cv_predict)\ncm=pd.crosstab(y_valid,cv_predict)\ncm \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#roc_auc score\nroc_score=roc_auc_score(y_valid,cv_predict)\nprint('ROC Score:',roc_score) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#roc_auc curve\nplt.figure()\nfalse_positive_rate,recall,thresholds=roc_curve(y_valid,cv_predict)\nroc_auc=auc(false_positive_rate,recall)\nplt.title('Reciver Operating Characteristics(ROC)')\nplt.plot(false_positive_rate,recall,'b',label='ROC(area=%0.3f)' %roc_auc)\nplt.legend()\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall(True Positive Rate)')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC:',roc_auc) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report:- \nclassification_scores=classification_report(y_valid,cv_predict)\nprint(classification_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model performance on test data:- \nX_test=df_test.drop(['ID_code'],axis=1)\nlr_pred=lr_model.predict(X_test)\nprint(lr_pred) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy of the model is not the best metric to use when evaluating the imbalanced datasets as it may be misleading. So, we are going to change the performance metric.\n\nOversample Minority Class:-\n- Adding more copies of minority class.\n- It can be a good option we dont have that much large data to work.\n- Drawback of this process is we are adding info. That can lead to overfitting or poor performance on test data.\nUndersample Majority class:-\n- Removing some copies of majority class.\n- It can be a good option if we have very large amount of data say in millions to work.\n- Drawback of this process is we are removing some valuable info. that can leads to underfitting & poor performance on test data.\n\nAs per the drawbacks of both the model we will use SMOTE (Synthetic Minority Oversampling technique) that is more best than the above as compare to above one's.\n\n\n# Synthetic Minority Oversampling Technique (SMOTE)\n\nSMOTE uses a nearest neighborâ€™s algorithm to generate new and synthetic data to use for\ntraining the model. In order to balance imbalanced data we are going to use SMOTE\nsampling method. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_smote,y_smote=sm.fit_sample(X_train,y_train)\nX_smote_v,y_smote_v=sm.fit_sample(X_valid,y_valid) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote=LogisticRegression(random_state=42)\n#fitting the smote model:-\nsmote.fit(X_smote,y_smote) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote_score=smote.score(X_smote,y_smote)\nprint('Accuracy of the smote_model :',smote_score) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cross validation prediction for SMOTE:-\ncv_pred=cross_val_predict(smote,X_smote_v,y_smote_v,cv=5)\n#Cross validation score:-\ncv_score=cross_val_score(smote,X_smote_v,y_smote_v,cv=5)\nprint('Cross validation score :',np.average(cv_score)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix:-\ncm=confusion_matrix(y_smote_v,cv_pred)\ncm=pd.crosstab(y_smote_v,cv_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC_AUC SCORE:-\nroc_score=roc_auc_score(y_smote_v,cv_pred)\nprint('ROC score:',roc_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ROC_AUC Curve:-\nplt.figure()\nfalse_positive_rate,recall,thresholds=roc_curve(y_smote_v,cv_pred)\nroc_auc=auc(false_positive_rate,recall)\nplt.title('Reciver Operating Characteristics(ROC)')\nplt.plot(false_positive_rate,recall,'b',label='ROC(area=%0.3f)' %roc_auc)\nplt.legend()\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall(True Positive Rate)')\nplt.xlabel('False Positive Rate')\nplt.show()\nprint('AUC:',roc_auc) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=classification_report(y_smote_v,cv_pred)\nprint(scores) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=df_test.drop(['ID_code'],axis=1)\nsmote_pred=smote.predict(X_test)\nprint(smote_pred) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LightGBM is a gradient boosting framework that uses tree based learning algorithms. We\nare going to use LightGBM model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train=lgb.Dataset(X_train,label=y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_valid=lgb.Dataset(X_valid,label=y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting best hyperparameters by tuning of different parameters:-\nparams={'boosting_type': 'gbdt',\n'max_depth' : -1, #no limit for max_depth if <0\n'objective': 'binary',\n'boost_from_average':False,\n'nthread': 20,\n'metric':'auc',\n'num_leaves': 50,\n'learning_rate': 0.01,\n'max_bin': 100, #default 255\n'subsample_for_bin': 100,\n'subsample': 1,\n'subsample_freq': 1,\n'colsample_bytree': 0.8,\n'bagging_fraction':0.5,\n'bagging_freq':5,\n'feature_fraction':0.08,\n'min_split_gain': 0.45, #>0\n'min_child_weight': 1,\n'min_child_samples': 5,\n'is_unbalance':True,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training lgbm model:-\nnum_rounds=10000\nlgbm=lgb.train(params,lgb_train,num_rounds,valid_sets=[lgb_train,lgb_valid],verbose_eval=1000,early_stopping_rounds = 5000)\nlgbm ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM model performance on test data\nX_test=df_test.drop(['ID_code'],axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict the model:-\n#probability predictions\nlgbm_predict_prob=lgbm.predict(X_test,random_state=42,num_iteration=lgbm.best_iteration) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convert to binary output 1 or 0\nlgbm_predict=np.where(lgbm_predict_prob>=0.5,1,0)\nprint(lgbm_predict_prob)\nprint(lgbm_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(lgbm,max_num_features=50,importance_type=\"split\",figsize=(20,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submission \ndf_sub=pd.DataFrame({'ID_code':df_test['ID_code'].values})\ndf_sub['target']=lgbm_predict\ndf_sub.set_index('ID_code',inplace=True)\ndf_sub.to_csv('submission.csv',index=True)\ndf_sub.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}