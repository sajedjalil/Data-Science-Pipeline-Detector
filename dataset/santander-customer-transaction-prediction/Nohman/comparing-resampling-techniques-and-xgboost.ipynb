{"cells":[{"metadata":{"_uuid":"319635a7a3d1fa5cf6469f651700978e0a219ae8"},"cell_type":"markdown","source":"Decision trees and neural nets have trouble classifying examples when trained on imbalanced data. This kernel will explore a wide range of resampling techniques, how they vary, and their effect on XGBoost.\n\n### Contents\n1. [Introduction](#introduction)\n2. [Table of techniques](#technique-table)\n    1. [Random Undersampling](#random-undersampling)\n    2. [Tomek Links](#tomek-links)\n    3. [AllKNN](#allknn)\n    4. [Edited Nearest Neighbor](#enn)\n    5. [Random Oversampling](#random-oversampling)\n    6. [ADASYN](#adasyn)\n    7. [SMOTE](#smote)\n    2. [SMOTETOMEK](#smotetomek)\n    2. [SMOTEENN](#smoteenn)\n3. [Training XGBoost](#training-xgboost)\n4. [Conclusion](#conclusion)"},{"metadata":{"trusted":true,"_uuid":"f4c121570f9023f423a38807801a60893823e6e7","_kg_hide-input":true},"cell_type":"code","source":"import time\nimport math\nimport logging \nimport random\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sci\nfrom imblearn import under_sampling, over_sampling, combine\nfrom sklearn.decomposition import PCA\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nimport catboost as cb\nimport xgboost as xgb\nimport seaborn as sns\nfrom scipy.stats import spearmanr\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt import fmin\n\nfrom sklearn.metrics import precision_score, roc_auc_score, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight') \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4106c23979ea1ec4b2ae553108c19ae0f0bf1a8"},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv', low_memory=True)\ncount_yes = len(df[df.target == 1])\ncount_no = len(df[df.target== 0])\n\nplt.bar(['No', 'Yes'], [count_no, count_yes])\nplt.title('Santander Customer Transaction Prediction')\nplt.xlabel('Whether Successful Transaction')\nplt.ylabel('Count of Customers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d89d4d12c450dfa2ac2de8421297e4671ce2a46d"},"cell_type":"markdown","source":"<a id='introduction'></a>\n# Target Class Imbalance\n---\nIn the Santander customer transaction prediction data we have a binary target variable where 1 is a successful future transaction and 0 is no future transaction. The problem is that we have an imbalance of about 7:1. If we train on this data we are likely to have a model that will missclassify the minority class, 'yes', because it has seen so few examples. \n\nTo deal with class imbalance we can resample. Resampling can mean that we oversample a minority class or undersample a majority class to introduce bias to select a more even distribution of classes. Class imbalance is something we will regularly see in tasks like network intrusion, rare disease diagnosing, and fraud detection. "},{"metadata":{"trusted":true,"_uuid":"10ce314ee971a0326f86b900c368230e96150bd0","_kg_hide-input":true},"cell_type":"code","source":"description = pd.DataFrame(index=['observations(rows)', 'percent missing', 'dtype', 'range'])\nnumerical = []\ncategorical = []\n# Construct a dataframe of Santander metadata\nfor col in df.columns:\n    obs = df[col].size\n    p_nan = round(df[col].isna().sum()/obs, 2)\n    num_nan = f'{p_nan}% ({df[col].isna().sum()}/{obs})'\n    dtype = 'categorical' if df[col].dtype == object else 'numerical'\n    numerical.append(col) if dtype == 'numerical' else categorical.append(col)\n    rng = f'{len(df[col].unique())} labels' if dtype == 'categorical' else f'{df[col].min()}-{df[col].max()}'\n    description[col] = [obs, num_nan, dtype, rng]\n\nfinal_results = pd.DataFrame(columns = ['parameters', 'training auc score',\n                                       'precision', 'training time', 'parameter tuning time'])\n\npd.set_option('display.max_columns', 150)\ndisplay(description)\ndisplay(df.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc32f236f1af76a94fde30a49b52d0e8dda443b5"},"cell_type":"markdown","source":"<a id='technique-table'></a>\n# Resampling Techniques\n---\n### Undersampling Techniques \n1. Random Undersampling\n2. Tomek Links\n3. AllKNN\n4. ENN (Edited Nearest Neighbours)\n\n### Oversampling Techniques\n1. Random Oversampling\n2. ADASYN (Adaptive Synthetic Sampling)\n3. SMOTE (Synthetic Minority Over-Sampling Technique)\n\n### Combined Resampling\n1. SMOTETomek\n2. SMOTEENN"},{"metadata":{"trusted":true,"_uuid":"c36320ba84c1438245d01d4c3e7b3f76888c1da4"},"cell_type":"code","source":"sample = df.sample(n=100)\nsample = sample.drop(columns=['ID_code'])\nclass_1 = len(sample[sample.target == 1])\nclass_0 = len(sample[sample.target== 0])\n\nplt.bar(['Zero', 'One'], [class_0, class_1])\nplt.title('Size 100 Sample Distribution')\nplt.xlabel('Target Class Label')\nplt.ylabel('Count of Customers')\nplt.show()\nprint(f'Zero: {class_0} \\nOne: {class_1}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d52344bfc73cf34a5824998ea598480c77a538a6"},"cell_type":"markdown","source":"<a id='random-undersampling'></a>\n### Random Undersampling\nThe simplest form of undersampling is to remove random records from the majority class. With imblearn's implementation we can choose to remove samples with or without replacement. The biggest drawback to this form of undersampling is loss of information."},{"metadata":{"trusted":true,"_uuid":"b9798392a87a02f37089c6c8c90124626b54eccf","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nrus = under_sampling.RandomUnderSampler(random_state=0)\nresamp_x, resamp_y= rus.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Undersampled Majority Class')\naxs[1].scatter(ono_x, ono_y, label='Original Class0')\naxs[1].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"074c018414960038c1d5d975439b9df67836662b"},"cell_type":"markdown","source":"<a id='tomek-links'></a>\n### Tomek Links\nTomek links can be used as an under-sampling method or as a data cleaning method. A Tomek link is any place where two samples of different classes are nearest neighbors. When we find a Tomek link we can choose which observatin to delete- in undersampling we remove the majority class. \n\nThe difference between the data before and after Tomek links is subtle but clear- Tomek links is a great technique we can use to clear up our boundaries in classificatino problems. "},{"metadata":{"trusted":true,"_uuid":"0024f7a6d1e5350fc6fc8c7077b8def7fbcc11ef","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\ntom = under_sampling.TomekLinks(random_state=0)\nresamp_x, resamp_y= tom.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Undersampled Majority Class')\naxs[1].scatter(ono_x, ono_y, label='Original Class0')\naxs[1].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b089d6e374269eef043aa114ac32a3cd8d26ed7"},"cell_type":"markdown","source":"<a id='allknn'></a>\n### AllKNN\nAllKNN is a method also created by the Ivan Tomek that deletes an object if a KNN classifier misclassifies it. In imblearn the default value of k is 3, but we can also pass a value. In the below cell its worth passing different values to `n_neighbors`. AllKNN tends to delete more datapoints than ENN, especially as the value of k increases. I think that it undersamples too haphazardly. "},{"metadata":{"trusted":true,"_uuid":"4b07f04357c1be567600f352686b01a886b23e53","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\naknn = under_sampling.AllKNN(random_state=0, n_neighbors=5)\nresamp_x, resamp_y= aknn.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Undersampled Majority Class')\naxs[1].scatter(ono_x, ono_y, label='Original Class0')\naxs[1].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e42e4f9c440ff834cb72b8652aaf5fa4700c2a79"},"cell_type":"markdown","source":"<a id='enn'></a>\n### ENN (Edited Nearest Neighbours)\nENN removes examples whose class label differs from the class of at least half of its k nearest neighbors. The benefit of ENN is that we can remove examples of the majority class while retaining as much information as possible because we are only removing redundant observations. "},{"metadata":{"trusted":true,"_uuid":"35cfc0e29c6d93181bfa6aa11f9eafb793759f13","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nenn = under_sampling.EditedNearestNeighbours(random_state=0, n_neighbors=3)\nresamp_x, resamp_y= enn.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Undersampled Majority Class')\naxs[1].scatter(ono_x, ono_y, label='Original Class0')\naxs[1].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04c405974fd8cf31a14d263281d22fdee92d85ab"},"cell_type":"markdown","source":"<a id='random-oversampling'></a>\n### Random Oversampling\nThe simplest implementation of oversampling is to duplicate random records from the minority class, this can cause overfitting. "},{"metadata":{"trusted":true,"_uuid":"cc42e4a37a7ad3695dee08671076cad18178e5b3","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nros = over_sampling.RandomOverSampler(random_state=0, ratio=0.5)\nresamp_x, resamp_y= ros.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Oversampled Minority Class')\naxs[1].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(ono_x, ono_y, label='Original Class0')\naxs[2].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2b2a3184c07c806677fa5b17392d26afff3c463"},"cell_type":"markdown","source":"<a id='adasyn'></a>\n### ADASYN (Adaptive Synthetic Sampling)\nADASYN adaptively generates samples next to original observations which are wrongly classified by a KNN classifier. Unlike SMOTE that generates new samples that lie inside the class boundary, ADASYN tends to generate new samples near existing outliers. \n\nYou can run these code cells with different data samples to see how  ADASYN tends to change the data distribution, but especially in contrast to SMOTE we can see how it tends to constuct points on the frontier of our existing data. "},{"metadata":{"trusted":true,"_uuid":"6f8dd8778e0c9236db47f90c31c56115c53666bd","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nada = over_sampling.ADASYN(random_state=0, ratio=0.5)\nresamp_x, resamp_y= ada.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Oversampled Minority Class')\naxs[1].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(ono_x, ono_y, label='Original Class0')\naxs[2].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6250ccafd0e00e9824524154bc2662192f32cd85"},"cell_type":"markdown","source":"<a id='smote'></a>\n### SMOTE (Synthetic Minority Over-Sampling Technique)\nSMOTE synthesizes new examples by interpolating existing observations. SMOTE begins by iterating over every minority class instace and choosing its k nearest neighbors. The algorithm then constructs new instances halfway between the chosen obervations and its k neighbors. The greatest limitation of SMOTE is that it can only construct examples within the body of observations, never outside. If we compare the rebalanced data in the SMOTE plot against the plot for ADASYN we can see this exact effect. \n\nSMOTE has several variants like SVMSMOTE and BorderlineSMOTE."},{"metadata":{"trusted":true,"_uuid":"80c5766137277a4ae59d40a9b7ad332f48e75388","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nsmo = over_sampling.SMOTE(random_state=0, ratio=0.5)\nresamp_x, resamp_y= smo.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Oversampled Minority Class')\naxs[1].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(ono_x, ono_y, label='Original Class0')\naxs[2].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\nfig.delaxes(axs[3])   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c476a602120bcad1d235447131a11393bc101823"},"cell_type":"markdown","source":"<a id='smotetomek'></a>\n### SMOTETomek\nSMOTETomek is the combination of using Tomek links to undersample the majoirty class and the use of SMOTE to oversample the minority class. "},{"metadata":{"trusted":true,"_uuid":"628256ac0d9e2a627def94f32cc2bda313004980","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nsmotom = combine.SMOTETomek(random_state=0, ratio=0.5)\nresamp_x, resamp_y= smotom.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Oversampled Minority Class')\naxs[1].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[3].set_title('Undersampled Majority Class')\naxs[3].scatter(ono_x, ono_y, label='Original Class0')\naxs[3].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\naxs[3].legend()\n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"176e253b7e1a0ee632386e8b9c16e0db5bf8ff30"},"cell_type":"markdown","source":"<a id='smoteenn'></a>\n### SMOTEENN\nSMOTEENN is the combination of SMOTE and Edited Nearest Neighbor. ENN removes any example whose class label differs from the class label of at least two of its three nearest neighbors. ENN tends to remove more examples then the Tomek links. \n\nThere's a really interesting difference here between SMOTETomek and SMOTEENN. There are so few minority class examples that Tomek Links are not nearly as effective at undersampling the majority class. If we we're using a built in method we could first perform SMOTE and then perform the Tomek Links step but "},{"metadata":{"trusted":true,"_uuid":"a7093276abed1c8387f10c9b3c0bfd2072562849","_kg_hide-input":true},"cell_type":"code","source":"y = sample.target\nx = sample.drop(columns=['target'])\nsmotenn = combine.SMOTEENN(random_state=0, ratio=0.5)\nresamp_x, resamp_y= smotenn.fit_resample(x, y)\n# Transform the resampled data into principal components\npca = PCA(n_components=2)\nresamp = pd.DataFrame(np.hstack((np.vstack(resamp_y), resamp_x)))\n\nresamp_0 = resamp[resamp[0] == 0.0]\nresamp_1 = resamp[resamp[0] == 1.0]\norig_0 = sample[sample.target == 0]\norig_1 = sample[sample.target == 1]\n\norig_no = pca.fit_transform(orig_0)\norig_yes = pca.fit_transform(orig_1)\nresamp_no = pca.fit_transform(resamp_0)\nresamp_yes = pca.fit_transform(resamp_1)\n\nono_x = orig_no[:, 0]\nono_y = orig_no[:, 1]\noyes_x = orig_yes[:, 0]\noyes_y = orig_yes[:, 1]\nrno_x = resamp_no[:, 0]\nrno_y = resamp_no[:, 1]\nryes_x = resamp_yes[:, 0]\nryes_y = resamp_yes[:, 1]\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\naxs= axs.flatten()\naxs[0].set_title('Original Data')\naxs[0].scatter(ono_x, ono_y, label='Original Class0')\naxs[0].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].set_title('Oversampled Minority Class')\naxs[1].scatter(oyes_x, oyes_y, label='Original Class1')\naxs[1].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[3].set_title('Undersampled Majority Class')\naxs[3].scatter(ono_x, ono_y, label='Original Class0')\naxs[3].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].set_title('More Balanced Data')\naxs[2].scatter(rno_x, rno_y, label='Undersampled Class0')\naxs[2].scatter(ryes_x, ryes_y, label='Oversampled Class1')\naxs[0].legend()\naxs[1].legend()\naxs[2].legend()\naxs[3].legend()\n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15122379d1b9f7ddc15727d382c7088cc2188c53"},"cell_type":"markdown","source":"<a id='training-xgboost'></a>\n# XGBoost with Unbalanced Data\nNow that we know about strategies to deal with unbalanced data, here is the effect of unbalanced data on our gradient boosted random forests. \n\nWe will fit XGBoost models with Bayesian hyperparameter optimization to two datasets, first our unbalanced dataset- and second, a dataset that we have balanced with Smotetomek. The hyperparameter optimization library Hyperopt is great because it will do the optimization for us if we pass (1) a hyperparameter feature space, (2) an objective function that fits the model and returns a score to minimize, and (3) a `Trials` object that we can store arbitary data in from the model training. "},{"metadata":{"trusted":true,"_uuid":"b5453e1f9d00584ac6ff0f27c3719e694ae9f820"},"cell_type":"code","source":"# Organizes XGB results and extracts metadata from Trials object\ndef org_results(trials, hyperparams, ratio, model_name):\n    fit_idx = -1\n    for idx, fit  in enumerate(trials):\n        hyp = fit['misc']['vals']\n        xgb_hyp = {key:[val] for key, val in hyperparams.items()}\n        if hyp == xgb_hyp:\n            fit_idx = idx\n            break\n            \n    train_time = str(trials[-1]['refresh_time'] - trials[0]['book_time'])\n    acc = round(trials[fit_idx]['result']['accuracy'], 3)\n    train_auc = round(trials[fit_idx]['result']['train auc'], 3)\n    test_auc = round(trials[fit_idx]['result']['test auc'], 3)\n    conf_matrix = trials[fit_idx]['result']['conf matrix']\n\n    results = {\n        'model': model_name,\n        'ratio': ratio,\n        'parameter search time': train_time,\n        'accuracy': acc,\n        'test auc score': test_auc,\n        'training auc score': train_auc,\n        'confusion matrix': conf_matrix,\n        'parameters': hyperparams\n    }\n    return results\n\ndef data_ratio(y):\n    unique, count = np.unique(y, return_counts=True)\n    ratio = round(count[0]/count[1], 2)\n    return f'{ratio}:1 ({count[0]}/{count[1]})'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea4c016455d498c9c15d9cb7e7298de616938e5a"},"cell_type":"code","source":"batch_size = 10000\nxgb_df = df.sample(batch_size)\ny = xgb_df['target'].reset_index(drop=True)\nx = xgb_df.drop(columns=['target','ID_code'])\nsmotomek = combine.SMOTETomek(random_state=0, ratio=0.5)\nbal_x, bal_y= smotomek.fit_resample(x, y)\n\nsamp_len = len(bal_y)\nxgb_df2 = df.sample(samp_len - batch_size)\nxgb_df = pd.concat([xgb_df, xgb_df2])\nimb_y = xgb_df['target'].reset_index(drop=True)\nimb_x = xgb_df.drop(columns=['target','ID_code'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b64755c19a25e06bc35ad9503c13a7ab1ae3ce22","_kg_hide-input":true},"cell_type":"code","source":"def xgb_train(data_x, data_y, md_name):\n    ratio = data_ratio(data_y)\n    train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.20)\n   \n    def xgb_objective(space, early_stopping_rounds=50):\n\n        model = XGBClassifier(\n            learning_rate = space['learning_rate'], \n            n_estimators = int(space['n_estimators']), \n            max_depth = int(space['max_depth']), \n            min_child_weight = space['m_child_weight'], \n            gamma = space['gamma'], \n            subsample = space['subsample'], \n            colsample_bytree = space['colsample_bytree'],\n            objective = 'binary:logistic'\n        )\n\n        model.fit(train_x, train_y, \n                  eval_set = [(train_x, train_y), (test_x, test_y)],\n                  eval_metric = 'auc',\n                  early_stopping_rounds = early_stopping_rounds,\n                  verbose = False)\n\n        predictions = model.predict(test_x)\n        test_preds = model.predict_proba(test_x)[:,1]\n        train_preds = model.predict_proba(train_x)[:,1]\n\n        xgb_booster = model.get_booster()\n        train_auc = roc_auc_score(train_y, train_preds)\n        test_auc = roc_auc_score(test_y, test_preds)\n        accuracy = accuracy_score(test_y, predictions) \n        conf_matrix = confusion_matrix(test_y, predictions)\n\n        return {'status': STATUS_OK, 'loss': 1-test_auc, 'accuracy': accuracy,\n                'test auc': test_auc, 'train auc': train_auc, 'conf matrix': conf_matrix\n               }\n\n    space = {\n        'n_estimators': hp.quniform('n_estimators', 50, 1000, 25),\n        'max_depth': hp.quniform('max_depth', 1, 12, 1),\n        'm_child_weight': hp.quniform('m_child_weight', 1, 6, 1),\n        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n        'learning_rate': hp.loguniform('learning_rate', np.log(.001), np.log(.3)),\n        'colsample_bytree': hp.quniform('colsample_bytree', .5, 1, .1)\n    }\n\n    trials = Trials()\n    xgb_hyperparams = fmin(fn = xgb_objective, \n                     max_evals = 25, \n                     trials = trials,\n                     algo = tpe.suggest,\n                     space = space\n                     )\n    \n    results = org_results(trials.trials, xgb_hyperparams, ratio, md_name)\n    return results\n\nbal_results = xgb_train(bal_x, bal_y, 'Balanced Data')\nimb_results = xgb_train(imb_x, imb_y, 'Imbalanced Data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a49cc080b25559694dbb4cfd4409dfd6065beca"},"cell_type":"code","source":"bal_confusion = bal_results.pop('confusion matrix')\nimb_confusion = imb_results.pop('confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3d9e01485a59a9f20c532cbbe379e78ea5809b35"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nsns.heatmap(bal_confusion, annot=True, cmap= 'viridis_r', ax=ax[0])\nsns.heatmap(imb_confusion, annot=True, cmap= 'viridis_r', ax=ax[1])\nax[0].set_title('Balanced Dataset')\nax[1].set_title('Imbalanced Dataset')\nplt.show()\nfinal_results = pd.DataFrame([bal_results, imb_results])\ndisplay(final_results) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff9fc4f98503d6f93d6df41560b10032f0170aa"},"cell_type":"markdown","source":"<a id='conclusion'></a>\n# Imbalanced Dataset Conclusion\n---\nIn the confusion matrix we can see how much more often the imbalanced dataset correctly identifies class one observations as class one- the bottom right hand square. \n\nThe dropoff in accuracy between the two sets is a little over 5% depending on how the sample distribution shakes out, and the test auc scores tends to be about 10% different. This is the difference between a 2:1 target label imbalance and 9:1. \n\nImbalances in trees can have a significant effect on our classification power. While we focus on target label imbalance here, imbalances in the distributions of values and classes is an important topic in tree based models. If we go back and look at the charts of points that we use as examples we can see how tightly grouped points of two different classes can be- the job of XGBoost is to be able to disambiguate between these points and and that requires robust data. Imabalnce problems dont just have to be in the class of target label, we can face imbalances where there are two few examples of one category in a categorical variable, outliers in the distribution of a numerical variable, and imbalances between train and test sets. \n\nSomething you should try on your own is rerunning this kernel and seeing how a technique like ADASYN produces a different model than the SMOTE-based technique we used here. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}