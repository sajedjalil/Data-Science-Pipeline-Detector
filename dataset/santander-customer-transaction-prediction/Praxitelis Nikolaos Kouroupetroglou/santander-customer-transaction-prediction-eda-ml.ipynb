{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.santander.co.uk/themes/custom/santander_web18/logo.svg)\n\n[image-source](https://www.santander.co.uk/themes/custom/santander_web18/logo.svg)\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Main outline"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"data:image/png;base64,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\" style=\"cursor:pointer;max-width:100%;\" onclick=\"(function(img){if(img.wnd!=null&&!img.wnd.closed){img.wnd.focus();}else{var r=function(evt){if(evt.data=='ready'&&evt.source==img.wnd){img.wnd.postMessage(decodeURIComponent(img.getAttribute('src')),'*');window.removeEventListener('message',r);}};window.addEventListener('message',r);img.wnd=window.open('https://www.draw.io/?client=1&lightbox=1&edit=_blank');}})(this);\"/>"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n- [Problem Definition and Objectives](#intro)\n- [Exploratory Data Analysis](#EDA)\n- [Machine Learning Modeling](#ML)\n    - [Feature Engineering](#FE)\n    - [Baseline Modeling](#base)\n    - [Feature Selection](#FS)\n    - [Bayesian Optimization](#Bayes)\n    - [Tuned Model Training](#tuned)\n- [Conclusion](#conclusion)"},{"metadata":{},"cell_type":"markdown","source":"## Problem Definition and Objectives\n<a id=\"intro\"></a>"},{"metadata":{},"cell_type":"markdown","source":"At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n\nSantander is continually challenging its machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, Kagglers are invited to help Santander identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nYou are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of target column in the test set."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n<a id=\"EDA\"></a>\n\n![](http://blog.k2analytics.co.in/wp-content/uploads/2016/12/Exploratory_Data_Analysis.png)\n\n[image-source](http://blog.k2analytics.co.in/wp-content/uploads/2016/12/Exploratory_Data_Analysis.png)"},{"metadata":{},"cell_type":"markdown","source":"### Importing the dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport gc\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance we have many uncharacterized numerical features, their names has the prefix \"var_\" and they are 200 in numbers. There are so many variables that some histograms will shed light to their numerical appearance."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts().plot(kind=\"pie\", figsize=(12,9), colormap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have a typical imbalanced dataset."},{"metadata":{},"cell_type":"markdown","source":"#### check for missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have no NA values which is very nice!!"},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect();\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = train.columns[2:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Distributions columns')\nplt.figure(figsize=(30, 185))\nfor i, col in enumerate(numerical_features):\n    plt.subplot(50, 4, i + 1)\n    plt.hist(train[col]) \n    plt.title(col)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all features shows a normal distribution shape. Lets see the distributions for for all numerical features per each class."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Distributions columns')\nplt.figure(figsize=(30, 185))\nfor i, col in enumerate(numerical_features):\n    plt.subplot(50, 4, i + 1)\n    plt.hist(train[train[\"target\"] == 0][col], alpha=0.5, label='0', color='b')\n    plt.hist(train[train[\"target\"] == 1][col], alpha=0.5, label='1', color='r')    \n    plt.title(col)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\ntrain[numerical_features].mean().plot('hist');\nplt.title('Mean Frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\ntrain[numerical_features].median().plot('hist');\nplt.title('Median Frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\ntrain[numerical_features].std().plot('hist');\nplt.title('Standard Deviation Frequency');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the distributions show small std. deviations, and very few more than 20. Maybe a log transformation or a scaling technique to all features will alter the graph above to a normal one. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\ntrain[numerical_features].skew().plot('hist');\nplt.title('Skewness Frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\ntrain[numerical_features].kurt().plot('hist');\nplt.title('Kurtosis Frequency');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both Skewness and Kurtosis show that the features distributions are like a normal one."},{"metadata":{},"cell_type":"markdown","source":"#### correlations between numerical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,28)})\n\n# Compute the correlation matrix\ncorr = train[numerical_features].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, \n            #annot=True, \n            #fmt=\".2f\", \n            cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the figure above shows that most of the pearson correlations between the numerical data are close to zero, in fact is between 0 and 0.2. That means that most of the numerical data are almost uncorrelated between them."},{"metadata":{},"cell_type":"markdown","source":"#### Most correlated features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = corr.unstack().drop_duplicates()\nso = s.sort_values(kind=\"quicksort\")\nso = so.drop_duplicates()\n\nprint(\"Top most highly positive correlated features:\")\nprint(so[(so<1) & (so>0.5)].sort_values(ascending=False))\n\nprint()\n\nprint(\"Top most highly megative correlated features:\")\nprint(so[(so < - 0.005)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA Summary\n\n- We have 200 features that are mostly uncorrelated between them\n- 200 numerical features that their histograms have a shape like the one of a normal distribution"},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Modeling\n<a id=\"ML\"></a>"},{"metadata":{},"cell_type":"markdown","source":"![](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)\n[image-source](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)"},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering\n<a id=\"FE\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# special thanks to https://www.kaggle.com/gpreda/santander-eda-and-prediction\n# also big help for feature engineering :https://www.kaggle.com/hjd810/keras-lgbm-aug-feature-eng-sampling-prediction\n# last but not least: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87486#latest-506429\n\nfs_params = dict()\nfs_params[\"descriptives\"] = True\nfs_params[\"standardization\"] = False\nfs_params[\"percentiles\"] = False\nfs_params[\"squared\"] = False\nfs_params[\"frequency\"] = True\n\ngc.collect();\nturn = 0\nfrom sklearn.preprocessing import StandardScaler\nfor df in [test, train]:\n    \n    if turn == 0:\n        print(\"Train set\")\n        turn = 1\n    else:\n        print(\"Test set\")\n    \n    if (fs_params[\"descriptives\"] == True):\n        print('\\t*descriptive statistics Feature Engineering:')\n        df['sum'] = df[numerical_features].sum(axis=1)  \n        df['min'] = df[numerical_features].min(axis=1)\n        df['max'] = df[numerical_features].max(axis=1)\n        df['mean'] = df[numerical_features].mean(axis=1)\n        df['std'] = df[numerical_features].std(axis=1)\n        df['skew'] = df[numerical_features].skew(axis=1)\n        df['kurt'] = df[numerical_features].kurtosis(axis=1)\n        df['med'] = df[numerical_features].median(axis=1)\n        print('\\t*descriptive statistics Feature Engineering done!')\n    \n    if (fs_params[\"standardization\"] == True):\n        print('\\t*Standardizing the data:')\n        #inf values can result from squaring\n        scaler = StandardScaler()\n        df[numerical_features] = scaler.fit_transform(df[numerical_features])\n        print('\\t*Data Standardized!')\n    \n    if (fs_params[\"percentiles\"] == True):\n        print('\\t*percentiles Feature Engineering:')\n        perc_list = [1,2,5,10,25,50,60,75,80,85,95,99]\n        for i in perc_list:\n            df['perc_'+str(i)] =  df[numerical_features].apply(lambda x: np.percentile(x, i), axis=1)\n        print('\\t*Done percentiles Feature Engineering!')\n    \n    if (fs_params[\"squared\"] == True):\n        print('\\t*Loading Squared data:')\n        for i in range(200):\n            df['var_sq_'+str(i)] = np.square(df['var_'+str(i)])\n        print('\\t*Done squaring!')\n    \n    if (fs_params[\"frequency\"] == True):\n        #thanks to  https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87486#latest-506429\n        print('\\t*Loading frequency:')\n        for var in numerical_features:\n            hist, bin_edges = np.histogram(df[var], bins=1000, density=True)\n            df['hist_'+var] = [ hist[np.searchsorted(bin_edges,ele)-1] for ele in df[var]]\n        print('\\t*Done Loading frequency!')\n    \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['target']\nX = train.drop(['target', \"ID_code\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_stats_df = pd.DataFrame(columns=[\"clf_name\", \"F1-score\", \"auc-score\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baseline Modeling\n<a id=\"base\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgboost_all_purpose(X, y, type_of_training, name, num_of_folds=3, params=None, in_folds_sampling = False, max_early_stopping = 100):\n    \n    from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n    from collections import Counter\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import precision_score\n    from sklearn.metrics import recall_score\n    from sklearn.metrics import f1_score\n    from sklearn.metrics import classification_report\n    from sklearn.metrics import roc_auc_score\n    import scikitplot as skplt\n    import time\n    import random\n    \n    import xgboost as xgb\n    \n    global clf_stats_df\n    \n    if params is None:\n        params = dict()\n        params[\"learning_rate\"] = 0.1\n        params[\"n_estimators\"] = 500\n        params[\"max_depth\"] = 2\n        params[\"min_child_weight\"] = 1\n        params[\"gamma\"] = 0\n        params[\"subsample\"] = 1\n        params[\"colsample_bytree\"] = 1\n        params[\"colsample_bylevel\"] = 1\n        params[\"reg_alpha\"] = 0\n        params[\"reg_lambda\"] = 1\n        params[\"scale_pos_weight\"] = np.round(y.value_counts()[0] / y.value_counts()[1],3)\n        params[\"max_delta_step\"] = 1\n    \n    print(\"params\", params)\n    print(\"max_early_stopping:\", max_early_stopping)\n    \n    if type_of_training == \"baseline\":\n        \n        print(\"baseline\")\n        \n        # create a 70/30 stratified split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)\n    \n        import xgboost as xgb\n\n        start_time = time.time()\n        \n        predictions_probas_list = np.zeros([len(yvalid), 2])\n        predictions_test = np.zeros(len(test))\n        num_fold = 0\n        #feature_importance_df = pd.DataFrame()\n        \n        folds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n        \n        for train_index, valid_index in folds.split(xtrain, ytrain):\n            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n            ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n            \n            print()\n            print(\"Stratified Fold:\", num_fold)\n            num_fold = num_fold + 1\n            print()\n            \n            clf_stra_xgb = xgb.XGBClassifier(learning_rate=params[\"learning_rate\"], \n                                    n_estimators=params[\"n_estimators\"], \n                                    max_depth=params[\"max_depth\"],\n                                    min_child_weight=params[\"min_child_weight\"],\n                                    gamma=params[\"gamma\"],\n                                    subsample=params[\"subsample\"],\n                                    colsample_bytree=params[\"colsample_bytree\"],\n                                    colsample_bylevel=params[\"colsample_bylevel\"],\n                                    objective= 'binary:logistic',\n                                    nthread=-1,\n                                    scale_pos_weight=params[\"scale_pos_weight\"],\n                                    reg_alpha = params[\"reg_alpha\"],\n                                    reg_lambda = params[\"reg_lambda\"],\n                                    max_delta_step = params[\"max_delta_step\"],\n                                    seed=42)\n\n            clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                        early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)\n            \n            #fold_importance_df = pd.DataFrame()\n            #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n            #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n            #fold_importance_df[\"fold\"] = n_fold + 1\n            #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n            predictions = clf_stra_xgb.predict(xvalid)\n            predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n            predictions_probas_list += predictions_probas/num_of_folds\n            \n            predictions_test += clf_stra_xgb.predict_proba(test.drop(\"ID_code\", axis=\"columns\")[xtrain.columns])[:,1]/num_of_folds\n            \n        \n        predictions = np.argmax(predictions_probas, axis=1)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n        \n        print()\n        print(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n        \n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(12, 38)})\n        xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\n        clf_stats_df = clf_stats_df.append({\"clf_name\": name,\n                             \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                             \"auc-score\": roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\")}, ignore_index=True)\n        \n        print()\n        gc.collect();\n        return clf_stra_xgb, predictions_test\n\n    \n    elif type_of_training == \"oversampling\":\n        \n        print(\"oversampling\")\n        #### resampling techniques:\n        from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n\n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)\n\n        # RandomOverSampler\n        ros = RandomOverSampler(random_state=42)\n        X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)\n        \n        from collections import Counter\n        print(sorted(Counter(y_resampled).items()))\n        \n        xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n        ytrain = y_resampled\n        del X_resampled\n        del y_resampled\n        \n        predictions_probas_list = np.zeros([len(yvalid), 2])\n        predictions_test = np.zeros(len(test))\n        num_fold = 0        \n\n        start_time = time.time()\n        \n        folds = KFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n        \n        for train_index, valid_index in folds.split(xtrain, ytrain):\n            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n            ytrain_stra, yvalid_stra = ytrain[train_index], ytrain[valid_index]\n            \n            print()\n            print(\"Fold:\", num_fold)\n            num_fold = num_fold + 1\n            print()\n\n            clf_ros_xgb = xgb.XGBClassifier(learning_rate=params[\"learning_rate\"], \n                                        n_estimators=params[\"n_estimators\"], \n                                        max_depth=params[\"max_depth\"],\n                                        min_child_weight=params[\"min_child_weight\"],\n                                        gamma=params[\"gamma\"],\n                                        subsample=params[\"subsample\"],\n                                        colsample_bytree=params[\"colsample_bytree\"],\n                                        colsample_bylevel=params[\"colsample_bylevel\"],\n                                        objective= 'binary:logistic',\n                                        nthread=-1,\n                                        scale_pos_weight=params[\"scale_pos_weight\"],\n                                        reg_alpha = params[\"reg_alpha\"],\n                                        reg_lambda = params[\"reg_lambda\"],\n                                        max_delta_step = params[\"max_delta_step\"],\n                                        seed=42)\n\n            clf_ros_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)\n\n            predictions = clf_ros_xgb.predict(xvalid)\n            predictions_probas = clf_ros_xgb.predict_proba(xvalid)\n            predictions_probas_list += predictions_probas/num_of_folds  \n            \n            predictions_test += clf_ros_xgb.predict_proba(test.drop(\"ID_code\", axis=\"columns\")[xtrain.columns])[:,1]/num_of_folds\n            \n        predictions = np.argmax(predictions_probas, axis=1)\n            \n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n        \n        print()\n        print(\"roc_auc_score\", roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        sns.set(rc={'figure.figsize':(8, 8)})\n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(12, 38)})\n        xgb.plot_importance(clf_ros_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n        \n        clf_stats_df = clf_stats_df.append({\"clf_name\": name,\n                             \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                             \"auc-score\": roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\")}, ignore_index=True)\n\n        print()\n        gc.collect();\n        return clf_ros_xgb, predictions_test\n    \n    # still needs some work to work\n    elif type_of_training == \"smote\":\n        print(\"smote\")\n        #### resampling techniques, I will use Synthetic minority:\n        from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n\n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)\n\n        # SMOTE\n        smote = SMOTE(random_state=42)\n        X_resampled, y_resampled = smote.fit_resample(xtrain, ytrain)\n        \n        from collections import Counter\n        print(sorted(Counter(y_resampled).items()))\n        \n        xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n        ytrain = y_resampled\n\n        start_time = time.time()\n\n        clf_smote_xgb = xgb.XGBClassifier(learning_rate=params[\"learning_rate\"], \n                                    n_estimators=params[\"n_estimators\"], \n                                    max_depth=params[\"max_depth\"],\n                                    min_child_weight=params[\"min_child_weight\"],\n                                    gamma=params[\"gamma\"],\n                                    subsample=params[\"subsample\"],\n                                    colsample_bytree=params[\"colsample_bytree\"],\n                                    objective= 'binary:logistic',\n                                    nthread=-1,\n                                    scale_pos_weight=params[\"scale_pos_weight\"],\n                                    reg_alpha = params[\"reg_alpha\"],\n                                    reg_lambda = params[\"reg_lambda\"],\n                                    max_delta_step = params[\"max_delta_step\"],\n                                    seed=42)\n\n        clf_smote_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], \n                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)\n\n        predictions = clf_smote_xgb.predict(xvalid)\n        predictions_probas = clf_smote_xgb.predict_proba(xvalid)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n        \n        print()\n        print(\"roc_auc_score\", roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(12, 38)})\n        xgb.plot_importance(clf_smote_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n        \n        clf_stats_df = clf_stats_df.append({\"clf_name\": name,\n                             \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                             \"auc-score\": roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\")}, ignore_index=True)\n\n        print()\n        gc.collect();\n        return clf_smote_xgb\n    \n    # still needs some work to work\n    elif type_of_training == \"undersampling\":\n        print(\"undersampling\")\n        #### resampling techniques:\n        from imblearn.under_sampling import RandomUnderSampler\n\n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)\n\n        # RandomUnderSampler\n        rus = RandomUnderSampler(random_state=42)\n        X_resampled, y_resampled = rus.fit_resample(xtrain, ytrain)\n        \n        print(sorted(Counter(y_resampled).items()))\n        \n        xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n        ytrain = y_resampled\n\n        start_time = time.time()\n\n        clf_rus_xgb = xgb.XGBClassifier(learning_rate=params[\"learning_rate\"], \n                                    n_estimators=params[\"n_estimators\"], \n                                    max_depth=params[\"max_depth\"],\n                                    min_child_weight=params[\"min_child_weight\"],\n                                    gamma=params[\"gamma\"],\n                                    subsample=params[\"subsample\"],\n                                    colsample_bytree=params[\"colsample_bytree\"],\n                                    objective= 'binary:logistic',\n                                    nthread=-1,\n                                    scale_pos_weight=params[\"scale_pos_weight\"],\n                                    reg_alpha = params[\"reg_alpha\"],\n                                    reg_lambda = params[\"reg_lambda\"],\n                                    max_delta_step = params[\"max_delta_step\"],\n                                    seed=42)\n\n        clf_rus_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], \n                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)\n\n        predictions = clf_rus_xgb.predict(xvalid)\n        predictions_probas = clf_rus_xgb.predict_proba(xvalid)\n        \n        \n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n        \n        print()\n        print(\"roc_auc_score\", roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(12, 38)})\n        xgb.plot_importance(clf_rus_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n        \n        clf_stats_df = clf_stats_df.append({\"clf_name\": name,\n                             \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                             \"auc-score\": roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\")}, ignore_index=True)\n\n        print()\n        gc.collect();\n        #return clf_rus_xgb, predictions, predictions_probas\n        return clf_rus_xgb\n    \n    elif type_of_training == \"augmentation\":\n        \n        print(\"augmentation_by_fraction\")\n        \n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.3)\n        \n        print(\"ytrain target values count before augmentation:\\n\", sorted(Counter(ytrain).items()))\n        \n        #thanks to https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment\n        def augment(x,y,t=2):\n            xs,xn = [],[]\n            for i in range(t):\n                mask = y>0\n                x1 = x[mask].copy()\n                ids = np.arange(x1.shape[0])\n                for c in range(x1.shape[1]):\n                    np.random.shuffle(ids)\n                    x1[:,c] = x1[ids][:,c]\n                xs.append(x1)\n\n            for i in range(t//2):\n                mask = y==0\n                x1 = x[mask].copy()\n                ids = np.arange(x1.shape[0])\n                for c in range(x1.shape[1]):\n                    np.random.shuffle(ids)\n                    x1[:,c] = x1[ids][:,c]\n                xn.append(x1)\n\n            xs = np.vstack(xs)\n            xn = np.vstack(xn)\n            ys = np.ones(xs.shape[0])\n            yn = np.zeros(xn.shape[0])\n            x = np.vstack([x,xs,xn])\n            y = np.concatenate([y,ys,yn])\n            return x,y\n        \n        start_time = time.time()\n        \n        predictions_probas_list = np.zeros([len(yvalid), 2])\n        predictions_test = np.zeros(len(test))\n        num_fold = 0\n        #feature_importance_df = pd.DataFrame()\n        \n        folds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n        \n        for train_index, valid_index in folds.split(xtrain, ytrain):\n            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n            ytrain_stra, yvalid_stra = ytrain[train_index], ytrain[valid_index]\n            \n            print()\n            print(\"Stratified Fold:\", num_fold)\n            num_fold = num_fold + 1\n            print()\n            \n            X_t, ytrain_stra = augment(xtrain_stra.values, ytrain_stra.values)\n            print('\\tAugmentation Succeeded..')\n            xtrain_stra = pd.DataFrame(X_t, columns=X.columns)\n            del X_t\n\n            clf_aug_xgb = xgb.XGBClassifier(learning_rate=params[\"learning_rate\"], \n                                        n_estimators=params[\"n_estimators\"], \n                                        max_depth=params[\"max_depth\"],\n                                        min_child_weight=params[\"min_child_weight\"],\n                                        gamma=params[\"gamma\"],\n                                        subsample=params[\"subsample\"],\n                                        colsample_bytree=params[\"colsample_bytree\"],\n                                        colsample_bylevel=params[\"colsample_bylevel\"],\n                                        objective= 'binary:logistic',\n                                        nthread=-1,\n                                        scale_pos_weight=params[\"scale_pos_weight\"],\n                                        reg_alpha = params[\"reg_alpha\"],\n                                        reg_lambda = params[\"reg_lambda\"],\n                                        max_delta_step = params[\"max_delta_step\"],\n                                        seed=42)\n\n            clf_aug_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)\n            \n            #fold_importance_df = pd.DataFrame()\n            #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n            #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n            #fold_importance_df[\"fold\"] = n_fold + 1\n            #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n            predictions = clf_aug_xgb.predict(xvalid)\n            predictions_probas = clf_aug_xgb.predict_proba(xvalid)\n            predictions_probas_list += predictions_probas/num_of_folds  \n            \n            predictions_test += clf_aug_xgb.predict_proba(test.drop(\"ID_code\", axis=\"columns\")[xtrain.columns])[:,1]/num_of_folds\n            \n        \n        predictions = np.argmax(predictions_probas, axis=1)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n        \n        print()\n        print(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(12, 38)})\n        xgb.plot_importance(clf_aug_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n        \n        clf_stats_df = clf_stats_df.append({\"clf_name\": name,\n                             \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                             \"auc-score\": roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\")}, ignore_index=True)\n\n        print()\n        gc.collect();\n        #return clf_rus_xgb, predictions, predictions_probas\n        return clf_aug_xgb, predictions_test\n    \n    \n    elif type_of_training == \"augmentation_by_fraction\":\n        \n        # the main idea here is to reducing the imbalance ratio from 9:1 to 3:1 without using a manual function for resampling as the previous elif statement\n        print(\"augmentation_by_fraction\")\n        \n        # create a 70/30 split of the data \n        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.3)\n\n        print(\"ytrain target values count before augmentation:\\n\", sorted(Counter(ytrain).items()))\n\n        \n        def augment_by_frac(xtrain, ytrain):\n            \n            from collections import Counter\n            # Augmenting both minority and majority classes via RandomOverSampler by 3 times\n\n            X_y = pd.DataFrame(xtrain, columns=xtrain.columns)\n            X_y[\"target\"] = ytrain\n            X_y = X_y.sample(frac=3, replace=True)\n            X_y.target.value_counts()\n            ytrain = X_y['target']\n            print(\"ytrain target values count after oversampling:\\n\",sorted(Counter(ytrain).items()))\n            xtrain = X_y.drop(['target'], axis=1)\n            del X_y\n\n            from imblearn.under_sampling import RandomUnderSampler\n\n            # reducing the majority class almost back to its original form\n            rus = RandomUnderSampler(sampling_strategy=0.33, random_state=42)\n            X_resampled, y_resampled = rus.fit_resample(xtrain, ytrain)\n\n            print(\"ytrain target values count after Augmentation:\\n\",sorted(Counter(y_resampled).items()))\n\n            xtrain=pd.DataFrame(X_resampled, columns = xtrain.columns)\n            ytrain = y_resampled\n\n            del X_resampled\n            del y_resampled\n\n            return xtrain, ytrain\n        \n        \n        if in_folds_sampling == False:\n            print(\"augmentation before stratification\")\n            xtrain, ytrain = augment_by_frac(xtrain, ytrain)\n\n        start_time = time.time()\n        \n        predictions_probas_list = np.zeros([len(yvalid), 2])\n        predictions_test = np.zeros(len(test))\n        num_fold = 0\n        #feature_importance_df = pd.DataFrame()\n        \n        folds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n        \n        for train_index, valid_index in folds.split(xtrain, ytrain):\n            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n            ytrain_stra, yvalid_stra = ytrain[train_index], ytrain[valid_index]\n                        \n            print()\n            print(\"Stratified Fold:\", num_fold)\n            num_fold = num_fold + 1\n            print()\n            \n            #if in_folds_sampling is True:\n            #    print(\"augmentation during stratification\")\n            #    xtrain_stra, ytrain_stra = augment_by_frac(xtrain_stra, ytrain_stra)\n\n            clf_aug_xgb = xgb.XGBClassifier(learning_rate=params[\"learning_rate\"], \n                                        n_estimators=params[\"n_estimators\"], \n                                        max_depth=params[\"max_depth\"],\n                                        min_child_weight=params[\"min_child_weight\"],\n                                        gamma=params[\"gamma\"],\n                                        subsample=params[\"subsample\"],\n                                        colsample_bytree=params[\"colsample_bytree\"],\n                                        colsample_bylevel=params[\"colsample_bylevel\"],\n                                        objective= 'binary:logistic',\n                                        nthread=-1,\n                                        scale_pos_weight=params[\"scale_pos_weight\"],\n                                        reg_alpha = params[\"reg_alpha\"],\n                                        reg_lambda = params[\"reg_lambda\"],\n                                        max_delta_step = params[\"max_delta_step\"],\n                                        seed=42)\n\n            clf_aug_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)\n            \n            #fold_importance_df = pd.DataFrame()\n            #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n            #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n            #fold_importance_df[\"fold\"] = n_fold + 1\n            #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n            predictions = clf_aug_xgb.predict(xvalid)\n            predictions_probas = clf_aug_xgb.predict_proba(xvalid)\n            predictions_probas_list += predictions_probas/num_of_folds  \n            \n            predictions_test += clf_aug_xgb.predict_proba(test.drop(\"ID_code\", axis=\"columns\")[xtrain.columns])[:,1]/num_of_folds\n            \n        \n        predictions = np.argmax(predictions_probas, axis=1)\n\n        print()\n        print(classification_report(yvalid, predictions))\n\n        print()\n        print(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n        \n        print()\n        print(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\"))\n\n        print()\n        print(\"elapsed time in seconds: \", time.time() - start_time)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_roc(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\n        sns.set(rc={'figure.figsize':(8,8)})\n        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n        \n        sns.set(rc={'figure.figsize':(12, 38)})\n        xgb.plot_importance(clf_aug_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n        \n        clf_stats_df = clf_stats_df.append({\"clf_name\": name,\n                             \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                             \"auc-score\": roc_auc_score(yvalid, predictions_probas[:,1], average = \"macro\")}, ignore_index=True)\n\n        print()\n        gc.collect();\n        #return clf_rus_xgb, predictions, predictions_probas\n        return clf_aug_xgb, predictions_test\n    \n    else:\n        print(\"Please specify for the argument 'type_of_training'one of the following parameters: (baseline, oversampling, smote, undersampling, augmentation_by_fraction)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"untuned_model_flag = True\ntype_of_training = \"augmentation_by_fraction\"\n\nif untuned_model_flag == True:\n    \n    num_of_folds = 2 ### must be more than 2\n    in_folds_sampling = False\n\n    clf_xgb, predictions_test_xgb = xgboost_all_purpose(X,y, num_of_folds = num_of_folds, \n                                                        type_of_training =type_of_training, \n                                                        in_folds_sampling = in_folds_sampling, \n                                                        max_early_stopping = 100, \n                                                        name=\"clf_xgb\")\n    \n    del clf_xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test set predictions probabilities histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"if untuned_model_flag == True:\n    sns.set(rc={'figure.figsize':(8,8)})\n    plt.hist(predictions_test_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection - Permutation Importance\n<a id=\"FS\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nfeature_selection_flag = True\n\nif feature_selection_flag == True:\n    from sklearn.model_selection import train_test_split\n    from sklearn.ensemble import RandomForestClassifier\n\n    gc.collect();\n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, test_size=0.3, random_state=42)\n\n\n    rfc_model = RandomForestClassifier(random_state=42, class_weight={0: 1, 1: np.round(y.value_counts()[0] / y.value_counts()[1],3)}).fit(xtrain, ytrain)\n\n    import eli5\n    from eli5.sklearn import PermutationImportance\n\n    perm = PermutationImportance(rfc_model, random_state=42).fit(xvalid, yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if feature_selection_flag == True:\n    eli5.show_weights(perm, feature_names = xvalid.columns.tolist(), top=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Select top 100 features after permutation importance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"if feature_selection_flag == True:\n    from sklearn.feature_selection import SelectFromModel\n\n    max_selected_features = 300\n    sel = SelectFromModel(perm, max_features = max_selected_features, prefit=True)\n\n    feature_idx = sel.get_support()\n    selected_feature_names = X.columns[feature_idx]\n\n    \n    X_fs = X[selected_feature_names]\n    print(X_fs.shape)\n\n    del xtrain\n    del xvalid\n    del ytrain\n    del yvalid\n    del rfc_model\n    del eli5\n    del perm\n    del max_selected_features\n    del sel\n    del feature_idx\n    del selected_feature_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Training after Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"if feature_selection_flag == True:\n\n    num_of_folds = 2 ### must be more than 2\n\n    fs_clf_xgb, predictions_test_fs_xgb = xgboost_all_purpose(X_fs,\n                                                              y,\n                                                              type_of_training =type_of_training, \n                                                              num_of_folds = num_of_folds, \n                                                              max_early_stopping= 100, \n                                                              name=\"fs_clf_xgb\")\n    \n    del fs_clf_xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test set prediction probabilities distribution after feature selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"if feature_selection_flag == True:\n    sns.set(rc={'figure.figsize':(8,8)})\n    plt.hist(predictions_test_fs_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if feature_selection_flag == True:\n    print(clf_stats_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I believe that Feature selection worsen the auc-score, so I will not use it for the future experiments."},{"metadata":{},"cell_type":"markdown","source":"### ML Bayesian Optimization Tuning\n<a id=\"Bayes\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"if type_of_training == \"baseline\":\n    \n    print(\"baseline\")\n    from sklearn.model_selection import train_test_split\n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)\n\nelif type_of_training == \"oversampling\":\n    \n    print(\"oversampling\")\n    \n    from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n\n    # create a 70/30 split of the data \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)\n\n    # RandomOverSampler\n    ros = RandomOverSampler(random_state=42)\n    X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)\n\n    from collections import Counter\n    print(sorted(Counter(y_resampled).items()))\n\n    xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n    ytrain = y_resampled\n    del X_resampled\n    del y_resampled\n\n    \nelif type_of_training == \"augmentation_by_fraction\":\n    \n    from sklearn.model_selection import train_test_split\n    # the main idea here is to reducing the imbalance ratio from 9:1 to 3:1\n    print(\"augmentation\")\n    \n    from collections import Counter\n\n    # create a 70/30 split of the data \n    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)\n\n    print(\"ytrain target values count before augmentation:\\n\", sorted(Counter(ytrain).items()))\n\n    # Augmenting both minority and majority classes via RandomOverSampler by 3 times\n    X_y = pd.DataFrame(xtrain, columns=X.columns)\n    X_y[\"target\"] = ytrain\n    X_y = X_y.sample(frac=3, replace=True)\n    X_y.target.value_counts()\n    ytrain = X_y['target']\n    print(\"ytrain target values count after oversampling:\\n\",sorted(Counter(ytrain).items()))\n    xtrain = X_y.drop(['target'], axis=1)\n    del X_y\n\n    from imblearn.under_sampling import RandomUnderSampler\n\n    # reducing the majority class almost back to its original form\n    rus = RandomUnderSampler(sampling_strategy=0.33, random_state=42)\n    X_resampled, y_resampled = rus.fit_resample(xtrain, ytrain)\n\n    print(\"ytrain target values count after Augmentation:\\n\",sorted(Counter(y_resampled).items()))\n\n    xtrain=pd.DataFrame(X_resampled, columns = X.columns)\n    ytrain = y_resampled\n\n    del X_resampled\n    del y_resampled\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skopt import BayesSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\n\n# Classifier\nbayes_cv_tuner = BayesSearchCV(\n    estimator = xgb.XGBClassifier(\n        nthread = -1,\n        objective = 'binary:logistic',\n        eval_metric = 'auc',\n        silent=1,\n        tree_method='auto'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'n_estimators': (50, 100),\n        'max_depth': (0, 10),\n        'gamma': (1e-4, 20, 'log-uniform'),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-2, 10, 'log-uniform'),\n        'reg_alpha': (1e-4, 1.0, 'log-uniform'),\n        'max_delta_step': (0, 20),\n        'scale_pos_weight': (1e-1, 10, 'uniform')\n    },    \n    scoring = 'roc_auc',\n    cv = StratifiedKFold(\n        n_splits=3,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 1,\n    n_iter = 7,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\n\ndef status_print(optim_result):\n    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n    \n    # Get all the models tested so far in DataFrame format\n    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n    \n    # Get current parameters and the best parameters    \n    best_params = pd.Series(bayes_cv_tuner.best_params_)\n    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n        len(all_models),\n        np.round(bayes_cv_tuner.best_score_, 4),\n        bayes_cv_tuner.best_params_\n    ))\n    \n    ### Save all model results\n    #clf_name = bayes_cv_tuner.estimator.__class__.__name__\n    #all_models.to_csv(clf_name+\"_cv_results.csv\")\n    ###\n    \n# Fit the model\nresult = bayes_cv_tuner.fit(xtrain, ytrain, callback=status_print)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\n\ndel bayes_cv_tuner\nresult.best_params_['n_estimators'] = 3000\n\n#params['learning_rate'] = 0.01\n#params['scale_pos_weight'] = np.round(y.value_counts()[0] / y.value_counts()[1],3)\n#params['max_delta_step'] = 1\n\nresult.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retraining after tuning\n<a id=\"tuned\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_folds = 4 ### must be more than 2\n\ntuned_clf_xgb, predictions_test_tuned_xgb = xgboost_all_purpose(X,\n                                                                y,\n                                                                type_of_training = type_of_training, \n                                                                num_of_folds=num_of_folds, \n                                                                params = result.best_params_, \n                                                                max_early_stopping = 400, \n                                                                in_folds_sampling = False,\n                                                                name=\"tuned_clf_xgb\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test set predictions probabilities without Feature Selection and with Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(8,8)})\nplt.hist(predictions_test_tuned_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_stats_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML Blends\n** To be updated **"},{"metadata":{},"cell_type":"markdown","source":"## Preparing for submmission"},{"metadata":{"trusted":true},"cell_type":"code","source":"if untuned_model_flag == True:\n    gc.collect();\n    submission = pd.read_csv('../input/sample_submission.csv')\n    submission['target'] = predictions_test_xgb\n    submission.to_csv('clf_xgb.csv', index=False)\n\n\nif feature_selection_flag == True:\n    gc.collect();\n    submission = pd.read_csv('../input/sample_submission.csv')\n    submission['target'] = predictions_test_fs_xgb\n    submission.to_csv('fs_clf_xgb.csv', index=False)\n\n\ngc.collect();\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission['target'] = predictions_test_tuned_xgb\nsubmission.to_csv('tuned_clf_xgb.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n<a id=\"conclusion\"></a>\nWe can see from EDA and ML Modeling that class #1 is very unbalanced and difficult to identified and classified."},{"metadata":{},"cell_type":"markdown","source":"_________________________________________\n*I would be happy if my kernel helped you understand this problem and the data better.\nif this kernel is helpful to you a thumb up or follow would be much appreciated!*"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}