{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# General information\n\nIn Santander Customer Transaction Prediction competition we have a binary classification task. Train and test data have 200k samples each and we have 200 anonimyzed numerical columns. It would be interesting to try good models without overfitting and knowing the meaning of the features.\nIn fact this competition seems to be similar to another current competition: don't overfit II, so I'll use a lot of ideas from my [kernel](https://www.kaggle.com/artgor/how-to-not-overfit).\n\nIn this kernel I'll write the following things:\n\n* EDA on the features and trying to get some insights;\n* Using permutation importance to select most impactful features;\n* Comparing various models: linear models, tree based models and others;\n* Trying various approaches to feature selection including taking top features from eli5;\n* Hyperparameter optimization for models;\n* Feature generation;\n* Other things;\n\n![](https://i.imgur.com/e5vPHpJ.png)\n\n*Work still in progress*"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn import metrics\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom tqdm import tqdm_notebook\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"083197bdca25ab31331d0dcd1e236bc0d8b4ec1d"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b40dd26ead2705ebf0058b0aa528c89a18aedbe"},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"trusted":true,"_uuid":"67a26174603f6a28709b7e0431aa431832ae608d"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"504b609f61494c21f8b078182e06191581ecb2a6"},"cell_type":"code","source":"train[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7b0f65d2d5c999a44a3f71e013b1b6a6ff08980"},"cell_type":"code","source":"train[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc9c24aa3f65d47de55ead31b586779d33a74b3"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d205e01b009224a3189903e1858dd592fb222d2d"},"cell_type":"code","source":"# we have no missing values\ntrain.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30e64cca712542d662201263914d8fc25496563e","scrolled":true},"cell_type":"code","source":"print('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227daacd39977e5658c7e27db2686d8f65fdff3c"},"cell_type":"code","source":"train['target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbacb1c196fda0a843d834a67abb15d84fc3d997"},"cell_type":"markdown","source":"From this overview we can see the following things:\n* target is binary and has disbalance: 10% of samples belong to 1 class;\n* values in columns are more or less similar;\n* columns have high std (up to 20)\n* columns have a high range of means;"},{"metadata":{"_uuid":"06df27b43428261da7daf02e708b934519d78ac2"},"cell_type":"markdown","source":"Let's have a look at correlations now!"},{"metadata":{"trusted":true,"_uuid":"ae63462aa70238f0a2858de687dc7d2ae319589a","scrolled":true},"cell_type":"code","source":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139717e82051932cdf9ee85a9025888f8bdf9e26"},"cell_type":"code","source":"corrs.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d921a5d3bf606b88853988c10acad020685334"},"cell_type":"markdown","source":"We can see that all features have a low correlation with target. So we have no highly correlated features which we could drop, on the other hand we could drop some columns with have little correlation with the target."},{"metadata":{"_uuid":"a4f28e1e3c847e2fe165034dd870154afb7fe939"},"cell_type":"markdown","source":"## Basic modelling"},{"metadata":{"trusted":true,"_uuid":"8f3eef02d6beac1b76f88c75bb842da9a313f592"},"cell_type":"code","source":"X = train.drop(['ID_code', 'target'], axis=1)\ny = train['target']\nX_test = test.drop(['ID_code'], axis=1)\nn_fold = 4\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=20, random_state=42)\n\n# scaler = StandardScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"32b8fe75f240c11df7eaf3ed91b76d9260f999c9"},"cell_type":"code","source":"def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=1000,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            y_pred = model_results.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.05, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test)[:, 1]\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n        if averaging == 'usual':\n            prediction += y_pred\n        elif averaging == 'rank':\n            prediction += pd.Series(y_pred).rank().values  \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction, scores\n    \n    else:\n        return oof, prediction, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fe9017199e72183e9686e55a3608c9339b779302"},"cell_type":"code","source":"# %%time\n# model = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.1)\n# oof_lr, prediction_lr, scores = train_model(X, X_test, y, params=None, folds=folds, model_type='sklearn', model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b482e006b077109f725547d616818eb92107d23","scrolled":true},"cell_type":"code","source":"params = {'num_leaves': 128,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}\n# oof_lgb, prediction_lgb, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c7c9391d1a6e233e48464200183cafc97f7a82"},"cell_type":"code","source":"# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = prediction_lgb\n# sub.to_csv('lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8722f00ff969f10183c8d946eb01dfc04d5c8636"},"cell_type":"markdown","source":"## ELI5"},{"metadata":{"trusted":true,"_uuid":"cc4e11a19364093ba5c214bc11662f88c36bc135"},"cell_type":"code","source":"model = lgb.LGBMClassifier(**params, n_estimators = 20000, n_jobs = -1)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b40c5ac094c412d87228eba631e0e510d215d7ac"},"cell_type":"code","source":"eli5.show_weights(model, targets=[0, 1], feature_names=list(X_train.columns), top=40, feature_filter=lambda x: x != '<BIAS>')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7365d1a85f65b6ca1038e4b1109774cca45c32b"},"cell_type":"markdown","source":"ELI5 didn't help up to eliminate features, but let's at least try to take top-100 and see how it helps."},{"metadata":{"trusted":true,"_uuid":"f1bd94678b5f57d6dfad1a046122ee38e7d07cf1"},"cell_type":"code","source":"top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:100]\nX1 = X[top_features]\nX_train, X_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, stratify=y)\nmodel.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=1000, early_stopping_rounds=200)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dd1507d82569695d3c0e84f351a3993caa426842"},"cell_type":"code","source":"def calculate_metrics(model, X_train: pd.DataFrame() = None, y_train: pd.DataFrame() = None, X_valid: pd.DataFrame() = None,\n                      y_valid: pd.DataFrame() = None, columns: list = []) -> pd.DataFrame():\n    columns = columns if len(columns) > 0 else list(X_train.columns)\n    train_pred = model.predict_proba(X_train[columns])\n    valid_pred = model.predict_proba(X_valid[columns])\n    f1 = 0\n    best_t = 0\n    for t in np.arange(0.1, 1, 0.05):\n        valid_pr = (valid_pred[:, 1] > t).astype(int)\n        valid_f1 = metrics.f1_score(y_valid, valid_pr)\n        if valid_f1 > f1:\n            f1 = valid_f1\n            best_t = t\n\n    t = best_t\n    train_pr = (train_pred[:, 1] > t).astype(int)\n    valid_pr = (valid_pred[:, 1] > t).astype(int)\n    train_f1 = metrics.f1_score(y_train, train_pr)\n    valid_f1 = metrics.f1_score(y_valid, valid_pr)\n    score_df = []\n    print(f'Best threshold: {t:.2f}. Train f1: {train_f1:.4f}. Valid f1: {valid_f1:.4f}.')\n    score_df.append(['F1', np.round(train_f1, 4), np.round(valid_f1, 4)])\n    train_r = metrics.recall_score(y_train, train_pr)\n    valid_r = metrics.recall_score(y_valid, valid_pr)\n\n    score_df.append(['Recall', np.round(train_r, 4), np.round(valid_r, 4)])\n    train_p = metrics.precision_score(y_train, train_pr)\n    valid_p = metrics.precision_score(y_valid, valid_pr)\n\n    score_df.append(['Precision', np.round(train_p, 4), np.round(valid_p, 4)])\n    train_roc = metrics.roc_auc_score(y_train, train_pred[:, 1])\n    valid_roc = metrics.roc_auc_score(y_valid, valid_pred[:, 1])\n\n    score_df.append(['ROCAUC', np.round(train_roc, 4), np.round(valid_roc, 4)])\n    train_apc = metrics.average_precision_score(y_train, train_pred[:, 1])\n    valid_apc = metrics.average_precision_score(y_valid, valid_pred[:, 1])\n\n    score_df.append(['APC', np.round(train_apc, 4), np.round(valid_apc, 4)])\n    print(metrics.confusion_matrix(y_valid, valid_pr))\n    score_df = pd.DataFrame(score_df, columns=['Metric', 'Train', 'Valid'])\n    print(score_df)\n\n    return score_df, t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d5f9039a5686e2ca4af971c588088ba99392d25"},"cell_type":"code","source":"_ = calculate_metrics(model, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e05845adf82ca59744a9b5646e52b6fccb2a0b"},"cell_type":"markdown","source":"## Feature generation"},{"metadata":{"_uuid":"75df89f616d2350df01bc9450299a523725ee4f2"},"cell_type":"markdown","source":"### Feature interaction\n\nDidn't improve score"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"811de0f3a350ec1551fb67fb4d5e74f2e51e632e"},"cell_type":"code","source":"# X = train.drop(['ID_code', 'target'], axis=1)\n# X_test = test.drop(['ID_code'], axis=1)\n\n# columns = top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:20]\n# for col1 in tqdm_notebook(columns):\n#     for col2 in columns:\n#         X[col1 + '_' + col2] = X[col1] * X[col2]   \n#         X_test[col1 + '_' + col2] = X_test[col1] * X_test[col2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62d08ba8cfe665b32be838d9192ab39792637f58"},"cell_type":"code","source":"# oof_lgb, prediction_lgb_inter, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36986ec02ddef9a26eca3ef6cb575739f5bccdb9"},"cell_type":"code","source":"# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = prediction_lgb_inter\n# sub.to_csv('lgb_inter.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eec4da0a4f89dd20c6674463ab1cba90b8619213"},"cell_type":"markdown","source":"### Scaling\n\n! **Notice** scaling severely decreases score"},{"metadata":{"trusted":true,"_uuid":"03932dadcd6b4a6dd89a5b568d6bb4d506b92e08"},"cell_type":"code","source":"# X = train.drop(['ID_code', 'target'], axis=1)\n# X_test = test.drop(['ID_code'], axis=1)\n# scaler = StandardScaler()\n# X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n# X_test[X_train.columns] = scaler.transform(X_test[X_train.columns])\n# oof_lgb, prediction_lgb_scaled, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\n# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = prediction_lgb_scaled\n# sub.to_csv('lgb_scaled.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91238a2383ed53097ea56e580467a983cb98ee87"},"cell_type":"markdown","source":"### Statistics"},{"metadata":{"trusted":true,"_uuid":"bc581b40445bec9ef01f2cde0299d788739d5d34"},"cell_type":"code","source":"# X = train.drop(['ID_code', 'target'], axis=1)\n# X_test = test.drop(['ID_code'], axis=1)\n\n# X['std'] = X.std(1)\n# X_test['std'] = X_test.std(1)\n\n# X['mean'] = X.mean(1)\n# X_test['mean'] = X_test.mean(1)\n# oof_lgb, prediction_lgb_stats, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\n# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = prediction_lgb_stats\n# sub.to_csv('lgb_stats.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4014f1c8e6a3069df0b5dee0b3302a03a8d22bb"},"cell_type":"markdown","source":"Training with these features gives the same score on LB: 0.899"},{"metadata":{"_uuid":"94f70ed8fb086a7d6874dd8326bc2ef52a029554"},"cell_type":"markdown","source":"### NN features\n\nTakes several hours."},{"metadata":{"trusted":true,"_uuid":"69d2584d7b60bb617723ac00a7eab6bba129ad10"},"cell_type":"code","source":"%%time\nX = train.drop(['ID_code', 'target'], axis=1)\nX_test = test.drop(['ID_code'], axis=1)\nneigh = NearestNeighbors(3, n_jobs=-1)\nneigh.fit(X)\n\ndists, _ = neigh.kneighbors(X, n_neighbors=3)\nmean_dist = dists.mean(axis=1)\nmax_dist = dists.max(axis=1)\nmin_dist = dists.min(axis=1)\n\nX['mean_dist'] = mean_dist\nX['max_dist'] = max_dist\nX['min_dist'] = min_dist\n\ntest_dists, _ = neigh.kneighbors(X_test, n_neighbors=3)\n\ntest_mean_dist = test_dists.mean(axis=1)\ntest_max_dist = test_dists.max(axis=1)\ntest_min_dist = test_dists.min(axis=1)\n\nX_test['mean_dist'] = test_mean_dist\nX_test['max_dist'] = test_max_dist\nX_test['min_dist'] = test_min_dist\n\noof_lgb, prediction_lgb_dist, scores = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\nsub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = prediction_lgb_dist\nsub.to_csv('lgb_dist.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e47b5f437591fea8f65271652cd4f4d6fcad0e40"},"cell_type":"markdown","source":"## Blend"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8b422c7df1b88d65eee397bf2ab0f987c9d8feba"},"cell_type":"code","source":"# xgb_params = {'eta': 0.05, 'max_depth': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, \n#           'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True, 'nthread': 4}\n# oof_xgb, prediction_xgb, scores = train_model(X, X_test, y, params=xgb_params, folds=folds, model_type='xgb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f271a33d4aae27cd7e9486af21826376ca01f1e"},"cell_type":"code","source":"# cat_params = {'depth': 13,\n#               'l2_leaf_reg': 10,\n#               'bootstrap_type': 'Bernoulli',\n#               #'metric_period': 500,\n#               'od_type': 'Iter',\n#               'od_wait': 50,\n#               'random_seed': 11,\n#               'allow_writing_files': False}\n# oof_cat, prediction_cat, _ = train_model(X, X_test, y, params=cat_params, folds=folds, model_type='cat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41b037acb317f5031073010b51ea0f53cb326e12"},"cell_type":"code","source":"# sub['target'] = (prediction_lgb + prediction_xgb) / 2\n# sub.to_csv('blend1.csv', index=False)\n# sub['target'] = (prediction_lgb + prediction_xgb + prediction_cat) / 3\n# sub.to_csv('blend2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"152a2060154c96fac04f0703bc2515b210608ee3"},"cell_type":"markdown","source":"### Rounding data"},{"metadata":{"trusted":true,"_uuid":"b7d26a804f9b74e21d90c68522048c378411621a"},"cell_type":"code","source":"# oof_lgb, prediction_lgb, scores = train_model(np.round(X, 3), np.round(X_test, 3), y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)\n# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = prediction_lgb\n# sub.to_csv('lgb_rounded_3.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}