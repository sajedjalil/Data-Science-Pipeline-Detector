{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SoftOrdering1DCNN on Santander Customer Transaction Prediction (SCTP)\n\nIn this notebook we will apply the neural network architecture winner of the 2nd place on MoA (https://www.kaggle.com/c/lish-moa/discussion/202256) to the Santander customer transaction prediction problem.\n\nThe results of this network (which we call SoftOrdering1DCNN) are benchmarked against a plain MLP and LightGBM.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# needed for deterministic output\nSEED = 2\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# device in which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## data preparation","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"../input/santander-customer-transaction-prediction/train.csv\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset split: train 60% - valid 20% - test 20%\n\nindex = np.array(dataset.index)\nnp.random.shuffle(index)\nn = len(index)\n\ntrain_index = index[0:int(0.6*n)]\nvalid_index = index[int(0.6*n):int(0.8*n)]\ntest_index = index[int(0.8*n):]\n\ntrain_dset = dataset.loc[train_index].reset_index(drop=True)\nvalid_dset = dataset.loc[valid_index].reset_index(drop=True)\ntest_dset = dataset.loc[test_index].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_features = dataset.columns[2:].tolist()\ntarget = \"target\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parsing inputs as pytorch tensor dataset\n\ntrain_tensor_dset = TensorDataset(\n    torch.tensor(train_dset[input_features].values, dtype=torch.float),\n    torch.tensor(train_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\nvalid_tensor_dset = TensorDataset(\n    torch.tensor(valid_dset[input_features].values, dtype=torch.float),\n    torch.tensor(valid_dset[target].values.reshape(-1,1), dtype=torch.float)\n)\n\ntest_tensor_dset = TensorDataset(\n    torch.tensor(test_dset[input_features].values, dtype=torch.float),\n    torch.tensor(test_dset[target].values.reshape(-1,1), dtype=torch.float) \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## 3-layers MLP","metadata":{}},{"cell_type":"code","source":"class DNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, dropout, momentum):\n        super().__init__()\n\n        self.bn_in = nn.BatchNorm1d(input_dim, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout)\n        self.ln_in = nn.Linear(input_dim, nn_width, bias=False)\n\n        self.bnorms = nn.ModuleList([nn.BatchNorm1d(nn_width, momentum=momentum) for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList([nn.Dropout(dropout) for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList([nn.Linear(nn_width, nn_width, bias=False) for i in range(nn_depth-1)])\n        \n        self.bn_out = nn.BatchNorm1d(nn_width, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout/2)\n        self.ln_out = nn.Linear(nn_width, output_dim, bias=False)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = nn.functional.relu(self.ln_in(x))\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    nn_depth=3, \n    nn_width=256, \n    dropout=0.2, \n    momentum=0.1\n)\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=20,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(callbacks=[early_stop_callback], min_epochs=10, max_epochs=200, gpus=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summarize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=2048, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## SoftOrdering1DCNN","metadata":{}},{"cell_type":"code","source":"class SoftOrdering1DCNN(pl.LightningModule):\n\n    def __init__(self, input_dim, output_dim, sign_size=32, cha_input=16, cha_hidden=32, \n                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):\n        super().__init__()\n\n        hidden_size = sign_size*cha_input\n        sign_size1 = sign_size\n        sign_size2 = sign_size//2\n        output_size = (sign_size//4) * cha_hidden\n\n        self.hidden_size = hidden_size\n        self.cha_input = cha_input\n        self.cha_hidden = cha_hidden\n        self.K = K\n        self.sign_size1 = sign_size1\n        self.sign_size2 = sign_size2\n        self.output_size = output_size\n        self.dropout_input = dropout_input\n        self.dropout_hidden = dropout_hidden\n        self.dropout_output = dropout_output\n\n        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n        self.dropout1 = nn.Dropout(dropout_input)\n        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n        self.dense1 = nn.utils.weight_norm(dense1)\n\n        # 1st conv layer\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n        conv1 = conv1 = nn.Conv1d(\n            cha_input, \n            cha_input*K, \n            kernel_size=5, \n            stride = 1, \n            padding=2,  \n            groups=cha_input, \n            bias=False)\n        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = sign_size2)\n\n        # 2nd conv layer\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)\n        self.dropout_c2 = nn.Dropout(dropout_hidden)\n        conv2 = nn.Conv1d(\n            cha_input*K, \n            cha_hidden, \n            kernel_size=3, \n            stride=1, \n            padding=1, \n            bias=False)\n        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n\n        # 3rd conv layer\n        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n        self.dropout_c3 = nn.Dropout(dropout_hidden)\n        conv3 = nn.Conv1d(\n            cha_hidden, \n            cha_hidden, \n            kernel_size=3, \n            stride=1, \n            padding=1, \n            bias=False)\n        self.conv3 = nn.utils.weight_norm(conv3, dim=None)\n        \n\n        # 4th conv layer\n        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n        conv4 = nn.Conv1d(\n            cha_hidden, \n            cha_hidden, \n            kernel_size=5, \n            stride=1, \n            padding=2, \n            groups=cha_hidden, \n            bias=False)\n        self.conv4 = nn.utils.weight_norm(conv4, dim=None)\n\n        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        self.batch_norm2 = nn.BatchNorm1d(output_size)\n        self.dropout2 = nn.Dropout(dropout_output)\n        dense2 = nn.Linear(output_size, output_dim, bias=False)\n        self.dense2 = nn.utils.weight_norm(dense2)\n\n        self.loss = nn.BCEWithLogitsLoss()\n\n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = nn.functional.celu(self.dense1(x))\n\n        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n\n        x = self.batch_norm_c1(x)\n        x = nn.functional.relu(self.conv1(x))\n\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = nn.functional.relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c3(x)\n        x = self.dropout_c3(x)\n        x = nn.functional.relu(self.conv3(x))\n\n        x = self.batch_norm_c4(x)\n        x = self.conv4(x)\n        x =  x + x_s\n        x = nn.functional.relu(x)\n\n        x = self.avg_po_c4(x)\n\n        x = self.flt(x)\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.dense2(x)\n\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self.forward(X)\n        loss = self.loss(y_hat, y)\n        self.log('valid_loss', loss)\n        \n    def test_step(self, batch, batch_idx):\n        X, y = batch\n        y_logit = self.forward(X)\n        y_probs = torch.sigmoid(y_logit).detach().cpu().numpy()\n        loss = self.loss(y_logit, y)\n        metric = roc_auc_score(y.cpu().numpy(), y_probs)\n        self.log('test_loss', loss)\n        self.log('test_metric', metric)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=1e-2, momentum=0.9)\n        scheduler = {\n            'scheduler': ReduceLROnPlateau(\n                optimizer, \n                mode=\"min\", \n                factor=0.5, \n                patience=5, \n                min_lr=1e-5),\n            'interval': 'epoch',\n            'frequency': 1,\n            'reduce_on_plateau': True,\n            'monitor': 'valid_loss',\n        }\n        return [optimizer], [scheduler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SoftOrdering1DCNN(\n    input_dim=len(input_features), \n    output_dim=1, \n    sign_size=16, \n    cha_input=64, \n    cha_hidden=64, \n    K=2, \n    dropout_input=0.3, \n    dropout_hidden=0.3, \n    dropout_output=0.2\n)\n\nearly_stop_callback = EarlyStopping(\n   monitor='valid_loss',\n   min_delta=.0,\n   patience=21,\n   verbose=True,\n   mode='min'\n)\n\ntrainer = pl.Trainer(callbacks=[early_stop_callback], min_epochs=10, max_epochs=200, gpus=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summarize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(\n    model, \n    DataLoader(train_tensor_dset, batch_size=2048, shuffle=True, num_workers=4),\n    DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on validation dataset\ntrainer.test(model, DataLoader(valid_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on test dataset\ntrainer.test(model, DataLoader(test_tensor_dset, batch_size=2048, shuffle=False, num_workers=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***\n## LightGBM","metadata":{}},{"cell_type":"code","source":"train_dataset = lgb.Dataset(\n    train_dset[input_features].values,\n    train_dset[target].values,\n    free_raw_data=False\n)\n\nvalid_dataset = lgb.Dataset(\n    valid_dset[input_features].values,\n    valid_dset[target].values,\n    free_raw_data=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params = dict(\n    objective = \"binary\",\n    learning_rate = 0.1,\n    num_leaves = 32,\n    seed = 2,\n    deterministic = True,\n    metric = \"auc\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lgb.train(\n    model_params, \n    train_dataset, \n    valid_sets=[valid_dataset,],\n    num_boost_round=1000,\n    early_stopping_rounds=20,\n    verbose_eval=50,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on validation dataset\nmodel.best_score[\"valid_0\"][\"auc\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AUC on test dataset\npreds = model.predict(test_dset[input_features].values)\nroc_auc_score(test_dset[target].values, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{}}]}