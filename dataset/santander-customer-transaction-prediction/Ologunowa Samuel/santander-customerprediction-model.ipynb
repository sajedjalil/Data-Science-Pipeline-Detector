{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport sklearn.decomposition as skde\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nimport sklearn.metrics as sklm\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import Datasets****\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import required datasets\ntest_data = pd.read_csv(\"../input/test.csv\")\ntrain_data= pd.read_csv(\"../input/train.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explore Datasets**\n- View Data information\n- Check for missing values\n- Check correlation \n- Visualize class distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#view Data shape\nprint(train_data.shape,test_data.shape)\n#check for missing values\nprint(train_data.isnull().values.any(),test_data.isnull().values.any())\n#check data correlation \nprint(train_data.corr())\n#Visualize class distribution\nsns.countplot(train_data['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Deductions from the above**\n- There are no missing vallues in either the test or train sets\n- The train set contains an extra column which is the known target to be used in training our model\n- There is no correlation between the features in the train dataset\n- There is a significant class imbalance between the 0s and 1s\n\nThe percentage of each class is calculated in the cell below"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check class distribution in percentage\ncount_0 = len(train_data[train_data[\"target\"] == 0])\ncount_1 = len(train_data[train_data[\"target\"] == 1])\npercentage_count_0 = ((count_0)/(count_0+count_1)) * 100\npercentage_count_1 = 100-percentage_count_0\nprint(\"{}{}{}{}{}\".format(\"Percentage of 0 class is \",percentage_count_0,\"\\n\",\"Percentage of 1 class is \",percentage_count_1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split train_data into train and validation sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_data[\"target\"]\nnew_train_data = train_data.drop([\"target\",\"ID_code\"],axis =1)\nnew_train_data.head()\nx_train, x_test, y_train, y_test = train_test_split(new_train_data, labels, test_size = 0.25, random_state = 0)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use StratifiedKFold to ensure test and train datasets contains equal percentage of both classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=2319)\nparam = {\n    'bagging_freq': 5, \n    'bagging_fraction': 0.33,\n    'boost_from_average':'false',   \n    'boost': 'gbdt',\n    'feature_fraction': 0.0405,\n    'learning_rate': 0.083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,     \n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 4,            \n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1\n}\noof = np.zeros(len(train_data))\npredictions = np.zeros(len(test_data))\nfeatures = [c for c in train_data.columns if c not in ['ID_code', 'target']]\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data.values, labels.values)):\n    trn_data = lgb.Dataset(train_data.iloc[trn_idx][features], label=labels.iloc[trn_idx])\n    val_data = lgb.Dataset(train_data.iloc[val_idx][features], label=labels.iloc[val_idx])\n    clf = lgb.train(param, trn_data, 1000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train_data.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(test_data[features], num_iteration=clf.best_iteration) / folds.n_splits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_data.iloc[val_idx]['target']\nprint(\"\\n >> CV score: {:<8.5f}\".format(roc_auc_score(target, oof[val_idx])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_code = test_data[\"ID_code\"]\nsubmission = pd.DataFrame({'ID_code' : ID_code,\n                            'target' : predictions})\nsubmission.to_csv('./version1.csv', index=False)\nsub = pd.read_csv('./version1.csv')\nsub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}