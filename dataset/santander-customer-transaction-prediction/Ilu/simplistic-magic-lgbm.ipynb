{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"def get_logger():\n    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\n    logging.basicConfig(format=FORMAT)\n    logger = logging.getLogger('main')\n    logger.setLevel(logging.DEBUG)\n    return logger\n    \nlogger = get_logger()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(nrows=None):\n    logger.info('Input data')\n    train_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv',nrows=nrows)\n    test_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now comes the magic in the steps I found it:\n1. Find unique values and mark the spots with a 1 (feature: IsUnique) \n1. Frequency encoding adds a little bit to the final score (feature: freq)\n1. Map the real values to IsUnique: (feature: OnlyUnique)\n1. Mark all missing values as NAN (not zero) to increase the score\n1. Add a feature that includes all  non unique values (feature: NotUnique)\n1. Again: mark all missing values as NAN (not zero) to increase the score\n1. IsUnique may be removed at this point as the information is included in the other features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(train_df, test_df):\n    logger.info('Features engineering')\n    \n    synthetic = np.load('../input/publicprivate/synthetic_samples_indexes.npy')\n    synthetic = synthetic-200000\n    synthetic = np.array(synthetic)\n    test_df = test_df.iloc[~test_df.index.isin(synthetic)]\n\n    idx = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n    \n    traintest = pd.concat([train_df, test_df])\n    traintest = traintest.reset_index(drop=True)\n    \n    for col in idx:\n        #find unique values\n        varname = col + '_IsUnique'\n        traintest[varname] = 0\n        _, index_, count_ = np.unique(traintest.loc[:,col].values, return_counts=True, return_index=True)\n        traintest[varname][index_[count_ == 1]] += 1\n        traintest[varname] = traintest[varname] / (traintest[varname] == 1).sum()\n    \n    #frequency encoding\n    for col in idx:\n        traintest[col+'_freq'] = traintest[col].map(traintest.groupby(col).size())\n    \n    #Fill values from traintest to IsUnique or NotUnique. Replace zeroes with NANs\n    for col in idx:\n        varname = col + '_IsUnique'\n        tmp_col = traintest.loc[traintest[varname] > 0][col]\n        traintest[col + '_OnlyUnique'] = tmp_col\n        traintest[col + '_OnlyUnique'] = traintest[col + '_OnlyUnique'].fillna(0)\n        traintest[col + '_NotUnique'] = traintest[col] - traintest[col + '_OnlyUnique']\n        traintest[col + '_NotUnique'] = traintest[col + '_NotUnique'].replace(0,np.nan)\n        traintest[col + '_OnlyUnique'] = traintest[col + '_OnlyUnique'].replace(0,np.nan)\n        traintest.pop(varname)\n\n    train_df = traintest[:200000]\n    test_df = traintest[200000:]\n    \n    print('Train and test shape:',train_df.shape, test_df.shape)\n    return train_df, test_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Almost no hyperparameter tuning needed\ndef run_model(train_df, test_df):\n    logger.info('Prepare the model')\n    features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n    target = train_df['target']\n    logger.info('Run model')\n    param = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38, \n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.0095,\n        'max_depth': -1,  #-1\n        'metric':'auc',\n        'min_data_in_leaf': 20,\n        'min_sum_hessian_in_leaf': 10.0, \n        'num_leaves': 3,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }\n    num_round = 1000000\n    folds = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n        print(\"Fold {}\".format(fold_))\n        trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n        val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3500)\n        oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n        predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / folds.n_splits\n    print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(test_df, predictions):\n    logger.info('Prepare submission')\n    \n    all_test_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n    sub = pd.DataFrame({\"ID_code\": all_test_df.ID_code.values})\n    sub[\"target\"] = 0\n    sub_real = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n    sub_real[\"target\"] = predictions\n    sub = sub.set_index('ID_code')\n    sub_real = sub_real.set_index('ID_code')\n    sub.update(sub_real)\n    sub = sub.reset_index()\n    \n    sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(nrows=None):\n    train_df, test_df = read_data(nrows)\n    train_df, test_df = process_data(train_df, test_df)\n    predictions = run_model(train_df, test_df)\n    submit(test_df, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}