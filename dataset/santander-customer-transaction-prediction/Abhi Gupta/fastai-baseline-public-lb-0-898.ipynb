{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.tabular import *","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = Path('../input')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls {PATH}","execution_count":3,"outputs":[{"output_type":"stream","text":"sample_submission.csv  test.csv  train.csv\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(PATH/'train.csv')\ntest_df = pd.read_csv(PATH/'test.csv')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [feature for feature in df.columns if 'var_' in feature]\nlen(features)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"200"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_df(df):\n    for feature in features:\n        df[f'sq_{feature}'] = df[feature]**2\n        df[f'repo_{feature}'] = df[feature].apply(lambda x: 0 if x==0 else 1/x)\n    df['min'] = df[features].min(axis=1)\n    df['mean'] = df[features].mean(axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['std'] = df[features].std(axis=1)\n    df['var'] = df[features].var(axis=1)\n    df['abs_mean'] = df[features].abs().mean(axis=1)\n    df['abs_median'] = df[features].abs().median(axis=1)\n    df['abs_std'] = df[features].abs().std(axis=1)\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    df['sq_kurt'] = df[[f'sq_{feature}' for feature in features]].kurt(axis=1)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augment_df(df)\ndf.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   ID_code  target    var_0    ...          skew      kurt    sq_kurt\n0  train_0       0   8.9255    ...      0.101580  1.331023  23.320706\n1  train_1       0  11.5006    ...     -0.351734  4.110215  21.143081\n2  train_2       0   8.6093    ...     -0.056957  0.546438  10.849758\n3  train_3       0  11.0604    ...     -0.480116  2.630499  14.138277\n4  train_4       0   9.8369    ...     -1.463426  9.787399  64.143395\n\n[5 rows x 614 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>sq_var_186</th>\n      <th>repo_var_186</th>\n      <th>sq_var_187</th>\n      <th>repo_var_187</th>\n      <th>sq_var_188</th>\n      <th>repo_var_188</th>\n      <th>sq_var_189</th>\n      <th>repo_var_189</th>\n      <th>sq_var_190</th>\n      <th>repo_var_190</th>\n      <th>sq_var_191</th>\n      <th>repo_var_191</th>\n      <th>sq_var_192</th>\n      <th>repo_var_192</th>\n      <th>sq_var_193</th>\n      <th>repo_var_193</th>\n      <th>sq_var_194</th>\n      <th>repo_var_194</th>\n      <th>sq_var_195</th>\n      <th>repo_var_195</th>\n      <th>sq_var_196</th>\n      <th>repo_var_196</th>\n      <th>sq_var_197</th>\n      <th>repo_var_197</th>\n      <th>sq_var_198</th>\n      <th>repo_var_198</th>\n      <th>sq_var_199</th>\n      <th>repo_var_199</th>\n      <th>min</th>\n      <th>mean</th>\n      <th>max</th>\n      <th>median</th>\n      <th>std</th>\n      <th>var</th>\n      <th>abs_mean</th>\n      <th>abs_median</th>\n      <th>abs_std</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sq_kurt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>140.211649</td>\n      <td>0.084452</td>\n      <td>388.716713</td>\n      <td>-0.050720</td>\n      <td>308.856020</td>\n      <td>0.056901</td>\n      <td>0.343044</td>\n      <td>1.707359</td>\n      <td>19.672773</td>\n      <td>0.225459</td>\n      <td>15.714882</td>\n      <td>0.252258</td>\n      <td>9.837005</td>\n      <td>0.318837</td>\n      <td>2.859481</td>\n      <td>0.591366</td>\n      <td>343.090415</td>\n      <td>0.053988</td>\n      <td>5.749445</td>\n      <td>-0.417049</td>\n      <td>62.069187</td>\n      <td>0.126929</td>\n      <td>73.333532</td>\n      <td>0.116775</td>\n      <td>163.336068</td>\n      <td>0.078245</td>\n      <td>1.191154</td>\n      <td>-0.916254</td>\n      <td>-21.4494</td>\n      <td>7.281591</td>\n      <td>43.1127</td>\n      <td>6.77040</td>\n      <td>9.331540</td>\n      <td>87.077642</td>\n      <td>9.433039</td>\n      <td>8.41525</td>\n      <td>7.136966</td>\n      <td>0.101580</td>\n      <td>1.331023</td>\n      <td>23.320706</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>58.588308</td>\n      <td>0.130646</td>\n      <td>253.825438</td>\n      <td>-0.062767</td>\n      <td>177.355806</td>\n      <td>0.075089</td>\n      <td>0.127164</td>\n      <td>-2.804262</td>\n      <td>58.401692</td>\n      <td>0.130854</td>\n      <td>59.620018</td>\n      <td>0.129510</td>\n      <td>6.675506</td>\n      <td>0.387042</td>\n      <td>119.937543</td>\n      <td>0.091311</td>\n      <td>238.100330</td>\n      <td>0.064807</td>\n      <td>4.136749</td>\n      <td>0.491666</td>\n      <td>66.043253</td>\n      <td>0.123051</td>\n      <td>77.244763</td>\n      <td>0.113780</td>\n      <td>336.942736</td>\n      <td>0.054478</td>\n      <td>3.809523</td>\n      <td>0.512348</td>\n      <td>-47.3797</td>\n      <td>7.076818</td>\n      <td>40.5632</td>\n      <td>7.22315</td>\n      <td>10.336130</td>\n      <td>106.835574</td>\n      <td>9.684721</td>\n      <td>8.03470</td>\n      <td>7.931169</td>\n      <td>-0.351734</td>\n      <td>4.110215</td>\n      <td>21.143081</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>28.885250</td>\n      <td>0.186064</td>\n      <td>39.262756</td>\n      <td>-0.159591</td>\n      <td>103.905404</td>\n      <td>0.098103</td>\n      <td>0.708459</td>\n      <td>-1.188072</td>\n      <td>8.443092</td>\n      <td>0.344151</td>\n      <td>95.853890</td>\n      <td>0.102140</td>\n      <td>2.790236</td>\n      <td>0.598659</td>\n      <td>2.841922</td>\n      <td>0.593190</td>\n      <td>466.741458</td>\n      <td>0.046287</td>\n      <td>9.870279</td>\n      <td>0.318299</td>\n      <td>42.527354</td>\n      <td>-0.153344</td>\n      <td>68.351556</td>\n      <td>0.120956</td>\n      <td>216.743173</td>\n      <td>0.067925</td>\n      <td>0.157212</td>\n      <td>2.522068</td>\n      <td>-22.4038</td>\n      <td>6.204483</td>\n      <td>33.8820</td>\n      <td>5.89940</td>\n      <td>8.753387</td>\n      <td>76.621777</td>\n      <td>8.618451</td>\n      <td>7.28465</td>\n      <td>6.376510</td>\n      <td>-0.056957</td>\n      <td>0.546438</td>\n      <td>10.849758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>62.942009</td>\n      <td>0.126046</td>\n      <td>164.555018</td>\n      <td>-0.077955</td>\n      <td>154.067674</td>\n      <td>0.080565</td>\n      <td>3.418431</td>\n      <td>0.540862</td>\n      <td>19.950516</td>\n      <td>0.223884</td>\n      <td>22.498895</td>\n      <td>0.210824</td>\n      <td>0.515237</td>\n      <td>1.393146</td>\n      <td>2.020378</td>\n      <td>0.703532</td>\n      <td>530.597404</td>\n      <td>0.043413</td>\n      <td>1.614424</td>\n      <td>-0.787030</td>\n      <td>8.570256</td>\n      <td>-0.341588</td>\n      <td>105.929381</td>\n      <td>0.097161</td>\n      <td>322.910118</td>\n      <td>0.055649</td>\n      <td>80.992800</td>\n      <td>-0.111116</td>\n      <td>-35.1659</td>\n      <td>6.441159</td>\n      <td>38.1015</td>\n      <td>6.70260</td>\n      <td>9.594064</td>\n      <td>92.046058</td>\n      <td>9.095397</td>\n      <td>7.74480</td>\n      <td>7.113447</td>\n      <td>-0.480116</td>\n      <td>2.630499</td>\n      <td>14.138277</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>41.227957</td>\n      <td>0.155741</td>\n      <td>35.129329</td>\n      <td>0.168719</td>\n      <td>256.643604</td>\n      <td>0.062422</td>\n      <td>0.080032</td>\n      <td>-3.534818</td>\n      <td>2.221590</td>\n      <td>-0.670916</td>\n      <td>90.657058</td>\n      <td>0.105027</td>\n      <td>0.022741</td>\n      <td>-6.631300</td>\n      <td>84.533314</td>\n      <td>0.108764</td>\n      <td>176.560314</td>\n      <td>0.075258</td>\n      <td>2.286446</td>\n      <td>-0.661332</td>\n      <td>15.418973</td>\n      <td>0.254667</td>\n      <td>90.308910</td>\n      <td>0.105229</td>\n      <td>323.906407</td>\n      <td>0.055564</td>\n      <td>77.623148</td>\n      <td>-0.113502</td>\n      <td>-65.4863</td>\n      <td>6.771155</td>\n      <td>41.1037</td>\n      <td>6.94735</td>\n      <td>11.287122</td>\n      <td>127.399113</td>\n      <td>9.884620</td>\n      <td>7.89575</td>\n      <td>8.676483</td>\n      <td>-1.463426</td>\n      <td>9.787399</td>\n      <td>64.143395</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"augment_df(test_df)\ntest_df.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"  ID_code    var_0    var_1    ...          skew      kurt    sq_kurt\n0  test_0  11.0656   7.7798    ...     -0.088518  1.871262  15.607325\n1  test_1   8.5304   1.2543    ...     -0.559785  3.391068  20.159428\n2  test_2   5.4827 -10.3581    ...     -0.135084  2.326901  12.575372\n3  test_3   8.5374  -1.3222    ...     -0.167741  2.253054  30.958931\n4  test_4  11.7058  -0.1327    ...      0.293484  2.044943  24.871054\n\n[5 rows x 613 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>...</th>\n      <th>sq_var_186</th>\n      <th>repo_var_186</th>\n      <th>sq_var_187</th>\n      <th>repo_var_187</th>\n      <th>sq_var_188</th>\n      <th>repo_var_188</th>\n      <th>sq_var_189</th>\n      <th>repo_var_189</th>\n      <th>sq_var_190</th>\n      <th>repo_var_190</th>\n      <th>sq_var_191</th>\n      <th>repo_var_191</th>\n      <th>sq_var_192</th>\n      <th>repo_var_192</th>\n      <th>sq_var_193</th>\n      <th>repo_var_193</th>\n      <th>sq_var_194</th>\n      <th>repo_var_194</th>\n      <th>sq_var_195</th>\n      <th>repo_var_195</th>\n      <th>sq_var_196</th>\n      <th>repo_var_196</th>\n      <th>sq_var_197</th>\n      <th>repo_var_197</th>\n      <th>sq_var_198</th>\n      <th>repo_var_198</th>\n      <th>sq_var_199</th>\n      <th>repo_var_199</th>\n      <th>min</th>\n      <th>mean</th>\n      <th>max</th>\n      <th>median</th>\n      <th>std</th>\n      <th>var</th>\n      <th>abs_mean</th>\n      <th>abs_median</th>\n      <th>abs_std</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sq_kurt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>11.0656</td>\n      <td>7.7798</td>\n      <td>12.9536</td>\n      <td>9.4292</td>\n      <td>11.4327</td>\n      <td>-2.3805</td>\n      <td>5.8493</td>\n      <td>18.2675</td>\n      <td>2.1337</td>\n      <td>8.8100</td>\n      <td>-2.0248</td>\n      <td>-4.3554</td>\n      <td>13.9696</td>\n      <td>0.3458</td>\n      <td>7.5408</td>\n      <td>14.5001</td>\n      <td>7.7028</td>\n      <td>-19.0919</td>\n      <td>15.5806</td>\n      <td>16.1763</td>\n      <td>3.7088</td>\n      <td>18.8064</td>\n      <td>1.5899</td>\n      <td>3.0654</td>\n      <td>6.4509</td>\n      <td>14.1192</td>\n      <td>-9.4902</td>\n      <td>-2.1917</td>\n      <td>5.7107</td>\n      <td>3.7864</td>\n      <td>-1.7981</td>\n      <td>9.2645</td>\n      <td>2.0657</td>\n      <td>12.7753</td>\n      <td>11.3334</td>\n      <td>8.1462</td>\n      <td>-0.0610</td>\n      <td>3.5331</td>\n      <td>9.7804</td>\n      <td>...</td>\n      <td>86.255799</td>\n      <td>0.107673</td>\n      <td>555.568470</td>\n      <td>-0.042426</td>\n      <td>175.941654</td>\n      <td>0.075390</td>\n      <td>2.752613</td>\n      <td>0.602736</td>\n      <td>4.646611</td>\n      <td>-0.463908</td>\n      <td>140.410650</td>\n      <td>0.084392</td>\n      <td>2.044900</td>\n      <td>-0.699301</td>\n      <td>6.006421</td>\n      <td>0.408030</td>\n      <td>187.997005</td>\n      <td>0.072933</td>\n      <td>6.085596</td>\n      <td>0.405367</td>\n      <td>19.056717</td>\n      <td>0.229074</td>\n      <td>114.918400</td>\n      <td>0.093284</td>\n      <td>239.388973</td>\n      <td>0.064632</td>\n      <td>76.033168</td>\n      <td>-0.114683</td>\n      <td>-31.9891</td>\n      <td>7.083202</td>\n      <td>42.0248</td>\n      <td>7.31440</td>\n      <td>9.910632</td>\n      <td>98.220627</td>\n      <td>9.576049</td>\n      <td>8.47350</td>\n      <td>7.515516</td>\n      <td>-0.088518</td>\n      <td>1.871262</td>\n      <td>15.607325</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>8.5304</td>\n      <td>1.2543</td>\n      <td>11.3047</td>\n      <td>5.1858</td>\n      <td>9.1974</td>\n      <td>-4.0117</td>\n      <td>6.0196</td>\n      <td>18.6316</td>\n      <td>-4.4131</td>\n      <td>5.9739</td>\n      <td>-1.3809</td>\n      <td>-0.3310</td>\n      <td>14.1129</td>\n      <td>2.5667</td>\n      <td>5.4988</td>\n      <td>14.1853</td>\n      <td>7.0196</td>\n      <td>4.6564</td>\n      <td>29.1609</td>\n      <td>0.0910</td>\n      <td>12.1469</td>\n      <td>3.1389</td>\n      <td>5.2578</td>\n      <td>2.4228</td>\n      <td>16.2064</td>\n      <td>13.5023</td>\n      <td>-5.2341</td>\n      <td>-3.6648</td>\n      <td>5.7080</td>\n      <td>2.9965</td>\n      <td>-10.4720</td>\n      <td>11.4938</td>\n      <td>-0.9660</td>\n      <td>15.3445</td>\n      <td>10.6361</td>\n      <td>0.8966</td>\n      <td>6.7428</td>\n      <td>2.3421</td>\n      <td>12.8678</td>\n      <td>...</td>\n      <td>139.861372</td>\n      <td>0.084557</td>\n      <td>75.885005</td>\n      <td>-0.114795</td>\n      <td>253.039012</td>\n      <td>0.062865</td>\n      <td>0.962753</td>\n      <td>1.019160</td>\n      <td>112.710072</td>\n      <td>0.094193</td>\n      <td>78.055458</td>\n      <td>0.113187</td>\n      <td>0.884164</td>\n      <td>1.063490</td>\n      <td>102.580435</td>\n      <td>0.098734</td>\n      <td>242.627352</td>\n      <td>0.064199</td>\n      <td>0.227815</td>\n      <td>2.095118</td>\n      <td>2.205819</td>\n      <td>-0.673310</td>\n      <td>97.444538</td>\n      <td>0.101303</td>\n      <td>365.930118</td>\n      <td>0.052276</td>\n      <td>439.992576</td>\n      <td>-0.047674</td>\n      <td>-41.1924</td>\n      <td>6.248430</td>\n      <td>35.6020</td>\n      <td>6.43960</td>\n      <td>9.541267</td>\n      <td>91.035772</td>\n      <td>8.960003</td>\n      <td>7.94920</td>\n      <td>7.042000</td>\n      <td>-0.559785</td>\n      <td>3.391068</td>\n      <td>20.159428</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>5.4827</td>\n      <td>-10.3581</td>\n      <td>10.1407</td>\n      <td>7.0479</td>\n      <td>10.2628</td>\n      <td>9.8052</td>\n      <td>4.8950</td>\n      <td>20.2537</td>\n      <td>1.5233</td>\n      <td>8.3442</td>\n      <td>-4.7057</td>\n      <td>-3.0422</td>\n      <td>13.6751</td>\n      <td>3.8183</td>\n      <td>10.8535</td>\n      <td>14.2126</td>\n      <td>9.8837</td>\n      <td>2.6541</td>\n      <td>21.2181</td>\n      <td>20.8163</td>\n      <td>12.4666</td>\n      <td>12.3696</td>\n      <td>4.7473</td>\n      <td>2.7936</td>\n      <td>5.2189</td>\n      <td>13.5670</td>\n      <td>-15.4246</td>\n      <td>-0.1655</td>\n      <td>7.2633</td>\n      <td>3.4310</td>\n      <td>-9.1508</td>\n      <td>9.7320</td>\n      <td>3.1062</td>\n      <td>22.3076</td>\n      <td>11.9593</td>\n      <td>9.9255</td>\n      <td>4.0702</td>\n      <td>4.9934</td>\n      <td>8.0667</td>\n      <td>...</td>\n      <td>24.184757</td>\n      <td>0.203343</td>\n      <td>4.293184</td>\n      <td>-0.482625</td>\n      <td>133.148521</td>\n      <td>0.086663</td>\n      <td>1.397360</td>\n      <td>0.845952</td>\n      <td>0.560103</td>\n      <td>-1.336184</td>\n      <td>120.857042</td>\n      <td>0.090963</td>\n      <td>3.921588</td>\n      <td>0.504974</td>\n      <td>4.752400</td>\n      <td>0.458716</td>\n      <td>168.514150</td>\n      <td>0.077034</td>\n      <td>4.528810</td>\n      <td>0.469903</td>\n      <td>50.532194</td>\n      <td>-0.140675</td>\n      <td>49.869019</td>\n      <td>0.141607</td>\n      <td>395.834899</td>\n      <td>0.050262</td>\n      <td>537.284584</td>\n      <td>-0.043142</td>\n      <td>-34.3488</td>\n      <td>7.151299</td>\n      <td>39.3654</td>\n      <td>7.26355</td>\n      <td>9.967466</td>\n      <td>99.350374</td>\n      <td>9.554517</td>\n      <td>8.27835</td>\n      <td>7.681205</td>\n      <td>-0.135084</td>\n      <td>2.326901</td>\n      <td>12.575372</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>8.5374</td>\n      <td>-1.3222</td>\n      <td>12.0220</td>\n      <td>6.5749</td>\n      <td>8.8458</td>\n      <td>3.1744</td>\n      <td>4.9397</td>\n      <td>20.5660</td>\n      <td>3.3755</td>\n      <td>7.4578</td>\n      <td>0.0095</td>\n      <td>-5.0659</td>\n      <td>14.0526</td>\n      <td>13.5010</td>\n      <td>8.7660</td>\n      <td>14.7352</td>\n      <td>10.0383</td>\n      <td>-15.3508</td>\n      <td>2.1273</td>\n      <td>21.4797</td>\n      <td>14.5372</td>\n      <td>12.5527</td>\n      <td>2.9707</td>\n      <td>4.2398</td>\n      <td>13.7796</td>\n      <td>14.1408</td>\n      <td>1.0061</td>\n      <td>-1.3479</td>\n      <td>5.2570</td>\n      <td>6.5911</td>\n      <td>6.2161</td>\n      <td>9.5540</td>\n      <td>2.3628</td>\n      <td>10.2124</td>\n      <td>10.8047</td>\n      <td>-2.5588</td>\n      <td>6.0720</td>\n      <td>3.2613</td>\n      <td>16.5632</td>\n      <td>...</td>\n      <td>10.957424</td>\n      <td>0.302097</td>\n      <td>391.323568</td>\n      <td>-0.050551</td>\n      <td>180.899810</td>\n      <td>0.074350</td>\n      <td>1.717148</td>\n      <td>0.763126</td>\n      <td>91.588728</td>\n      <td>0.104491</td>\n      <td>82.384668</td>\n      <td>0.110173</td>\n      <td>2.748964</td>\n      <td>0.603136</td>\n      <td>12.825710</td>\n      <td>0.279228</td>\n      <td>230.657119</td>\n      <td>0.065844</td>\n      <td>10.021023</td>\n      <td>0.315896</td>\n      <td>15.655475</td>\n      <td>0.252736</td>\n      <td>85.183670</td>\n      <td>0.108348</td>\n      <td>169.437082</td>\n      <td>0.076824</td>\n      <td>17.730837</td>\n      <td>-0.237485</td>\n      <td>-21.4797</td>\n      <td>7.057223</td>\n      <td>40.3383</td>\n      <td>6.89675</td>\n      <td>8.257204</td>\n      <td>68.181422</td>\n      <td>8.826497</td>\n      <td>7.91085</td>\n      <td>6.319616</td>\n      <td>-0.167741</td>\n      <td>2.253054</td>\n      <td>30.958931</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>11.7058</td>\n      <td>-0.1327</td>\n      <td>14.1295</td>\n      <td>7.7506</td>\n      <td>9.1035</td>\n      <td>-8.5848</td>\n      <td>6.8595</td>\n      <td>10.6048</td>\n      <td>2.9890</td>\n      <td>7.1437</td>\n      <td>5.1025</td>\n      <td>-3.2827</td>\n      <td>14.1013</td>\n      <td>8.9672</td>\n      <td>4.7276</td>\n      <td>14.5811</td>\n      <td>11.8615</td>\n      <td>3.1480</td>\n      <td>18.0126</td>\n      <td>13.8006</td>\n      <td>1.6026</td>\n      <td>16.3059</td>\n      <td>6.7954</td>\n      <td>3.6015</td>\n      <td>13.6569</td>\n      <td>13.8807</td>\n      <td>8.6228</td>\n      <td>-2.2654</td>\n      <td>5.2255</td>\n      <td>7.0165</td>\n      <td>-15.6961</td>\n      <td>10.6239</td>\n      <td>-4.7674</td>\n      <td>17.5447</td>\n      <td>11.8668</td>\n      <td>3.0154</td>\n      <td>4.2546</td>\n      <td>6.7601</td>\n      <td>5.9613</td>\n      <td>...</td>\n      <td>69.948132</td>\n      <td>0.119567</td>\n      <td>616.300485</td>\n      <td>-0.040281</td>\n      <td>132.084452</td>\n      <td>0.087011</td>\n      <td>2.663750</td>\n      <td>0.612708</td>\n      <td>17.858231</td>\n      <td>0.236636</td>\n      <td>84.131087</td>\n      <td>0.109024</td>\n      <td>1.647372</td>\n      <td>0.779120</td>\n      <td>11.409533</td>\n      <td>0.296051</td>\n      <td>382.366738</td>\n      <td>0.051140</td>\n      <td>0.081796</td>\n      <td>-3.496503</td>\n      <td>26.637985</td>\n      <td>-0.193753</td>\n      <td>53.117859</td>\n      <td>0.137208</td>\n      <td>193.933476</td>\n      <td>0.071808</td>\n      <td>84.356877</td>\n      <td>-0.108878</td>\n      <td>-24.8254</td>\n      <td>7.118682</td>\n      <td>45.5510</td>\n      <td>6.83375</td>\n      <td>10.043542</td>\n      <td>100.872731</td>\n      <td>9.634084</td>\n      <td>8.60380</td>\n      <td>7.649904</td>\n      <td>0.293484</td>\n      <td>2.044943</td>\n      <td>24.871054</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features + [f'sq_{feature}' for feature in features] + [f'repo_{feature}' for feature in features]\nnum_features = len(features)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(2)\nvalid_idx = random.sample(list(df.index.values), int(len(df)*0.05))\ntrain_idx = df.drop(valid_idx).index","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = df.iloc[train_idx].describe()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class roc(Callback):\n    def on_epoch_begin(self, **kwargs):\n        self.total = 0\n        self.batch_count = 0\n    \n    def on_batch_end(self, last_output, last_target, **kwargs):\n        preds = F.softmax(last_output, dim=1)\n        try: \n            roc_score = roc_auc_score(to_np(last_target), to_np(preds[:, -1]))\n            self.total+=roc_score\n            self.batch_count+=1\n        except:\n            pass\n    def on_epoch_end(self, num_batch, **kwargs):\n        self.metric = self.total/self.batch_count","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note** In a previous version, people were running out of memory in the kernel because for 10 epochs, each time we were creating a new data object and learn object which is quite memory intensive. So, I deleted them at last in the method and saved memory. You can use this technique at places which take quite a lot amount of memory. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_eval_tabular_learner(train_df, train_features, valid_idx, add_noise=False, lr=0.02, epochs=1, layers=[200, 100], ps=[0.5, 0.2], name='learner'):\n    data = TabularDataBunch.from_df(path='.', df=train_df, dep_var='target', valid_idx=valid_idx,\n                                    cat_names=[], cont_names=train_features, bs=bs, procs=[FillMissing, Normalize], test_df=test_df)\n    learn = tabular_learner(data, layers=layers, ps=ps, metrics=[roc()])\n    if add_noise:\n        learn.data = None\n        data = None\n        noise = np.random.normal(summary[features].loc['mean'].values, summary[features].loc['std'].values, (len(train_df), num_features))/100\n        train_df[features]+=noise\n        data = TabularDataBunch.from_df(path='.', df=train_df, dep_var='target', valid_idx=valid_idx,\n                                    cat_names=[], cont_names=train_features, bs=bs, procs=[FillMissing, Normalize], test_df=test_df)\n        learn.data = data\n        learn.fit_one_cycle(epochs, lr)\n        train_df[features]-=noise\n        noise = None\n    learn.fit_one_cycle(epochs, lr)\n    learn.save(name, with_opt=False)\n    valid_preds, _ = learn.get_preds(ds_type=DatasetType.Valid)\n    valid_probs = np.array(valid_preds[:, -1])\n    valid_targets = train_df.loc[valid_idx].target.values\n    valid_score = roc_auc_score(valid_targets, valid_probs)\n    test_preds, _ = learn.get_preds(ds_type=DatasetType.Test)\n    test_probs = to_np(test_preds[:, -1])\n    del(data)\n    del(learn)\n    gc.collect()\n    return valid_score, valid_probs, test_probs","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_features = []\nvalid_scores = []\nvalid_preds = []\npreds = []\nnum_epochs = 10\ncv_counts = len(df)//num_epochs\nsaved_model_prefix = 'learner'","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_features = ['min', 'mean', 'max', 'median', 'std', 'abs_mean', 'abs_median', 'abs_std', 'skew', 'kurt', 'sq_kurt']","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bs = 2048","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":68,"outputs":[{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"979"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(num_epochs):\n    print('Training model: ', i)\n    sub_features.append(random.sample(list(features), int(num_features*0.75)) + augmented_features)\n    name = f'{saved_model_prefix}_{i}'\n    score, valid_probs, test_probs = train_and_eval_tabular_learner(df, sub_features[-1], valid_idx,\n                                                                    add_noise=True, epochs=3, lr=0.02, name=name)\n    valid_scores.append(score)\n    valid_preds.append(valid_probs)\n    preds.append(test_probs)","execution_count":null,"outputs":[{"output_type":"stream","text":"Training model:  4\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\noof = np.zeros(len(df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(skf.split(df.values, target.values)):\n    print(\"fold nÂ°{}\".format(fold_))\n    trn_data = lgb.Dataset(df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) / 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1 = np.array(sum(preds)/num_epochs)\npreds1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape, preds1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_ensemble_values = [0., 0.125, 0.25, 0.375, 0.5, 0.675, 0.75, 0.875, 1.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_ensemble_array = np.array(all_ensemble_values[0]*preds1 + (1-all_ensemble_values[0])*predictions)\nsample_ensemble_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(all_ensemble_values)):\n    predict_array = np.array(all_ensemble_values[i]*preds1 + (1-all_ensemble_values[i])*predictions)\n    sub_df = pd.DataFrame({'ID_code': test_df['ID_code'].values})\n    sub_df['target'] = predict_array\n    sub_df.to_csv(f'submission_{i}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References:\n\nhttps://www.kaggle.com/chocozzz/santander-lightgbm-baseline-lb-0-899\n\nhttps://www.kaggle.com/quanghm/fastai-1-0-tabular-learner-with-ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}