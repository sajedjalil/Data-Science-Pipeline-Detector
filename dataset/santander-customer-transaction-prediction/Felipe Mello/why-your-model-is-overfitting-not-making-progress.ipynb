{"cells":[{"metadata":{},"cell_type":"markdown","source":"The objective of this Kernel is to show why your new engineered features might be causing overfitting or not having any impact in your score at all.\n\nIf you like this kernel, please, consider upvoting. Thanks :)"},{"metadata":{},"cell_type":"markdown","source":"### **1. PROPOSED FEATURE ENGINEERING EXPERIMENT**\n\nHere we are going to engineer a feature based on the difference between the Probability Density Functions (PDF) of clients with target = 1 and target = 0 in the train dataset.\n\nThis experiment will give us a 0.910 local CV, without any tunning, but 0.901 when you submit it.\n\n#### **1.1. QUICK REVIEW: PROBABILITY DENSITY FUNCTION**\n\n`The PDF is used to specify the probability of a random variable falling within a particular range of values, as opposed to taking an specific value. (Wikipedia)`\n\nTo understand it better, check out [wikipedia](https://en.wikipedia.org/wiki/Probability_density_function)\n\n### **2. CALCULATING THE PDF FOR THE SANTANDER'S DATASET**\n\n#### **2.1. COMPETITION SUMMARY AND DATASET PDF**\n\nOur dataset is composed of 200 features and a target column specifying if a client made a specific type of transaction. Our objective in this competition is, based on the train dataset, predict in the test dataset who will also make a transfer. Let's take a look at the PDF of the features 81 and 139 for values of target = 1 and target = 0. <br>\n*Note: This variable choice was arbitrary, all variables are gonna behave very similarly.*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#import libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#import datasts\ntrain_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\ntest_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\nfull = pd.concat([train_df, test_df], sort = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feat1, feat2 = 'var_81', 'var_139'\n\nfig = plt.subplots(figsize=(15, 5))\n\n#plot pdf feat 1\nplt.subplot(1, 2, 1)\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=True, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=True, color=\"r\", label = 'target = 1')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=True, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=True, color=\"r\", label = 'target = 1')\nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, there is a great motivation to calculate the PDF difference between target = 0 and target = 1 distributions. They are clearly different, so it would make sense to say that, if pdf(target = 1) - pdf(target = 0) > 0, then there is a high probability of the client making a transfer.\n\n#### **2.2. CREATING OUR PDF FUNCTION**\n\nHere is the strategy used in this kernel:\n- Calculate the PDF for each feature;\n- Aggregate each feature values within bins;\n- Use the PDF's difference between target = 0 and target = 1 for each bin as a new feature;\n\nNotice that we could create this feature differently, by, for example, getting the target probability.\n\nBelow we will see the difference between the previously seen PDF, where the graph is very smooth, and the plot of the PDF for each bin we created."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.neighbors import KernelDensity\nfrom operator import itemgetter\n\nfeat = 'var_81'\n\ndef calculate_pdf_difference(feat, df_feature, df_target, IQR_multiplier, bin_bandwidth_multiplier, print_number_bins):\n    #Agreggating feature values in bin format using the Freedman-Diaconis rule\n    IQR = df_feature[feat].quantile([0.75]).values - df_feature[feat].quantile([0.25]).values #Interquartile range (IQR)\n    n = len(df_feature[feat])\n    bin_size = IQR_multiplier*IQR/n**(1/3)\n    bin_number = int(np.round((df_feature[feat].max() - df_feature[feat].min())/bin_size))\n    binvalues = pd.cut(df_feature[feat], bins = bin_number, labels = range(bin_number)).astype('float')\n    \n    if print_number_bins:\n        print('There are {} bins in the feature {}'.format(bin_number, feat))\n\n    #Calculate the PDFs using the df_target\n    pdf_0 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n    pdf_0.fit(np.array(df_target[feat][df_target['target'] == 0]).reshape(-1,1))\n\n    pdf_1 = KernelDensity(kernel='gaussian', bandwidth=bin_size*bin_bandwidth_multiplier)\n    pdf_1.fit(np.array(df_target[feat][df_target['target'] == 1]).reshape(-1,1))\n\n    #Creates an X array with the average feature value for each bin\n    x = np.array(np.arange(min(df_feature[feat]) + bin_size/2 ,max(df_feature[feat]), bin_size)).reshape(-1,1)\n\n    #gets the pdf values based on the X array\n    log_pdf_0 = np.exp(pdf_0.score_samples(x))\n    log_pdf_1 = np.exp(pdf_1.score_samples(x))\n\n    #creates a dictionary that links the bin number with the PDFs value difference\n    pdf_dict = dict()\n    for i in range(bin_number):\n        pdf_dict[i] = log_pdf_1[i] - log_pdf_0[i] \n\n    #gets the PDF difference for each row of the dataset based on its equivalent bin.\n    bin_pdf_values = np.array(itemgetter(*list(binvalues))(pdf_dict))\n\n    return bin_pdf_values, x, log_pdf_0, log_pdf_1\n\nfeat1, feat2 = 'var_81', 'var_139'\n\nfig = plt.subplots(figsize=(15, 5))\n\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = 2, bin_bandwidth_multiplier = 1.5, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3. UNDERSTANDING OVERFITTING**\n\nThe difference of smoothness on the graph above is caused by the IQR_multiplier and the bin_bandwidth_multiplier parameters.\n\n**IQR_multiplier**<br>\nIQR stands for Inter-Quartile Range and is used to define the number of bins for our distribution. This parameter is proportional to the bin size. A bigger IQR_multiplier will yield a bigger bin size and, therefore, less bins per distribution.\n\n**bin_bandwidth_multiplier**<br>\nThe bandwidth value is used to smooth the graph\n\nLet's see how the PDFs behaves as we play with those parameters"},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"feat1, feat2 = 'var_81', 'var_139'\n\n\n\nprint('-----------------------------------------------')\nIQR_multiplier = 0.5\nbin_bandwidth_multiplier = 0.2\nprint('IQR_multiplier', IQR_multiplier)\nprint('bin_bandwidth_multiplier', bin_bandwidth_multiplier)\nprint('-----------------------------------------------')\nfig = plt.subplots(figsize=(15, 5))\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()\n\nprint('-----------------------------------------------')\nIQR_multiplier = 5\nbin_bandwidth_multiplier = 0.2\nprint('IQR_multiplier', IQR_multiplier)\nprint('bin_bandwidth_multiplier', bin_bandwidth_multiplier)\nprint('-----------------------------------------------')\nfig = plt.subplots(figsize=(15, 5))\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()\n\nprint('-----------------------------------------------')\nIQR_multiplier = 3\nbin_bandwidth_multiplier = 0.2\nprint('IQR_multiplier', IQR_multiplier)\nprint('bin_bandwidth_multiplier', bin_bandwidth_multiplier)\nprint('-----------------------------------------------')\n\nfig = plt.subplots(figsize=(15, 5))\n\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()\n\nprint('-----------------------------------------------')\nIQR_multiplier = 3\nbin_bandwidth_multiplier = 3\nprint('IQR_multiplier', IQR_multiplier)\nprint('bin_bandwidth_multiplier', bin_bandwidth_multiplier)\nprint('-----------------------------------------------')\n\nfig = plt.subplots(figsize=(15, 5))\n\n\n#plot pdf feat 1\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 1)\n\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'target = 1')\nplt.plot(x, log_pdf_0)\nplt.plot(x, log_pdf_1) \nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When you describe your data with a high number of bins and a low smoothing parameter, you get really \"noisy\" PDF, because your pool of candidates for each bin is too small and specific. This is the opposite of what we want for a robust solution.\n\nWhen we use this model to predict the target on the test dataset, it will overfit, not because the datasets are way too different, but because you didn't give the model any chance to adapt for small changes.\n\nTo prove that to you, let's check the test dataset distribution\n\n#### **3.1. THE TEST DATASET**\n\nHere we are going to plot the target = 1 and target = 0 for the train and test dataset in the same graph.\n\nTo do that, we will get the submission with score 0.901 shared [here](https://www.kaggle.com/darbin/clustering-blender-of-0-901-solutions) and say that the 20098 highest probabilities (same number of target = 1 in the train_df) are equivalent to target = 1 in the test_df. Everything else will be set as target = 0."},{"metadata":{"trusted":false},"cell_type":"code","source":"#defines the test_df target columns based on a submission of score 0.901\npd.options.mode.chained_assignment = None  # disables copy warning from pandas library. default='warn'\nsubmission = pd.read_csv('../input/submission-0901/submission_0901.csv')\ntest_df['target'] = submission['target']\ntest_df.sort_values('target', ascending = False, inplace = True)\ntest_df['target'].iloc[:20098] = 1\ntest_df['target'].iloc[20098:] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, let's plot the smoothed PDFs to see if there is any significant difference"},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"feat1, feat2 = 'var_81', 'var_139'\n\nfig = plt.subplots(figsize=(15, 5))\n\n#plot pdf feat 1\nplt.subplot(1, 2, 1)\nsns.kdeplot(train_df[feat1][train_df['target'] == 0], shade=False, color=\"b\", label = 'train target = 0')\nsns.kdeplot(test_df[feat1][test_df['target'] == 0], shade=False, color=\"g\", label = 'test target = 0')\nsns.kdeplot(train_df[feat1][train_df['target'] == 1], shade=False, color=\"r\", label = 'train target = 1')\nsns.kdeplot(test_df[feat1][test_df['target'] == 1], shade=False, color=\"k\", label = 'test target = 1')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nplt.subplot(1, 2, 2)\nsns.kdeplot(train_df[feat2][train_df['target'] == 0], shade=False, color=\"b\", label = 'train target = 0')\nsns.kdeplot(test_df[feat2][test_df['target'] == 0], shade=False, color=\"g\", label = 'test target = 0')\nsns.kdeplot(train_df[feat2][train_df['target'] == 1], shade=False, color=\"r\", label = 'train target = 1')\nsns.kdeplot(test_df[feat2][test_df['target'] == 1], shade=False, color=\"k\", label = 'test target = 1')\nplt.title(feat2)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a small, but still significant difference in the distribution of target = 1. Now, let's see how big is this difference when we don't smooth the PDF."},{"metadata":{"trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"print('-----------------------------------------------')\nIQR_multiplier = 3\nbin_bandwidth_multiplier = 0.2\nprint('IQR_multiplier', IQR_multiplier)\nprint('bin_bandwidth_multiplier', bin_bandwidth_multiplier)\nprint('-----------------------------------------------')\n\nfig = plt.subplots(figsize=(15, 5))\n\n\n#plot pdf feat 1\n\n\nplt.subplot(1, 2, 1)\n\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\nplt.plot(x, log_pdf_0, 'b', label = 'train target = 0')\nplt.plot(x, log_pdf_1, 'r', label = 'train target = 1')\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat1, df_feature = full, df_target = test_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = False)\nplt.plot(x, log_pdf_0, 'g', label = 'test target = 0')\nplt.plot(x, log_pdf_1, 'k', label = 'test target = 1')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\n#plot pdf feat 2\nplt.subplot(1, 2, 2)\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = train_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = True)\nplt.plot(x, log_pdf_0, 'b', label = 'train target = 0')\nplt.plot(x, log_pdf_1, 'r', label = 'train target = 1')\nbin_pdf_values, x, log_pdf_0, log_pdf_1 = calculate_pdf_difference(feat = feat2, df_feature = full, df_target = test_df, IQR_multiplier = IQR_multiplier, bin_bandwidth_multiplier = bin_bandwidth_multiplier, print_number_bins = False)\nplt.plot(x, log_pdf_0, 'g', label = 'test target = 0')\nplt.plot(x, log_pdf_1, 'k', label = 'test target = 1')\nplt.title(feat1)\nplt.xlabel('Feature Values')\nplt.ylabel('Probability')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, every point representing a bin has a different PDF for train_df and test_df. Expecting that their distributions will match is equivalent of what is happening in the image below:\n\nYou can find [here](https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42) the article about over/underfitting from where this image was taken.\n![overfitting](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle---Why-your-model-is-overfitting/master/1_SBUK2QEfCP-zvJmKm14wGQ.png) \n\nI hope that these graphs can make it clear why your model overfits when you are too specific about a single variable, instead of a group of variables.\n\nA good question here would be: What happens if I use this feature with smoothed values? Well, the smoothier it is, the lesser the impact on your score, and here is why."},{"metadata":{},"cell_type":"markdown","source":"### **4. HOW THE ALGORITHM WORKS AND WHY MY FEATURES DON'T POSITIVELY IMPACT IT**\n\nLGB, catboost and XGBoost are tree based algorithms. Each of them has their specificity, but let's try to understand the main concept behind it.\n\n#### **4.1. TREE BASED ALGORITHM**\n\n![Decision trees](https://raw.githubusercontent.com/fmellomascarenhas/Kaggle---Why-your-model-is-overfitting/master/Screenshot%20from%202019-04-05%2018-49-51.png) <center> *source: superdatascience* </center>\n\nThose algorithms work by finding the best places to split your data in each dimension/axis, so you can find groups that better represent a target based on the mean value of that group. Parameter tunning has a great whole here, for example: If you do too many splits, then you will overfit, because you won't be getting the average result of a relevant number of points per split.\n\nTo understand better how the algorithm is making those splits, check out this code shared by Chris Deotte [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87133#502814).\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn import tree\nimport graphviz\n\ncols = [c for c in train_df.columns if (c not in ['ID_code', 'target'])]\n\nvalid = train_df.sample(frac=0.2, random_state=42)\ntrain = train_df[ ~train_df.index.isin(valid.index) ].sample(frac=1)\n\n#model = tree.DecisionTreeClassifier(max_leaf_nodes=4)\nmodel = tree.DecisionTreeRegressor(max_leaf_nodes=4)\nmodel.fit(train[cols], train['target'])\n#pred_val = model.predict_proba(valid[cols])[:,1]\npred_val = model.predict(valid[cols])\n\nprint('AUC = ',round( roc_auc_score(valid['target'],pred_val),4 ) )\ntree_graph = tree.export_graphviz(model, out_file=None, max_depth = 10,\n    impurity = False, feature_names = cols, class_names = ['0', '1'],\n    rounded = True, filled= True )\ngraphviz.Source(tree_graph)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **4.2. CALCULATING SOMETHING THE MODEL ALREADY DOES**\n\nFeatures engineered based on mean values, for example, are worthless when using tree based algorithms. They already calculate it, and probably in a way better than you do, because they compare different ways to split the data so that the result is not biased. The same thing happens for:\n- Scaling the data;\n- Calculating the probability of something based on the region where your variable is;\n- Telling the algorithm where the bumps of frequency are.\n\nThere is a good discussion [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87486#latest-506429) and [here](https://www.youtube.com/watch?v=iRwf7wIfEoI&list=PLpQWTe-45nxL3bhyAJMEs90KF_gZmuqtm&index=9) about things that the models \"can't see\"."},{"metadata":{},"cell_type":"markdown","source":"### **5. SUMMARY**\n\nIn this kernel we saw:\n- How your new engineered features might be overfitting because your are being to specific about the target value; \n- How they might not have any impact because you are telling the model something it already knows;\n\nGood luck on your feature engineering journey and, please, if you found this kernel interesting, consider upvoting, since it took effort and time to make it. Thank you :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}