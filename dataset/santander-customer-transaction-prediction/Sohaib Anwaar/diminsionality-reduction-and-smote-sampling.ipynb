{"cells":[{"metadata":{"_uuid":"3dd2f020f64be1da81e5e36ba4ae873c9fb4279e"},"cell_type":"markdown","source":"# Introduction\n    * Understanding Data    \n    * Outlier Analysis\n    \n    * Smote Sampling    \n        * OverSampling with Smote\n        * UnderSampling with Smote    \n        \n    * Diminsionality Reduction with Decesion Trees\n    \n    * Machine Learning\n        * Decesion Trees\n        * Neural Networks   \n        * Checking Other 20 Classifiers at the same time\n        * Ensembling Techniques\n            * Bagging\n            * Boasting\n            * Stacking\n            * Votting            \n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f17dd34c6cd6535dc425adf5248e5a4f5b605416"},"cell_type":"markdown","source":"> # **UnderStanding Data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/train.csv\")\ndf.drop(\"ID_code\",axis=1,inplace=True)\nprint(df.head(5))\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a9a049cf4cb31987a274889264f52016b479346"},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(df.corr())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a78df76a723c4ee655b3d083ee37706f8607e60"},"cell_type":"markdown","source":"# **Missing Values**"},{"metadata":{"trusted":true,"_uuid":"ca3e1ae21c66c987f1a52f437c17cc6777011d83","_kg_hide-output":true},"cell_type":"code","source":"print(\"Total df Values\",str(len(df)))\nfor i in df.columns:\n    print(i,\"\\t\\t\",str(df[i].isnull().sum()))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cf1d266e6c0b9295bbe551243beb056e21494a3"},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntarget_0=len(df[df[\"target\"]==0])\ntarget_1=len(df[df[\"target\"]==1])\n# Data to plot\nlabels = df[\"target\"].unique()\nsizes = (target_0,target_1)\n\n \n# Plot\nplt.pie(sizes, labels=labels, \nautopct='%1.1f%%', shadow=True, startangle=140)\n \nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a047f4e30b1d181927ef1c021f5079f63e27d1a2"},"cell_type":"markdown","source":"We have 90 percent 0 label values and 10 percent 1 label values "},{"metadata":{"_uuid":"05db0d3bf37599c314e49ce3252bf25ee2e99bd0"},"cell_type":"markdown","source":"# **Outliers Analysis**"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6d33b9a267b1814bc19a755f71f748d09cc6a1f3"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfeature_names = df.columns\nfor i in range(len(feature_names)-1):\n    figure = plt.figure()\n    ax = sns.boxplot(x='target', y=feature_names[i], data=df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87f211b5deeeb5f82b1605579ecc72313961f31"},"cell_type":"markdown","source":"**Detecting Outlier **"},{"metadata":{"trusted":true,"_uuid":"57d4731062eaf7eeaa64cfd0e1367e8d313cf401"},"cell_type":"code","source":"# The diamond shaped dots outside the boxplot indicates the outliers \n# There are some extreme cases in K, Ba and Fe. \n# That why it is necessary to rule out the outliers \n\ndf_old = df.copy(deep=True) # Make a copy of original data, just in case\n\n# Create new dataframe for each type\n\ntypes = df['target'].unique()\n# seperating Dataframe by class Labels\nd = {type: df[df['target'] == type] for type in types}\n\nlow = .25\nhigh = .75\n\nbounds = {}\nfor type in types:\n    filt_df = d[type].loc[:, d[type].columns != 'target'] # Remove 'Type' Column\n    quant_df = filt_df.quantile([low, high])\n    IQR = quant_df.iloc[1,:]-  quant_df.iloc[0,:]\n    quant_df.iloc[0,:] = quant_df.iloc[0,:] - 1.5*IQR\n    quant_df.iloc[1,:] = quant_df.iloc[1,:] + 1.5*IQR\n    bounds[type] = quant_df\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5462d9bf840b0afe5fe4a8d44a20f8cc55c89f70"},"cell_type":"code","source":"df_new = {}\n\nfor type in types:\n    filt_df = d[type].loc[:, d[type].columns != 'target'] # Remove 'Type' Column\n    filt_df = filt_df.apply(lambda x: x[(x>bounds[type].loc[low,x.name]) & (x < bounds[type].loc[high,x.name])], axis=0)\n    df_new[type] = pd.concat([filt_df,d[type].loc[:,'target']], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f4d97ee5c7d4f6261468646b7f18d6b49ae757"},"cell_type":"code","source":"df_OR=pd.concat(df_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a2394eabc9942e58b613be6feafa9d17fe3679a","_kg_hide-output":true},"cell_type":"code","source":"for i in range(len(feature_names)-1):\n    figure = plt.figure()\n    ax = sns.boxplot(x='target', y=feature_names[i], data=df_OR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21a3251552c66e14b2f0c3ca78c3da1475aa5289"},"cell_type":"code","source":"len(df_old)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06dd2c1c5cac79a4240ed9b0a98874cc4a8bb591"},"cell_type":"markdown","source":"**Outliers Removed**"},{"metadata":{"_uuid":"f0e6ed61d60e1c364e5a971b029eab6cd3b112b1"},"cell_type":"markdown","source":" # **Over Sampling**\n\n[Smote oversampling](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/smote)"},{"metadata":{"trusted":true,"_uuid":"0851f8b3f8c29068ff0ec65523fd489247a60dfc","_kg_hide-input":true},"cell_type":"code","source":"X=df.drop(\"target\",axis=1)\ny=df[\"target\"]\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cf88de4cbc0ea69c76af5a80132b45808245ab1"},"cell_type":"code","source":"\n\n# Data to plot\nlabels = df[\"target\"].unique()\nsizes = ([sum(y_train_res==0),sum(y_train_res==1)])\n\n \n# Plot\nplt.pie(sizes, labels=labels, \nautopct='%1.1f%%', shadow=True, startangle=140)\n \nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a792b1d2f20abe5eae71312e44852b9d442577c"},"cell_type":"markdown","source":"# Diminsionality reduction\nWE have alot of diminsions here I am going to reduce some diminsions with the help of D-Trees"},{"metadata":{"trusted":true,"_uuid":"ccc773512185afd5ec9c36e04fe139ebdc64d37a"},"cell_type":"code","source":"#you can choose first 10 coloums to see or 100 or all coloums dynaimic code to chekc importance to\n# fetures with decesion trees\ncoloums=[]\ncoloums_selected=200\nfor i in range (0,coloums_selected):\n    coloums.append(i)\nX_train_res=np.asarray(X_train_res)\nX_test=np.asarray(X_test)\nX_sliced_array_train=X_train_res[:20000,coloums]\nX_sliced_array_test=X_test[:,coloums]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb7dbc07b7621f24fec2eb68cf58f8c2c06c0009","_kg_hide-output":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\n# i am taking 20000 exampling and just taking first 10 coloums\nclf.fit(X_sliced_array_train,y_train_res[:20000])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a68595e40670443742af8edf7fc2e5bab367bc4c"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_sliced_array_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"833313f762226b6681f5fc6a304c86c6abc48587","_kg_hide-input":true},"cell_type":"code","source":"from matplotlib.pyplot import figure\n\ncol=X_train.columns\nfeature_names = col[0:coloums_selected]\nimportance_frame = pd.DataFrame()\nimportance_frame['Features'] =  col[0:coloums_selected]\nimportance_frame['Importance'] =clf.feature_importances_\nimportance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)\nfigure(num=None, figsize=(8, 40), dpi=80, facecolor='w', edgecolor='k')\nplt.barh(coloums, importance_frame['Importance'], align='center', alpha=0.5)\nplt.yticks(coloums, importance_frame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91c6c29214618c07889761f27af2a8250333544e","scrolled":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"#now we have our Important Features\nimportance_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8fa1e5164d9c5265efdb350eed2d970f8645490"},"cell_type":"markdown","source":"Selecting coloums having importance greater than 0.01"},{"metadata":{"trusted":true,"_uuid":"f66760a6cac671617cfc8becfd10668df8eb0511"},"cell_type":"code","source":"df_selected_coloums=importance_frame[importance_frame['Importance']>0.01]\nprint(\"We have selected\",str(len(df_selected_coloums)),\"Coloums\")\ndf_selected_coloums","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24571a393aa7cd9437f9c1c699e041431c533d32"},"cell_type":"markdown","source":"# New Dataframe After Reduction"},{"metadata":{"trusted":true,"_uuid":"ad2e9ec1134284a49020af26ef7502cc46c4bb28"},"cell_type":"code","source":"Coloum_Names=[\"target\"]\nfor i in df_selected_coloums.Features:\n    Coloum_Names.append(i)\nNew_df=df.loc[ :, Coloum_Names]    \nNew_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfcf449bfe452a91c7d995242f734b4084a96fac"},"cell_type":"markdown","source":"**Lets see data Distribution**"},{"metadata":{"trusted":true,"_uuid":"34d0ca794305f60317a9f0f3cd682e33dac6e275"},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 25))\nj = 0\n#Droping_Characters and string coloums because graph donot support them\n\nfor i in New_df.columns:\n    plt.subplot(7, 7, j+1)\n    j += 1\n    sns.distplot(New_df[i][New_df['target']==1], color='g', label = 'Positive')\n    sns.distplot(New_df[i][New_df['target']==0], color='r', label = 'Negative')\n    plt.legend(loc='best')\nfig.suptitle('Standard Customer Transaction ')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"face8ed1fc766417a8048453d16acede1fa6ea35"},"cell_type":"markdown","source":"**Decesion Tree on Raw Data**"},{"metadata":{"trusted":true,"_uuid":"51af547f9b75f71451611d9bcc3b1dec8ed75b13","_kg_hide-output":true},"cell_type":"code","source":"X=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\n# i am taking 20000 exampling and just taking first 10 coloums\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b5beb4eb86a6da33de384298d96c70417117ded","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50d1fc1e9e218add98c536c11211abcfd5f9a562","_kg_hide-input":true},"cell_type":"code","source":"coloums=[]\ncoloums_selected=len(X.columns)\nfor i in range(0,coloums_selected):\n    coloums.append(i)\nprint(coloums_selected)\nfrom matplotlib.pyplot import figure\n\ncol=New_df.columns\nfeature_names = col[1:coloums_selected+1]\nimportance_frame = pd.DataFrame()\nimportance_frame['Features'] =  col[1:coloums_selected+1]\nimportance_frame['Importance'] =clf.feature_importances_\nimportance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)\nfigure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.barh(coloums, importance_frame['Importance'], align='center', alpha=0.5)\nplt.yticks(coloums, importance_frame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f141f03319e21be11494a62728285f8c9569dccd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce5e8a9af089aadf7fc30f1e4260a70d97a900a4"},"cell_type":"code","source":"len(clf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dce09c554e40a0b08965d08bc89019639391ccc"},"cell_type":"markdown","source":"# **UnderSampling Data**\n\n[Resource](https://www.kaggle.com/residentmario/undersampling-and-oversampling-imbalanced-data)"},{"metadata":{"trusted":true,"_uuid":"b623e8e6c91e09aaa0f74edfbd811204aa78c3f8","_kg_hide-input":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nX=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\n\nrus = RandomUnderSampler(random_state=0,ratio={0: 10000, 1:11000})\nrus.fit(X_train, y_train)\nX_resampled, y_resampled = rus.fit_sample(X_train, y_train.ravel())\n\nprint('After UnderSampling, the shape of train_X: {}'.format(X_resampled.shape))\nprint('After UnderSampling, the shape of train_y: {} \\n'.format( y_resampled.shape))\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\nprint(\"After UnderSampling, counts of label '0': {}\".format(sum(y_resampled==0)))\n\n\ncolors = ['#05990d' if v == 0 else '#990606' if v == 1 else '#000000' for v in y_resampled]\nplt.scatter(X_resampled[:, 0], X_resampled[:, 1], c=colors, linewidth=0.5, edgecolor='black')\nsns.despine()\nplt.title(\"RandomUnderSampler Output ($n_{class}=4700)$\")\npass\n\n#red is 0 \n#green is 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cdf396c208b0df7135b6988284e6f2f74966fbf"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\nresults=cross_val_score(clf,X_resampled ,y_resampled , cv=10)\nprint(results)\nAverage=sum(results) / len(results) \nprint(\"Average Accuracy :\",Average)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec77149ee0e1050dbbd3cb14be82b2cde357db46","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nclf.fit(X_resampled ,y_resampled)\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffae084a97cfb4c2cb84e7b13b263501ff1e6248"},"cell_type":"markdown","source":"# **OverSampling**"},{"metadata":{"trusted":true,"_uuid":"7ad330fe6db6f61c59c7967c3fd1102f38ae8006","_kg_hide-input":true},"cell_type":"code","source":"X=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb07012c22ff302cf8ecd68919e10e4ab3c03832"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\nresults=cross_val_score(clf,X_train_res, y_train_res  , cv=10)\nprint(results)\nAverage=sum(results) / len(results) \nprint(\"Average Accuracy :\",Average)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8543f4d85d651a0059ed0335af09088a69e9c496","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nclf.fit(X_train_res, y_train_res)\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ebcf09d1113297055f3416f93fdabfa4e40b51e"},"cell_type":"markdown","source":"# **Raw Data **"},{"metadata":{"trusted":true,"_uuid":"07ee146ba1adeba87d750f19928774b11c80f2fc"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\nresults=cross_val_score(clf,X_train, y_train  , cv=10)\nprint(results)\nAverage=sum(results) / len(results) \nprint(\"Average Accuracy :\",Average)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40af2316284b9a48558d2a6154e44401cbee44db","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nclf.fit(X_train, y_train)\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75f597ca6009e9efab7d748ece71bae24ac36995"},"cell_type":"markdown","source":"**In our case Our Raw Data is performing well so we didnt go for Undersampling or Oversampling**\n\n**Lets move forward and see our assumption of getting good scores on raw data is True or not**"},{"metadata":{"_uuid":"23b93dc282070b37ea364aec6947b414e0e876e9"},"cell_type":"markdown","source":"# **Machine Learning**"},{"metadata":{"trusted":true,"_uuid":"6818b18123686c2ec32bb0c7dce699320f13fa60","_kg_hide-input":true},"cell_type":"code","source":"from sklearn import ensemble\nfrom sklearn import gaussian_process\nfrom sklearn import linear_model\nfrom sklearn import naive_bayes\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import discriminant_analysis\nfrom sklearn import model_selection\nfrom xgboost.sklearn import XGBClassifier \nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"_uuid":"d7a3e2d6077864b249569ba5fb2b2e4531384225"},"cell_type":"code","source":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    \n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    \n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    #XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy(Scoring)',\"Accuracy(Confusion Matrix)\",\"true_positive\",\"false_positive\",\"true_negative\",\"false_negative\" ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nrow_index = 0\n#ground Truth\nMLA_compare.loc[row_index, 'MLA Name'] = \"Ground Truth\"\nMLA_compare.loc[row_index, 'MLA Parameters'] = \"NaN\"\nMLA_compare.loc[row_index, 'MLA Test Accuracy'] =0\nMLA_compare.loc[row_index, 'Accuracy(Confusion Matrix)'] =0\nMLA_compare.loc[row_index, 'true_positive'] =sum(y_test==1)\nMLA_compare.loc[row_index, 'true_negative'] =sum(y_test==0)\nMLA_compare.loc[row_index, 'false_positive'] =0\nMLA_compare.loc[row_index, 'false_negative'] =0\nrow_index+=1\n\n\n#index through MLA and save performance to table\n\nfor alg in MLA:\n    print(\"\\n\\nClassifier\",alg)\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train, y_train)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n    \n    #COnfusion Matrix\n\n    true_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, y_pred).ravel()\n    Accuracy=(true_positive+true_negative)/(true_positive+false_positive+true_negative+false_negative)\n    MLA_compare.loc[row_index, 'Accuracy(Confusion Matrix)'] =Accuracy\n    MLA_compare.loc[row_index, 'true_positive'] =true_positive\n    MLA_compare.loc[row_index, 'true_negative'] =true_negative\n    MLA_compare.loc[row_index, 'false_positive'] =false_positive\n    MLA_compare.loc[row_index, 'false_negative'] =false_negative\n    row_index+=1\n\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eca42b4eefd2f3e416fab128905212eea27b4b3"},"cell_type":"code","source":"MLA_compare","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ec020fcf60c47e245cd92ff8f488cf12756821b"},"cell_type":"markdown","source":"# **Justification why Raw data is performing Well**\n**Most of them are givig 89% accuracy **\n\n**Our models are overfitting on negative Examples**\n\n**My Assumption was wrong that Raw data is giving Better accuracy than UnderSampling and OverSampling because Our Models are classifying Negative examples very good. And we have alot of negative Examples in our Dataset Now we Have to do something Different**\n\n**Its better to Experiment than make assumptions in machine Learnig**"},{"metadata":{"_uuid":"5f22296a2dc9a0fdc153ca2f14228f6a375db9f8"},"cell_type":"markdown","source":"# OverSampling data"},{"metadata":{"_uuid":"d194e2ad5e55ee9e0953cdfc5df5dd940f482597"},"cell_type":"markdown","source":"I reapeat this step because at every step I am changing my X and Y. Some where I am putthing Raw data into X and Y and some wehre I am putting Undersampled data. So for confirming I repeat this step that read from data frame and perform the specefic step (over sample -- undersample ) on data"},{"metadata":{"trusted":true,"_uuid":"4aaa60b704c00786aded6dc288b4aaaf9860d9c1","_kg_hide-input":true},"cell_type":"code","source":"X=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0b637d3c04a08541925ddfb3bcb55c398ee3ad9","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    \n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    \n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    #XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy(Scoring)',\"Accuracy(Confusion Matrix)\",\"true_positive\",\"false_positive\",\"true_negative\",\"false_negative\" ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nrow_index = 0\n#ground Truth\nMLA_compare.loc[row_index, 'MLA Name'] = \"Ground Truth\"\nMLA_compare.loc[row_index, 'MLA Parameters'] = \"NaN\"\nMLA_compare.loc[row_index, 'MLA Test Accuracy'] =0\nMLA_compare.loc[row_index, 'Accuracy(Confusion Matrix)'] =0\nMLA_compare.loc[row_index, 'true_positive'] =sum(y_test==1)\nMLA_compare.loc[row_index, 'true_negative'] =sum(y_test==0)\nMLA_compare.loc[row_index, 'false_positive'] =0\nMLA_compare.loc[row_index, 'false_negative'] =0\nrow_index+=1\n\n\n#index through MLA and save performance to table\n\nfor alg in MLA:\n    print(\"\\n\\nClassifier\",alg)\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_train_res, y_train_res)\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n    \n    #COnfusion Matrix\n\n    true_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, y_pred).ravel()\n    Accuracy=(true_positive+true_negative)/(true_positive+false_positive+true_negative+false_negative)\n    MLA_compare.loc[row_index, 'Accuracy(Confusion Matrix)'] =Accuracy\n    MLA_compare.loc[row_index, 'true_positive'] =true_positive\n    MLA_compare.loc[row_index, 'true_negative'] =true_negative\n    MLA_compare.loc[row_index, 'false_positive'] =false_positive\n    MLA_compare.loc[row_index, 'false_negative'] =false_negative\n    row_index+=1\n\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06537c779d9f74bb84e3b47d7afd59ffc6fd63f9"},"cell_type":"markdown","source":"# OverSampled Data Results"},{"metadata":{"trusted":true,"_uuid":"5abcaf028c0afd442790dee5a099315aab0c0fb3"},"cell_type":"code","source":"MLA_compare","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2632a8b8b3348528229d08255e71b63beec9eb6e"},"cell_type":"markdown","source":"Now we have 50 percent positive examples and 50 percent negative examples "},{"metadata":{"_uuid":"709452689973b52dfdfee7e546f9683398a6e6b8"},"cell_type":"markdown","source":"# **Lets see now How Undersampling is working On our Data**"},{"metadata":{"trusted":true,"_uuid":"5c7f7b9d92fe6f918668b8d6684a9f6a26a674e8","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nX=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\n\nrus = RandomUnderSampler(random_state=0,ratio={0: 9000, 1:13000})\nrus.fit(X_train, y_train)\nX_resampled, y_resampled = rus.fit_sample(X_train, y_train.ravel())\n\nprint('After UnderSampling, the shape of train_X: {}'.format(X_resampled.shape))\nprint('After UnderSampling, the shape of train_y: {} \\n'.format( y_resampled.shape))\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\nprint(\"After UnderSampling, counts of label '0': {}\".format(sum(y_resampled==0)))\n\n\n\n\n#red is 0 \n#green is 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16649c4e1c2ca52cec64f1cf6da6b8f0780dea8c","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    \n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    \n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    #XGBClassifier()    \n    ]\n\n\n\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy(Scoring)',\"Accuracy(Confusion Matrix)\",\"true_positive\",\"false_positive\",\"true_negative\",\"false_negative\" ]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nrow_index = 0\n#ground Truth\nMLA_compare.loc[row_index, 'MLA Name'] = \"Ground Truth\"\nMLA_compare.loc[row_index, 'MLA Parameters'] = \"NaN\"\nMLA_compare.loc[row_index, 'MLA Test Accuracy'] =0\nMLA_compare.loc[row_index, 'Accuracy(Confusion Matrix)'] =0\nMLA_compare.loc[row_index, 'true_positive'] =sum(y_test==1)\nMLA_compare.loc[row_index, 'true_negative'] =sum(y_test==0)\nMLA_compare.loc[row_index, 'false_positive'] =0\nMLA_compare.loc[row_index, 'false_negative'] =0\nrow_index+=1\n\n\n#index through MLA and save performance to table\n\nfor alg in MLA:\n    print(\"\\n\\nClassifier\",alg)\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n   # cv_results = model_selection.cross_validate(alg, X_train, y_train)\n    alg.fit(X_resampled, y_resampled )\n    y_pred=alg.predict(X_test)\n    score=metrics.accuracy_score(y_test, y_pred)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] =score\n    \n    #COnfusion Matrix\n\n    true_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, y_pred).ravel()\n    Accuracy=(true_positive+true_negative)/(true_positive+false_positive+true_negative+false_negative)\n    MLA_compare.loc[row_index, 'Accuracy(Confusion Matrix)'] =Accuracy\n    MLA_compare.loc[row_index, 'true_positive'] =true_positive\n    MLA_compare.loc[row_index, 'true_negative'] =true_negative\n    MLA_compare.loc[row_index, 'false_positive'] =false_positive\n    MLA_compare.loc[row_index, 'false_negative'] =false_negative\n    row_index+=1\n\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58a90fc7f7abb027de95f3f66669ac4e2a21be52"},"cell_type":"markdown","source":"# Under Sampling Data Results"},{"metadata":{"trusted":true,"_uuid":"67018b67e8cdce7565e9d4b7165a5f1c72e90e97"},"cell_type":"code","source":"MLA_compare","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b600bc4607c7d47bde2b26a8c76129e008d6ae2b"},"cell_type":"markdown","source":"**From here I just Realized that I can achive better accuracy through neural netwrork(Perceptron, MLP) Classifier If I am able to do fine Tuning**"},{"metadata":{"_uuid":"6369ba987a25a27939f937597c273d2dd9e0bf38"},"cell_type":"markdown","source":"# Neural Networks"},{"metadata":{"_uuid":"65ce3726507142ac4895c693bba7e0473c85a825"},"cell_type":"markdown","source":"**Setting up Dataset**"},{"metadata":{"trusted":true,"_uuid":"5de753d4957540421d52dfaa9fdf14c031335a13","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nfrom keras.utils.np_utils import to_categorical\nX=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\n\nrus = RandomUnderSampler(random_state=0,ratio={0: 9000, 1:13000})\nrus.fit(X_train, y_train)\nX_resampled, y_resampled = rus.fit_sample(X_train, y_train)\n\n\nX_resampled=np.asarray(X_resampled)\ny_resampled=np.asarray(y_resampled)\n\ny_resampled =to_categorical(y_resampled)\ny_test=to_categorical(y_test)\n\ny_resampled=y_resampled.astype(\"int64\")\ny_test=y_test.astype(\"int64\")\nprint('After UnderSampling, the shape of train_X: {}'.format(X_resampled.shape))\nprint('After UnderSampling, the shape of train_y: {} \\n'.format( y_resampled.shape))\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\nprint(\"After UnderSampling, counts of label '0': {}\".format(sum(y_resampled==0)))\n\n\n\n\n#red is 0 \n#green is 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"401773bf7b3cca843e6f99a2b60823a5b1590260"},"cell_type":"markdown","source":"# **Ramdom Search on MLP**"},{"metadata":{"_uuid":"bae442e34229f4de8256229cd3686b1e71744e89"},"cell_type":"markdown","source":"**I am just undersampling data to choose equal number of traning good sample rather than picking up random samples. It take less time to run my model for small example rather than whole dataset to set my hyper parameters this undersampling is only done for seeing the best parameters. Once we get good parameters we take that parameters and than check it on whole dataset.**"},{"metadata":{"_uuid":"3685a8aff88175cd4a61c6fe6a2095335d8a88b6"},"cell_type":"markdown","source":"**For Just Taking Best Hyperpatameters**"},{"metadata":{"trusted":true,"_uuid":"f2e6c6ea32a21efa2e388123b7889d2dedb374d1"},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nX=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\n\nrus = RandomUnderSampler(random_state=0,ratio={0: 1000, 1:1100})\nrus.fit(X_train, y_train)\nX_resampled, y_resampled = rus.fit_sample(X_train, y_train.ravel())\n\nprint('After UnderSampling, the shape of train_X: {}'.format(X_resampled.shape))\nprint('After UnderSampling, the shape of train_y: {} \\n'.format( y_resampled.shape))\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\nprint(\"After UnderSampling, counts of label '0': {}\".format(sum(y_resampled==0)))\n\n\n#red is 0 \n#green is 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d6126137ae0858547f9567a0b26d6ca9e03fa88"},"cell_type":"markdown","source":"**For Fitting our Dataset of Best Parameters**"},{"metadata":{"trusted":true,"_uuid":"5a79ae004666f36efb794bf9866ddce31c662dbb"},"cell_type":"code","source":"X=New_df.drop(\"target\",axis=1)\ny=New_df[\"target\"]\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\n#taking small sample for just selecting hyperparatmers\n\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe460cb38ed9d4ed3d11ff228c67637506a63987","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"import math\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import make_scorer\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense,Dropout,Activation,BatchNormalization\nfrom keras import losses\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\n\ndef best_parameter_model(lr=0.01,dropout=0.5,optimizer=\"adam\",loss='mean_squared_error',\n                    last_activation=\"softmax\",activation=\"relu\",clipnorm=0.1,\n                    decay=1e-2,momentum=0.5,l1=0.01,l2=0.001,No_of_CONV_and_Maxpool_layers=3,\n                    No_of_Dense_Layers=3,No_of_Units_in_dense_layers=24,Conv2d_filters=60):\n    momentum=0.2  \n    decay=0\n    lr=0.0001 \n    dropout=0.1\n    optimizer='Adam'\n    loss='mean_squared_error' \n    last_activation='softmax'\n    activation='relu'\n    clipnorm=1 \n    l2=0.01 \n    \n    '''\n    momentum=best[\"momentum\"]\n    decay=best[\"decay\"]\n    lr=best[\"lr\"] \n    dropout=best[\"dropout\"]\n    optimizer=best[\"optimizer\"]\n    loss=best[\"loss\"]\n    last_activation=best[\"last_activation\"]\n    activation=best[\"activation\"]\n    clipnorm=best[\"clipnorm\"]\n    l2=best[\"l2\"]\n    '''\n    #setting up loss fucntions\n    loss=losses.mean_squared_error\n    if(loss=='mean_squared_error'):\n        loss=losses.mean_squared_error\n    if(loss==\"poisson\"):\n        loss=keras.losses.poisson\n    if(loss==\"mean_absolute_error\"):\n        loss=keras.losses.mean_absolute_percentage_error\n    if(loss==\"mean_squared_logarithmic_error\"):\n        loss=keras.losses.mean_squared_logarithmic_error\n    if(loss==\"binary_crossentropy\"):\n        loss=keras.losses.binary_crossentropy\n    if(loss==\"hinge\"):\n        loss=keras.losses.hinge\n        \n    #setting up Optimizers\n    opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adam\":\n        opt=keras.optimizers.Adam(lr=lr, decay=decay, beta_1=0.9, beta_2=0.999)\n    if optimizer==\"Adagrad\":\n        opt=keras.optimizers.Adagrad(lr=lr, epsilon=None, decay=decay)\n    if optimizer==\"sgd\":\n        opt=keras.optimizers.SGD(lr=lr, momentum=momentum, decay=decay, nesterov=False)\n    if optimizer==\"RMSprop\":\n        opt=keras.optimizers.RMSprop(lr=lr, rho=0.9, epsilon=None, decay=0.0)\n    if optimizer==\"Adamax\":\n        opt=keras.optimizers.Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n        \n        \n    #last activation layer\n    if last_activation==\"softmax\":\n        last_activation=keras.activations.softmax\n    if last_activation==\"sigmoid\":\n        last_activation=keras.activations.sigmoid\n    \n    \n    \n    \n    \n    model=Sequential()\n    \n    \n    \n    \n\n    model.add(Dense(units=84,activation=activation))\n    model.add(Dropout(dropout))\n    \n\n    model.add(Dense(units=62,activation=activation))\n    model.add(Dropout(dropout))\n    \n\n    model.add(Dense(units=31,activation=activation))\n    model.add(Dropout(dropout))\n    \n\n\n    model.add(Dense(units=16,activation=activation))\n    model.add(Dropout(dropout))\n    \n\n\n    model.add(Dense(units=16,activation=activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=2,activation=last_activation))\n\n    model.compile(loss=loss,optimizer=opt,\n                     metrics=['accuracy'])\n    \n    model.compile(loss=loss ,optimizer=opt,\n                 metrics=['accuracy'])\n    \n    \n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae459fc82e0e95d1d608f79a7bd14c6b14db490e","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"params = {'lr': (0.0001, 0.01,0.0009,0.001,0.002 ),\n     'epochs': [500,100,1000,1500],\n     'dropout': (0, 0.2,0.4, 0.8),\n     'optimizer': ['Adam','Adagrad','sgd','RMSprop','Adamax'],\n     'loss': [\"binary_crossentropy\",\"mean_squared_error\",\"hinge\",\"mean_absolute_error\",\"mean_squared_logarithmic_error\",\"poisson\"],\n     'last_activation': [\"softmax\",\"sigmoid\"],\n     'activation' :[\"relu\",\"selu\",\"linear\",\"sigmoid\"],\n     'clipnorm':(0.0,0.5,1),\n     'decay':(1e-6,1e-4,1e-8),\n     'momentum':(0.9,0.5,0.2),\n     'l1': (0.01,0.001,0.0001),\n     'l2': (0.01,0.001,0.0001),\n     'No_of_Dense_Layers': [4,6,8,10,12,14]\n     }\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"9028832110f2302aa66a902088ad3e1e5db48419","_kg_hide-input":false},"cell_type":"code","source":"\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import make_scorer\n# model class to use in the scikit random search CV \n\nmodel = KerasClassifier(build_fn=best_parameter_model, epochs=10, batch_size=500, verbose=1)\nRandomizedSearchfit = RandomizedSearchCV(estimator=model, cv=KFold(3), param_distributions=params, \n                          verbose=1,  n_iter=10, n_jobs=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"f207569da3da2b171ea338a6d00add98978041cb","_kg_hide-input":false},"cell_type":"code","source":"#having some problem in this line\nfrom keras.utils import to_categorical\ny_resampled=to_categorical(y_resampled)\nRandomizedSearch_result = RandomizedSearchfit.fit(X_resampled, y_resampled )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74a732472c0a884d92001e6b0c177c44753d2bd1"},"cell_type":"markdown","source":"** Having Some problem in random search of MLP if you guys get the solution please let me inform Thank you!**\n\n**Kindly See the hide cells to take my Problem Under Considration**\n\n[Link To Stack OverFlow](https://stackoverflow.com/questions/55145261/randomized-search-valued-passed-value-passed-to-parameter-shape-has-datatype-f)\n\n"},{"metadata":{"trusted":true,"_uuid":"fb6847269bc19428baa4128180f3abb976b2c18a"},"cell_type":"markdown","source":"# **Random Search on Random Forest**"},{"metadata":{"trusted":true,"_uuid":"134c6bc564997de8f8908439020d302b59e02174","_kg_hide-output":true},"cell_type":"code","source":"\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# build a classifier\nclf = RandomForestClassifier(n_estimators=20)\n\n\n\n# specify parameters and distributions to sample from\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"],\n             \"n_estimators\":[100,500,1000,2000,5000],\n             }\n\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5,verbose=1)\n\n#Already Searched Parameters\n#random_search.fit(X_resampled, y_resampled )\n#clf = RandomForestClassifier(max_depth=random_search.best_params_['max_depth'],\n #                            max_features=random_search.best_params_['max_depth'],min_samples_split=random_search.best_params_['min_samples_split'],\n  #                          bootstrap=random_search.best_params_['bootstrap'],\n   #                          criterion=random_search.best_params_['criterion'],\n    #                         n_estimators=random_search.best_params_['n_estimators']\n     #                        ,random_state=0)\n#print(random_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfe5f476bccba9e9e371a30d571575825cd00997"},"cell_type":"markdown","source":"best params are\nGet through Random Search\n\nclf = RandomForestClassifier(n_estimators= 1000, min_samples_split= 2, max_features= 3,\n                             max_depth= None, criterion= 'gini', bootstrap= True\n                             ,random_state=0)"},{"metadata":{"_uuid":"253c0752991bf157c2202c56008cbf0ac5a30c4c"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"2713d605149c1266db19101bb120c266792b4a8f"},"cell_type":"markdown","source":"**Now I a going to fit my dataset on best parameters thats why Increasing the size of dataset by Oversampling**"},{"metadata":{"trusted":true,"_uuid":"975e63a234b93fb2e5b17f278dcd263998b29a76"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nclf = RandomForestClassifier(n_estimators= 1000, min_samples_split= 2, max_features= 3,\n                             max_depth= None, criterion= 'gini', bootstrap= True\n                             ,random_state=0)\nclf.fit(X_train_res, y_train_res)\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e15479b03601a94d7e33a35ca5f7babdfa2b2b"},"cell_type":"markdown","source":"**Accuracy is increased but we can Improve more accuracy lets try some other classifier**"},{"metadata":{"trusted":true,"_uuid":"e58f95fb615a6cd056c1a804dfaef99e5712af7a"},"cell_type":"markdown","source":"# **Random Search On Extra Tree Classifier**"},{"metadata":{"trusted":true,"_uuid":"5820799200cf1fbbb4b7956fa583cb7fb638cb4b","_kg_hide-output":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import tree\n\n\nclfETC=tree.ExtraTreeClassifier()\n\n\nparam_dist = {\"max_depth\": [3,5,8,13,17,None],\n              \"max_features\": [0.01,0.03,0.07,0.10,0.15,0.20,\"auto\"],\n              \"min_samples_split\": [2, 5, 10,15,20],\n              \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"splitter\" :['best','random'],\n             \n             }\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clfETC, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=10,verbose=1)\n\n#Already Searched Parameter\n#random_search.fit(X_resampled, y_resampled)\n#print(random_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ae9176580d816aa2f0bdc11098d1b115bec1293"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclfET = tree.ExtraTreeClassifier( min_samples_split= 2, max_features= 0.01,max_depth= 8,\n                              criterion= 'entropy')\n                            \n\nfrom sklearn.metrics import confusion_matrix\nclfET.fit(X_train_res, y_train_res)\nX_test=np.asarray(X_test)\ny_pred=clf.predict(X_test)\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test,y_pred).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\nAccuracy=(true_positive+true_negative)/(true_positive+false_negative+true_negative+false_positive)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n#FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36e52731cc129f3c9e951cfe0a470a0dc3bbe4e2"},"cell_type":"markdown","source":"**Extra trees are not performing well after hyperparameter Optimization**"},{"metadata":{"_uuid":"b2b6d36f574679315020828f6abd3196bb7ee834"},"cell_type":"markdown","source":"# **Ensembling Methods**"},{"metadata":{"_uuid":"92d7af6f69b12fd12c917adc0dcaf0c70a6e0a20"},"cell_type":"markdown","source":"# **Bagging**\n\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."},{"metadata":{"trusted":true,"_uuid":"57bd75fb9896877754a646824ef70db55416e2ac"},"cell_type":"code","source":"from sklearn import ensemble\nfrom sklearn.ensemble import RandomForestClassifier\n\nbstlfy=ensemble.BaggingClassifier(base_estimator=clf,n_estimators=5)\nbstlfy=bstlfy.fit(X_train_res, y_train_res)\nprediction=bstlfy.predict(X_test)\n    \n\ntrue_negative,false_positive,false_negative,true_positive=confusion_matrix(y_test, prediction).ravel()\n\nprint(\"true_negative: \",true_negative)\nprint(\"false_positive: \",false_positive)\nprint(\"false_negative: \",false_negative)\nprint(\"true_positive: \",true_positive)\nprint(\"\\n\\n Accuracy Measures\\n\\n\")\n\nAccuracy=(true_positive+true_negative)/(true_positive+false_positive+true_negative+false_negative)\nprint(\"Accuracy: \",Accuracy)\n\nSensitivity=true_positive/(true_positive+false_negative)\nprint(\"Sensitivity: \",Sensitivity)\n\nFalse_Positive_Rate=false_positive/(false_positive+true_negative)\nprint(\"False_Positive_Rate: \",False_Positive_Rate)\n\nSpecificity=true_negative/(false_positive + true_negative)\nprint(\"Specificity: \",Specificity)\n\n        #FDR à 0 means that very few of our predictions are wrong\nFalse_Discovery_Rate=false_positive/(false_positive+true_positive)\nprint(\"False_Discovery_Rate: \",False_Discovery_Rate)\n\nPositive_Predictive_Value =true_positive/(true_positive+false_positive)\nprint(\"Positive_Predictive_Value: \",Positive_Predictive_Value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39e0fa1afb24b959b82259a6d8b2b7fe801319e6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8f94c3ee074f6047c71a1ff0a2fbe2f703d87fd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c3a18a07c5040cdb1b3e1f6f3680e88b1275135"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8824a41d273e8a058ee00e71cb972bb94d2d6f29"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbd76b01f94f6e118c81b8dc579d57015010f68f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1d473437fdd24065fa9ca9ee2e7df53bd7d3793"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50a6aa1c9b4c299f3f7ec9e887be1f1e37f7e087"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be1f25f13c58d1aaede187b626a3725fa0d330c8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5034fb13cd2ff1c025e56cd58a1baff0e829378"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e28a543e65311c6d262e802e851a35afc689a8df"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d044aab17df4c98b90868ad2c1a7f121ea91041c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7873d751a6718f2f6bd1be3d7e315313bcfde8a7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85d3184b94d5f31ad20d5ab9d2dbd47c7c4d5c4b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e64c7083583fe0d013739ab758264aa9d408beb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"210b2314f3cc802fe03c6ac18285b5f49b904ce6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59e9425489b290645d6e015d53a2c9cdda752057"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2755c502aee3488b5698a08f9e0faff5c8b9c1fc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9472f0554a4799957701255336c7acd737b891b3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f4011bff8acc421e0412c92811299c500b40963"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6089a37df7befbad91b3485ada095e6a0f5c82a3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be500d946b32da568a6d1778453808692f44a277"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"355b2754ecef7099401b4796a7621cad58644e1a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32f412d016731a7e03da9aad8ba1547fc418565f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e31dfdf6b98c272fbe569bcb7ebb985e71ff2cc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d66784d0f10d29a22cfeb6b52f2e2cc48c669a22"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08e16b089e01c7f8567fa64278559f294d4e1e9c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7f6ffda7a92fda33a1a9845a3cf5468c49b08ee"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"437a63e17a444e219a6e239d46c1883e5d89f65c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"136d0b17e1d2d5bd429a6f9cdb4e05e51a30b880"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f336c8fedcdb99fab23818b33bcae956ff0dc708"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52dcec0369b83c353150facaf763278ba57a5e79"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cedb2dfdd92b83aa38b174b229fc1fd813b1488"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45d0ab7e8dd0709c553bfd7f498c15c9af70288f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58ef63bf65f8f21bf8e9dc2dab05f1e04aabdb9c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9d2e10ad14421fffbd88928eeceb3470857a91a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cedaf0c66f342d690ecda20550918c9a64aef6a9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d5d979943235bbe292e14892c735cbe1cb5c919"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d4995ffa6246679edf7268b440a5e679c5a17dc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"269a31c72f8c39a2c7c4f7d563eb4b24f2615769"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380fc0380ad001205a1fe138db1284b59164a5dd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8611d175959fcfd527427a2194c219ca040e6ec2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"620a1bbf42263194d1c457c290ad01846cb6cb14"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdbeac186726b9436f88eeb45dc7120b9f80346e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acb8e697bf7406a9fbd42d8398c2dce3b41327db"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d39c1cec5f8e924968be83944cf01e4606543ff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca89bd6ffb3eedc1d447889bcf2b0448ad7a3874"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bb406089f565ba620146f22a39a9a1952e1ced3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"143d970d4a22b2ad7b208bcbc5330dcedacd4f30"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9502f38b0777bc492df7b5423ad79c73836d103"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83059cb969fccdd44b66285e7ccb3db864e6d320"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32fd8e28906ab8b3587fcff2e8509ffca82a729b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea103cdad30e9077007a1693074ecba8ea8a6fc8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5353d7641a619a7b9d7dbecdec47f45cbd9442a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"081df1da7a1c35339dd7ad9426d8d73f9d7c512d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bea6c6c73c6de7392f34788fb0477d45c657725"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bad059aabb542ff2212dca52e291eca2c99aa4d2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b679f1bb97cb16c9fda9d029aa82140aad2291d0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a2d81d2dfffdf5c1742b2d5017257d490872c71"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1aa1cfd8e714449ecccf9b086791c00fbe42d7d3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45fdf95042e36b96bf8b1ff79b37f008bf04e1a9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64ff6eaa95599c69722de4084815be4e5921af7b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11d0944db117e9413e7b7900177718b9a8169414"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9413e1f7d0f7af61d8156ef9b2b1f916ed9e23bd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f1b20f2588765e70db384d967b2e8c25488b890"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fdd4ab1cebcecd5eceeeb95580f8ce31b59eecf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8587aa0fff4eb55605c9953adbbd978da1e5ffc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77a11871ec6a97f9b9f10da03b83a8efafe07ea4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99d7cc74a0d57505e08f38f3a83d11701e00bdf8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbe983508e2242a009ea2450a8442057397752f5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2be84a7830e3a49c73886bc9acd37c2f3905e055"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c3629e9d198e4a7026cbb3122e5c8021c880d1e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d9e14de3be24697f690c997c52a5b7fa9de991"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f91d59e45f57991fe505c3363d1df682134ec6b6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7747a36a076eb49537097c165a3f153d31b0209"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c8c94f02820a0aee9815a27687b131e1c5efdfa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"255f40aa21a1954ad304ce5a731721be29415f0d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19784251815e87f370844d86ec07aebf4457269e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffab8f7f35af44250a378e9fbc307d3e0c18affa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17d00be1000e373963cbf09746ab3728e5430b45"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e3785a986eda7e18ff6b37e66e5e7b9b08ead4d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8157a8e0a48d75da6582e4af552eeb069bbd04f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35c2ceba1b78fb9f49317aeb376ac710412f92ba"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"687ec0232d88df147ce97f670120c733cbf6ebfe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95ce17eb335c58ccd97dc819f524d4943fdfb9b2"},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}