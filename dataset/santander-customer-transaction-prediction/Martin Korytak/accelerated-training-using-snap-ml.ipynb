{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Santander Customer Transaction Prediction"},{"metadata":{},"cell_type":"markdown","source":"Authors: Martin Korytak, Thomas Parnell"},{"metadata":{},"cell_type":"markdown","source":"In this challenge, the Santander team invited Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money involved. The data provided for this competition has the same structure as the real data Santander have available to solve this problem.\n\nIn this kernel, we are going to show how we can use IBM's **Snap ML** library to **accelerate training**. We will demonstrate how Snap ML enables us to construct a high-ranking submission in much less time than the equivalent approach using scikit-learn. We will use the heuristic introduced in this [kernel](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split) and extend the idea presented in this [kernel](https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920). Furthermore, this kernel uses fixed `seed` in order to make the results easily reproducible.\n\nFor more information about the capabilities of Snap ML please visit the project homepage at: https://www.zurich.ibm.com/snapml/"},{"metadata":{},"cell_type":"markdown","source":"### Import and Install Necessary Packages"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"!pip install snapml","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import decomposition\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_sample_weight\nimport sklearn.ensemble as skl\nfrom os import path\nimport time\nfrom scipy.stats import normaltest\nfrom scipy.special import logit\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport snapml\nimport multiprocessing\nfrom typing import List, Dict, Union\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"np.random.seed(130720)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data Sets"},{"metadata":{},"cell_type":"markdown","source":"The data sets are ready for use immediately after loading into the memory. The only difference between `train.csv` and `test.csv` is that `train.csv` contains also `target` column with ground truth. The only thing in both data sets, which we will not use during training, is `ID_code` column."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"path_to_folder = '/kaggle/input/santander-customer-transaction-prediction'\ndf = pd.read_csv(path.join(path_to_folder, 'train.csv'))\ndf_test = pd.read_csv(path.join(path_to_folder, 'test.csv'))\n\nid_codes = df_test['ID_code'] # we need to keep ID_code for a submission of our predictions\ndf_test.drop('ID_code', axis=1, inplace=True)\ndf.drop('ID_code', axis=1, inplace=True)\nfeatures = df.drop('target', axis=1)\ncolumns = features.columns\ntarget = df.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Auxiliary Functions"},{"metadata":{"trusted":false},"cell_type":"code","source":"def time_decorator(function):\n    \"\"\"\n    Decorator which measure time spent within a particular function.\n    :param function: timed function\n    :return: function which measures time\n    \"\"\"\n    def timed(*args, **kwargs):\n        ts = time.time_ns()\n        result = function(*args, **kwargs)\n        te = time.time_ns()\n        return result, (te - ts) * 1e-9\n\n    return timed","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def get_real_synthetic_testing_samples(testing_set: pd.DataFrame) -> tuple:\n    # idea from https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n    \"\"\"\n    Discovers which rows are artificially created and which are real.\n    :param testing_set: dataframe of testing samples\n    :return: indices of real and synthetic samples\n    \"\"\"\n    counts = np.zeros(shape=testing_set.shape)\n    for i, col in enumerate(testing_set.columns):\n        counts[:, i] = testing_set[col].map(testing_set[col].value_counts())\n\n    is_row_real_sample = np.any(counts == 1, axis=1)\n    fake_samples_indices = np.argwhere(~is_row_real_sample).flatten()\n    real_samples_indices = np.argwhere(is_row_real_sample).flatten()\n    return real_samples_indices, fake_samples_indices","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def get_aggregated_features(column: pd.Series, column_name: str, decimals: int, agg_function: str) -> pd.Series:\n    \"\"\"\n    Creates a new column created by `agg_function` per each unique value in `column`.\n    :param column: a column which will be used for calculation\n    :param column_name: a name of the column in original dataframe\n    :param decimals: a number of decimal places for rounding values before group by function is called\n    :param agg_function: a function which will be used for calculating new values in the new column\n    :return: a new column consisting of values calculated by `agg_function` per each unique value in the column\n    \"\"\"\n    return column.to_frame().groupby(column.round(decimals))[column_name].agg(agg_function)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def add_features_in_batch(data_set_for_calculation: pd.DataFrame, all_dataframes: List, columns: np.array, decimal_places: Dict) -> List:\n    \"\"\"\n    Adds new columns per each specified feature in a particular batch.\n    :param data_set_for_calculation: a batch of data for calculating new features\n    :param all_dataframes: a list of dataframes where new features will be added\n    :param columns: a list of column names\n    :param decimal_places: a number of decimal places used for calculating new features\n    :return: a list of dataframes with added new features\n    \"\"\"\n    for col in tqdm(columns):\n        for feature in decimal_places.keys():\n            x = get_aggregated_features(data_set_for_calculation[col], col, decimal_places[feature], feature)\n            for dataframe in all_dataframes:\n                dataframe[f'{col}_{feature}'] = dataframe[col].round(decimal_places[feature]).map(x)\n                if feature == 'std':\n                    dataframe.fillna(value=0, inplace=True)  # std might introduce some NaN values\n    return all_dataframes","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def estimate_counts_based_on_real_testing_samples(training_and_validation_sets: List[pd.DataFrame], testing_set: pd.DataFrame, columns: List) -> List:\n    # idea from https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920\n    \"\"\"\n    Add new columns to all provided dataframes with counts of particular entries.\n    :param training_and_validation_sets: list of dataframes with training and validation rows\n    :param testing_set: dataframe of testing set\n    :param columns: names of columns in original data set\n    :return: new dataframes with additional columns\n    \"\"\"\n    real_samples_indices, _ = get_real_synthetic_testing_samples(testing_set)\n\n    # estimating counts using only real testing samples\n    data_set_for_estimation = pd.concat([*training_and_validation_sets, testing_set.loc[real_samples_indices]])\n    # create a copy of provided dataframes\n    all_dataframes = [*[d.copy(deep=True) for d in training_and_validation_sets], testing_set.copy(deep=True)]\n\n    decimals = {  # optimal number of decimal places per a new feature\n        'count': 4,\n        'mean': 4,\n        'std': 4,\n        'sum': 4,\n        'min': 2,\n        'max': 3\n    }\n\n    args = []\n    n_processes = 10\n\n    columns = np.array(columns)  # convert to NumPy array due to the need to access indices\n    batch_size = int(np.ceil(len(columns) / n_processes))\n    for i in range(n_processes):\n        indices = list(range(batch_size * i, batch_size * (i + 1)))\n        args.append((data_set_for_estimation.iloc[:, indices], [dataframe.iloc[:, indices] for dataframe in all_dataframes], columns[indices], decimals))\n\n    with multiprocessing.Pool(processes=n_processes) as p:\n        chunks = p.starmap(add_features_in_batch, args)\n\n    all_dataframes = [pd.concat(d, axis=1) for d in zip(*chunks)]\n    return all_dataframes","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def get_column_indices(df: pd.DataFrame, query_cols: List) -> np.array:\n    # idea from: https://stackoverflow.com/a/38489403\n    \"\"\"\n    Retrieve indices of given columns.\n    :param df: dataframe from which this function retrieves indices\n    :param query_cols: names of columns whose names are requested as indices \n    :return: array with indices coresponding to `query_cols`\n    \"\"\"\n    cols = df.columns.values\n    sidx = np.argsort(cols)\n    return sidx[np.searchsorted(cols, query_cols, sorter=sidx)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"@time_decorator\ndef train(model: Union[snapml.RandomForestClassifier, skl.RandomForestClassifier], X_train: np.ndarray, y_train: np.array, weights: np.array):\n    \"\"\"\n    Train a model using `X_train` data set.\n    :param model: instance of a machine learning model which implements `fit` method\n    :param X_train: training examples\n    :param y_train: training labels\n    :param weights: weight of each example in `X_train`\n    \"\"\"\n    model.fit(X_train, y_train, weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_model(library: str, params: Dict, r_seed: int, n_cpus: int) -> Union[snapml.RandomForestClassifier, skl.RandomForestClassifier]:\n    \"\"\"\n    Build a machine learning model using Snap ML or scikit-learn library.\n    :param library: name of library which is going to be used\n    :param params: hyper-parameters with optimal values of random forest\n    :param r_seed: random seed of a particular model\n    :param n_cpus: number of CPUs available for training\n    :return: created model\n    \"\"\"\n    if library == 'snapml':\n        return snapml.RandomForestClassifier(**params, use_gpu=False, random_state=r_seed, n_jobs=n_cpus)\n    elif library == 'sklearn':\n        return skl.RandomForestClassifier(**params, random_state=r_seed, n_jobs=n_cpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_barchart(sklearn_time: float, snapml_time: float):\n    \"\"\"\n    Plot bar chart with training time for Snap ML and scikit-learn library.\n    :param sklearn_time: training time of scikit-learn model in seconds\n    :param snapml_time: training time of Snap ML model in seconds\n    \"\"\"\n    plt.bar(0.5, sklearn_time, 0.4, ecolor='black')\n    plt.bar(1.5, snapml_time, 0.4, ecolor='black')\n    plt.xticks([0.5, 1.5], ['scikit-learn', 'Snap ML'])\n    plt.ylabel('Training Time (s)')\n    plt.xlabel('Machine Learning Libraries')\n    plt.title('Training Time of Snap ML and scikit-learn Library')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"def plot_roc_curve(y_val: np.array, y_pred: np.array):\n    \"\"\"\n    Plot ROC curve with baseline score and AUC score.\n    :param y_val: true labels\n    :param y_pred: predicted labels\n    \"\"\"\n    fpr, tpr, _ = roc_curve(y_val,  y_pred)\n    auc = roc_auc_score(y_val, y_pred)\n    plt.plot(fpr, tpr, label=f'target 1, auc={auc:.4f}', color='r')\n    plt.plot([0, 1], [0, 1], color='k', linestyle='dashed')\n    plt.legend(loc=4)\n    plt.ylabel('TPR - \"probability of detection\"')\n    plt.xlabel('FPR - \"probability of false alarm\"')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"First of all, let's explore the data set. We can see that all feature columns contain continuous numbers."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can confirm that there are no missing values either in `train.csv` or `test.csv`."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"print(f'Number of NaN entries in train: {sum(df.isnull().sum())}, test: {sum(df_test.isnull().sum())}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features are not correlated at all and we will leverage this fact in the training section. There is neither negative nor possitive correlation between any pair of features."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"corr = np.tril(features.corr(), k=-1) # without the diagonal ones\nprint(f'Maximal correlation: {corr.max():.4f}, minimal correlation {corr.min():.4f}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create the first plot to get an intuition what is happening under the hood. As we can see, the particular features are not normally distributed even though it might be the human intuition at first sight."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"for col in features.columns:\n    features[col].hist(alpha=0.5, bins=30);\n    \n# check normality with statistics -> data is not normally distributed\nalpha = 0.05\nfor col in features.columns:\n    stat, p = normaltest(features[col])\n    if p > alpha: # null hypothesis: feature column comes from a normal distribution\n        print(f'p-value was {p} > {alpha}, null hypothesis cannot be rejected for feature {col}, therefore column {col} is normally distributed.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Means of features are normally distributed, whereas standard deviations of features are not normally distributed which we can confirm looking at the plots as well as using tests of data normality."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"features.mean().plot.hist(bins=10);\nprint(normaltest(features.mean())) # means of features are normally distributed","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"features.std().plot.hist(bins=10);\nprint(normaltest(features.std())) # standard deviations of features are not normally distributed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the box plot, it seems there are some outliers but none of them is too far from whiskers and therefore we do not remove any row from the original data set."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"features.plot.box(vert=False, figsize=(13, 32));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data set is highly imbalanced and we need to keep this in mind when training a model."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"sns.countplot(x=target, hue='target', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using PCA analysis, we can see that `target=1` is randomly distributed across the figure. This fact may make accurate predictions more difficult."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"portion_of_data = features.sample(frac=0.01, random_state=1)\nprint(target.iloc[portion_of_data.index].value_counts())\npca = decomposition.KernelPCA(n_components=2, kernel='poly')\nX = pca.fit_transform(portion_of_data)\n\ntarget_one_condition = target.iloc[portion_of_data.index] == 1\ntarget_zero_condition = target.iloc[portion_of_data.index] == 0\nplt.scatter(X[target_zero_condition, 0], X[target_zero_condition, 1], label='0');\nplt.scatter(X[target_one_condition, 0], X[target_one_condition, 1], label='1');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"We prepare our data set for training, the first thing we will do is discovering which samples are real and which samples are synthetic. The rule, how to detect synthetic samples, is very simple. We say the sample is real if and only if at least one feature value is unique within a particular feature vector. Then, we create multiple new features using aggregate functions. A closer look shows us that many values are repeating in a column and they can be grouped into bins. The new features are created using `count`, `mean`, `standard deviation`, `sum`, `minimum` and `maximum` function, respectively. We tried to round the number of decimal places but in most cases it did not bring any additional improvement in terms of AUC score. Please note that the original precision is set to 4 decimal places."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"X, X_test = estimate_counts_based_on_real_testing_samples([features], df_test, columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build and Train Models"},{"metadata":{},"cell_type":"markdown","source":"In this section, we train a random forest classifier using Snap ML as well as the equivalent classifier from scikit-learn. Finally, we compare the time spent within the training phase and we compare the performance of both models.\n\nHere, we use the fact that features are not correlated and therefore we can build and train multiple models on different features separately and leverage the new features added to data set in `Feature Engineering` section. At the end, we add the predictions together. We also use 5-fold cross validation in order to reduce a possible variance. And due to the unbalanced data set stratified cross validation is used together with `sample_weight` parameter in `fit` method. The optimal hyper-parameters have been found using a random search beforehand."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"optimal_hyperparams = {\n    'max_depth': 5,\n    'min_samples_leaf': 386,\n    'n_estimators': 88\n}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# create array with training columns for each base learner\nfeatures = [get_column_indices(X, X.filter(regex=fr'{col}(?!\\d)').columns) for col in columns]\n\n# cast all pd.DataFrames to NumPy arrays since Snap ML does not support pd.DataFrame\nX = X.to_numpy()\nX_test = X_test.to_numpy()\ny = target.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_cpus = multiprocessing.cpu_count() # get number of available CPUs","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"verbose = True\nk = 5 # number of folds\ny_pred_logit_snapml = np.zeros(X_test.shape[0])\nvalidation_auc = 0\nsnapml_fit_time = 0\n\ncv = StratifiedKFold(n_splits=k, shuffle=True, random_state=1)\nfor n_fold, (train_indices, val_indices) in enumerate(cv.split(X, y), start=1):\n    print(f'{n_fold}. fold (out of {k}) is running.')\n    \n    X_train = X[train_indices, :]\n    X_val = X[val_indices, :]\n    y_train = y[train_indices]\n    y_val = y[val_indices]\n    \n    w_train = compute_sample_weight('balanced', y_train)\n\n    tmp_test = np.zeros(shape=X_test.shape[0])\n    tmp_val = np.zeros(shape=X_val.shape[0])\n    for idx in tqdm(range(len(columns))):       \n        rf = create_model('snapml', optimal_hyperparams, idx, n_cpus)\n        _, t_fit = train(rf, X_train[:, features[idx]], y_train, w_train)\n        snapml_fit_time += t_fit\n        \n        tmp_val += logit(rf.predict_proba(X_val[:, features[idx]])[:, 1])\n        tmp_test += logit(rf.predict_proba(X_test[:, features[idx]])[:, 1])    \n    y_pred_logit_snapml += tmp_test\n    \n    validation_auc += roc_auc_score(y_val, tmp_val)\n    \n    if verbose:\n        plot_roc_curve(y_val, tmp_val)\n    \nprint(f'The average AUC estimated from {k} validation splits is: {validation_auc / k:.4f}.')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":false},"cell_type":"code","source":"verbose = True\nk = 5 # number of folds\ny_pred_logit_skl = np.zeros(X_test.shape[0])\nvalidation_auc = 0\nsklearn_fit_time = 0\n\ncv = StratifiedKFold(n_splits=k, shuffle=True, random_state=1)\nfor n_fold, (train_indices, val_indices) in enumerate(cv.split(X, y), start=1):\n    print(f'{n_fold}. fold (out of {k}) is running.')\n    \n    X_train = X[train_indices, :]\n    X_val = X[val_indices, :]\n    y_train = y[train_indices]\n    y_val = y[val_indices]\n    \n    w_train = compute_sample_weight('balanced', y_train)\n\n    tmp_test = np.zeros(shape=X_test.shape[0])\n    tmp_val = np.zeros(shape=X_val.shape[0])\n    for idx in tqdm(range(len(columns))):        \n        rf = create_model('sklearn', optimal_hyperparams, idx, n_cpus)\n        _, t_fit = train(rf, X_train[:, features[idx]], y_train, w_train)\n        sklearn_fit_time += t_fit\n        \n        tmp_val += logit(rf.predict_proba(X_val[:, features[idx]])[:, 1])\n        tmp_test += logit(rf.predict_proba(X_test[:, features[idx]])[:, 1])    \n    y_pred_logit_skl += tmp_test\n    \n    validation_auc += roc_auc_score(y_val, tmp_val)\n    \n    if verbose:\n        plot_roc_curve(y_val, tmp_val)\n    \nprint(f'The average AUC estimated from {k} validation splits is: {validation_auc / k:.4f}.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(f'[scikit-learn] training time: {sklearn_fit_time:.2f} (s)')\nprint(f'[Snap ML] training time: {snapml_fit_time:.2f} (s)')\nprint(f'Speed-up: {sklearn_fit_time / snapml_fit_time:.2f}x')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_barchart(sklearn_fit_time, snapml_fit_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the training above it is clear that (1) both models perform very similarly in terms of AUC score and (2) the Snap ML model **trains significantly faster** than the scikit-learn model."},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{},"cell_type":"markdown","source":"The final step is to submit our Snap ML predictions to the competition to see what our position in the leaderboard is."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"filepath = 'submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"ID_code\": id_codes,\n        \"target\": y_pred_logit_snapml\n    })\nsubmission.to_csv(filepath, index=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}