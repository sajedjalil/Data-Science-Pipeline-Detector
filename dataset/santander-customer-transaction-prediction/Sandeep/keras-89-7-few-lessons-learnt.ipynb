{"cells":[{"metadata":{"_uuid":"dadfc2aac90be8fd8a28d9026832cf59230cbf17"},"cell_type":"markdown","source":"# Few lessons learnt while working on this competetion. Might be helpful for beginners.\n\n\n1) Initially predict hard values 0 or 1 and roc_auc scores ended up in 65 - 70. Came to know that we need to use probabilities for AUC, which increased the score to 86.\n\n2) Changing the input data dimension from 1D --> 2D improved score a lot. Got this idea from this [kernal](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/82863)\n\n3) Dimensionality reduction: If we want to visualize 2 variables(2 dimensional) we can do it using any type of plot, 1 variable at x-axis and another in y-axis. But, what if want to visualize 200 variables (200 dimension)? we can use dimensionality reduction techniques like PCA,t-SNE, UMAP for the same. These techniques intelligently summarizes/group information related to multi dimension to the required low dimension. Unfortunately this techniques didn't  help much in this competetion. [PCA](https://www.kaggle.com/sandeep8530/pca-for-santander)\n\n4) Cyclelr: Learning rate is one of the important hyperparameters. Varying learning rate helps in fast and effective model. [Reference](https://github.com/bckenstler/CLR) \n\n5) K-fold: Came to know that apart from using it for cross_val_score to know the robustness of the model, it can used to predict values at each fold which improves the score(kind of ensemble)."},{"metadata":{"_uuid":"756fb749440871b3bb21771d5571d7a8bef2ff92"},"cell_type":"markdown","source":"# Please upvote, if you find this kernel interesting"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nnp.random.seed(697)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b0036350c858dae7d051e2bf834c4b0820afe17"},"cell_type":"code","source":"from keras.callbacks import Callback\nfrom keras import backend as K\n\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency.\n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    The amplitude of the cycle can be scaled on a per-iteration or\n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    For more detail, please see paper.\n    # Example for CIFAR-10 w/ batch size 100:\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    # References\n      - [Cyclical Learning Rates for Training Neural Networks](\n      https://arxiv.org/abs/1506.01186)\n    \"\"\"\n\n    def __init__(\n            self,\n            base_lr=0.001,\n            max_lr=0.006,\n            step_size=2000.,\n            mode='triangular',\n            gamma=1.,\n            scale_fn=None,\n            scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        if mode not in ['triangular', 'triangular2',\n                        'exp_range']:\n            raise KeyError(\"mode must be one of 'triangular', \"\n                           \"'triangular2', or 'exp_range'\")\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma ** x\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr - self.base_lr) * \\\n                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_batch_end(self, epoch, logs=None):\n\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n        self.history.setdefault(\n            'lr', []).append(\n            K.get_value(\n                self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbb86ccf1a071dfb89afcd361564fe8559fadeb1"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', index_col=0)\ntest_df = pd.read_csv('../input/test.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4cb2703f9e905a7a7c2c10fe61de590b3c9686b"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02d43a4f833ce5cf7aaa4f0e8b332174d5611033"},"cell_type":"code","source":"X = train_df.drop(['target'],axis=1)\ny = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82adb37abae35634b1f2ea199b4468289c8caaa3"},"cell_type":"code","source":"X_test = test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a134a51ab449a1e93afb88040a22a7811b6867b4"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_s=scaler.fit_transform(X)\nX_test_s=scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b58784c95a5eca0a4d3323d39d870f41a26c2dc3"},"cell_type":"code","source":"X=pd.DataFrame(X_s)\nX_test=pd.DataFrame(X_test_s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07c5100d06512cb3f55920b8acaaa93b9ca98da4"},"cell_type":"code","source":"X_test.index=test_df.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a06cc62de0ec6075dd51dc551cb231dc4db7c3d6"},"cell_type":"code","source":"seed = 7\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b500eaf782edfeb08b99c1a0637ac7f8ea6a1d8"},"cell_type":"code","source":"# CROSS VALIDATION\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30aa5a9deb28cff7a59357aacfb8b208b424780f"},"cell_type":"code","source":"adam = optimizers.adam\nmodel = Sequential()\nmodel.add(Dense(64, input_shape=(200,1),\n                kernel_initializer='normal',\n                activation=\"relu\"))\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\", optimizer='adam',metrics=['accuracy'])\n\nclr = CyclicLR(base_lr=0.001, max_lr=0.006,step_size=400., mode='triangular')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f2bc43ec13b74e41d95e61f5aebf7736df9d130"},"cell_type":"code","source":"preds = []\nc = 0\noof_preds = np.zeros((len(X), 1))\n\nfor train, valid in cv.split(X, y):\n    print(\"VAL %s\" % c)\n    X_train = np.reshape(X.iloc[train].values, (-1, 200, 1))\n    y_train_ = y.iloc[train].values\n    X_valid = np.reshape(X.iloc[valid].values, (-1, 200, 1))\n    y_valid = y.iloc[valid].values\n    early = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=20)\n    model.fit(X_train, y_train_, validation_data=(X_valid, y_valid), epochs=200, verbose=2, batch_size=1024,\n              callbacks=[early,clr])\n    X_test1 = np.reshape(X_test.values, (200000, 200, 1))\n    curr_preds = model.predict(X_test1, batch_size=1024)\n    oof_preds[valid] = model.predict(X_valid)\n    preds.append(curr_preds)\n    c += 1\nauc = roc_auc_score(y, oof_preds)\nprint(\"CV_AUC: {}\".format(auc))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"377c918f882b9be4a868d21fa9505af88e16d49e"},"cell_type":"code","source":"preds = np.asarray(preds)\npreds = preds.reshape((5, 200000))\npreds_final = np.mean(preds.T, axis=1)\nsubmission = pd.read_csv('./../input/sample_submission.csv')\nsubmission['target'] = preds_final\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"857b6ba9e50307f586e3d444fa3e460565fc2667"},"cell_type":"markdown","source":"# Please upvote, if you find this kernel interesting"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}