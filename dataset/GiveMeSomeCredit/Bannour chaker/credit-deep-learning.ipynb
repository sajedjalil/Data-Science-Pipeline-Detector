{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" \n<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Get Started in Deep Learning</center></h3>\n\n* [The 5-Step Model Life-Cycle](#1)\n* [Import](#2)\n* [Prepare Data/Impute Missing Value](#3)\n* [Sequential Model API](#4)   \n* [Modeling](#100)\n* [Submission](#100)\n    \nPredictive modeling with deep learning is a skill that modern developers need to know.\n\nTensorFlow is the premier open-source deep learning framework developed and maintained by Google. Although using TensorFlow directly can be challenging, the modern tf.keras API beings the simplicity and ease of use of Keras to the TensorFlow project.\n\nUsing tf.keras allows you to design, fit, evaluate, and use deep learning models to make predictions in just a few lines of code. It makes common deep learning tasks, such as classification and regression predictive modeling, accessible to average developers looking to get things done.\n\nIn this Notebook, you will discover a step-by-step guide to developing deep learning models in TensorFlow using the tf.keras API.\n Deep Learning Model Life-Cycle\n\nIn this section, you will discover the life-cycle for a deep learning model and the two tf.keras APIs that you can use to define models.\n    \n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>The 5-Step Model Life-Cycle</center></h3>\n\nA model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the tf.keras API.\n\nThe five steps in the life-cycle are as follows:\n    \n\n                Define the model.\n\n                Compile the model.\n\n                Fit the model.\n\n                Evaluate the model.\n\n                Make predictions.    \n\n   \nhttps://www.kaggle.com/bannourchaker/deep-learning-starter-gpu\n    \nhttps://www.kaggle.com/bannourchaker/10-deeplearning-embedding-rnn-tf-keras/edit/run/74137751    \n    \n<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Import</center></h3>","metadata":{"papermill":{"duration":0.057219,"end_time":"2022-01-10T16:34:03.667902","exception":false,"start_time":"2022-01-10T16:34:03.610683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"###############################################################################\n#                       Load Library                                          #\n###############################################################################\n\n#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\n# Preparation  \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\n# Import StandardScaler from scikit-learn\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer,ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score,recall_score\nfrom sklearn.metrics import make_scorer,mean_absolute_error\nfrom sklearn.metrics import mean_squared_error,classification_report,f1_score\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.metrics import AUC\n#import smogn\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone,ClassifierMixin\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif,chi2\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom xgboost import XGBClassifier,XGBRegressor\nfrom sklearn import set_config\nfrom itertools import combinations\n# Cluster :\nfrom sklearn.cluster import MiniBatchKMeans\n#from yellowbrick.cluster import KElbowVisualizer\n#import smong \nimport category_encoders as ce\nimport warnings\n#import optuna \nfrom joblib import Parallel, delayed\nimport joblib \nfrom sklearn import set_config\nfrom typing import List, Optional, Union\nimport itertools\nimport shap\n\n# Imbalanced data \nfrom imblearn.datasets import fetch_datasets\n# to correctly set up the cross-validation\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import (\n    RandomUnderSampler,\n    CondensedNearestNeighbour,\n    TomekLinks,\n    OneSidedSelection,\n    EditedNearestNeighbours,\n    RepeatedEditedNearestNeighbours,\n    AllKNN,\n    NeighbourhoodCleaningRule,\n    NearMiss,\n    InstanceHardnessThreshold\n)\nfrom imblearn.over_sampling import (\n    RandomOverSampler,\n    SMOTE,\n    ADASYN,\n    BorderlineSMOTE,\n    SVMSMOTE,\n)\nimport gc\nset_config(display='diagram')\nwarnings.filterwarnings('ignore')\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":15.142189,"end_time":"2022-01-10T16:34:18.865811","exception":false,"start_time":"2022-01-10T16:34:03.723622","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-13T16:56:38.641601Z","iopub.execute_input":"2022-01-13T16:56:38.641956Z","iopub.status.idle":"2022-01-13T16:56:42.330619Z","shell.execute_reply.started":"2022-01-13T16:56:38.64185Z","shell.execute_reply":"2022-01-13T16:56:42.329698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Load Data</center></h3>\n## Load the training data","metadata":{"papermill":{"duration":0.061156,"end_time":"2022-01-10T16:34:18.988952","exception":false,"start_time":"2022-01-10T16:34:18.927796","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time \n###############################################################################\n#                        Read train data                                      #\n###############################################################################\n\ntrain = pd.read_csv('../input/GiveMeSomeCredit/cs-training.csv')\ntest = pd.read_csv('../input/GiveMeSomeCredit/cs-test.csv')\ntrain.head(3)","metadata":{"papermill":{"duration":0.486631,"end_time":"2022-01-10T16:34:19.534787","exception":false,"start_time":"2022-01-10T16:34:19.048156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-13T16:56:42.332755Z","iopub.execute_input":"2022-01-13T16:56:42.333075Z","iopub.status.idle":"2022-01-13T16:56:42.608914Z","shell.execute_reply.started":"2022-01-13T16:56:42.333034Z","shell.execute_reply":"2022-01-13T16:56:42.608217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                       Cast dtypes                                           #\n###############################################################################\n\n# Convert Dtypes :\ntrain[train.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns] = train[train.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','category']).columns] = train.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))\n# Convert Dtypes :\ntest[test.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns] = test[test.select_dtypes(['int64','int16','float16','float32','float64','int8']).columns].apply(pd.to_numeric)\ntest[test.select_dtypes(['object','category']).columns] = test.select_dtypes(['object','category']).apply(lambda x: x.astype('category'))","metadata":{"papermill":{"duration":0.138461,"end_time":"2022-01-10T16:34:19.728935","exception":false,"start_time":"2022-01-10T16:34:19.590474","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-13T16:56:42.610368Z","iopub.execute_input":"2022-01-13T16:56:42.611Z","iopub.status.idle":"2022-01-13T16:56:42.679517Z","shell.execute_reply.started":"2022-01-13T16:56:42.610953Z","shell.execute_reply":"2022-01-13T16:56:42.678744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                        Reduce Memory                                        #\n###############################################################################\n\n# Author : https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        name =df[col].dtype.name \n        \n        if col_type != object and col_type.name != 'category':\n        #if name != \"category\":    \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\ntrain= reduce_mem_usage(train)\ntest= reduce_mem_usage(test)","metadata":{"papermill":{"duration":0.131689,"end_time":"2022-01-10T16:34:19.916012","exception":false,"start_time":"2022-01-10T16:34:19.784323","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-13T16:56:42.682067Z","iopub.execute_input":"2022-01-13T16:56:42.682328Z","iopub.status.idle":"2022-01-13T16:56:42.742959Z","shell.execute_reply.started":"2022-01-13T16:56:42.682293Z","shell.execute_reply":"2022-01-13T16:56:42.742145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['SeriousDlqin2yrs']","metadata":{"papermill":{"duration":0.126533,"end_time":"2022-01-10T16:34:20.104838","exception":false,"start_time":"2022-01-10T16:34:19.978305","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-01-13T16:56:42.744118Z","iopub.execute_input":"2022-01-13T16:56:42.745006Z","iopub.status.idle":"2022-01-13T16:56:42.750297Z","shell.execute_reply.started":"2022-01-13T16:56:42.744967Z","shell.execute_reply":"2022-01-13T16:56:42.748194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Prepare Data/Impute Missing Value</center></h3>\n\n\n## Prepare Data :","metadata":{"papermill":{"duration":0.091206,"end_time":"2022-01-10T16:34:20.289063","exception":false,"start_time":"2022-01-10T16:34:20.197857","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time \n###############################################################################\n#                        Impute Data                                          #\n###############################################################################\nparam =  {   \"verbosity\": 0,\n            #\"objective\": \"binary:logistic\",\n            #\"eval_metric\": \"auc\",\n            'random_state': 42,\n            # regression\n            'objective':'reg:squarederror', \n             'eval_metric': 'mae',\n            #early_stopping_rounds=100 ,\n            'gpu_id':0, \n            'predictor':\"gpu_predictor\",\n            # use exact for small dataset.\n            #\"tree_method\": \"exact\",\n            # big data :\n            'tree_method': 'gpu_hist',\n            # defines booster, gblinear for linear functions.\n             'booster': 'gbtree', \n            'lambda': 8.544792472633987e-07,\n            'alpha': 0.31141671752487043,\n            'subsample': 0.8779467596981366, \n            'colsample_bytree': 0.9759532762677546,\n            'learning_rate': 0.008686087328805853, \n            'n_estimators': 6988,\n            'max_depth': 9,\n            'min_child_weight': 2, \n            'eta': 3.7603213457541647e-06,\n            'gamma': 2.1478058456847449e-07,\n            'grow_policy': 'lossguide'}\n                \n\n#model_xgb = XGBRegressor(\n       #objective=\"mae\",\n #   **xgb_params2)\n\nnumeric_transformer1 = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                           # ('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\nnumeric_transformer2 = Pipeline(\n                            steps=[\n                            #('imputer', SimpleImputer(strategy='median'\n                             #                         ,add_indicator=True)),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                           # ('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\n\npipe_xgbr1 = Pipeline(\n                    steps=[\n                        ('preprocessor', numeric_transformer1),\n                        ('classifier', XGBRegressor(\n                      #objective=\"mae\",\n                       **param))\n                    ]\n                )\npipe_xgbr2 = Pipeline(\n                    steps=[\n                        ('preprocessor', numeric_transformer2),\n                        ('classifier', XGBRegressor(\n                      #objective=\"mae\",\n                       **param))\n                    ]\n                )\ntrain=train.drop(['Unnamed: 0','SeriousDlqin2yrs'], axis=1)\ntest=test.drop(['Unnamed: 0','SeriousDlqin2yrs'], axis=1)\ntrain_final= pd.concat( [train, test], axis=0) \n#testdf_income= train_final[train_final['MonthlyIncome'].isnull()==True]\ntraindf_income = train_final[train_final['MonthlyIncome'].isnull()==False]\ny_income = traindf_income['MonthlyIncome']\nX_income=traindf_income.drop([\"MonthlyIncome\"],axis=1)\npipe_xgbr1.fit(X_income, y_income)\ntrain_income_missing=train[train['MonthlyIncome'].isnull()==True].drop([\"MonthlyIncome\"],axis=1)\ntest_income_missing=test[test['MonthlyIncome'].isnull()==True].drop([\"MonthlyIncome\"],axis=1)\ntrain_predicted = pipe_xgbr1.predict(train_income_missing)\ntest_predicted = pipe_xgbr1.predict(test_income_missing)\ntrain.loc[(train.MonthlyIncome.isnull()), 'MonthlyIncome'] = train_predicted\ntest.loc[(test.MonthlyIncome.isnull()), 'MonthlyIncome'] = test_predicted","metadata":{"papermill":{"duration":167.120316,"end_time":"2022-01-10T16:37:07.501502","exception":false,"start_time":"2022-01-10T16:34:20.381186","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-13T16:56:42.752Z","iopub.execute_input":"2022-01-13T16:56:42.752334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \ntraindf_NumberOfDependents = train_final[train_final['NumberOfDependents'].isnull()==False]\ny_NumberOfDependents = traindf_NumberOfDependents['NumberOfDependents']\nX_NumberOfDependents=traindf_NumberOfDependents.drop([\"NumberOfDependents\"],axis=1)\npipe_xgbr2.fit(X_NumberOfDependents, y_NumberOfDependents)\ntrain_NumberOfDependents_missing=train[train['NumberOfDependents'].isnull()==True].drop([\"NumberOfDependents\"],axis=1)\ntest_NumberOfDependents_missing=test[test['NumberOfDependents'].isnull()==True].drop([\"NumberOfDependents\"],axis=1)\ntrain_predicted = pipe_xgbr2.predict(train_NumberOfDependents_missing)\ntest_predicted = pipe_xgbr2.predict(test_NumberOfDependents_missing)\ntrain.loc[(train.NumberOfDependents.isnull()), 'NumberOfDependents'] = train_predicted\ntest.loc[(test.NumberOfDependents.isnull()), 'NumberOfDependents'] = test_predicted","metadata":{"papermill":{"duration":198.301409,"end_time":"2022-01-10T16:40:25.857744","exception":false,"start_time":"2022-01-10T16:37:07.556335","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                        Add Bin Features                                     #\n###############################################################################\n\n# Add bin data \n# initializing append_str\nappend_str = 'cat_'\n# Append suffix / prefix to strings in list\nnum_features1=[\"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\",\"MonthlyIncome\"]\nnum_features2=[\"NumberOfDependents\",\n                       \"NumberOfTime60-89DaysPastDueNotWorse\",\n                       \"NumberRealEstateLoansOrLines\",\n                       \"NumberOfTimes90DaysLate\",\n                       \"NumberOfOpenCreditLinesAndLoans\",\n                       \"NumberOfTime30-59DaysPastDueNotWorse\",\n                       \"age\"]\ncat_features1 = [append_str + sub for sub in num_features1]\ncat_features2 = [append_str + sub for sub in num_features2]\n\n# create the discretizer object with strategy quantile and 1000 bins\ndiscretizer1 = KBinsDiscretizer(n_bins=40, encode='ordinal',strategy='quantile')\ndiscretizer2 = KBinsDiscretizer(n_bins=4, encode='ordinal',strategy='quantile')\n\npipeline1 = Pipeline([\n        ('imputer', SimpleImputer( strategy='median')),\n        ('bin', discretizer1)\n    ])\n# fit the discretizer to the train set\npipeline1.fit(train.loc[:,num_features1])\n# apply the discretisation\ntrain_cat1 = pipeline1.transform(train.loc[:,num_features1])\ntest_cat1 = pipeline1.transform(test.loc[:,num_features1])\ntrain_df1=pd.DataFrame(train_cat1,columns=cat_features1).astype('category')\ntest_df1=pd.DataFrame(test_cat1,columns=cat_features1).astype('category')\ntrain_final1= pd.concat( [train.loc[:,num_features1], train_df1], axis=1) \ntest_final1= pd.concat( [test.loc[:,num_features1], test_df1], axis=1) \n\npipeline2 = Pipeline([\n        ('imputer', SimpleImputer( strategy='median')),\n        ('bin', discretizer2)\n    ])\n# fit the discretizer to the train set\npipeline2.fit(train.loc[:,num_features2])\n# apply the discretisation\ntrain_cat2 = pipeline2.transform(train.loc[:,num_features2])\ntest_cat2 = pipeline2.transform(test.loc[:,num_features2])\ntrain_df2=pd.DataFrame(train_cat2,columns=cat_features2).astype('category')\ntest_df2=pd.DataFrame(test_cat2,columns=cat_features2).astype('category')\ntrain_final2= pd.concat( [train.loc[:,num_features2], train_df2], axis=1) \ntest_final2= pd.concat( [test.loc[:,num_features2], test_df2], axis=1) \ntrain_final= pd.concat( [train_final1, train_final2], axis=1) \ntest_final= pd.concat( [test_final1, test_final2], axis=1) ","metadata":{"papermill":{"duration":0.643247,"end_time":"2022-01-10T16:40:26.554121","exception":false,"start_time":"2022-01-10T16:40:25.910874","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                        Final X and Y                                    #\n###############################################################################\n\n# Pour le train test\ntarget= \"SeriousDlqin2yrs\"\nX = train_final# axis=1\nX_test_final =test_final# axis=1\ndel train\ndel test \ndel train_final\ndel test_final","metadata":{"papermill":{"duration":0.062288,"end_time":"2022-01-10T16:40:26.672431","exception":false,"start_time":"2022-01-10T16:40:26.610143","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                        Select Dtypes                                         #\n###############################################################################\n\n# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','int16','float16','float32','float64','int8']).columns\n###############################################################################\n#                        Select Dtypes                                         #\n###############################################################################\n\n# select the float columns\nnum_columns = X.select_dtypes(include=['int64','int16','float16','float32','float64','int8']).columns\n###############################################################################\n#                       Fe Class                                              #\n###############################################################################\n\nclass MiniKmeansTransformerEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, num_clusters = 11, encoder=ce.woe.WOEEncoder()):\n        self.num_clusters = num_clusters\n        self.encoder= encoder\n        if self.num_clusters > 0:\n            self.kmeans = MiniBatchKMeans(n_clusters=self.num_clusters, random_state=0)\n    \n    def fit(self, X, y=None):\n        if self.num_clusters > 0:\n            self.kmeans.fit(X)\n            preds=self.kmeans.predict(X)\n            preds=pd.DataFrame(preds, columns=['kmeans']).astype('category')\n            self.encoder.fit(preds,y)\n        return self\n    \n    def transform(self, X, y=None):\n        pred_classes = self.kmeans.predict(X)\n        pred_classes=pd.DataFrame(pred_classes, columns=['kmeans']).astype('category')\n        pred_encoded = self.encoder.transform(pred_classes)\n        return np.hstack((X, pred_encoded))\n        #return pred_encoded","metadata":{"papermill":{"duration":0.10194,"end_time":"2022-01-10T16:40:26.829128","exception":false,"start_time":"2022-01-10T16:40:26.727188","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                        Final Pipe                                          #\n###############################################################################\n\n#Define vnum pipeline\nnumeric_transformer_final = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            #('general_features',FeaturesEngineer()),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                            ('kmeans',MiniKmeansTransformerEncoder()),\n                            ('polynominal_features', PolynomialFeatures(degree=2)),\n                            ('reducedim',  SelectPercentile(f_classif,percentile=95)),\n                            #('scaler_minmax',MinMaxScaler())\n                            ]\n                            )\n# Features union cat + num \n# WOE+PowerTransformer\npreprocessor_woe_powertransformer_final = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer_final, num_columns),\n               # ('categorical', categorical_transformer, cat_columns)\n               \n            ])\npreprocessor_woe_powertransformer_final","metadata":{"papermill":{"duration":0.162544,"end_time":"2022-01-10T16:40:27.047225","exception":false,"start_time":"2022-01-10T16:40:26.884681","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pipe 2: \n# Cat pipeline\ncategorical_transformer2 = Pipeline(\n                    steps=[\n                        ('imputer', SimpleImputer(strategy='most_frequent',\n                                                  fill_value='missing',\n                                                  add_indicator=True)),\n                        ('encoder', ce.ordinal.OrdinalEncoder()),#(Numerical Input, Categorical Output)\n                        #('sparse_features', SparseInteractions(degree=2)),\n                        #('reducedim',  SelectPercentile( mutual_info_classif, percentile=90))\n\n                    ]\n                    ) \n#Define vnum pipeline\nnumeric_transformer2 = Pipeline(\n                            steps=[\n                            ('imputer', SimpleImputer(strategy='median'\n                                                      ,add_indicator=True)),\n                            #('general_features',FeaturesEngineer()),\n                            ('scaler', PowerTransformer()),#(Numerical Input, Numerical Output)\n                            # Create an SelectKBest object to select features with two best ANOVA F-Values\n                            #The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different\n                            ('polynominal_features', PolynomialFeatures(degree=2)),\n                            ('kmeans',MiniKmeansTransformerEncoder()),\n                            #('reducedim',  SelectPercentile(f_classif,percentile=90))\n                            ]\n                            )\n# Features union cat + num \n# WOE+PowerTransformer\npreprocessor_woe_powertransformer2 = ColumnTransformer(\n            transformers=[\n                ('numerical', numeric_transformer2, num_columns),\n                ('categorical', categorical_transformer2, cat_columns)\n            ])","metadata":{"papermill":{"duration":0.067408,"end_time":"2022-01-10T16:40:27.169608","exception":false,"start_time":"2022-01-10T16:40:27.1022","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n###############################################################################\n#                         Preprocess                                          #\n###############################################################################\npreprocessor_woe_powertransformer_final.fit(X,y)\nx_pre = preprocessor_woe_powertransformer_final.transform(X)\nx_test_final_pre = preprocessor_woe_powertransformer_final.transform(X_test_final)\n\n\n#preprocessor_woe_powertransformer2l.fit(X,y)\n#x_pre = preprocessor_woe_powertransformer2.transform(X)\n#x_test_final_pre = preprocessor_woe_powertransformer2.transform(X_test_final)\n\n#preprocessor_embd.fit(x_final)\n#x_pre =  preprocessor_embd.transform(x_final)\n#x_test_pre = preprocessor_embd.transform(x_test)\n\ngc.collect()","metadata":{"papermill":{"duration":6.473884,"end_time":"2022-01-10T16:40:33.698854","exception":false,"start_time":"2022-01-10T16:40:27.22497","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Sequential Model API (Simple)</center></h3>\n\n# More deep :Regularization in Deep Learning — L1, L2,BatchNormalization, and Dropout\n\n**Get Better Model Performance**\n\n**Reduce Overfitting With Dropout:**\nDropout is a clever regularization method that reduces overfitting of the training dataset and makes the model more robust.\n\nThis is achieved during training, where some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look like – and be treated like – a layer with a different number of nodes and connectivity to the prior layer.\n\nDropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\n\nYou can add dropout to your models as a new layer prior to the layer that you want to have input connections dropped-out.\n\nThis involves adding a layer called Dropout() that takes an argument that specifies the probability that each output from the previous to drop. E.g. 0.4 means 40% percent of inputs will be dropped each update to the model.\n\n**How to Accelerate Training With Batch Normalization**\n\nThe scale and distribution of inputs to a layer can greatly impact how easy or quickly that layer can be trained.\n\nThis is generally why it is a good idea to scale input data prior to modeling it with a neural network model.\n\nBatch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n\nYou can use batch normalization in your network by adding a batch normalization layer prior to the layer that you wish to have standardized inputs. You can use batch normalization with MLP, CNN, and RNN models.\n\n**How to Halt Training at the Right Time With Early Stopping**\n\nNeural networks are challenging to train. Too little training and the model is underfit; too much training and the model overfits the training dataset. Both cases result in a model that is less effective than it could be.\n\nOne approach to solving this problem is to use early stopping. This involves monitoring the loss on the training dataset and a validation dataset (a subset of the training set not used to fit the model). As soon as loss for the validation set starts to show signs of overfitting, the training process can be stopped.\n\nEarly stopping can be used with your model by first ensuring that you have a validation dataset. You can define the validation dataset manually via the validation_data argument to the fit() function, or you can use the validation_split and specify the amount of the training dataset to hold back for validation.\n\nYou can then define an EarlyStopping and instruct it on which performance measure to monitor, such as ‘val_loss‘ for loss on the validation dataset, and the number of epochs to observed overfitting before taking action, e.g. \n\nThis configured EarlyStopping callback can then be provided to the fit() function via the “callbacks” argument that takes a list of callbacks.\n\nThis allows you to set the number of epochs to a large number and be confident that training will end as soon as the model starts overfitting. You might also like to create a learning curve to discover more insights into the learning dynamics of the run and when training was halted.\n\n\n\n\n## Define model/more deep \n","metadata":{"papermill":{"duration":0.065858,"end_time":"2022-01-10T16:40:33.830472","exception":false,"start_time":"2022-01-10T16:40:33.764614","status":"completed"},"tags":[]}},{"cell_type":"code","source":"###############################################################################\n#                         Hardware configurations                             #\n###############################################################################\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)","metadata":{"papermill":{"duration":0.094737,"end_time":"2022-01-10T16:40:33.990824","exception":false,"start_time":"2022-01-10T16:40:33.896087","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()","metadata":{"papermill":{"duration":1.764787,"end_time":"2022-01-10T16:40:35.822631","exception":false,"start_time":"2022-01-10T16:40:34.057844","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.test.is_gpu_available()","metadata":{"papermill":{"duration":0.080655,"end_time":"2022-01-10T16:40:35.963974","exception":false,"start_time":"2022-01-10T16:40:35.883319","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","metadata":{"papermill":{"duration":0.075447,"end_time":"2022-01-10T16:40:36.101669","exception":false,"start_time":"2022-01-10T16:40:36.026222","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\nprint(tf.version.VERSION)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"papermill":{"duration":0.070086,"end_time":"2022-01-10T16:40:36.231683","exception":false,"start_time":"2022-01-10T16:40:36.161597","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                        Define DNN Architectur                               #\n###############################################################################\ndef simple_basic_dl():\n    with strategy.scope():\n        from tensorflow.keras import backend as K\n        # determine the number of input features\n        n_features = x_pre.shape[1]\n        # define model\n        # Create model here\n        # DeepNN\n        ### layer input\n        inputs = layers.Input(name=\"input\", shape=(n_features,))\n        ### hidden layer 1\n        h1 = layers.Dense(name=\"h1\", units=int(round((n_features+1)/2)),\n                          activation='relu')(inputs)\n        h1 = layers.Dropout(name=\"drop1\", rate=0.2)(h1)\n        ### hidden layer 2\n        h2 = layers.Dense(name=\"h2\", units=int(round((n_features+1)/4)),\n                          activation='relu')(h1)\n        h2 = layers.Dropout(name=\"drop2\", rate=0.2)(h2)\n        ### layer output\n        outputs = layers.Dense(name=\"output\", units=1, activation='sigmoid')(h2)\n        model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name=\"DeepNN\")\n        # Compile model here\n        # define metrics\n        def Recall(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n            recall = true_positives / (possible_positives + K.epsilon())\n            return recall\n\n        def Precision(y_true, y_pred):\n            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n            precision = true_positives / (predicted_positives + K.epsilon())\n            return precision\n\n        def F1(y_true, y_pred):\n            precision = Precision(y_true, y_pred)\n            recall = Recall(y_true, y_pred)\n            return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\n        # compile the neural network\n        # https://towardsdatascience.com/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0\n        model.compile(optimizer='adam', loss='binary_crossentropy', \n                        #metrics=['accuracy',F1])\n                         metrics=[AUC(name='auc')])\n    return model\n#######################################################################\ndef generate_baseline1():\n    with strategy.scope():\n        BATCH_SIZE=256\n        SHUFFLE_BUFFER_SIZE = 256\n        N_FOLD = 10\n        EPOCH = 30\n        LR = 5e-4\n        DECAY_STEP = 3000\n        DECAY_RATE =0.9\n        QUANT = 256\n        BINS = 256\n        # determine the number of input features\n        n_features = x_pre.shape[1]\n        # define model\n        # Create model here\n        model =tf.keras.Sequential()\n        model.add(layers.Dense(20,  kernel_initializer='he_normal',\n                               input_shape=(n_features,), \n                               activation = 'relu')) # Rectified Linear Unit Activation Function\n        model.add(layers.Dense(10, activation = 'relu'))\n        model.add(layers.Dense(1, activation='sigmoid')) # Sigmoid for binary classifcation  # linear for regression \n        # Compile model here\n        lr_schedule =tf. keras.optimizers.schedules.ExponentialDecay(\n                initial_learning_rate=LR,\n                decay_steps = DECAY_STEP,\n                decay_rate= DECAY_RATE)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n        optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-5)\n        model.compile(loss='binary_crossentropy',\\\n                               metrics=[AUC(name='auc')],\n                               optimizer= optimizer_adam)\n    return model\n#############################################################\ndef generate_baseline2():\n    with strategy.scope():\n        BATCH_SIZE=256\n        SHUFFLE_BUFFER_SIZE = 256\n        N_FOLD = 10\n        EPOCH = 30\n        LR = 5e-4\n        DECAY_STEP = 3000\n        DECAY_RATE =0.9\n        QUANT = 256\n        BINS = 256\n        # determine the number of input features\n        n_features = x_pre.shape[1]\n        # define model\n        # Create model here\n        model_reg =tf.keras.Sequential()\n        model_reg.add(layers.Dense(60, activation = 'relu',\n                                    input_shape=(n_features,), \n                                   kernel_initializer='he_normal'))\n        model_reg.add(layers.BatchNormalization())\n        model_reg.add(layers.Dropout(0.2))\n        model_reg.add(layers.Dense(30, activation = 'relu',kernel_initializer='he_normal'))\n        model_reg.add(layers.Dense(15, activation = 'relu',kernel_initializer='he_normal'))\n        model_reg.add(layers.Dense(15, activation = 'relu',kernel_initializer='he_normal'))\n        model_reg.add(layers.Dense(1, activation='sigmoid')) # Sigmoid for binary classifcation  # linear for regression \n        # Compile model here\n        lr_schedule =tf. keras.optimizers.schedules.ExponentialDecay(\n                initial_learning_rate=LR,\n                decay_steps = DECAY_STEP,\n                decay_rate= DECAY_RATE)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n        optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-7)\n        model_reg.compile(loss='binary_crossentropy',\\\n                               metrics=[AUC(name='auc')],\n                               optimizer= optimizer_adam)\n        \n    return model_reg\n\n","metadata":{"papermill":{"duration":0.105265,"end_time":"2022-01-10T16:40:36.396659","exception":false,"start_time":"2022-01-10T16:40:36.291394","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                         Define and compile model                            #\n###############################################################################\nmodel_reg=generate_baseline2()\nsimple_dl =simple_basic_dl()\n#model_reg.summary()\nsimple_dl.summary()","metadata":{"papermill":{"duration":0.290443,"end_time":"2022-01-10T16:40:36.747468","exception":false,"start_time":"2022-01-10T16:40:36.457025","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                         Visualization                                       #\n###############################################################################\n\ntf.keras.utils.plot_model(model=simple_dl, show_shapes=True, dpi=76, )","metadata":{"papermill":{"duration":1.11503,"end_time":"2022-01-10T16:40:37.92604","exception":false,"start_time":"2022-01-10T16:40:36.81101","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                         Visualization                                       #\n###############################################################################\n'''\nExtract info for each layer in a keras model.\n'''\ndef utils_nn_config(model):\n    lst_layers = []\n    if \"Sequential\" in str(model): #-> Sequential doesn't show the input layer\n        layer = model.layers[0]\n        lst_layers.append({\"name\":\"input\", \"in\":int(layer.input.shape[-1]), \"neurons\":0, \n                           \"out\":int(layer.input.shape[-1]), \"activation\":None,\n                           \"params\":0, \"bias\":0})\n    for layer in model.layers:\n        try:\n            dic_layer = {\"name\":layer.name, \"in\":int(layer.input.shape[-1]), \"neurons\":layer.units, \n                         \"out\":int(layer.output.shape[-1]), \"activation\":layer.get_config()[\"activation\"],\n                         \"params\":layer.get_weights()[0], \"bias\":layer.get_weights()[1]}\n        except:\n            dic_layer = {\"name\":layer.name, \"in\":int(layer.input.shape[-1]), \"neurons\":0, \n                         \"out\":int(layer.output.shape[-1]), \"activation\":None,\n                         \"params\":0, \"bias\":0}\n        lst_layers.append(dic_layer)\n    return lst_layers\n\n\n\n'''\nPlot the structure of a keras neural network.\n'''\ndef visualize_nn(model, description=False, figsize=(10,8)):\n    ## get layers info\n    lst_layers = utils_nn_config(model)\n    layer_sizes = [layer[\"out\"] for layer in lst_layers]\n    \n    ## fig setup\n    fig = plt.figure(figsize=figsize)\n    ax = fig.gca()\n    ax.set(title=model.name)\n    ax.axis('off')\n    left, right, bottom, top = 0.1, 0.9, 0.1, 0.9\n    x_space = (right-left) / float(len(layer_sizes)-1)\n    y_space = (top-bottom) / float(max(layer_sizes))\n    p = 0.025\n    \n    ## nodes\n    for i,n in enumerate(layer_sizes):\n        top_on_layer = y_space*(n-1)/2.0 + (top+bottom)/2.0\n        layer = lst_layers[i]\n        color = \"green\" if i in [0, len(layer_sizes)-1] else \"blue\"\n        color = \"red\" if (layer['neurons'] == 0) and (i > 0) else color\n        \n        ### add description\n        if (description is True):\n            d = i if i == 0 else i-0.5\n            if layer['activation'] is None:\n                plt.text(x=left+d*x_space, y=top, fontsize=10, color=color, s=layer[\"name\"].upper())\n            else:\n                plt.text(x=left+d*x_space, y=top, fontsize=10, color=color, s=layer[\"name\"].upper())\n                plt.text(x=left+d*x_space, y=top-p, fontsize=10, color=color, s=layer['activation']+\" (\")\n                plt.text(x=left+d*x_space, y=top-2*p, fontsize=10, color=color, s=\"Σ\"+str(layer['in'])+\"[X*w]+b\")\n                out = \" Y\"  if i == len(layer_sizes)-1 else \" out\"\n                plt.text(x=left+d*x_space, y=top-3*p, fontsize=10, color=color, s=\") = \"+str(layer['neurons'])+out)\n        \n        ### circles\n        for m in range(n):\n            color = \"limegreen\" if color == \"green\" else color\n            circle = plt.Circle(xy=(left+i*x_space, top_on_layer-m*y_space-4*p), radius=y_space/4.0, color=color, ec='k', zorder=4)\n            ax.add_artist(circle)\n            \n            ### add text\n            if i == 0:\n                plt.text(x=left-4*p, y=top_on_layer-m*y_space-4*p, fontsize=10, s=r'$X_{'+str(m+1)+'}$')\n            elif i == len(layer_sizes)-1:\n                plt.text(x=right+4*p, y=top_on_layer-m*y_space-4*p, fontsize=10, s=r'$y_{'+str(m+1)+'}$')\n            else:\n                plt.text(x=left+i*x_space+p, y=top_on_layer-m*y_space+(y_space/8.+0.01*y_space)-4*p, fontsize=10, s=r'$H_{'+str(m+1)+'}$')\n    \n    ## links\n    for i, (n_a, n_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n        layer = lst_layers[i+1]\n        color = \"green\" if i == len(layer_sizes)-2 else \"blue\"\n        color = \"red\" if layer['neurons'] == 0 else color\n        layer_top_a = y_space*(n_a-1)/2. + (top+bottom)/2. -4*p\n        layer_top_b = y_space*(n_b-1)/2. + (top+bottom)/2. -4*p\n        for m in range(n_a):\n            for o in range(n_b):\n                line = plt.Line2D([i*x_space+left, (i+1)*x_space+left], \n                                  [layer_top_a-m*y_space, layer_top_b-o*y_space], \n                                  c=color, alpha=0.5)\n                if layer['activation'] is None:\n                    if o == m:\n                        ax.add_artist(line)\n                else:\n                    ax.add_artist(line)\n    plt.show()\n    ##########################################\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['auc']\n    val_acc = history.history['val_auc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training auc')\n    plt.plot(x, val_acc, 'r', label='Validation auc')\n    plt.title('Training and validation MAE')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    \n#plot_history(history)","metadata":{"papermill":{"duration":0.104123,"end_time":"2022-01-10T16:40:38.091564","exception":false,"start_time":"2022-01-10T16:40:37.987441","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_nn(simple_dl, description=True, figsize=(10,8))","metadata":{"papermill":{"duration":11.540557,"end_time":"2022-01-10T16:40:49.691787","exception":false,"start_time":"2022-01-10T16:40:38.15123","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr_callback(epoch,lr):\n    lr_start   = 0.00001\n    lr_max     = 0.01#0.00000125 * 1 * batch_size\n    lr_min     = 0.0001\n    lr_ramp_ep = 2\n    lr_sus_ep  = 2\n    lr_decay   = 0.8\n    \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    \n    return lrfn(epoch)\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(get_lr_callback, verbose=True)\nx = [x for x in range(1000)]\nplt.plot(x,[get_lr_callback(x,.1) for x in x])\nplt.show()","metadata":{"papermill":{"duration":0.299891,"end_time":"2022-01-10T16:41:00.702173","exception":false,"start_time":"2022-01-10T16:41:00.402282","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###############################################################################\n#                         Configuration                                       #\n###############################################################################\nEPOCHS =1000\n# configure early stopping\n# es = EarlyStopping(monitor='val_loss',mode='auto',patience=20,#,min_delta=0.000001, restore_best_weights=True)\n\nes = EarlyStopping(monitor='val_loss',mode='min', \n                   patience=15, #,min_delta=0.000001,\n                   restore_best_weights=True)\n#filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n# define the checkpoint\nfilepath = \"model.hdf5\"\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, \n                             monitor='val_auc',\n                             verbose=1, \n                            #save_weights_only=True,\n                             save_best_only=True,\n                             mode='max')\n###############################################################################\n#                        Learning Rate Scheduler                              #\n###############################################################################\n\n# set learning rate scheduler\n# we can chage learning rate during learning\ndef lr_schedul(epoch):\n    x = 0.0001\n    if epoch >= 20:\n        x= 0.00001\n    if epoch >= 40:\n        x = 0.00001\n    if epoch >= 60:\n        x = 0.000001\n    if epoch >= 80:\n        x = 0.0000001\n    if epoch >= 100:\n        x = 0.00000001\n    if epoch >= 120:\n        x = 0.000000001        \n    return x\n\nlr_decay = LearningRateScheduler(\n    lr_schedul,\n    verbose=1,\n)\n###############################################################################\n#                         Train  model                               #\n###############################################################################\n\n#batch_size=1000\n# fit model using our gpu\n# fit model using our gpu\nwith strategy.scope():\n#with tf.device('/gpu:0'):\n    history = model_reg.fit(x_pre,y,batch_size=64,epochs=EPOCHS, \n                        validation_split = 0.1,\n                        verbose=1 ,\n                        # callbacks=[lr_callback ,es,checkpoint])\n                        callbacks=[lr_decay ,es,checkpoint])\n                        #callbacks=[es,checkpoint])\n    \n#with strategy.scope():\n#with tf.device('/gpu:0'):\n #       history = model_emb.fit(input_dict,y,batch_size=64,epochs=EPOCHS, \n  #                       validation_split = 0.1,\n   #                      verbose=1 ,\n    #                     callbacks=[lr_decay ,es,checkpoint],shuffle=True) \ngc.collect()    ","metadata":{"papermill":{"duration":384.571237,"end_time":"2022-01-10T16:47:25.342449","exception":false,"start_time":"2022-01-10T16:41:00.771212","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"papermill":{"duration":4.066554,"end_time":"2022-01-10T16:47:31.905871","exception":false,"start_time":"2022-01-10T16:47:27.839317","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate the keras model\nmodel = tf.keras.models.load_model(filepath)\n# evaluate the keras model\nloss, auc = model.evaluate( x_pre ,y, verbose=2)\n#loss, auc = model.evaluate( input_dict ,y, verbose=2)","metadata":{"papermill":{"duration":15.403069,"end_time":"2022-01-10T16:47:49.660658","exception":false,"start_time":"2022-01-10T16:47:34.257589","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"auc:{}\".format(auc))\nprint(\"loss:{}\".format(loss))","metadata":{"papermill":{"duration":2.359944,"end_time":"2022-01-10T16:47:54.725503","exception":false,"start_time":"2022-01-10T16:47:52.365559","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explain DL ","metadata":{"papermill":{"duration":2.370474,"end_time":"2022-01-10T16:47:59.431228","exception":false,"start_time":"2022-01-10T16:47:57.060754","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''\nUse shap to build an a explainer.\n:parameter\n    :param model: model instance (after fitting)\n    :param X_names: list\n    :param X_instance: array of size n x 1 (n,)\n    :param X_train: array - if None the model is simple machine learning, if not None then it's a deep learning model\n    :param task: string - \"classification\", \"regression\"\n    :param top: num - top features to display\n:return\n    dtf with explanations\n'''\ndef explainer_shap(model, X_names, X_instance, X_train=None, task=\"classification\", top=10):\n    ## create explainer\n    ### machine learning\n    if X_train is None:\n        explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(X_instance)\n    ### deep learning\n    else:\n        explainer = shap.DeepExplainer(model, data=X_train[:100])\n        shap_values = explainer.shap_values(X_instance.reshape(1,-1))[0].reshape(-1)\n\n    ## plot\n    ### classification\n    if task == \"classification\":\n        shap.decision_plot(explainer.expected_value, shap_values, link='logit', feature_order='importance',\n                           features=X_instance, feature_names=X_names, feature_display_range=slice(-1,-top-1,-1))\n    ### regression\n    else:\n        shap.waterfall_plot(explainer.expected_value[0], shap_values, \n                            features=X_instance, feature_names=X_names, max_display=top)","metadata":{"papermill":{"duration":3.324314,"end_time":"2022-01-10T16:48:05.092769","exception":false,"start_time":"2022-01-10T16:48:01.768455","status":"completed"},"tags":[],"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nlist_feature_names =['num_'+str(i) for i in range(int(x_pre.shape[1])) ]\n\n#explainer_shap(model, X_names=list_feature_names, X_instance=x_pre[i], X_train=x_pre, \n #              task=\"classification\", #task=\"regression\"\n  #             top=10)","metadata":{"papermill":{"duration":3.83009,"end_time":"2022-01-10T16:48:11.263719","exception":false,"start_time":"2022-01-10T16:48:07.433629","status":"completed"},"tags":[],"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Unseen data : ","metadata":{"papermill":{"duration":2.37211,"end_time":"2022-01-10T16:48:16.332271","exception":false,"start_time":"2022-01-10T16:48:13.960161","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#predictions_final = model.predict(test_dict)\npredictions = model.predict(x_test_final_pre)","metadata":{"papermill":{"duration":4.959223,"end_time":"2022-01-10T16:48:23.69237","exception":false,"start_time":"2022-01-10T16:48:18.733147","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the predictions to a CSV file\nsub = pd.read_csv('../input/GiveMeSomeCredit/sampleEntry.csv')\nsub['Probability']=predictions\nsub.to_csv('StackingAveragedModelsBin22.csv', index=False)\nsub","metadata":{"papermill":{"duration":2.776558,"end_time":"2022-01-10T16:48:29.205443","exception":false,"start_time":"2022-01-10T16:48:26.428885","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summuray \n\nWe tried  encoding tecthniques in order to have better results \nAs is always the case with neural networks, there are a huge number of possible architectures and parameters to play around with. Possible interesting avenues include:\n\n    adding / removing / widening / shortening layers in the network\n    adjusting the regularization layers / parameters (dropout / l2)\n    adding new regularization parameters\n    changing activation functions\n    changing the gradient descent optimizer\n    adjusting the learning rate and no of epochs\n    etc, etc.....\n\nWith some tinkering, you'll find that the networks performance can quickly approach that of the aforementioned popular models. Neural networks like this also offer a huge amount of flexibility which can offer some really promising avenues for improving your scores.\n\nNote: you can dramatically improve the training time of the model by increasing the batch size and utilizing a GPU session!\n\nreference : \nto try : \ndeep + kfold : \n\nhttps://www.kaggle.com/lucamassaron/deep-learning-for-tabular-data\n\n\n\nhttps://mmuratarat.github.io/2019-06-12/embeddings-with-numeric-variables-Keras\n\nhttps://www.slideshare.net/MeetupDataScienceRoma/deep-learning-for-tabular-data-luca-massaron\n\nhttps://medium.com/analytics-vidhya/tensorflow-2-tutorial-on-categorical-features-embedding-93dd81027ea9\n\nhttps://www.kaggle.com/mtinti/keras-starter-with-bagging-1111-84364\n\nhttps://www.kaggle.com/faressayah/tensorflow-2-tutorial-get-started-in-deep-learning\n\nhttps://www.kaggle.com/colinmorris/embedding-layers\n\nhttps://www.kaggle.com/dustyturner/dense-nn-with-categorical-embeddings\n\nwrap keras regressor : \n\nhttps://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n\n\n\n\n\ncat embedding : \n\nhttps://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n\n# Reference  : \nhttps://www.kaggle.com/lucamassaron/deep-learning-for-tabular-data\n\nhttps://www.kaggle.com/lukaszborecki/tps-09-nn\n\nhttps://www.kaggle.com/siavrez/kerasembeddings\n\n\nThings to try : \n\nhttps://www.kaggle.com/datafan07/top-1-approach-eda-new-models-and-stacking\n\n\nDeep  : \nhttps://www.kaggle.com/shivansh002/tame-your-neural-network-once-for-all\n\nhttps://www.kaggle.com/lukaszborecki/tps-09-nn/\n\nhttps://www.kaggle.com/bannourchaker/10-deeplearning-embedding-rnn-tf-keras\n\nhttps://www.kaggle.com/bannourchaker/single-nn/edit\n\nhttps://www.kaggle.com/bannourchaker/deep-learning-starter-gpu\n\nlearing rate adaptative vs scheulde : \n\nhttps://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n\nBest mine : \n\n\nhttps://www.kaggle.com/deepchaka/deep-learning-ventilator-time-series-regress/edit/run/78653879\n\nhttps://www.kaggle.com/deepchaka/deeplearning2/edit/run/78470709\n\n\nhttps://www.kaggle.com/deepchaka/deeplearning2/edit/run/78470709https://www.kaggle.com/deepchaka/deeplearning2/edit/run/78470709\n\nhttps://www.kaggle.com/deepchaka/deep-lstm-test-kaggle-quantile-cat-freduced-tpu/edit/run/78738587\n\nhttps://www.kaggle.com/deepchaka/deep-lstm-test-kaggle-quantile-cat-freduced-tpu/edit/run/78738587\n\n\n\n","metadata":{"papermill":{"duration":2.333327,"end_time":"2022-01-10T16:48:33.871754","exception":false,"start_time":"2022-01-10T16:48:31.538427","status":"completed"},"tags":[]}}]}