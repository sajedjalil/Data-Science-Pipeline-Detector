{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GIVE ME SOME CREDIT\n\nThis notebook is created 9 years after this competition ended. The main aim of this project is to predict the probabily whether a customer will default in the future given his record present in the dataset. We will be using **predict_proba** to determine the delinquency probabilities of the customer.\n\nThe Highlights of the notebook are:\n\n- **Exploratory Data Analysis**\n    - **Outlier Analysis\n    - **Null Handling\n    - **Distribution Analysis\n    - **Skewness Reduction (using Box Cox Transformation)\n- **Feature Engineering**\n- **LightGBM using RandomizedSearchCV (Classification)**\n    - **Evaluation Metrics**\n        - Mean Squared Error\n        - Root Mean Squared Error\n        - Mean Absolute Error\n        - Mean Squared Logarithmic Error\n        - Root Mean Square Logarithmic Error\n        - Accuracy on Training Set\n        - Accuracy on Test Set\n        - F-Beta Score (Beta = 2)\n        - F1 Score\n        - Precision\n        - Recall\n        - Confusion Matrix\n        - AUC Curve\n    - **Probability Prediction on Validation Sets**\n    - **Delinquency Prediction on Validation Sets**\n    - **Feature Importances**\n        - Summary Plot\n        - SHAP Analysis\n- **XGBoost using RandomizedSearchCV (Classification)**\n    - **Evaluation Metrics**\n        - Mean Squared Error\n        - Root Mean Squared Error\n        - Mean Absolute Error\n        - Mean Squared Logarithmic Error\n        - Root Mean Square Logarithmic Error\n        - Accuracy on Training Set\n        - Accuracy on Test Set\n        - F-Beta Score (Beta = 2)\n        - F1 Score\n        - Precision\n        - Recall\n        - Confusion Matrix\n        - AUC Curve\n    - **Probability Prediction on Validation Sets**\n    - **Delinquency Prediction on Validation Sets**\n    - **Feature Importances**\n        - Summary Plot\n        - SHAP Analysis\n        \n        \nLet's begin with importing the libraries we will be requiring for this notebook","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import ensemble\nfrom sklearn import tree\nfrom sklearn import linear_model\nimport os, datetime, sys, random, time\nimport seaborn as sns\nimport xgboost as xgs\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom mlxtend import classifier\nplt.style.use('fivethirtyeight')\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats, special\nimport shap\nimport catboost as ctb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainingData = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-training.csv')\ntestData = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLet's first try to identify the column by column datatypes and null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the observations are:\n\n- There are 150,000 rows for 11 features in our data.\n- We see in the training data, that all the datatypes belong to a numeric class i.e. **int** and **float**.\n- Columns **MonthlyIncome** and **NumberOfDependents** have some null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainingData.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here we can conclude that the column **Unnamed: 0** will have no significance in the predictive modelling because it represents ID of the customer,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trainingData.shape)\nprint(testData.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing similar analysis on the Test Data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"testData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testData.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the observations on the testing data:\n\n- The total rows for our 11 features are 101,503. \n- Like the Training Data (as it should be), we observe numeric class's datatypes i.e. **int** and **float**.\n- Nulls were observed for features **MonthlyIncome** and **NumberOfDependents** just like the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"testData.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a copy of our two datasets, so the changes we are gonna make forward does not affect the original data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain = trainingData.copy()\nfinalTest = testData.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, we need to predict the probability of Delinquency in the test data, we need to remove the additional column from it first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTest.drop('SeriousDlqin2yrs', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also as mentioned above, let's take the ID column i.e. **Unnamed: 0** and store it in seperate variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainID = finalTrain['Unnamed: 0']\ntestID = finalTest['Unnamed: 0']\n\nfinalTrain.drop('Unnamed: 0', axis=1, inplace=True)\nfinalTest.drop('Unnamed: 0', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imbalance Ratio\n\nSince we have a total data of 150,000. There are high chances that it can be an imbalanced dataset. Therefore, checking the positive and negative delinquency ratio.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(12,6))\nfinalTrain['SeriousDlqin2yrs'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=axes[0])\naxes[0].set_title('SeriousDlqin2yrs')\n#ax[0].set_ylabel('')\nsns.countplot('SeriousDlqin2yrs',data=finalTrain,ax=axes[1])\naxes[1].set_title('SeriousDlqin2yrs')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ratio of negative to positive delinquency outliers are found to be 93.3% to 6.7%, which is approximately a ratio of 14:1. Therefore, our dataset is highly imbalanced. We cannot rely on the accuracy scores to predict the model's success. Many other evaluation metrics would be considered here. But more on that later.\n\nNow let's move on the Outlier Analysis section of our EDA. Here we will remove potential outliers which might affect our predictive modelling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Outlier Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[30,30])\nfor col,i in zip(finalTrain.columns,range(1,13)):\n    axes = fig.add_subplot(7,2,i)\n    sns.regplot(finalTrain[col],finalTrain.SeriousDlqin2yrs,ax=axes)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graphs we can observe:\n\n- In the columns **NumberOfTime30-59DaysPastDueNotWorse** , **NumberOfTime60-89DaysPastDueNotWorse** and **NumberOfTimes90DaysLate**, we see delinquency range beyond 90 which is common across all 3 features.\n- There are some unusually high values for **DebtRatio** and **RevolvingUtilizationOfUnsecuredLines**.\n\nStep 1: Fixing the columns **NumberOfTime30-59DaysPastDueNotWorse** , **NumberOfTime60-89DaysPastDueNotWorse** and **NumberOfTimes90DaysLate**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in '30-59 Days' values that are more than or equal to 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                       ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                    ['NumberOfTimes90DaysLate']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                           ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                        ['NumberOfTimes90DaysLate']))\n\n\nprint(\"Proportion of positive class with special 96/98 values:\",\n      round(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]['SeriousDlqin2yrs'].sum()*100/\n      len(finalTrain[finalTrain['NumberOfTime30-59DaysPastDueNotWorse']>=90]['SeriousDlqin2yrs']),2),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the following that when records in column 'NumberOfTime30-59DaysPastDueNotWorse' are more than 90, the other columns that records number of times payments are past due X days also have the same values. We will classify these as special labels since the proportion of positive class is abnormally high at 54.65%.\n\nThese 96 and 98 values can be viewed as accounting errors. Hence, we would replace them with the maximum value before 96 i.e. 13, 11 and 17","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain.loc[finalTrain['NumberOfTime30-59DaysPastDueNotWorse'] >= 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 13\nfinalTrain.loc[finalTrain['NumberOfTime60-89DaysPastDueNotWorse'] >= 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 11\nfinalTrain.loc[finalTrain['NumberOfTimes90DaysLate'] >= 90, 'NumberOfTimes90DaysLate'] = 17","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in 30-59Days\", np.unique(finalTrain['NumberOfTime30-59DaysPastDueNotWorse']))\nprint(\"Unique values in 60-89Days\", np.unique(finalTrain['NumberOfTime60-89DaysPastDueNotWorse']))\nprint(\"Unique values in 90Days\", np.unique(finalTrain['NumberOfTimes90DaysLate']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performing a similar analysis on the Test Set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in '30-59 Days' values that are more than or equal to 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                       ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n                                                                                                    ['NumberOfTimes90DaysLate']))\n\n\nprint(\"Unique values in '60-89 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                           ['NumberOfTime60-89DaysPastDueNotWorse']))\n\n\nprint(\"Unique values in '90 Days' when '30-59 Days' values are less than 90:\",np.unique(finalTest[finalTest['NumberOfTime30-59DaysPastDueNotWorse']<90]\n                                                                                        ['NumberOfTimes90DaysLate']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, these values exist in Test Set as well. Therefore, replacing them with maximum values before 96 and 98 i.e. 19, 9 and 18.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTest.loc[finalTest['NumberOfTime30-59DaysPastDueNotWorse'] >= 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 19\nfinalTest.loc[finalTest['NumberOfTime60-89DaysPastDueNotWorse'] >= 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 9\nfinalTest.loc[finalTest['NumberOfTimes90DaysLate'] >= 90, 'NumberOfTimes90DaysLate'] = 18\n\nprint(\"Unique values in 30-59Days\", np.unique(finalTest['NumberOfTime30-59DaysPastDueNotWorse']))\nprint(\"Unique values in 60-89Days\", np.unique(finalTest['NumberOfTime60-89DaysPastDueNotWorse']))\nprint(\"Unique values in 90Days\", np.unique(finalTest['NumberOfTimes90DaysLate']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2: Checking for **DebtRatio** and **RevolvingUtilizationOfUnsecuredLines**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Debt Ratio: \\n',finalTrain['DebtRatio'].describe())\nprint('\\nRevolving Utilization of Unsecured Lines: \\n',finalTrain['RevolvingUtilizationOfUnsecuredLines'].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see a massive difference between the 75th Quantile and the Max Value. Let's explore this in a greater depth.\n\n**Debt Ratio**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"quantiles = [0.75,0.8,0.81,0.85,0.9,0.95,0.975,0.99]\n\nfor i in quantiles:\n    print(i*100,'% quantile of debt ratio is: ',finalTrain.DebtRatio.quantile(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there is a huge rise in quantile post 81%. So, our main aim would be to check the potential outliers beyond 81% quantiles. However, since our data is 150,000, let's consider 95% and 97.5% quantiles for our further analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain[finalTrain['DebtRatio'] >= finalTrain['DebtRatio'].quantile(0.95)][['SeriousDlqin2yrs','MonthlyIncome']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can observe:\n\n- Out of 7501 customers who have debt ratio greater than 95% i.e. the number of times their debt is higher than their income, only 379 have Monthly Income values.\n- The Max for Monthly Income is 1 and Min is 0 which makes us wonder that are data entry errors. Let's check whether the Serious Delinquency in 2 years and Monthly Income values are equal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain[(finalTrain[\"DebtRatio\"] > finalTrain[\"DebtRatio\"].quantile(0.95)) & (finalTrain['SeriousDlqin2yrs'] == finalTrain['MonthlyIncome'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, our suspects are true and there are 331 out of 379 rows where Monthly Income is equal to the Serious Delinquencies in 2 years. Hence we will remove these 331 outliers from our analysis as their current values aren't useful for our predictive modelling and will add to the bias and variance.\n\nThe reason behind this, is we have 331 rows where the debt ratio is massive compared to the customer's income and they arent't scrutinized for defaulting which is nothing but a data entry error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain = finalTrain[-((finalTrain[\"DebtRatio\"] > finalTrain[\"DebtRatio\"].quantile(0.95)) & (finalTrain['SeriousDlqin2yrs'] == finalTrain['MonthlyIncome']))]\nfinalTrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Revolving Utilization of Unsecured Lines**\n\nThis field basically represents the ratio of the amount owed by the credit limit of a customer. A ratio higher than 1 is considered to be a serious defaulter. A Ratio of 10 functionally also seems possible, let's see how many of these customers have the Revolving Utilization of Unsecured Lines greater than 10.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain[finalTrain['RevolvingUtilizationOfUnsecuredLines']>10].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here if you see the difference between the 50th and 75 quantile for Revolving Utilization of Unsecured Lines, you'll observe that there is a massive increase from 13 to 1891.25. Since 13 seems like a reasonable ratio too (but way too high), let's check how many of these counts lie above 13.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain[finalTrain['RevolvingUtilizationOfUnsecuredLines']>13].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite owing thousands, these 238 people do not show any default which means this might be another error. Even if it is not an error, these numbers will add huge bias and variance to our final predictions. Therefore, the best decision is to remove these values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain = finalTrain[finalTrain['RevolvingUtilizationOfUnsecuredLines']<=13]\nfinalTrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The outliers are now handled. Next, we will move on to handling the missing data, as we observed at the start of this notebook that MonthlyIncome and NumberOfDependents had null values.\n\n### Null Handling\n\n- Since MonthlyIncome is an integer value, we will replace the nulls with the median values.\n- Number of Dependents can be characterized as a categorical variable, hence if customers have NA for number of dependents, it means that they do not have any dependents. Therefore, we fill them with zeros.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def MissingHandler(df):\n    DataMissing = df.isnull().sum()*100/len(df)\n    DataMissingByColumn = pd.DataFrame({'Percentage Nulls':DataMissing})\n    DataMissingByColumn.sort_values(by='Percentage Nulls',ascending=False,inplace=True)\n    return DataMissingByColumn[DataMissingByColumn['Percentage Nulls']>0]\n\nMissingHandler(finalTrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, we have 19.76% and 2.59% Nulls for MonthlyIncome and NumberOfDependents respectively. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTrain['MonthlyIncome'].fillna(finalTrain['MonthlyIncome'].median(), inplace=True)\nfinalTrain['NumberOfDependents'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rechecking Nulls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MissingHandler(finalTrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying Similar Analysis for the Testing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MissingHandler(finalTest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to the training data, we have 19.71% and 2.56% nulls for MonthlyIncome and NumberOfDependents respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"finalTest['MonthlyIncome'].fillna(finalTrain['MonthlyIncome'].median(), inplace=True)\nfinalTest['NumberOfDependents'].fillna(0, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rechecking Nulls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MissingHandler(finalTest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(finalTrain.shape)\nprint(finalTest.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Additional EDA\n\nLet's study a few more things about the dataset to get more familiar with it.\n\n**CORRELATION MATRIX**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = [15,10])\nmask = np.zeros_like(finalTrain.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(finalTrain.corr(), cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9), mask = mask, annot=True, center = 0)\nplt.title(\"Correlation Matrix (HeatMap)\", fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the correlation heatmap above, we can see the most correlated values to **SeriousDlqin2yrs** are **NumberOfTime30-59DaysPastDueNotWorse** , **NumberOfTime60-89DaysPastDueNotWorse** and **NumberOfTimes90DaysLate**.\n\nNow let's move to the Feature Engineering section of our Notebook\n\n# Feature Engineering\n\nLet's first combine the train and test sets to add features on both the data and conduct further analyses. We will split them later before Model Testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SeriousDlqIn2Yrs = finalTrain['SeriousDlqin2yrs']\n\nfinalTrain.drop('SeriousDlqin2yrs', axis = 1 , inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalData = pd.concat([finalTrain, finalTest])\n\nfinalData.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding some new features:\n\n- **MonthlyIncomePerPerson**: Monthly Income divided by the number of dependents\n\n- **MonthlyDebt**: Monthly Income multiplied by the Debt Ratio\n\n- **isRetired**: Person whose monthly income is 0 and age is greater than 65 (Assumed Retirement Age)\n\n- **RevolvingLines**: Difference between Number of Open Credit Lines and Loans and Number of Real Estate Lines and Loans\n\n- **hasRevolvingLines**: If RevolvingLines exists then 1 else 0\n\n- **hasMultipleRealEstates**: If the Number of Real Estates is greater than 2\n\n- **incomeDivByThousand**: Monthly Income divided by 1000. Fraud might be more likely for these or it might signal the person is in a new job and hasn’t had a percent raise in pay yet. Both groups signal higher risk.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#New Features\n\nfinalData['MonthlyIncomePerPerson'] = finalData['MonthlyIncome']/(finalData['NumberOfDependents']+1)\nfinalData['MonthlyIncomePerPerson'].fillna(0, inplace=True)\n\nfinalData['MonthlyDebt'] = finalData['MonthlyIncome']*finalData['DebtRatio']\nfinalData['MonthlyDebt'].fillna(finalData['DebtRatio'],inplace=True)\nfinalData['MonthlyDebt'] = np.where(finalData['MonthlyDebt']==0, finalData['DebtRatio'],finalData['MonthlyDebt'])\n\nfinalData['isRetired'] = np.where((finalData['age'] > 65), 1, 0)\n\nfinalData['RevolvingLines'] = finalData['NumberOfOpenCreditLinesAndLoans']-finalData['NumberRealEstateLoansOrLines']\n\nfinalData['hasRevolvingLines']=np.where((finalData['RevolvingLines']>0),1,0)\n\nfinalData['hasMultipleRealEstates'] = np.where((finalData['NumberRealEstateLoansOrLines']>=2),1,0)\n\nfinalData['incomeDivByThousand'] = finalData['MonthlyIncome']/1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MissingHandler(finalData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have now added new features to our dataset. Next, we will perform a skewness check on our data by analysing the distributions of individual columns and perform Box Cox Transformation to reduce the skewness.\n\n# Skewness Check and Box Cox Transformation\n\nLet's check the distribution of each values first","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columnList = list(finalData.columns)\ncolumnList\n\nfig = plt.figure(figsize=[20,20])\nfor col,i in zip(columnList,range(1,19)):\n    axes = fig.add_subplot(6,3,i)\n    sns.distplot(finalData[col],ax=axes, kde_kws={'bw':1.5}, color='purple')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above distribution plots, we can see that majority of our data is skewed in either of the directions. We can only see Age forming close to normal distribution. Let's check the skewness values of each column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def SkewMeasure(df):\n    nonObjectColList = df.dtypes[df.dtypes != 'object'].index\n    skewM = df[nonObjectColList].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending = False)\n    skewM=pd.DataFrame({'skew':skewM})\n    return skewM[abs(skewM)>0.5].dropna()\n\nskewM = SkewMeasure(finalData)\nskewM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Skewness is massively high for all the columns. We would apply Box Cox Transformation with **λ = 0.15** in order to reduce this skewness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in skewM.index:\n    finalData[i] = special.boxcox1p(finalData[i],0.15) #lambda = 0.15\n    \nSkewMeasure(finalData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Skewness have reduced on a much higher scale now that the Box Cox Transformation is applied. Let's check the distribution plots for individual columns again:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[20,20])\nfor col,i in zip(columnList,range(1,19)):\n    axes = fig.add_subplot(6,3,i)\n    sns.distplot(finalData[col],ax=axes, kde_kws={'bw':1.5}, color='purple')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, our graphs look much better now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model Training\n\n## Train-Validation Split\n\nWe will currently split the train and validation sets into a 70-30 proportion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDF = finalData[:len(finalTrain)]\ntestDF = finalData[len(finalTrain):]\nprint(trainDF.shape)\nprint(testDF.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xTrain, xTest, yTrain, yTest = model_selection.train_test_split(trainDF.to_numpy(),SeriousDlqIn2Yrs.to_numpy(),test_size=0.3,random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM\n\n**Hyperparameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbAttributes = lgb.LGBMClassifier(objective='binary', n_jobs=-1, random_state=2020, importance_type='gain')\n\nlgbParameters = {\n    'max_depth' : [2,3,4,5],\n    'learning_rate': [0.05, 0.1,0.125,0.15],\n    'colsample_bytree' : [0.2,0.4,0.6,0.8,1],\n    'n_estimators' : [400,500,600,700,800,900],\n    'min_split_gain' : [0.15,0.20,0.25,0.3,0.35], #equivalent to gamma in XGBoost\n    'subsample': [0.6,0.7,0.8,0.9,1],\n    'min_child_weight': [6,7,8,9,10],\n    'scale_pos_weight': [10,15,20],\n    'min_data_in_leaf' : [100,200,300,400,500,600,700,800,900],\n    'num_leaves' : [20,30,40,50,60,70,80,90,100]\n}\n\nlgbModel = model_selection.RandomizedSearchCV(lgbAttributes, param_distributions = lgbParameters, cv = 5, random_state=2020)\n\nlgbModel.fit(xTrain,yTrain.flatten(),feature_name=trainDF.columns.to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestEstimatorLGB = lgbModel.best_estimator_\nbestEstimatorLGB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the best estimator from RandomSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bestEstimatorLGB = lgb.LGBMClassifier(colsample_bytree=0.4, importance_type='gain', max_depth=5,\n               min_child_weight=6, min_data_in_leaf=600, min_split_gain=0.25,\n               n_estimators=900, num_leaves=50, objective='binary',\n               random_state=2020, scale_pos_weight=10, subsample=0.9).fit(xTrain,yTrain.flatten(),feature_name=trainDF.columns.to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yPredLGB = bestEstimatorLGB.predict_proba(xTest)\nyPredLGB = yPredLGB[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yTestPredLGB = bestEstimatorLGB.predict(xTest)\nprint(metrics.classification_report(yTest,yTestPredLGB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(yTest,yTestPredLGB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBMMetrics = pd.DataFrame({'Model': 'LightGBM', \n                            'MSE': round(metrics.mean_squared_error(yTest, yTestPredLGB)*100,2),\n                            'RMSE' : round(np.sqrt(metrics.mean_squared_error(yTest, yTestPredLGB)*100),2),\n                            'MAE' : round(metrics.mean_absolute_error(yTest, yTestPredLGB)*100,2),\n                            'MSLE' : round(metrics.mean_squared_log_error(yTest, yTestPredLGB)*100,2), \n                            'RMSLE' : round(np.sqrt(metrics.mean_squared_log_error(yTest, yTestPredLGB)*100),2),\n                            'Accuracy Train' : round(bestEstimatorLGB.score(xTrain, yTrain) * 100,2),\n                            'Accuracy Test' : round(bestEstimatorLGB.score(xTest, yTest) * 100,2),\n                            'F-Beta Score (β=2)' : round(metrics.fbeta_score(yTest, yTestPredLGB, beta=2)*100,2)},index=[1])\n\nLGBMMetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC AUC**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr,_ = metrics.roc_curve(yTest,yPredLGB)\nrocAuc = metrics.auc(fpr, tpr)\nplt.figure(figsize=(12,6))\nplt.title('ROC Curve')\nsns.lineplot(fpr, tpr, label = 'AUC for LightGBM Model = %0.2f' % rocAuc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE IMPORTANCE**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(bestEstimatorLGB, importance_type='gain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE IMPORTANCE USING SHAP**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame(xTrain, columns=trainDF.columns.to_list())\n\nexplainer = shap.TreeExplainer(bestEstimatorLGB)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values[1], X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost\n\n**Hyperparameter Tuning**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbAttribute = xgs.XGBClassifier(tree_method='gpu_hist',n_jobs=-1, gpu_id=0)\n\nxgbParameters = {\n    'max_depth' : [2,3,4,5,6,7,8],\n    'learning_rate':[0.05,0.1,0.125,0.15],\n    'colsample_bytree' : [0.2,0.4,0.6,0.8,1],\n    'n_estimators' : [400,500,600,700,800,900],\n    'gamma':[0.15,0.20,0.25,0.3,0.35],\n    'subsample': [0.6,0.7,0.8,0.9,1],\n    'min_child_weight': [6,7,8,9,10],\n    'scale_pos_weight': [10,15,20]\n    \n}\n\nxgbModel = model_selection.RandomizedSearchCV(xgbAttribute, param_distributions = xgbParameters, cv = 5, random_state=2020)\n\nxgbModel.fit(xTrain,yTrain.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestEstimatorXGB = xgbModel.best_estimator_\nbestEstimatorXGB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting the best estimator from RandomizedSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bestEstimatorXGB = xgs.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4, gamma=0.25, gpu_id=0,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.125, max_delta_step=0, max_depth=5,\n              min_child_weight=9,\n              monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n              n_estimators=800, n_jobs=-1, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=10, subsample=1,\n              tree_method='gpu_hist', validate_parameters=1, verbosity=None).fit(xTrain,yTrain.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yPredXGB = bestEstimatorXGB.predict_proba(xTest)\nyPredXGB = yPredXGB[:,1]\n\nyTestPredXGB = bestEstimatorXGB.predict(xTest)\nprint(metrics.classification_report(yTest,yTestPredXGB))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics.confusion_matrix(yTest,yTestPredXGB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBMetrics = pd.DataFrame({'Model': 'XGBoost', \n                            'MSE': round(metrics.mean_squared_error(yTest, yTestPredXGB)*100,2),\n                            'RMSE' : round(np.sqrt(metrics.mean_squared_error(yTest, yTestPredXGB)*100),2),\n                            'MAE' : round(metrics.mean_absolute_error(yTest, yTestPredXGB)*100,2),\n                            'MSLE' : round(metrics.mean_squared_log_error(yTest, yTestPredXGB)*100,2), \n                            'RMSLE' : round(np.sqrt(metrics.mean_squared_log_error(yTest, yTestPredXGB)*100),2),\n                            'Accuracy Train' : round(bestEstimatorLGB.score(xTrain, yTrain) * 100,2),\n                            'Accuracy Test' : round(bestEstimatorLGB.score(xTest, yTest) * 100,2),\n                            'F-Beta Score (β=2)' : round(metrics.fbeta_score(yTest, yTestPredXGB, beta=2)*100,2)},index=[2])\n\nXGBMetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ROC AUC**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr,_ = metrics.roc_curve(yTest,yPredXGB)\nrocAuc = metrics.auc(fpr, tpr)\nplt.figure(figsize=(12,6))\nplt.title('ROC Curve')\nsns.lineplot(fpr, tpr, label = 'AUC for XGBoost Model = %0.2f' % rocAuc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE IMPORTANCE**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bestEstimatorXGB.get_booster().feature_names = trainDF.columns.to_list()\nxgs.plot_importance(bestEstimatorXGB, importance_type='gain')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE IMPORTANCE USING SHAP**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# resolve a conflict/bug with latest version of XGBoost and SHAP\nmybooster = bestEstimatorXGB.get_booster()\nmodel_bytearray = mybooster.save_raw()[4:]\ndef myfun(self=None):\n    return model_bytearray\n\nmybooster.save_raw = myfun\n\n\nX = pd.DataFrame(xTrain, columns=trainDF.columns.to_list())\n\nexplainer = shap.TreeExplainer(mybooster)\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [LGBMMetrics, XGBMetrics]\nTrainingResult = pd.concat(frames)\nTrainingResult.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGBM Submission\n\nSince, we can see our LGBM performs better, we will submit this. (Late Submission)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbProbs = bestEstimatorLGB.predict_proba(testDF)\nlgbDF = pd.DataFrame({'ID': testID, 'Probability': lgbProbs[:,1]})\nlgbDF.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence the delinquency probabilities.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbDF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}