{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Replicate credit scoring.\nusing information value (IV), weight of evidence (WoE) for feature selection, logistic regression for scoring model and reverse back to credit score.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-training.csv')\ndf_test = pd.read_csv('/kaggle/input/GiveMeSomeCredit/cs-test.csv')\ndf_train.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_val_sums = df_train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'Column': null_val_sums.index, 'Number of Null Values': null_val_sums.values, 'Proportions': null_val_sums.values/len(df_train)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.fillna(df_train.median())\nprint(df_train.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='SeriousDlqin2yrs', data=df_train)\nprint('Default Rate: {}'.format(df_train['SeriousDlqin2yrs'].sum()/len(df_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Binning","metadata":{}},{"cell_type":"code","source":"age_bins = [-math.inf, 25, 40, 50, 60, 70, math.inf]\ndependent_bin = [-math.inf,2,4,6,8,10,math.inf]\ndpd_bins = [-math.inf,1,2,3,4,5,6,7,8,9,math.inf]\ndf_train['bin_age'] = pd.cut(df_train['age'],bins=age_bins).astype(str)\ndf_train['bin_NumberOfDependents'] = pd.cut(df_train['NumberOfDependents'],bins=dependent_bin).astype(str)\ndf_train['bin_NumberOfTimes90DaysLate'] = pd.cut(df_train['NumberOfTimes90DaysLate'],bins=dpd_bins)\ndf_train['bin_NumberOfTime30-59DaysPastDueNotWorse'] = pd.cut(df_train['NumberOfTime30-59DaysPastDueNotWorse'], bins=dpd_bins)\ndf_train['bin_NumberOfTime60-89DaysPastDueNotWorse'] = pd.cut(df_train['NumberOfTime60-89DaysPastDueNotWorse'], bins=dpd_bins)\n\n\ndf_train['bin_RevolvingUtilizationOfUnsecuredLines'] = pd.qcut(df_train['RevolvingUtilizationOfUnsecuredLines'],q=5,duplicates='drop').astype(str)\ndf_train['bin_DebtRatio'] = pd.qcut(df_train['DebtRatio'],q=5,duplicates='drop').astype(str)\ndf_train['bin_MonthlyIncome'] = pd.qcut(df_train['MonthlyIncome'],q=5,duplicates='drop').astype(str)\ndf_train['bin_NumberOfOpenCreditLinesAndLoans'] = pd.qcut(df_train['NumberOfOpenCreditLinesAndLoans'],q=5,duplicates='drop').astype(str)\ndf_train['bin_NumberRealEstateLoansOrLines'] = pd.qcut(df_train['NumberRealEstateLoansOrLines'],q=5,duplicates='drop').astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bin_cols = [c for c in df_train.columns.values if c.startswith('bin_')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features Extracting\nuse IV and WoE for extracting","metadata":{}},{"cell_type":"code","source":"def calculate_IV(df_train, feature, target):\n    crstab = pd.crosstab(df_train[feature], df_train[target], normalize = 'columns')\n    \n    #crstab['Log_a_b'] = np.log(crstab[0]) - np.log(crstab[1])\n    crstab['WOE'] =   np.log(crstab[crstab.columns[0]]/crstab[crstab.columns[1]]) \n    crstab = crstab.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n    crstab['diff_0_1'] =  crstab[crstab.columns[0]] - crstab[crstab.columns[1]]\n    crstab['IV_i'] = crstab['WOE']*crstab['diff_0_1']\n    IV = sum(crstab['IV_i'])\n    # print(crstab)\n    return IV\n\n# recalculate IV 1 4\n# f = bin_cols[2]\n# calculate_IV(df_train,f,'SeriousDlqin2yrs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recalculate IV\nIV_list_ = []\nfor f in bin_cols:\n#     IV_list_.append([f,cal_IV(df_train,f,'SeriousDlqin2yrs')])\n     IV_list_.append([f,calculate_IV(df_train,f,'SeriousDlqin2yrs')])\nIV_data_ = pd.DataFrame(IV_list_, columns=['features','IV'])\nprint(IV_data_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We choose only those features with IV>0.1\nfeature_cols = ['RevolvingUtilizationOfUnsecuredLines','NumberOfTime30-59DaysPastDueNotWorse','age','NumberOfTimes90DaysLate','NumberOfTime60-89DaysPastDueNotWorse']\nbin_cols = ['bin_RevolvingUtilizationOfUnsecuredLines','bin_NumberOfTime30-59DaysPastDueNotWorse','bin_age','bin_NumberOfTimes90DaysLate','bin_NumberOfTime60-89DaysPastDueNotWorse']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_WOE(df,features,target):\n    df_new = df\n    for f in features:\n        df_woe = df_new.groupby(f).agg({target:['sum','count']})\n        df_woe.columns = list(map(''.join, df_woe.columns.values))\n        df_woe = df_woe.reset_index()\n        df_woe = df_woe.rename(columns = {target+'sum':'bad'})\n        df_woe = df_woe.rename(columns = {target+'count':'all'})\n        df_woe['good'] = df_woe['all']-df_woe['bad']\n        df_woe = df_woe[[f,'good','bad']]\n        df_woe['bad_rate'] = df_woe['bad']/df_woe['bad'].sum()\n        df_woe['good_rate'] = df_woe['good']/df_woe['good'].sum()\n        df_woe['woe'] = df_woe['bad_rate'].divide(df_woe['good_rate'],fill_value=1)\n        df_woe.columns = [c if c==f else c+'_'+f for c in list(df_woe.columns.values)]\n        df_new = df_new.merge(df_woe,on=f,how='left')\n    return df_new\n\ndef WOE(df_train, feature, target):\n    crstab = pd.crosstab(df_train[feature], df_train[target], normalize = 'columns')\n    crstab_ = pd.crosstab(df_train[feature], df_train[target])\n    crstab['woe'] = crstab[crstab.columns[1]].divide(crstab[crstab.columns[0]], fill_value=1) \n    crstab = crstab.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n    df_woe = pd.DataFrame(crstab.values, index=crstab.index)\n    df_woe.columns = ['good_rate', 'bad_rate', 'woe']\n    df_woe.columns = [st + \"_\" + feature for st in df_woe.columns]\n    \n    return df_woe\n\ndef calculate_WOE(df,features,target):\n    df_new = df\n    for f in features:\n        df_woe = WOE(df_new, f, target)\n        df_new = df_new.merge(df_woe, on = f, how='left')\n    woe_cols = [c for c in df_new.columns if 'woe' in c]\n    # return df_new[woe_cols]\n    return df_new # full data frame\n#print(bin_cols)\n#calculate_WOE(df_train,bin_cols,'SeriousDlqin2yrs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_woe = cal_WOE(df_train,bin_cols,'SeriousDlqin2yrs')\ndf_woe.head()\n# woe_cols = [c for c in list(df_woe.columns.values) if 'woe' in c]\n# df_woe[woe_cols]\n\n# my calculation\ndf_woe = calculate_WOE(df_train,bin_cols,'SeriousDlqin2yrs')\nwoe_cols = [c for c in df_woe.columns if 'woe' in c]\ndf_woe[woe_cols].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bin_to_woe = pd.DataFrame(columns = ['features','bin','woe'])\nfor f in feature_cols:\n    b = 'bin_'+f\n    w = 'woe_bin_'+f\n    df = df_woe[[w,b]].drop_duplicates()\n    df.columns = ['woe','bin']\n    df['features'] = f\n    df=df[['features','bin','woe']]\n    df_bin_to_woe = pd.concat([df_bin_to_woe,df])\ndf_bin_to_woe.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling and Training","metadata":{}},{"cell_type":"markdown","source":"Used logistc regression rather than advanced NN or xgboost model for credit scoring. \n\nLogistic is intuitively explanable and meets the model requirements under survilence in banks  ","metadata":{"trusted":true}},{"cell_type":"code","source":"# split data 80/20 for training and validating\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test = train_test_split(df_woe[woe_cols], df_woe['SeriousDlqin2yrs'], test_size=0.2, random_state=66)\nmodel = LogisticRegression().fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.score(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = model.predict_proba(X_test)\n\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# K-S value, max(tpr - fpr) to measure the discrimination of predict model\n# >=0.5 is considered good differentiation \nprint(max(tpr - fpr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix\ny_pred = model.predict(X_test)\ncm = metrics.confusion_matrix(y_test, y_pred)\ncm_display = metrics.ConfusionMatrixDisplay(cm).plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.coef_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get credit scores\ncredit score is linear transformation of log odds","metadata":{}},{"cell_type":"code","source":"theta_0 = 1/1\nP_0 = 650 # base score\nPDO = 50 # point of double","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"B = PDO/np.log(2)\nA = P_0+B*np.log(theta_0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_scorecard(model_coef,binning_df,features,B):\n    lst = []\n    cols = ['Variable','Binning','Score']\n    coef = model_coef[0]\n    for i in range(len(features)):\n        f = features[i]\n        df = binning_df[binning_df['features']==f]\n        for index,row in df.iterrows():\n            lst.append([f,row['bin'],int(round(-coef[i]*row['woe']*B))])\n    data = pd.DataFrame(lst, columns=cols)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_card = generate_scorecard(model.coef_,df_bin_to_woe,feature_cols,B)\nscore_card.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort_scorecard = score_card.groupby('Variable').apply(lambda x: x.sort_values('Score', ascending=False))\nsort_scorecard.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def str_to_int(s):\n    if s == '-inf':\n        return -999999999.0\n    elif s=='inf':\n        return 999999999.0\n    else:\n        return float(s)\n    \ndef map_value_to_bin(feature_value,feature_to_bin):\n    for idx, row in feature_to_bin.iterrows():\n        bins = str(row['Binning'])\n        left_open = bins[0]==\"(\"\n        right_open = bins[-1]==\")\"\n        binnings = bins[1:-1].split(',')\n        in_range = True\n        # check left bound\n        if left_open:\n            if feature_value<= str_to_int(binnings[0]):\n                in_range = False   \n        else:\n            if feature_value< str_to_int(binnings[0]):\n                in_range = False   \n        #check right bound\n        if right_open:\n            if feature_value>= str_to_int(binnings[1]):\n                in_range = False \n        else:\n            if feature_value> str_to_int(binnings[1]):\n                in_range = False   \n        if in_range:\n            return row['Binning']\n    return null\n\ndef map_to_score(df,score_card):\n    scored_columns = list(score_card['Variable'].unique())\n    score = 0\n    for col in scored_columns:\n        feature_to_bin = score_card[score_card['Variable']==col]\n        feature_value = df[col]\n        selected_bin = map_value_to_bin(feature_value,feature_to_bin)\n        selected_record_in_scorecard = feature_to_bin[feature_to_bin['Binning'] == selected_bin]\n        score += selected_record_in_scorecard['Score'].iloc[0]\n    return score  \n\ndef calculate_score_with_card(df,score_card,A):\n    df['score'] = df.apply(map_to_score,args=(score_card,),axis=1)\n    df['score'] = df['score']+A\n    df['score'] = df['score'].astype(int)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate score\nSample candiate from training data","metadata":{}},{"cell_type":"code","source":"good_sample = df_train[df_train['SeriousDlqin2yrs']==0].sample(1000)\ngood_sample = good_sample[feature_cols]\nbad_sample = df_train[df_train['SeriousDlqin2yrs']==1].sample(1000)\nbad_sample = bad_sample[feature_cols]\n\ngood_candidate = calculate_score_with_card(good_sample,score_card,A)\nbad_candidate = calculate_score_with_card(bad_sample,score_card,A)\nres = pd.concat([good_candidate, bad_candidate])\nres.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Approve card critera\n### >= 650   Approved\n### 450-650  Pending\n### <450     Declined","metadata":{}},{"cell_type":"code","source":"def approve_or_not(score):\n    if score < 450:\n        return \"Declined\"\n    elif score >= 450 and score < 650:\n        return \"Pending\"\n    else:\n        return \"Approved\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res['Status'] = res['score'].apply(approve_or_not)\nprint(res['Status'].value_counts())\nfig, axs = plt.subplots(1,2)\naxs[0].hist(res['score']) \naxs[1].hist(res['Status']) \nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1000 good and bad candidates are selected each, 1335 were pending.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df_train['SeriousDlqin2yrs'], res['Status'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}