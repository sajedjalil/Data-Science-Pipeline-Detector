{"cells":[{"metadata":{},"cell_type":"markdown","source":"Credit goes to **Jesper** for providing a good kernel and establishing a foundation to build on. I forked his kernel and added Nested U-Net model for those who would like to try it. Enclosed is the link to the Nested U-Net Architecture paper along with the repo. \n\nhttps://www.researchgate.net/publication/324574642_UNet_A_Nested_U-Net_Architecture_for_Medical_Image_Segmentation\n    \nhttps://github.com/MrGiovanni/UNetPlusPlus.git\n\n**Please upvote Jesper's kernel.** If time permits, I'll publish my own kernel. "},{"metadata":{},"cell_type":"markdown","source":"Your patient comes in. They're having a stinging pain in the chest. The X-Ray shows a shadow on the lung. And that shadow is air. If not treated a \"collapsed lung\"(-ish), or \"air on the wrong side of the lung\", can result in death.\n\n[What's a Pneumothorax?](https://en.wikipedia.org/wiki/Pneumothorax)\n\nProblem is in an X-Ray, air is usually the thing you ignore. The general idea is:\n\n- Black: Air\n- Gray: Fluids and Tissue\n- White: Bone and Solids\n\nSo the issue is that an air enclosure may just be a mild disturbance in the chest xray. Considering convolutional neural networks are exceptional at identifying abnormalities, we may want them to take a look, as not to miss these tiny abnormalities.\n\n\n![Futuristic view of human](https://www.publicdomainpictures.net/pictures/50000/nahled/anatomy-high-tech.jpg)\n\nIn this challenge, we get chest xrays and masks. In challenges I link below, only bounding boxes were available. Here, we actually get to do dense prediction.\n\nI'm hiding some cells for readability below, just fork the kernel or click the \"Show code\" on the right to see imports etc. This kernel is just giving some starting info and how to look at the data with the given tools. \n\nSources I used and further reading:\n\n- https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\n- https://www.kaggle.com/schlerp/getting-to-know-dicom-and-the-data/data\n\nSome ideas that might be helpful:\n\n- [Check out Pneumonia X-Ray Challenge](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)\n- [Use TFRecords](https://www.kaggle.com/lyonzy/convert-dicom-images-to-tfrecords)\n- [Check Out Unets](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport glob\nimport keras\nimport pydicom\n\nprint(os.listdir(\"../input/siim-acr-pneumothorax-segmentation\"))\nprint()\nprint(os.listdir(\"../input/siim-acr-pneumothorax-segmentation/sample images\"))\n# Any results you write to the current directory are saved as output.\n\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\n\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras import backend as K\n\nimport keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.layers import Input, merge, Conv2D, ZeroPadding2D, UpSampling2D, Dense, concatenate, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D, MaxPooling2D\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import BatchNormalization, Dropout, Flatten, Lambda\nfrom keras.layers.advanced_activations import ELU, LeakyReLU\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.regularizers import l2\nfrom keras.layers.noise import GaussianDropout\n\nimport numpy as np\n\nsmooth = 1.\ndropout_rate = 0.3\nact = \"relu\"\n\n\nimport tensorflow as tf\n\nfrom tqdm import tqdm_notebook\n\nimport sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nfrom mask_functions import rle2mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What is DICOM?\n\nDicom is a format that has metadata, as well as Pixeldata attached to it. Below I extract some basic info with an image. You will know about the gender and age of the patient, as well as info how the image is sampled and generated. Quite useful to programatically read. Here's the [Wikipedia](https://en.wikipedia.org/wiki/DICOM) article for it."},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"def show_dcm_info(dataset):\n    print(\"Filename.........:\", file_path)\n    print(\"Storage type.....:\", dataset.SOPClassUID)\n    print()\n\n    pat_name = dataset.PatientName\n    display_name = pat_name.family_name + \", \" + pat_name.given_name\n    print(\"Patient's name......:\", display_name)\n    print(\"Patient id..........:\", dataset.PatientID)\n    print(\"Patient's Age.......:\", dataset.PatientAge)\n    print(\"Patient's Sex.......:\", dataset.PatientSex)\n    print(\"Modality............:\", dataset.Modality)\n    print(\"Body Part Examined..:\", dataset.BodyPartExamined)\n    print(\"View Position.......:\", dataset.ViewPosition)\n    \n    if 'PixelData' in dataset:\n        rows = int(dataset.Rows)\n        cols = int(dataset.Columns)\n        print(\"Image size.......: {rows:d} x {cols:d}, {size:d} bytes\".format(\n            rows=rows, cols=cols, size=len(dataset.PixelData)))\n        if 'PixelSpacing' in dataset:\n            print(\"Pixel spacing....:\", dataset.PixelSpacing)\n\ndef plot_pixel_array(dataset, figsize=(10,10)):\n    plt.figure(figsize=figsize)\n    plt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first take a look at the sample images that are available. You'll be able to transfer this kernel to downloaded data, to visualize other bits and explore their metadata."},{"metadata":{"trusted":true},"cell_type":"code","source":"for file_path in glob.glob('../input/siim-acr-pneumothorax-segmentation/sample images/*.dcm'):\n    dataset = pydicom.dcmread(file_path)\n    show_dcm_info(dataset)\n    plot_pixel_array(dataset)\n    break # Comment this out to see all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How do the masks look like?\n\nFirst let's look at all the sample images. We can see different modes of collection. It becomes very evident, that we have to be careful about the top right marker on the image. The different L may mess with our data. Could it be usable leakage as it points to the hospital it was taken at? Yes, yes it could, but I'm *sure* Kaggle took care of this.\n\nThen we'll look at 3 images and the masks that come with it. Personally, I can't really make out how to find the pneumothorax in the images. Play around with it, in some of the other images, it is definitely more visible than in others. Also (thanks to Ehsan) make sure to transpose the masks!"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_img = len(glob.glob('../input/siim-acr-pneumothorax-segmentation/sample images/*.dcm'))\nfig, ax = plt.subplots(nrows=1, ncols=num_img, sharey=True, figsize=(num_img*10,10))\nfor q, file_path in enumerate(glob.glob('../input/siim-acr-pneumothorax-segmentation/sample images/*.dcm')):\n    dataset = pydicom.dcmread(file_path)\n    #show_dcm_info(dataset)\n    \n    ax[q].imshow(dataset.pixel_array, cmap=plt.cm.bone)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = 4   # Starting index of images\nnum_img = 5 # Total number of images to show\n\nfig, ax = plt.subplots(nrows=1, ncols=num_img, sharey=True, figsize=(num_img*10,10))\nfor q, file_path in enumerate(glob.glob('../input/siim-acr-pneumothorax-segmentation/sample images/*.dcm')[start:start+num_img]):\n    dataset = pydicom.dcmread(file_path)\n    #show_dcm_info(dataset)\n    \n    ax[q].imshow(dataset.pixel_array, cmap=plt.cm.bone)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/siim-acr-pneumothorax-segmentation/sample images/train-rle-sample.csv', header=None, index_col=0)\n\nfig, ax = plt.subplots(nrows=1, ncols=num_img, sharey=True, figsize=(num_img*10,10))\nfor q, file_path in enumerate(glob.glob('../input/siim-acr-pneumothorax-segmentation/sample images/*.dcm')[start:start+num_img]):\n    dataset = pydicom.dcmread(file_path)\n    #print(file_path.split('/')[-1][:-4])\n    ax[q].imshow(dataset.pixel_array, cmap=plt.cm.bone)\n    if df.loc[file_path.split('/')[-1][:-4],1] != '-1':\n        mask = rle2mask(df.loc[file_path.split('/')[-1][:-4],1], 1024, 1024).T\n        ax[q].set_title('See Marker')\n        ax[q].imshow(mask, alpha=0.1, cmap=\"Reds\")\n    else:\n        ax[q].set_title('Nothing to see')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vanilla Unet\nSo how would we work the data on GCP?\n\nI'd suggest a very nice [Unet](https://arxiv.org/abs/1505.04597), maybe use a pretty pre-trained encoder instead of the following. They're excellent on small-ish datasets and particularly on image segmentation. There are many others you may try, but maybe this one will get you started.\n\n![](http://deeplearning.net/tutorial/_images/unet.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### Load Full Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_glob = '../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-train/*/*/*.dcm'\ntest_glob = '../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-test/*/*/*.dcm'\ntrain_fns = sorted(glob.glob(train_glob))[:5000]\ntest_fns = sorted(glob.glob(test_glob))[:5000]\ndf_full = pd.read_csv('../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/train-rle.csv', index_col='ImageId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the point I shake my fist at unstripped strings..."},{"metadata":{"trusted":true},"cell_type":"code","source":"im_height = 1024\nim_width = 1024\nim_chan = 1\n# Get train images and masks\nX_train = np.zeros((len(train_fns), im_height, im_width, im_chan), dtype=np.uint8)\nY_train = np.zeros((len(train_fns), im_height, im_width, 1), dtype=np.bool)\nprint('Getting train images and masks ... ')\nsys.stdout.flush()\nfor n, _id in tqdm_notebook(enumerate(train_fns), total=len(train_fns)):\n    dataset = pydicom.read_file(_id)\n    X_train[n] = np.expand_dims(dataset.pixel_array, axis=2)\n    try:\n        if '-1' in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n            Y_train[n] = np.zeros((1024, 1024, 1))\n        else:\n            if type(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']) == str:\n                Y_train[n] = np.expand_dims(rle2mask(df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels'], 1024, 1024), axis=2)\n            else:\n                Y_train[n] = np.zeros((1024, 1024, 1))\n                for x in df_full.loc[_id.split('/')[-1][:-4],' EncodedPixels']:\n                    Y_train[n] =  Y_train[n] + np.expand_dims(rle2mask(x, 1024, 1024), axis=2)\n    except KeyError:\n        print(f\"Key {_id.split('/')[-1][:-4]} without mask, assuming healthy patient.\")\n        Y_train[n] = np.zeros((1024, 1024, 1)) # Assume missing masks are empty masks.\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build Patches\nReshape to get non-overlapping patches."},{"metadata":{"trusted":true},"cell_type":"code","source":"im_height = 256\nim_width = 256\nX_train = X_train.reshape((-1, im_height, im_width, 1))\nY_train = Y_train.reshape((-1, im_height, im_width, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\n\n\n#bce\n\ndef bce(y_pred):\n    #see https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/keras/backend.py#L3525\n    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n    return tf.log(y_pred / (1 - y_pred))\n\ndef bceloss(y_true, y_pred):\n    beta = 2.0\n    y_pred = bce(y_pred)\n    pos_weight = beta / (1 - beta)\n    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=pos_weight)\n\n    return tf.reduce_mean(loss * (1 - beta))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/MrGiovanni/UNetPlusPlus.git\n# 2D Standard\ndef standard_unit(input_tensor, stage, nb_filter, kernel_size=3):\n\n    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(input_tensor)\n    x = Dropout(dropout_rate, name='dp'+stage+'_1')(x)\n    x = Conv2D(nb_filter, (kernel_size, kernel_size), activation=act, name='conv'+stage+'_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(x)\n    x = Dropout(dropout_rate, name='dp'+stage+'_2')(x)\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nStandard UNet++ [Zhou et.al, 2018]\nTotal params: 9,041,601\n\"\"\"\ndef Nest_Net(img_rows, img_cols, color_type=1, num_class=1, deep_supervision=False):\n\n    nb_filter = [16,32,64,128,256,512]\n\n    # Handle Dimension Ordering for different backends\n    global bn_axis\n    if K.image_dim_ordering() == 'tf':\n      bn_axis = 3\n      img_input = Input(shape=(img_rows, img_cols, color_type), name='main_input')\n    else:\n      bn_axis = 1\n      img_input = Input(shape=(color_type, img_rows, img_cols), name='main_input')\n\n    conv1_1 = standard_unit(img_input, stage='11', nb_filter=nb_filter[0])\n    pool1 = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(conv1_1)\n\n    conv2_1 = standard_unit(pool1, stage='21', nb_filter=nb_filter[1])\n    pool2 = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(conv2_1)\n\n    up1_2 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up12', padding='same')(conv2_1)\n    conv1_2 = concatenate([up1_2, conv1_1], name='merge12', axis=bn_axis)\n    conv1_2 = standard_unit(conv1_2, stage='12', nb_filter=nb_filter[0])\n\n    conv3_1 = standard_unit(pool2, stage='31', nb_filter=nb_filter[2])\n    pool3 = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(conv3_1)\n\n    up2_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up22', padding='same')(conv3_1)\n    conv2_2 = concatenate([up2_2, conv2_1], name='merge22', axis=bn_axis)\n    conv2_2 = standard_unit(conv2_2, stage='22', nb_filter=nb_filter[1])\n\n    up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up13', padding='same')(conv2_2)\n    conv1_3 = concatenate([up1_3, conv1_1, conv1_2], name='merge13', axis=bn_axis)\n    conv1_3 = standard_unit(conv1_3, stage='13', nb_filter=nb_filter[0])\n\n    conv4_1 = standard_unit(pool3, stage='41', nb_filter=nb_filter[3])\n    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(conv4_1)\n\n    up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up32', padding='same')(conv4_1)\n    conv3_2 = concatenate([up3_2, conv3_1], name='merge32', axis=bn_axis)\n    conv3_2 = standard_unit(conv3_2, stage='32', nb_filter=nb_filter[2])\n\n    up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up23', padding='same')(conv3_2)\n    conv2_3 = concatenate([up2_3, conv2_1, conv2_2], name='merge23', axis=bn_axis)\n    conv2_3 = standard_unit(conv2_3, stage='23', nb_filter=nb_filter[1])\n\n    up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up14', padding='same')(conv2_3)\n    conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], name='merge14', axis=bn_axis)\n    conv1_4 = standard_unit(conv1_4, stage='14', nb_filter=nb_filter[0])\n\n    conv5_1 = standard_unit(pool4, stage='51', nb_filter=nb_filter[4])\n\n    up4_2 = Conv2DTranspose(nb_filter[3], (2, 2), strides=(2, 2), name='up42', padding='same')(conv5_1)\n    conv4_2 = concatenate([up4_2, conv4_1], name='merge42', axis=bn_axis)\n    conv4_2 = standard_unit(conv4_2, stage='42', nb_filter=nb_filter[3])\n\n    up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), name='up33', padding='same')(conv4_2)\n    conv3_3 = concatenate([up3_3, conv3_1, conv3_2], name='merge33', axis=bn_axis)\n    conv3_3 = standard_unit(conv3_3, stage='33', nb_filter=nb_filter[2])\n\n    up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), name='up24', padding='same')(conv3_3)\n    conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], name='merge24', axis=bn_axis)\n    conv2_4 = standard_unit(conv2_4, stage='24', nb_filter=nb_filter[1])\n\n    up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), name='up15', padding='same')(conv2_4)\n    conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], name='merge15', axis=bn_axis)\n    conv1_5 = standard_unit(conv1_5, stage='15', nb_filter=nb_filter[0])\n\n    nestnet_output_1 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_1', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_2)\n    nestnet_output_2 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_2', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_3)\n    nestnet_output_3 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_3', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_4)\n    nestnet_output_4 = Conv2D(num_class, (1, 1), activation='sigmoid', name='output_4', kernel_initializer = 'he_normal', padding='same', kernel_regularizer=l2(1e-4))(conv1_5)\n\n    if deep_supervision:\n        model = Model(input=img_input, output=[nestnet_output_1,\n                                               nestnet_output_2,\n                                               nestnet_output_3,\n                                               nestnet_output_4])\n    else:\n        model = Model(input=img_input, output=[nestnet_output_4])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Nest_Net(None, None, im_chan)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[bceloss])\n#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[balanced_cross_entropy])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train a scrappy network\n\nDefinitely work in progress though."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\n\n\ncheckpoint = ModelCheckpoint('../working/unet.h5', monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n                                   verbose=1, mode='auto', epsilon=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=6)\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit (X_train, Y_train, validation_split=.2, batch_size=32, epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\nfrom PIL import *\nimport PIL\nfrom mask_functions import rle2mask,mask2rle\n\nimg_size = 512\ndef test_images_pred(test_fns):\n    pred_rle = []\n    ids = []\n    model.load_weights('../working/unet.h5')\n    for f in tqdm_notebook(test_fns):\n        img = pydicom.read_file(f).pixel_array\n        img = cv2.resize(img,(img_size,img_size))\n        img = model.predict(img.reshape(1,img_size,img_size,1))\n        img = img.reshape(img_size,img_size)\n        ids.append('.'.join(f.split('/')[-1].split('.')[:-1]))\n        #img = PIL.Image.fromarray(((img.T*255).astype(np.uint8)).resize(1024,1024))\n        img = PIL.Image.fromarray((img.T*255).astype(np.uint8)).resize((1024,1024))\n        img = np.asarray(img)\n        pred_rle.append(mask2rle(img,1024,1024))\n    return pred_rle,ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,ids = test_images_pred(test_fns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(preds[10])\nprint(len(preds),len(ids))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1 = pd.DataFrame({'ImageId':ids,'EncodedPixels':preds})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission1.to_csv('newsubmit.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nhtml = \"<a href = submission.csv>d</a>\"\nHTML(html)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = submission1\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"mobassir_submission.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame(sub_df1)\n\n# create a link to download the dataframe\ncreate_download_link(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This isn't working as well as I'd like it to, but I'll leave it here for now. The data is loading, the model is training, I'm still hoping for clarification on the submission, as there is a mismatch between the `smaple_submission.csv` and the provided data.\n\nBetter ideas are to use proper train / validation splits, possibly with [stratification](https://en.wikipedia.org/wiki/Stratified_sampling) and consider using a nice [Generator](https://keras.io/preprocessing/image/) instead. Particularly, it may be benefitial not training on 1024x1024 images, but patches of the image. [This kernel](https://www.kaggle.com/toregil/a-lung-u-net-in-keras) might be interesting, but there are many on kaggle on lung segmentation, [mine on salt segmentation](https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics), or the [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge) challenge."},{"metadata":{},"cell_type":"markdown","source":"## Learnings from Other Segmentation Challenges\n\nThese are of course some learnings I gained, but they are from the amazing kagglers in all the links, so please give them the credits. (Especially [Heng CherKeng](https://www.kaggle.com/hengck23), learned a bunch from them and the list below is heavily influenced by them.)\n\nAs losses go, definitely check out [IOU / Jaccard](Intersection over union), [Lovasz](https://arxiv.org/abs/1705.08790), and [Focal methods](https://arxiv.org/abs/1708.02002), although [Dice](https://arxiv.org/abs/1707.03237) is the LB loss. You'll probably enjoy [Hypercolumns](https://arxiv.org/abs/1411.5752), [Squeeze & Excitation](https://arxiv.org/abs/1803.02579), [Data Distillation](https://arxiv.org/abs/1712.04440), maybe some [Global Attention](https://arxiv.org/abs/1805.10180) in your Upsampling, and even sprinkle in some [Stochastic Weight Averaging](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a). Some [common tricks on kaggle image segmentation](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984#latest-425973) are:\n\n- Look at the masks! (There be dragons.)\n- Analyze the metadata! (There be leakage.)\n\n\n- Predict empty masks (Binary Classification)\n- Break down problem (Male / Female Networks? Multiclass?)\n- Active learning of sorting out easy and hard to classify images (Confidence Intervals)\n\n\n- Pesudo-labeling, semi-suervised learning, knowledge distillation, adversarial training\n- Additional labeling or supervisory signal \n- Clustering (KNN for image patches)\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}