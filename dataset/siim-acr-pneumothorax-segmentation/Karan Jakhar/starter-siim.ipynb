{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Let's Get started</h1>\n"},{"metadata":{},"cell_type":"markdown","source":"**What is given?**     \nTrain images and test images which we have to download using Healthcare API. And train_rle.csv which have image id and corresponding rle. Beware all train image don't have rle."},{"metadata":{},"cell_type":"markdown","source":"Importing the required libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport pydicom\nfrom tqdm import tqdm_notebook\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nimport gc\nfrom skimage.transform import resize\nimport PIL\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = pd.read_csv('../input/siim-train-test/siim/train-rle.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#These are the functions provided by kaggle to convert a mask to rle and vice-versa.\nimport numpy as np\n\ndef mask2rle(img, width, height):\n    rle = []\n    lastColor = 0;\n    currentPixel = 0;\n    runStart = -1;\n    runLength = 0;\n\n    for x in range(width):\n        for y in range(height):\n            currentColor = img[x][y]\n            if currentColor != lastColor: \n                if currentColor >= 127:\n                    runStart = currentPixel;\n                    runLength = 1;\n                else:\n                    rle.append(str(runStart));\n                    rle.append(str(runLength));\n                    runStart = -1;\n                    runLength = 0;\n                    currentPixel = 0;\n            elif runStart > -1:\n                runLength += 1\n            lastColor = currentColor;\n            currentPixel+=1;\n\n    return \" \".join(rle)\n\ndef rle2mask(rle, width, height):\n    \n    mask= np.zeros(width* height)\n    if rle == ' -1' or rle == '-1':\n        return mask.reshape(width,height)\n    array = np.asarray([int(x) for x in rle.split()])\n    \n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        current_position += start\n        mask[current_position:current_position+lengths[index]] = 255\n        current_position += lengths[index]\n\n    return mask.reshape(width, height)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(rle2mask(tr[' EncodedPixels'][5],1024,1024))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Converting are rle into masks</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mask(encode,width,height):\n    if encode == [] or encode == ' -1':\n        return rle2mask(' -1',width,height)\n    else:\n        return rle2mask(encode[0],width,height)       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Gettting image and corresponding mask</h4>\nI am using only 2000 train images due to time and memory contraint. Moreover I am checking everything works fine."},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_n_encode(train_images_names,encode_df):\n    train_imgs = [] \n    train_encode = []\n    c = 0\n    for f in tqdm_notebook(train_images_names):\n        if c >= 2000:\n            break\n        try:\n            img = pydicom.read_file(f).pixel_array\n            c += 1\n            encode = list(encode_df.loc[encode_df['ImageId'] == '.'.join(f.split('/')[-1].split('.')[:-1]),\n                               ' EncodedPixels'].values)\n            \n            encode = get_mask(encode,img.shape[1],img.shape[0])\n            encode = resize(encode,(img_size,img_size))\n            train_encode.append(encode)\n            img = resize(img,(img_size,img_size))\n            train_imgs.append(img)\n        except pydicom.errors.InvalidDicomError:\n            print('come here')\n        \n    return train_imgs,train_encode\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to @seesee for uploading [this dataset.](https://www.kaggle.com/seesee/siim-train-test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting path of all the train and test images\nfrom glob import glob\ntrain_fns = sorted(glob('../input/siim-train-test/siim/dicom-images-train/*/*/*.dcm'))\ntest_fns = sorted(glob('../input/siim-train-test/siim/dicom-images-test/*/*/*.dcm'))\n\nprint(len(train_fns))\nprint(len(test_fns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images,mask_e = image_n_encode(train_fns,tr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(images),len(mask_e))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(images[102],cmap = 'gray')\nplt.show()\nplt.imshow(mask_e[102])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation metric\n#ref https://www.kaggle.com/jesperdramsch/intro-chest-xray-dicom-viz-u-nets-full-data\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Building our model</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(input_layer, start_neurons):\n    #ref: https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n    # 128 -> 64\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(0.25)(pool1)\n\n    # 64 -> 32\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(0.5)(pool2)\n\n    # 32 -> 16\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(0.5)(pool3)\n\n    # 16 -> 8\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.5)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n\n    # 8 -> 16\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.5)(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n\n    # 16 -> 32\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])\n    uconv3 = Dropout(0.5)(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n\n    # 32 -> 64\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n    uconv2 = Dropout(0.5)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n\n    # 64 -> 128\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    uconv1 = Dropout(0.5)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n\n    #uconv1 = Dropout(0.5)(uconv1)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    \n    return output_layer\n\ninput_layer = Input((img_size, img_size, 1))\noutput_layer = build_model(input_layer, 16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint = ModelCheckpoint(\"./unet_best1.model\", \n                                   mode = 'max', save_best_only=True, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[dice_coef])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(np.array(images).reshape(-1,img_size,img_size,1),np.array(mask_e).reshape(-1,img_size,img_size,1),validation_split = 0.1,\n          epochs = 1,batch_size = 16,\n         callbacks = [model_checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del images,mask_e","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4> Predicting the mask and converting it into rle </h4>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def test_images_pred(test_fns):\n    pred_rle = []\n    ids = []\n    for f in tqdm_notebook(test_fns):\n        img = pydicom.read_file(f).pixel_array\n        img = resize(img,(img_size,img_size))\n        img = model.predict(img.reshape(1,img_size,img_size,1))\n        \n        img = img.reshape(img_size,img_size)\n        ids.append('.'.join(f.split('/')[-1].split('.')[:-1]))\n        #img = PIL.Image.fromarray(((img.T*255).astype(np.uint8)).resize(1024,1024))\n        img = PIL.Image.fromarray((img.T*255).astype(np.uint8)).resize((1024,1024))\n        img = np.asarray(img)\n        #print(img)\n        pred_rle.append(mask2rle(img,1024,1024))\n    return pred_rle,ids\n        "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"preds,ids = test_images_pred(test_fns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(preds[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#print(preds[10])\nprint(len(preds),len(ids))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'ImageId':ids,'EncodedPixels':preds})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nhtml = \"<a href = unet_best1.model>d</a>\"\nHTML(html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Work in progress. I will update it.</h4>\n<h3>Any suggestion. Let me know in comments.</h3>\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}