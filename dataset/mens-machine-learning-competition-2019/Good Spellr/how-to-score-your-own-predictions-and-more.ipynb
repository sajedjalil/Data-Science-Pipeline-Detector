{"cells":[{"metadata":{"_uuid":"d4c500e32c4b4cca53c1c66504056a9bee38a5b6"},"cell_type":"markdown","source":"# Overview\nIn this kernel, we will go through everything we need to score our submission ourself (and a bit more).  When we're done, we'll be able to:\n1. Create a submission file based on a simple seed-based model.\n2. Efficiently add seed information to tournament games/results and filter out \"play-in\" games (which are not used in scoring).\n3.  Create an appropriate Results file.\n4. Score predictions for every NCAA Tournament from 1985 (as opposed to just the 2014 - 2018 tournaments included in the official Stage 1 scoring).\n5.  Add a few checks to make sure we are not doing something obviously wrong.\n6. Analyze scoring results from individual games for the simple seed-based model.\n\n\n# I Introduction\n\nSay we've created a model in Stage 1, and we're generating a file for submission.  In that submission file, we'll have two columns: 'ID' and 'Pred'.  Then we'll want to see how well that submission file scores.  We have two options: \n1. We can submit the file to Kaggle for scoring.\n    * Advantage:  It's easy\n    * Disadvantages: You can submit predictions for only the 2014-2018 tournaments; you don't learn which games (or even which tournaments) are contributing the most/least to your score; you're not warned when you've made an obvious mistake (e.g., your probabilities are not between 0 and 1).\n2.  We can score the predictions ourselves.\n    * Advantage:  We can make it more transparent and get some feedback about individual predictions; we can build in functionality to warn us we've made obvious errors.\n    * Disadvantage:  It takes a little bit of work.  That's where this kernel comes in.    \n"},{"metadata":{"_uuid":"003ae0baf985049f9fd85bacefe45ca6eb54c99e"},"cell_type":"markdown","source":"# II  Creating a Submission File\n\nFor this competition, the organizers provide us with a file called SampleSubmissionStage1.csv.   In that file is a list of game ID's for the 2014-2018 season (under the heading 'ID') and a list of predictions, all set to the 50/50 benchmark value of 0.5.  If you make a submission where all of your predictions are 0.5, you are guaranteed to get a score of ln(2) = 0.693...  In this section we will create a Submission file from scratch using the seed-based benchmark of years' past.\n\nThe seed-benchmark was a linear model with the formula\nPr(Team X beats Team Y) = 0.5 + 0.03*(Seed(Team Y) - Seed(Team X))\n\nIt's not a great model, but it will suit our purposes here.  To get started, we grab the file which contains historical seed information"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting (when it can't be done under the pandas hood)\n\n# Input data files are available in the \"../input/\" directory.\nimport os\ndata_directory = '../input/datafiles'\n\ndf_seeds = pd.read_csv(os.path.join(data_directory,'NCAATourneySeeds.csv'))\ndf_seeds.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b06d08dae3de01c64a739f354176c430adbdfcac"},"cell_type":"markdown","source":"We want to create a submission file which lists every possible game that could have been played.  We're going to ignore the fact that most of them weren't played.  At this point we are operating on no information of what actually occurred.  We want each game to be listed once and only once.  The following code does this."},{"metadata":{"trusted":true,"_uuid":"926b2e75b34bf17176528cc64dbc3d307285c7da"},"cell_type":"code","source":"# Merge the seeds file with itself on Season.  This creates every combination of two teams by season.\ndf_sub = df_seeds.merge(df_seeds, how='inner', on='Season')\n\n# We want a little less than half the records in this data frame.  \n# Every game appears twice, once with the lower id team in the TeamID_x column, and \n# once in the TeamID_y column.  We also have the impossible matchups of teams with themselves.\n# To fix this, we keep only the games where the lower team ID is in the TeamID_x columns.\ndf_sub = df_sub[df_sub['TeamID_x'] < df_sub['TeamID_y']]\n\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"661e6dc9dc43e712f764190088ddb967cd1b5f31"},"cell_type":"markdown","source":"We are now able to make our submission file.  The convention is to write the game ID as a 14 character string of the form 'XXXX_YYYY_ZZZZ', where 'XXXX' is the season, 'YYYY' is the lower team ID, and 'ZZZZ' is the greater team ID.  For the prediction column ('Pred'), we'll get the integer value of the seed and use the formula\n\nPr(Team X beats Team Y) = 0.5 + 0.03*(Seed(Team Y) - Seed(Team X))"},{"metadata":{"trusted":true,"_uuid":"1c74222c1f792e1fb6667b8d4c6cee01a7e8796a"},"cell_type":"code","source":"df_sub['ID'] = df_sub['Season'].astype(str) + '_' \\\n              + df_sub['TeamID_x'].astype(str) + '_' \\\n              + df_sub['TeamID_y'].astype(str)\n\ndf_sub['SeedInt_x'] = [int(x[1:3]) for x in df_sub['Seed_x']]\ndf_sub['SeedInt_y'] = [int(x[1:3]) for x in df_sub['Seed_y']]\n\ndf_sub['Pred'] = 0.5 + 0.03*(df_sub['SeedInt_y'] - df_sub['SeedInt_x'])\n\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50a999692a929c97467ef5ac01db4afe1b2f7dcc"},"cell_type":"markdown","source":"The only thing left is to get rid of the unnecessary columns and pare down to just 'ID' and 'Pred'.  Just before we do that, let's save out the submission for 2014-2018 as a csv file so we can submit it to Kaggle for Stage 1."},{"metadata":{"trusted":true,"_uuid":"8c6d0f9aa9dee62a17a752df037739dd75fa07fc"},"cell_type":"code","source":"# save out the 2014-2018 predictions for later submission\ndf_sub.loc[(df_sub['Season'] >= 2014) & (df_sub['Season'] <= 2018), ['ID', 'Pred']].to_csv('./Submission.csv',index=False)\n\n# now pare down existing df_sub\ndf_sub = df_sub[['ID','Pred']]\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"befc99b786ee3b5d09b1b3c2333c9d401979901a"},"cell_type":"markdown","source":"# III Creating a Results File\n\nNow that we have a submission file that we want to score, we need a results file to score against. We will create one with every relevant tournament game since 1985.  To get this, we will start with the compact tourney results:"},{"metadata":{"trusted":true,"_uuid":"7ccd40adebaa2929894a79c8b5a84f3b3004eb1a"},"cell_type":"code","source":"df_tc = pd.read_csv(os.path.join(data_directory,'NCAATourneyCompactResults.csv'))\ndf_tc.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5f3fdd45a9b64363c29a214847157ea4c73a355"},"cell_type":"markdown","source":"We will now generate the column for the game ID and the column with the result.  The convention is to write the game ID as a 14 character string of the form 'XXXX_YYYY_ZZZZ', where 'XXXX' is the season, 'YYYY' is the lower team ID, and 'ZZZZ' is the greater team ID.  For the Result, the convention is to record a result of 1 if the lower team id team wins, and a result of 0 if the greater team id wins.\n\nA quick way to do this using pandas is the following:"},{"metadata":{"trusted":true,"_uuid":"ac42eff7000d93f6ea371cb5b4ca282abcd2bc73"},"cell_type":"code","source":"df_tc['ID'] = df_tc['Season'].astype(str) + '_' \\\n              + (np.minimum(df_tc['WTeamID'],df_tc['LTeamID'])).astype(str) + '_' \\\n              + (np.maximum(df_tc['WTeamID'],df_tc['LTeamID'])).astype(str)\n\ndf_tc['Result'] = 1*(df_tc['WTeamID'] < df_tc['LTeamID'])\ndf_tc.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e64ab5caf45764916217df0f0bc91223ee16bdd1"},"cell_type":"markdown","source":"Up until 2001, this would have been all that we needed. However, things got more complicated in 2001 with the addition of \"play-in\" games. Why is this more complicated? In this competition, we are scored only on the games that occur in the official tournament (i.e., Round 1 or later). To get the real results file, we need to get rid of the play-in games that appear in this data frame. To do that, we have to figure out which tournament games were play-in games.  Fortunately, this isn't too bad.  We just first have to add the seed information for the winning and losing teams."},{"metadata":{"trusted":true,"_uuid":"f296f8ef3d3151f4944e8fc4aae76dbcb44122e1"},"cell_type":"code","source":"df_tc = df_tc.merge(df_seeds.rename(columns={'Seed':'WSeed','TeamID':'WTeamID'}), \n                    how='inner', on=['Season', 'WTeamID'])\ndf_tc = df_tc.merge(df_seeds.rename(columns={'Seed':'LSeed','TeamID':'LTeamID'}), \n                    how='inner', on=['Season', 'LTeamID'])\ndf_tc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e93b201ac27fc05e1336883928371c16b5f96783"},"cell_type":"markdown","source":"The play-in games are the games where the the first three characters of the teams' seeds are identical.  For example, in one play-in game, you might see seed 'X16a' facing seed 'X16b'.  Only in a play-in game will you see both seeds start with 'X16'."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_playin = df_tc[df_tc['WSeed'].str[0:3] == df_tc['LSeed'].str[0:3]]\ndf_playin.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4323a76bbadf1ac24caa73bbc96d7f815029091"},"cell_type":"markdown","source":"We can thus get rid of these games from the tourney results dataframe in the following way:"},{"metadata":{"trusted":true,"_uuid":"444a4e6e7c4e142ed4ab47a30b36785f342009ea"},"cell_type":"code","source":"df_tc = df_tc[df_tc['WSeed'].str[0:3] != df_tc['LSeed'].str[0:3]]\ndf_tc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"775269cafef772edf0b94507548b96129f309be6"},"cell_type":"markdown","source":"# IV Scoring Predictions Against Results\n\nIn this section we present a function which scores a submission file against an existing results file.  We use the log loss as defined by Kaggle (including the clipping), but when we score the submission, we will make note of which predictions contained obvious errors."},{"metadata":{"trusted":true,"_uuid":"a50975da872d95d5cad0b2e60bfff44fa52c1285"},"cell_type":"code","source":"def kaggle_clip_log(x):\n    '''\n    Calculates the natural logarithm, but with the argument clipped within [1e-15, 1 - 1e-15]\n    '''\n    return np.log(np.clip(x,1.0e-15, 1.0 - 1.0e-15))\n\ndef kaggle_log_loss(pred, result):\n    '''\n    Calculates the kaggle log loss for prediction pred given result result\n    '''\n    return -(result*kaggle_clip_log(pred) + (1-result)*kaggle_clip_log(1.0 - pred))\n    \ndef score_submission(df_sub, df_results, on_season=None, return_df_analysis=True):\n    '''\n    Scores a submission against relevant tournament results\n    \n    Parameters\n    ==========\n    df_sub: Pandas dataframe containing predictions to be scored (must contain a column called 'ID' and \n            a column called 'Pred')\n            \n    df_results: Pandas dataframe containing results to be compared against (must contain a column \n            called 'ID' and a column called 'Result')\n            \n    on_season: array-like or None.  If array, should contain the seasons for which a score should\n            be calculated.  If None, will use all seasons present in df_results\n            \n    return_df_analysis: Bool.  If True, will return the dataframe used for calculations.  This is useful\n            for future analysis\n            \n    Returns\n    =======\n    df_score: pandas dataframe containing the average score over predictions that were scorable per season\n           as well as the number of obvious errors encountered\n    df_analysis:  pandas dataframe containing information about all results used in scoring\n                  Only provided if return_df_analysis=True\n    '''\n    \n    df_analysis = df_results.copy()\n    \n    # this will overwrite if there's already a season column but it should be the same\n    df_analysis['Season'] = [int(x.split('_')[0]) for x in df_results['ID']]\n    \n    if not on_season is None:\n        df_analysis = df_analysis[np.in1d(df_analysis['Season'], on_season)]\n        \n    # left merge with the submission.  This will keep all games for which there\n    # are results regardless of whether there is a prediction\n    df_analysis = df_analysis.merge(df_sub, how='left', on='ID')\n    \n    # check to see if there are obvious errors in the predictions:\n    # Obvious errors include predictions that are less than 0, greater than 1, or nan\n    # You can add more if you like\n    df_analysis['ObviousError'] = 1*((df_analysis['Pred'] < 0.0) \\\n                                  | (df_analysis['Pred'] > 1.0) \\\n                                  | (df_analysis['Pred'].isnull()))\n    \n    df_analysis['LogLoss'] = kaggle_log_loss(df_analysis['Pred'], df_analysis['Result'])\n    \n    df_score = df_analysis.groupby('Season').agg({'LogLoss' : 'mean', 'ObviousError': 'sum'})\n    \n    if return_df_analysis:\n        return df_score, df_analysis\n    else:\n        return df_score\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6a4a3ef738bfdc17e8792f66fcd65148cc5cb1f"},"cell_type":"markdown","source":"As a test, let's look at the resulting scores from our submission and results files.  Let's look at 2014-2018, because that's what the official Stage 1 scoring does."},{"metadata":{"trusted":true,"_uuid":"c5d90fefc51ba0a1cb8432d8a5aa08a425591666"},"cell_type":"code","source":"df_score, df_analysis = \\\n    score_submission(df_sub, df_tc, on_season = np.arange(2014,2019), return_df_analysis=True)\ndf_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"414f41deedd3777bdfe8a313a71b5f674a4bae45"},"cell_type":"markdown","source":"Yay! No obvious errors encountered.  And the mean score is the same as what Kaggle claims (I've checked using the csv written out earlier):"},{"metadata":{"trusted":true,"_uuid":"b035339013529a088f698557104fcdda29cef738"},"cell_type":"code","source":"df_score.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f508acc2e2a3a42d9853fd1fd2401edc5dd19be"},"cell_type":"markdown","source":"While not great, the simple seed model is not that bad.  There have been a few recent tournaments where it was quite good.  What about if we expand over a few more years?"},{"metadata":{"trusted":true,"_uuid":"15b8f05697e8360db9599caf7ffa87f8ee86b44f"},"cell_type":"code","source":"df_score, df_analysis = \\\n    score_submission(df_sub, df_tc, on_season = np.arange(2010,2018), return_df_analysis=True)\ndf_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f12ed8233f22a50b4c47a491af46917d8545641"},"cell_type":"markdown","source":"Again, success!  What if we randomly forget to make a few predictions?  Let's submit only half as many predictions."},{"metadata":{"trusted":true,"_uuid":"a21fd3800a3cc84de03778279034360402275446"},"cell_type":"code","source":"df_score, df_analysis = \\\n    score_submission(df_sub.sample(frac=0.5), df_tc, on_season = np.arange(2010,2018), return_df_analysis=True)\ndf_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bbb3dcdf710178c4cd16c4579c8e94e92b66489"},"cell_type":"markdown","source":"Now there are lots of errors because we didn't submit predictions for half the games (some of which the scoring function looked for).  Note that the scoring in this function will just ignore missing values for the purposes of calculating the mean score.  Kaggle is not so kind, but I don't know if they give you the max penalty for null values or if they just refuse to accept your submission.\n\nWe can look at which games had an error by entering something like the following:"},{"metadata":{"trusted":true,"_uuid":"9b676598940fb23e946b777e2c9ee62dbd72fb0d"},"cell_type":"code","source":"df_analysis[df_analysis['ObviousError']==1].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f775b401ab50faaf52433dd7b5ac73270e853bc8"},"cell_type":"markdown","source":"Scrolling over to the right, we see the reason for the 'obvious' error: there's not prediction for a game that occurred.  Similarly, we should see an obvious problem if the prediction is less than 0 or greater than 1."},{"metadata":{"_uuid":"e8775710275ab7b7324712ff8e0297ad649b5ac7"},"cell_type":"markdown","source":"# V Analyzing Results for the Simple Seed Model\n\nLet's consider all years in the results file."},{"metadata":{"trusted":true,"_uuid":"bcf26891245ae3d70b686b5b54c46046c6cd5cf7"},"cell_type":"code","source":"df_score, df_analysis = \\\n    score_submission(df_sub, df_tc, on_season = None, return_df_analysis=True)\n\ndf_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2dae394bbe4cc8913e1117b6e6af1f79e515471"},"cell_type":"markdown","source":"We can look at the mean score over all years:"},{"metadata":{"trusted":true,"_uuid":"bca019d497b7627f782222251b2981680f79dd12"},"cell_type":"code","source":"df_score.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99672ecbf5801edb336e94cf5c06c06e37450617"},"cell_type":"markdown","source":"We can plot the score as a function of year "},{"metadata":{"trusted":true,"_uuid":"10b4a6cf6ea055d2aceeb75ecb4f879ca4128f7f"},"cell_type":"code","source":"df_score.reset_index().plot('Season','LogLoss')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd42ced5fe85a359855f53abf72f2c0455a60fb9"},"cell_type":"markdown","source":"We can make a histogram of the yearly score"},{"metadata":{"trusted":true,"_uuid":"2ab70b0fa6511c224b44ab17bde1a3a6005de579"},"cell_type":"code","source":"df_score.hist('LogLoss',bins='auto')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d89ac54332f96b661a6bee3882d19f3b58319a0"},"cell_type":"markdown","source":"And we can use  df_analysis to look at the log loss from individual games.  Let's sort df_analysis in order of descending log loss and look at the worst offenders.  Let's also make a histogram of the log losses that we have experienced."},{"metadata":{"trusted":true,"_uuid":"c534dd067d7010056bb4ec8be65e8ab73130dd99"},"cell_type":"code","source":"df_analysis.hist('LogLoss',bins=10)\ndf_analysis.sort_values('LogLoss',ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06b7b1846d0790ed9b00a3e01402856e2372ba5"},"cell_type":"markdown","source":"No surprise that the worst scoring prediction we've had is from last year, when a number 16 seed upset a number one seed for the first time in history.  That's especially bad for a model that bases its entire prediction on the seed difference of the teams.\n\nFor models that are not based just on seeds, we could similarly go through the process that we have gone through:  Create a submission, test against the tournament results, and look at which individual games contributed to our losses.  Ideally, we'll be able to spot some pattern in the worst offenders and then tweak our models so that we are not prone to those types of errors in the future.  But, as always beware of overtraining and leakage!\n\n"},{"metadata":{"trusted":true,"_uuid":"319615aa1f9f728b02f867346ea021a146667be6"},"cell_type":"markdown","source":"# Conclusion\n\nWe've gone through how to create a submission file from scratch based on a seed difference model, how to create a results file containing only the relevant tournament games, and how to score that submission file against the relevant results in such a way that you can avoid obvious errors and gain transparency into what individual games are most affecting your results.  I hope that you have found this kernel instructive and I look forward to your feedback.\n\nBest of luck in the competition."},{"metadata":{"trusted":true,"_uuid":"23f26d5783e6569aa2fb4d22162a714018e89e72"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}