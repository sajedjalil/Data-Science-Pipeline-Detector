{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"735b88b4-ddfe-8dd4-b4d1-b2cfb1837a2d","_active":false},"source":"This is my final for my Machine Learning class.  In the end I should have picked a easier project. When I selected to try and compete in the competition I had high hopes that I would get an A and $25,000. Now, I am ready to settle for a C and probably never submit a sample to get checked. I had been working on the competition for a week before I started doing it as a school project and I really had no clue what I was doing but looking at other kernels got me started and honestly carried me through this. Big shout out to Radu Stoicescu, he has an amazing kernel that uses blob detection to find coordinates of the sea lions. I used his methods and  then used a classifier I had used in class. I give little sections throughout on what/ why I did things. If you are someone besides my Professor feel free to look through and see what I did, it might help. Comment if you really want to. Wasn't the best project but I had a good time and can say I tried.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"301cd266-eae6-5612-cabc-10d47709c7f8","_active":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ba944df-1a1e-48e3-6f75-1956acc67df3","_active":false},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport skimage.feature\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport keras\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, Cropping2D\nfrom keras.utils import np_utils\nimport random\nfrom random import randint\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.model_selection import cross_val_predict\nimport pylab as pl\nimport seaborn as sn\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom sklearn.neural_network import MLPClassifier\nfrom collections import Counter\nfrom sklearn.metrics import roc_auc_score\n\n\n%matplotlib inline","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"80fade0f-adbb-0623-d444-074c9839bc40","_active":false},"source":"Displaying Pictures of training and training dotted, then a zoomed in version to give a better view","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e75dc320-2445-4827-bc4a-d2c27ae29e3a","_active":false},"outputs":[],"source":"train_data = pd.read_csv('../input/Train/train.csv')\ntrain_imgs = sorted(glob.glob('../input/Train/*.jpg'), key=lambda name: int(os.path.basename(name)[:-4]))\ntrain_dot_imgs = sorted(glob.glob('../input/TrainDotted/*.jpg'), key=lambda name: int(os.path.basename(name)[:-4]))\nindex = 0\n\nsl_counts = train_data.iloc[index]\nprint(sl_counts)\n\nprint(train_imgs[index])\nimg = cv2.cvtColor(cv2.imread(train_imgs[index]), cv2.COLOR_BGR2RGB)\nimg_dot = cv2.cvtColor(cv2.imread(train_dot_imgs[index]), cv2.COLOR_BGR2RGB)\n\nf, ax = plt.subplots(1,2,figsize=(16,8))\n(ax1, ax2) = ax.flatten()\n\nax1.imshow(img)\nax2.imshow(img_dot)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12e44de3-ac27-e940-9305-a30707cc77f7","_active":false},"outputs":[],"source":"crop_img = img[1350:1900, 3000:3400]\ncrop_img_dot = img_dot[1350:1900, 3000:3400]\n\nf, ax = plt.subplots(1,2,figsize=(16,8))\n(ax1, ax2) = ax.flatten()\n\nax1.imshow(crop_img)\nax2.imshow(crop_img_dot)\n\nplt.show()","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"023af638-948d-776f-b70b-6c5892e1223b","_active":false},"source":"Histograms to show the distribution of data within the entire dataset and then of the first picture which was the main picture used to create a training set.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"801292ee-f397-dbb5-f2dc-3ffde1d8c42b","_active":false},"outputs":[],"source":"hist = train_data.sum(axis=0)\nprint(hist)\n\n\nsea_lions_types = hist[1:]\nf, ax1 = plt.subplots(1,1,figsize=(5,5))\nsea_lions_types.plot(kind='bar', title='Count of Sea Lion Types (Train)', ax=ax1)\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"342a27ce-546b-f8b4-324b-f4f04d740cb5","_active":false},"outputs":[],"source":"index = 0\nsl_counts = train_data.iloc[index]\nprint(sl_counts)\n\nplt.figure()\nsl_counts.plot(kind='bar', title='Count of Sea Lion Types')\nplt.show()","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"e72c4ac6-23ee-563a-b7a6-8b115412367f","_active":false},"source":"Radu Stoicescu Blob detection method. The only major thing that I changed was the sample size, I used the first, second, and third picture. But honestly it wouldn't have made that much of a difference. As shown in the histogram below that the second and third photos don't contribute much to the sea lions, but they add about 100 more 'negative' photos each, which is important for making a model.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"326f074e-f67c-4f53-4acb-42b754a9c490","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nclass_names = ['adult_females', 'adult_males', 'juveniles', 'pups', 'subadult_males']\n\nfile_names = os.listdir(\"../input/Train/\")\nfile_names = sorted(file_names, key=lambda \n                    item: (int(item.partition('.')[0]) if item[0].isdigit() else float('inf'), item)) \n\n# select a subset of files to run on\nfile_names = file_names[0:3]\nprint(file_names)\n# dataframe to store results in\ncoordinates_df = pd.DataFrame(index=file_names, columns=class_names)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f4fbe00-dd4b-d900-9644-8c8ae21b641d","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nfor filename in file_names:\n    \n    # read the Train and Train Dotted images\n    image_1 = cv2.imread(\"../input/TrainDotted/\" + filename)\n    image_2 = cv2.imread(\"../input/Train/\" + filename)\n    \n    cut = np.copy(image_2)\n    \n    # absolute difference between Train and Train Dotted\n    image_3 = cv2.absdiff(image_1,image_2)\n    \n    # mask out blackened regions from Train Dotted\n    mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n    mask_1[mask_1 < 20] = 0\n    mask_1[mask_1 > 0] = 255\n    \n    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n    mask_2[mask_2 < 20] = 0\n    mask_2[mask_2 > 0] = 255\n    \n    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_2) \n    \n    # convert to grayscale to be accepted by skimage.feature.blob_log\n    image_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2GRAY)\n    \n    # detect blobs\n    blobs = skimage.feature.blob_log(image_3, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n    \n    adult_males = []\n    subadult_males = []\n    pups = []\n    juveniles = []\n    adult_females = [] \n    \n    image_circles = image_1\n    \n    for blob in blobs:\n        # get the coordinates for each blob\n        y, x, s = blob\n        # get the color of the pixel from Train Dotted in the center of the blob\n        g,b,r = image_1[int(y)][int(x)][:]\n        \n        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n        if r > 200 and g < 50 and b < 50: # RED\n            adult_males.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (0,0,255), 10) \n        elif r > 200 and g > 200 and b < 50: # MAGENTA\n            subadult_males.append((int(x),int(y))) \n            cv2.circle(image_circles, (int(x),int(y)), 20, (250,10,250), 10)\n        elif r < 100 and g < 100 and 150 < b < 200: # GREEN\n            pups.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (20,180,35), 10)\n        elif r < 100 and  100 < g and b < 100: # BLUE\n            juveniles.append((int(x),int(y))) \n            cv2.circle(image_circles, (int(x),int(y)), 20, (180,60,30), 10)\n        elif r < 150 and g < 50 and b < 100:  # BROWN\n            adult_females.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (0,42,84), 10)  \n            \n        cv2.rectangle(cut, (int(x)-112,int(y)-112),(int(x)+112,int(y)+112), 0,-1)\n            \n    coordinates_df[\"adult_males\"][filename] = adult_males\n    coordinates_df[\"subadult_males\"][filename] = subadult_males\n    coordinates_df[\"adult_females\"][filename] = adult_females\n    coordinates_df[\"juveniles\"][filename] = juveniles\n    coordinates_df[\"pups\"][filename] = pups","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c8eadcf-325f-6c97-6743-fc1e97543834","_active":false},"outputs":[],"source":"f, ax = plt.subplots(1,1,figsize=(10,16))\nax.imshow(cv2.cvtColor(image_circles, cv2.COLOR_BGR2RGB))\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18f1682c-127a-1c64-d749-420ee56c8951","_active":false},"outputs":[],"source":"f, ax = plt.subplots(1,1,figsize=(10,16))\nax.imshow(cv2.cvtColor(cut, cv2.COLOR_BGR2RGB))\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7c97094-187d-bf60-3ba8-edb1e11822ba","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nx = []\ny = []\n\nfor filename in file_names:    \n    image = cv2.imread(\"../input/Train/\" + filename)\n    for lion_class in class_names:\n        for coordinates in coordinates_df[lion_class][filename]:\n            thumb = image[coordinates[1]-32:coordinates[1]+32,coordinates[0]-32:coordinates[0]+32,:]\n            if np.shape(thumb) == (64, 64, 3):\n                x.append(thumb)\n                y.append(lion_class)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bc7d83e-dd12-b301-9e7b-ff42b6eb8756","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nfor i in range(0,np.shape(cut)[0],224):\n    for j in range(0,np.shape(cut)[1],224):                \n        thumb = cut[i:i+64,j:j+64,:]\n        if np.amin(cv2.cvtColor(thumb, cv2.COLOR_BGR2GRAY)) != 0:\n            if np.shape(thumb) == (64,64,3):\n                x.append(thumb)\n                y.append(\"negative\")  ","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c57e5262-91e5-ca2e-2cf0-1a10aea4ffc6","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nclass_names.append(\"negative\")\ny.count('negative')","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"8c2ce861-901b-19c4-abe5-5b74dc44f5c0","_active":false},"source":"In previous cell it shows the count of negative photo count to be 337.  It is important to have a lot of negatives and in the future I feel it would be better to add more. Histogram and Table showing sea lion counts and distribution.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f980c069-1338-6521-f637-e56e47f4398f","_active":false},"outputs":[],"source":"sl_counts = train_data.iloc[0:3]\nprint(sl_counts)\n\nplt.figure()\nsl_counts.plot(kind='bar', title='Count of Sea Lion Types')\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dceba86-8acc-4ea8-1c1b-2c2fecbc70c0","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nx = np.array(x)\ny = np.array(y)","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"04ced0c4-edc7-7b38-8a45-fac7c6b09754","_active":false},"source":"I did this next cell, the purpose was that the multi layer classifier I used doesn't accept anything over 2D so I converted from (1368,64,64,3) to (1368,12288) small but very important change for what I did.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"662c9ec8-e7c7-6691-130b-c9689894f091","_active":false},"outputs":[],"source":"z = []\nfor img in x:\n    img = np.array(img).reshape(12288)\n    z.append(img)\nz = np.array(z)\nx.shape","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee4bea6b-869a-7ffc-9486-2a1cba478561","_active":false},"source":"Really cool function that displays the first 10 pictures of the classes. But I think it has some insight into why the model may be very inaccurate when there are females present but no pups. In the pups row pictures 7 and 9 are the exact same. But in 7 the picture is focused on the pup and in 9 the picture is focused on the female. I believe that this is causing the model to confuse females and pups and misidentify them. ","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40363d9c-c2e8-f7c9-1021-8ba046675d52","_active":false},"outputs":[],"source":"# CREDITS GO TO:  Radu Stoicescu\nfor lion_class in class_names:\n    f, ax = plt.subplots(1,10,figsize=(12,1.5))\n    f.suptitle(lion_class)\n    axes = ax.flatten()\n    j = 0\n    for a in axes:\n        a.set_xticks([])\n        a.set_yticks([])\n        for i in range(j,len(x)):\n            if y[i] == lion_class:\n                j = i+1\n                a.imshow(cv2.cvtColor(x[i], cv2.COLOR_BGR2RGB))\n                break","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9438ff2c-7112-d066-27d3-3eda2bcbbcfc","_active":false},"outputs":[],"source":"encoder = LabelBinarizer()\nencoder.fit(y)\ny = encoder.transform(y).astype(float)","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"d79a2e4b-a84f-1f4d-578c-ab53fa6e15e5","_active":false},"source":"Initiating the Multilayer classifier. And yes it would be a much better idea to use a neural network but I wanted to try and get the classifier to work since I'm classifying sea lions. The model can pretty easily get to around 90% accuracy but above that is hard to attain. And for my purposes it works. ","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89f61ce9-6ec6-1525-b94b-f9c4b9e9f01d","_active":false},"outputs":[],"source":"clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(100, 100), random_state=42)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd596818-cc9a-a225-dc6f-a4e9e81c7fae","_active":false},"outputs":[],"source":"clf.fit(z,y)","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"8912c8e3-84ff-7865-980f-c783a81dec5a","_active":false},"source":"Now I create my test set. I did it in the exact same fashion as the training set. But the key difference was that I only did one picture at a time. Again this is not even close to what the competition would call for because I'm extracting images from the training images and not iterating over the test images and counting what sea lions I see. But this works with what I wanted to accomplish.","outputs":[],"execution_count":null,"execution_state":"idle"},{"metadata":{"_cell_guid":"61ded6c8-8a83-0cfa-5545-fb97b0aadf9d","_active":false,"collapsed":false},"source":"# CREDITS GO TO:  Radu Stoicescu\nclass_names = ['adult_females', 'adult_males', 'juveniles', 'pups', 'subadult_males']\n\nfile_names = os.listdir(\"../input/Train/\")\nfile_names = sorted(file_names, key=lambda \n                    item: (int(item.partition('.')[0]) if item[0].isdigit() else float('inf'), item)) \n\n# select a subset of files to run on\nfile_names = file_names[8:9]\n\n# dataframe to store results in\ncoordinates_df = pd.DataFrame(index=file_names, columns=class_names)\n\n\nfor filename in file_names:\n    \n    # read the Train and Train Dotted images\n    image_1 = cv2.imread(\"../input/TrainDotted/\" + filename)\n    image_2 = cv2.imread(\"../input/Train/\" + filename)\n    \n    cut = np.copy(image_2)\n    \n    # absolute difference between Train and Train Dotted\n    image_3 = cv2.absdiff(image_1,image_2)\n    \n    # mask out blackened regions from Train Dotted\n    mask_1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)\n    mask_1[mask_1 < 20] = 0\n    mask_1[mask_1 > 0] = 255\n    \n    mask_2 = cv2.cvtColor(image_2, cv2.COLOR_BGR2GRAY)\n    mask_2[mask_2 < 20] = 0\n    mask_2[mask_2 > 0] = 255\n    \n    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_1)\n    image_3 = cv2.bitwise_or(image_3, image_3, mask=mask_2) \n    \n    # convert to grayscale to be accepted by skimage.feature.blob_log\n    image_3 = cv2.cvtColor(image_3, cv2.COLOR_BGR2GRAY)\n    \n    # detect blobs\n    blobs = skimage.feature.blob_log(image_3, min_sigma=3, max_sigma=4, num_sigma=1, threshold=0.02)\n    \n    adult_males = []\n    subadult_males = []\n    pups = []\n    juveniles = []\n    adult_females = [] \n    \n    image_circles = image_1\n    \n    for blob in blobs:\n        # get the coordinates for each blob\n        y, x, s = blob\n        # get the color of the pixel from Train Dotted in the center of the blob\n        g,b,r = image_1[int(y)][int(x)][:]\n        \n        # decision tree to pick the class of the blob by looking at the color in Train Dotted\n        if r > 200 and g < 50 and b < 50: # RED\n            adult_males.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (0,0,255), 10) \n        elif r > 200 and g > 200 and b < 50: # MAGENTA\n            subadult_males.append((int(x),int(y))) \n            cv2.circle(image_circles, (int(x),int(y)), 20, (250,10,250), 10)\n        elif r < 100 and g < 100 and 150 < b < 200: # GREEN\n            pups.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (20,180,35), 10)\n        elif r < 100 and  100 < g and b < 100: # BLUE\n            juveniles.append((int(x),int(y))) \n            cv2.circle(image_circles, (int(x),int(y)), 20, (180,60,30), 10)\n        elif r < 150 and g < 50 and b < 100:  # BROWN\n            adult_females.append((int(x),int(y)))\n            cv2.circle(image_circles, (int(x),int(y)), 20, (0,42,84), 10)  \n            \n        cv2.rectangle(cut, (int(x)-112,int(y)-112),(int(x)+112,int(y)+112), 0,-1)\n            \n    coordinates_df[\"adult_males\"][filename] = adult_males\n    coordinates_df[\"subadult_males\"][filename] = subadult_males\n    coordinates_df[\"adult_females\"][filename] = adult_females\n    coordinates_df[\"juveniles\"][filename] = juveniles\n    coordinates_df[\"pups\"][filename] = pups\n\n\n\nx_test = []\ny_test = []\n\nfor filename in file_names:    \n    image = cv2.imread(\"../input/Train/\" + filename)\n    for lion_class in class_names:\n        for coordinates in coordinates_df[lion_class][filename]:\n            thumb = image[coordinates[1]-32:coordinates[1]+32,coordinates[0]-32:coordinates[0]+32,:]\n            if np.shape(thumb) == (64, 64, 3):\n                x_test.append(thumb)\n                y_test.append(lion_class)\n\nfor i in range(0,np.shape(cut)[0],224):\n    for j in range(0,np.shape(cut)[1],224):                \n        thumb = cut[i:i+64,j:j+64,:]\n        if np.amin(cv2.cvtColor(thumb, cv2.COLOR_BGR2GRAY)) != 0:\n            if np.shape(thumb) == (64,64,3):\n                x_test.append(thumb)\n                y_test.append(\"negative\") \nclass_names.append(\"negative\")\nx_test = np.array(x_test)\ny_test = np.array(y_test)\nz_test = []\nfor img in x_test:\n    img = np.array(img).reshape(12288)\n    z_test.append(img)\nz_test = np.array(z_test)\n\nencoder = LabelBinarizer()\nencoder.fit(y_test)\ny_test = encoder.transform(y_test)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"4b081913-2d21-267f-940f-842930bf9ce9","_active":false},"source":"Predicting accuracy: The first value in the array of three is what the first runs accuracy had for output so I took that to be the main run and what I based my success on.  So, with the funs and the accuracy calculations I have strong evidence that the model is getting Females and Pups confused. Picture 8 below shows 2% accuracy. But if we inspect eh histogram below we see that most of the data is pup and females. Then in Picture 4 there was 87% accuracy for predicting classes and looking at the histogram below there were very few females and no pups present. This may be a coincidence and further study is needed but this is a very interesting trend.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c0409b3-671d-733c-3c15-8d4e6965fff8","_active":false},"outputs":[],"source":"cvs = cross_val_score(clf, z_test, y_test, cv=3, scoring=\"accuracy\")","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84ef5314-ea64-b59f-6a2b-6aa2a9dd19a5","_active":false},"outputs":[],"source":"cvs","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04c121d8-f0cd-2941-47eb-f456b1478f05","_active":false},"outputs":[],"source":"index = 8\nsl_counts = train_data.iloc[index]\nprint(sl_counts)\n\nplt.figure()\nsl_counts.plot(kind='bar', title='Count of Sea Lion Types')\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65b11bbb-6150-83b8-b290-ea83568ba113","_active":true},"outputs":[],"source":"cvs = cross_val_score(clf, z_test, y_test, cv=3, scoring=\"accuracy\")\ncvs","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54af2687-4bca-e6fe-c426-49bedc9cedda","_active":false},"outputs":[],"source":"index = 4\nsl_counts = train_data.iloc[index]\nprint(sl_counts)\n\nplt.figure()\nsl_counts.plot(kind='bar', title='Count of Sea Lion Types')\nplt.show()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00db7b84-f6aa-10ba-c2df-60b611174c69","_active":false},"outputs":[],"source":null,"execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"7211a880-3b8a-e6bf-94d3-8188a4efbe45","_active":false},"source":"Frustrating to say the least: For the 4th picture I got 87% accuracy, 5th was 61%, 6th is 61%, 8th was 2%, 10th 81% and 12 was 81%. I really have no idea what to think about this. I believe finding the perfect distribution of data would be the best idea for trying to get a better model in my case.  Then I also believe that it is a valid point to say that pups and females are being confused and try to develop pictures where it is just isolated pups not have most of the pictures be a female and a pup. Then adding a lot of different negatives, there were all types of pictures with all types of topography and getting a good selection of different negatives would be ideal. The next goal would be to use a neural network and not a classifier like I did. I worked with Tensorflow for about 5 days then scrapped it because I wasn't getting anywhere. This was a bit of a baptism by fire into my first data science big project but I'm going to take what I learned and try and do better next time.   ","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33f45328-c789-4070-2d72-46151ff93707","_active":false},"outputs":[],"source":null,"execution_state":"idle"}]}