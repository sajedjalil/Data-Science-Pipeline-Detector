{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/XoU1Jmh.png\">\n\n<h1><center>- Building a Smarter and Faster Path to Greatness -</center></h1>\n\n#### Functions below ‚¨á"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../input/2020-cost-of-living/alerts.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"</style>\")\ncss_styling()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"HTML(\"\"\"\n<style>\n@import url('https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap');\n</style>\n\"\"\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def add_unique_id(data):\n    '''Adds new column with a unique id to the original survey data.'''\n    \n    # Add unique ID\n    data[\"id\"] = data.index + 1\n    return data\n\n\ndef get_McMeal_units(data):\n    '''Returns a table containing per ID the pay, the middle value and the McMeal units - our new currency.\n    data: the raw survey data'''\n    \n    # There is no missing data for country\n    pay = \"What is your current yearly compensation (approximate $USD)?\"\n    country = \"In which country do you currently reside?\"\n\n    df = data[['id', pay, country]]\n\n    # Remove people with NA for yearly compensation\n    # This operation removes half of the data\n    df = df[df[pay].isna()!=True].reset_index(drop=True)\n\n    # Remove Country == Other\n    df = df[df[country]!='Other'].reset_index(drop=True)\n\n    # Compute middle pay (the middle of the interval)\n    df[pay] = df[pay].apply(lambda x: x.replace('$', '').strip())\n    df[pay] = df[pay].apply(lambda x: x.replace('>', '').strip())\n\n    df['lower_pay'] = df[pay].apply(lambda x: int(x.split('-')[0].replace(',', '')))\n    df['upper_pay'] = df[pay].apply(lambda x: int(x.split('-')[1].replace(',', '')) \n                                    if x.split('-')[0] != '500,000' \n                                    else 1000000)\n    df['middle_pay'] = -(-(df['upper_pay'] + df['lower_pay']) // 2)\n\n    # Rename columns\n    df.columns = ['id', 'Pay', 'Country', 'lower_pay', 'upper_pay', 'middle_pay']\n\n    # Add the price of a McDonalds Menu\n    df = df.replace({'United States of America':'United States',\n                     'Viet Nam':'Vietnam',\n                     'United Kingdom of Great Britain and Northern Ireland' : 'United Kingdom',\n                     'Iran, Islamic Republic of...' : 'Iran',\n                     'Republic of Korea' : 'South Korea'})\n\n    merged = pd.merge(left=df, right=col_data[['Country', 'McMeal($)']], how='inner', on='Country')\n\n    # Remove countries where no McMeal price was found (meaning Ghana)\n    merged = merged[merged['Country'] != 'Ghana']\n\n    # Compute the McMeal(units)\n    merged['McMeal(units)'] = -(-merged['middle_pay'] // merged['McMeal($)']).astype(int)\n    \n    return merged[['id', 'Country', 'middle_pay', 'McMeal(units)']]\n\n      \n\n        \n\ndef show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    '''Plots the value at the end of the a seaborn barplot.\n    axs: the ax of the plot\n    h_v: weather or not the barplot is vertical/ horizontal'''\n    \n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() / 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, format(value, ','), ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, format(value, ','), ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n        \n        \n\ndef segment_units(x):\n    '''Segments the McMeal units to Low, High, very High and Crazy High.\n    Use with .apply() function on the McMeal(units) column. '''\n    \n    if x < 10000:\n        return 'Other'\n    elif x >= 10000 and x < 20000:\n        return 'High'\n    elif x >= 20000 and x < 50000:\n        return 'Very High'\n    else:\n        return 'Crazy High'\n    \n    \n    \ndef frame_image(img, frame_width):\n    '''Adds a border to the image.\n    source: https://stackoverflow.com/questions/43261338/adding-a-border-to-and-image-in-my-code'''\n    \n    b = frame_width # border size in pixel\n    ny, nx = img.shape[0], img.shape[1] # resolution / number of pixels in x and y\n    if img.ndim == 3: # rgb or rgba array\n        framed_img = np.zeros((b+ny+b, b+nx+b, img.shape[2]))\n    elif img.ndim == 2: # grayscale image\n        framed_img = np.zeros((b+ny+b, b+nx+b))\n    framed_img[b:-b, b:-b] = img\n    return framed_img\n\n\n\ndef offset_image(x, y, label, bar_is_too_short, ax, zoom, offset):\n    '''For adding flags to the graph.\n    source: https://stackoverflow.com/questions/61971090/how-can-i-add-images-to-bars-in-axes-matplotlib'''\n    \n    img = plt.imread(f'../input/2020-cost-of-living/flags/{label} flag-icon-256.png')\n    img = frame_image(img, 2)\n    im = OffsetImage(img, zoom=zoom)\n    im.image.axes = ax\n    x_offset = offset\n#     if bar_is_too_short:\n#         x = 0\n    ab = AnnotationBbox(im, (x, y), xybox=(x_offset, 0), frameon=False,\n                        xycoords='data', boxcoords=\"offset points\", pad=0)\n    ax.add_artist(ab)\n    \n    \ndef offset_png(x, y, path, ax, zoom, offset, border=2):\n    '''For adding other .png images to the graph.\n    source: https://stackoverflow.com/questions/61971090/how-can-i-add-images-to-bars-in-axes-matplotlib'''\n    \n    img = plt.imread(path)\n    img = frame_image(img, border)\n    im = OffsetImage(img, zoom=zoom)\n    im.image.axes = ax\n    x_offset = offset\n    ab = AnnotationBbox(im, (x, y), xybox=(x_offset, 0), frameon=False,\n                        xycoords='data', boxcoords=\"offset points\", pad=0)\n    ax.add_artist(ab)\n    \n    \n    \ndef gender_coding(x):\n    '''To encode gender - for pie graph readability.'''\n    if x == \"Man\":\n        return \"M\"\n    elif x == \"Woman\":\n        return \"W\"\n    elif x == \"Prefer not to say\":\n        return \"An\"\n    elif x == \"Nonbinary\":\n        return \"N\"\n    else:\n        return \"SD\"\n    \n    \ndef code_education(x):\n    if x == \"Bachelor‚Äôs degree\":\n        return \"Bachelor\"\n    elif x == \"Doctoral degree\":\n        return \"Doctoral\"\n    elif x == \"I prefer not to answer\":\n        return \"Anon\"\n    elif x == \"Master‚Äôs degree\":\n        return \"Master\"\n    elif x == \"No formal education past high school\":\n        return \"HS\"\n    elif x == \"Professional degree\":\n        return \"Prof\"\n    else:\n        return \"College\"\n    \n    \n    \ndef encode_mlLong(x):\n    '''Encoding for question: how long have you been writing ML code?'''\n    if x == \"Under 1 year\":\n        return \"< 1 years\"\n    elif x == \"I do not use machine learning methods\":\n        return \"Never\"\n    elif x == \"20 or more years\":\n        return \"20+ years\"\n    else:\n        return x\n    \n\ndef encode_codeLong(x):\n    '''Encoding for question: how long have you been writing code?'''\n    if x == \"I have never written code\":\n        return \"Never\"\n    else:\n        return x\n    \n    \n    \ndef code_role(x):\n    if x == \"Product/Project Manager\":\n        return \"Project Manager\"\n    elif x == \"Machine Learning Engineer\":\n        return \"ML Engineer\"\n    elif x == \"DBA/Database Engineer\":\n        return \"DB Engineer\"\n    elif x == 'Data Analyst':\n        return \"Analyst\"\n    elif x == 'Business Analyst':\n        return \"Analyst\"\n    elif x == 'Statistician':\n        return \"Analyst\"\n    else:\n        return x\n    \n    \n\ndef prep_role_duties_data(df):\n    '''Function to prepare data for the Sankey - Role and Duties at Work subchapter.'''\n    \n    # Columns\n    role = \"Select the title most similar to your current role (or most recent title if retired): - Selected Choice\"\n    dutie = [\"Analyze and understand data to influence product or business decisions\",\n         \"Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data\",\n         \"Build prototypes to explore applying machine learning to new areas\",\n         \"Build and/or run a machine learning service that operationally improves my product or workflows\",\n         \"Do research that advances the state of the art of machine learning\",\n         \"None of these activities are an important part of my role at work\"]\n    duties = []\n    for k, d in enumerate(dutie):\n        duties.append(f\"Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - {d}\")\n\n    # Select data\n    columns = [k for k in duties]\n    columns.append(role)\n    dt = df[df[role] != \"Other\"][columns]\n\n    # Rename\n    dt.columns = [\"dutie1\", \"dutie2\", \"dutie3\", \"dutie4\", \"dutie5\", \"dutie6\", \"role\"]\n    \n    # Apply role coding\n    dt[\"role\"] = dt[\"role\"].apply(lambda x: code_role(x))\n    \n    return dt\n\n\ndef code_ml_spent(x):\n    if x == \"$0 ($USD)\":\n        return \"0\"\n    elif x == \"$100,000 or more ($USD)\":\n        return \">100,000\"\n    else:\n        return x\n    \n    \ndef code_advice(x):\n    if x == \"Python\":\n        return \"Python\"\n    elif x == \"R\":\n        return \"R\"\n    elif x == \"SQL\":\n        return \"SQL\"\n    else:\n        return \"Other\"\n    \n    \n    \ndef get_environment_questions():\n    '''Returns all column names for the environment portion.'''\n    \n    # Environment\n    ides = [\"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice - Jupyter (JupyterLab, Jupyter Notebooks, etc) \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  RStudio \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  Visual Studio / Visual Studio Code \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  PyCharm \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -   Spyder  \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -   Notepad++  \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -   Sublime Text  \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -   Vim / Emacs  \",\n            \"Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  MATLAB \"]\n\n\n    notebooks = [\"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Kaggle Notebooks\",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice - Colab Notebooks\",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice - Azure Notebooks\",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Paperspace / Gradient \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Binder / JupyterHub \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Code Ocean \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  IBM Watson Studio \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Amazon Sagemaker Studio \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Amazon EMR Notebooks \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice - Google Cloud AI Platform Notebooks \",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice - Google Cloud Datalab Notebooks\",\n                 \"Which of the following hosted notebook products do you use on a regular basis?  (Select all that apply) - Selected Choice -  Databricks Collaborative Notebooks \"]\n\n    computation = \"What type of computing platform do you use most often for your data science projects? - Selected Choice\"\n    \n    all_columns = ides.copy()\n    all_columns.extend(notebooks)\n    all_columns.append(computation)\n    \n    return all_columns\n\n\n\ndef multiple_choice_prep(data, category_name=\"Categ\", var_name=\"IDE\"):\n    '''Receives a dataframe which contains on columns the multiple choice questions + Categ colum.\n    Return the cleaned dataframe.'''\n    \n    data = data.melt(id_vars=[category_name], var_name=\"Name\", value_name=\"count\").dropna().reset_index(drop=True)\n    data = data.groupby(category_name)[\"count\"].value_counts().unstack().reset_index()\n    data = data.melt(id_vars=[category_name], var_name=var_name, value_name=\"count\").dropna().reset_index(drop=True)\n    \n    return data\n\ndef one_choice_prep(data, category_name=\"Categ\", count_on=\"Computing Platform\", var_name=\"Computing\"):\n    '''Received the dataframe containing on first column the choices and on second the \"Category\".\n    Returns the cleaned Grouped By version of the data.'''\n    \n    data = data.groupby(category_name)[count_on].value_counts().unstack().reset_index()\n    data = data.melt(id_vars=[category_name], var_name=var_name, value_name=\"count\").dropna().reset_index(drop=True)\n    \n    return data\n\n\n\n\ndef code_ide(x):\n    if x == \"Jupyter (JupyterLab, Jupyter Notebooks, etc) \":\n        return \"Jupyter\"\n    else:\n        return x\n    \n    \ndef code_notebook(x):\n    if x == \"Colab Notebooks\":\n        return \"Colab\"\n    elif x == \" Kaggle Notebooks\":\n        return \"Kaggle\"\n    elif x == \"Google Cloud Datalab Notebooks\":\n        return \"Google Cloud Datalab\"\n    elif x == \"Azure Notebooks\":\n        return \"Azure\"\n    elif x == \"Google Cloud AI Platform Notebooks \":\n        return \"Google Cloud AI Platform\"\n    elif x == \" Databricks Collaborative Notebooks \":\n        return \"Databricks Collaborative\"\n    elif x == \" Amazon EMR Notebooks \":\n        return \"Amazon EMR\"\n    else:\n        return x.strip()\n    \n    \ndef code_course(x):\n    if x == \"Cloud-certification programs (direct from AWS, Azure, GCP, or similar)\":\n        return \"Cloud Programs\"\n    elif x == \"Kaggle Learn Courses\":\n        return \"Kaggle Learn\"\n    elif x == \"University Courses (resulting in a university degree)\":\n        return \"University Courses\"\n    else:\n        return x\n    \n\n    \ndef code_media(x):\n    if x == \"Blogs (Towards Data Science, Analytics Vidhya, etc)\":\n        return \"Blogs\"\n    elif x == \"Course Forums (forums.fast.ai, Coursera forums, etc)\":\n        return \"Course Forums\"\n    elif x == \"Email newsletters (Data Elixir, O'Reilly Data & AI, etc)\":\n        return \"Newsletters\"\n    elif x == \"Journal Publications (peer-reviewed journals, conference proceedings, etc)\":\n        return \"Publications\"\n    elif x == \"'Kaggle (notebooks, forums, etc)\":\n        return \"Kaggle\"\n    elif x == \"Podcasts (Chai Time Data Science, O‚ÄôReilly Data Show, etc)\":\n        return \"Podcasts\"\n    elif x == \"Reddit (r/machinelearning, etc)\":\n        return \"Reddit\"\n    elif x == \"Slack Communities (ods.ai, kagglenoobs, etc)\":\n        return \"Slack\"\n    elif x == \"Twitter (data science influencers)\":\n        return \"Twitter\"\n    else:\n        return \"YouTube\"\n\n\n    \ndef get_reliable_sources(df):\n    '''Input the original dataframe and returns 2 tables containing the courses & media information.\n    Used for the \"Reliable Sources\" chapter.'''\n\n    # Social Media and First Steps\n\n    # Columns\n    courses = [\"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Coursera\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - edX\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Kaggle Learn Courses\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - DataCamp\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Fast.ai\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udacity\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udemy\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - LinkedIn Learning\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Cloud-certification programs (direct from AWS, Azure, GCP, or similar)\",\n               \"On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - University Courses (resulting in a university degree)\"]\n    media = [\"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Twitter (data science influencers)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Email newsletters (Data Elixir, O\\'Reilly Data & AI, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Reddit (r/machinelearning, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Kaggle (notebooks, forums, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Course Forums (forums.fast.ai, Coursera forums, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - YouTube (Kaggle YouTube, Cloud AI Adventures, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Podcasts (Chai Time Data Science, O‚ÄôReilly Data Show, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Blogs (Towards Data Science, Analytics Vidhya, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Journal Publications (peer-reviewed journals, conference proceedings, etc)\",\n             \"Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Slack Communities (ods.ai, kagglenoobs, etc)\"]\n\n    courses.append(categ)\n    media.append(categ)\n\n    courses_data = df[courses]\n    media_data = df[media]\n\n    courses_data.columns = [\"Coursera\", \"edX\", \"Kaggle Learn\", \"DataCamp\", \"Fast.ai\",\n                            \"Udacity\", \"Udemy\", \"LinkedIn Learn\", \"Cloud programs\", \"University\", \"Categ\"]\n    media_data.columns = [\"Twitter\", \"Newsletter\", \"Reddit\", \"Kaggle\", \"Course Forums\",\n                          \"YouTube\", \"Podcasts\", \"Blogs\", \"Publications\", \"Slack\", \"Categ\"]\n\n    courses_data = multiple_choice_prep(courses_data, category_name=\"Categ\", var_name=\"Course\")\n    media_data = multiple_choice_prep(media_data, category_name=\"Categ\", var_name=\"Media\")\n\n    courses_data[\"Course\"] = courses_data[\"Course\"].apply(lambda x: code_course(x))\n    media_data[\"Media\"] = media_data[\"Media\"].apply(lambda x: code_media(x))\n    \n    return courses_data, media_data\n\n\n\ndef code_viz(x):\n    if x == \" Ggplot / ggplot2 \":\n        return \"Ggplot\"\n    elif x == \" Leaflet / Folium \":\n        return \"Folium\"\n    elif x == \" Plotly / Plotly Express \":\n        return \"Plotly\"\n    else:\n        return x.strip()\n    \n    \ndef get_frameworks(df):\n    '''Prepares data for framework subchapter.\n    Returns viz_data and framework_data.'''\n    viz_cols = [col for col in df.columns if \n                \"What data visualization libraries or tools do you use on a regular basis?\" in col]\n    framework_cols = [col for col in df.columns if \n                      \"Which of the following machine learning frameworks do you use on a regular basis?\" in col]\n\n    # Exclude none & others\n    viz_cols = viz_cols[:-2] ; viz_cols.append(categ)\n    framework_cols = framework_cols[:-2] ; framework_cols.append(categ)\n\n    # Data\n    viz_data = df[viz_cols]\n    framework_data = df[framework_cols]\n\n    viz_data = multiple_choice_prep(viz_data, category_name=\"Category\", var_name=\"Visualization\")\n    framework_data = multiple_choice_prep(framework_data, category_name=\"Category\", var_name=\"Frameworks\")\n\n    viz_data[\"Visualization\"] = viz_data[\"Visualization\"].apply(lambda x: code_viz(x))\n    \n    return viz_data, framework_data\n\n\n\ndef code_ml(x):\n    if x == \"Bayesian Approaches\":\n        return \"Bayesian\"\n    elif x == \"Convolutional Neural Networks\":\n        return \"CNNs\"\n    elif x == \"Decision Trees or Random Forests\":\n        return \"Tree Based\"\n    elif x == \"Dense Neural Networks (MLPs, etc)\":\n        return \"Dense NNs\"\n    elif x == \"Evolutionary Approaches\":\n        return \"Evolutionary\"\n    elif x == \"Generative Adversarial Networks\":\n        return \"GANs\"\n    elif x == \"Gradient Boosting Machines (xgboost, lightgbm, etc)\":\n        return \"Gradient Boosting\"\n    elif x == \"Linear or Logistic Regression\":\n        return \"Regressions\"\n    elif x == \"Recurrent Neural Networks\":\n        return \"RNNs\"\n    else:\n        return \"Transformers\"\n    \n    \n    \ndef code_compvis(x):\n    if x == \"General purpose image/video tools (PIL, cv2, skimage, etc)\":\n        return \"General Image\"\n    elif x == \"Generative Networks (GAN, VAE, etc)\":\n        return \"GANs\"\n    elif x == \"Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)\":\n        return \"Image Classification\"\n    elif x == \"Image segmentation methods (U-Net, Mask R-CNN, etc)\":\n        return \"Image Segmentation\"\n    else:\n        return \"Object Detection Methods\"\n    \n    \ndef code_nlp(x):\n    if x == \"Contextualized embeddings (ELMo, CoVe)\":\n        return \"Context Embeddings\"\n    elif x == \"Encoder-decorder models (seq2seq, vanilla transformers)\":\n        return \"Encoder-Decoder\"\n    elif x == \"Transformer language models (GPT-3, BERT, XLnet, etc)\":\n        return \"Transformers\"\n    else:\n        return \"Word Embeddings\"\n    \n    \ndef get_ml_dl_data(df):\n    '''Returns 3 tables for the ML and DL subchapter.'''\n\n    ml_cols = [col for col in df.columns if \n               \"Which of the following ML algorithms do you use on a regular\" in col][:-2]\n    compvision_cols = [col for col in df.columns if \n                       \"Which categories of computer vision methods do you use on a regular basis?\" in col][:-2]\n    nlp_cols = [col for col in df.columns if\n                \"Which of the following natural language processing (NLP) methods do you use on a regular basis?\" in col][:-2]\n\n    ml_cols.append(categ)\n    compvision_cols.append(categ)\n    nlp_cols.append(categ)\n\n    ml_data = df[ml_cols]\n    compvis_data = df[compvision_cols]\n    nlp_data = df[nlp_cols]\n\n    # Data prep\n    ml_data = multiple_choice_prep(ml_data, category_name=\"Category\", var_name=\"ML\")\n    compvis_data = multiple_choice_prep(compvis_data, category_name=\"Category\", var_name=\"CompVis\")\n    nlp_data = multiple_choice_prep(nlp_data, category_name=\"Category\", var_name=\"NLP\")\n\n    ml_data[\"ML\"] = ml_data[\"ML\"].apply(lambda x: code_ml(x))\n    compvis_data[\"CompVis\"] = compvis_data[\"CompVis\"].apply(lambda x: code_compvis(x))\n    nlp_data[\"NLP\"] = nlp_data[\"NLP\"].apply(lambda x: code_nlp(x))\n    \n    return ml_data, compvis_data, nlp_data\n\n\n\ndef prep_ml_extended(df):\n\n    # ML Products\n    ml_prods = [col for col in df.columns if \n                \"Do you use any of the following machine learning products on a regular basis?\" in col]\n    ml_prods_f = [col for col in df.columns if \n                  \"In the next 2 years, do you hope to become more familiar with any of these specific machine learning\" in col]\n\n    # ML Experiments\n    ml_experm = [col for col in df.columns if \"Do you use any tools to help manage machine learning experiments?\" in col]\n    ml_experm_f = [col for col in df.columns if \n                   \"In the next 2 years, do you hope to become more familiar with any of these tools for managing ML experiments?\" in col]\n\n    # Auto ML\n    automl1 = [col for col in df.columns if \n               \"Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis?\" in col]\n    automl2 = [col for col in df.columns if \n               \"Which of the following automated machine learning tools (or partial AutoML tools) do you use\" in col]\n    automl1_f = [col for col in df.columns if \n                 \"Which categories of automated machine learning tools (or partial AutoML tools) do you hope to\" in col]\n    automl2_f = [col for col in df.columns if \n                 \"Which specific automated machine learning tools (or partial AutoML tools) do you hope to become\" in col]\n\n    # Append category\n    ml_prods.append(categ)\n    ml_prods_f.append(categ)\n    ml_experm.append(categ)\n    ml_experm_f.append(categ)\n    automl1.append(categ)\n    automl2.append(categ)\n    automl1_f.append(categ)\n    automl2_f.append(categ)\n\n\n    # Create dataframes\n    ml_prods_data = df[ml_prods]\n    ml_prods_f_data = df[ml_prods_f]\n    ml_experm_data = df[ml_experm]\n    ml_experm_f_data = df[ml_experm_f]\n    automl1_data = df[automl1]\n    automl2_data = df[automl2]\n    automl1_f_data = df[automl1_f]\n    automl2_f_data = df[automl2_f]\n\n    # Change dataframes\n    var_names = [\"Prods\", \"Prods F\", \"Experiment\", \"Experiment F\",\n                 \"AutoML1\", \"AutoML2\", \"AutoML1 F\", \"AutoML2 F\"]\n\n    ml_prods_data = multiple_choice_prep(data=ml_prods_data, category_name=\"Category\", var_name=var_names[0])\n    ml_prods_f_data = multiple_choice_prep(data=ml_prods_f_data, category_name=\"Category\", var_name=var_names[1])\n    ml_experm_data = multiple_choice_prep(data=ml_experm_data, category_name=\"Category\", var_name=var_names[2])\n    ml_experm_f_data = multiple_choice_prep(data=ml_experm_f_data, category_name=\"Category\", var_name=var_names[3])\n    automl1_data = multiple_choice_prep(data=automl1_data, category_name=\"Category\", var_name=var_names[4])\n    automl2_data = multiple_choice_prep(data=automl2_data, category_name=\"Category\", var_name=var_names[5])\n    automl1_f_data = multiple_choice_prep(data=automl1_f_data, category_name=\"Category\", var_name=var_names[6])\n    automl2_f_data = multiple_choice_prep(data=automl2_f_data, category_name=\"Category\", var_name=var_names[7])\n    \n    return ml_prods_data, ml_prods_f_data, ml_experm_data, ml_experm_f_data, automl1_data, automl2_data, automl1_f_data, automl2_f_data\n\n\ndef code_mlprods(x):\n    x = x.strip()\n    if x == \"Azure Cognitive Services\":\n        return \"Azure Cognitive S.\"\n    elif x == \"Azure Machine Learning Studio\":\n        return \"Azure ML Studio\"\n    elif x == \"Google Cloud AI Platform / Google Cloud ML Engine\":\n        return \"Google Cloud AI/ML\"\n    elif x == \"Google Cloud Natural Language\":\n        return \"Google Cloud NLP\"\n    elif x == \"No / None\":\n        return \"None\"\n    else:\n        return x\n    \n    \ndef code_mlexper(x):\n    x = x.strip()\n    if x == \"Sacred + Omniboard\":\n        return \"Sacred+Omniboard\"\n    elif x == \"Weights & Biases\":\n        return \"Weights&Biases\"\n    elif x == \"No / None\":\n        return \"None\"\n    else:\n        return x\n    \n    return x\n\n\ndef code_automl(x):\n    x = x.strip()\n    \n    if x == \"Automated data augmentation (e.g. imgaug, albumentations)\":\n        return \"Data Augmentation\"\n    elif x == \"Automated feature engineering/selection (e.g. tpot, boruta_py)\":\n        return \"Feature Engineering\"\n    elif x == \"Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)\":\n        return \"Hyperparam. Tuning\"\n    elif x == \"Automated model architecture searches (e.g. darts, enas)\":\n        return \"Model Architectures\"\n    elif x == \"Automated model selection (e.g. auto-sklearn, xcessiv)\":\n        return \"Model Selection\"\n    elif x == \"Automation of full ML pipelines (e.g. Google AutoML, H20 Driverless AI)\":\n        return \"Full ML Pipeline\"\n    else:\n        return \"None\"\n    \n    \ndef code_deploy(x):\n    x = x.strip()\n    \n    if x == \"I do not share my work publicly\":\n        return \"I don't share\"\n    else:\n        return x\n    \ndef code_tool(x):\n    if x == \"Local development environments (RStudio, JupyterLab, etc.)\":\n        return \"RStudio/Jupyter etc.\"\n    elif x == \"Basic statistical software (Microsoft Excel, Google Sheets, etc.)\":\n        return \"Excel/Google Sheets etc.\"\n    elif x == \"Cloud-based data software & APIs (AWS, GCP, Azure, etc.)\":\n        return \"AWS/Azure etc.\"\n    elif x == \"Advanced statistical software (SPSS, SAS, etc.)\":\n        return \"SPSS/SAS etc.\"\n    elif x == \"Business intelligence software (Salesforce, Tableau, Spotfire, etc.)\":\n        return \"Salesforce/Tableau etc.\"\n    else:\n        return \"Other\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Introducing the subject \n**Ahoy**! If you are like me, you've always been looking up to the **top Data Scientists**, the **cream of the community**, the very few that, somehow, through some *magic* only they know, manage to understand, teach and perform like none others.\n\n**But how**? This survey is an opportunity to take this curiosity very *close and personal* in the <span style=\"background:#fed56f; font-weight:bold; color:black\">search for the treasure</span> that might reveal us the steps to reach that greatness.\n\n**Buckle up pirates; the treasure hunt is <span style=\"background:#8EEA7D; font-weight:bold; color:black\">ON</span>**.\n\n<h3 style=\"font-family: 'Source Code Pro', monospace\">Criteria</h3>\n\nUnfortunately, there will need to be some *bias* involved.\n\nIn the Kaggle Survey, there is no feature or clue to identify which of the respondents are Masters/Grandmasters - neither the rank nor any performance within the Kaggle Community.\n\nHence, I had to *define* what is a *successful* data scientist. Some assumptions were:\n\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:\"#010307\";\n    text-align:center;\n    vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-c3ow\">Criteria</font></th>\n    <th class=\"tg-c3ow\">Value</font></th>\n    <th class=\"tg-c3ow\">Strength</font></th>\n    <th class=\"tg-c3ow\">Flaw</font></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-c3ow\">Level of Education</td>\n    <td class=\"tg-c3ow\">Masters / Doctoral</td>\n    <td class=\"tg-c3ow\">Majority of Grandmasters have<br>some level of upper education</td>\n    <td class=\"tg-c3ow\">Many bright data scientists are<br>self-taught</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">Years of Programming/ML</td>\n    <td class=\"tg-c3ow\">4 - 5+ years</td>\n    <td class=\"tg-c3ow\">Might show increased experience</td>\n    <td class=\"tg-c3ow\">The passing of time doesn't reflect<br>how much one has learned</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">Pay</td>\n    <td class=\"tg-c3ow\">USD 100,000+</td>\n    <td class=\"tg-c3ow\">Very high pay can signal high skill</td>\n    <td class=\"tg-c3ow\">High pay doesn't guarantee skill;<br>Regional bias also involved</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">Spending</td>\n    <td class=\"tg-c3ow\">USD 100,000+</td>\n    <td class=\"tg-c3ow\">A company investing thousands in ML<br>is investing in bright DS employees</td>\n    <td class=\"tg-c3ow\">How much a company spends on ML doesn't<br>define the skill of the employer</td>\n  </tr>\n</tbody>\n</table>\n\n<div class=\"alert simple-alert\">\n  <p>In the end, I decided to use <span style=\"background:#fed56f; font-weight:bold; color:black\">pay</span> as my delimiter. Firstly, I consider that it has the lowest bias out of all. Secondly, I could lower the regional bias significantly (more of that in the next chapter). I would also believe that highly skilled people usually have very high pay and the exceptions from that rule aren't that many.</p>\n</div>\n\n#### Libraries below ‚¨á"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Results: https://www.kaggle.com/kaggle-survey-2020\n\n# Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport matplotlib.patches as patches\nfrom pylab import text\nimport matplotlib.image as mpimg\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplotly.offline.init_notebook_mode(connected = True)\n\n\n%matplotlib inline\n\n# --- Set defaults of the notebook ---\nsns.set(font=\"'Source Code Pro', monospace\")\nsns.set_style(\"whitegrid\")\nplt.rcParams[\"font.family\"] = \"'Source Code Pro', monospace\"\n\n# Color Palettes\ntreasure_colors = [\"#703728\", \"#c86b25\", \"#dc9555\", \"#fed56f\", \"#c89a37\"]\npirate_colors = [\"#010307\", \"#395461\", \"#449FAF\", \"#B1F4FC\", \n                 \"#F4D499\", \"#835211\"]\nsns.palplot(sns.color_palette(treasure_colors))\nsns.palplot(sns.color_palette(pirate_colors))\n\nall_colors = treasure_colors.copy()\nall_colors.extend(pirate_colors[1:])\n\n\n# --- Data Import ---\ndata = pd.read_csv(\"../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv\", \n                   skiprows=1)\n# Cost of Living Data\ncol_data = pd.read_csv(\"../input/2020-cost-of-living/cost of living 2020.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. How many McMeal menus can you buy?\n\n## 2.1 The problem\nOk, so we agreed on using the <span style=\"background:#fed56f; font-weight:bold; color:black\">pay</span> as the indicator. However, this feature has many issues on its own:\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:\"#010307\";\n    text-align:center;\n    vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-c3ow\">No.</th>\n    <th class=\"tg-c3ow\">Problem</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-c3ow\">1.</td>\n    <td class=\"tg-c3ow\">It doesn't take into account the purchasing power (e.g., a person in Ukraine with 90k a year may buy more \"valuables\" in their country than another person in Japan with 150k a year).</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">2.</td>\n    <td class=\"tg-c3ow\">It might be misunderstood: there might be people that completed the salary in their base currency. Nevertheless, this column is expressed in dollars.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">3.</td>\n    <td class=\"tg-c3ow\">More than 50% of the respondents didn't respond altogether, so only half remain available.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">4.</td>\n    <td class=\"tg-c3ow\">There are the occasional trolls who might give a lower or a much higher salary than they actually have.</td>\n  </tr>\n</tbody>\n</table>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Data\nq = \"What is your current yearly compensation (approximate $USD)?\"\norder = ['$0-999', '1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999',\n         '5,000-7,499', '7,500-9,999', '10,000-14,999',  '15,000-19,999',\n         '20,000-24,999', '25,000-29,999', '30,000-39,999', '40,000-49,999', \n         '50,000-59,999', '60,000-69,999', '70,000-79,999', '80,000-89,999',\n         '90,000-99,999', '100,000-124,999', '125,000-149,999', '150,000-199,999', \n         '200,000-249,999', '250,000-299,999', '300,000-500,000', '> $500,000']\n\n# Plot\npaper = mpimg.imread('../input/2020-cost-of-living/paper_scroll1.jpg')\nimagebox = OffsetImage(paper, zoom=0.25)\nxy = (0.5, 0.7)\nab = AnnotationBbox(imagebox, xy, frameon=False, pad=1, xybox=(1900, 19.7))\n\nplt.figure(figsize=(16, 12))\nplt.rcParams['figure.dpi'] = 360\n\nax = sns.countplot(y = data[q], order = order, orient=\"v\", palette = \"YlOrBr_r\",\n                   saturation=1)\nshow_values_on_bars(ax, h_v=\"h\", space=0.4)\n# ax.add_artist(ab)\n\nplt.text(670, 20, '???', size=18, color=pirate_colors[1])\nplt.plot([0, 650], [15.5, 15.5], lw=2, color=pirate_colors[1])\nplt.plot([0, 650], [24.5, 24.5], lw=2, color=pirate_colors[1])\nplt.plot([650, 650], [15.5, 24.5], lw=2, color=pirate_colors[1])\nplt.plot([3, 3], [15.5, 24.5], lw=2, color=pirate_colors[1])\n\n# Arrow\nstyle = \"Simple, tail_width=5, head_width=16, head_length=23\"\nkw = dict(arrowstyle=style, color=pirate_colors[1])\narrow = patches.FancyArrowPatch((1850, 1), (1850, 24),\n                             connectionstyle=\"arc3,rad=-.15\", **kw)\nplt.gca().add_patch(arrow)\n\nplt.text(1890, 1.5, 'less pay', size=14, color=pirate_colors[1])\nplt.text(1650, 23, 'more pay', size=14, color=pirate_colors[1])\n\n\nax.set_xlabel(\"Frequency\", size = 18, color = pirate_colors[0])\nax.set_ylabel(\"Pay\", size = 18, color = pirate_colors[0])\nax.set_title(\"- Pay distribution around respondents -\", \n             size = 26, color = treasure_colors[0], weight='bold')\nplt.xticks([])\nplt.yticks(fontsize=11)\nsns.despine(left=True, bottom=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Hence ... **what do we choose**? We can't just take all people with a salary > 80k - this would be highly bias and inefficient (80k is lots of money in the UK, but not that much in the US). Unfortunately, points *2.*, *3.* and *4.* are systematic issues, so we'll have to go ahead and trust the Kagglers that they completed the *pay* question to the best of their abilities.\n\n## 2.2 The Solution üçü\n\n**Thanks to a good friend** who gave me this idea, we can all have a snack break now.\n\n<img src=\"https://i.imgur.com/Tkhvnyg.png\" width=700>\n\nA McMeal may solve our regional bias problem. Instead of using <span style=\"background:#fed56f; font-weight:bold; color:black\">pay</span>, we can look around the world at **how many McMeals can one respondent buy with their salary in their own country**. Afterward, we can use the <span style=\"background:#fed56f; font-weight:bold; color:black\">McMeal(units)</span> as our non-bias indicator.\n\nNow, the only thing that remains is where do we draw the line in the McMeal units?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Get mcmeal_units\ndata = add_unique_id(data)\nmcmeal_units = get_McMeal_units(data)\nq3 = int(mcmeal_units['McMeal(units)'].quantile([0.75]).item())\n\n# Plot\nplt.figure(figsize=(16, 11))\nplt.rcParams['figure.dpi'] = 360\n\nax = sns.boxplot(x = mcmeal_units['McMeal(units)'], color=pirate_colors[2])\n\nplt.text(q3 + 800, 0.46, f'Q3 value: {q3:,} meals', size=13, color=treasure_colors[1], weight='bold')\nplt.text(120000, 0.085, 'outliers we want to analyze', size=13, color=treasure_colors[1], weight='bold')\n\n# Arrow\nstyle = \"Simple, tail_width=0.5, head_width=6, head_length=5\"\nkw = dict(arrowstyle=style, color=treasure_colors[1])\narrow = patches.FancyArrowPatch((35000, 0.42), (q3, 0.3),\n                             connectionstyle=\"arc3,rad=.3\", **kw)\nplt.gca().add_patch(arrow)\n\n# Fries\npath='../input/2020-cost-of-living/fries.png'\noffset_png(x=240000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=215000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=205000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=185000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=190000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=150000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=141000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=129000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=120000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=115000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=98000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=92500, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=86000, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=83500, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=80500, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=77500, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=74500, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\noffset_png(x=66500, y=0, path=path, ax=ax, zoom=0.05, offset=0, border=1)\n\n\nplt.plot([26000, 245000], [0.045, 0.045], lw=2, color=treasure_colors[1])\nplt.plot([26000, 25300], [0.045, 0.02], lw=2, color=treasure_colors[1])\nplt.plot([245000, 245700], [0.045, 0.02], lw=2, color=treasure_colors[1])\n\nax.set_xlabel(\"McMeals\", size = 15, color = pirate_colors[0])\nax.set_ylabel(\"\")\nax.set_title(\"- McMeal : Units variability -\", size = 26, color = pirate_colors[0], weight='bold')\n# plt.xticks([])\nplt.yticks(fontsize=11)\nsns.despine(left=True)\nax.get_xaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 2,300 people have at least 10,000 Meals or more (10,000 threshold was chosen looking at Q3). I will be analyzing these people from now on. However, I will also be segmenting them into <span style=\"background:#fed56f; font-weight:bold; color:black\">three categories</span>:\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:\"#010307\";\n    text-align:center;\n    vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-c3ow\"><font>Category Name</font></th>\n    <th class=\"tg-c3ow\"><font>Meaning</font></th>\n    <th class=\"tg-c3ow\"><font>Number of respondents</font></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-c3ow\">High</td>\n    <td class=\"tg-c3ow\"><b>10,000 - 20,000 units</b>: These people can buy more McMeals than more than <b>75%</b> of our base users.</td>\n    <td class=\"tg-c3ow\">~1600</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">Very High</td>\n    <td class=\"tg-c3ow\"><b>20,000 - 50,000 units</b>: These people can buy more McMeals than more than <b>90%</b> of our base users.</td>\n    <td class=\"tg-c3ow\">~700</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">Crazy High</td>\n    <td class=\"tg-c3ow\"><b>50,000 + units</b>: These people can buy more McMeals than more than <b>99%</b> of our base users.</td>\n    <td class=\"tg-c3ow\">~100</td>\n  </tr>\n</tbody>\n</table>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = mcmeal_units[mcmeal_units[\"McMeal(units)\"] <= 100000]\nx0 = 10000\nx1 = 20000\nx2 = 50000\n\nplt.figure(figsize=(16, 11))\n\nax = sns.distplot(x[\"McMeal(units)\"][::20], hist=False, kde=True, kde_kws = {'lw':3}, color= treasure_colors[0])\nkde_x, kde_y = ax.lines[0].get_data()\n\np1 = plt.axvline(x=x0,color=pirate_colors[1], lw=2, ls=\"--\")\np2 = plt.axvline(x=x1,color=pirate_colors[1], lw=2, ls=\"--\")\np2 = plt.axvline(x=x2,color=pirate_colors[1], lw=2, ls=\"--\")\n\nax.fill_between(kde_x, kde_y, where=(kde_x>x0) & (kde_x<x1) , \n                interpolate=True, color=treasure_colors[1])\nax.fill_between(kde_x, kde_y, where=(kde_x>x1) & (kde_x<x2) , \n                interpolate=True, color=treasure_colors[2])\nax.fill_between(kde_x, kde_y, where=(kde_x>x2) , \n                interpolate=True, color=treasure_colors[3])\n\nplt.text(10350, 0.000075, '75%', size=13, color=pirate_colors[1], weight='bold')\nplt.text(20350, 0.000075, '90%', size=13, color=pirate_colors[1], weight='bold')\nplt.text(50350, 0.000075, '99%', size=13, color=pirate_colors[1], weight='bold')\n\nax.set_xlabel(\"Units\", size = 15, color = pirate_colors[0])\nax.set_ylabel(\"\")\nax.set_title(\"- McMeal Units Distribution -\", size = 26, color = pirate_colors[0], weight='bold')\nplt.yticks([])\nplt.xticks(np.arange(min(x[\"McMeal(units)\"])-34, max(x[\"McMeal(units)\"])+1, 10000))\nsns.despine(left=True)\nax.get_xaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Pareto Principle\n\nOur McMeal units distribution match the Pareto Principle very well: [roughly 80% of consequences come from 20% of the causes](https://en.wikipedia.org/wiki/Pareto_principle).\n\nHence, we are noticing a **\"top 20%\" of the \"top 20%\" of the \"top 20%\"** situation, meaning:\n* out of 9,893 total respondents, 2,301 (*25%*) have more than 10,000 units\n* out of the 2,301 respondents, 705 (*~ 20%*) have more than 20,000 units\n* out of the 705 respondents, 100 (*~ 15%*) have more than 50,000 units\n\n> What does this mean? It doesn't matter to which category you look, the distribution is always going to be **skewed to the right**. This is why <span style=\"background:#fed56f; font-weight:bold; color:black\">segmentation</span> is so important here."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Data\ndist1 = mcmeal_units[mcmeal_units[\"McMeal(units)\"] >= 10000]\ndist2 = mcmeal_units[mcmeal_units[\"McMeal(units)\"] >= 20000]\ndist3 = mcmeal_units[mcmeal_units[\"McMeal(units)\"] >= 50000]\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 11))\nfig.suptitle('- Pareto Principle -', size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2, ax3]\n\nsns.distplot(dist1[\"McMeal(units)\"][::30], hist=False, kde=True, kde_kws = {'lw':3}, color= treasure_colors[0], ax=ax1)\nsns.distplot(dist2[\"McMeal(units)\"][::10], hist=False, kde=True, kde_kws = {'lw':3}, color= treasure_colors[0], ax=ax2)\nsns.distplot(dist3[\"McMeal(units)\"], hist=False, kde=True, kde_kws = {'lw':3}, color= treasure_colors[0], ax=ax3)\n\nax1.axvspan(20000, 200000, color=pirate_colors[4], alpha=0.5)\nax2.axvspan(50000, 200000, color=pirate_colors[4], alpha=0.5)\n\nax1.text(110000, 0.000035, '>= 20,000 --->', size=13, color=treasure_colors[2], weight='bold')\nax2.text(110000, 0.000026, '>= 50,000 --->', size=13, color=treasure_colors[2], weight='bold')\n\nax1.set_title('>= 10,000 units distribution', size = 13, color = treasure_colors[0], weight='bold')\nax2.set_title('>= 20,000 units distribution', size = 13, color = treasure_colors[0], weight='bold')\nax3.set_title('>= 50,000 units distribution', size = 13, color = treasure_colors[0], weight='bold')\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_yticks([])\n    ax.get_xaxis().set_major_formatter(\n        matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n    \nevery_nth = 2\nfor n, label in enumerate(ax3.xaxis.get_ticklabels()):\n    if n % every_nth != 0:\n        label.set_visible(False)\n        \nsns.despine(left=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.86, wspace=None, hspace=None);\n\n\n# --- Finalize Preprocessing ---\n# Apply the Category and merge data together\nmcmeal_units[\"Category\"] = mcmeal_units[\"McMeal(units)\"].apply(lambda x: segment_units(x))\ndf_all = pd.merge(left=data, right=mcmeal_units, how=\"inner\", on=\"id\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. On our way to finding the treasure\n\nAlright! We established our feature, normalized it, excluded the regional bias, and segmented our target respondents. We can say **our map** is already laid out in front of us (well structured and ready, but still empty); hence we now need to follow the steps to our <span style=\"background:#fed56f; font-weight:bold; color:black\">treasure</span>.\n\n<div class=\"alert simple-alert\">\n  <p>The map is BLANK for the moment, but it will start revealing itself once we begin discovering new Realms.</p>\n</div>\n\n\n*üìå Note: From now on, when I'll mention **pay**, I will refer to the units we got after the pay normalization on the McMeal units :)*\n\n<h2><span style=\"background:#F3E9D9; font-weight:normal; color:black\">3.1 The Personal Profile</span></h2>\n\n<h3 style=\"font-family: 'Source Code Pro', monospace\">Where do they reside?</h3>\n\nSome pointers we observe here:\n* The top countries do **match the profile** of the majority **of masters/grandmasters** on Kaggle.\n* The top 2 countries for all categories are **the USA** and **India**.\n* **Japan, China, Indonesia, Russia, Canada, and the UK** are the other 6 places where these highly skilled people reside."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# --- Filter on target respondents ---\ndf = df_all[df_all['McMeal(units)'] >= 10000]\n\n# Data\ncountry = 'Country'\nunits = 'McMeal(units)'\ncateg = 'Category'\n\nx = df.groupby([country, categ])[units].sum().reset_index()\\\n                                        .sort_values(units, ascending=False).groupby(categ).head(5).reset_index(drop=True)\nx[\"McMeal(units)\"] = x[\"McMeal(units)\"]/100000\n\n# Incorporate all\ndatas = [x[x[categ] == \"High\"], x[x[categ] == \"Very High\"], x[x[categ] == \"Crazy High\"]]\nlabels = [datas[0][country].unique(), datas[1][country].unique(), datas[2][country].unique()]\nheight = 0.9\n\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,11))\nfig.suptitle('- Top 5 Countries on Categories (in 100k) -', size = 26, color = pirate_colors[0],\n             weight='bold')\naxs = [ax1, ax2, ax3]\n\nfor data, ax in zip(datas, axs):\n    sns.barplot(y = data[country], x = data[units], ax=ax, color = treasure_colors[2])\n\nax1.set_title('High Pay', size = 13, color = treasure_colors[0], weight='bold')\nax2.set_title('Very High Pay', size = 13, color = treasure_colors[0], weight='bold')\nax3.set_title('Crazy High Pay', size = 13, color = treasure_colors[0], weight='bold')\n\n\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticks([])\n    ax.tick_params(axis='y', rotation=80)\n    show_values_on_bars(ax, h_v=\"h\", space=1)\n    \nfor i in range(len(datas)):\n    \n    max_value = datas[i][units].max()\n    for k, (label, value) in enumerate(zip(labels[i], datas[i][units].values)):\n        offset_image(value, k, label, bar_is_too_short=value < max_value / 10, ax=axs[i],\n                     zoom=0.14, offset=17)\n        \nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family: 'Source Code Pro', monospace\">Who are they?</h3>\n\nLet's take it step by step:\n* As the percentage of the entire population of the survey is **mostly formed by males**, the \"high end\" people we're studying match accordingly.\n* Looking at age, we can observe that the *average* age increases by category:\n    * High Pay: the average age is around *35* yo\n    * Very High Pay: average age starts moving towards *40 yo and 45 yo* (these bars start to rise)\n    * Crazy High Pay: the 30s drop suddenly, whereas the beginning of *40s* stays the same. Later age (*45, 50s*) are also visible.\n* Hence, there is a clear, **direct correlation between high pay and age** - <span style=\"background:#fed56f; font-weight:bold; color:black\">the older, the wiser, the wealthier</span>.\n\nMy personal opinion is that the 20s and 30s are hectic anyway, and the golden ages are still after 40. Glad that this survey also reflects that. üòÅ [@Dieter](https://www.kaggle.com/christofhenkel) might agree with me as well. üëÄ"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --- OVERALL DATA ---\nage = \"What is your age (# years)?\"\ngender = \"What is your gender? - Selected Choice\"\ncateg = 'Category'\n\nagc = df[[age, gender, categ]]\n\n\n\n# Gender plot data \nd = agc[[gender, categ]].value_counts().reset_index()\nd[gender] = d[gender].apply(lambda x: gender_coding(x))\ndatas = [d[d[categ] == \"High\"], d[d[categ] == \"Very High\"], d[d[categ] == \"Crazy High\"]]\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,11))\nfig.suptitle('- Gender on Pay Categories -', size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2, ax3]\n\nfor k, (data, ax) in enumerate(zip(datas, axs)):\n    labels = data[gender].values\n    sizes = data[0].values\n    explode = (0.2, 0, 0, 0)\n    \n    if k == 0:\n        explode = (0.2, 0, 0, 0, 0)\n    \n    colors = treasure_colors\n    patch, texts = ax.pie(sizes, explode=explode,\n                            colors=colors, startangle=90)\n    ax.legend(patch, labels, loc=\"best\",\n              bbox_to_anchor=(0.3, 0.9))\n    \n    ax.axis('equal')\n\nax1.set_title('High Pay', size = 13, color = pirate_colors[0], weight='bold',pad=-30)\nax2.set_title('Very High Pay', size = 13, color = pirate_colors[0], weight='bold',pad=-30)\nax3.set_title('Crazy High Pay', size = 13, color = pirate_colors[0], weight='bold',pad=-30)\n\nfig.text(0.15, 0.25, 'M:Male | W:Woman | An:Prefer not to say | N:Nonbinary | SD:Prefer to self describe', \n         size=12, color=\"#BDC3C7\", weight='bold');\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.93, wspace=None, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Man & Woman View\ny = agc[agc[gender].isin(['Man', 'Woman'])].reset_index(drop=True)\ny = pd.concat([y, pd.get_dummies(y[gender])],\n              axis=1)\ny = y.groupby([categ, age])[['Man', 'Woman']].sum().reset_index()\ndatas = [y[y[categ] == \"High\"], y[y[categ] == \"Very High\"], y[y[categ] == \"Crazy High\"]]\n\n# Plot\nfig, axs = plt.subplots(ncols=6, sharey=True, figsize=(16, 11))\nfig.suptitle('- Population Pyramid : view over Pay Categories -', size = 26, color = pirate_colors[0], \n             weight='bold')\npairs = [[0, 1], [2, 3], [4, 5]]\n\nfor pair, data in zip(pairs, datas):\n\n    #define x and y limits\n    y_axis = range(0, len(data))\n    x_male = data['Man']\n    x_female = data['Woman']\n\n    #define male and female bars\n    axs[pair[0]].barh(y_axis, x_male, align='center', color=pirate_colors[1])\n    axs[pair[0]].set_title(f'Male - {data[categ].unique()[0]}',\n                     color = treasure_colors[0], weight='bold', size=12)\n    axs[pair[1]].barh(y_axis, x_female, align='center', color=treasure_colors[1])\n    axs[pair[1]].set_title(f'Female - {data[categ].unique()[0]}',\n                     color = treasure_colors[0], weight='bold', size=12)\n\n    #adjust grid parameters and specify labels for y-axis\n    axs[pair[1]].grid()\n    axs[pair[0]].set(yticks=y_axis, yticklabels=data[age])\n    axs[pair[0]].invert_xaxis()\n    axs[pair[0]].grid()\n\n    \nplt.rcParams['figure.dpi'] = 360;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n  <p><b>Takeaway</b>: We now know that the high end of respondents is mostly from the US and India, but also located in Japan, China, Indonesia, UK, Russia, or Canada, and have a Male 30-45 yo profile.</p>\n  <p>However, these \"personal\" aspects don't define an outstanding Data Scientist. Hence, we'll start from now on to look at education, expertise, work ethic, and knowledge, rather than focusing on biological, racial, or other environmental aspects.</p>\n</div>\n\n> Oh, and look! The first portion of the map is clear now!\n\n<img src=\"https://i.imgur.com/MA7TBEF.png\">\n\n> Closer look üîé\n<img src=\"https://i.imgur.com/UlLtWKF.jpg\" width=700>\n\n<h2><span style=\"background:#F3E9D9; font-weight:normal; color:black\">3.2 The Education</span></h2>\n\nAs for education, the 3 Pay Categories differentiate through:\n* *High Pay* and *Very High Pay* have very similar distributions. **50% of the respondents** have or plan to complete a **Masters's degree** in the next 2 years.\n* However, *Crazy High Pay* steals ~15 percentage points from the *Master* category and adds to the **Doctoral** category.\n* Hence, the majority of extremely <span style=\"background:#fed56f; font-weight:bold; color:black\">well-paid respondents choose to continue their superior studies to a Doctoral</span>."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"education = \"What is the highest level of formal education that \\\nyou have attained or plan to attain within the next 2 years?\"\ncateg = 'Category'\n\ndt = df[[education, categ]].reset_index(drop=True)\ndt = dt.groupby(categ)[education].value_counts().unstack(fill_value=0).reset_index()\ndt = dt.melt(id_vars=[categ], var_name=education, value_name=\"count\")\ndt[education] = dt[education].apply(lambda x: code_education(x))\n\ntotal = dt.groupby(categ)[\"count\"].sum().reset_index()\ndt = pd.merge(left=dt, right=total, on=categ)\ndt[\"perc\"] = round(dt[\"count_x\"]/dt[\"count_y\"]*100, 0).astype(int)\n\ndatas = [dt[dt[categ] == \"High\"], dt[dt[categ] == \"Very High\"], dt[dt[categ] == \"Crazy High\"]]\n\n\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,14))\nfig.suptitle('- Education on Pay Categories -', size = 26, color = pirate_colors[0],\n             weight='bold')\naxs = [ax1, ax2, ax3]\n\norder = [\"Doctoral\", \"Master\", \"Bachelor\", \"Prof\", \"College\", \"HS\", \"Anon\"]\nfor data, ax in zip(datas, axs):\n    sns.barplot(y = data[education], x = data[\"perc\"], ax=ax, color = treasure_colors[0],\n                order = order)\n\nax1.set_title('High Pay (%)', size = 13, color = pirate_colors[0], weight='bold')\nax2.set_title('Very High Pay (%)', size = 13, color = pirate_colors[0], weight='bold')\nax3.set_title('Crazy High Pay (%)', size = 13, color = pirate_colors[0], weight='bold')\n\npath='../input/2020-cost-of-living/education_white.png'\noffset_png(x=50, y=1, path=path, ax=ax1, zoom=0.04, offset=-22, border=1)\noffset_png(x=50, y=1, path=path, ax=ax2, zoom=0.04, offset=-22, border=1)\noffset_png(x=31, y=0.03, path=path, ax=ax3, zoom=0.04, offset=-22, border=1)\noffset_png(x=36, y=1, path=path, ax=ax3, zoom=0.04, offset=-22, border=1)\n\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticks([])\n    ax.tick_params(axis='y', rotation=80)\n    show_values_on_bars(ax, h_v=\"h\", space=1)\n        \nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=0.3, right=None, top=None, wspace=0.22, hspace=None)\nfig.text(0.10, 0.25, \n         'Anon:Prefer not to answer | HS:No formal education past high school | College:College study wout earning a degree | Prof:Professional degree', \n         size=12, color=\"#BDC3C7\", weight='bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n  <p><b>Takeaway</b>: The high-end respondents have mostly a high to very high education; the majority have at least a Masters's complete. The difference between High Pay and Crazy High Pay is in majority's choice to pursue their passion further to a Doctoral.</p>\n  <p>As you may know, indeed, many Masters/Grandmasters discuss (on Twitter, podcasts, etc.) that they have a Doctoral in a field supported by Data Science work.</p>\n</div>\n\n> And the Education Mountains revealed themselves!\n\n<img src=\"https://i.imgur.com/swURPrJ.jpg\">\n\n> Closer look üîé\n<img src=\"https://i.imgur.com/2tGUCaZ.jpg\" width=700>\n\n<h2><span style=\"background:#F3E9D9; font-weight:normal; color:black\">3.3 The Expertise and Work Environment</span></h2>\n\nHere is our chance to take a glimpse into the work environment and skills acquired. How long does it take to get there? How hard do we need to work? What job roles should we pursue?\n\n<h3 style=\"font-family: 'Source Code Pro', monospace\">For how long have they been practicing?</h3>\n\nThe most important points here:\n* These exceptional people have **more coding experience** than ML in terms of time.\n* Looking at the coding expertise, we see more than 50% of them having **10+ years of coding experience**.\n* Looking at the ML expertise, the donut is more evenly split between **1 and 10 years**, with *less than 10% having 10+ years of experience*.\n* What does this mean? It means that <span style=\"background:#fed56f; font-weight:bold; color:black\">exceptional people do have lots of coding experience but NOT ML necessarily</span>, and most of them are on Kaggle because they might have recently (or in the last years) found their passion in data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --- OVERALL DATA ---\ncode = \"For how many years have you been writing code and/or programming?\"\nml = \"For how many years have you used machine learning methods?\"\n\ndt = df[[code, ml]]\ndt[code] = dt[code].apply(lambda x: encode_codeLong(x))\ndt[ml] = dt[ml].apply(lambda x: encode_mlLong(x))\ndatas = [dt[code].value_counts().reset_index(), dt[ml].value_counts().reset_index()]\ndatas[0] = datas[0].reindex([6, 5, 4, 3, 1, 0, 2]).reset_index(drop=True)\ndatas[1] = datas[1].reindex([6, 1, 2, 3, 5, 4, 0, 7, 8]).reset_index(drop=True)\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 11))\nfig.suptitle('- For how long have they been practicing? -', size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2, ax3]\n\n# Pie 1\nlabels = datas[0][\"index\"].values\nsizes = datas[0][code].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#D7C8C0\", \"#CBBBB2\", \"#AB978D\", \"#C77952\", \"#C36130\", \"#A8410E\", \"#802C02\"]\n\nax1.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels, \n        autopct='%1.0f%%', pctdistance=0.8)\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax1.add_artist(centre_circle)\nax1.axis('equal')\n\n# Pie 2\nlabels = datas[1][\"index\"].values\nsizes = datas[1][ml].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#A6ADB1\", \"#80AABD\", \"#6196AE\", \"#4C829A\", \"#2E7493\", \"#255D76\", \"#0C5372\", \"#BBC2C5\", \"#D0D9DC\"]\n\nax2.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.8)\n\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax2.add_artist(centre_circle)\nax2.axis('equal')\n\n# Settings\nax1.set_title('Coding in General', size = 14, color = pirate_colors[0], weight='bold', pad=20)\nax2.set_title('Used ML Methods', size = 14, color = pirate_colors[0], weight='bold', pad=20)\n\n# Image\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/icons/code_icon.png\", \n           ax=ax1, zoom=0.06, offset=0, border=1)\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/icons/ml_icon.png\",\n           ax=ax2, zoom=0.05, offset=0, border=1)\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.8, wspace=0.4, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">Role and Duties at Work</h3>\n\nPointers here:\n* The most frequent jobs for the top respondents are <span style=\"background:#fed56f; font-weight:bold; color:black\">Data Scientist, ML Engineer or Software Engineer</span> (Analyst incorporates 3 jobs - Data Analyst, Business Analyst, and Statistician).\n* Some of the most crucial duties are:\n    * *Software Engineer*: more oriented towards **data infrastructure, exploration, and creating ML models**.\n    * *ML Engineer*: most prominent duties are for **building and exploring new ideas for ML models**.\n    * *Data Scientist*: the **most versatile** out of all, it incorporates almost equally all the duties (however, lower in the research areas). Basically, they need to know everything üëÄ."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Get data\ndt = prep_role_duties_data(df)\n\n# Filter out none\ndt = dt.drop(columns=[\"dutie6\"], axis=1)\n\n# Prep data for Sankey\nrole_data = dt[\"role\"].value_counts().reset_index()\nrole_data.columns = [\"role\", \"count\"]\n\ndutie_data = dt.melt(id_vars=[\"role\"], var_name=\"Name\", value_name=\"Dutie\")\ndutie_data.dropna(inplace=True)\n\ndutie_data = dutie_data.groupby(\"role\")[\"Dutie\"].value_counts().unstack().reset_index()\ndutie_data = dutie_data.melt(id_vars=[\"role\"], var_name=\"dutie\", value_name=\"count\")\ndutie_data = dutie_data.sort_values(['count'], ascending=False).groupby('role').head(5).sort_values(['role', 'dutie'])\n\ndutie_data = dutie_data[dutie_data[\"role\"] != \"DB Engineer\"]\ndutie_data = dutie_data[dutie_data[\"role\"] != \"Data Engineer\"]\ndutie_data = dutie_data[dutie_data[\"role\"] != \"Research Scientist\"]\ndutie_data = dutie_data[dutie_data[\"role\"] != \"Project Manager\"]\n\n\n# Get the graph data\nlabel = ['Analyst', 'Data Scientist', 'ML Engineer', 'Software Engineer',\n          \n         'Analyze and Understand Data',\n         'Build and run ML',\n         'Build and run data infrastructure',\n         'Create ML to explore new areas',\n         'Research to advance the state of ML']\n\nsource = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]\ntarget = [4, 5, 6, 7, 8, 4, 5, 6, 7, 8, 4, 5, 6, 7, 8, 4, 5, 6, 7, 8]\nvalue = dutie_data[\"count\"].values\n\n# Colors\ncolor_node = [\"#CC5600\", \"#9D4800\", \"#91281A\", \"#DA9300\",\n              \"#325C6E\", \"#325C6E\", \"#325C6E\", \"#325C6E\", \"#325C6E\"]\n\ncolor_link = [\"#F8E8DC\",\"#CC5600\", \"#F8E8DC\", \"#F8E8DC\", \"#CC5600\",\n              \"#EBD5C3\",\"#EBD5C3\", \"#9D4800\", \"#EBD5C3\", \"#9D4800\",\n              \"#DDCECC\", \"#DDCECC\", \"#91281A\", \"#DDCECC\", \"#91281A\",\n              \"#F8EED9\", \"#DA9300\", \"#F8EED9\", \"#F8EED9\", \"#DA9300\"]\n\n# Data to dict, dict to sankey\nlink = dict(source = source, target = target, value = value, color = color_link)\nnode = dict(label = label, pad=10, thickness=21, color=color_node, \n            line = dict(color = \"black\", width = [5,10]))\ndata = [go.Sankey(link = link, node=node, arrangement='snap')]\n\nlayout = go.Layout(hovermode = 'x',\n                   title=\"- Top Roles and Duties at Work -\",\n                   font=dict(size = 14, color = pirate_colors[0], family=\"Source Code Pro\",),\n                   paper_bgcolor='#FCFCFC',\n                   height=900)\n\n# PLOT\nfig = go.Figure(data, layout)\n\nfig.add_annotation(x=0, y=1.017,\n                   text=\"Roles\",\n                    showarrow=False,\n                   font=dict(\n                       family=\"Source Code Pro\",\n                       size=14,\n                       color=\"#325C6E\"))\nfig.add_annotation(x=1, y=1.033,\n                   text=\"Duties\",\n                    showarrow=False,\n                   font=dict(\n                       family=\"Source Code Pro\",\n                       size=14,\n                       color=\"#325C6E\"))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">How large are the company and the team?</h3>\n\nThe 2 plots <span style=\"background:#fed56f; font-weight:bold; color:black\">match very well the overall distribution of the respondents</span> ([you can see the overall summary here](https://www.kaggle.com/kaggle-survey-2020)):\n* On average, most respondents (regardless of their pay) are located in **small companies in 20% of the cases**, and more than **50% of the cases in large ones** (1000+ employees).\n* Also, the team size is **half the time bigger than 10 people**; however, **40% of cases are in small teams, of a maximum of 4 people**. This is a direct implication of the companies' size (there are many respondents in tiny companies, hence smaller DS teams and vice versa)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# --- OVERALL DATA ---\nsize = \"What is the size of the company where you are employed?\"\nds_team = \"Approximately how many individuals are responsible for data science workloads at your place of business?\"\n\ndt = df[[size, ds_team]]\ndatas = [dt[size].value_counts().reset_index(), dt[ds_team].value_counts().reset_index()]\ndatas[0] = datas[0].reindex([2, 3, 4, 1, 0]).reset_index(drop=True)\ndatas[1] = datas[1].reindex([4, 1, 2, 3, 5, 6, 0]).reset_index(drop=True)\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 11))\nfig.suptitle('- How large are the company and the team? -', size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2]\n\n# Pie 1\nlabels = datas[0][\"index\"].values\nsizes = datas[0][size].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#6E2D0A\", \"#DEC4B3\", \"#C2A99A\", \"#C5551A\", \"#913707\"]\n\nax1.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels, \n        autopct='%1.0f%%', pctdistance=0.8)\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax1.add_artist(centre_circle)\nax1.axis('equal')\n\n# Pie 2\nlabels = datas[1][\"index\"].values\nsizes = datas[1][ds_team].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#ADC2CF\", \"#69BDD7\", \"#3C9FBD\", \"#1785A6\", \"#99ADBA\", \"#839AA9\", \"#055E79\"]\n\nax2.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.8)\n\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax2.add_artist(centre_circle)\nax2.axis('equal')\n\n# Settings\nax1.set_title('Company Size', size = 14, color = pirate_colors[0], weight='bold', pad=20)\nax2.set_title('Team Size', size = 14, color = pirate_colors[0], weight='bold', pad=20)\n\n# Image\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/icons/company_icon.png\", \n           ax=ax1, zoom=0.06, offset=0, border=1)\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/icons/team_icon.png\",\n           ax=ax2, zoom=0.05, offset=0, border=1)\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.8, wspace=0.4, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">The Interest of the Company in Machine Learning</h3>\n\nThis question *might have some bias* because some people could have guessed a rough estimate, as they might not know of the \"business side\" of the company. However, because we're talking about high-end Data Scientists, who most certainly are involved in their projects' finances, this bias might be lower.\n\n*Or are they?*\n\nThe following graphs show some fascinating insights:\n* Firstly, respondents around **all groups** agreed that their company is either using  well established ML - or - they've just started implementing ML into the business ([and these numbers are increasing as the years pass, according to the general summary](https://www.kaggle.com/kaggle-survey-2020)).\n* We can see that the respondents with **High Pay** are located in companies that spend much less on ML than the other 2 groups. In the **Very High Pay** and **Crazy High Pay** groups, *more than 50% of the respondents* are employed in companies that *spend tens of thousands of dollars* on their ML equipment and team.\n* Hence, the graph shows that <span style=\"background:#fed56f; font-weight:bold; color:black\">the bigger the individual pay, the more the company invests in ML in its business model</span>."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ml_business = \"Does your current employer incorporate machine learning methods into their business?\"\nml_spent = \"Approximately how much money have you (or your team) spent on machine learning and/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?\"\n\ndt = df[[ml_business, ml_spent, categ]].reset_index(drop=True)\ndt[ml_spent] = dt[ml_spent].apply(lambda x: code_ml_spent(x))\n\n\n# ======= ML SPENT =======\ndt1 = dt.groupby(categ)[ml_spent].value_counts().unstack(fill_value=0).reset_index()\ndt1 = dt1.melt(id_vars=[categ], var_name=ml_spent, value_name=\"count\")\n\ntotal = dt1.groupby(categ)[\"count\"].sum().reset_index()\ndt1 = pd.merge(left=dt1, right=total, on=categ)\ndt1[\"perc\"] = round(dt1[\"count_x\"]/dt1[\"count_y\"]*100, 0).astype(int)\n\ndatas1 = [dt1[dt1[categ] == \"High\"], dt1[dt1[categ] == \"Very High\"], dt1[dt1[categ] == \"Crazy High\"]]\n\norder = [\"0\", \"$1-$99\", \"$100-$999\", \"$1000-$9,999\", \"$10,000-$99,999\", \">100,000\"]\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,14))\nfig.suptitle('- Company Spending on ML Technology (on Pay Category) -', size = 26, color = pirate_colors[0],\n             weight='bold')\naxs = [ax1, ax2, ax3]\n\n\nfor data, ax in zip(datas1, axs):\n    sns.barplot(y = data[ml_spent], x = data[\"perc\"], ax=ax, color = pirate_colors[1],\n                order = order)\n    \nax1.set_title('High Pay (%)', size = 13, color = treasure_colors[0], weight='bold')\nax2.set_title('Very High Pay (%)', size = 13, color = treasure_colors[0], weight='bold')\nax3.set_title('Crazy High Pay (%)', size = 13, color = treasure_colors[0], weight='bold')\n\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticks([])\n    ax.tick_params(axis='y', rotation=0)\n    show_values_on_bars(ax, h_v=\"h\", space=1)\n    \npath='../input/2020-cost-of-living/moneybag.png'\noffset_png(x=25, y=0, path=path, ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=21, y=3, path=path, ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=26, y=5, path=path, ax=ax2, zoom=0.05, offset=-22, border=1)\noffset_png(x=21, y=4, path=path, ax=ax2, zoom=0.05, offset=-22, border=1)\noffset_png(x=44, y=5, path=path, ax=ax3, zoom=0.05, offset=-22, border=1)\noffset_png(x=22, y=4, path=path, ax=ax3, zoom=0.05, offset=-22, border=1)\n\n# Arrow\nstyle = \"Simple, tail_width=0.5, head_width=6, head_length=13\"\nkw = dict(arrowstyle=style, color=\"#828E9D\")\narrow = patches.FancyArrowPatch((23, 2), (32, 4.3),\n                                connectionstyle=\"arc3,rad=-.3\", **kw)\nplt.gca().add_patch(arrow)\n    \nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.85, wspace=0.4, hspace=None)\nfig.text(0.04, 0.91, \n         'The majority of **all** groups agreed that they either have well established ML methods - or - they have just started using ML.', \n         size=14, color=\"#B76148\", weight='bold')\nfig.text(0.76, 0.56, '-more than 65%-', \n         size=12, color=\"#828E9D\", weight='bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n  <p><b>Takeaway</b>: We've learned that our high-end respondents are usually Data Scientists or Software/Machine Learning Engineers, with lots of coding experience, but not necessarily ML seniority.</p>\n  <p> They come from large and small companies, but the higher the investment and interest of the company in ML, the higher the individual income received.</p>\n</div>\n\n> And another area of the map has revealed itself: the Work Habitat!\n\n<img src=\"https://i.imgur.com/e05nc2Z.jpg\">\n\n> Closer look üîé\n<img src=\"https://i.imgur.com/6EkCvou.jpg\" width=700>\n\n<h2><span style=\"background:#F3E9D9; font-weight:normal; color:black\">3.4 The Coding Preferences</span></h2>\n\nNow we know that more than half of our respondents have 10+ years of coding experience. But let's discover what languages they use most, what is their DS setup, how do they deal with cloud and big data, and what advice they have to share.\n\n<h3 style=\"font-family:'Source Code Pro', monospace\">Languages Used and Advice to Community</h3>\n\nMost coders secretly root for Python, but are all the other languages obsolete?\n* **Python** is by far the most popular language in most areas, both in **usage** and as **a recommendation** for future users.\n* The next 3 most used languages are **SQL, R, and Bash**, all very useful in the Data Science discipline.\n* Hence, all our top respondents (regardless of the Pay Category) <span style=\"background:#fed56f; font-weight:bold; color:black\">use and recommend the most Python, followed by SQL, R, and Bash</span>."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Language\nlanguages = [\"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Python\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SQL\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C++\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Java\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Javascript\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Julia\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Swift\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash\",\n             \"What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB\"]\nadvice = \"What programming language would you recommend an aspiring data scientist to learn first? - Selected Choice\"\n\nall_columns = languages.copy()\nall_columns.append(advice)\nall_columns.append(categ)\n\n\n# Data\ndt = df[all_columns]\ndt.columns = [\"Python\", \"R\", \"SQL\", \"C\", \"C++\", \"Java\", \"Javascript\", \"Julia\", \n              \"Swift\", \"Bash\", \"MATLAB\", \"Advice\", \"Category\"]\n\nadvice_data = pd.DataFrame(dt.groupby(\"Advice\")[\"Category\"].count()).reset_index()\nadvice_data[\"Advice\"] = advice_data[\"Advice\"].apply(lambda x: code_advice(x))\nadvice_data = advice_data.groupby(\"Advice\")[\"Category\"].sum().reset_index()\n\nlang_data = dt.drop(columns=\"Advice\", axis=1)\nlang_data = lang_data.melt(id_vars=[\"Category\"], var_name=\"Language\", value_name=\"count\").dropna().reset_index(drop=True)\nlang_data = lang_data.groupby(\"Category\")[\"Language\"].value_counts().unstack().reset_index()\nlang_data = lang_data.melt(id_vars=[\"Category\"], var_name=\"Language\", value_name=\"count\").dropna().reset_index(drop=True)\nlang_data = lang_data.groupby(\"Language\")[\"count\"].sum().reset_index().sort_values('count', ascending=False)\n\n\n\n# ===== PLOT ======\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 12))\nfig.suptitle('- What are the most popular languages? -', size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2]\n\n# The barplot\nsns.barplot(y = lang_data[\"Language\"], x = lang_data[\"count\"], ax=ax1, color = treasure_colors[0])\n\n\n# Pie\nlabels = advice_data[\"Advice\"].values\nsizes = advice_data[\"Category\"].values\nexplode = (0.05, 0.05, 0.05, 0.05)\ncolors = [\"#8BADBC\", \"#09516F\", \"#267292\", \"#508DA6\"]\n\nax2.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.8)\n\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax2.add_artist(centre_circle)\nax2.axis('equal')\n\n# Beautify\n\nax1.set_title('Languages used on a regular basis', size = 13, color = treasure_colors[0], weight='bold')\nax2.set_title('What should you learn first?', size = 13, color = treasure_colors[0], weight='bold')\n\nax1.set_ylabel(\"\")\nax1.set_xlabel(\"\")\nax1.set_xticks([])\nax1.tick_params(axis='y', rotation=0)\nshow_values_on_bars(ax1, h_v=\"h\", space=1)\n\npath='../input/2020-cost-of-living/language_logos'\noffset_png(x=1901, y=0, path=f'{path}/python_logo.png', ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=1212, y=1, path=f'{path}/sql_logo.png', ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=706, y=2, path=f'{path}/r_logo.png', ax=ax1, zoom=0.025, offset=-22, border=1)\noffset_png(x=445, y=3, path=f'{path}/bash_logo.png', ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=374, y=4, path=f'{path}/javascript_logo.png', ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=366, y=5, path=f'{path}/java_logo.png', ax=ax1, zoom=0.05, offset=-22, border=1)\noffset_png(x=323, y=6, path=f'{path}/c++_logo.png', ax=ax1, zoom=0.03, offset=-22, border=1)\noffset_png(x=260, y=7, path=f'{path}/c_logo.png', ax=ax1, zoom=0.02, offset=-22, border=1)\noffset_png(x=181, y=8, path=f'{path}/matlab_logo.png', ax=ax1, zoom=0.04, offset=33, border=1)\noffset_png(x=56, y=9, path=f'{path}/julia_logo.png', ax=ax1, zoom=0.025, offset=33, border=1)\noffset_png(x=36, y=10, path=f'{path}/swift_logo.png', ax=ax1, zoom=0.04, offset=33, border=1)\n\n\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/snake.png\", ax=ax2, zoom=0.2, offset=0, border=1)\n\nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.89, wspace=None, hspace=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">What's the best setup?</h3>\n\nThe graphs below are about the same for all 3 Pay Categories:\n* Most work on their **personal laptop/computer**.\n* When working on the personal computer, the usage is usually oriented towards the classics: **Jupyter, RStudio, and PyCharm**.\n* They also use cloud computing services, like **Google Cloud Datalab/ AI Platform**, but most excessively **Colab** or **Kaggle Notebooks**.\n* To conclude, the top data scientists have *similar behavior in terms of environments and IDE with the average Kaggler*: use the most the <span style=\"background:#fed56f; font-weight:bold; color:black\">personal gear, Colab and Kaggle Notebooks and work on a combination of Jupyter, PyCharm, and RStudio</span> environments."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Environment\nall_columns = get_environment_questions()\n\nide = all_columns[:9] ; ide.append(categ)\nnotebook = all_columns[9:21] ; notebook.append(categ)\ncomputing = all_columns[21:22] ; computing.append(categ)\n\nide_names = [\"Jupyter\", \"R Studio\", \"Visual Studio\", \"PyCharm\", \n             \"Spyder\", \"Notepad++\", \"Sublime Text\", \"Vim/Emacs\", \"MATLAB\", \"Categ\"]\nnotebook_names = [\"Kaggle\", \"Colab\", \"Azure\", \"Paperspace\", \"Binder\", \n                  \"Code Ocean\", \"IBM Watson Studio\", \"Amazon Sagemaker Studio\",\n                  \"Amazon EMR\", \"Google Cloud AI Platform\", \"Google Cloud Datalab\", \"Databrick\", 'Categ']\ncomputing_names = [\"Computing Platform\", \"Categ\"]\n\n# Data\nide_data = df[ide].reset_index(drop=True) ; ide_data.columns = ide_names\nnotebook_data = df[notebook].reset_index(drop=True) ; notebook_data.columns = notebook_names\ncomputing_data = df[computing] ; computing_data.columns = computing_names\ncomputing_data.dropna(axis=0, inplace=True)\n\n# Prepping\nide_data = multiple_choice_prep(data=ide_data, category_name=\"Categ\", var_name=\"IDE\")\nnotebook_data = multiple_choice_prep(data=notebook_data, category_name=\"Categ\", var_name=\"Notebook\")\ncomputing_data = one_choice_prep(data=computing_data, category_name=\"Categ\", \n                                 count_on=\"Computing Platform\", var_name=\"Computing\")\n\nide_data[\"IDE\"] = ide_data[\"IDE\"].apply(lambda x: code_ide(x).strip())\nnotebook_data[\"Notebook\"] = notebook_data[\"Notebook\"].apply(lambda x: code_notebook(x))\nnotebook_data = notebook_data.groupby(\"Notebook\")[\"count\"].sum().reset_index().\\\n                    sort_values(by=\"count\", ascending=False)\nide_data = ide_data.groupby(\"IDE\")[\"count\"].sum().reset_index().\\\n                            sort_values(by=\"count\", ascending=False)\n\n\n\n# ===== PLOT ======\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 14))\nfig.suptitle(\"- What's the best setup? -\", size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2]\n\n# The barplot\nsns.barplot(y = notebook_data[\"Notebook\"], x = notebook_data[\"count\"], ax=ax1, color = pirate_colors[1])\n\n\n\n# Pie\nlabels = ide_data[\"IDE\"].values\nsizes = ide_data[\"count\"].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#771C01\", \"#8F2C0E\", \"#AB4121\", \"#C6684C\", \"#C68A78\", \n          \"#C79383\", \"#CAA194\", \"#D0B7AF\", \"#D1C5C1\"]\n\nax2.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.8)\n\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax2.add_artist(centre_circle)\nax2.axis('equal')\n\n# Beautify\n\nax1.set_title('Choice of Notebooks', size = 13, color = treasure_colors[0], weight='bold')\nax2.set_title('Choice of IDE', size = 13, color = treasure_colors[0], weight='bold')\n\nax1.set_ylabel(\"\")\nax1.set_xlabel(\"\")\nax1.set_xticks([])\nax1.tick_params(axis='y', rotation=0)\nshow_values_on_bars(ax1, h_v=\"h\", space=1)\n\npath='../input/2020-cost-of-living/language_logos'\noffset_png(x=651, y=0, path=f'{path}/colab_logo.png', ax=ax1, zoom=0.07, offset=-25, border=1)\noffset_png(x=608, y=1, path=f'{path}/kaggle_logo.png', ax=ax1, zoom=0.07, offset=-26, border=1)\noffset_png(x=0, y=0, path=f'{path}/ide_logo.png', ax=ax2, zoom=0.15, offset=0, border=1)\n\nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.86, wspace=0.05, hspace=None)\n\n# # Arrow\n# style = \"Simple, tail_width=4, head_width=8, head_length=16\"\n# kw = dict(arrowstyle=style, color=\"#000000\")\n# arrow = patches.FancyArrowPatch((-8, 0), (-1, 1),\n#                                 connectionstyle=\"arc3,rad=0\", **kw)\n# plt.gca().add_patch(arrow)\n\nfig.text(0.19, 0.92, '* Most use either a personal laptop/computer - and/or - a cloud computing plaftom *', \n         size=14, color=treasure_colors[2], weight='bold')\n\nfig.text(0.33, 0.50, '30% also use', \n         size=12, color=pirate_colors[1], weight='bold')\nfig.text(0.32, 0.47, 'Cloud Instances', \n         size=12, color=pirate_colors[1], weight='bold'); ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">Accelerators : Yay or Nay?</h3>\n\nFirst, let's understand some concepts:\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;\n     border-spacing:0;}\n.tg td{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg th{border-color:\"#010307\";\n    border-style:solid;\n    border-width:1px;\n    font-family:'Source Code Pro', monospace;\n    font-size:14px;\n    font-weight:normal;\n    overflow:hidden;\n    padding:10px 5px;\n    word-break:normal;}\n.tg .tg-c3ow{border-color:\"#010307\";\n    text-align:center;\n    vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-c3ow\"><font>Accelerator</font></th>\n    <th class=\"tg-c3ow\"><font>Explanation</font></th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-c3ow\">GPU (Graphics Processing Unit)</td>\n    <td class=\"tg-c3ow\">Designed to rapidly manipulate and alter memory to accelerate the creation of images. Used greatly in all areas of Data Science.</td>\n  </tr>\n  <tr>\n    <td class=\"tg-c3ow\">TPU (Tensor Processing Unit)</td>\n    <td class=\"tg-c3ow\">AI accelerator developed by Google specifically for neural network machine learning. Mostly used in Deep Learning problems.</td>\n  </tr>\n</tbody>\n</table>\n\n\nIs GPU better? Technically no - either if you choose CPU, GPU or TPU, they all outperform in some areas and underperform in others. [Learn more about this comparison here.](https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/#:~:text=TPU%20vs%20GPU%20vs%20CPU%3A%20A%20Cross%2DPlatform%20Comparison&text=TPU%3A%20Tensor%20Processing%20Unit%20is,small%20batches%20and%20nonMatMul%20computations.)\n\nSo, what are the top Data Scientists using to move fast during competitions and achieve the best scores? We know that they mostly use their personal laptop/ workstation, but how do these look like?\n* The visualization below is *representative* of all 3 Pay Categories.\n* Respondents are *split in half* (with very few exceptions): **~44% use no Acceleration**, while the other **44% use GPUs** most often.\n* Most of this **88% percent have never even tried TPUs** before, and if they did, they only used it between 2 to 5 times.\n* TPU users are **~ 7%** out of all respondents; however, half of them have been using TPUs more than 6 times (and around a quarter of them more than 25 times).\n* Hence, the <span style=\"background:#fed56f; font-weight:bold; color:black\">vast majority of our respondents use GPU acceleration</span>; however, there are a few <span style=\"background:#fed56f; font-weight:bold; color:black\">very experienced TPU users that rely mostly on TPU</span> during their work/competitions. Moreover, this graph is also a relief for beginners: <span style=\"background:#fed56f; font-weight:bold; color:black\">you don't necessarily need heavy computing power to be very good or earn well</span>."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Set columns\naccelerators = [\"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - GPUs\",\n                \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - TPUs\",\n                \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - None\",\n                \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - Other\"]\ntpu = \"Approximately how many times have you used a TPU (tensor processing unit)?\"\n\nall_columns = accelerators.copy()\nall_columns.append(tpu)\nall_columns.append(categ)\n\ndt = df[all_columns]\ndt.columns = [\"GPUs\", \"TPUs\", \"None\", \"Other\", \"times_tpu_used\", \"categ\"]\n\n# Speciffic data\nacc_data = dt[[\"GPUs\", \"TPUs\", \"None\", \"Other\", \"categ\"]]\ntpu_data = dt[[\"times_tpu_used\", \"categ\"]]\n\nacc_data = multiple_choice_prep(data=acc_data, category_name=\"categ\", var_name=\"accelerator\")\ntpu_data = one_choice_prep(data=tpu_data, category_name=\"categ\", \n                           count_on=\"times_tpu_used\", var_name=\"tpu_used\")\n\n# Prep\nacc_data = acc_data.groupby(\"accelerator\")[\"count\"].sum().reset_index().sort_values(\"count\", ascending=False)\ntpu_data = tpu_data.groupby(\"tpu_used\")[\"count\"].sum().reset_index()\n\n\n# Sankey Data\nsankey_data = dt[[\"GPUs\", \"TPUs\", \"None\", \"Other\", \"times_tpu_used\"]]\nsankey_data = multiple_choice_prep(sankey_data, category_name=\"times_tpu_used\", var_name=\"Acc\")\nsankey_data = sankey_data.reindex([4, 0, 1, 2, 3,\n                                     19, 15, 16, 17, 18,\n                                     9, 5, 6, 7, 8,\n                                     14, 10, 11, 12, 13])\nsankey_data = sankey_data[[\"Acc\", \"times_tpu_used\", \"count\"]]\n\n# Get the graph data\nlabel2 = ['GPUs', 'TPUs', 'None', 'Other',\n         'Once', '2-5 times', '6-25 times', '>25 times', 'Never']\nsource2 = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]\ntarget2 = [4, 5, 6, 7, 8, 4, 5, 6, 7, 8, 4, 5, 6, 7, 8, 4, 5, 6, 7, 8]\nvalue2 = sankey_data[\"count\"].values\n\n# Colors\ncolor_node = [\"#CC5600\", \"#DA9300\", \"#91281A\", \"#9D4800\",\n              \"#325C6E\", \"#325C6E\", \"#325C6E\", \"#325C6E\", \"#325C6E\"]\n\ncolor_link = [\"#CC5600\",\"#F8E8DC\", \"#CC5600\", \"#CC5600\", \"#F8E8DC\",\n              \"#DA9300\", \"#DA9300\", \"#DA9300\", \"#DA9300\", \"#DA9300\",\n              \"#DDCECC\", \"#91281A\", \"#91281A\", \"#91281A\", \"#DDCECC\",\n              \"#D0BBA7\",\"#D0BBA7\", \"#D0BBA7\", \"#D0BBA7\", \"#D0BBA7\"]\n\n# Data to dict, dict to sankey\nlink = dict(source = source2, target = target2, value = value2, color = color_link)\nnode = dict(label = label2, pad=10, thickness=21, color=color_node, \n            line = dict(color = \"black\", width = [5,10]))\ndata = [go.Sankey(link = link, node=node, arrangement='snap')]\n\nlayout = go.Layout(hovermode = 'x',\n                   title=\"- Accelerators -\",\n                   font=dict(size = 14, color = pirate_colors[0], family=\"Source Code Pro\",),\n                   paper_bgcolor='#FCFCFC',\n                   height=850)\n\n# PLOT\nfig2 = go.Figure(data, layout)\nfig2.add_annotation(x=0.5, y=1.07,\n                   text=\"* more than 88% are divided between -no Acceleration- and -GPU Acceleration- *\",\n                    showarrow=False,\n                   font=dict(\n                       family=\"Source Code Pro\",\n                       size=14,\n                       color=\"#CC5600\"))\nfig2.add_annotation(x=0, y=1.017,\n                   text=\"Accelerator\",\n                    showarrow=False,\n                   font=dict(\n                       family=\"Source Code Pro\",\n                       size=14,\n                       color=\"#325C6E\"))\nfig2.add_annotation(x=1, y=1.033,\n                   text=\"Times TPU used\",\n                    showarrow=False,\n                   font=dict(\n                       family=\"Source Code Pro\",\n                       size=14,\n                       color=\"#325C6E\"))\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Cloud\ncloud_platform = \"Which of the following cloud computing platforms do you use on a regular basis?\\\n            (Select all that apply) - Selected Choice -  ______ \"\ncloud_prod = \"Do you use any of the following cloud computing products on a regular basis?\\\n            (Select all that apply) - Selected Choice -  ______\"\ncloud_platform_f = \"Which of the following cloud computing platforms do you hope to become more familiar with\\\n                in the next 2 years? - Selected Choice - _____\"\ncloud_prod_f = \"In the next 2 years, do you hope to become more familiar with any of these specific cloud \\\n                computing products? (Select all that apply) - Selected Choice - ____\"\n\n\n# Big Data\nbigdata_regular = \"Which of the following big data products (relational databases, data warehouses, \\\n                    data lakes, or similar) do you use on a regular basis? (Select all that apply)\\\n                        - Selected Choice - ____ \"\nbigdata_often = \"Which of the following big data products (relational database, data warehouse, \\\n                    data lake, or similar) do you use most often? - Selected Choice\"\nbigdata_f = \"Which of the following big data products (relational databases, data warehouses, data \\\n                    lakes, or similar) do you hope to become more familiar with in the next 2 years? \\\n                            (Select all that apply) - Selected Choice - _____ \"\n\n\n# BI Tools\nbi_regular = \"Which of the following business intelligence tools do you use on a \\\n            regular basis? (Select all that apply) - Selected Choice - _____ \"\nbi_often = \"Which of the following business intelligence tools do you use most often? - Selected Choice\"\nbi_f = \"Which of the following business intelligence tools do you hope to become more familiar \\\n                with in the next 2 years? (Select all that apply) - Selected Choice - ______ \"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n  <p><b>Takeaway</b>: We've learned that our high-end respondents are Python people, usually coding on their personal gear. The usual environments used are Jupyter Notebooks, PyCharm, Colab, and Kaggle.</p>\n  <p>They most often use GPUs, but no worries, No Accelerator whatsoever is a popular choice as well - meaning skill only can bring you some pretty awesome results as well. ;)</p>\n</div>\n\n> The Code Waters is now visible!\n\n<img src=\"https://i.imgur.com/UgwKjQH.jpg\">\n\n> Closer look üîé\n<img src=\"https://i.imgur.com/LlgBbjN.jpg\" width=700>\n\n<h2><span style=\"background:#F3E9D9; font-weight:normal; color:black\">3.5 The Machine Learning Preferences</span></h2>\n\nIn this last chapter, we'll find out what machine learning tools and resources these Data Science veterans use, as well as what we need to focus on when we start our Machine Learning journey.\n\nLet the hunt conclude in elegance!\n\n<h3 style=\"font-family:'Source Code Pro', monospace\">Reliable Sources to Keep Informed</h3>\n\nThe learnings below are representative for all 3 levels of Pay Categories (meaning that all behave in the same manner, so the split wasn't shown):\n* For *learning*, the most popular choice was **Coursera**; as you may know, Coursera has the most popular [Machine Learning course](https://www.coursera.org/learn/machine-learning) from Stanford University, with the top instructor being the King of ML, **Andrew Ng**. This aspect might have significantly influenced the overall decision.\n* Other very popular Course platforms were **Kaggle Learn, Udemy, and University-specific courses**.\n* As *resources for getting the daily Data Science intake*, **Youtube** had the lead (possibly from the multitude of videos on different abstract topics, which are better understood from animated content rather than by only reading them). **Blogs** (such as Towards Data Science), **Twitter, and Publications** were a popular choice as well.\n* Hence, the \"go-to\" reliable sources for getting that Data Science information were <span style=\"background:#fed56f; font-weight:bold; color:black\">Coursera, Kaggle Learn, Udemy, Youtube, and Blogs</span>."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Data Prep\ncourses_data, media_data = get_reliable_sources(df)\n\ncourses_data = courses_data.groupby(\"Course\")[\"count\"].sum().reset_index()\nmedia_data = media_data.groupby(\"Media\")[\"count\"].sum().reset_index()\n\nx = [2, 3.4, 3, 6, 5, 5.5, 8, 8.5, 5.3, 8]\ny = [2, 8, 4, 4.5, 6, 2.5, 7, 9, 9.7, 3.7]\ncourses_data[\"x\"] = x\ncourses_data[\"y\"] = y\n\nx = [4, 2.5, 3, 6, 5, 7.5, 8, 7.5, 4.9]\ny = [2.8, 7, 4, 4.5, 6, 3.5, 5.3, 7.7, 8.3]\nmedia_data[\"x\"] = x\nmedia_data[\"y\"] = y\n\n# ===== Plots =====\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 12))\nfig.suptitle(\"- Reliable Sources -\", size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2]\n\n# Plot 1\na1 = sns.scatterplot(x=courses_data[\"x\"], y = courses_data[\"y\"],\n                     size = courses_data[\"count\"], sizes=(150,20000),\n                     hue = courses_data[\"Course\"], ax=ax1,\n                     palette = all_colors[:10])\n\na1.legend([],[], frameon=False)\nfor line in range(0,courses_data.shape[0]):\n     ax1.text(courses_data.x[line]+0.4, courses_data.y[line], courses_data.Course[line], \n             horizontalalignment='left', size=10, color='#1B2631', weight='bold')\n\n# Plot 2\na2 = sns.scatterplot(x=media_data[\"x\"], y = media_data[\"y\"],\n                     size = media_data[\"count\"], sizes=(150,30000),\n                     hue = media_data[\"Media\"], ax=ax2,\n                     palette = all_colors[:9])\n\na2.legend([],[], frameon=False)\nfor line in range(0,media_data.shape[0]):\n     ax2.text(media_data.x[line]+0.5, media_data.y[line], media_data.Media[line], \n             horizontalalignment='left', size=10, color='#1B2631', weight='bold')\n        \n\n# Beautify\nax1.set_title('Favorite Platforms to learn DS', size = 15, color = pirate_colors[0], weight='bold',\n              pad=14)\nax2.set_title('Favorite Media Sources', size = 15, color = pirate_colors[0], weight='bold',\n              pad=14)\n\nax1.set_xlim(1.8, 10)\nax1.set_ylim(1.5, 10.5)\nax2.set_xlim(1.9, 9.5)\nax2.set_ylim(1.9, 9.3)\n\n\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \noffset_png(x=3.5, y=9, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax1, zoom=0.05, offset=0, border=1)\noffset_png(x=2.8, y=8.4, path=f'../input/2020-cost-of-living/pirate_patch.png', \n           ax=ax1, zoom=0.05, offset=0, border=1)\n\noffset_png(x=4.9, y=9.2, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax2, zoom=0.05, offset=0, border=1)\noffset_png(x=4.2, y=8.7, path=f'../input/2020-cost-of-living/pirate_patch.png', \n           ax=ax2, zoom=0.055, offset=0, border=1)\n\noffset_png(x=4, y=3.5, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax2, zoom=0.05, offset=0, border=1)\noffset_png(x=3.4, y=3, path=f'../input/2020-cost-of-living/pirate_patch_white.png', \n           ax=ax2, zoom=0.05, offset=0, border=1)\n\n    \nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=0.05, hspace=None)\n\nax2.plot([0.5, 0.5], [0.13, 0.9], color='#D0D3D4', lw=5,\n         transform=plt.gcf().transFigure, clip_on=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">What Frameworks to have in your Pocket?</h3>\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* For the *overall Frameworks*, **Scikit-learn** has the lead. This is also **the oldest** (13 years), the **most volatile and versatile** library, so the fact that it has the most popularity isn't a shock.\n* Out of the deep learning frameworks, **Tensorflow** is the most popular, but PyTorch is starting to gain some visibility itself (it is *1 year younger* than Tensorflow, so it's only natural that it is a little bit behind).\n* As for the *visualization libraries*, our top-end respondents mainly prefer and use **the originals** Matplotlib, Seaborn, and Plotly, with Ggplot for R.\n* Hence, the industry's top data scientists use lots of <span style=\"background:#fed56f; font-weight:bold; color:black\">Scikit-learn, Tensorflow, Matplotlib, and Seaborn</span>. However, we mustn't lose sight of the emerging libraries that have the potential to overcome soon the giants, such as <span style=\"background:#fed56f; font-weight:bold; color:black\">PyTorch</span> (can you already tell I'm a fan of PyTorch?üëÄ)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Libraries\nviz_data, framework_data = get_frameworks(df)\n\nviz_data = viz_data.groupby(\"Visualization\")[\"count\"].sum().reset_index()\nframework_data = framework_data.groupby(\"Frameworks\")[\"count\"].sum().reset_index()\nframework_data = framework_data.sort_values(\"count\", ascending=False)\n\n\n# ===== PLOT ======\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 12))\nfig.suptitle(\"- Frameworks in you Pocket -\", size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2]\n\n# The barplot\na1 = sns.barplot(y = framework_data[\"Frameworks\"], x = framework_data[\"count\"], \n                 ax=ax1, color = treasure_colors[2])\n\n# Bubbles\nx = [2.5, 5.5, 3, 6, 3.7, 5.5, 6, 7.7, 4, 8]\ny = [2.5, 9, 4, 4.5, 5.5, 3.1, 6.5, 8, 8, 3.7]\nviz_data[\"x\"] = x\nviz_data[\"y\"] = y\n\na2 = sns.scatterplot(data = viz_data,\n                     x=\"x\", y = \"y\", size = \"count\", sizes=(150,20000),\n                     hue = \"Visualization\", ax=ax2,\n                     palette = all_colors[:10])\n\na2.legend([],[], frameon=False)\nfor line in range(0,viz_data.shape[0]):\n     ax2.text(viz_data.x[line]+0.4, viz_data.y[line], viz_data.Visualization[line], \n             horizontalalignment='left', size=10, color='#1B2631', weight='bold')\n        \nax2.set_xlim(2, 9.3)\nax2.set_ylim(2, 9.7)\n\n\n# Beautify\n\nax1.set_title('Overall Frameworks', size = 13, color = treasure_colors[0], weight='bold')\nax2.set_title('Visualization Frameworks', size = 13, color = treasure_colors[0], weight='bold')\n\nax1.set_ylabel(\"\")\nax1.set_xlabel(\"\")\nax1.set_xticks([])\nax1.tick_params(axis='y', rotation=20)\nshow_values_on_bars(ax1, h_v=\"h\", space=1)\n\nax2.set_ylabel(\"\")\nax2.set_xlabel(\"\")\nax2.set_xticks([])\nax2.set_yticks([])\n\n# Images\noffset_png(x=6.0, y=7.3, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax2, zoom=0.05, offset=0, border=1)\noffset_png(x=5.4, y=6.8, path=f'../input/2020-cost-of-living/pirate_patch.png', \n           ax=ax2, zoom=0.05, offset=0, border=1)\n\noffset_png(x=4, y=8.75, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax2, zoom=0.03, offset=0, border=1)\noffset_png(x=7.7, y=8.6, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax2, zoom=0.03, offset=0, border=1)\noffset_png(x=5.5, y=3.7, path=f'../input/2020-cost-of-living/pirate_hat.png', \n           ax=ax2, zoom=0.03, offset=0, border=1)\npath='../input/2020-cost-of-living/language_logos'\noffset_png(x=1487, y=0, path=f'{path}/scikit_logo.png', ax=ax1, zoom=0.04, offset=-27, border=1)\noffset_png(x=949, y=1, path=f'{path}/tensorflow_logo.png', ax=ax1, zoom=0.034, offset=-27, border=1)\noffset_png(x=868, y=2, path=f'{path}/keras_logo.png', ax=ax1, zoom=0.22, offset=-27, border=1)\noffset_png(x=755, y=3, path=f'{path}/xgboost_logo.png', ax=ax1, zoom=0.07, offset=-27, border=1)\noffset_png(x=648, y=4, path=f'{path}/pytorch_logo.png', ax=ax1, zoom=0.08, offset=-26, border=1)\n\nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.04, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">Machine Learning & Deep Learning Basics</h3>\n\nThe below graphs and explanations are representative for all 3 Pay Categories:\n* Awkwardly enough, the most frequent choices for *ML Methods* were **Regressions and Tree-Based methods**, which are more straightforward approaches than Neural Nets. Hence, complicated methodologies aren't necessarily better.\n* **Classification** and **General Image Methods** (like cv2, PIL, or skimage) are the most popular in the *Computer Vision* department.\n* Regarding *NLP*, our top respondents were **Word Embeddings** and **Transformers**.\n* Hence, the most popular choices were the simpler ones, such as <span style=\"background:#fed56f; font-weight:bold; color:black\">Regressions, Tree-Based Methods, General & Classification methods, and Word Embeddings</span>."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"colors1 = [\"#703428\", \"#7c3d2c\", \"#884630\", \"#934f34\", \"#9f5938\", \n          \"#ab633c\", \"#b66d40\", \"#c17844\", \"#cc8347\", \"#d68e4b\"]\ncolors2 = [\"#c89a37\", \"#cfa651\", \"#d6b269\", \"#dcbe81\", \"#e2ca99\"]\ncolors3 = [\"#449faf\", \"#6abbc8\", \"#8ed7e2\", \"#b1f4fc\"]\n\n\n# ML & DL\nml_data, compvis_data, nlp_data = get_ml_dl_data(df)\nml_data = ml_data.groupby(\"ML\")[\"count\"].sum().reset_index()\ncompvis_data = compvis_data.groupby(\"CompVis\")[\"count\"].sum().reset_index()\nnlp_data = nlp_data.groupby(\"NLP\")[\"count\"].sum().reset_index()\n\nml_data = ml_data.sort_values(\"count\", ascending=False)\ncompvis_data = compvis_data.sort_values(\"count\", ascending=False)\nnlp_data = nlp_data.sort_values(\"count\", ascending=False)\n\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 11))\nfig.suptitle('- What ML/DL Methods to use? -', size = 26, weight='bold')\naxs = [ax1, ax2, ax3]\n\n# Bar1\nsns.barplot(data=ml_data, y=\"ML\", x=\"count\", ax=ax1, palette = sns.color_palette(colors1, 10))\n\n# Bar2\nsns.barplot(data=compvis_data, y=\"CompVis\", x=\"count\", ax=ax2, palette=sns.color_palette(colors2, 5))\n\n# Bar3\nsns.barplot(data=nlp_data, y=\"NLP\", x=\"count\", ax=ax3, palette=sns.color_palette(colors3, 4))\n\n# Beautify\nax1.set_title('Machine Learning Methods', size = 14, color = pirate_colors[0], weight='bold', pad=20)\nax2.set_title('Deep Learning Methods', size = 14, color = pirate_colors[0], weight='bold', pad=20)\nax3.set_title('Natural Language Processing Methods', size = 14, color = pirate_colors[0], weight='bold', pad=20)\n\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticks([])\n    ax.tick_params(axis='y', rotation=40)\n    show_values_on_bars(ax, h_v=\"h\", space=1)\n    \n    \noffset_png(x=1650, y=0, path='../input/2020-cost-of-living/pirate_hat2.png', ax=ax1, zoom=0.14, offset=-16, border=1)\noffset_png(x=552, y=0, path='../input/2020-cost-of-living/pirate_hat2.png', ax=ax2, zoom=0.2, offset=-29, border=1)\noffset_png(x=434, y=0, path='../input/2020-cost-of-living/pirate_hat2.png', ax=ax3, zoom=0.25, offset=-32, border=1)\n\nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.6, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">Analysis at Work and Deployment</h3>\n\nThe visualization and discussion below are representative for all 3 Pay Categories:\n* Regarding *Deployment*, most of our top respondents prefer to either deploy on **GitHub** or **keep their models locally on their personal gear**. **Kaggle and Colab**, however are popular options as well.\n* For *Tools Analysis*, the most popular are the **Local Development Environments**, like *Jupyter Notebooks* or *R Studio*. \n* So, our top respondents analyze data mostly on <span style=\"background:#fed56f; font-weight:bold; color:black\">Local Environments</span> and deploy models either on <span style=\"background:#fed56f; font-weight:bold; color:black\">GitHub, or keep them on their local machine</span>."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# ML at Work/School\n\n# Columns\ndeploy = [col for col in df.columns if \n          \"Where do you publicly share or deploy your data analysis or machine learning applications?\" in col]\ndeploy.append(categ)\ntool = [\"What is the primary tool that you use at work or school to analyze data? (Include text response) - Selected Choice\"]\ntool.append(categ)\n\n#Data\ndeploy_data = df[deploy]\ntool_data = df[tool]\n\ndeploy_data = multiple_choice_prep(data=deploy_data, category_name=\"Category\", var_name=\"Deploy\")\ntool_data = multiple_choice_prep(data=tool_data, category_name=\"Category\", var_name=\"Tool\")\n\ndeploy_data = deploy_data.groupby(\"Deploy\")[\"count\"].sum().reset_index().sort_values(\"count\", ascending=False)\ntool_data = tool_data.groupby(\"Tool\")[\"count\"].sum().reset_index().sort_values(\"count\", ascending=False)\n\ndeploy_data[\"Deploy\"] = deploy_data[\"Deploy\"].apply(lambda x: code_deploy(x))\ntool_data[\"Tool\"] = tool_data[\"Tool\"].apply(lambda x: code_tool(x))\n\ndeploy_data = deploy_data[deploy_data[\"Deploy\"]!= \"NBViewer\"]\ndeploy_data = deploy_data[deploy_data[\"Deploy\"]!= \"Streamlit\"]\ndeploy_data = deploy_data[deploy_data[\"Deploy\"]!= \"Other\"]\ndeploy_data = deploy_data[deploy_data[\"Deploy\"]!= \"Plotly Dash\"]\n\n# ===== Plot =====\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 11))\nfig.suptitle('- Deployment and Work Analysis -', size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2]\n\n# Pie 1\nlabels = deploy_data[\"Deploy\"].values\nsizes = deploy_data[\"count\"].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#7D3100\", \"#A34000\", \"#BA5B1D\", \"#CD753C\", \"#D99162\", \"#E5AE8A\"]\n\nax1.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels, \n        autopct='%1.0f%%', pctdistance=0.8)\n\n#draw circle\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax1.add_artist(centre_circle)\nax1.axis('equal')\n\n# Pie 2\nlabels = tool_data[\"Tool\"].values\nsizes = tool_data[\"count\"].values\nexplode = (0.05, 0.05, 0.05, 0.05, 0.05, 0.05)\ncolors = [\"#005071\", \"#006995\", \"#118BBE\", \"#3BA0CA\", \"#65AECC\", \"#88B8CD\"]\n\nax2.pie(sizes, explode=explode, colors=colors, startangle=90, labels=labels,\n        autopct='%1.0f%%', pctdistance=0.8)\n\ncentre_circle = plt.Circle((0,0),0.65,fc='white')\nax2.add_artist(centre_circle)\nax2.axis('equal')\n\n# Settings\nax1.set_title('Deployment', size = 14, color = pirate_colors[0], weight='bold', pad=20)\nax2.set_title('Analysis at Work', size = 14, color = pirate_colors[0], weight='bold', pad=20)\n\n# Image\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/deployment.png\", \n           ax=ax1, zoom=0.13, offset=0, border=1)\noffset_png(x=0, y=0, path=\"../input/2020-cost-of-living/analysis.png\",\n           ax=ax2, zoom=0.1, offset=0, border=1)\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.8, wspace=0.5, hspace=None);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"font-family:'Source Code Pro', monospace\">Machine Learning - Miscellaneous Tools</h3>\n\nFinally, the visualization and discussion below are representative for all 3 Pay Categories as well:\n* For *ML Products*, there is a general lack of interest for all the presented options, as **None** was the most popular choice. However, most respondents say that they would like to gain knowledge of **Google Cloud Products** in the next 2 years.\n* Regarding *ML Experiments*, there is even a more prominent lack of interest, as **None** is the predominant answer, followed from far by **TensorBoard**. The **carelessness continues** in the projection over the next 2 years. So, ML Experiments, not so popular at the moment.ü§∑‚Äç‚ôÄÔ∏è\n* There is a lack of frequent usage in *Automated ML* as well; however, there can be seen a general trend for **Automated Model Selection** for the next 2 years.\n* Hence, *ML products, Experiments, or Automated ML* aren't a popular choice between our top respondents. A reason for that could be the inclination towards old school coding, as they might <span style=\"background:#fed56f; font-weight:bold; color:black\">prefer writing everything from scratch</span>, going through the data, the patterns, and the models with \"their own hands\", <span style=\"background:#fed56f; font-weight:bold; color:black\">instead of relying on a service</span>. But I am sure this fact will change over the years."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Get Data\nml_prods_data, ml_prods_f_data, ml_experm_data, ml_experm_f_data, automl1_data, automl2_data, \\\n        automl1_f_data, automl2_f_data = prep_ml_extended(df)\n\n# Erase \"Other\" category\nml_prods_data = ml_prods_data[ml_prods_data[\"Prods\"] != \"Other\"]\nml_experm_data = ml_experm_data[ml_experm_data[\"Experiment\"] != \"Other\"]\nautoml1_data = automl1_data[automl1_data[\"AutoML1\"] != \"Other\"]\n\n# Group by\nml_prods_data = ml_prods_data.groupby(\"Prods\")[\"count\"].sum().reset_index()\nml_experm_data = ml_experm_data.groupby(\"Experiment\")[\"count\"].sum().reset_index()\nautoml1_data = automl1_data.groupby(\"AutoML1\")[\"count\"].sum().reset_index()\n\n# Code values\nml_prods_data[\"Prods\"] = ml_prods_data[\"Prods\"].apply(lambda x: code_mlprods(x))\nml_experm_data[\"Experiment\"] = ml_experm_data[\"Experiment\"].apply(lambda x: code_mlexper(x))\nautoml1_data[\"AutoML1\"] = automl1_data[\"AutoML1\"].apply(lambda x: code_automl(x))\n\nx = [1.2, 3.4, 4.5, 1.8, 5, 2.5, 2, 5, 4.3, 8]\ny = [1.2, 8, 4, 5, 6, 2.5, 7, 9, 9.7, 1.7]\nml_prods_data[\"x\"] = x\nml_prods_data[\"y\"] = y\n\nx = [1, 3.4, 4.5, 1.8, 5, 2.5, 2, 5, 4.3, 8]\ny = [1, 8, 4, 5, 6, 2.5, 7, 9, 9.7, 1.7]\nml_experm_data[\"x\"] = x\nml_experm_data[\"y\"] = y\n\nx = [1.5, 4.4, 4, 5, 2, 6, 8]\ny = [1.5, 8, 4, 5, 6.5, 9.4, 3]\nautoml1_data[\"x\"] = x\nautoml1_data[\"y\"] = y\n\n# ===== Plots =====\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 12))\nfig.suptitle(\"- Miscellaneous ML Tools -\", size = 26, color = pirate_colors[0], weight='bold')\naxs = [ax1, ax2, ax3]\n\n# Plot 1\na1 = sns.scatterplot(x=ml_prods_data[\"x\"], y = ml_prods_data[\"y\"],\n                     size = ml_prods_data[\"count\"], sizes=(30,3000),\n                     hue = ml_prods_data[\"Prods\"], ax=ax1,\n                     palette = all_colors[:10])\n\na1.legend([],[], frameon=False)\nfor line in range(0,ml_prods_data.shape[0]):\n     ax1.text(ml_prods_data.x[line]+0.4, ml_prods_data.y[line], ml_prods_data.Prods[line], \n             horizontalalignment='left', size=10, color='#1B2631', weight='bold')\n\n# Plot 2\na2 = sns.scatterplot(x=ml_experm_data[\"x\"], y = ml_experm_data[\"y\"],\n                     size = ml_experm_data[\"count\"], sizes=(30,3000),\n                     hue = ml_experm_data[\"Experiment\"], ax=ax2,\n                     palette = all_colors[:10])\n\na2.legend([],[], frameon=False)\nfor line in range(0,ml_experm_data.shape[0]):\n     ax2.text(ml_experm_data.x[line]+0.5, ml_experm_data.y[line], ml_experm_data.Experiment[line], \n             horizontalalignment='left', size=10, color='#1B2631', weight='bold')\n        \n# Plot 2\na3 = sns.scatterplot(x=automl1_data[\"x\"], y = automl1_data[\"y\"],\n                     size = automl1_data[\"count\"], sizes=(30,3000),\n                     hue = automl1_data[\"AutoML1\"], ax=ax3,\n                     palette = all_colors[:7])\n\na3.legend([],[], frameon=False)\nfor line in range(0,automl1_data.shape[0]):\n     ax3.text(automl1_data.x[line]+0.5, automl1_data.y[line], automl1_data.AutoML1[line], \n             horizontalalignment='left', size=10, color='#1B2631', weight='bold')\n        \n\n# Beautify\nax1.set_title('Favorite ML Products', size = 15, color = pirate_colors[0], weight='bold',\n              pad=35)\nax2.set_title('Favorite ML Experiments', size = 15, color = pirate_colors[0], weight='bold',\n              pad=35)\nax3.set_title('Favorite Auto ML Solutions', size = 15, color = pirate_colors[0], weight='bold',\n              pad=35)\n\n\nfor ax in axs:\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlim(0.9, 10)\n    ax.set_ylim(0.9, 10)\n    \n# offset_png(x=3.5, y=9, path=f'../input/2020-cost-of-living/pirate_hat.png', \n#            ax=ax1, zoom=0.05, offset=0, border=1)\n# offset_png(x=2.8, y=8.4, path=f'../input/2020-cost-of-living/pirate_patch.png', \n#            ax=ax1, zoom=0.05, offset=0, border=1)\n\n    \nsns.despine(left=True, bottom=True)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.84, wspace=None, hspace=None)\n\nfig.text(0.16, 0.865, '*2 years knowledge resolution:', \n         size=10, color=treasure_colors[2], weight='bold')\nfig.text(0.18, 0.85, 'Google Cloud Products', \n         size=10, color=treasure_colors[2], weight='bold')\n\nfig.text(0.435, 0.865, '*2 years knowledge resolution:', \n         size=10, color=treasure_colors[2], weight='bold')\nfig.text(0.455, 0.85, 'NONE (&TensorBoard)', \n         size=10, color=treasure_colors[2], weight='bold')\n\nfig.text(0.71, 0.865, '*2 years knowledge resolution:', \n         size=10, color=treasure_colors[2], weight='bold')\nfig.text(0.75, 0.85, 'Model Selection', \n         size=10, color=treasure_colors[2], weight='bold')\n\nax2.plot([0.37, 0.37], [0.13, 0.9], color='#D0D3D4', lw=5,\n         transform=plt.gcf().transFigure, clip_on=False);\nax2.plot([0.65, 0.65], [0.13, 0.9], color='#D0D3D4', lw=5,\n         transform=plt.gcf().transFigure, clip_on=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert success-alert\">\n  <p><b>Takeaway</b>: Our top respondents use Youtube, Personal Blogs, and Twitter to keep informed on the latest Data Science \"gossips\" while having lots of trust in Coursera as a reliable source of learning.</p>\n  <p>As for tools and methods, even though one would expect some complicated answers, they still use and rely the most on simple structures, like Regressions, Tree-Based models, Image Classification, or Word Embeddings for NLP. Most loved frameworks are Scikit-learn, Tensorflow, and Matplotlib, with no complicated or additional current preference for miscellaneous tools.</p>\n  <p>There IS something to learn here. There was a tweet from Grandmaster <a href=\"https://www.kaggle.com/tunguz\">Bojan Tunguz</a> that was sarcastically stating that all models besides Neural Nets are obsolete and you should focus only on these. The takeaway here is that no simple method, tool, or model is \"redundant\", not even for the great of the great. And this chapter shows precisely that.</p>\n</div>\n\n<img src=\"https://i.imgur.com/OmlXHgw.png\" width=700>\n\n> And finally, our map is completely revealed!\n\n<img src=\"https://i.imgur.com/0EanMig.jpg\">\n\n> Closer look üîé\n<img src=\"https://i.imgur.com/kv3fJhx.jpg\" width=700>\n\n\n# 4. Conclusion\n\nI for one would have loved to see a survey like this when I started, some 1 year and a half ago. With no experience and no slight clue what data science is, what programming language do I need to know, how do you do machine learning, what is a notebook, how deep is deep learning, and ... for the love of God, GPUs? And, with the internet full of healthy and diverse opinions, it is easy to get lost and start not to trust the sources.\n\nHence, I truly hope that this analysis will bring some light for anybody in any query they might have. However small or big, I think that it's best to ask a professional, a great veteran in the discipline, a guru, a <span style=\"background:#fed56f; font-weight:bold; color:black\">Grandmaster :)</span> what is the best way to learn that specific subject? And if this notebook helped you just a bit getting closer to your answers, or a mentor, I can declare myself happy and fulfilled. I know it helped me, at least.\n\nGood luck, and may we all succeed with grace. Happy Data Sciencin'!\n\n# 5. References üìú\n\n* [Numbero Cost of Living Index](https://www.numbeo.com/cost-of-living/rankings_by_country.jsp)\n* [Pareto Principle](https://en.wikipedia.org/wiki/Pareto_principle)\n* [TPU vs GPU](https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/#:~:text=TPU%20vs%20GPU%20vs%20CPU%3A%20A%20Cross%2DPlatform%20Comparison&text=TPU%3A%20Tensor%20Processing%20Unit%20is,small%20batches%20and%20nonMatMul%20computations.)\n* [Machine Learning Course - Stanford](https://www.coursera.org/learn/machine-learning)\n\n<img src=\"https://i.imgur.com/0eIXf89.png\">\n\n# Specs ‚å®Ô∏èüé®\n### (*tools that helped visualisation & creating the pirate map*)\n* Z8 G4 Workstation üñ•\n* 2 CPUs & 96GB Memory üíæ\n* NVIDIA Quadro RTX 8000 üéÆ\n* RAPIDS version 0.17 üèÉüèæ‚Äç‚ôÄÔ∏è"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}