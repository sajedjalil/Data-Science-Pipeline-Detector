{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.display.max_columns = 500\nimport re\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n# matplotlib\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nclass Tweet(object):\n    def __init__(self, embed_str=None):\n        self.embed_str = embed_str\n\n    def _repr_html_(self):\n        return self.embed_str\n\ndf_2020 = pd.read_csv('../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv')\ndf_2020 = df_2020.drop(0, axis=0)\ndf_2019 = pd.read_csv('../input/kaggle-survey-2019/multiple_choice_responses.csv')\ndf_2019 = df_2019.drop(0, axis=0)\n\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Are we entering the era of hardware lottery ?        \n**Mhamed Jabri ‚Äî 01/04/2021**\n***\n\nOver the last few years, Machine Learning no longer remained this cool concept only computer and math nerds are intrigued by but rather became globally talked about : Media outlets regularly have posts about Artificial Intelligence, Google's ads for the Pixel phone emphasize the use of AI in processing the pictures and d Deep Fakes are now merely an everyday casual subject.                 \n\nHow did we get here ? **Deep Learning, and GANs specifically, definitely helped a lot.** GANs, which LeCun described at one point as *the most interesting idea in the last 10 years in ML* [1] , produced results that caught everyone's attention : Let's be honest, who didn't try at least once the Picasso filter on their pictures ?\n\nML progress and prowess didn't stop there as more impressive (and/or hyped) results made it to the news : \n* Deepmind's AlphaGo was such a sensation, a documentary was made about it, showing the upbringing of the algorithm and how it ended up beating Lee Sedol 4-1 in a 5 games series.    \n* One of the more recent examples is OpenAI's GPT-3[2], a general language model that is trained on a large amount of uncategorized text from the internet and that, given some text, will guess what text comes next.              \nGPT-3 was able to showcase some really eye-popping results as it wasn't only able to answer a lot of common sense questions (already impressive) but also produce HTML rather than natural language for example, effectively creating web-page layouts [3] (That being said, GPT-3 definitely has a lot of concerning limitations and we'll come back to it later on).\n\n\nWhat do most of these innovations have in common ? **They require an absurd amount of hardware resources** (and in GPT-3's case : amount of data) : At the time GPT-3 was published, the cost was estimated at $4.6M for a single training run, more on that can be found here [4].        \n\nIn this notebook, I will be leveraging this year's Kaggle survey, last year's Kaggle survey as well as other outlets to explore the importance of hardware to researchers in ML, data scientists and kagglers and how it potentially impacts success."},{"metadata":{},"cell_type":"markdown","source":"# The case for ML becoming a Hardware lottery \n*** \n\nIn this first section, I will be exploring how top-notch hardware accessibility can play a crucial role in achieving meaningful results in research, industry and kaggle. "},{"metadata":{},"cell_type":"markdown","source":"## In Research :\nI first came across the term \"Hardware lottery\" while reading this brilliant essay / position paper [5] by Sara Hooker. In her paper, she uses hardware lottery to describe **when a research idea wins because it is suited to the available software and hardware and not because the idea is universally superior to alternative research directions.**              \n\nHistorically, this has happened more often than we may realize. Let's take the example of deep learning and backpropagation : while it was first introduced in the 1960s (made popular in the context of machine learning by Hinton in 1986), it only became widely used in the 2000s after GPUs were repurposed to train deep neural networks.              \nSo while as concepts backpropagation and neuralnets were probably universally superior to many other research directions at the time they were introduced, they only got the spotlight they deserved when the available hardware was good enough to showcase what those techniques are capable of doing.  \n\nIn my introduction, I brought up GPT-3 and the fact that it is a gigantic DL model but that trend has been seen in academic conferences as well : **The never-ending quest to beat SOTA in NLP and computer vision tasks has led to a ‚Äúbigger is better‚Äù race in the size of deep learning architectures** thus requiring better hardware and higher costs constantly [6].       \nOne of the possible consequences of this is that only the biggest schools and private research labs, which can actually afford those possibly really high costs, end up publishing in the top flight AI conferences. \nOur own @CPMP, who also happens to hold a PhD in ML, seems to share this concern : "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"s = (\"\"\"<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I am glad I did my PhD a while ago. It was possible to publish in top AI conferences (IJCAI, AAAI at the time) if you had solid work you could do on a desktop machine ( an Apple Mackintosh for me). Nowadays you need to be at some top school with enough hardware to have a chance.</p>&mdash; JFPuget Wash Hands Social Distancing Wear Mask (@JFPuget) <a href=\"https://twitter.com/JFPuget/status/1346771347448455169?ref_src=twsrc%5Etfw\">January 6, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\"\"\")\nTweet(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While at some point it was believed that universities were losing their best AI scientists solely because of the salaries offered at a handful companies [7], it's now becoming not only a salary argument but also a hardware availability one.         \nIndeed, many AI scientists are tempted by the realm of possibilities in research they could explore when working in environments equipped with the latest and best hardware, which is exactly what they are offered by the best AI companies. "},{"metadata":{},"cell_type":"markdown","source":"## Survey : Computing plateforms and TPUs\n\nIn the survey, there were multiple questions adressing what tools are being used by data scientists.        \nFrom a hardware standpoint, I explored two specific questions : **Which computing plateforms are being used** and **how often TPUs are used**. For the second one, I compared the usage rate in 2020 to the one of 2019 as well, since that question was in the 2019 survey too. "},{"metadata":{},"cell_type":"markdown","source":"### Computing Plateforms "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmp_computing = df_2020['Q11'].value_counts(normalize=True).reset_index()\ntmp_computing.columns = ['category','ratio_2020']\n\ntrace = go.Bar(\n                x = tmp_computing.category,\n                y = tmp_computing.ratio_2020,\n                name = \"Computing Platform\",\n                marker = dict(color = 'rgba(255, 255, 128, 0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n)\ndata = [trace]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TPU Adoption "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tpu_2020 = df_2020['Q13'].value_counts(normalize=True).reset_index()\ntpu_2020.columns = ['category','ratio_2020']\ntpu_2019 = df_2019['Q22'].value_counts(normalize=True).reset_index()\ntpu_2019.columns = ['category','ratio_2019']\ntpu_2019.replace({'6-24 times' : '6-25 times', \n                  '> 25 times' : 'More than 25 times'}, inplace=True)\n\ntpu_freq = tpu_2019.merge(tpu_2020, how='inner', on='category')\n\n# create trace1 \ntrace1 = go.Bar(\n                x = tpu_2019.category,\n                y = tpu_2019.ratio_2019,\n                name = \"2019\",\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5))\n)\n# create trace2 \ntrace2 = go.Bar(\n                x = tpu_2020.category,\n                y = tpu_2020.ratio_2020,\n                name = \"2020\",\n                marker = dict(color = 'rgba(255, 255, 128, 0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n)\ndata = [trace1, trace2]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From computing platforms perspective, there's about 19% of responders that use either a cloud computing platform or a deep learning workstation more than their personnal laptop for ML.           \nSo basically, for about 1/5 of the ML tasks/work, a personnal laptop may not be enough as hardware.  \n\n\n* As for TPU adoption : In 2019, 80% of the respondents never used TPUs. In 2020, that ratio decreased to 70%. As TPUs are definitely more widely used, it's not necessarily out of necessity as it may be due to more public availability, thanks to Kaggle notebooks for example (https://www.kaggle.com/product-feedback/129828).\n\n**The takeaway here is that there's definitely a trend by data scientists of using more advanced hardware : Adoption and use of TPUs is increasing + many responders need dedicated cloud computing platforms to perform their ML tasks rather than just their personal laptop.**"},{"metadata":{},"cell_type":"markdown","source":"## Kaggle Competitions\n\nLet's take a look at how discussions about hardware / hardware used in Kaggle competitions evolved. \n\nI'll start this one by the following discussion post on hardware for competitions : https://www.kaggle.com/general/9444. Seems like 7 years ago, things were definitely simpler on Kaggle and an i3 with 12gb of RAM was enough for a Grandmaster.                      \n3 years after that post, came another one with the same question, https://www.kaggle.com/general/22970 : We notice that comments start including GPUs and AWS instances.                          \nThings seem to have evolved in that direction as time went by.\n\nRecently, competitions hosted by Kaggle are often either Computer Vision or NLP competitions. In other words, competitions that most definitely require using deep learning techniques (and GPUs/TPUs).\n\nI took the liberty of looking at two of the more recently completed competitions (one NLP, one CV), for which the winners not only published their winning solutions but also their hardware requirements (unrelated : Always head to the discussion part of a competition once it ends, you'll find some absolute GEMS !!) : \n\n* **Jigsaw Multilingual Toxic Comment Classification :** One of the models of the final ensemble was trained using dual RTX Titans [8]. \n* **RSNA STR Pulmonary Embolism Detection :**  The winner used 4 x RTX Titan for image-level model training, and 1 x 2070 for study-level training [9]. \n\nLimerobot, a Kaggler who became a grandmaster in the span of one year, has specified that the specs of his PC include 5 x NVIDIA RTX2080Ti 11G (2 GPUs in 1 PC) [10]. \n\n\nAll of these examples definitely seem to indicate that having success on Kaggle, or at least being on the podium, not only requires top-notch software/coding skills but also very solid hardware. \n"},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"# Silver Lining \n*** \nIn the previous section, we've seen how hardware can be the bottleneck to ML research or the difference between finishing 5th and 1st in a Kaggle competition.\n\nIn this section, I will use survey responses, my own personal experience as a data scientist who worked with multiple clients in mulitple industries as well as Kaggle competitions' results to argue that notable accomplishments can be obtained all over the place with the more accessible, widely available resources. "},{"metadata":{},"cell_type":"markdown","source":"## Research \nWhile a lot of research groups are focused on building large language models with jaw-dropping numbers when it comes to amount of training data and parameters of the neural net, **some others took a different route and tried to build compressed models that may not be as performant but compensate with reasonable training time / reproducibility capability.** One of the leading research groups in this line of work is HuggingFace.          \n\nOne of their models, DistilBERT [11], a distilled version of BERT, was able to achieve 95% of the performance of the original BERT while being 60% smaller and faster. Although they're not chasing SOTA results, they've been able to publish in NeurIPS regularly while making NLP models more accessible to all. To top it all, their transformers package finished the 2020 year in top 10 most active python package on GitHub ! "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"s = (\"\"\"<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Look who is in the 10 most active python packages of the *whole* GitHub in 2020 ü§Ø<br><br>More in GitHub&#39;s yearly report: <a href=\"https://t.co/88CqEqVcvF\">https://t.co/88CqEqVcvF</a><br><br>Thanks to @ 8bitmp3 for the pointer: <a href=\"https://t.co/HYQMrfxozP\">https://t.co/HYQMrfxozP</a><br><br>And thanks a million to each contributor üî• <a href=\"https://t.co/d3mKCEEHHj\">pic.twitter.com/d3mKCEEHHj</a></p>&mdash; Thomas Wolf (@Thom_Wolf) <a href=\"https://twitter.com/Thom_Wolf/status/1346863465156276226?ref_src=twsrc%5Etfw\">January 6, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\"\"\")\nTweet(s)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Survey : Frameworks and Algorithms \n\nWe'll start by looking at the most used frameworks and algorithms by the respondents to the survey and discuss how computationally expensive they are :"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Q16_cats = []\nQ16_vals = []\n\nfor col in df_2020.columns:\n    if 'Q16' in col:\n        Q16_cats.extend(df_2020[col].value_counts().reset_index()['index'].to_list())\n        Q16_vals.extend(df_2020[col].value_counts().reset_index()[col].to_list())\n\ntmp_frameworks = pd.DataFrame({'Framework':Q16_cats,\n                               'Values':Q16_vals})\n\ntmp_frameworks.sort_values(by='Values', inplace=True)\n# create trace1 \ntrace1 = go.Bar(\n                x = tmp_frameworks.Values,\n                y = tmp_frameworks.Framework,\n                orientation='h',\n                name = \"citations\",\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.2)),\n)\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\n\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Q17_cats = []\nQ17_vals = []\n\nfor col in df_2020.columns:\n    if 'Q17' in col:\n        Q17_cats.extend(df_2020[col].value_counts().reset_index()['index'].to_list())\n        Q17_vals.extend(df_2020[col].value_counts().reset_index()[col].to_list())\n\ntmp_algos = pd.DataFrame({'Algorithm':Q17_cats,\n             'Values':Q17_vals})\n\ntmp_algos.sort_values(by='Values', inplace=True)\n# create trace1 \ntrace1 = go.Bar(\n                x = tmp_algos.Values,\n                y = tmp_algos.Algorithm,\n                orientation='h',\n                name = \"citations\",\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.2)),\n)\n\ndata = [trace1]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\n\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the use of deep learning frameworks (Tensorflow, Keras, Pytorch) and algorithms (CNN, RNN, Transformers) is significant as shown by the two barplots above, they are still widely surpassed by more traditionnal frameworks (sklearn) and algorithms (Linear/Log regression, boosted trees etc). \n\nAs a data scientist myself working in industry, while there are definitely instances in which the use of deep learning models is necessary (basically any vision or NLP task), standard architectures and pre-trained models usually achieve satisfactory results there and do not require any exceptionnal hardware capabilities. \nMore often that not though, in all other tasks, a good old xgboost alongwith SHAP plots, detailed EDAs and sensitivity analysis provide what the client needs.           \nIs there potential for achieving better results (whatever the metric may be for a predictive task) by stacking more sophisicated models ? Probably. But the marginal gain is almost never worth the 1/ time 2/ loss of interpretability brought by linear or tree-based models. \n\nAnother metric I was interested in exploring is the $ spending in hardware. Since this question was asked in the 2019 and 2020 survey, I used both to see wether there's a year to year increase by data scientists / organizations."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"spending_2020 = df_2020['Q25'].value_counts(normalize=True).reset_index()\nspending_2020.columns = ['category','ratio_2020']\nspending_2020.replace({'$0 ($USD)' : '$0 (USD)', '$100,000 or more ($USD)' : '> $100,000 ($USD)'}, inplace=True)\nspending_2019 = df_2019['Q11'].value_counts(normalize=True).reset_index()\nspending_2019.columns = ['category','ratio_2019']\n\nspending_hardware = spending_2019.merge(spending_2020, how='inner', on='category')\nspending_hardware['category'] = spending_hardware['category'].apply(lambda x : re.sub('[$]','',x))\n# print(spending_hardware)\n\n# create trace1 \ntrace1 = go.Bar(\n                x = spending_hardware.category,\n                y = spending_hardware.ratio_2019,\n                name = \"2019\",\n                marker = dict(color = 'rgba(255, 174, 255, 0.5)',\n                             line=dict(color='rgb(0,0,0)',width=1.5))\n)\n# create trace2 \ntrace2 = go.Bar(\n                x = spending_hardware.category,\n                y = spending_hardware.ratio_2020,\n                name = \"2020\",\n                marker = dict(color = 'rgba(255, 255, 128, 0.5)',\n                              line=dict(color='rgb(0,0,0)',width=1.5)),\n)\ndata = [trace1, trace2]\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, it seems that spendings actually decreased between 2019 and 2020 (although it's not the same population of respondents so this is not a perfect apple-to-apple comparison). There are reasons to believe that this holds true though : \n* There are more and more laptops equipped with GPUs powerful enough to run deep learning models. Apple's latest M1 MacBook pro is a prime example of this trend and we can expect that over the next few years, laptops with solid GPUs will be more widely availble to the general public.\n* By giving access to entirely free GPUs / TPUs, while being able to privately upload data, platforms like Kaggle allow data scientists to do heavy work without spending a dime. "},{"metadata":{},"cell_type":"markdown","source":"## Kaggle Competitions \n*** \n\nIn the first section, I've showed competitions where winning solutions required top notch, up to date hardware. But it would be bad faith to not also talk about other competitions where winning or achieving a very high ranking only required widely available resources : \n\n* **Google Landmark Retrieval 2020 :** In this one, the winner only used Colab TPUs (similar to Kaggle TPUs) to achieve the first place. [12]\n* **M5 Forecasting - Accuracy :** While many highly ranked submissions for this competition used heavy deep learning architectures, the 4th solution showed that with great focus on feature engineering and validation strategy, a good old single LightGBM model, which in practice would be ideal for a production setting, was more than enough to achieve great results. [13]\n\nAlso, as a follow-up to the previous tweet by @CPMP, I asked him if he thinks that top hardware requirements applies to Kaggle as much as it does in research. Here's what he had to say : "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"s = (\"\"\"<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Great question! <br><br>I won two prizes in two different comps early last year with a PC with 2 1080 Ti GPU. That was not top notch hardware a year ago. But definitely more than a regular PC.<br><br>I do think Silver medal is reachable with a single, GPU, say a 2080 Ti or better.</p>&mdash; JFPuget Wash Hands Social Distancing Wear Mask (@JFPuget) <a href=\"https://twitter.com/JFPuget/status/1346795134529232896?ref_src=twsrc%5Etfw\">January 6, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\"\"\")\nTweet(s)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"There you go then, another instance of a grandmaster winning prizes without using the best available hardware resources. \n\nAnother solution Kaggle came up with to adress the emerging discrepancy caused by hardware access to competitors is \"kernel only\" competitions where all kagglers use the same resources. One of such competitions was : https://www.kaggle.com/c/mercari-price-suggestion-challenge"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion \n*** \n\nIn this notebook, I've provided examples of how hardware might be a determining factor in ML research advancements as well as Kaggle rankings while also showing that one is not doomed if he doesn't have access to such top-notch hardware, since many applications and valuable discoveries only require limited ressources. \n\nThat being said, in addition to the $ cost and hardware availability, there are definitely other important factors one must take into account when discussing these large models. Here are some : \n\n* Environmental cost : **It appears that training a single AI model can emit as much carbon as five cars, hold your breath ... in their lifetimes !![14].** Indeed, in [15], researchers at the University of Massachusetts Amherst, performed a life cycle assessment for training several common large AI models. They found that the Neural Architecture Search used to find an optimal Transformer architecture (with 213M parameters) can emit more than 626,000 pounds of carbon dioxide equivalent. (Thanks @CPMP for correcting the initial framing)\n\n* AGI and reasoning false narrative / hype : Whenever one of these large models lands impressive results (that are often cherry picked for blog post publications to increase the hype), people, especially from the media side, start claiming that we're closing on AGI. That's balantly false, unfortunately. In [16], the reviewer is giving GPT-3 a turing test that shows how impressive it can be in some tasks and how subhuman it still is in others. In [17], you can see a collection of tweets by ML researchers highlighting both the feats and downfalls of the model.        \nUnfortunately, what can be even more alarming in such models is their ability to generate, or shall we say reproduce, hateful speech based on the training data it has received. Here's one particular instance : "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"s = (\"\"\"<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">It takes an average of 20 pizzas to stop the language model from generating violence-related content on muslims! I think I know what I&#39;m ordering for dinner...</p>&mdash; Marzyeh (@MarzyehGhassemi) <a href=\"https://twitter.com/MarzyehGhassemi/status/1336341910215151616?ref_src=twsrc%5Etfw\">December 8, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\"\"\")\nTweet(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This was extracted from a NeurIPS 2020 workshop and you can find more extracts in [18] and potential recordings on the workshop website. \n\nAll in all, does hardware advancements lead to interesting models and applications ? Yes, without a doubt. But as data scientists, we have a responsability to adress all potential risks and concerns of these large models before deploying and commercialzing them.  "},{"metadata":{},"cell_type":"markdown","source":"# References \n*** \n\n[1] https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning\n\n[2] https://en.wikipedia.org/wiki/GPT-3\n\n[3] https://twitter.com/sharifshameem/status/1282676454690451457\n\n[4] https://lambdalabs.com/blog/demystifying-gpt-3/\n\n[5] https://arxiv.org/abs/2009.06489\n\n[6] https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/\n\n[7] https://www.theguardian.com/science/2017/nov/01/cant-compete-universities-losing-best-ai-scientists\n\n[8] https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160862\n\n[9] https://github.com/GuanshuoXu/RSNA-STR-Pulmonary-Embolism-Detection\n\n[10] https://medium.com/kaggle-blog/zero-to-grandmaster-in-a-year-a-winners-interview-with-limerobot-18ddb3a1aae1\n\n[11] https://medium.com/huggingface/distilbert-8cf3380435b5\n\n[12] https://www.kaggle.com/c/landmark-retrieval-2020/discussion/176037\n\n[13] https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163216\n\n[14] https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/\n\n[15] https://arxiv.org/abs/1906.02243\n\n[16] https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html\n\n[17] https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/\n\n[18] https://twitter.com/kchonyc/status/1336340712703537153"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}