{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The 2020 Kaggle ML & DS survey discovery\n## or How to discover a new dataset with Jupyter"},{"metadata":{},"cell_type":"markdown","source":"**Acknowledgements** :  \nThis notebook based on the datasets : https://www.kaggle.com/c/kaggle-survey-2020/"},{"metadata":{},"cell_type":"markdown","source":"For the fourth year, Kaggle proposes an annual Machine Learning and Data Science Survey Challenge [[Kaggle2020]](#Kaggle2020), where the goal is to create a notebook who tells a rich story about a dataset based of the data science and machine learning community. These data provide an overview of the Kaggle community usage. \n\nIn this notebook, we try to show a way to deal with an unknown database. Our story will be about how to discover a database for the first time. It is always difficult to know where to start and what we are  looking for. What sense do we want to give to these data? What information are we looking for? Do there exist hidden informations or knowledge, which could be the basis of new ideas or new concepts?\n\nThis notebook will propose a data preparation and some data analysis approach for this dataset. \n\n\n## Contents\n1. [Import librairies](#1)\n2. [Download data](#2)\n3. [Data preparation](#3)\n4. [Data analysis](#4)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Load useful package\nimport sys    # library for accessing system-specific parameters and functions\nimport gc     # library for garbage collector\nimport warnings    # library to deal with warning messages\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 - Import librairies\n<a id=\"1\"></a>\nFirstly, we import the used libraries:\n\n* **[[matplotlib.pyplot]](#matplotlib)**: This library provides basic charts to visualize data.\n* **[[missingno]](#missingno)**: This library proproses utility functions to filter records in your dataset based on completion. \n* **[[numpy]](#numpy)**: This library is for scientific computing. It provides a high-performance multidimensional array object, and tools for working with these arrays.\n* **[[pandas]](#pandas)**: This library provides fast, powerful, flexible and easy-to-use data structures, as well as the means to quickly perform operations on these structures.\n* **[[plotly]](#plotly)**: This library provides interactive charts to visualize data.\n* **[[seaborn]](#seaborn)**: This library is data visualization library based on matplotlib. It provides a high-level interface for informative statistical graphics."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Load used package for data science\nimport numpy as np   # library for scientific computing\nimport pandas as pd  # library for data processing, CSV file I/O\npd.set_option(\"display.max.columns\", None)  # Display all columns \n#pd.set_option(\"display.precision\", 3)   # Use 3 decimal places in output display\n\n# Libraries used for visualization\nimport matplotlib.pyplot as plt  # library for graphics\n%matplotlib inline\nplt.style.use('ggplot')\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport seaborn as sns\n\n# Library specific for missing data\nimport missingno as msno","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 - Download data\n<a id=\"2\"></a>\n\nThe 2020 Kaggle Machine Learning & Data Science survey [[Kaggle2020]](#Kaggle2020) was lived for 3.5 weeks in October. The data have been cleaned by the organisers. They include raw numbers about who is working with data, what’s happening with machine learning in different industries, and the best ways for new data scientists to break into the field. The data have been published in as raw a format as possible without compromising anonymization, which makes it an unusual example of a survey dataset."},{"metadata":{},"cell_type":"markdown","source":"To begin, we have to load the data file and to well-name all columns. In the data file, the two first rows contain informations about the column contents: the first one contents the name of the question and the second one the label of the question. We decided to use only the first one to name the columns of the dataset."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Load survey data from 2020\n# Load column names (which are in the 2 first rows)\ncolName = pd.read_csv(\"../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv\", header=None, nrows=2)\ncolName.loc[2,:] = colName.loc[0,:]+\" \"+colName.loc[1,:]\ncolName.rename(columns=colName.loc[0,:], \n               index={0:'number',1:'question',2:'numberAndQuestion'}, \n               inplace=True)\n\n# Load data in a dataframe\ndf_data = pd.read_csv(\"../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv\", \n                      header=1, \n                      names=colName.loc['number'])\ndf_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 2020 Kaggle DS & ML Survey dataset is made up of 20 036 entries and 355 columns. All columns (except the first one which corresponds to the user time to fill the survey) contain categorical informations. The size of used memory on the server to store the dataset is 255.1 Mb."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Size of the load dataset\nprint(\"Size of the dataframe :\", df_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Type of data stored in the columns\nprint(\"Types of data stored in the columns : \\n\", df_data.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Size of used memory to store the dataset\n# <=> print(sys.getsizeof(df_data)/1024/1024)      # bytes\n# <=> print(df_data.memory_usage(deep=True).sum()/1024/1024)    # bytes\ndf_data.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 - Data preparation\n<a id=\"3\"></a>\nData preparation [[Kuhn2019]](#Kuhn2019) is the most important and time-consuming part of data analysis. This involves transforming raw data into a representation that could be understandable for machine learning algorithms and run quickly with data science tools and techniques. This step is highly specific to the used data, to the goals of the project and to the algorithms that will be used to model them or to extract information from them. \n\nThere are common or standard tasks to use or explore during the data preparation step in a data analysis or a machine learning project. These tasks include:\n* **Data cleaning**: Identifying and correcting mistakes or errors in the data.\n* **Data encoding**: Reducing used memory to store the data in order to optimize future used algorithms.\n* **Missing data**: Identifying if there are no data whatsoever for a contributor (non-response) or when some variables for a participant are unknown (item non-response) because of refusal to provide or failure to collect the response.\n* **Feature engineering**: Deriving new variables from available data.\n* **Data transforms**: Changing the scale or distribution of variables.\n* **Feature selection**: Identifying which variables are most relevant to our analysis.\nEach of these tasks is a whole field of study with specialized algorithms.\n\nIn some cases, variables must be encoded or transformed before we can apply a machine learning algorithm, such as converting strings to numbers. In other cases, it is less clear, such as scaling a variable may or may not be useful to an algorithm. We decided to apply the 4 first steps in this part and to deal with the 2 last one in the next part according to the processing we will apply (if it is useful)."},{"metadata":{},"cell_type":"markdown","source":"## a) Data cleaning\nData cleaning is the most important step before analyzing or modeling data. It could be boring and considered as a lost of time, but it is fundamental to do better data analysis.\n\nThe 2020 Kaggle DS & ML Survey database have been prepared for the challenge by the organisers. So we could consider that the data is clean. There contains no error."},{"metadata":{},"cell_type":"markdown","source":"## b) Data encoding \nIt could be interesting to optimize the size of our dataframe. It allows to use low memory in the environment. One way to address that is to specify data types of your dataframe in a more efficient way than the automatic detection done by Pandas.\n\nThe 2020 Kaggle DS & ML Survey database is composed of the answers of 39 questions :\n* 1 numerical question with the time taken to fill up the form, \n* 18 single answer questions,\n* 21 questions with multiple choice: The participants have the ability to select all options and one column for each option has be created. In these questions, there are also 8 specific answers to a particular question leads to an additional (earlier-choice specific or an extra) question. The answers of these questions are recorded in 336 columns of the dataframe."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Rename with short names the longest country names\ndf_data.loc[df_data['Q3']==\"Iran, Islamic Republic of...\", \"Q3\"] = \"Iran\"\ndf_data.loc[df_data['Q3']==\"United Kingdom of Great Britain and Northern Ireland\", \"Q3\"] = \"United Kingdom\"\ndf_data.loc[df_data['Q3']==\"United States of America\", \"Q3\"] = \"USA\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# List of questions with multiple choice answers\nlistQuestMCA = ['Q7', 'Q9', 'Q10', 'Q12', 'Q14', 'Q16', 'Q17', 'Q18', \n                'Q19', 'Q23', 'Q36', 'Q37', 'Q39']\nlistQuestMCAB = ['Q26', 'Q27', 'Q28', 'Q29', 'Q31', 'Q33', 'Q34', 'Q35']\nlistQuestMC = listQuestMCA + [i+\"_A\" for i in listQuestMCAB] + [i+\"_B\" for i in listQuestMCAB]\n#print(listQuestMC)\n\n# List of questions with single answers\ntmpNb = 0\nlistQuestSA = list(colName.loc['number',:])\n# For all questions, verify the \"None\" answer and calculate the binary representaiton of answers\nfor quest in listQuestMC:\n    # Select column which names begin with \"question\"\n    tmpQuest = [col for col in df_data.columns if col.startswith(quest)]\n    # Delete found columns to list of single answer questions\n    for quest in tmpQuest:\n        listQuestSA.remove(quest)\n        tmpNb += 1\n#print(listQuestSA)\n\nprint(\"Number of questions with single answer :\", len(listQuestSA))\nprint(\"Number of questions with multiple choice answers :\", \n     (len(listQuestMCA)+len(listQuestMCAB)), \n     \"(\",tmpNb,\")\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Add new rows in the colName \n# with the number of the value in the column\ntmpData = pd.DataFrame(df_data.apply(lambda x: x.count()), columns={'nbValue'})\ntmpData = tmpData.transpose()\ncolName = pd.concat([colName, tmpData]) # colName.append(tmpData,ignore_index=False)\n# with the number of the different value in the column\ntmpData = pd.DataFrame(df_data.apply(lambda x: len(x.unique())), columns={'nbDiffValue'})\ntmpData = tmpData.transpose()\ncolName = pd.concat([colName, tmpData]) # colName.append(tmpData,ignore_index=False)\n# with the different values in the columns\ntmpData = pd.DataFrame(df_data.apply(lambda x: x.unique()), columns={'value'})\ntmpData = tmpData.transpose()\ncolName = pd.concat([colName, tmpData])    #colName.append([colName, tmpData], ignore_index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Single answer questions contains a finite number of text values. In general, we consider them as categorical variables, but, by default, Pandas stores them as objects. This type of storage is not optimal, because it creates a list of pointers to the memory address of each value of your column. For columns with low cardinality (the amount of unique values is lower than 50% of the count of these values), this can be optimized by forcing pandas to use a virtual mapping table where all unique values are mapped via an integer instead of a pointer. This is done using the category datatype. It is a hybrid data type. It looks and behaves like a string in many instances but internally is represented by an array of integers. This allows the data to be sorted in a custom order and to more efficiently store the data. We apply an encoding method to consider our single answer questions as category."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# List of single answer questions\ntmpQuest = [col for col in listQuestSA if col.startswith('Q')]\n# Transform object in category\ndf_data[tmpQuest] = df_data[tmpQuest].apply(lambda x:x.astype('category'), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Size of used memory to store the dataset\ndf_data.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous outputs show us that many columns contain \"Yes or No\" answers  represented as object. The binary encoding could be used to effectively store these answers. It is a standard type in Python. It is possible to reduce the memory usage of the dataframe without losing information in transforming these objects as boolean value. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# A list which contains the information mentioned in the 2-valued column\ntmpList = []\n# Transform 2-valued categorical in boolean value\nfor col in colName.columns:\n    tmpCol = colName[col]\n    if tmpCol['nbDiffValue'] == 2:\n        #print(tmpCol['value'])\n        # Collect the name mentioned in the studied column \n        tmpData = pd.Series(tmpCol['value'])\n        tmpData = list(tmpData.dropna())\n        # Add this information to the list of category name\n        tmpList.append(tmpData[0])\n        # Encoding as boolean values the found columns\n        df_data.loc[:,tmpCol['number']] = np.where(df_data[tmpCol['number']].isnull(), False, True)\n    else :\n        tmpList.append(np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Add a new row in the colName with the value concerned by the column\ntmpData = pd.DataFrame(tmpList, columns=['refValue'], index=list(colName.loc['number']))\ntmpData = tmpData.transpose()\ncolName = pd.concat([colName, tmpData])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Size of used memory to store the dataset\ndf_data.info(memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The both representation convertion allows to save 97,3% of the used memory on the server (21Mb with the categorical type and 226.7Mb with the binary encoding), which will have consequences on performance of further processings. The new dataframe is consuming only 6.9 Mb."},{"metadata":{},"cell_type":"markdown","source":"## c) Missing data\nThe second step consists in exploring data in order to know if there exists missing and/or duplicate data elements and then to decide how we are going to deal with them.\n\nThe 2020 Kaggle DS & ML Survey database have been prepared for the challenge by the organisers. So we could consider that we have no duplicate data. However, the following outputs show the count of missing values by columns for the single question. Some of them are very numerous. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Selection of columns of single questions\ntmpCol = listQuestSA[1:len(listQuestSA)]\n# Drawing matrix of missing data\nmsno.bar(df_data[tmpCol], color=(0, 0, 1))\nplt.title(\"Number of answer by one-answer questions\", fontsize=26)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Drawing matrix of missing data\nmsno.matrix(df_data[tmpCol], labels=True,\n            figsize=(20,20), color=(0, 0, 1))\nplt.title(\"Completeness of the answer to one-answer questions\", fontsize=26)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we explore more precisely theses questions, these missing data become normal: the questions are so specific on a domain that the participants might not known it and they do not have the possibility to say it. For example, the question Q30 concerns big data products and Q32 the business intelligence. "},{"metadata":{},"cell_type":"markdown","source":"## d) Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"Sometimes it could be interesting to transform a set of binary values in a single condensed representation with all the binary value to apply a machine learning approach. The comparison between 2 data becomes then only a AND-operation between them. This processing could so allow to improve the further performance. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Function to delete None answer in the questions\ndef verifyNoneAnswer(question, colName):\n    # Select column which names begin with \"question\"\n    colQuestion = [col for col in colName if col.startswith(question)]\n    # Select column which names begin with \"question\" (without \"None\" answer)\n    colWithoutNone = colQuestion.copy()\n    colWithoutNone.pop(-2)\n    # Number of columns for which the user answered \"Yes\" (without \"None\" answer)\n    tabNbYes = df_data[colWithoutNone].sum(axis=1)    \n\n    # Comparison between column \"question_None\" and number of answers\n    tabBin = pd.concat([df_data[colQuestion].iloc[:,-2], tabNbYes], axis=1)\n    # Est ce que tous les \"None\" n'ont rien saisi d'autres ? \n    s1 = tabBin.index[~(tabBin.iloc[:,0].isnull())]\n    s2 = tabBin.index[(tabBin.iloc[:,1] == 0)] \n    tmpCol = question+\"_NoneWD\"\n    tabBin.loc[:,tmpCol] = np.where((((~(tabBin.iloc[:,0].isnull())) & (tabBin.iloc[:,1] == 0))\n                                     | (((tabBin.iloc[:,0].isnull())) & (tabBin.iloc[:,1] != 0))),\n                                     True, False)\n    \n    # Return the columns with None\n    return colWithoutNone, tabNbYes, tabBin[tmpCol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Function used to translate the multiple choice questions in binarary representation\ndef translateBinAnswer(colQuestion):\n    # Transformation du nom du langage en 0 ou 1 dans la colonne concernée\n    tabBin = df_data[colQuestion]*1\n    # Concaténation des 0 et des 1 des colonnes correspondants à la question\n    colBin = tabBin.astype(str).apply(''.join, 1)\n    return colBin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# For all questions, verify the \"None\" answer and calculate the binary representaiton of answers\nfor quest in listQuestMC:\n    # Verify the \"None\" answer \n    # Easy way : df_raw.count() : count the number of values not NaN per column, \n    # but do not take into account the raw representation of multiple choice questions\n    tmpQuestion, tmpNbAnswer, tmpNoneWD = verifyNoneAnswer(quest, colName)\n    # Add the number of answer to the question in the dataset\n    tmpCol = quest+\"_Nb\"\n    df_data.insert(len(df_data.columns), tmpCol, tmpNbAnswer, True)\n    # Add the validation of the \"None\" question in the dataset\n    tmpCol = quest+\"_WD\"\n    df_data.insert(len(df_data.columns), tmpCol, tmpNoneWD, True)\n    # Define a binary representation of the answers to the question\n    tmpAnswer = translateBinAnswer(tmpQuestion)\n    # Add the validation of the \"None\" question in the dataset\n    df_data.insert(len(df_data.columns), quest, tmpAnswer, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, we did not have the time to explore these new data in the next step."},{"metadata":{},"cell_type":"markdown","source":"## e) Knowledge adding\nThe questions could be grouped by domain. We identified 10 specific domains in the proposed dataset:\n* **time**: the time taken to fill up the survey (Time from Start to Finish (seconds)),\n* **person**: short description of the user (Q1, Q2, Q3, Q4, Q5),\n* **programming**: his programming knowledge (Q6, Q7, Q8, Q9),\n* **notebook**: his usage of programming notebook (Q10, Q11, Q12, Q13, Q14),\n* **machine learning**: his machine learning environment (Q15, Q16, Q17, Q18, Q19, Q25, Q28, Q33, Q34, Q35, Q36),\n* **work**: his professional environment (Q20, Q21, Q22, Q23, Q24), \n* **cloud**: his cloud computing usage (Q26, Q27),\n* **databases**: his practice in database (Q29, Q30),\n* **business intelligence**: his business intelligence usage (Q31, Q32),\n* **course**: his educational background and future learning (Q37, Q38, Q39).\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Definition of question category \nmyCategory = {'time': ['Time from Start to Finish (seconds)'],\n              'individual': ['Q1', 'Q2', 'Q3', 'Q4', 'Q5'],\n              'programming': ['Q6','Q7', 'Q8', 'Q9'],\n              'notebook': ['Q10', 'Q11', 'Q12', 'Q13', 'Q14'],\n              'ml': ['Q15', 'Q16', 'Q17', 'Q18', 'Q19', 'Q25', \n                     'Q28_A', 'Q28_B', 'Q33_A', 'Q33_B', 'Q34_A', 'Q34_B', 'Q35_A', 'Q35_B', 'Q36'],\n              'professional': ['Q20', 'Q21', 'Q22', 'Q23', 'Q24'], \n              'cloud': ['Q26_A', 'Q26_B', 'Q27_A', 'Q27_B'],\n              'db': ['Q29_A', 'Q29_B', 'Q30'],\n              'businessintelligence': ['Q31_A', 'Q31_B', 'Q32'],\n              'course': ['Q37', 'Q38', 'Q39']\n             }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Explicitly delete the unsused variables\ndel tmpNb, tmpQuest, tmpData, tmpList, tmpCol\n# Explicit call to the garbage collector\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 - Data analysis\n\nAfter cleaning up our data, now it is the time to analyze them with some data visualization [[Jacques2019]](#Jacques2019) to better understand their meaning.\n\n\n\n457 / 5000\nRésultats de traduction\nIt is quite simply a process of putting into perspective information that is apparently complex or embedded in a large amount of parameters by representing it in graphical form.\nThere are different methods of obtaining this information. Some use sites that offer quick “drag and drop” viewing services. We will favor approaches by code using Python and its libraries."},{"metadata":{},"cell_type":"markdown","source":"## a) Visualization of univariate analysis\nDescriptive analysis (or univariate analysis) provides an understanding of the characteristics of each attribute of the dataset. "},{"metadata":{},"cell_type":"markdown","source":"### i. What is the gender of participants ?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Draw the gender of the participants\n# Selection of the used data\ntmpData = df_data['Q2'].value_counts() # count the number of participants by age\ntmpData = tmpData.reset_index() # rebuild the index \n# Draw the figure\n#tmp = df['Q2'][1:].value_counts().reset_index()\nfig = go.Figure(data=[go.Pie(\n    labels=tmpData['index'], \n    values=tmpData['Q2'],\n    marker_colors=px.colors.qualitative.Prism\n)])\nfig.update_layout(title_text='The gender of the participants', \n                 title_x=0.5, title_y=0.85)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As in the computer science world, the gender parity has not yet arrived in the world of data science. The women remain in  minority with 19.4% of our cohort."},{"metadata":{},"cell_type":"markdown","source":"### ii. What is the age of participants ?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Draw the age of the participants\n# Selection of the used data\ntmpData = df_data['Q1'].value_counts() # count the number of participants by age\ntmpData = tmpData.sort_index() # order by the \"label\"\ntmpData = tmpData.reset_index() # rebuild the index \n# Drawing of the figure\nfig = go.Figure(data=[go.Bar(\n    x=tmpData['index'],\n    y=tmpData['Q1']\n)])\nfig.update_layout(title='The age of the participants',\n                  title_x=0.5,\n                  title_y=0.9,\n                  xaxis=dict(title='Age group'),\n                  yaxis=dict(title='Count'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data science is an emerging technolgy. It is obvious to have many young people in our cohort."},{"metadata":{},"cell_type":"markdown","source":"### iii. From which country come the participants ?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Draw an hemisphere with the number of participations by country\n# Selection of the used data\ntmpData = pd.DataFrame({'country':df_data.Q3.value_counts().index, \n                        'count':df_data.Q3.value_counts().values})\n# Drawing of the hemisphere\ndata = dict (\n    type='choropleth',\n    locations=tmpData['country'], \n    locationmode='country names',\n    colorscale='portland',\n    colorbar={'len':.6},\n    z=tmpData['count'])\nlayout = dict(\n    title = \"Repartition of the country participants\",\n    title_x = 0.425,\n    title_y = 0.92,\n    margin={\"l\":0, \"r\":0, \"t\":0, \"b\":0}\n)\nfig = go.Figure(data=[data], layout=layout)\nfig.update_geos(projection_type=\"kavrayskiy7\") \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The participants come from all over the world with a great population from India and USA. Very few participants live in the African continent."},{"metadata":{},"cell_type":"markdown","source":"## b) Visualization of bivariate analysis\n\n\nBivariate analysis examines the relationship between two attributes and determines whether the two are correlated. This analysis could be done from two perspectives: qualitative or quantitative analysis."},{"metadata":{},"cell_type":"markdown","source":"### i. Relationship between age and job "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Draw the age of the participants\n# Selection of the used data\ntmpData = df_data.loc[:,['Q1','Q5']] # age and job\ntmpIndex = df_data.index[tmpData.isnull().any(axis=1)]\n# Drop lines with Nan value\ntmpData.drop(tmpIndex, 0, inplace=True)\n#print(tmpData)\n\n# Create the two-way table between two variables.\ntmpData2 = pd.crosstab(tmpData['Q1'], tmpData['Q5'])\ntmpData2 = tmpData2.transpose()\n# Draw the heatmap\nsns.heatmap(tmpData2, \n            annot=True, fmt=\"d\", annot_kws={\"size\": 7},\n            linewidths=.5, cmap='Blues') \n#plt.title(\"The job by age\")\nplt.xticks(rotation=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This figure shows that many students spend time with Kaggle. The fact that in a notebook is fundamental to have a step-by-step approach of a problem and to visualize most of these steps, it is a good way to learn to deal with data. The second people who use Kaggle are data scientists for which a notebook is a good form to analyse and present their results. "},{"metadata":{},"cell_type":"markdown","source":"### ii. Relationship between age and programmation experience"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Draw the age of the participants\n# Selection of the used data\ntmpData = df_data.loc[:,['Q1','Q6']] # age and job\ntmpIndex = df_data.index[tmpData.isnull().any(axis=1)]\n# Drop lines with Nan value\ntmpData.drop(tmpIndex, 0, inplace=True)\n\n# Create the two-way table between two variables.\ntmpData2 = pd.crosstab(tmpData['Q1'], tmpData['Q6'])\ntmpData2 = tmpData2.transpose()\n# Well-order the index\ntmpDico = {'I have never written code':6,'< 1 years':5,'1-2 years':4,\n          '3-5 years':3, '5-10 years':2, '10-20 years':1, '20+ years':0}\ntmpData2 = pd.DataFrame(tmpData2, index=sorted(tmpDico, key=tmpDico.get))\n\n# Draw the heatmap\nsns.heatmap(tmpData2, \n            annot=True, fmt=\"d\", annot_kws={\"size\": 7},\n            linewidths=.5, cmap='Blues') \nplt.title(\"Programmation experience by age\")\nplt.xticks(rotation=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As most of our population is young students, it is obvious to find 1-5 years of coding experience in our population."},{"metadata":{},"cell_type":"markdown","source":"### iii. Relationship between age and programming language used"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# -- Draw the age of the participants\n# Selection of the question 7\ntmpCol = [col for col in df_data.columns if col.startswith('Q7_Part')]\n# Browse all value available for age\ntmpData = pd.DataFrame()\nfor i in df_data['Q1'].value_counts().index.to_list():\n    tmpData2 = df_data[df_data['Q1'] == str(i)]\n    tmpData2 = tmpData2[tmpCol].apply(pd.Series.value_counts)\n    tmpData2.columns = colName.loc['refValue', tmpCol]  \n    tmpData2.sort_index(axis=1, inplace=True)\n    tmpData2.reset_index(drop=True, inplace=True)\n    tmpData2.rename(index={1:i}, inplace=True)\n    tmpData2 = tmpData2.loc[i]\n    tmpData = tmpData.append(tmpData2, ignore_index=False) \n\n# Create the two-way table between two variables.\ntmpData = tmpData.fillna(0)\ntmpData = tmpData.astype(int)\ntmpData = tmpData.sort_index(axis=0) # order by the \"label\"\ntmpData = tmpData.transpose()\n\n# Draw the heatmap\nsns.heatmap(tmpData, \n            annot=True, fmt=\"d\", annot_kws={\"size\": 7},\n            linewidths=.5, cmap='Blues') \nplt.title(\"Programming languages used by age\")\nplt.xticks(rotation=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure shows us that most of the participants use Python and the most unpopular languages are Julia and Swift. This information is not surprising, because the survey was filled up by the Kaggle community and the Kaggle community programs in Python or R."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nAn essential skill for any data scientist is knowing how to present results in the right format. The creation of a notebook is a good way to deal with this problem. It forces the analyst to move forward step by step and to go further to find the best and well-understanding representation.\n\nThanks to this challenge, I manage to discover a new way to explore a new dataset. I am used to analyse data with statistics and machine learning approachs defined in C++, Java and R and to visualise in the last step of my work the results. Jupyter notebook offers a new approach in which I discover the importance to show the usual black box and to explain it. I still have a lot of features to explore in Jupyter and can't wait to do it with my own datasets in order to compare the performance with my tradition algorithms in R or Java."},{"metadata":{},"cell_type":"markdown","source":"#### Webography\n\n<a id=\"matplotlib\">[matplotlib]</a> Mathplotlib library : https://matplotlib.org/<br>\n<a id=\"missingno\">[missingno]</a> MissingNo library : https://pypi.org/project/missingno/0.4.2/ <br>\n<a id=\"numpy\">[numpy]</a> NumpPy library : https://numpy.org/ <br>\n<a id=\"pandas\">[pandas]</a> Pandas library : https://pandas.pydata.org/ <br>\n<a id=\"plotly\">[plotly]</a> Plotly library : https://plotly.com/python/ <br>\n<a id=\"seaborn\">[seaborn]</a> Seaborn library : https://seaborn.pydata.org/ <br>\n\n\n<a id=\"Jacques2019\">[Jacques2019]</a>JACQUES W. (2019) Data visualization en Python avec des librairies telles que Matplotlib et Seaborn. https://medium.com/france-school-of-ai/data-visualization-en-python-avec-des-librairies-telles-que-matplotlib-et-seaborn-6811385df020<br>\n<a id=\"kaggle2020\">[Kaggle2020]</a> Kaggle (2020) 2020 Kaggle Machine Learning and Data Science Survey : https://www.kaggle.com/c/kaggle-survey-2020<br>\n<a id=\"Keita2017\">[Keita2017]</a> KEITA Moussa (2017) Data Science with Python: Algorithm, Statisitcs, DataViz, DataMining and Machine-Learning. MPRA report  : https://mpra.ub.uni-muenchen.de/76653/1/MPRA_paper_76653.pdf<br>\n<a id=\"Kuhn2019\">[Kuhn2019]</a> KUHN Max, JOHNSON Kjell (2019) Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.<br>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}