{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Answering Questions on How to Start Your Data Career Based on Current Trends"},{"metadata":{},"cell_type":"markdown","source":"Stuck on your career? Want some challenge? Or just want to follow hype on current trends?  Try the data industry!\n\n\"But how to go there? I mean I dont even know where to start and what to do...\"\n\nIf that thought comes across your mind, this following research might help you find your answers"},{"metadata":{},"cell_type":"markdown","source":"## 0. Preparation : Data, Module, Utilify Function Loading, and Data Overview "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport itertools\nimport pandas as pd \n\nfrom matplotlib.offsetbox import OffsetImage,AnnotationBbox\nimport seaborn as sns\nsns.set()\nsns.set_style(style='white')\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.graph_objects as go\n\nimport scipy.stats as ss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"question = np.array([x+\"_\" for x in df.columns], np.str)\nsub_question = np.array(list(df.iloc[0].values), np.str)\njoined_question = np.char.add(question,sub_question)\ndf.columns = joined_question\ndf.drop(0, axis=0, inplace=True)\ndf.replace(np.nan, \"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data contains some attributes:\n\nCustomer Profile Related Attribute : \n\n1. Age (Q1) - Categorical\n2. Gender (Q2) - Categorical\n3. Country (Q3) - Categorical\n4. Education (Q4) - Categorical\n\nProfessional Related Attribute\n\n1. Job (Q5) - Categorical\n2. Company Size (Q20) - Categorical\n3. Coding Experience (Q6) - Categorical\n4. Machine Learning Experience (Q15) - Categorical\n5. DS Team in Company (Q21) - Categorical\n6. Incorporation of ML Methods in Company (Q22) - Categorical\n7. Yearly Compensation (Q24) - Categorical\n8. Money spent on ML or Cloud Computing (Q25) - Categorical"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ATTRIBUTE_JOB = 'Q5_Select the title most similar to your current role (or most recent title if retired): - Selected Choice'\nATTRIBUTE_COMP_SIZE = 'Q20_What is the size of the company where you are employed?'\nATTRIBUTE_CODING_EXP = 'Q6_For how many years have you been writing code and/or programming?'\nATTRIBUTE_ML_EXP = 'Q15_For how many years have you used machine learning methods?'\nATTRIBUTE_DS_TEAMSIZE = 'Q21_Approximately how many individuals are responsible for data science workloads at your place of business?'\nATTRIBUTE_ML_USAGE_COMPANNY = 'Q22_Does your current employer incorporate machine learning methods into their business?'\nATTRIBUTE_SALARY = 'Q24_What is your current yearly compensation (approximate $USD)?'\nATTRIBUTE_MONEY_CLOUDML = 'Q25_Approximately how much money have you (or your team) spent on machine learning and/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?'\nALL_ATTRIBUTE = [ATTRIBUTE_JOB,ATTRIBUTE_COMP_SIZE,\n                 ATTRIBUTE_CODING_EXP,ATTRIBUTE_ML_EXP,ATTRIBUTE_DS_TEAMSIZE,ATTRIBUTE_ML_USAGE_COMPANNY,ATTRIBUTE_SALARY,ATTRIBUTE_MONEY_CLOUDML]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"COURSE_COLOR ={\"Coursera\" : \"#2a73cc\", \"Kaggle\":\"#88ddff\", \"Udemy\":\"#ea5656\", \"University\":\"#074b5c\"}\nMEDIA_COLOR = {\"Kaggle\":\"#88ddff\", \"YouTube\":\"#ff0000\", \"Blogs\":\"#355876\"}\nVIZ_FRAMEWORK_COLOR ={\"Matplotlib\" : \"#ffde71\", \"Seaborn\" : \"#7db0bc\", \"None\" : \"grey\", \"Plotly\" : \"#3d4d71\"}\nML_FRAMEWORK_COLOR = {\"Sklearn\" : \"#3294c7\", \"TF\" : \"#f09437\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"DICTIONARY_CHANGE_VALUE = {\n    ATTRIBUTE_ML_EXP : {\n        \"I do not use machine learning methods\" : \"No\"\n    },\n    ATTRIBUTE_ML_USAGE_COMPANNY : {\n        \"We are exploring ML methods (and may one day put a model into production)\" : \"Exploration and Insights\",\n        \"We use ML methods for generating insights (but do not put working models into production)\" : \"Exploration and Insights\",\n        \"No (we do not use ML methods)\" : \"Not using\",\n        \"We have well established ML methods (i.e., models in production for more than 2 years)\" : \"Production > 2 years\",\n        \"We recently started using ML methods (i.e., models in production for less than 2 years)\" : \"Production < 2 years\",\n        \"I do not know\" : \"Unknown\"\n    },\n    ATTRIBUTE_MONEY_CLOUDML : {\n        \"$100,000 or more ($USD)\" : \"More than $100,000 ($USD)\"\n    },\n    ATTRIBUTE_CODING_EXP : {\n        \"I have never written code\" : \"No or < 1 year\",\n        \"< 1 years\" : \"No or < 1 year\"\n    },\n    \"Big Data Technology\" : {\n        \"Microsoft SQL Server\" : \"Ms. SQLServer\",\n        \"Google Cloud BigQuery\" : \"BigQuery\",\n        \"Microsoft Azure Data\" : \"Azure DataLake\",\n        \"Google Cloud Firestore\" : \"Firestore\",\n        \"Oracle Database\" : \"Oracle DB\",\n        \"Microsoft Access\" : \"Ms. Access\",\n        \"Amazon Redshift\" : \"Redshift\",\n        \"Amazon Athena\" : \"Athena\",\n        \"Amazon DynamoDB\" : \"DynamoDB\",\n        \"Google Cloud SQL\" : \"GC SQL\"\n    },\n    \"BI Tools\" : {\n        \"Amazon QuickSight\" : \"QuickSight\",\n        \"Microsoft Power BI\" : \"Power BI\",\n        \"Google Data Studio\" : \"Data Studio\",\n        \"SAP Analytics Cloud\" : \"SAP\",\n        \"TIBCO Spotfire\" : \"Spotfire\"\n    },\n    \"ML Frameworks\" : {\n        \"Scikit-learn\" : \"Sklearn\",\n        \"TensorFlow\" : \"TF\",\n        \"Xgboost\" : \"XGB\",\n        \"LightGBM\" : \"LGBM\",\n        \"Tidymodels\" : \"Tdymdls\"\n        \n    },\n    \"ML Algorithms\" : {\n        \"Decision Trees or Random Forests\" : \"DT/RF\",\n        \"Gradient Boosting Machines (xgboost, lightgbm, etc)\" : \"GBM\",\n        \"Bayesian Approaches\" : \"Bayesian\",\n        \"Dense Neural Networks (MLPs, etc)\" : \"MLP\",\n        \"Convolutional Neural Networks\" : \"CNN\",\n        \"Recurrent Neural Networks\" : \"RNN\",\n        \"Linear or Logistic Regression\" : \"LR\",\n        \"Transformer Networks (BERT, gpt-3, etc)\" : \"Transformer\",\n        \"Evolutionary Approaches\" : \"Evol. Appr\",\n        \"Generative Adversarial Networks\" : \"GAN\"\n    },\n    \"AutoML Workflow\" : {\n        \"Automated data augmentation (e.g. imgaug, albumentations)\" : \"Augmentation\",\n        \"Automated feature engineering/selection (e.g. tpot, boruta_py)\" : \"Feature Engineering\",\n        \"Automated model selection (e.g. auto-sklearn, xcessiv)\" : \"Model Selection\",\n        \"Automated model architecture searches (e.g. darts, enas)\" : \"Architecture Search\",\n        \"Automated hyperparameter tuning (e.g. hyperopt, ray.tune, Vizier)\" : \"Hyperparams Tuning\",\n        \"Automation of full ML pipelines (e.g. Google AutoML, H20 Driverless AI)\" : \"Full Pipeline\"\n    },\n    \"Data Role\" : {\n        \"Analyze and understand data to influence product or business decisions\" : \"Analyzing Business\",\n        \"Do research that advances the state of the art of machine learning\" : \"Doing Research\",\n        \"Build and/or run a machine learning service that operationally improves my product or workflows\" : \"Building Service\",\n        \"Experimentation and iteration to improve existing ML models\" : \"Doing Research\",\n        \"Build prototypes to explore applying machine learning to new areas\" : \"Building Service\",\n        \"Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data\" : \"Building Infras\"\n    },\n    \"Cloud\" : {\n        \"Amazon Web Services\" : \"AWS\",\n        \"Azure\" : \"Azure\",\n        \"Google Cloud Platform\" : \"GCP\",\n        \"Salesforce Cloud\" : \"Salesforce\",\n        \"VMware\" : \"VMware\",\n        \"IBM Cloud /\" : \"IBM\",\n        \"Alibaba\" : \"Alibaba\",\n        \"SAP Cloud\" : \"SAP\"\n    }\n}\nDICT_ZOOM_ICON = {\"Coursera\":0.05,\"Kaggle\":0.07,\"Udemy\":0.07,\"University\":0.0425,\"DataCamp\":0.065,\"edX\":0.08,\"Udacity\":0.035,\"LinkedIn\":0.095,\"Fast.ai\":0.035}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_first_word(x, cnt_word) :\n    x = x.split()[:cnt_word]\n    x = \" \".join(x)\n    return x\nget_first_word = np.vectorize(get_first_word)\n\ndef sort_two_arrays(x,y, reverse=True) :\n    sorted_x_y = list(zip(*sorted(zip(x, y), reverse=True)))\n    return sorted_x_y\n\ndef show_values_on_bars(axs, threshold, unit, bartype):\n    def _show_on_single_plot(ax, bartype):\n        if bartype == \"vertical\" : \n            for idx, p in enumerate(ax.patches):\n                if idx <= threshold : \n                    _x = p.get_x() + p.get_width() / 2\n                    _y = p.get_y() + p.get_height() + 0.55\n                    value = '{:.2f} {}'.format(p.get_height(), unit)\n                    ax.text(_x, _y, value, ha=\"center\")\n        if bartype == \"horizontal\" :\n            for idx, p in enumerate(ax.patches):\n                if idx <= threshold :\n                    _x = p.get_width() + 1.5\n                    _y = p.get_y() + p.get_height() / 2\n                    value = '{:.2f} {}'.format(p.get_width(), unit)\n                    ax.text(_x, _y, value, ha=\"center\")\n                    \n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax, bartype)\n    else:\n        _show_on_single_plot(axs, bartype)\n\ndef barplot(unique, count, title, y_title, x_title, annotation_unit, category_to_show, colors, bartype, title_position=(0,1)) :\n    \n    if bartype == \"vertical\" : \n        ax = sns.barplot(unique, count, palette=colors)\n        ax.set_title(title, fontsize=20, pad=20)\n    elif bartype == \"horizontal\" :\n        ax = sns.barplot(count, unique, palette=colors)\n        ax.set_title(title, fontsize=20, pad=100, position=title_position)\n    ax.set_ylabel(y_title)\n    ax.set_xlabel(x_title)\n    ax.yaxis.labelpad = 20\n    ax.xaxis.labelpad = 20\n    ax.tick_params(axis='both', which='major', pad=15)\n    show_values_on_bars(ax, category_to_show, annotation_unit, bartype)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"top\"].set_visible(False)\n        \n    return ax\n\n\ndef important_attribute(attributes, measured_question, data, num_attribute) :\n    def cramers_corr(x,y):\n\n        \"\"\" calculate Cramers V statistic for categorial-categorial association.\n            uses correction from Bergsma and Wicher, \n            Journal of the Korean Statistical Society 42 (2013): 323-328\n        \"\"\"\n        result=-1\n        x = data[x]\n        y = data[y]\n        if len(x.value_counts())==1 :\n            print(\"First variable is constant\")\n        elif len(y.value_counts())==1:\n            print(\"Second variable is constant\")\n        else:   \n            conf_matrix=pd.crosstab(x, y)\n\n            if conf_matrix.shape[0]==2:\n                correct=False\n            else:\n                correct=True\n\n            chi2 = ss.chi2_contingency(conf_matrix, correction=correct)[0]\n\n            n = sum(conf_matrix.sum())\n            phi2 = chi2/n\n            r,k = conf_matrix.shape\n            phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n            rcorr = r - ((r-1)**2)/(n-1)\n            kcorr = k - ((k-1)**2)/(n-1)\n            result=np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n        return abs(round(result,6))\n    \n    \n    def cramers_corr_total(x,y) :\n        \n        list_cramers = list(map(cramers_corr, itertools.repeat(x,len(y)), measured_vars))\n        average_cramers = sum(list_cramers) / len(list_cramers)\n        return average_cramers\n    \n    total_cramers_overall = []\n    measured_vars = data[[c for c in df.columns if measured_question in c]]\n    \n    for att in attributes :\n        total_cramers_per_attribute = cramers_corr_total(att, measured_vars)\n        total_cramers_overall.append(total_cramers_per_attribute)\n        \n    sorted_idx = sorted(range(len(total_cramers_overall)), key=lambda k: total_cramers_overall[k], reverse=True)[:num_attribute]\n    \n    \n    important_attributes = [attributes[i] for i in sorted_idx]\n    \n    \n    return important_attributes\n\ndef group_least_category(series, group) :\n    \n    unique_val = list(series.value_counts().index)\n    unique_val.remove('')\n    least_category = unique_val[4:]\n    series = series.apply(lambda x : group if x in least_category else x)\n    return series\n\ndef data_for_plotting(data, question, cnt_word, focused_category=None ,focused_value=None):\n    if focused_value is not None : \n        selected_data = data[data[focused_category]==focused_value][[c for c in data.columns if question in c]]\n    else :\n        selected_data = data[[c for c in data.columns if question in c]]\n    all_data = np.concatenate(selected_data.values)\n    all_data = np.unique(all_data, return_counts=True)\n    all_data_unique = all_data[0]\n    all_data_cnt =  all_data[1]\n    nan = np.where(all_data_unique==\"\")[0][0]\n    all_data_unique = np.delete(all_data_unique, nan)\n    if cnt_word != None : \n        all_data_unique = get_first_word(all_data_unique, cnt_word)\n    all_data_cnt = np.delete(all_data_cnt, nan)\n    all_data_cnt = all_data_cnt / selected_data.shape[0] * 100\n    sorted_data = sort_two_arrays(all_data_cnt, all_data_unique)\n    all_data_cnt, all_data_unique = np.asarray(sorted_data[0]), np.asarray(sorted_data[1])\n    \n    return all_data_cnt, all_data_unique\n\ndef get_color_barplot(values, dictionary_color=None) : \n    \n    if dictionary_color == None :\n        color = None\n    else : \n        selected_color = [dictionary_color[values[0]], dictionary_color[values[1]], dictionary_color[values[2]]]\n        other_color = ['grey'] * (values.shape[0] - 3)\n        color = selected_color + other_color\n    return color\n\ndef change_value(series, attribute) :\n    series = series.replace(DICTIONARY_CHANGE_VALUE[attribute])\n    return series\n\ndef barplot_comparison_2_year(width ,input1, unique1, input2, label1, label2, xaxis_label, yaxis_label, title, ind_growth, growth, num_to_show, title_position) :\n    \n    ind = np.arange(start = 0, stop = input1.shape[0]*1.5, step=1.5)\n    # Plotting\n    ax = plt.bar(ind, input1 , width, label=label1)\n    ax = plt.bar(ind + width, input2, width, label=label2)\n    plt.xlabel(xaxis_label)\n    plt.ylabel(yaxis_label)\n    plt.title(title, position=title_position)\n    plt.xticks(ind + width / 2, unique1)\n    plt.title(title, {\"fontsize\":20}, pad=20)\n    plt.ylabel(yaxis_label, labelpad = 20)\n    plt.xlabel(xaxis_label, labelpad = 20)\n    plt.tick_params(axis='both', which='major', pad=15)\n    \n    rects = ax.patches\n    \n    \n    i = 0\n    for rect in rects:\n        if i in ind_growth[:num_to_show] : \n            height = rect.get_height()\n            plt.text(rect.get_x() + rect.get_width() / 2, height + 1, f\"Growth: {round(growth[i],2)}%\",\n                    ha='center', va='bottom')\n        i+=1\n\n    # Finding the best position for legends and putting it\n    plt.legend(loc='best')\n    plt.grid(b=None, axis='y')\n    plt.grid(axis='y', linestyle=\"--\", linewidth=0.5, color='black')\n    \n    return ax\n\ndef make_crosstab(input1, input2, special_case=False) :\n    \n        \n    tmp1 = df[[c for c in df.columns if (input1 in c)]]\n    tmp2 = df[[c for c in df.columns if (input2 in c)]]\n    tmp1_unique = pd.unique(tmp1.values.flatten())\n    if tmp1_unique[0]==\"\" :\n        tmp1_unique = np.delete(tmp1_unique,0)\n    tmp2_unique = pd.unique(tmp2.values.flatten())\n    if tmp2_unique[0]==\"\" :\n        tmp2_unique = np.delete(tmp2_unique,0)\n    dict_tmp = {}\n    list_len = []\n    for i in tmp1_unique :\n        try : \n            if (\"None\" not in i) and (i!=\"\") :\n                if special_case : \n                    grouper = tmp1.iloc[:,0].apply(lambda x : x if x==i else \"\")\n                else : \n                    grouper = tmp1[[c for c in tmp1.columns if (i in c)]].iloc[:,0]\n                list_tmp = []\n                for j in tmp2_unique :\n                    if (\"None\" not in j) and (j!=\"\") : \n                        column = tmp2[[c for c in tmp2.columns if (j in c)]].iloc[:,0]\n                        value = pd.crosstab(grouper, [column]).iloc[1,1]\n                        list_tmp.append(value)\n                dict_tmp[i] = list_tmp\n        except :\n            pass\n    for j in tmp2_unique :\n        if (\"None\" not in j) and (j!=\"\") : \n            column = tmp2[[c for c in tmp2.columns if (j in c)]].iloc[:,0]\n            length = len([c for c in column.tolist() if c!=\"\"])\n            list_len.append(length)\n    crosstab = pd.DataFrame.from_dict(dict_tmp)\n    crosstab[\"number\"] = list_len\n    crosstab.loc[:,:] =  crosstab.loc[:,:].div(crosstab[\"number\"], axis=0) * 100\n    crosstab.drop(columns=\"number\", axis=1, inplace=True)\n    crosstab.index = [c for c in tmp2_unique if (\"None\" not in c) and (c!=\"\")]\n    crosstab = crosstab.round(2)\n    return crosstab\n\ndef render_table(data, dict_change_value, col_width=3.0, row_height=0.625, font_size=12,\n                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, **kwargs):\n    \n    data = data.reset_index().rename(columns={\"index\":\"\"})\n    maximum = data.iloc[:,1:].idxmax() + 1\n    maximum.index = list(range(1,len(data.columns)))\n    list_maximum = list(zip(maximum, maximum.index))\n    data = data.applymap(lambda x : f\"{x} %\" if isinstance(x,str)==False else x)\n    \n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n    table = ax.table(cellText=data.values, cellLoc='center', bbox=bbox, colLabels=[dict_change_value[c.strip()] if c.strip() in dict_change_value else c for c in data.columns], **kwargs)\n    table.auto_set_font_size(False)\n    table.set_fontsize(font_size)\n\n    for k, cell in table._cells.items():\n        cell.set_edgecolor(edge_color)\n        if ((k[0] == 0) and (k[1] > 0)) or ((k[0] > 0) and (k[1]==0)):\n            cell.set_text_props(weight='bold', color='w')\n            cell.set_facecolor(header_color)\n        elif (k[0] == 0) and (k[1] == 0):\n            cell.set_facecolor('white')\n        elif (k[0]!=0) and (k[1] > 0) and (k not in list_maximum):\n            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n        elif (k[0]!=0) and (k[1] > 0) and (k in list_maximum): \n            cell.set_text_props(weight=\"bold\", color=\"white\")\n            cell.set_facecolor(\"green\")   \n    return ax.get_figure(), ax\n\ndef genSankey(df,cat_cols=[],value_cols='',title='Sankey Diagram'):\n    # maximum of 6 value cols -> 6 colors\n    colorPalette = ['#4B8BBE','#306998','#FFE873','#FFD43B','#646464']\n    labelList = []\n    colorNumList = []\n    for catCol in cat_cols:\n        labelListTemp =  list(set(df[catCol].values))\n        colorNumList.append(len(labelListTemp))\n        labelList = labelList + labelListTemp\n        \n    # remove duplicates from labelList\n    labelList = list(dict.fromkeys(labelList))\n    \n    # define colors based on number of levels\n    colorList = []\n    for idx, colorNum in enumerate(colorNumList):\n        colorList = colorList + [colorPalette[idx]]*colorNum\n        \n    # transform df into a source-target pair\n    for i in range(len(cat_cols)-1):\n        if i==0:\n            sourceTargetDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n            sourceTargetDf.columns = ['source','target','count']\n        else:\n            tempDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n            tempDf.columns = ['source','target','count']\n            sourceTargetDf = pd.concat([sourceTargetDf,tempDf])\n        sourceTargetDf = sourceTargetDf.groupby(['source','target']).agg({'count':'sum'}).reset_index()\n        \n    # add index for source-target pair\n    sourceTargetDf['sourceID'] = sourceTargetDf['source'].apply(lambda x: labelList.index(x))\n    sourceTargetDf['targetID'] = sourceTargetDf['target'].apply(lambda x: labelList.index(x))\n    \n    # creating the sankey diagram\n    data = dict(\n        type='sankey',\n        node = dict(\n          pad = 15,\n          thickness = 20,\n          line = dict(\n            color = \"black\",\n            width = 0.5\n          ),\n          label = labelList,\n          color = colorList\n        ),\n        link = dict(\n          source = sourceTargetDf['sourceID'],\n          target = sourceTargetDf['targetID'],\n          value = sourceTargetDf['count']\n        )\n      )\n    \n    layout =  dict(\n        title = title,\n        font = dict(\n          size = 10\n        )\n    )\n       \n    fig = dict(data=[data], layout=layout)\n    return fig\n\ndef get_flag(name, path):\n    path = f\"{path}/{name}.png\"\n    im = plt.imread(path)\n    return im\n\ndef offset_image(coord, name, ax, path):\n    img = get_flag(name, path)\n    if name in DICT_ZOOM_ICON : \n        im = OffsetImage(img, zoom=DICT_ZOOM_ICON[name])\n    else : \n        im = OffsetImage(img, zoom=0.05)\n    im.image.axes = ax\n\n    ab = AnnotationBbox(im, (0, coord), xybox=(-50, -5.), frameon=False,\n                        xycoords='data',  boxcoords=\"offset points\", pad=0)\n    \n    list(ax.get_yticklabels())[coord].set_color('white')\n\n    ax.add_artist(ab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df[[c for c in df.columns if 'Q23_Part' in c]] = df[[c for c in df.columns if 'Q23_Part' in c]].replace(DICTIONARY_CHANGE_VALUE[\"Data Role\"])\n\ndf[\"Q23_Part_3_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build prototypes to explore applying machine learning to new areas\"]=df[[c for c in df.columns if ('Q23_Part_3' in c) or ('Q23_Part_4' in c)]].max(axis=1)\ndf.drop(columns=\"Q23_Part_4_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build and/or run a machine learning service that operationally improves my product or workflows\", axis=1,inplace=True)\n\ndf[\"Q23_Part_5_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Experimentation and iteration to improve existing ML models\"]=df[[c for c in df.columns if ('Q23_Part_5' in c) or ('Q23_Part_6' in c)]].max(axis=1)\ndf.drop(columns=\"Q23_Part_6_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Do research that advances the state of the art of machine learning\", axis=1,inplace=True)\n\ndf = df.rename(columns = {\n    \"Q23_Part_3_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build prototypes to explore applying machine learning to new areas\" : \"Q23_Part_3_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Building Service\",\n    \"Q23_Part_5_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Experimentation and iteration to improve existing ML models\":\"Q23_Part_4_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Doing Research\",\n    \"Q23_Part_1_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyze and understand data to influence product or business decisions\":\"Q23_Part_1_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Analyzing Business\",\n    \"Q23_Part_2_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data\" : \"Q23_Part_2_Select any activities that make up an important part of your role at work: (Select all that apply) - Selected Choice - Building Infras\"\n})\n\ndf[\"Q9_Part_3_Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio / Visual Studio Code\"] = df[[c for c in df.columns if (\"Q9_Part_3\" in c) or (\"Q9_Part_4\" in c)]].max(axis=1)\ndf.drop(columns=\"Q9_Part_4_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice - Click to write Choice 13\",axis=1,inplace=True)\ndf.drop(columns=\"Q9_Part_3_Which of the following integrated development environments (IDE's) do you use on a regular basis?  (Select all that apply) - Selected Choice -  Visual Studio / Visual Studio Code \",axis=1,inplace=True)\ndf = df.rename(columns={\"Q9_Part_3_Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio / Visual Studio Code\":\n                       \"Q9_Part_3_Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio / Visual Studio Code (VSCode)\"})\ndf[\"Q9_Part_3_Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio / Visual Studio Code (VSCode)\"] = df[\"Q9_Part_3_Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio / Visual Studio Code (VSCode)\"].replace({\"Visual Studio\":\"VSCode\",\"Visual Studio Code (VSCode)\":\"VSCode\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Where should I learn?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_course_cnt, all_course_unique = data_for_plotting(df, 'Q37', 1)\ncourse_colors = get_color_barplot(all_course_unique, COURSE_COLOR)\nplt.figure(figsize=(16,10))\nax = barplot(all_course_unique, all_course_cnt, title=\"Source to Study Data Science\", \n                         y_title=\"Source\", x_title=\"Percentage of User (%)\", annotation_unit='%', category_to_show=3, colors=course_colors, bartype=\"horizontal\", title_position=(0,1))\nax.text(-6.5,-2, \"The most favourite sources are:\")\nax.text(2.5,-2, \"Coursera, Kaggle, and Udemy\", bbox={'facecolor': 'green', 'alpha': 0.5, 'pad': 10}, color=\"white\", weight='bold')\nfor i, c in enumerate(all_course_unique):\n    try : \n        offset_image(i, c, ax, \"../input/ds-study-sources-icons\")\n    except :\n        pass\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"36.84% of the respondent choose **Coursera as their most favourite place to learn data science**, followed by Kaggle and Udemy. But we may dig deeper of this information. So what factor effect this choice of learning place the most?"},{"metadata":{"trusted":true},"cell_type":"code","source":"important_attribute(ALL_ATTRIBUTE, 'Q37', df, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By applying the function \"important_attribute\", we can get the factors that affecting choice of learning place the most. That is **years of experience in using machine learning methods**. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"temp_data = df.copy()\ntemp_data[ATTRIBUTE_ML_EXP] = group_least_category(temp_data[ATTRIBUTE_ML_EXP], 'More than 3 years')\ntemp_data[ATTRIBUTE_ML_EXP] = change_value(temp_data[ATTRIBUTE_ML_EXP], ATTRIBUTE_ML_EXP)\nunique_value = list(temp_data[ATTRIBUTE_ML_EXP].unique())\nunique_value.remove('')\nidx=np.array([1,3,0,4,2])\nunique_value = np.array(unique_value)[idx]\ntemp_data = temp_data[temp_data[ATTRIBUTE_ML_EXP]!='']\nplt.figure(figsize=(26,20))\nfor i, value in enumerate(unique_value) : \n    all_course_grouped_cnt, all_course_grouped_unique = data_for_plotting(temp_data, 'Q37', 1, ATTRIBUTE_ML_EXP, value)\n    course_colors = get_color_barplot(all_course_grouped_unique, COURSE_COLOR)\n    i+=1\n    plt.subplot(3,2,i)\n    ax = barplot(all_course_grouped_unique, all_course_grouped_cnt, title=f\"Source to Study Data Science \\n {value} Experiences in Machine Learning\", \n                         y_title=\"Percentage of User (%)\", x_title=\"Source\", annotation_unit='%', category_to_show=3, colors=course_colors, bartype=\"vertical\")\n    plt.subplots_adjust(hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From our data we can clearly see that Coursera still be the best place to learn among all of the choice. But we could see that people with < 2 years of experience in machine learning favor Kaggle more than Udemy and University. \n\nFrom this information we could conclude that people at the early days really use the learning platform and formal education to learn data science. But nowadays Kaggle successfully make it easy to learn data science. This maybe because we really learn **more \"real-case-like\" data science from Kaggle**, but still the **need of studying \"basic concept\" of data science could be fulfilled from Coursera**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_media_cnt, all_media_unique = data_for_plotting(df, 'Q39', 1)\nmedia_colors = get_color_barplot(all_media_unique, MEDIA_COLOR)\nplt.figure(figsize=(16,10))\nax = barplot(all_media_unique,all_media_cnt ,title=\"Media to Get Data Science Latest Topics\", \n                         y_title=\"Media\", x_title=\"Percentage of User (%)\", annotation_unit='%', category_to_show=3, colors=media_colors, bartype=\"horizontal\", title_position=(0.1,1))\nax.text(-5.25,-2, \"The most favourite media are:\")\nax.text(4.5,-2, \"Kaggle, YouTube, and Blogs\", bbox={'facecolor': 'blue', 'alpha': 0.5, 'pad': 10}, color=\"white\", weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_attribute(ALL_ATTRIBUTE, 'Q39', df, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"temp_data = df.copy()\ntemp_data[ATTRIBUTE_ML_EXP] = group_least_category(temp_data[ATTRIBUTE_ML_EXP], 'More than 3 years')\ntemp_data[ATTRIBUTE_ML_EXP] = change_value(temp_data[ATTRIBUTE_ML_EXP], ATTRIBUTE_ML_EXP)\nunique_value = list(temp_data[ATTRIBUTE_ML_EXP].unique())\nunique_value.remove('')\nidx=np.array([1,3,0,4,2])\nunique_value = np.array(unique_value)[idx]\ntemp_data = temp_data[temp_data[ATTRIBUTE_ML_EXP]!='']\nplt.figure(figsize=(26,20))\nfor i, value in enumerate(unique_value) :\n    all_media_grouped_cnt, all_media_grouped_unique = data_for_plotting(temp_data, 'Q39', 1, ATTRIBUTE_ML_EXP, value)\n    course_colors = get_color_barplot(all_media_grouped_unique, MEDIA_COLOR)\n    i+=1\n    plt.subplot(3,2,i)\n    ax = barplot(all_media_grouped_unique, all_media_grouped_cnt, title=f\"Media to Get Data Science Latest Topics \\n {value} Experiences in Machine Learning\", \n                         y_title=\"Percentage of User (%)\", x_title=\"Source\", annotation_unit='%', category_to_show=3, colors=course_colors, bartype=\"vertical\")\n    plt.subplots_adjust(hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The trend of \"need for practical case with the easiest way\" continues in the context of choosing media to get data science latest topics. People with less experience tends to choose Youtube because it might be easier to consume. We could get the information only by sitting down watching other people explain it to us.\n\nBut people with more experience tends to choose Kaggle because we could directly read many tricks and latest code implementation for various competition so it could be more practical. This trend stops for people with more than 3 years of experience in machine learning because they tend to choose Blogs (ex: Towards Data Science). Personally I think this because people with more than 3 years of experience could implement the idea posted in the blog post without reading full code implementation like in Kaggle. They just need the overview of the idea, simple code implementation (without all of the data processing etc), and voila! They could make cool stuffs out of those things."},{"metadata":{},"cell_type":"markdown","source":"## 2. I have already taken some courses, read books, and subscribing articles, but there are bunch of tools out there! Which one should I pick?"},{"metadata":{},"cell_type":"markdown","source":"Oke now we will get deeper into Data Science workflow.\nGenerally there are 4 processes : ETL, Exploration, Preprocessing and Modelling, and Deployment.\n\nWhich tool is needed for each of the processe?"},{"metadata":{},"cell_type":"markdown","source":"### We begin with Big Data Tools (ETL tools)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_etl_cnt, all_etl_unique = data_for_plotting(df, 'Q29_A', 3)\nall_etl2_cnt, all_etl2_unique = data_for_plotting(df, 'Q29_B', 3)\nindices = list(np.where(all_etl_unique[:, None] == all_etl2_unique[None, :])[1])\nall_etl2_cnt = all_etl2_cnt[indices]\nall_etl_unique = np.array([DICTIONARY_CHANGE_VALUE[\"Big Data Technology\"][x] if x in DICTIONARY_CHANGE_VALUE[\"Big Data Technology\"] else x for x in all_etl_unique])\ngrowth = (all_etl2_cnt - all_etl_cnt)/all_etl_cnt * 100\nind_growth = np.argsort(growth)[::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(26,10))\nax = barplot_comparison_2_year(0.6,all_etl_cnt, all_etl_unique, all_etl2_cnt, \"Now\", \"Next 2 Years\", \"Tech Stack\", \"Percentage of User (%)\", \"Commonly Used Big Data Technologies\", ind_growth, growth, 3, (0.083,1.1))\nplt.text(-3.2,23,\"Most commonly used Big Data Technologies in the future (prediction) are:\")\nplt.text(6.3,23,\"Google Cloud SQL, Google Firestore (Firebase), and Google BigQuery\", bbox={'facecolor': '#00A5FF', 'pad': 10}, color=\"white\", weight='bold')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could see that **MySQL and PostgreSQL still be the champion in the market**. But what about 2 years from now? **Google Cloud SQL, Google Firestore (Firebase), and Google BigQuery are predicted to have the steepest growth of user**. All Google Technologies!"},{"metadata":{},"cell_type":"markdown","source":"### After getting the data, We continue doing Exploratory Data Analysis. Here are the tools."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_bi_cnt, all_bi_unique = data_for_plotting(df, 'Q31_A', 3)\nall_bi2_cnt, all_bi2_unique = data_for_plotting(df, 'Q31_B', 3)\nindices = list(np.where(all_bi_unique[:, None] == all_bi2_unique[None, :])[1])\nall_bi2_cnt = all_bi2_cnt[indices]\nall_bi_unique = np.array([DICTIONARY_CHANGE_VALUE[\"BI Tools\"][x] if x in DICTIONARY_CHANGE_VALUE[\"BI Tools\"] else x for x in all_bi_unique])\ngrowth = (all_bi2_cnt - all_bi_cnt)/all_bi_cnt * 100\nind_growth = np.argsort(growth)[::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(26,10))\nax = barplot_comparison_2_year(0.6,all_bi_cnt, all_bi_unique, all_bi2_cnt, \"Now\", \"Next 2 Years\", \"Tech Stack\", \"Percentage of User (%)\", \"Commonly Used Business Intelligence Tools\", ind_growth, growth, 4, (0.089,1.1))\nplt.text(-2.9,21,\"Most commonly used BI Tools in the future (prediction) are:\")\nplt.text(3.5,21,\"Einstein Analytics, SAP, Amazon QuickSight, Google Data Studio\", bbox={'facecolor': '#FF8C00', 'pad': 10}, color=\"white\", weight='bold')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the data exploration by BI tools, **Tableau and Power BI still are the champion in the market**. But what about 2 years from now? You better also be prepared for the era of **SAP, Amazon Quicksight, Einstein Analytics**, and.... of course our strongest contender, **Google Data Studio**!"},{"metadata":{},"cell_type":"markdown","source":"### Already gain the insights from data? Go to preprocessing and modelling part!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"crosstab = make_crosstab('Q17','Q16')\nfig,ax = render_table(crosstab, DICTIONARY_CHANGE_VALUE[\"ML Algorithms\"], header_columns=0, col_width=2.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table above shows you how many percent of a certain workflow user, also use a certain algorithm in the daily basis. I hope that this could be a proxy for measuring how comfortable is using a certain workflow/library for an algorithm. For example, 89,56% Caret user use Decision Tree / Random Forest in the daily basis. \n\nOf course this method is not perfect. This doesnt mean that Tensorflow could also be used for making Decision Tree / Random Forest algorithm. Also there is another drawback. Some of the library/workflow are for R language while the others are for Python language make they are not really apple to apple to compare.\n\nDespite all of the drawbacks, from this method we could see that **JAX from Google secure the highest percentage in almost all of ML Algorithms**. Still, Google Technologies are the best here."},{"metadata":{},"cell_type":"markdown","source":"### How about automated machine learning workflow? I've heard so many convenient tools out there."},{"metadata":{},"cell_type":"markdown","source":"#### First let see what part of machine learning workflows people would like to automate"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmp = df[[c for c in df.columns if \"Q33_A\" in c]].iloc[:,:5].replace(DICTIONARY_CHANGE_VALUE[\"AutoML Workflow\"])\ntmp.columns = [\"Augmentation\",\"Feature Engineering\",\"Model Selection\",\"Architecture Search\",\"Hyperparams Tuning\"]\nall_user = tmp[(tmp[\"Augmentation\"]!=\"\")|(tmp[\"Feature Engineering\"]!=\"\")|(tmp[\"Model Selection\"]!=\"\")|(tmp[\"Architecture Search\"]!=\"\")|(tmp[\"Hyperparams Tuning\"]!=\"\")].shape[0]\ntmp = pd.DataFrame(tmp.groupby(list(tmp.columns)).size()).reset_index().rename(columns={0:\"Percentage\"}).iloc[1:,:]\ntmp[\"Percentage\"] = round(tmp[\"Percentage\"] / all_user * 100,2)\nfor c in tmp.columns :\n    tmp[c] = tmp[c].apply(lambda x : f\"No {c}\" if x==\"\" else x)\nsorted_tmp = tmp.sort_values(by=\"Percentage\",ascending=False)\n\nsize = (np.array(sorted_tmp.shape[::-1]) + np.array([0, 1])) * np.array([3.5, 0.1])\nfig, ax = plt.subplots(figsize=size)\nax.axis('off')\ntmp_edited = sorted_tmp.iloc[:6,:]\ntmp_edited.loc[:,\"Percentage\"] = tmp_edited.loc[:,\"Percentage\"].apply(lambda x : f\"{x}%\")\ntable = ax.table(cellText=tmp_edited.values, cellLoc='center', bbox=[0, 0, 1, 1], colLabels=sorted_tmp.columns)\ntable.auto_set_font_size(False)\ntable.set_fontsize(14)\n\nfor k, cell in table._cells.items():\n    if k[0]==0 :\n        cell.set_text_props(weight='bold', color='w')\n        cell.set_facecolor(\"#FF4500\")\n    s = table.get_celld()[k].get_text()\n    if \"No\" in s._text :\n        s.set_color(\"red\")\n    elif (\"No\" not in s._text) and (\"%\" not in s._text) and (k[0]!=0):\n        s.set_color(\"green\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is it clear now? Most of the automatic pipeline user only use Model Selection process. So the most important thing for us in the context of automated ML workflow is Model Selection part."},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = genSankey(tmp,cat_cols=['Augmentation','Feature Engineering','Model Selection','Architecture Search',\"Hyperparams Tuning\"],value_cols='Percentage',title='AutoML')\n\n# fig = go.Figure(data=[go.Sankey(\n#     valueformat = \".0f\",\n#     # Define nodes\n#     node = dict(\n#       pad = 15,\n#       thickness = 15,\n#       line = dict(color = \"black\", width = 0.5),\n#       label =  data['data'][0]['node']['label'],\n#       color =  data['data'][0]['node']['color']\n#     ),\n#     # Add links\n#     link = dict(\n#       source =  data['data'][0]['link']['source'],\n#       target =  data['data'][0]['link']['target'],\n#       value =  data['data'][0]['link']['value'],\n#       label =  [\"\"]*len(data['data'][0]['link']['source']),\n#       color =  [\"yellow\",\"yellow\",\"green\",\"green\",\"blue\",\"blue\",\"red\",\"red\",\"green\",\"green\",\"red\",\"red\",\"green\",\"green\",\"yellow\",\"yellow\",\"grey\"]+[\"grey\"]*(len(data['data'][0]['link']['source'])-17)\n# ))])\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"crosstab = make_crosstab('Q33_A','Q34_A')\nfig,ax = render_table(crosstab, DICTIONARY_CHANGE_VALUE[\"AutoML Workflow\"], header_columns=0, col_width=3.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And afterall, **Tpot and Excessiv wins the race of Automatic ML Workflow**. But dont forget that the most important part, **Model Selection, is led by Auto-sklearn**."},{"metadata":{},"cell_type":"markdown","source":"## 3. By the way, there are so much buzz-word related to cloud computing out there. Do I also need to learn them all?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"crosstab = make_crosstab('Q23','Q26_A')\nfig,ax = render_table(crosstab, DICTIONARY_CHANGE_VALUE[\"Data Role\"], header_columns=0, col_width=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Analyzing Business role and Doing Research doesnt give me surprise with the result. Salesforce is a software focused on Customer Relationship Manager and AWS give you simple way of doing ML research. But Tencent is might be the best new player that I have ever heard and will rise soon!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_cloud_cnt, all_cloud_unique = data_for_plotting(df, 'Q26_A', 3)\nall_cloud2_cnt, all_cloud2_unique = data_for_plotting(df, 'Q26_B', 3)\nindices = list(np.where(all_cloud_unique[:, None] == all_cloud2_unique[None, :])[1])\nall_cloud2_cnt = all_cloud2_cnt[indices]\nall_cloud_unique = np.array([DICTIONARY_CHANGE_VALUE[\"Cloud\"][x] if x in DICTIONARY_CHANGE_VALUE[\"Cloud\"] else x for x in all_cloud_unique])\ngrowth = (all_cloud2_cnt - all_cloud_cnt)/all_cloud_cnt * 100\nind_growth = np.argsort(growth)[::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(26,10))\nax = barplot_comparison_2_year(0.6,all_cloud_cnt, all_cloud_unique, all_cloud2_cnt, \"Now\", \"Next 2 Years\", \"Tech Stack\", \"Percentage of User (%)\", \"Commonly Used Cloud Technologies\", ind_growth, growth, 3, (0.083,1.1))\nplt.text(-2,28.75,\"Most commonly used Cloud Technologies in the future (prediction) are:\")\nplt.text(4,28.75,\"Tencent, IBM, and Oracle\", bbox={'facecolor': '#00A5FF', 'pad': 10}, color=\"white\", weight='bold')\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As for the growth, Tencent also predicted to have 300% growth for 2 years a head. We will see..."},{"metadata":{},"cell_type":"markdown","source":"## 4. Ok Cool. Is there anything else I need to know?"},{"metadata":{},"cell_type":"markdown","source":"### Many people debating about IDEs. Here are the most used IDEs"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"all_tool_cnt, all_tool_unique = data_for_plotting(df, 'Q9', 1)\ntool_colors = get_color_barplot(all_tool_unique, None)\nplt.figure(figsize=(16,10))\nax = barplot(all_tool_unique,all_tool_cnt ,title=\"Most Used Text Editor\", \n                         y_title=\"tool\", x_title=\"Percentage of User (%)\", annotation_unit='%', category_to_show=3, colors=tool_colors, bartype=\"horizontal\", title_position=(0,1))\nax.text(-7.1,-1.5, \"The most favourite IDEs are:\")\nax.text(6,-1.5, \"Jupyter, VSCode, and PyCharm\", bbox={'facecolor': 'blue', 'alpha': 0.5, 'pad': 10}, color=\"white\", weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And this is IDEs by the role of the person"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df[ATTRIBUTE_JOB] = df[ATTRIBUTE_JOB].replace({\"DBA/Database Engineer\" : \"Data Engineer\",\"Product/Project Manager\":\"Project Manager\", \"Machine Learning Engineer\" : \"ML Engineer\", \"Business Analyst\":\"Data Analyst\"})\ncrosstab = make_crosstab('Q5','Q9', special_case=True)\nfig,ax = render_table(crosstab, DICTIONARY_CHANGE_VALUE[\"Data Role\"], header_columns=0, col_width=2.95)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}