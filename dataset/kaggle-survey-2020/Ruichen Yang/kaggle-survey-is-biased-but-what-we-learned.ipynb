{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 5000)\npd.set_option('display.width', 1000)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom xgboost import XGBRegressor\nfrom sklearn.inspection import permutation_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# we load 2020 data, and later we will create a dense format data which is easier to aggregate \nk_2020 = pd.read_csv(r'/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv')\nk_2020_adj = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now that I say Kaggle Survey is biased, are you surprised üôÄ? I'm sorry but truly, there is nothing wrong with the survey itself, it's just no survey is flawless. Here are some reasons:\n* The survey sample is the Kaggle user. And the Kaggle users are active learners. They üìô, üìù, ‚å®Ô∏è and they üå±üîúüåøüîúüåª. They may locate in the upper quartile of the population and don't represent the true üìä of interest, data scientists, data engineers, you name it.\n* Say what if we are in the perfect üåé? Nah it's not gonna happen, and you know, not everybody has the time or even willingness to finish a survey with 39 choices, 39 üò®??!! Yes it is.\n* Well, and a lot of them are multiple selections, ok, please select all that apply so that we can get better data üôåüôåüôå! But w8, I don't find my choice then what should I do now?\n* Besides, sometimes people would like to take the survey but they just wanna get the shxt done ASAP. Curious how fast? Like really really fast, like üèÉ, üêÜ, üèéÔ∏è, üöÖ, üõ´ (and it's shown below)."},{"metadata":{},"cell_type":"markdown","source":"## However, the Kaggle Survey has so much information and aspects we can explore! It's truly awesome! Yea I know I said it's biased, but maybe I'm biased too, and at least we can explore the following two points:\n* What Kagglers think important. Believe me or not, it's the truth that what human see and choose is what they think important to them. \n* Kagglers' growth mindset, from üå± to üåª. It's the üîë to data scientists, and the only path to anything and anybody right? "},{"metadata":{},"cell_type":"markdown","source":"## Here are the 39 Survey Questions:\n#### (It took me 28 scrolls to the bottom lol, test yours, and don't forget to read the questions!)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# collect all questions and descriptions into a dictionary\nquestions = dict(zip(k_2020.columns.tolist(), k_2020.iloc[0, :].tolist()))\n\n# better print thess questions as future reference\nfor question, description in questions.items():\n    print('{}  ===>  {} \\n'.format(question, description))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# here we delete the first row, which are the questions itself\nk_2020 = k_2020.iloc[1:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here I list all the functions potentially being used later\ndef change_col_dtype(df, col='', dtype=str):\n    \"\"\"\n    Change the data type of a column\n    \n    Args:\n        df (pd.DataFrame): the dataset\n        col (str): the column name\n        dtype (dtype): the expected data type, e.g. str, float\n    \n    Return:\n        The column with the expected dtype\n    \"\"\"\n    assert col in df.columns.tolist(), col+'is not found in the data...'\n    \n    return df[col].astype(dtype)\n    \ndef to_dense_col(df, q_number='', questions=questions):\n    \"\"\"\n    Convert a multiple-choices question from a sparse format into a dense format. \n    e.g. Select cloud services you regularly use (select all that apply)\n       | GCP | Azure | AWS |            | cloud_service |\n         NaN   Azure   AWS    ====>         Azure|AWS \n         NaN    NaN    AWS                     AWS   \n    \n    Args:\n        df (pd.DataFrame): the dataset\n        q_number (string): the question number. e.g. for the 5th question question='Q5' \n        questions: all the questions in the survey\n        \n    Returns:\n        The dense column\n    \"\"\"\n    cols = []\n    for q in questions.keys():\n        if q_number in q:\n            cols.append(q)\n\n    return df.apply(lambda x: '|'.join([x[col].strip() for col in cols if str(x[col]) != 'nan']), axis=1)\n\ndef get_ratio_of_people(df, col):\n    \"\"\"\n    Get the ratios of people in each category\n    \n    Args:\n        df (pd.DataFrame): the dataset\n        col (string): the column name you want to focus on\n    \n    Returns:\n        Aggregated dataset in pd.Series\n    \n    \"\"\"\n    freq = pd.Series('|'.join(df.loc[~df[col].isin(['', 'None']), col].astype(str).tolist()).split('|')).value_counts().reset_index().rename(columns={'index':col, 0:'freq'})\n    output = freq.sort_values(by=['freq'], ascending=False)\n    output['ratio'] = output['freq']/df.shape[0]\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"###### Q0. Time from Start to Finish (seconds)\nk_2020_adj['t_survey'] = change_col_dtype(k_2020, col='Time from Start to Finish (seconds)', dtype=int)\n\n# Q1. What is your age?\nk_2020_adj['age'] = change_col_dtype(k_2020, col='Q1', dtype='O')\n\n# Q2. What is your gender?\nk_2020_adj['gender'] = change_col_dtype(k_2020, col='Q2', dtype='O')\n\n# Q3. In which country do you currently reside?\nk_2020_adj['country'] = change_col_dtype(k_2020, col='Q3', dtype='O')\n\n# Q4. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\nk_2020_adj['formal_edu'] = change_col_dtype(k_2020, col='Q4', dtype='O')\n\n# Q5. Select the title most similar to your current role (or most recent title if retired)\nk_2020_adj['job_title'] = change_col_dtype(k_2020, col='Q5', dtype='O')\n\n# Q6. For how many years have you been writing code and/or programming?\nk_2020_adj['code_exp'] = change_col_dtype(k_2020, col='Q6', dtype='O')\n\n# Q7. What programming languages do you use on a regular basis? (Select all that apply)\nk_2020_adj['language'] = to_dense_col(k_2020, q_number='Q7')\n\n# Q8. What programming language would you recommend an aspiring data scientist to learn first?\nk_2020_adj['rec_learn'] = change_col_dtype(k_2020, col='Q8', dtype='O')\n\n# Q9. Which of the following integrated development environments (IDE's) do you use on a regular basis? (Select all that apply)\nk_2020_adj['ide'] = to_dense_col(k_2020, q_number='Q9')\n\n# Q10. Which of the following hosted notebook products do you use on a regular basis? (Select all that apply)\nk_2020_adj['notebook'] = to_dense_col(k_2020, q_number='Q10')\n\n# Q11. What type of computing platform do you use most often for your data science projects?\nk_2020_adj['comp_plfm'] = change_col_dtype(k_2020, col='Q11', dtype='O')\n\n# Q12. Which types of specialized hardware do you use on a regular basis? (Select all that apply)\nk_2020_adj['hardware'] = to_dense_col(k_2020, q_number='Q12')\n\n# Q13. Approximately how many times have you used a TPU (tensor processing unit)?\nk_2020_adj['num_use_TPU'] = change_col_dtype(k_2020, col='Q13', dtype='O')\n\n# Q14. What data visualization libraries or tools do you use on a regular basis? (Select all that apply)\nk_2020_adj['lib_visual'] = to_dense_col(k_2020, q_number='Q14')\n\n# Q15. For how many years have you used machine learning methods?\nk_2020_adj['ml_exp'] = change_col_dtype(k_2020, col='Q15', dtype='O')\n\n# Q16. Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply)\nk_2020_adj['ml_framework'] = to_dense_col(k_2020, q_number='Q16')\n\n# Q17. Which of the following ML algorithms do you use on a regular basis? (Select all that apply):\nk_2020_adj['ml_alg'] = to_dense_col(k_2020, q_number='Q17')\n\n# Q18. Which categories of computer vision methods do you use on a regular basis? (Select all that apply)\nk_2020_adj['cv'] = to_dense_col(k_2020, q_number='Q18')\n\n# Q19. Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply)\nk_2020_adj['nlp'] = to_dense_col(k_2020, q_number='Q19')\n\n# Q20. What is the size of the company where you are employed?\nk_2020_adj['employer_size'] = k_2020['Q20']\n\n# Q21. Approximately how many individuals are responsible for data science workloads at your place of business?\nk_2020_adj['bus_ds_size'] = k_2020['Q21']\n\n# Q22. Does your current employer incorporate machine learning methods into their business?\nk_2020_adj['employer_ml_stage'] = k_2020['Q22']\n\n# Q23. Select any activities that make up an important part of your role at work: (Select all that apply)\nk_2020_adj['jd'] = to_dense_col(k_2020, q_number='Q23')\n\n# Q24. What is your current yearly compensation (approximate $USD)?\nk_2020_adj['annual_comp'] = change_col_dtype(k_2020, col='Q24', dtype='O')\n\n# Q25. Approximately how much money have you (or your team) spent on machine learning and/or cloud computing services at home (or at work) in the past 5 years (approximate $USD)?\nk_2020_adj['ml_cost_prev_5_year'] = k_2020['Q25']\n\n# Q26-A. Which of the following cloud computing platforms do you use on a regular basis? (Select all that apply)\nk_2020_adj['cloud_plfm'] = to_dense_col(k_2020, q_number='Q26_A')\n\n# Q27-A. Do you use any of the following cloud computing products on a regular basis? (Select all that apply) (Q27-A were asked given Q26-A)\nk_2020_adj['cloud_compute'] = to_dense_col(k_2020, q_number='Q27_A')\n\n# Q28-A. Do you use any of the following machine learning products on a regular basis? (Select all that apply) (Q28-A were asked given Q26-A)\nk_2020_adj['cloud_ml'] = to_dense_col(k_2020, q_number='Q28_A')\n\n# Q29-A. Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis? (Select all that apply)\nk_2020_adj['big_data'] = to_dense_col(k_2020, q_number='Q29_A')\n\n# Q30. Which of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often?\n#print('Q30 were asked given Q29-A\\n')\nk_2020_adj['big_data_tool'] = change_col_dtype(k_2020, col='Q30', dtype='O')\n\n# Q31-A. Which of the following business intelligence tools do you use on a regular basis? (Select all that apply)\nk_2020_adj['bus_intel'] = to_dense_col(k_2020, q_number='Q31_A')\n\n# Q32. Which of the following business intelligence tools do you use most often? (Q32 were asked given Q31-A)\nk_2020_adj['bus_intel_tool'] = change_col_dtype(k_2020, col='Q32', dtype='O')\n\n# Q33-A. Do you use any automated machine learning tools (or partial AutoML tools) on a regular basis? (Select all that apply)\nk_2020_adj['auto_ml'] = to_dense_col(k_2020, q_number='Q33_A')\n\n# Q34-A. Which of the following automated machine learning tools (or partial AutoML tools) do you use on a regular basis? (Select all that apply)(Q34-A were asked given Q33-A)\nk_2020_adj['auto_ml_tool'] = to_dense_col(k_2020, q_number='Q34_A')\n\n# Q35-A. Do you use any tools to help manage machine learning experiments? (Select all that apply)\nk_2020_adj['ml_exp_mngmt'] = to_dense_col(k_2020, q_number='Q35_A')\n\n# Q36. Where do you publicly share or deploy your data analysis or machine learning applications? (Select all that apply)\nk_2020_adj['share_plfm'] = to_dense_col(k_2020, q_number='Q36')\n\n# Q37. On which platforms have you begun or completed data science courses? (Select all that apply)\nk_2020_adj['ds_course_plfm'] = to_dense_col(k_2020, q_number='Q37')\n\n# Q38. What is the primary tool that you use at work or school to analyze data? (Include text response)\nk_2020_adj['prime_analytical_tool'] = change_col_dtype(k_2020, col='Q38', dtype='O')\n\n# Q39. Who/what are your favorite media sources that report on data science topics? (Select all that apply)\nk_2020_adj['ds_media'] = to_dense_col(k_2020, q_number='Q39')\n\n# Q26-B.Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'cloud_plfm'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q26_B')\n\n# Q27-B. In the next 2 years, do you hope to become more familiar with any of these specific cloud computing products? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'cloud_compute'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q27_B')\n\n# Q28-B. In the next 2 years, do you hope to become more familiar with any of these specific machine learning products? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'cloud_ml'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q28_B')\n\n# Q29-B. Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you hope to become more familiar with in the next 2 years? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'big_data'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q29_B')\n\n# Q31-B. Which of the following business intelligence tools do you hope to become more familiar with in the next 2 years? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'bus_intel'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q31_B')\n\n# Q33-B. Which categories of automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'auto_ml'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q33_B')\n\n# Q34-B. Which specific automated machine learning tools (or partial AutoML tools) do you hope to become more familiar with in the next 2 years? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'auto_ml_tool'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q34_B')\n\n# Q35-B. In the next 2 years, do you hope to become more familiar with any of these tools for managing ML experiments? (Select all that apply)\nk_2020_adj.loc[lambda x: (x['ml_cost_prev_5_year'].isnull()) | (x['ml_cost_prev_5_year'] == '$0 ($USD)'), 'ml_exp_mngmt'] = to_dense_col(k_2020.loc[lambda x: (x['Q25'].isnull()) | (x['Q25'] == '$0 ($USD)')], q_number='Q35_B')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Like I promised to show how fast a Kaggler can be to finish a survey, here is the answer: 20 seconds\n* I think they quitted the survey in half becuase 20/39 = 0.51s/question is impossible.\n* Finish time that is too short won't be considered, you know, the time when we choose the answers before thinking, jump around and leave them blank, and treat the survey just as getting a trivial thing done. The üòé feels not right.\n* So here we make an assumption that to make sure the survey validity, the minimal time of reading/answering a question in second is 5, total is expected to be 5 * 39 = 195 seconds"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# time in minutes\n# assume avg. min time in second per question is 5, total is expected to be 5 * 35 = 175 seconds\nk_2020 = k_2020.rename(columns={'Time from Start to Finish (seconds)': 't_survey'})\nk_2020['t_survey'] = k_2020['t_survey'].astype(int)\n\nplt.figure(figsize=(8, 4))\nplt.xlim(0, 400)\n(k_2020.loc[lambda x: x['t_survey'] <= 400]['t_survey']).hist(bins=30)\nplt.title('A Closer Look at Survey Time < 400s', fontsize=15)\nplt.axvline(x=195, ymax=0.565, color='red')\nplt.text(x=200, y=130, s='threshold @195s', color='red', fontsize='medium', fontfamily='fantasy', fontweight='semibold')\n\nk_2020_adj = k_2020_adj.loc[lambda x: x['t_survey'] > 195]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# just make a copy \ndf = k_2020_adj.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check out how the data in the dense format look like:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Now we have {:d} surveys for further analysis.'.format(df.shape[0]))\nprint('Our dense data looks like this:')\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Before we jump into the following analysis, think about these two questions again ü§î:\n* **Things Kagglers think important**\n* **Kagglers' growth mindset**"},{"metadata":{},"cell_type":"markdown","source":"## Last question, if annual compensation is the ‚òùüèºÔ∏è issue, what's more important?\n* personal attributes?\n* skillsets and abilities?\n* career experiences?\n* continuous growths?"},{"metadata":{},"cell_type":"markdown","source":"# Take-aways and Actions üîëüîëüîë\n\n### Growth üöÄüöÄüöÄ\n* Focus on learning new things in the first 4 years of career. Find your job needs, build up and stick to 4-5 skills as your core ü§òüèºü§òüèºü§òüèº to make yourself unique and standout. But don't forget to be open-minded and have some knowledge on other things\n* Engage with the community and interact with others by sharing your work and learning from others, it is all-time important!\n\n### Important Things To Note ‚≠ê‚≠ê‚≠ê\n* Having at least a Master's degree is important for data-related jobs, but it's becoming less important.\n* Know Python üêç, learn üêç, master üêç, and integrate üêç with the other languages you regularly use to build a network üï∏Ô∏è in your brain üß†.\n* Free services like Google Colab and Kaggle Notebook are the must to know, but don't completely ignore pay services like Amazon Sagemaker because pay üí∞ means better experiences and better quality, and it might be closer to the industry needs.\n* Visalization is intended to be used to deliver people's thoughts, which is beyond being fancy. It could be simple or complicate and it could be built using any tool/packages (Tableau|Matplotlib|Seaborn|Ggplot|...). However, it must be easy to understand and interactive ‚ú®. \n\n### When You Consider A Higher Annual Compensation ‚ùó‚ùó‚ùó\n* Career experience is the most important aspect\n    - One needs to know how to drive large business impacts using data and their skills.\n    - One needs to know how to build things in large scale by knowing what different scales of companies or data teams are doing üë∂-üßí-üßëüèº. \n* Skillsets and abilities is the 2nd important aspect\n    - Plotly is strongly recommended! Or any other interactive package also works. And don't forget to know how to put it in production üì±.\n    - Get to know and apply more ides, languages, visualizations, and machine learning frameworks whenever needed.\n* p.s. If you are having a data job in the United States, you've been on the pirate boat ‚öì-üí∞.\n\n\n### Here I collected my findings from the analysis. Help me check it out, tell me what you think and whether they are ‚úîÔ∏è or ‚ùå with your understandings. \n* People age from 18 to 34 act as more active learners. More specifically, once they dive into a new area or career, they grow and learn really fast in the first 5 years.\n* Data Scientists market lowers the bar of higher formal education (Master's or Doctor's) and become more open to Bachelor's degrees. However, Data Scientists with Master's degrees still take up the majority part of the market, which is 51.14% in 2020.\n* The most regularly used language is still Python üêç and it's the one Kagglers think most important, no matter if they use it. People tend to use python with other one or two languages as a language combo. Popular combos are (Python|SQL), (Python|R), (Python|SQL|R), and (Python|C|C++). However, what's interesting is that for analytics (people using one of Python, R, or SQL), some think Python is very important but some think it is not at all.\n* Popular two notebook platforms are Google Colab and Kaggle Notebook (ps. this is Kaggle survey anyway ü§∑üèº). Both the experienced and students like to use them üÜì! As for Amazon Sagemaker, compared with students who almost don't use it at all, a lot more experienced people use it üí∞. \n* Data Scientists' favorite visualization tools are Seaborn | Ggplot | Plotly | Shiny | Bokeh | Leaflet/Folium. These packages have things in common: easthetical, easy-to-use, interactive. Especially for plotly ‚ú®, comparing with other jobs, data scientists have more than 10% of people using it.\n* For machine learning and deep learning techniques in general, on average, Kagglers gain 1 skill/working year and tend to become stable after 4-5 years working experience. For more advanced techniques like Computer Vision or Natural Language Processing, people learn 1-2 skills in the first 5 years and stick with it across the career.\n* Job responsibilities can be categorized into three: analytics, machine learning R&D, and pipeline/architecture. Jobs define which job responsibility you should treat more importantly. However, if you are data scientists, machine learning engineers, or research scientists, you will take multiple responsibilities. \n* The cost on Machine learning and cloud services is positively correlated to the company size üè¶, the data science team size üë•, and the company's stage of machine learning developements üåòüåóüåí. More specifically, compared with other jobs, data scientists have the highest willingness to spend money on these services.\n* Top1 platform to share work is Github. Machine learning engineers, data scientists, and data engineers have higher willingness to share work on platforms while students and the unemployeed don't share work at all. \n* The top3 course platforms are Coursera, Kaggle Learn Course, and Udemy, having 18%, 12.5%, and 12% of users, respectively. Cloud-certification Programs and Fast.ai are the two platforms having least users, which is only 2.5%.\n* The top3 media sources are Kaggle, Youtube, and blogs. Students prefer the more entertaining media type like Youtube while the experienced prefer various of readable and interactive media like Kaggle and blogs."},{"metadata":{},"cell_type":"markdown","source":"## Business Understanding\n* Data science is everywhere and it has been the sexiest job all over the world. However, it is so popular that so many advertisements are disguising what real data scientists think important and how they pursue their growth in the careers. As a consequence, students and aspiring data scientists neither exactly know how should they grow nor what they should expected themselves to be in 1 year, 2 years, and even 5 years in the future. "},{"metadata":{},"cell_type":"markdown","source":"## Data Understanding\n* First we want to understand some personal basic attributes like **age**, **formal education**, and **location**\n* Then we want to understand some work and/or job related attributes like **experience**, **job title**, **languages**, **skills**, **platforms**, and **tools**"},{"metadata":{},"cell_type":"markdown","source":"### Some Kagglers' basic attributes are üéÇ, üéìÔ∏è, üìç"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_age_order(df):\n    \"\"\"Create age ordinal column\"\"\"\n    df['age_order'] = df['age'].replace({'18-21':1, \n                                         '22-24':2, \n                                         '25-29':3, \n                                         '30-34':4, \n                                         '35-39':5, \n                                         '40-44':6, \n                                         '45-49':7, \n                                         '50-54':8, \n                                         '55-59':9, \n                                         '60-69':10, \n                                         '70+':11})\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nage = df['age'].value_counts(normalize=True).reset_index().rename(columns={'index':'age', 'age':'ratio'})\nage = get_age_order(age)\n\nsns.barplot(data=age.sort_values(by=['age_order']), y='age', x='ratio', orient='h')\nplt.title('Ratio of Users - Age', fontsize=15)\nplt.xlabel('ratio')\nplt.ylabel('age')\nprint(\"Most people's ages range from 18 to 34.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_formal_edu_order(df):\n    \"\"\"create formal edu ordinal column\"\"\"\n    df['edu_order'] = df['formal_edu'].replace({'I prefer not to answer':0, \n                                                'No formal education past high school':1,\n                                                'Some college/university study without earning a bachelor‚Äôs degree':2,\n                                                'Professional degree':3,\n                                                'Bachelor‚Äôs degree':4,\n                                                'Master‚Äôs degree':5,\n                                                'Doctoral degree':6})\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"formal_edu = df['formal_edu'].value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 'formal_edu':'ratio'})\nformal_edu = get_formal_edu_order(formal_edu)\n\nplt.figure(figsize=(8, 4))\nsns.barplot(data=formal_edu.sort_values(by=['edu_order']), x='ratio', y='formal_edu', orient='h')\nplt.title('Ratio of Users - Formal Education', fontsize=15)\nplt.xlabel('ratio')\nprint(\"Most of the people hold Bachelor's or Master's as the highest degree, however, Doctoral degree doesn't occupy that much.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"k_2019 = pd.read_csv(r'/kaggle/input/kaggle-survey-2019/multiple_choice_responses.csv')\nk_2018 = pd.read_csv(r'/kaggle/input/kaggle-survey-2018/multipleChoiceResponses.csv')\n\nds_to_edu_2020 = pd.Series(k_2020.loc[lambda x: x['Q5'] == 'Data Scientist']['Q4'].values[1:]).value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 0:'ratio'}).iloc[:3, :]\nds_to_edu_2020.loc[:, 'year'] = 2020\nds_to_edu_2019 = pd.Series(k_2019.loc[lambda x: x['Q5'] == 'Data Scientist']['Q4'].values[1:]).value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 0:'ratio'}).iloc[:3, :]\nds_to_edu_2019.loc[:, 'year'] = 2019\nds_to_edu_2018 = pd.Series(k_2018.loc[lambda x: x['Q6'] == 'Data Scientist']['Q4'].values[1:]).value_counts(normalize=True).reset_index().rename(columns={'index':'formal_edu', 0:'ratio'}).iloc[:3, :]\nds_to_edu_2018.loc[:, 'year'] = 2018\n\nds_to_edu_by_year = pd.concat([ds_to_edu_2018, ds_to_edu_2019, ds_to_edu_2020], axis=0)\nds_to_edu_by_year\n\nplt.figure(figsize=(8, 4))\nsns.pointplot(data=ds_to_edu_by_year, x='year', y='ratio', hue='formal_edu')\nplt.title('Ratio of Data Scientists - Year by Formal Education', fontsize=15)\n\ndiff_mb = ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Master')]['ratio'].mean() - ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Bachelor')]['ratio'].mean()\ndiff_md = ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Master')]['ratio'].mean() - ds_to_edu_by_year.loc[lambda x: x['formal_edu'].str.contains('Doctor')]['ratio'].mean()\nprint(\"* Kagglers holding the Master's degree are still the majority of data scientists, on average {:.2%} higher than Bachelor's degree and {:.2%} higher than Doctor's degree.\".format(diff_mb, diff_md))\nprint(\"* The ratio of data scientists holding Master's degrees or the ratio for Doctor's degrees are dropping.\")\n\nratio_b_2018 = ds_to_edu_by_year.loc[lambda x: (x['year'] == 2018) & (x['formal_edu'].str.contains('Bachelor'))]['ratio'].values[0]\nratio_b_2020 = ds_to_edu_by_year.loc[lambda x: (x['year'] == 2020) & (x['formal_edu'].str.contains('Bachelor'))]['ratio'].values[0]\nprint(\"* The ratio for Bachelor's degree is increasing, 2018 was {:.2%} and 2020 was {:.2%}, which is round {:.2%} increase.\".format(ratio_b_2018, ratio_b_2020, ratio_b_2020 - ratio_b_2018))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nplt.title('Ratio of Users - Country', fontsize=15)\ncountry = df['country'].value_counts(normalize=True).reset_index().rename(columns={'index':'country', 'country':'ratio'})\nsns.barplot(data=country, x='ratio', y='country', orient='h')\nprint('* Kagglers spread everywhere across countries, that is truly amazing!')\nprint('* India has the most people.')\nprint('* United States is the 2nd place, having {:.2%} people from United States, not that much.'.format(df.loc[df['country'] == 'United States of America'].shape[0]/df.shape[0]))\nprint('* While I know in China we have so many people actively engaging Kaggle competitions, some of them are even doing Kaggle consulting, it just located number 9.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - Job Title', fontsize=15)\njt = df['job_title'].value_counts(normalize=True).reset_index().rename(columns={'index':'job_title', 'job_title':'ratio'})\nsns.barplot(data=jt, x='ratio', y='job_title')\nprint('Students take up around 25% of the population.')\nprint('Data Scientists take up around 14% of the population.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_code_exp_order(df):\n    \"\"\"Create code experience ordinal column. new column name is 'exp_order'\"\"\"\n    df['exp_order'] = df['code_exp'].replace({'I have never written code':0,\n                                              '< 1 years':0.5,\n                                              '1-2 years':1.5,\n                                              '3-5 years':2.5,\n                                              '5-10 years':3.5,\n                                              '10-20 years':4.5,\n                                              '20+ years':7.5})\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - Coding Experience', fontsize=15)\ncode = df['code_exp'].value_counts(normalize=True).reset_index().rename(columns={'index':'code_exp', 'code_exp':'ratio'})\ncode = get_code_exp_order(code)\n\nsns.barplot(data=code.sort_values(by=['exp_order']), x='ratio', y='code_exp', orient='h')\nprint('Most people have coding experience, and most of them are between 0 and 5, that golden first 5 years!')\nprint('Since Kaggle users are the portion who are willing to take action and being positive on learning, according to the data, people with 0-5 years coding experience tend to be more active.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The language that Kagglers think important is still üêç, and it's not just because people use it.\n* People using python think python is more important, than people don't use python. but the consistency is no matter if using python, Python, R, and SQL are top 3 recommended language to learn, which means we treat these three important.\n* People don't use top3 languages at all still recommend Python, and then comes C++, while R is 5th and SQL is 7th"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"language = pd.Series('|'.join(df['language']).split('|')).value_counts().reset_index().rename(columns={'index':'language', 0:'num_people'})\nlanguage = language.loc[lambda x: x['language'] != '']\nlanguage['ratio'] = language['num_people'] / df['language'].shape[0]\n\nplt.figure(figsize=(10, 5))\nax = sns.barplot(data=language, x='ratio', y='language')\nax.set_title('Ratio of Users - Language', fontsize=15)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_num_X(df, col='', new_col='', dropna=False):\n    \"\"\"\n    Count the number of choices selected for a certain column. \n    e.g. if a person uses Python and SQL, the number of languages is 2\n    \n    Args:\n        df (pd.DataFrame): the dataset\n        col (string): the column name \n        new_col (string): the new column name\n        dropna (bool): if drop null values\n    Returns:\n        df\n    \"\"\"\n    df[new_col] = df[col].apply(lambda x: len(x.split('|')))\n    if dropna:\n        df.loc[df[col].isin(['', 'None', None]), new_col] = 0\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"lang_combo = df['language'].value_counts().reset_index().rename(columns={'index':'lang_combo', 'language':'num_people'})\nlang_combo = lang_combo.loc[lambda x: x['lang_combo'] != '']\nlang_combo['ratio'] = lang_combo['num_people'] / df['language'].shape[0]\nlang_combo = get_num_X(lang_combo, 'lang_combo', 'num_lang')\n\nplt.figure(figsize=(8, 4))\nplt.title('Number of Users - Number of Languages', fontsize=15)\nsns.pointplot(data=lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index(), x='num_lang', y='num_people')\nprint('* There are {:d} out of {:d} users solely using one language.'.format(lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index().loc[lambda x: x['num_lang'] == 1, 'num_people'].values[0], df['language'].shape[0]))\nprint('* There are {:d} out of {:d} users using two languages, which is the highest number'.format(lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index().loc[lambda x: x['num_lang'] == 2, 'num_people'].values[0], df['language'].shape[0]))\nprint('* There are {:d} out of {:d} users using three languages, after which the number of users drastically drops as the number of language increases'.format(lang_combo.groupby(['num_lang'])['num_people'].sum().reset_index().loc[lambda x: x['num_lang'] == 3, 'num_people'].values[0], df['language'].shape[0]))\n\nprint(\"\\nSince most people are using Python as the major language plus a lot of them are using one or two more languages as a combination, let's see what are the top 3 combos for pythoners.\")\n# top combo 2\nprint('* The top3 language combo-2:')\nfor i, val in enumerate(lang_combo.loc[lambda x: x['lang_combo'].str.contains('Python')].loc[lambda x: x['num_lang'] == 2].iloc[:3]['lang_combo'].values):\n    print('   {:d}. {:s}'.format(i + 1, val))\n\n# top combo 3\nprint('* The top3 language combo-3:')\nfor i, val in enumerate(lang_combo.loc[lambda x: x['lang_combo'].str.contains('Python')].loc[lambda x: x['num_lang'] == 3].iloc[:3]['lang_combo'].values):\n    print('   {:d}. {:s}'.format(i + 1, val))\nprint('It does show clear ds tracks: Analyst/Scientist, Developer, and Engineer? And needless to say, Python+SQL is the golden combo.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rec_py = pd.DataFrame([['Python', 0], ['no Python but SQL or R', 0], ['none of Python SQL or R', 0]], columns=['lang', 'ratio'])\nrec_py.loc[lambda x: x['lang'] == 'Python', 'ratio'] = df.loc[df['language'].str.contains('Python')]['rec_learn'].value_counts(normalize=True).reset_index().loc[lambda x: x['index'] == 'Python', 'rec_learn'].values[0]\nrec_py.loc[lambda x: x['lang'] == 'no Python but SQL or R', 'ratio'] = df.loc[(df['language'].str.contains('SQL|R')) & (~df['language'].str.contains('Python'))]['rec_learn'].value_counts(normalize=True).reset_index().loc[lambda x: x['index'] == 'Python', 'rec_learn'].values[0]\nrec_py.loc[lambda x: x['lang'] == 'none of Python SQL or R', 'ratio'] = df.loc[~df['language'].str.contains('Python|SQL|R')]['rec_learn'].value_counts(normalize=True).reset_index().loc[lambda x: x['index'] == 'Python', 'rec_learn'].values[0]\n\nplt.figure(figsize=(10, 4))\nax = sns.barplot(data=rec_py, x='lang', y='ratio')\nplt.title('Language People Use - Ratio of People Recommend Python to Aspiring DS', fontsize=15)\nplt.xlabel('langugage people use')\nplt.ylabel('ratio of people rec python')\nfor i, ratio in enumerate(rec_py['ratio'].values):\n    plt.text(x=(i - 0.15), y=0.89 * ratio, s=str(np.round(ratio * 100, 2)) + '%', color='white', fontfamily='fantasy', fontweight='bold', fontsize=15)\n\nprint('Analytics who use other analytical languages other than Python do not think Python very important comparing people not doing analytics at all.')\nprint('* Analytics may think Python can be replaced by other languages when doing data science work.')\nprint('* Non-analytics may think Python is very important when doing data science work.')\nprint('* Python is the good language to bridge gaps among different tracks and areas.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Working Environment Setup and Preference\n* Budget seems to be a big consideration about using host notebooks, top3: Colab, Kaggle, JupyterHub they all have free working space. 5170 leave the question empty, possibly using local tools.\n* For Colab, Kaggle, Google, and Amazon, number/ratio of users shifts across code experience and varies among different job titles."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ide = pd.Series('|'.join(df['ide'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'ide', 0:'ratio'})\nide = ide.loc[lambda x: (x['ide'] != '') & (x['ide'] != 'None')]\n\nplt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - IDE', fontsize=15)\nax = sns.barplot(data=ide, x='ide', y='ratio')\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Top 3 IDEs: Jupyter | Visual Studio Code | PyCharm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"notebook = pd.Series('|'.join(df['notebook'].values).split('|')).value_counts().reset_index().rename(columns={'index':'notebook', 0:'ratio'})\nnotebook = notebook.loc[lambda x: (x['notebook'] != '') & (x['notebook'] != 'None')]\n\nplt.figure(figsize=(8, 4))\nplt.title('Ratio of Users - Notebook Platform', fontsize=15)\nax = sns.barplot(data=notebook, x='notebook', y='ratio')\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Top 3 notebook platforms: Colab | Kaggle | JupyterHub')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"notebook_by_job = df.groupby(['job_title']).apply(lambda x: pd.Series('|'.join(x['notebook'].values).split('|')).value_counts()/x['notebook'].shape[0]).reset_index()\nnotebook_by_job.rename(columns={'level_1':'notebook', 0:'ingroup_ratio'}, inplace=True)\n\nnotebook_by_codeexp = df.groupby(['code_exp']).apply(lambda x: pd.Series('|'.join(x['notebook'].values).split('|')).value_counts()/x['notebook'].shape[0]).reset_index()\nnotebook_by_codeexp.rename(columns={'level_1':'notebook', 0:'ingroup_ratio'}, inplace=True)\nnotebook_by_codeexp = get_code_exp_order(notebook_by_codeexp)\nnotebook_by_codeexp = notebook_by_codeexp.sort_values(by=['exp_order'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nb_to_consider = ['Kaggle Notebooks', 'Colab Notebooks', 'Binder / JupyterHub', 'Google Cloud AI Platform Notebooks', 'Azure Notebooks', \n                  'Google Cloud Datalab Notebooks', 'IBM Watson Studio', 'Amazon Sagemaker Studio', 'Amazon EMR Notebooks']\njt_to_consider = ['Student', 'Business Analyst', 'Data Analyst', 'Data Engineer', 'Data Scientist', \n                  'Machine Learning Engineer', 'Product/Project Manager', 'Research Scientist']\n\nplt.figure(figsize=(15, 5))\nax = sns.barplot(data=notebook_by_job.loc[lambda x: \n                                          (x['notebook'].isin(nb_to_consider)) & \n                                          (x['job_title'].isin(jt_to_consider))], \n                 x='notebook', \n                 y='ingroup_ratio', \n                 hue='job_title')\nax.set_title('Notebook - Job Title', fontsize=20)\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Here, we see that Kaggle and Colab are Machine Learning Engineers favorite, while Data Scientists also like them.')\nprint(\"Amazon Sagemaker doesn't have much ratio of users comparing with Kaggle and Google Colab, maybe it's because Kaggle is more connected with Google products.\")\nprint(\"* Although there aren't many users in the survey, what's interesting is there is a gap between the use preference for students and people who work as ds related jobs.\")\nprint(\"* Only small portion of the students use Amazon Sagemaker, but Data Scientists, Data Engineers, and Machine Learning Engineers are using them.\")\nprint('\\n')\n\nplt.figure(figsize=(15, 5))\nax_1 = sns.barplot(data=notebook_by_codeexp.loc[lambda x: \n                                                (x['notebook'].isin(nb_to_consider))], \n                   x='notebook', \n                   y='ingroup_ratio', \n                   hue='code_exp')\nax_1.set_title('Notebook by Coding Experience', fontsize=20)\nax_1.set_xticklabels(labels=ax_1.get_xticklabels(), rotation=90)\nprint('People with 1-5 years coding experience are most actively using Kaggle and Colab.')\nprint('Great to see that those with 5-10 years are also active on the two platforms becuase they highly taking charge of who to hire in a typical DS team.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization Packages & Tools"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vis = df.copy()\nvis['num_vis'] = vis['lib_visual'].apply(lambda x: len(x.split('|')))\nvis = get_num_X(vis, 'lib_visual', 'num_vis', dropna=False)\nprint('{:.2%} are using at least two different visualization packages.'.format(vis.loc[vis['num_vis'] > 1].shape[0]/df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vis = pd.Series('|'.join(df['lib_visual'].values).split('|')).value_counts(dropna=False).reset_index() # however, we only consider people who filled out this part, so for the below plot we dropped '' and 'None' values \nvis.columns = ['package', 'frequency']\nvis['ratio'] = vis['frequency']/vis['frequency'].sum()\nvis.sort_values(by=['ratio'], ascending=False)\n\nplt.figure(figsize=(8, 4))\nax = sns.barplot(data=vis.loc[~vis['package'].isin(['', 'None', None])], x='package', y='ratio') \nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)\nprint('Top 4 visualization packages: Matplotlib | Seaborn | Plotly | ggplot')\nprint('Some less familiar packages: Bokeh | Geoplotlib | D3 js | Leaflet/Folium | Altair')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"vis_by_job = df.groupby(['job_title']).apply(lambda x: pd.Series('|'.join(x['lib_visual'].values).split('|')).value_counts()/x['lib_visual'].shape[0]).reset_index()\nvis_by_job.rename(columns={'level_1':'lib_visual', 0:'ingroup_ratio'}, inplace=True)\n\nplt.figure(figsize=(15, 5))\nax = sns.barplot(data=vis_by_job.loc[lambda x: (x['job_title'].isin(jt_to_consider)) & \n                                               (~x['lib_visual'].isin(['', 'None', None]))], \n                 x='lib_visual', \n                 y='ingroup_ratio', \n                 hue='job_title')\nax.set_title('Visualization Package by Job Title', fontsize=20)\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90);\nprint('Machine learning engineers love Matplotlib most, maybe it is easier to be integrated as part of the analytical pipeline?')\nprint('Data Scientist has no.1 usage on the following packages: Seaborn | Ggplot | Plotly | Shiny | Bokeh | Leaflet/Folium, and these packages have things in common: easthetical, easy-to-use, interactive')\nprint('Especially for plotly, comparing with other jobs titles, data scientists have more than 10% of people using it.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Machine Learning Skills"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ml_exp_order(df):\n    \"\"\"Create machine learning experience ordinal column. new column name is 'num_ml_exp'\"\"\"\n    df['num_ml_exp'] = df['ml_exp'].replace({'I do not use machine learning methods':0,\n                                             'Under 1 year':0.5,\n                                             '1-2 years':1.5,\n                                             '2-3 years':2.5,\n                                             '3-4 years':3.5,\n                                             '4-5 years':4.5,\n                                             '5-10 years':7.5,\n                                             '10-20 years':15,\n                                             '20 or more years':25})\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ml = df.copy().loc[lambda x: ~x['ml_exp'].isnull()] # here we only consider people who filled out this part (the condition)\nml = get_ml_exp_order(ml)\n\n# get number of skills\nml = (ml.pipe(get_num_X, col='ml_framework', new_col='num_ml_framework')\n        .pipe(get_num_X, col='ml_alg', new_col='num_ml_alg')\n        .pipe(get_num_X, col='cv', new_col='num_cv')\n        .pipe(get_num_X, col='nlp', new_col='num_nlp'))\n\n# people only select None or nothing should have 0 number \nml.loc[lambda x: (x['ml_framework'] == 'None') | (x['ml_alg'] == ''), 'num_ml_framework'] = 0\nml.loc[lambda x: (x['ml_alg'] == 'None') | (x['ml_alg'] == ''), 'num_ml_alg'] = 0\nml.loc[lambda x: (x['cv'] == 'None') | (x['cv'] == ''), 'num_cv'] = 0\nml.loc[lambda x: (x['nlp'] == 'None') | (x['nlp'] == ''), 'num_nlp'] = 0\n\nplt.figure(figsize=(10, 6))\nax = sns.lineplot(data=ml.sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_ml_framework', label='ml_framework')\nax = sns.lineplot(data=ml.sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_ml_alg', label='ml_alg')\nax = sns.lineplot(data=ml.loc[~ml['cv'].isin(['', None])].sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_cv', label='cv') #\nax = sns.lineplot(data=ml.loc[~ml['nlp'].isin(['', None])].sort_values(by=['num_ml_exp']), x='num_ml_exp', y='num_nlp', label='nlp')\nplt.title('Machine Learning Experience - Number of Skills')\nplt.ylabel('num_skills')\nplt.xticks([0, 0.5, 1.5, 2.5, 3.5, 4.5, 7.5, 15, 25], ml.sort_values(by=['num_ml_exp'])['ml_exp'].unique().tolist(), rotation=90)\nprint('Here, we can see the correlation between number of ml experience to number of ml frameworks and algorithms they reagularly use:')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for col in ['ml_framework', 'ml_alg', 'cv', 'nlp']: \n    data = ml.groupby(['num_ml_exp', 'ml_exp']).apply(lambda x: get_ratio_of_people(x, col=col)).reset_index().loc[lambda x: ~x[col].isin(['', 'None'])]\n    ax = sns.lmplot(data=data, x='num_ml_exp', y='ratio', hue=col, ci=None, order=2, truncate=True, size=5); \n    plt.xlim((0, 26))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check out some job responsibilities at work üíªüìÅüìäüìù. We can categorize them into the following three by universality üåê:\n* Top1: Analytics üìàüìä. Common tasks are Exploratory Data Analysis, Data Mining, Data Manipulation, Data Cleaning, Data Processing, Creating Business Insights, and Visualization\n* Top2: Machine Learning R&D üî©‚öôÔ∏è. Common tasks are exploring ML applications, building up ML frameworks and POCs, and algorithms optimization\n* Top3: Pipeline/Archetecture üö∞üï∏Ô∏è. Common tasks are building up Data Infrastructure and Pipeline using Platforms like AWS, GCP"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"jd = pd.Series('|'.join(df['jd']).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'jd', 0:'ratio'})\nplt.figure(figsize=(8, 4))\nsns.barplot(data=jd.loc[~jd['jd'].isin(['', 'None', None])], y='jd', x='ratio')\nplt.title('Job Responsibility - Ratio of Users', fontsize=15)\nprint('The most common job responsibility for ds-related jobs is analyze and understand data to drive thinking.')\nprint('The 2nd common job responsibility is accomplish Proof of Concepts in new aras by applying machine learning.')\nprint('The 3rd common job responsibility is build and run data manipulation infrastructure/pipeline.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's see what the following table tell us üôä:\n* For most data-related jobs, they have specific one job responsibility which is clearly more important than the rest. \n* However, data scientists, machine learning engineers, and research scientists don't have one which is apparently more important than the others, rather, they are taking multiple job responsibilities at the same time üí™üèºüí™üèºüí™üèº. Well, they are better expected to be the ü¶∏ knowing how to üèã projects end2end!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"jd_by_jt = df.groupby(['job_title']).apply(lambda x: pd.Series('|'.join(x['jd']).split('|')).value_counts().reset_index())\njd_by_jt = jd_by_jt.reset_index().rename(columns={'index':'jd', 0:'freq'}).drop(columns=['level_1'])\njd_by_jt = jd_by_jt.merge(jd_by_jt.groupby(['job_title'])['freq'].sum().reset_index().rename(columns={'freq':'total'}), on=['job_title'])\njd_by_jt['ingroup_ratio'] = np.round(jd_by_jt['freq']/jd_by_jt['total'], 3)\n\nplt.figure(figsize=(15, 8))\nax = sns.barplot(data=jd_by_jt.loc[lambda x: \n                                          (x['job_title'].isin(jt_to_consider)) & \n                                          (x['jd'] != '') &\n                                          (~x['jd'].isin(['None of these activities are an important part of my role at work', 'Other']))], \n                 y='jd', \n                 x='ingroup_ratio', \n                 hue='job_title', \n                 orient='h')\nax.set_title('Job Resposibility - Ratio of Users Per Job Title', fontsize=20)\nax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### User's Costs üíµ on ML & Cloud Services\n* Data scientists üë©‚Äçüî¨üë®‚Äçüî¨ stand out to be the group who have the strongest willingness to put money on ML & Cloud services: more than 25% of people spent 1-999üí∞, and another similar amount of people spent 10000+üí∞ in the last 5 years. I would doubt why they spent so much as data scientist maybe they are providing end-to-end service? Or they are building something huge by themselves for themselves? Or they just ... rich? ü§îü§îü§î (p.s. just a gentle reminder here never forget to stop all the instances whenever you finish using the services, otherwise your money will blow away üí∏üí∏üí∏)\n* The cost on machine learning and cloud services is positively correlated to the company size, the data science team size, and the company's stage of machine learning developements.\n* Data engineers, machine learning engineers, and project/product managers are also willing to spend money but a large group of them just make some small plays like spending 1-999üí∞ in the last 5 years. \n* In general, I would rank the spending power like this: data scientists > data engineers > machine learning engineers = project/product managers > research scientists > data analysts > business analysts"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ml_cost_cat(df):\n    \"\"\"Create the categorical column for machine learning cost in the previous 5 years. new column name is 'cost_cat'\"\"\"\n    df.loc[df['ml_cost_prev_5_year'] == '$0 ($USD)', 'cost_cat'] = 'no cost ($0)'\n    df.loc[df['ml_cost_prev_5_year'].isin(['$1-$99', '$100-$999']), 'cost_cat'] = 'small cost ($1-999)'\n    df.loc[df['ml_cost_prev_5_year'] == '$1000-$9,999', 'cost_cat'] = 'medium cost ($1000-9999)'\n    df.loc[df['ml_cost_prev_5_year'].isin(['$10,000-$99,999', '$100,000 or more ($USD)']), 'cost_cat'] = 'large cost ($10000+)'\n    \n    return df\n\ndef get_emp_size_cat(df):\n    \"\"\"Create the categorical column for employer size. new column name is 'employer_size_cat'\"\"\"\n    df.loc[lambda x: x['employer_size'].isin(['0-49 employees', '50-249 employees']), 'employer_size_cat'] = 'small'\n    df.loc[lambda x: x['employer_size'].isin(['250-999 employees']), 'employer_size_cat'] = 'medium'\n    df.loc[lambda x: x['employer_size'].isin(['1000-9,999 employees', '10,000 or more employees']), 'employer_size_cat'] = 'large'\n\n    return df\n\ndef get_bus_ds_size_cat(df):\n    \"\"\"Create the categorical column for the business data science team size. new column name is 'bus_ds_size_cat'\"\"\"\n    df.loc[lambda x: x['bus_ds_size'].isin(['0', np.nan]), 'bus_ds_size_cat'] = 'no'\n    df.loc[lambda x: x['bus_ds_size'].isin(['1-2', '3-4', '5-9']), 'bus_ds_size_cat'] = 'small'\n    df.loc[lambda x: x['bus_ds_size'].isin(['10-14']), 'bus_ds_size_cat'] = 'medium'\n    df.loc[lambda x: x['bus_ds_size'].isin(['15-19', '20+']), 'bus_ds_size_cat'] = 'large'\n    \n    return df\n\ndef get_emp_ml_stage_cat(df):\n    \"\"\"Create the categorical column for employer's machine learning stage. new column name is 'employer_ml_stage_cat'\"\"\"\n    df.loc[lambda x: x['employer_ml_stage'].isin([np.nan, 'No (we do not use ML methods)']), 'employer_ml_stage_cat'] = 'no'\n    df.loc[lambda x: x['employer_ml_stage'].isin(['We use ML methods for generating insights (but do not put working models into production)']), 'employer_ml_stage_cat'] = 'init'\n    df.loc[lambda x: x['employer_ml_stage'].isin(['We are exploring ML methods (and may one day put a model into production)', 'We recently started using ML methods (i.e., models in production for less than 2 years)']), 'employer_ml_stage_cat'] = 'grow'\n    df.loc[lambda x: x['employer_ml_stage'].isin(['We have well established ML methods (i.e., models in production for more than 2 years)']), 'employer_ml_stage_cat'] = 'devoped'\n    \n    return df\n\ndef get_ml_cost_order(df):\n    \"\"\"Create the ordinal column machine learning cost in the previous 5 years. new column name is 'ml_cost_order'\"\"\"\n    if 'ml_cost_prev_5_year' in df.columns:\n        df.loc[df['ml_cost_prev_5_year'] == '$0 ($USD)', 'ml_cost_order'] = 0\n        df.loc[df['ml_cost_prev_5_year'].isin(['$1-$99', '$100-$999']), 'ml_cost_order'] = 999\n        df.loc[df['ml_cost_prev_5_year'] == '$1000-$9,999', 'ml_cost_order'] = 9999\n        df.loc[df['ml_cost_prev_5_year'].isin(['$10,000-$99,999', '$100,000 or more ($USD)']), 'ml_cost_order'] = 20000\n    else:\n        df.loc[df['cost_cat'] == 'no cost ($0)', 'ml_cost_order'] = 0\n        df.loc[df['cost_cat'] == 'small cost ($1-999)', 'ml_cost_order'] = 999\n        df.loc[df['cost_cat'] == 'medium cost ($1000-9999)', 'ml_cost_order'] = 9999\n        df.loc[df['cost_cat'] == 'large cost ($10000+)', 'ml_cost_order'] = 20000     \n    \n    return df\n\ndef get_emp_size_order(df):\n    \"\"\"Create employer size ordinal column. new column name is 'emp_size_order'\"\"\"\n    if 'employer_size' in df.columns:\n        df.loc[lambda x: x['employer_size'].isin(['0-49 employees', '50-249 employees']), 'emp_size_order'] = 1\n        df.loc[lambda x: x['employer_size'].isin(['250-999 employees']), 'emp_size_order'] = 2\n        df.loc[lambda x: x['employer_size'].isin(['1000-9,999 employees', '10,000 or more employees']), 'emp_size_order'] = 3\n    else:\n        df.loc[lambda x: x['employer_size_cat'] == 'small', 'emp_size_order'] = 1\n        df.loc[lambda x: x['employer_size_cat'] == 'medium', 'emp_size_order'] = 2\n        df.loc[lambda x: x['employer_size_cat'] == 'large', 'emp_size_order'] = 3\n    \n    return df\n\ndef get_bus_ds_size_order(df):\n    \"\"\"Create ordinal column for business data science size. new column name is 'bus_ds_size_order'\"\"\"\n    if 'bus_ds_size' in df.columns:\n        df.loc[lambda x: x['bus_ds_size'].isin(['0', np.NaN, '', 'None', None]), 'bus_ds_size_order'] = 0\n        df.loc[lambda x: x['bus_ds_size'].isin(['1-2', '3-4', '5-9']), 'bus_ds_size_order'] = 1\n        df.loc[lambda x: x['bus_ds_size'].isin(['10-14']), 'bus_ds_size_order'] = 2\n        df.loc[lambda x: x['bus_ds_size'].isin(['15-19', '20+']), 'bus_ds_size_order'] = 3\n    else:\n        df.loc[lambda x: x['bus_ds_size_cat'] == 'no', 'bus_ds_size_order'] = 0\n        df.loc[lambda x: x['bus_ds_size_cat'] == 'small', 'bus_ds_size_order'] = 1\n        df.loc[lambda x: x['bus_ds_size_cat'] == 'medium', 'bus_ds_size_order'] = 2\n        df.loc[lambda x: x['bus_ds_size_cat'] == 'large', 'bus_ds_size_order'] = 3    \n    \n    return df\n\ndef get_emp_ml_stage_order(df):\n    \"\"\"Create ordinal column for employer machine learning stage. new column name is 'employer_ml_stage_order'\"\"\"\n    if 'employer_ml_stage' in df.columns:\n        df.loc[lambda x: x['employer_ml_stage'].isin([np.nan, 'No (we do not use ML methods)']), 'employer_ml_stage_order'] = 0\n        df.loc[lambda x: x['employer_ml_stage'].isin(['We use ML methods for generating insights (but do not put working models into production)']), 'employer_ml_stage_order'] = 1\n        df.loc[lambda x: x['employer_ml_stage'].isin(['We are exploring ML methods (and may one day put a model into production)', 'We recently started using ML methods (i.e., models in production for less than 2 years)']), 'employer_ml_stage_order'] = 2\n        df.loc[lambda x: x['employer_ml_stage'].isin(['We have well established ML methods (i.e., models in production for more than 2 years)']), 'employer_ml_stage_order'] = 3\n    else:\n        df.loc[lambda x: x['employer_ml_stage_cat'] == 'no', 'employer_ml_stage_order'] = 0\n        df.loc[lambda x: x['employer_ml_stage_cat'] == 'init', 'employer_ml_stage_order'] = 1\n        df.loc[lambda x: x['employer_ml_stage_cat'] == 'grow', 'employer_ml_stage_order'] = 2\n        df.loc[lambda x: x['employer_ml_stage_cat'] == 'devoped', 'employer_ml_stage_order'] = 3 \n        \n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cloud = df.copy().loc[lambda x: x['job_title'].isin(jt_to_consider)] # only data related jobs are our interest\ncloud['jt_cat'] = cloud['job_title'].apply(lambda x: 'student' if x == 'Student' else 'experienced')\n\n# get the categorical columns\ncloud = (cloud.pipe(get_ml_cost_cat)\n              .pipe(get_emp_size_cat)\n              .pipe(get_bus_ds_size_cat)\n              .pipe(get_emp_ml_stage_cat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cost_by_jt = cloud.groupby(['job_title']).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\n\nplt.figure(figsize=(12, 6))\nax = sns.barplot(data=cost_by_jt, y='job_title', x='ingroup_ratio', hue='cost_cat', orient='h')\nax.set_title(\"Job Title - Ratio of Users Per Costs Level on ML & Cloud Services\", fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col = 'employer_size_cat'\ncost_by_size = cloud.groupby([col]).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\ncost_by_size = get_emp_size_order(cost_by_size)\n\nsns.lmplot(data=cost_by_size, x='emp_size_order', y='ingroup_ratio', hue='cost_cat', ci=None, order=2, truncate=True, size=5); \nplt.title('User Cost Level on ML & Cloud Service by Employer DS Size', fontsize=15)\nax.set_title('User Cost Level on ML & Cloud Service by Employer DS Size')\nplt.xticks(np.arange(1, 4), ['small', 'medium', 'large'], rotation=90)\nplt.xlim((0.8, 3.2));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col = 'bus_ds_size_cat'\ncost_by_size = cloud.groupby([col]).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\ncost_by_size = get_bus_ds_size_order(cost_by_size)\n\nsns.lmplot(data=cost_by_size, x='bus_ds_size_order', y='ingroup_ratio', hue='cost_cat', ci=None, order=2, truncate=True, size=5); \nplt.title('User Cost Level on ML & Cloud Service by Business DS Size', fontsize=15)\nplt.xticks(np.arange(4), ['no', 'small', 'medium', 'large'], rotation=90)\nplt.xlim((-0.2, 4.2));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"col = 'employer_ml_stage_cat'\ncost_by_stage = cloud.groupby([col]).apply(lambda x: x['cost_cat'].value_counts(normalize=True)).reset_index().rename(columns={'level_1':'cost_cat', 'cost_cat':'ingroup_ratio'})\ncost_by_stage = get_emp_ml_stage_order(cost_by_stage)\n\nsns.lmplot(data=cost_by_stage, x='employer_ml_stage_order', y='ingroup_ratio', hue='cost_cat', ci=None, order=2, truncate=True, size=5); \nplt.title('User Cost Level on ML & Cloud Service by Employer ML Stage', fontsize=15)\nplt.xticks(np.arange(4), ['no', 'init', 'grow', 'devoped'], rotation=90)\nplt.xlim((-0.2, 4.2));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sharing work and projects to the public is really good for the community because it not only helps others understand and learn your ideas but you to get feedbacks from people in various of backgrounds. Kagglers are doing it on Kaggle ü•á! And when we dig a little more, we found:\n* In general, Machine learning engineers, data scientists, and data engineers have higher willingness to share work on platforms üòäüòäüòä\n* Students and the unemployeed don't share work to any platform at all\n* More than 50% of Kagglers don't share their work\n* Top1 share platform is Github"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"share = df.copy()\nshare.loc[share['share_plfm'].isin(['', 'I do not share my work publicly']), 'num_share_plfm'] = 0\nshare.loc[~share['share_plfm'].isin(['', 'I do not share my work publicly']), 'num_share_plfm'] = share.loc[~share['share_plfm'].isin(['', 'I do not share my work publicly']), 'share_plfm'].apply(lambda x: len(x.split('|')))\nprint('Machine Learning Engineers, Data Scientists, and Data Engineers have larger ratio of people who share their work on public platforms.')\nprint('{}({:.2%}) of Machine Learning Engineers, {}({:.2%}) of Data Scientists, and {}({:.2%}) of Data Engineers.'.format(share.loc[(share['job_title'] == 'Machine Learning Engineer') & (share['num_share_plfm'] >= 1)].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Machine Learning Engineer') & (share['num_share_plfm'] >= 1)].shape[0]/share.loc[(share['job_title'] == 'Machine Learning Engineer')].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Data Scientist') & (share['num_share_plfm'] >= 1)].shape[0], \n                                                                                                            share.loc[(share['job_title'] == 'Data Scientist') & (share['num_share_plfm'] >= 1)].shape[0]/share.loc[(share['job_title'] == 'Data Scientist')].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Data Engineer') & (share['num_share_plfm'] >= 1)].shape[0],\n                                                                                                            share.loc[(share['job_title'] == 'Data Engineer') & (share['num_share_plfm'] >= 1)].shape[0]/share.loc[(share['job_title'] == 'Data Engineer')].shape[0]))\nshare.loc[lambda x: x['job_title'].isin(jt_to_consider)].groupby(['job_title'])['num_share_plfm'].describe(percentiles=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"share_plfm = pd.Series('|'.join(df['share_plfm'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'share_plfm', 0:'ratio'})\nplt.figure(figsize=(10, 5))\nsns.barplot(data=share_plfm, x='ratio', y='share_plfm')\nplt.title('Ratio of Users - Share Platform', fontsize=20)\nprint(\"More than 50% of people don't share their work to public platforms.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"share_by_jt = share.loc[lambda x: x['num_share_plfm'] < 1]['job_title'].value_counts().reset_index().rename(columns={'index':'job_title', 'job_title':'num_no_share'})\nshare_by_jt = share_by_jt.merge(share.loc[lambda x: x['num_share_plfm'] >= 1]['job_title'].value_counts().reset_index().rename(columns={'index':'job_title', 'job_title':'num_share'}),\n                                on='job_title', \n                                how='outer').fillna(0)\n                             \nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 5))\nsns.barplot(data=share_by_jt, x='num_no_share', y='job_title', ax=ax1)\nsns.barplot(data=share_by_jt, x='num_share', y='job_title', ax=ax2)\nax2.set_ylabel('')\nprint(\"Students and people currently not employed doesn't share their work to platforms.\")\nprint(\"Data Scientists is the most active group sharing work.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Course Platforms\n* More than 18% of Kagglers like Coursera the most, followed by Kaggle Learn Courses (12.5%) and Udemy (12%)\n* Around 19% of students don't have online course platform to learn \n* Cloud-certification programs and Fast.ai have the least popularity having only 2.5% of Kagglers using them üòøüòøüòø"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"course = pd.Series('|'.join(df['ds_course_plfm'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'course_plfm', 0:'ratio'}).loc[lambda x: ~x['course_plfm'].isin(['', 'None'])]\n\nplt.figure(figsize=(10, 5))\nsns.barplot(data=course, x='ratio', y='course_plfm', orient='h')\nprint('most people use Coursera to learn.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"course_by_jt = df.copy().loc[lambda x: x['job_title'].isin(jt_to_consider)]\ncourse_by_jt['jt_cat'] = course_by_jt['job_title'].apply(lambda x: 'student' if x == 'Student' else 'experienced')\ncourse_by_jt = course_by_jt.groupby(['jt_cat']).apply(lambda x: pd.Series('|'.join(x['ds_course_plfm']).split('|')).value_counts(dropna=True, normalize=True)).reset_index().rename(columns={'level_1':'ds_course_plfm', 0:'ingroup_ratio'})\n\nplt.figure(figsize=(12, 6))\nax = sns.barplot(data=course_by_jt, y='ds_course_plfm', x='ingroup_ratio', hue='jt_cat', orient='h')\nprint('Experienced people tend to be more active on taking courses on most platforms, except Kaggle Learn Courses and University Courses.')\nprint(\"A larger ratio of students left it blank. Maybe they don't take courses or they aren't confident enough to say they take courses.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Science Media Sources\n* Kaggle takes the 1st place üò¨. But here, it shows that *Kaggle notebooks* and *Kaggle forums* are the two major ways, which are both in the readable and interactive formats\n* Youtube takes the 2nd place. What are the popular channels to follow then?\n* Blogs took the 3rd place. Popular accounts are *Towards Data Science* (everybody knows it ü§£) and *Analytics Vidhya*, and they are both in the readable format\n* What students like more: *Youtube*\n* What the experienced like more: *Kaggle*, *Blogs*, *Twitter*, *Journal Publications*, *Email Newsletters*, *Slack Communities*, *Podcasts*\n* What students and the experienced have the similar like: *Reddit*, *Course Forums*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"media = pd.Series('|'.join(df['ds_media'].values).split('|')).value_counts(normalize=True).reset_index().rename(columns={'index':'ds_media', 0:'ratio'}).loc[lambda x: ~x['ds_media'].isin(['', 'None'])]\n\nplt.figure(figsize=(10, 5))\nplt.title('Ratio of Users - Favorite Data Science Media Source', fontsize=20)\nsns.barplot(data=media, x='ratio', y='ds_media', orient='h');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"media_srce_by_jt = df.copy().loc[lambda x: x['job_title'].isin(jt_to_consider)] # only data related jobs are our interest\nmedia_srce_by_jt['jt_cat'] = media_srce_by_jt['job_title'].apply(lambda x: 'student' if x == 'Student' else 'experienced')\nmedia_srce_by_jt = media_srce_by_jt.groupby(['jt_cat']).apply(lambda x: pd.Series('|'.join(x['ds_media']).split('|')).value_counts(dropna=True, normalize=True)).reset_index().rename(columns={'level_1':'ds_media', 0:'ingroup_ratio'})\n\nplt.figure(figsize=(12, 6))\nplt.title('Ratio of Experienced and Students - Favorite Data Science Media', fontsize=20)\nax = sns.barplot(data=media_srce_by_jt, y='ds_media', x='ingroup_ratio', hue='jt_cat')\nprint('Experienced people tend to be more active on taking courses on most medias, except Youtube and course forums.')\nprint(\"A larger ratio of students left it blank. Maybe they don't watch ds medias or they sometimes just randomly browsing different channels without realizing whatever they are using.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = k_2020_adj.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = ['age', 'formal_edu', 'country', 'job_title', 'code_exp', 'language', 'ide', 'notebook', \n            'lib_visual', 'ml_exp', 'ml_framework', 'ml_alg', 'cv', 'nlp', 'jd', 'ml_cost_prev_5_year', \n            'employer_size', 'bus_ds_size', 'employer_ml_stage', 'share_plfm', 'ds_course_plfm', 'ds_media',\n            'annual_comp']\ndf = df[col_list]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_annual_comp_cat(df):\n    \"\"\"Create the categorical column for annual compensation. the new column name is 'comp_cat'\"\"\"\n    df.loc[df['annual_comp'].isin(['$0-999', '1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999',\n                                '5,000-7,499', '7,500-9,999', '10,000-14,999', '15,000-19,999', \n                                '20,000-24,999', '25,000-29,999', '30,000-39,999', '40,000-49,999',\n                                '50,000-59,999', '60,000-69,999', '70,000-79,999', '80,000-89,999',\n                                '90,000-99,999']), 'comp_cat'] = '<$100000'\n    df.loc[df['annual_comp'].isin(['100,000-124,999', '125,000-149,999', '150,000-199,999']), 'comp_cat'] = '$100000-200000'\n    df.loc[df['annual_comp'].isin(['200,000-249,999', '250,000-299,999', '300,000-500,000']), 'comp_cat'] = '$200000-500000'\n    df.loc[df['annual_comp'].isin(['> $500,000']), 'comp_cat'] = '>$500000'\n    \n    return df\n\ndef get_annual_comp_order(df, is_simple=False):\n    \"\"\"Create the ordinal column for annual compensation. the new column name is 'comp_order'\"\"\"\n    if is_simple:\n        if 'annual_comp' in df.columns:\n            df.loc[df['annual_comp'].isin(['$0-999', '1,000-1,999', '2,000-2,999', '3,000-3,999', '4,000-4,999',\n                                        '5,000-7,499', '7,500-9,999', '10,000-14,999', '15,000-19,999', \n                                        '20,000-24,999', '25,000-29,999', '30,000-39,999', '40,000-49,999',\n                                        '50,000-59,999', '60,000-69,999', '70,000-79,999', '80,000-89,999',\n                                        '90,000-99,999']), 'comp_order'] = 100000\n            df.loc[df['annual_comp'].isin(['100,000-124,999', '125,000-149,999', '150,000-199,999']), 'comp_order'] = 200000\n            df.loc[df['annual_comp'].isin(['200,000-249,999', '250,000-299,999', '300,000-500,000']), 'comp_order'] = 500000\n            df.loc[df['annual_comp'].isin(['> $500,000']), 'comp_order'] = 1000000\n        else:\n            df.loc[df['comp_cat'] == '<$100000', 'comp_order'] = 100000\n            df.loc[df['comp_cat'] == '$100000-200000', 'comp_order'] = 200000\n            df.loc[df['comp_cat'] == '$200000-500000', 'comp_order'] = 500000\n            df.loc[df['comp_cat'] == '>$500000', 'comp_order'] = 1000000\n    else:\n        df['comp_order'] = df['annual_comp'].replace({'$0-999':999, '1,000-1,999':1999, '2,000-2,999':2999, '3,000-3,999':3999, \n                                                      '4,000-4,999':4999, '5,000-7,499':7499, '7,500-9,999':9999, '10,000-14,999':14999, \n                                                      '15,000-19,999':19999, '20,000-24,999':24999, '25,000-29,999':29999, '30,000-39,999':39999, \n                                                      '40,000-49,999':49999, '50,000-59,999':59999, '60,000-69,999':69999, '70,000-79,999':79999, '80,000-89,999':89999,\n                                                      '90,000-99,999':99999, '100,000-124,999':124999, '125,000-149,999':149999, '150,000-199,999':199999,\n                                                      '200,000-249,999':249999, '250,000-299,999':299999, '300,000-500,000':500000, '> $500,000':1000000})\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# annual compensation is the target variable\ndf = (df\n      #.pipe(get_annual_comp_cat)\n      .pipe(get_annual_comp_order))\n\n# we drop all the records which don't have annual compensation \ndf = df.loc[~df['annual_comp'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in col_list:\n    print('###################')\n    print('column name:', col)\n    print('number of missing values =', df[col].isnull().sum())\n    print('number of n/a values', df.loc[df[col].isin(['', 'None', None])].shape[0])\n    print(df[col].unique()[:10])\n    print('\\n')\n    \n    # impute missing or n/a values with 'N/A' \n    #df.loc[(df[col].isnull()) | (df[col].isin(['', 'None', None])), col] = 'N/A'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_is_X(df, col='', word=''):\n    \"\"\"\n    Create boolean index column for word X. \n    \n    Args:\n        df (pd.DataFrame): the dataset\n        col (string): the column name\n        word (string): the keyword to search. if keyword, 1. else 0.\n    \n    Returns:\n        df\n    \"\"\"\n    assert word != '', 'have not pass a word argument...'\n    df['is_' + word.lower().replace(' ', '_')] = df[col].apply(lambda x: 1 if word.lower() in x.lower() else 0)\n    \n    return df\n\ndef get_dummy_X(df, col=''):\n    \"\"\"Convert job title into dummies.\"\"\"\n    dummies = pd.get_dummies(df[col])\n    dummies.columns = ['is_' + col.lower().replace(', ', '_').replace(' ', '_').replace('/', '_') for col in dummies.columns]\n    \n    return pd.concat([df, dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = (df.pipe(get_age_order)\n        .pipe(get_formal_edu_order)\n        .pipe(get_is_X, col='country', word='United States')\n        .pipe(get_dummy_X, col='job_title')\n        .pipe(get_code_exp_order)\n        .pipe(get_num_X, col='language', new_col='num_language', dropna=True)\n        .pipe(get_is_X, col='language', word='Python')\n        .pipe(get_is_X, col='language', word='R')\n        .pipe(get_is_X, col='language', word='SQL')\n        .pipe(get_is_X, col='language', word='C')\n        .pipe(get_is_X, col='language', word='C++')\n        .pipe(get_num_X, col='ide', new_col='num_ide', dropna=True)\n        .pipe(get_is_X, col='ide', word='Jupyter')\n        .pipe(get_is_X, col='ide', word='Visual Studio')\n        .pipe(get_is_X, col='ide', word='PyCharm')\n        .pipe(get_is_X, col='ide', word='Spyder')\n        .pipe(get_is_X, col='ide', word='RStudio')\n        .pipe(get_num_X, col='notebook', new_col='num_notebook', dropna=True)\n        .pipe(get_is_X, col='notebook', word='Colab Notebooks')\n        .pipe(get_is_X, col='notebook', word='Kaggle Notebooks')\n        .pipe(get_is_X, col='notebook', word='IBM Watson Studio')\n        .pipe(get_is_X, col='notebook', word='Amazon EMR Notebooks')\n        .pipe(get_is_X, col='notebook', word='Amazon Sagemaker Studio')\n        .pipe(get_is_X, col='notebook', word='Azure Notebooks')\n        .pipe(get_is_X, col='notebook', word='Google Cloud AI Platform Notebooks')\n        .pipe(get_is_X, col='notebook', word='Google Cloud Datalab Notebooks')\n        .pipe(get_is_X, col='notebook', word='Databricks Collaborative Notebooks')\n        .pipe(get_is_X, col='notebook', word='JupyterHub')\n        .pipe(get_num_X, col='lib_visual', new_col='num_vis', dropna=True)\n        .pipe(get_is_X, col='lib_visual', word='Matplotlib')\n        .pipe(get_is_X, col='lib_visual', word='Seaborn')\n        .pipe(get_is_X, col='lib_visual', word='Ggplot')\n        .pipe(get_is_X, col='lib_visual', word='Shiny')\n        .pipe(get_is_X, col='lib_visual', word='Plotly')\n        .pipe(get_is_X, col='lib_visual', word='Altair')\n        .pipe(get_is_X, col='lib_visual', word='Bokeh')\n        .pipe(get_is_X, col='lib_visual', word='Geoplotlib')\n        .pipe(get_is_X, col='lib_visual', word='D3 js')\n        .pipe(get_is_X, col='lib_visual', word='Leaflet')\n        .pipe(get_ml_exp_order)\n        .pipe(get_num_X, col='ml_framework', new_col='num_ml_framework', dropna=True)\n        .pipe(get_num_X, col='ml_alg', new_col='num_ml_alg', dropna=True)\n        .pipe(get_num_X, col='cv', new_col='num_cv', dropna=True)\n        .pipe(get_num_X, col='nlp', new_col='num_nlp', dropna=True)\n        .pipe(get_num_X, col='jd', new_col='num_jd', dropna=True)\n        .pipe(get_is_X, col='jd', word='Analyze and understand data to influence product or business decisions')\n        .pipe(get_is_X, col='jd', word='Build prototypes to explore applying machine learning to new areas')\n        .pipe(get_is_X, col='jd', word='Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data')\n        .pipe(get_is_X, col='jd', word='Experimentation and iteration to improve existing ML models')\n        .pipe(get_is_X, col='jd', word='Build and/or run a machine learning service that operationally improves my product or workflows')\n        .pipe(get_is_X, col='jd', word='Do research that advances the state of the art of machine learning')\n        .pipe(get_is_X, col='jd', word='None of these activities are an important part of my role at work')\n        .pipe(get_ml_cost_order)\n        .pipe(get_emp_size_order)\n        .pipe(get_bus_ds_size_order)\n        .pipe(get_emp_ml_stage_order)\n        .pipe(get_num_X, col='share_plfm', new_col='num_share_plfm', dropna=True)\n        .pipe(get_is_X, col='share_plfm', word='GitHub')\n        .pipe(get_is_X, col='share_plfm', word='Kaggle')\n        .pipe(get_is_X, col='share_plfm', word='Colab')\n        .pipe(get_is_X, col='share_plfm', word='blog')\n        .pipe(get_is_X, col='share_plfm', word='NBViewer')\n        .pipe(get_is_X, col='share_plfm', word='Plotly Dash')\n        .pipe(get_is_X, col='share_plfm', word='Shiny')\n        .pipe(get_is_X, col='share_plfm', word='Streamlit')\n        .pipe(get_is_X, col='share_plfm', word='I do not share my work publicly')\n        .pipe(get_num_X, col='ds_course_plfm', new_col='num_ds_course_plfm', dropna=True)\n        .pipe(get_is_X, col='ds_course_plfm', word='Coursera')\n        .pipe(get_is_X, col='ds_course_plfm', word='DataCamp')\n        .pipe(get_is_X, col='ds_course_plfm', word='Udemy')\n        .pipe(get_is_X, col='ds_course_plfm', word='Edx')\n        .pipe(get_is_X, col='ds_course_plfm', word='Udacity')\n        .pipe(get_is_X, col='ds_course_plfm', word='University Courses')\n        .pipe(get_is_X, col='ds_course_plfm', word='Kaggle Learn Courses')\n        .pipe(get_is_X, col='ds_course_plfm', word='DataCamp')\n        .pipe(get_is_X, col='ds_course_plfm', word='LinkedIn Learning')\n        .pipe(get_is_X, col='ds_course_plfm', word='Fast.ai')\n        .pipe(get_is_X, col='ds_course_plfm', word='Cloud-certification programs')\n        .pipe(get_num_X, col='ds_media', new_col='num_ds_media', dropna=True)\n        .pipe(get_is_X, col='ds_media', word='Twitter')\n        .pipe(get_is_X, col='ds_media', word='Reddit')\n        .pipe(get_is_X, col='ds_media', word='Kaggle')\n        .pipe(get_is_X, col='ds_media', word='Course Forums')\n        .pipe(get_is_X, col='ds_media', word='YouTube')\n        .pipe(get_is_X, col='ds_media', word='Blogs')\n        .pipe(get_is_X, col='ds_media', word='Slack Communities')\n        .pipe(get_is_X, col='ds_media', word='Email newsletters')\n        .pipe(get_is_X, col='ds_media', word='Journal Publications')\n        .pipe(get_is_X, col='ds_media', word='Podcasts')\n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns=col_list)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Imputations done for the following columns:')\nfor col in df.columns:\n    if df[col].isnull().sum().sum() > 0:\n        print(col)\n        df[col] = df[col].fillna(0)\n        \nprint('\\nLong column names:')\nfor col in df.columns:\n    if len(col) > 50:\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling For The Interpretability of What's Important\n* In this section, we used **Information Gain (IG)** and **Permutation** to find out feature importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training\nX_col_list = [col for col in df.columns if col not in ['comp_order']]\n\nmodel = XGBRegressor(n_estimators=100,\n                     max_depth=5,\n                     subsample=0.8,\n                     colsample_bytree=0.6,\n                     reg_alpha=1,\n                     reg_lambda=1)\nmodel.fit(X=df[X_col_list],\n          y=df['comp_order'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Feature Importance with IG\n#### by taking consideration of the top 10 important features, we can further categorize them into aspects as follows:\n     * personal attributes: 'is_united_states',\n     * skillsets & abilities: 'is_plotly_dash', 'is_jupyterhub', 'is_leaflet'\n     * career experiences: 'exp_order', 'is_slack_communities', 'ml_cost_order', 'employer_ml_stage_order', 'bus_ds_size_order', 'is_analyze_and_understand_data_to_influence_product_or_business_decisions'"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 25))\nplt.title(\"Feature Importances\", fontsize=20)\npd.Series(model.feature_importances_, index=X_col_list).sort_values().plot(kind='barh');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Feature Importance with Permutation\n#### Great üëçüèº! It gives us a clearer trend of what are the more important features:\n    * personal attributes: 'is_united_states', 'age_order', 'edu_order', \n    * skillsets & abilities: 'num_ide', 'num_language', 'num_vis', 'num_ml_framework'\n    * career experiences: 'ml_cost_order', 'exp_order', 'num_ml_exp', 'bus_ds_size_order', 'employer_ml_stage_order', 'num_ml_alg'\n    * continuous growths: 'num_ds_course_plfm', 'num_share_plfm'"},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = permutation_importance(model, \n                              X=df[X_col_list],\n                              y=df['comp_order'],\n                              n_repeats=300,\n                              random_state=422)\n\nsorted_idx = perm.importances_mean.argsort()\n\nplt.figure(figsize=(15, 25))\nplt.boxplot(perm.importances[sorted_idx].T, vert=False, labels=df[X_col_list].columns[sorted_idx])\nplt.title(\"Permutation Importances\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}