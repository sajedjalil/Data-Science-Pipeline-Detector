{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How unique are you on Kaggle?\nI apologize for the clickbaity title, but it encapsulates the question I wanted to answer in this notebook - given my answers to the survey, how unique am I compared to other people on kaggle.\n\n## My Answers\nI did not actually participate in the survey (and that wouldn't have mattered anyways since it's all annonymized), so first up I'll make a small interactive widget allowing me to quickly fill in the questionaire based on answers provided by other users. This is not perfect (i.e. nobody from Denmark apparently filled the survey, so I guess I'll be swedish today), but I reckon it's good enough for this exercise to be valid and fun anyways :)\n\n**Note:** By forking this notebook, you can of course fill in your answers to the questions, and get a personalized analysis :)"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install trimap scikit-plot --quiet | cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import time\nimport re\nfrom typing import Tuple\n\nimport numpy as np\nimport pandas as pd\n\nimport trimap\nfrom sklearn.preprocessing import normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom joblib import Parallel, delayed\n\nfrom ipywidgets import Label, Accordion, SelectMultiple, Dropdown, VBox\n\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n\nsns.set()\npd.set_option('display.max_colwidth', -1)\n\n# Read the raw data\nraw = pd.read_csv('/kaggle/input/kaggle-survey-2020/kaggle_survey_2020_responses.csv').iloc[:, 1:]\n\n# Strip whitespace from all entries\nraw = raw.apply(lambda column: column.str.strip(), axis=0)\n\n# We'll keep the processed data in 'df'. We replace everything in first row with empty strings; a placeholder for user answers \ndf = raw.copy().fillna('')\ndf.iloc[0] = ''\n\n# Get ordered list of questions names (i.e. exclude the _Part and _Other columns)\nquestions_names = [re.sub('(_Part_\\d+|_OTHER)', '', q) for q in raw.columns]\nquestions_names = [x for i, x in enumerate(questions_names) if x not in questions_names[:i]]\n\n# Keep track of which question names have multiple raw columns\nhas_multiple = [True if any([k for k in df.columns if f'{c}_Part' in k]) else False for c in questions_names]\n\n# Get the question texts (& remove anything after 'Selected Choice')\nquestions_texts = [raw.loc[0, q] if q in raw.columns else raw.loc[0, f'{q}_Part_1'] for q in questions_names]\nquestions_texts = [re.sub('\\s-\\sSelected.+', '', q) for q in questions_texts]\n\n# Create a tab for each of the questions\ntabs = []\nfor name, text in zip(questions_names, questions_texts):\n    \n    # Get all answer options (in all related columns) for this question\n    related = [c for c in raw.columns if name == c or f'{name}_' in c]\n    options = sorted(pd.unique(df[related].values.ravel()))\n    \n    # Type of widget\n    select_widget = (SelectMultiple(options=options) \n        if 'all that apply' in text\n        else Dropdown(options=options))\n\n    # Show the dropdown\n    tabs.append(VBox([\n        Label(text),\n        select_widget\n    ]))\n    \n# Set the question titles for each child of the accordian\ntab = Accordion(children=tabs)\nfor i, name in enumerate(questions_names):\n    tab.set_title(i, name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(tab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the questions have been filled out, we can continue to preparing the data for analysis."},{"metadata":{},"cell_type":"markdown","source":"# Featurizing the data\nIn order to quantify similarity between my answers and those in the survey, the data has to be featurized in some manner. To keep things simple, I went for the following approach:\n\n* As a simplification we'll assume that all data is categorical (even those categories that are strictly ordinal in nature), and one-hot-encode them all taking into account that some questions allow to multiple answers (i.e. multiple `1`'s in our encoding.\n* Apply the L2 norm to the data to scale all user vectors to unit norm. By doing this, we can calculate the cosine similarity between each user is simply as the dot product of their feature vectors."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# If no answers were provided, then use mine (note: I'm okay with this being public)\ndid_answer = any([True if t.children[1].index else False for t in tab.children])\nif not did_answer:\n    mathias_answers = [4,1,45,2,6,2,(1, 5, 10, 12),10,(1, 10, 12),(4, 6, 7, 11),2,(1,),1,(7, 10, 11),6,(3, 4, 6, 7, 11, 12, 13, 14, 16),(1, 2, 3, 4, 5, 7, 8, 11, 12),(1, 3, 4, 6),(1, 2, 5, 6),2,7,5,(1, 2, 3, 4, 5, 6),5,3,(2, 5),(1, 2, 3, 4, 5, 10),(10,),(2, 11, 16, 17),15,(8,),0,(1, 2, 3, 4, 5, 6),(1, 2, 3, 5, 11),(9, 11),(1, 2, 4),(2, 3, 4, 9),4,(1, 2, 5, 8, 12),3,(1, 5, 8),(10,),(18,),(8,),(1, 2, 3, 4, 5, 6),(5,),(9, 11)]\n    for child, answer in zip(tab.children, mathias_answers):\n        child.children[1].index = answer\n    did_answer = True\n    \n# Go through the questions one by one\nresult = []\nfor i, (question, multiple) in enumerate(zip(questions_names, has_multiple)):        \n    \n    # USER ANSWER\n    ############################\n    \n    # Get the values picked in the widget\n    value = tab.children[i].children[1].value    \n    \n    # Check if single or multiple label\n    if not multiple:\n        df.loc[0, question] = value\n    else:\n        columns = [c for c in df.columns if f'{question}_' in c]\n        for column in columns:\n            options = df[column].unique()\n            if any([k in options for k in value]):\n                df.loc[0, column] = 1 \n    \n    # FEATURIZE\n    ############################\n    \n    if not multiple:\n        # If a single-answer question, then just get OHE columns\n        result.append(pd.get_dummies(df[question]))    \n    else:\n        # Convert texts to 1s and nothings to 0s\n        columns = [c for c in df.columns if f'{question}_' in c]\n        result.append((df[columns] != '').astype(int))\n\n# Concatenate all the results. Output is [responses X features]\nresult = pd.concat(result, axis=1)\nfeature_names = result.columns\n\n# L2 Normalize the data\nresult = normalize(result, norm='l2', axis=1)\n\n# Calculate the angular distances between all users with all users\ndistances = result.dot(result.T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So, how unique am I?\nWith all the prep-work out of the way, we can now try to answer the question at hand. \"Uniqueness\" is something that could be quantified using a variety of approaches, e.g. multivariate density estimation, correlation metrics, or some other custom logic. To keep things simple, I simply calculated the dot product between the L2 norm of my answers with all the other response vectors (to find the angular distance). \n\nWith this I get the angular distances between my answers and those of all other respondees, with values of `1` being very similar and values of `0` being very dissimilar. To get a single \"score\" for uniqueness, I've chosen to take the mean of all these distances to all other users, so that a *lower score* means *more unique* and a *higher score* means *less unique* "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Calculate uniqueness score of all kaggles, as well as for notebook\nscores = distances.mean(axis=1)\nscore = scores[0]\n\n# Create histogram from scores\ncounts, bins = np.histogram(scores, bins=np.linspace(0.1, 0.4, num=101))\nbins = 0.5 * (bins[:-1] + bins[1:])\n\n# For histogram of notebooks answers, we only show one count\nnotebook_counts = np.zeros_like(counts)\nnotebook_counts[np.abs(bins - score).argmin()] = np.max(counts) + 100\n\n# Show plot with all kaggler scores & notebook scores\nfig = go.Figure([\n    go.Bar(\n        x=bins, \n        y=counts,\n        name='Other kaggler scores',\n        marker=dict(\n            color=sns.color_palette('deep', 1).as_hex()[0]\n        )\n    ),\n    go.Bar(\n        x=bins, \n        y=notebook_counts,\n        name='Answers in this notebook',\n        marker=dict(\n            color='darkred'\n        )\n    )\n], layout=dict(\n    xaxis_title='<-- More unique | Similarity Score | Less unique -->',\n    yaxis_title='# Responses',\n    yaxis=dict(range=[0, np.max(counts)+100]),\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.02,\n        xanchor=\"right\",\n        x=1\n    ), \n    barmode='overlay'\n))\n\nfig.add_annotation(\n    text=f\"\"\"\n    Notebook Score: {score:.3f}<br />\n    More unique answers: {(scores < score).sum()}<br />\n    Less unique answers: {(scores > score).sum()}\n    \"\"\",\n    xref=\"paper\", yref=\"paper\",\n    x=0.05, y=0.9, \n    showarrow=False\n)\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)For me I got a score of 0.238 and a uniqueness rank of 4097 - i.e. 4097 people are more unique than me. Guess I'm pretty average based off this metric :)"},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Survey\nWith the previous section we deduced a quantitative score for how similar the answers provided in this notebook are to answers given in the survey. It could also be interesting to see this visually - i.e. let us try to do some dimensionality reduction, and see where the answers filled into the notebook widget lie in a 2D plot containing all the other respondees in the survey. \n\nThe idea of the dimensionality reduction algorithm is to retain as much information about the dataset  as possible in a lower dimensional space (typically 2D), which can be achieved in different ways, see e.g. [examples of this in scikit-learn](https://scikit-learn.org/stable/modules/manifold.html). These days the type of dimensionality reduction we're trying to do is often achieved with either t-SNE or UMAP, but in this notebook I'll use a recent technique called TriMap [(See paper here)](https://arxiv.org/abs/1910.00204), which in my experience empirically works better than t-SNE/UMAP when it comes to global accuracy, and which is faster to run."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import trimap\n\n# Create TriMap embeddings\ntrimap_embedding = trimap.TRIMAP(\n  n_iters=1000,\n  distance='angular',\n  apply_pca=True,\n  weight_adj=5000\n).fit_transform(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# What column to color by\nCOLOR_COL = 'Q6'\n\n# Use the seaborn palette\nclasses = df[COLOR_COL].unique()\ncolor_palette = sns.color_palette('deep', len(classes)).as_hex()\ncolor_map = {c: color_palette[i] for i, c in enumerate(classes)}\n\n# Put TRIMAP embeddings into dataframe & attach question information\ntrimap_df = pd.DataFrame({\n    'Component 1': trimap_embedding[:, 0], \n    'Component 2': trimap_embedding[:, 1],\n    COLOR_COL: df[COLOR_COL]\n})\n\n# Visualize the embeddings in 2D plot\nfig = px.scatter(\n    trimap_df, \n    x='Component 1', y='Component 2', \n    color=COLOR_COL,\n    color_discrete_map=color_map,\n    opacity=0.8\n)\nfig.update_traces(\n    marker=dict(size=5, line=dict(width=1, color='DarkSlateGrey')),\n    selector=dict(mode='markers')\n)\nfig.add_trace(\n    go.Scattergl(\n        mode='markers',\n        x=[trimap_df.loc[0, 'Component 1']],\n        y=[trimap_df.loc[0, 'Component 2']],\n        name='My Answers',\n        marker=dict(\n            color='red',\n            size=20,\n            line=dict(\n                color='DarkSlateGrey',\n                width=2\n            )\n        ),\n        showlegend=True\n    )\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've colored the plots by years of coding experience - it's clear there's a long tail of un-filled responses, as well as a tail of \"I have never written code\" responses - these seem to be more \"unique\", in a sense. Interestingly, I find myself in the big pile of respondees with different experience levels. Could be fun to dig deeper to see if there are some groupings of people, which may align more or less with my (or yours, if you fork this notebook) responses.\n\n## Clustering\nI'll use a simple KMeans to cluster in the L2 normed space, restricting focus to those respondees that have written code before, i.e. ignore the long tails seen in the TriMAP embedding. First, let's find how many clusters to use, which I do by scoring the KMeans algorithm with different amounts of clusters."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def _scoreKMeans(X: pd.DataFrame, n_clusters: int) -> Tuple[float, float]:\n    \"\"\"Score KMeans with a given set of clusers on a dataset X. Return score, time_spent\"\"\"\n    start = time.time()\n    kmeans = KMeans(n_clusters=n_clusters)\n    return -kmeans.fit(X).score(X), time.time() - start\n\n\n# Get the respondees that have written code before\nidx = (df.Q6 != '') & (df.Q6 != 'I have never written code')\n\n# Perform KMeans with different numbers of clusters\ncluster_ranges = np.arange(1, 30, 3)\n       \n# Run the KMeans clusterings with different clustering ranges\ntuples = Parallel(n_jobs=-1)(\n    delayed(_scoreKMeans)(result[idx], i) for i in cluster_ranges\n)\ncluster_scores, cluster_times = zip(*tuples)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Color scheme\ncolors = sns.color_palette('deep', 2).as_hex()\n\n# Create plot with secondary y axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Create plot\nfig.add_trace(\n    go.Scatter(\n        x=cluster_ranges, \n        y=cluster_scores,\n        mode='lines+markers',\n        marker=dict(\n            color=colors[0]\n        )\n    ),\n    secondary_y=False,\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=cluster_ranges, \n        y=cluster_times,\n        mode='lines+markers',\n        marker=dict(\n            color=colors[1]\n        )\n    ),\n    secondary_y=True,\n)\n\n# Set axes titles\nfig.update_yaxes(\n    title_text=\"<b>Sum of Squared Errors</b>\", \n    title_font=dict(color=colors[0]),\n    secondary_y=False\n)\nfig.update_yaxes(\n    title_text=\"<b>Clustering duration (seconds)</b>\", \n    title_font=dict(color=colors[1]),\n    secondary_y=True\n)\nfig.update_xaxes(title_text=\"<b>Number of Clusters</b>\")\n\n# Remove legend\nfig.update_layout(showlegend=False)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like about 15 clusters is reasonable - i.e. here the sum or errors does not increase as rapidly with more clusters anymore. Let's see how these clusters would be distributed in the TriMap plot from before (note clustering is done in the original feature space, and not in the 2D TriMap space)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Create clusterer & fit it to those who've written code\nclusterer = KMeans(n_clusters=15)\nclusterer.fit(result[idx])\n\n# Create a dataframe with all respondees & their assigned clusters for those who write code\ncluster_df = pd.DataFrame(result[idx], columns=feature_names)\ncluster_df['cluster'] = -1\ncluster_df['cluster'] = clusterer.labels_\n\n# Create cluster name\ncluster_df['cluster_name'] = cluster_df['cluster'].apply(lambda x: f'Cluster {x}')\ncolor_palette = sns.color_palette('deep', len(cluster_df.cluster.unique())).as_hex()\ncolor_map = {f'Cluster {x}': color_palette[x] if x >= 0 else 'lightgrey' for x in clusterer.labels_}\n\n# Put TRIMAP embeddings into dataframe & attach question information\ntrimap_df = pd.DataFrame({'Component 1': trimap_embedding[idx, 0], 'Component 2': trimap_embedding[idx, 1]})\ntrimap_df = pd.concat([trimap_df, cluster_df], axis=1)\n\n# Sort by cluster to put -1 first\ntrimap_df = trimap_df.sort_values('cluster')\n\n# Visualize the embeddings in 2D plot\nfig = px.scatter(\n    trimap_df, \n    x='Component 1', y='Component 2', \n    color='cluster_name',\n    color_discrete_map=color_map,\n)\nfig.update_traces(\n    marker=dict(size=5, line=dict(width=1, color='DarkSlateGrey')),\n    selector=dict(mode='markers')\n)\nfig.add_trace(\n    go.Scattergl(\n        mode='markers',\n        x=[trimap_df.loc[0, 'Component 1']],\n        y=[trimap_df.loc[0, 'Component 2']],\n        name='My Answers',\n        marker=dict(\n            color='red',\n            size=20,\n            line=dict(\n                color='DarkSlateGrey',\n                width=2\n            )\n        ),\n        showlegend=True\n    )\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like clustering in the L2-normed space translates well to the 2D representation from TriMap, but although the plot looks nice, it does not tell us too much, except for the fact that we're in a cluster with some random ID. \n\n### Interpreting the clusters\nTo remedy this lack of cluster interpretation, I'll try to see if we can determine which answers are more important for each cluster, and from that gain a bit more insight. The way I'm going to go about doing this is by setting up a random forest classification model to predict whether a given respondee belongs to a given cluster or not. From the model fit, I then extract the feature importances for each feature, and with that see which features are important for a given cluster classification.\n\nUsing this approach I can also estimate how much I actually belong to any given cluster (i.e. with `predict_proba`), which is pretty neat."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\ndef getKeywords(cluster_id):\n    \n    # Let's try to predict this cluster based on features in the dataset\n    X = (cluster_df[feature_names] > 0).astype(int)\n    y = (cluster_df.cluster == cluster_id).astype(int)\n    \n    # Fit a logistic regression for predicting the cluster\n    model = ExtraTreesClassifier(n_jobs=-1)\n    model.fit(X.iloc[1:], y.iloc[1:])\n    \n    # Get probability of this notebook answers belonging to cluster\n    proba = model.predict_proba(X.iloc[[0]])[0, 1]\n    \n    # Get the 20 most important features for classifying this cluster\n    sorted_features = X.columns[np.argsort(model.feature_importances_)]\n    \n    # Get keywords either just as column name, or as entry from OHE columns\n    def _cleanFeatures(features):        \n        keywords = [[e for e in df[f].unique() if e not in [1, '']][0] if '_Part_' in f else f for f in features]\n        keywords = [f for f in keywords if f not in ['', 'No / None', 'None']]\n        return ', '.join(keywords)\n    \n    # Get the most positive keywords & most negative keywords\n    keywords = _cleanFeatures(sorted_features[-10:])\n    if not keywords:\n        keywords = 'Users did not fill, or mostly filled None, No/None in most answers'\n        \n    # Return cluster keywords, and probability of belonging to it\n    return keywords, proba\n\n# Get the number of respondees in each cluster\ncluster_counts = cluster_df[cluster_df.cluster >= 0].cluster.value_counts()\n\n# Create dataframe with keywords for each cluster\ndata, probas = [], []\nfor cluster_id, n_responses in cluster_counts.items():\n    keywords, proba = getKeywords(cluster_id)\n    data.append({\n        'Cluster': cluster_id,\n        'High Frequency Words': keywords,\n        'Respondees': n_responses,\n        'Notebook Answers [%]': int(proba*100)\n    })\n    probas.append(proba)\ndata = pd.DataFrame(data).sort_values('Cluster')\n\n# Display the resulting dataframe & mark notebook \n# answers with color based on probability\ndisplay(\n    data\n    .style\n    .background_gradient(subset='Notebook Answers [%]')\n    .hide_index()\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking my own answers in the notebook widget, with this I can conclude that I'm part of the clusters using GPUs, various deep learning networks, as well as various cloud technologies, which fits pretty well. \n\n# Kaggle Archetypes\nNow that we've ventured into clustering of people, it could be interesting to take it a bit further and see if we can determine some underlying archetypes of kagglers. My thought on this is that if we perform a principal component analysis (PCA) on our featurized dataset, we should be able to pick up components which are *orthorgonal* to each other; it could be interesting to investigate the largest of these components, as they may reflect overarching types within the community. First up, let us run the PCA on the dataset, to see how much variance is described by each component. I'll again restrict focus to those who have written code before."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Get the respondees that have written code before\nidx = (df.Q6 != '') & (df.Q6 != 'I have never written code')\n       \n# Run PCA\npca = PCA(random_state=2020)\ntrafo = pca.fit_transform(result[idx])\n\n# Get the cumulative explained variance\ncumulative_sum_ratios = np.cumsum(pca.explained_variance_ratio_)\ncomponents_list = list(range(len(cumulative_sum_ratios)))\n\n# Figure out how many components to explain 75% of variance\nidx = np.searchsorted(cumulative_sum_ratios, 0.75)\n\n# Show the plot\nfig = go.Figure(data=go.Scatter(\n    x=components_list,\n    y=cumulative_sum_ratios, \n    showlegend=False,\n    mode='lines+markers',\n    marker_color=color_palette[0]\n))\n\nfig.add_trace(\n    go.Scatter(\n        mode='lines',\n        x=components_list,\n        y=[cumulative_sum_ratios[idx]] * len(cumulative_sum_ratios),\n        name=f'75% variance explained with {idx} components',\n        showlegend=True,\n        line=dict(color='firebrick', width=2, dash='dash'),\n    )\n)\n\nfig.update_layout(\n    xaxis_title='Principal Components',\n    yaxis_title='Cumulative explained variance',\n    legend=dict(\n        x=0,\n        y=1.1,\n        traceorder='normal',\n        font=dict(size=12),\n    ),\n)\nfig.show()\n\nprint(f\"First three components: {pca.explained_variance_[0:3]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My main takeway from this is that there are not a lot of super strong components that explain e.g. 30 or 20% of the variance - rather it actually takes a whopping 122 components to describe 75% of the dataset; I reckon that's indicative of a lot of individuality on Kaggle. Looking a bit close, though, the first three components do explain to 7.0%, 3.8%, and 2.7% of the variance, respectively. So let us now say that these are our underlying archetypes for kagglers, and see what they contain."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Calculate the PCA feature loadings (pick out PC1-3)\nloadings = pd.DataFrame(\n    pca.components_.T * np.sqrt(pca.explained_variance_),\n    columns=[f'PC{i}' for i in range(len(pca.components_))],\n    index=feature_names\n)[['PC1', 'PC2', 'PC3']]\n\n# Give better names to features\nname_map = {\n    c: '{}'.format(f.split(\" - Selected Choice - \")[-1].strip())\n    if 'Selected Choice' in f else raw.loc[0, c]\n    for c, f in raw.iloc[0].items()\n}\nloadings.index = [name_map[i] if i in name_map else i for i in loadings.index]\n\n# Remove all features where all absolute loading < 0.02\nloadings = loadings[loadings.apply(lambda row: row.abs().max() > 0.02, axis=1)]\n\n# Remove meaningless features\nloadings = loadings[~loadings.index.isin(['None', 'Never', '', 'No / None'])]\n\n# Show heatmap plot\n_, ax = plt.subplots(1, 1, figsize=(10, 20))\nsns.heatmap(loadings, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So with that we can conclude the following for the archetypes:\n* Archetype 1 (7% of variance): High correlation with lots of cloud & ML technologies. Also anticorrelated with \"<1 year\", which is consistent with this type being more experienced. \n* Archetype 2 (3.8% of variance): This seems like a archetype that covers people not using machine learning methods, but who are more correlated with SQL, Tableau, PowerBI, and where their companies have not invested into ML.\n* Archetype 3 (2.7% of variance): This seems to be a more beginner level archetype, where development mostly happens locally and with various ML technologies"},{"metadata":{},"cell_type":"markdown","source":"# What is a normal Kaggler?\nFinally, having done all the above analyses, it could be interesting to look at the answers from the 'least' unique respondee - i.e. the person with the highest average cosine similarity to all other kagglers, just to see who the most stereotypical kaggler is :)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"name_map = {\n    c: '{}'.format(f.split(\" - Selected Choice - \")[0].strip())\n    if 'Selected Choice' in f else raw.loc[0, c]\n    for c, f in raw.iloc[0].items()\n}\n\nperson = df.loc[np.argmax(scores)]\nperson = pd.DataFrame({'Average Joe': person})\nperson = person[person['Average Joe'] != \"\"]\nperson.index = person.index.map(name_map)\nperson","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the most typical kaggler is relatively new to coding (1-2 years) and machine learning (<1 year), lives in india, uses python, SQL and knows some C/C++/Java, typically uses a personal computer/laptop, and hopes to familiarize himself with one of the three main cloudforms within the next 2 years. Makes sense to me; could be interesting to see if there has been a shift in this \"stereotypical\" kaggler over time."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}