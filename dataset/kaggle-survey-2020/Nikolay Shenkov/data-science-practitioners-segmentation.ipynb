{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from pathlib import Path\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport altair as alt\nfrom IPython.display import Image\n\nimport umap\nimport umap.plot\nfrom sklearn.mixture import GaussianMixture\n\n%config InlineBackend.figure_format = 'retina'\nalt.data_transformers.disable_max_rows();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Science Practitioner Segmentation\n\n* [Introduction](#intro)\n    - [What does a segment look like?](#segment-example)\n    - [Professionals and non-professionals](#prof-nonprof)\n* [Preprocessing](#preprocess)\n    - [An important choice!](#important-choice)\n* [Clustering](#clustering)\n    - [UMAP projection](#umap)\n    - [How about the number of clusters?](#num-clusters)\n* [Analysis](#analysis)\n    - [Cluster lift](#lift)\n* [Segmentation summary](#segm-summary)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"intro\"></a>\n## Introduction\n\nData science / ML practitioners tackle a large variety of tasks at work, from analyze data to come up with business insights to deploying ML models in cloud environments. In this notebook we try to tease out some of this complexity by grouping similar users together based on their responses to the Kaggle ML and DS survey. Some of the questions we tackle are:\n\n* what kind of problems do different segments focus on at work\n* what are the primary software tools (libraries, frameworks, cloud services) they use to solve these problems\n* what is the relationship between level of experience and types of problem at work\n\nImportantly, our focus is on **interpretable** segments.\n\n\n<a id=\"segment-example\"></a>\n### What does a segment look like?\n\nWe will use UMAP for dimensionality reduction and Gaussian Mixture models for clustering using the responses to most of the questions in the survey. We end up with a total of 9 user segments. We then create profiles that emphasize unique features in each cluster. Here is an example:\n\n**Cluster 5: Computer Vision**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/example-cluster/cluster_5_profile.png', width=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bars on the left (in lighter color) show the percent of users that have selected a given response to a question. For example, the top bar shows that about 40% of users have selected Convolutional Neural Networks (CNNs) as an answer to Q17 *Which of the following ML algorithms do you use on a regular basis?* The bars on the right in solid color show the **lift**, or the increase in response rate specific **to the given cluster** compared to the overall average in the dataset. This means that **an additional** 50% in Cluster 5 have selected CNNs as an answer as compared to the baseline, for a whopping 90% in total in this cluster.\n\nWe will go into more details about the segmentation in the analysis section. Here is my summary for this cluster (based on the visualization above and more analysis further down):\n\n> More than 90% of the users in this cluster use Convolutional Neural Networks to tackle various problems in image classification, object detection and segmentation. They are most likely using Tensorflow (78%) and/or Keras (73%). Interestingly, in addition to Python, more than a quarter of the users utilize C++, presumably for high-performance computer vision applications. \n>\n> These users utilize GPUs, but more than half do not use a cloud platform.\nThis is the group with highest proportion of PhDs (25%) and Research Scientists as the job role (19%). Other common job roles are Data Scientist, ML Engineer, Software Engineer. In terms of ML experience, this cluster falls in the middle, with about half the users having less than 2 years of experience with ML models. Users in this cluster are more likely to do research to advance ML methods compared to other segments. \n\nIf you want to jump to the rest of the cluster summaries, jump over to the **Segmentation summary** section near the bottom."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"prof-nonprof\"></a>\n### Professionals and non-professionals\n\nBefore we go any further, let's look at a projection of the survey dataset in two dimensions using UMAP: this will give us a bird's-eye view of the dataset. We will go into all the technical details below, but for now, let's examine projection. Each point represents a survey participant, and points that are closer together have similar responses. Points are colored based on the participant job role (Q5)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image('../input/segmentation-mappng/segmentation_map.png', width=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You will notice two main groups, one on the left, and one on the right, separated by a wide margin. This separation is in fact driven by the survey structure itself. From the survey document:\n\n> Non-professionals received questions with an alternate phrasing (questions for non-professionals asked what tools they hope to become familiar with in the next 2 years instead of asking what tools they use on a regular basis). Non-professionals were defined as students, unemployed, and respondents that have never spent any money in the cloud.\n\nIt is not surprising that many of the respondents in the group on the right (non-professionals) are students. On the other hand, the professional group on the left includes data scientists, analysts, machine learning engineers and more!\n\nMy focus is on understanding data science practitioners at work, so for the rest of the analysis I will use the professionals group on the left and perform segmentation on it. I am sure the group on the right will also provide lots of insights, and I invite others to analyze that subset as well. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"preprocess\"></a>\n## Preprocessing\n\nBelow is my code for preprocessing. Basically, this consists of:\n* selecting which questions to use for clustering, and which to use for validation / analysis \n* encoding each response to a question as a binary variable (but note that some questions are multi-option select, while others are single-option select)\n* adding annotations to the questions as well as the responses to make them easier to analyze, e.g. `Q14 -> viz libraries`, and `Q14_Part_2 -> seaborn`"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"DATA_DIR = Path('../input/kaggle-survey-2020/')\ndf = pd.read_csv(DATA_DIR / 'kaggle_survey_2020_responses.csv', skiprows=[1])\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"important-choice\"></a>\n### An important choice!\n\nWe need to select questions we will include in the clustering (`cluster_qs`) and questions we will use later to study and validate the clustering solution (`valid_qs`).\n\nMost of the questions in this survey ask about things data science practitioners do at work: types of problems they solve, frameworks they use, models they build. So we will use the responses to these questions for clustering. For the full list of questions included, you can open the code cell below.\n\nThe questions selected for analysis (post-clustering) relate to the users' prior experience (years of coding experience, formal education), as well as work environment (company size, company using ML methods). This selection will allow us to make conclusions such as: *Users from cluster X focus on building deep learning prototypes using Tensorflow. Most of them have at least 2 years of experience working with ML models.*\n\nWe do not include questions that relate to compensation in the analysis as these depend on the country and a lot of extraneous factors. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# A mapping between the question and a short description\n# question: (short_description, question_type)\nshort_qs = {\n    'Q2':  ('gender', 'valid'),\n    'Q4':  ('highest edu formal', 'valid'),\n    'Q5':  ('most similar role', 'valid'),\n    'Q6':  ('writing code years', 'valid'),\n    'Q7':  ('languages regular', 'program'),\n    'Q8':  ('language recommend', 'program'),\n    'Q9':  ('IDEs regular', 'program'),\n    'Q10': ('hosted notebooks', 'cloud'),\n    'Q11': ('compute platform', 'cloud'),\n    'Q12': ('hardware', 'tools'),\n    'Q13': ('used TPU', 'tools'),\n    'Q14': ('viz libraries', 'program'),\n    'Q15': ('years ML', 'valid'),\n    'Q16': ('ML frameworks', 'ML'),\n    'Q17': ('ML algos', 'ML'),\n    'Q18': ('CV methods', 'ML'),\n    'Q19': ('NLP methods', 'ML'),\n    'Q20': ('company size', 'valid'),\n    'Q21': ('num. DS individuals', 'valid'),\n    'Q22': ('employer ML', 'valid'),\n    'Q23': ('work activities', 'work'),\n    'Q25': ('cloud money', 'valid'),\n    'Q26': ('cloud platforms', 'cloud'),\n    'Q27': ('cloud compute', 'cloud'),\n    'Q28': ('cloud ML', 'cloud'),\n    'Q29': ('big data regular', 'tools'),\n    'Q30': ('big data most often', 'tools'),\n    'Q31': ('BI tools regular', 'tools'),\n    'Q32': ('BI tools most often', 'tools'),\n    'Q33': ('autoML tasks', 'ML'),\n    'Q34': ('autoML tools', 'ML'),\n    'Q35': ('ML experiments', 'ML'),\n    'Q36': ('share deploy apps', 'tools'),\n    'Q37': ('DS courses', 'learn'),\n    'Q38': ('primary tool', 'tools'),\n    'Q39': ('media sources DS', 'learn'),\n}\n\nfor q, v in short_qs.items():\n    assert len(v) == 2, v\n    \n# types of questions, will be used for coloring later\nqtypes = ['cloud', 'ML', 'program', 'tools', 'work', 'learn', 'valid']\nassert set(qtypes) == set(v[1] for v in short_qs.values())\n# this mapping will be used for plotting, 2 colors per question type\nqtype_map = {qtype: i * 2 for i, qtype in enumerate(qtypes)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_qs = [q for q, v in short_qs.items() if v[1] != 'valid']\nvalid_qs =   [q for q, v in short_qs.items() if v[1] == 'valid']\nvalid_qs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{q: short_qs[q][0] for q in valid_qs}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random sample of questions used for clustering\n{q: short_qs[q] for q in random.sample(cluster_qs, 5)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `uniques` dictionary will keep track of unique responses per column (one-hot encoded), e.g.\n\n```\n{\n    'Q7_Part_1': 'Python',\n    'Q7_Part_2': 'R'\n }\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_uniques_dict(df):\n    \"\"\"Make a dictionary mapping a given multiple-selection option to the actual text response.\n    e.g. 'Q7_Part_1' -> 'Python'\n    \"\"\"\n    uniques = {}\n    for col in df.columns:\n        if df[col].nunique() == 1:\n            # keep track of the response text - to be used in analysis\n            uniques[col] = df[col].dropna().unique()[0].strip()\n    return uniques\n            \nuniques = make_uniques_dict(df)\n\n# convert each column (corresponding to a single selection) to 0 / 1 col\nfor col in uniques:\n    df[col] = df[col].notnull().astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# some of the categorical questions have an ordering so we assign the order manually\n# used for visualizations later\nordered_cats = {\n    'Q4': [\n        # somewhat arbitrary how we order the education degrees\n        'I prefer not to answer',\n        'No formal education past high school',\n        'Some college/university study without earning a bachelor’s degree',\n        'Bachelor’s degree',\n        'Master’s degree',\n        'Professional degree',\n        'Doctoral degree'\n    ],\n    'Q6': [\n        'I have never written code',\n        '< 1 years',\n        '1-2 years',\n        '3-5 years',\n        '5-10 years',\n        '10-20 years',\n        '20+ years'\n    ],\n    'Q15': [\n        'I do not use machine learning methods',\n        'Under 1 year',\n        '1-2 years',\n        '2-3 years',\n        '3-4 years',\n        '4-5 years',\n        '5-10 years',\n        '10-20 years',\n        '20 or more years'\n    ],\n    'Q20': [\n        '0-49 employees',\n        '50-249 employees',\n        '250-999 employees',\n        '1000-9,999 employees',\n        '10,000 or more employees',\n    ],\n    'Q21': ['0', '1-2', '3-4', '5-9', '10-14', '15-19', '20+'],\n    'Q22': [\n        'No (we do not use ML methods)', \n        'I do not know',\n        'We are exploring ML methods (and may one day put a model into production)',\n        'We use ML methods for generating insights (but do not put working models into production)',\n        'We recently started using ML methods (i.e., models in production for less than 2 years)',\n        'We have well established ML methods (i.e., models in production for more than 2 years)',\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def col_to_q(col):\n    return col.split('_')[0]\n\n# drop OTHER responses since we cannot correlate them with a segment\ndf = df[[col for col in df.columns if 'OTHER' not in col]]\n\n# keep only questions for analysis / validation\ndf = df[[col for col in df.columns if col_to_q(col) in short_qs]]\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def object_cols_to_cats(df, ordered_cats):\n    \"\"\"Convert any columns with dtype = object to categorical.\n    If a column contains ordered categories (specified in the ordered_cats dict),\n    the order will be preserved in the resulting pandas categorical.\n    Modifies df inplace.\n    \n    Returns a dictionary which maps each a column to its categorical response.\n    For example: {\n       'Q6_0': 'I have never written code',\n       'Q6_1': '< 1 years',\n       ...\n    }\n    \"\"\"\n    uniques_cat = {}\n    for col in df.select_dtypes('object').columns:\n        q = col_to_q(col)\n        cats = ordered_cats.get(q)\n        if cats is not None:\n            assert set(cats) == set(df[col].dropna())\n            df[col] = pd.Categorical(df[col], ordered=True, \n                                      categories=ordered_cats[col])\n        else:\n            df[col] = pd.Categorical(df[col])\n\n        for i, c in enumerate(df[col].cat.categories):\n            # uniques_cat['Q6_1'] = '< 1 years'\n            uniques_cat[f'{q}_{i}'] = c\n            \n    assert len(df.select_dtypes('object').columns) == 0\n    return uniques_cat\n    \nuniques_cat = object_cols_to_cats(df, ordered_cats)\n# merge the uniques + uniques_cat dicts\nuniques = {**uniques, **uniques_cat}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.to_feather('processed_responses.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cl_df only contains questions used for clustering\ncl_df = df[[col for col in df if col_to_q(col) in cluster_qs]].copy()\ncl_df.shape, df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_cols_to_dummies(cl_df):\n    \"\"\"onvert categorical columns to dummy\"\"\"\n    cat_cols = cl_df.select_dtypes('category').columns\n    for col in cat_cols:\n        cl_df[col] = cl_df[col].cat.codes\n\n    cl_df = pd.get_dummies(cl_df, columns=cat_cols)\n    return cl_df\n    \ncl_df = cat_cols_to_dummies(cl_df)\n# drop '-1 columns' corresponding to missing values\ncl_df = cl_df[[c for c in cl_df if '-1' not in c]]\ncl_df = cl_df.astype(np.int8)\n\n# check to ensure each col is mapped to a value\nfor col in cl_df:\n    assert col in uniques, col\n    \n# some users have not responded to any of the selected qs so we drop them\ncl_df = cl_df[cl_df.mean(axis=1) > 0.01]\ncl_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"clustering\"></a>\n## Clustering"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"umap\"></a>\n### UMAP Projection\n\n[UMAP](https://umap-learn.readthedocs.io/) is a very useful nonlinear dimensionality reduction technique and it can give insights into wide range of different datasets, provided it is configured correctly.\n\n#### Jaccard coefficient as a distance measure\n\nPerhaps the most important UMAP parameter is the distance measure. Our dataset includes binary features only so the Jaccard coefficient is a good choice. It is defined as the size of the intersection of two sets $u$ and $v$, divided by the size of their union:\n\n$$J(u, v) = \\frac{|u \\cap v|}{|u \\cup v |}$$\n\n\nWikipedia has a nice [graphic](https://en.wikipedia.org/wiki/Jaccard_index) of the Jaccard coefficient. In our case, the numerator will count the number of matches (shared selections) between users $u$ and $v$. The denominator will normalize this count by the total number of unique selections of both $u$ and $v$. It is easier to have more matches with a user that has made a lot of positive selections, so we need the denominator to control for this effect."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\njob_roles = df.loc[cl_df.index, 'Q5']\nmapper = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='jaccard', random_state=0)\nmapper.fit(cl_df);\n\n# uncomment this line to create the UMAP projection from the introduction section\n# throws an error with the Kaggle version of umap-learn, but visualization is still generated\n# umap.plot.points(mapper, labels=job_roles, color_key_cmap='tab20', width=700, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's study the professionals!"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://upload.wikimedia.org/wikipedia/en/0/03/Leon-poster.jpg\" align=\"center\"/>"},{"metadata":{},"cell_type":"markdown","source":"We will split the projection into two, and select the professionals for further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prof_df(mapper, cl_df, boundary_x):\n    \"\"\"Get df with professionals only by splitting the UMAP\n    projection based on a boundary value.\"\"\"\n    \n    proj = mapper.transform(cl_df)\n    prof_df = cl_df[proj[:, 0] < boundary_x]\n    plot_proj_with_boundary(proj, boundary_x)\n    return prof_df\n\ndef plot_proj(proj, i0, i1, ax=None, alpha=0.2, s=5, **kwargs):\n    \"\"\"Plot 2D UMAP projection for components i0 and i1.\"\"\"\n    ax = ax or plt.gca()\n    ax.scatter(proj[:, i0], proj[: , i1], alpha=alpha, s=s, **kwargs)\n    ax.set_xlabel(f'UMAP component {i0}', fontsize=12)\n    ax.set_ylabel(f'UMAP component {i1}', fontsize=12)\n    return ax\n\ndef plot_proj_with_boundary(proj, boundary_x):\n    \"\"\"Plot projection together with boundary value.\"\"\"\n    plt.figure(figsize=(8, 6))\n    ax = plot_proj(proj, 0, 1, ax=plt.gca())\n    ymin, ymax = ax.get_ylim()\n    ax.vlines(boundary_x, ymin, ymax, linestyle='dashed')\n    return ax\n    \n    \nprof_df = get_prof_df(mapper, cl_df, boundary_x=4)\nprof_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For the rest of the analysis, we focus on professionals (group on the left).\n\nFor clustering, we are going to refit UMAP on the professionas data only. We will use the UMAP-transformed data (rather than the sparse binary data) as input to the clustering algorithm. However, we will increase the number of UMAP components (dimensions) to 4, so we can capture more information in our projection. There is a discussion on using UMAP as a clustering preprocessing step [here](https://umap-learn.readthedocs.io/en/latest/clustering.html). "},{"metadata":{"trusted":true},"cell_type":"code","source":"prof_mapper = umap.UMAP(n_components=4, n_neighbors=15, min_dist=0, \n                        metric='jaccard', random_state=0)\nprof_mapper.fit(prof_df)\ntrans_prof = prof_mapper.transform(prof_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n# plot a few 2D projections\nplot_proj(trans_prof, 0, 1, ax=axes[0])\nplot_proj(trans_prof, 2, 3, ax=axes[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the 2D projections of the professionals, we can make a few observations: \n* some regions of the UMAP space are more dense than others\n* high-density regions are not spherical\n* high-density regions are not well separated from each other with one exception in the bottom left corner. Large separation between regions tends to be driven by the survey structure (as opposed to an inherent user segmentation), as we saw above in the projection of the full survey dataset.\n\nBased on this, we will use Gaussian Mixture with full covariance matrix for clustering. This will allow us to capture the non-spherical regions (as opposed to say, using K-means).\n\nI tried several different algorithms (e.g. spectral clustering on the raw data), and they all roughly agreed on the segments, which gives us some confidence about the results."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"num-clusters\"></a>\n### How about the number of clusters / segments?\n\n\nThere are a few important considerations when selecting the number of clusters for this type of tasks:\n* The overall purpose of this clustering is to enhance our understanding of the Kaggle userbase (as opposed to, e.g. feature engineering for an ML algorithm). Each cluster needs to be analyzed, so we can understand the segment of users it represents. We cannot use a large number of clusters, e.g. 100. We can limit ourselves to a few segments especially when getting started.\n* When doing the analysis, it is easy to **overcluster** (pick more clusters than what we expect the right number is), and then manually merge similar clusters together. Going in the opposite direction is harder. \n\nWith this in mind, we will use the following recipe:\n* Start with 10 clusters\n* Go through each of them and try to interpret the user segment it represents\n* Any clusters that are judged very similar to each other can be merged together."},{"metadata":{"trusted":true},"cell_type":"code","source":"clusterer = GaussianMixture(n_components=10, random_state=0)\nclusterer.fit(trans_prof)\ncl_labels_prof = clusterer.predict(trans_prof)\n# show the cluster sizes\nnp.unique(cl_labels_prof, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the clusters are particularly large or small, so we obtain a balanced solution.\n\nLet's plot a few 2D projections (colored by cluster label) to examine the solution qualitatively."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\nplot_proj(trans_prof, 0, 1, ax=axes[0], c=cl_labels_prof, cmap='tab10')\nplot_proj(trans_prof, 2, 3, ax=axes[1], c=cl_labels_prof, cmap='tab10');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some overlap between clusters near the borders, but overall, the regions of high density appear to be well-clustered. Note how the full covariance clustering allows us to capture non-spherical regions."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"analysis\"></a>\n## Analysis\n\nIt is time to actually make sense of our clustering. Below are some helper functions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def col_label(col, n_words=5):\n    \"\"\"Make a human-friendly (but short) label for a given column.\n    For example:\n    'Q26_Part_2' -> 'CV methods: Image segmentation methods (U-Net, Mask'\n    \"\"\"\n    answer_label = col_to_answer(col, n_words=5)\n    q = col_to_q(col)  # e.g. 'Q26'\n    q_label = short_qs[q][0]  # e.g. 'CV methods'\n    return q_label + ': ' + answer_label\n\ndef col_to_answer(col, n_words=5):\n    answer_words = uniques[col].split()\n    return ' '.join(answer_words[:n_words])  # limit answer to n_words\n\ndef col_to_qtype(col):\n    q = col_to_q(col)  # 'Q26'\n    qtype = short_qs[q][1]  # 'ML'\n    return qtype \n\n\nprint(col_label('Q26_A_Part_1'))\nprint(col_label('Q33_A_Part_2'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(col_to_qtype('Q26_A_Part_1'))\nprint(col_to_qtype('Q33_A_Part_2'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lift\"></a>\n### Cluster Lift\n\nSome responses to a question are inherently more popular than others. For example, for Q7 (programming languages regularly used), almost all users (87%) in the professionals dataset use Python. In comparison, only 16% use C++. If we only look at response rates per cluster, Python will drown all the other responses. \n\nIn order to emphasize differences between clusters, we use *cluster lift* - the difference between cluster response rate and overall response rate for a given selection. If we see a large lift for C++ for a given cluster, then we know that this cluster is using C++ more than the rest of the clusters. Meanwhile, we are unlikely to see large values of lift for Python because almost all clusters are using Python. In fact, we might see negative values in lift for clusters that tend to use Python less, which can be informative as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"prof_lift = prof_df - prof_df.mean()\ncenters_lift = prof_lift.groupby(cl_labels_prof).mean()\n\ndef cluster_top(idx, centers_df, n=15):\n    \"\"\"Given a cluster centers df, return the columns with largest value for a given cluster.\"\"\"\n    return centers_df.loc[idx].nlargest(n)\n\ncluster_top(0, centers_lift, n=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_lift_df(cl_idx, centers_lift, avg_resp, n=15):\n    \"\"\"Create a dataframe with the responses with highest lift for a given cluster.\n    \n    cl_idx: the cluster integer index\n    lift_centers: the lift_centers dataframe with lift data\n    avg_resp: series with the overall average response\n    n: the number of responses to include in the result\n    \n    Returns a dataframe with top n questions with highest lift.\n    Also included is the overall average response (across all clusters)\n    for a given question.\n    \"\"\"\n    lift_top = cluster_top(cl_idx, centers_lift, n=n)\n    avg_top  = avg_resp.loc[lift_top.index]\n    res = pd.DataFrame({'lift': lift_top, 'overall_avg':  avg_top})\n    res['cluster_avg'] = res['lift'] + res['overall_avg']\n    res = res.reset_index().rename(columns={'index': 'q'})\n    res['qtype'] = res['q'].apply(col_to_qtype)\n    res.index =    res['q'].apply(col_label)\n    res.index.name = 'question_label'\n    return res\n\ndef get_qcolors(qtypes, primary):\n    \"\"\"Create the paired color coding for the question types.\n    This is used in the plot_segment_qs below.\n    If primary=True, we use a solid color, otherwise we use a light color.\"\"\"\n    paired = plt.get_cmap('Paired')\n    \n    def qtype_color(qtype):\n        \"\"\"Map a question type to a color, e.g.\n        'cloud' -> (0.650, 0.808, 0.890, 1.0)\n        \"\"\"\n        if primary: return paired(qtype_map[qtype] + 1)\n        else:       return paired(qtype_map[qtype])\n        \n    return [qtype_color(qtype) for qtype in qtypes]\n\ndef plot_segment_qs(lift_df, ax=None):\n    \"\"\"Create a bar plot for the questions with highest lift in a given cluster.\n    \n    We show both the lift as well as the overall avg response for a given question.\n    Questions are colored by the question type (e.g. cloud, ML, etc.)\n    \"\"\"\n    # reverse df to plot values with highest lift at top\n    lift_df = lift_df.iloc[::-1]  \n    if ax is None:\n        fig = plt.figure(figsize=(6, 7))\n        ax = fig.gca()\n        \n    idx = np.arange(len(lift_df))\n    ax.barh(idx, lift_df['lift'] * 100,\n            color=get_qcolors(lift_df['qtype'], primary=True))\n    ax.barh(idx, -lift_df['overall_avg'] * 100,\n            color=get_qcolors(lift_df['qtype'], primary=False))\n    ax.set_yticks(idx)\n    ax.set_yticklabels(lift_df.index, fontsize=12)\n    ax.set_xlabel('% users', fontsize=12)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here is the lift dataframe for cluster 0 with top 5 entries\nmake_lift_df(0, centers_lift, prof_df.mean(), n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can interpret the result for cluster 0 as follows. Let's take a look at the first row. Across all users (in the professional group), 25% use Amazon EC2. However, in cluster 0, this proportion is much higher: 25% (overall) + 37% (lift) = 62%. More than double!\n\nLet's take a look at the fourth row (\"hardware: GPUs\"). The overall response rate is higher (53%) while the lift is slightly smaller (32%). But that means that the majority of cluster 0 users use GPU: 53% + 32% = 85%.\n\nIt is important to consider both the lift, as well as the overall response rate, when analyzing a given question. So let's plot them together!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change cluster index to analyze each cluster in turn\ncl_idx = 5\nax = plot_segment_qs(make_lift_df(cl_idx, centers_lift, prof_df.mean(), n=15))\nax.set_title(f'Cluster {cl_idx}', fontsize=14)\nax.grid(axis='x')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As explained in the introduction, this chart shows the overall response rate as well as the cluster-specific lift for a given answer choice. The answers are color-coded based on categories (manually assigned earlier)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_analysis_df(df, valid_qs, cluster_labels):\n    \"\"\"Make the analysis df based on the validation questions and cluster labels.\"\"\"\n    analysis_df = df[valid_qs].assign(cluster=cluster_labels)\n\n    # ordering info for plotting\n    for name, col in analysis_df.iteritems():\n        if hasattr(col, 'cat') and col.cat.ordered:\n            analysis_df[f'{name}_order'] = col.cat.codes + 1\n    return analysis_df\n            \nanalysis_df = make_analysis_df(df.loc[prof_df.index], valid_qs, cl_labels_prof)\nanalysis_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start exploring the data in the validation questions (`analysis_df`).\n\nBelow we can see the distribution of job roles for the different clusters. Hover over the bars to see the actual counts.\nSince the cluster numbers are arbitrary, all charts are ordered based on the level of experience with ML for each cluster."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prop_new_to_ml(s):\n    \"\"\"Find the proportion of users that are new to ML model.\"\"\"\n    return ((s == 'Under 1 year') | (s ==  'I do not use machine learning methods')).mean()\n\n# sort clusters by years of experience in ML models\nsort_order = (analysis_df\n              .groupby('cluster')['Q15']\n              .apply(prop_new_to_ml)\n              .sort_values()\n              .index.tolist())\n\ndef plot_job_titles(data, q='Q5'):\n    return alt.Chart(data=data[[q, 'cluster']]).mark_bar(size=35).encode(\n        x=alt.X('cluster:N', sort=sort_order),\n        y=alt.Y('count()', stack='normalize', title='Proportion (per cluster)'),\n        color=alt.Color(q, scale=alt.Scale(scheme='tableau20')),\n        tooltip=[q, 'count()']\n    ).properties(width=500)\n\nplot_job_titles(analysis_df).properties(title='Job title most similar to current role')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stacked_bar_cluster(data, q):\n    \"\"\"Stacked bar for a given question with possibly order information.\n    \n    data: dataframe to plot, needs to contain the question q, and cluster label\n    q: name of the question, e.g. 'Q6'\n    \"\"\"\n    q_order_col = f'{q}_order'\n    if q_order_col in data.columns:\n        order = q_order_col\n        cols = [q, q_order_col, 'cluster']\n    else:\n        order = []\n        cols = [q, 'cluster']\n        \n    return alt.Chart(data=data[cols]).mark_bar(size=25).encode(\n        x=alt.X('cluster:N', sort=sort_order),\n        y=alt.Y('count()', title='Proportion (per cluster)', stack='normalize'),\n        # need to provide a list with ordered categories to display correctly\n        color=alt.Color(f'{q}:O', scale=alt.Scale(scheme='inferno'), \n                        sort=list(data[q].cat.categories)),\n        tooltip=[q, 'count()'],\n        # force an order on a categorical variable\n        order=order\n    ).properties(\n        width=400,\n        height=280,\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_bar_cluster(analysis_df, 'Q6').properties(title='Years writing code')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_bar_cluster(analysis_df, 'Q15').properties(title='Years using machine learning methods')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# stacked_bar_cluster(analysis_df, 'Q2').properties(title='Gender')\n# stacked_bar_cluster(analysis_df, 'Q4').properties(title='Education')\nstacked_bar_cluster(analysis_df, 'Q22').properties(title='Employer incorporating ML methods')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note how the experience with ML models (Q15) correlates with employer incorporating ML methods (Q22). "},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_bar_cluster(analysis_df, 'Q20').properties(title='Company size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not a large variation in the company size across clusters, but we can notice that cluster 4 users tend to be employed in larger companies than cluster 0 and 3 users (but are otherwise similar in terms of ML experience)."},{"metadata":{},"cell_type":"markdown","source":"**Q23** (select activities that make an important part of your work) is central to this analysis so it deserves its own table with lift coefficients. Note that we included this question in the clustering."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"role_avg = centers_lift.filter(like='Q23').loc[sort_order]\nrole_avg.columns = role_avg.columns.map(lambda c: col_to_answer(c, n_words=50))\nrole_avg.index.name = 'cluster'\nrole_avg.style.background_gradient(cmap='PiYG').format(\"{:.1%}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You will notice \"more green near the top\" - more experienced users in clusters 0, 4, 3, 7 are more likely to make a selection (or multiple selections) that matches one of the standard data science roles.  "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"segm-summary\"></a>\n## Segmentation summary\n\nYou can find below the summaries I wrote for each cluster, based on the charts and analysis above."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cl_summary = {\n    0: 'Advanced ML',\n    1: 'ML Beginners',\n    2: 'Business Intelligence',\n    3: 'Google Cloud and models in production',\n    4: 'ML on tabular data',\n    5: 'Computer Vision',\n    6: 'R Users',\n    7: 'Azure and R and more',\n    8: 'Getting started with data',\n    9: 'Getting started with data'\n}\n\ncl_counts = pd.Series(cl_labels_prof).value_counts().loc[sort_order]\n# fix cluster label\ncl_counts.index = cl_counts.index.map({k: f'{k}: {v}' for k, v in cl_summary.items()})\nax = cl_counts.iloc[::-1].plot(kind='barh', figsize=(6, 6))\nax.set_yticklabels(ax.get_yticklabels(), fontsize=12)\nax.set_title('Cluster size', fontsize=14)\nax.set_xlabel('Number of participants', fontsize=12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cluster 0: advanced ML\n\nCluster 0 includes some of the most experienced users in terms of machine learning: more than 75% of them have been using ML methods for more than 2 years. From the lift plots, we see that they use a wide range of ML methods, and focus particularly on deep learning: anything from convolutional networks for CV to NLP models. PyTorch, Keras and Tensorflow are used, and the proportions are roughly equal between the three frameworks. In terms of cloud services, AWS (and in particular EC2) receive the highest lift. \n\nThis cluster includes the highest proportion of ML Engineers (nearly 25%) compared to other clusters, and also Data Scientists, Software Engineers, and Research Scientists. These participants focus heavily on building ML prototypes to explore new areas (73%) and experimentation to improve existing ML models (63%). \n\n### Cluster 1: ML beginners\n\nUsers in Cluster 1 tend to use standard Python libraries (matplotlib, scikit-learn) in their local Jupyter environment. The majority of them (more than 70%) have <= 2 years of experience using machine learng methods. They tend not to use cloud ML services and AutoML tools. They utilize classic algorithms such as linear and logistic regression, random forests. \n\nIn terms of job roles, there is a mix of Software Engineers, Data Scientists, Data Analysts, with fewer proportions for the rest of the roles. About 75% of these participants say that their employer is either exploring or already using ML methods, which suggests that there will be future opportunities for them to gain more experience in the field. \n\n### Cluster 2: Business Intelligence\n\nCluster 2 users tend to use BI tools such as Tableau, Microsoft Power BI, Excel. This is the cluster with the largest proportion of employers not using ML (about half).\n\nIn terms of job roles, this group includes a larger proportion of Business and Data Analysts (about 30% together), and also the highest proportion of unspecified roles (\"Other\") - more than a quater!\n\nNote: these survey participants have selected \"I do not use ML methods\", which is why they received some questions that are differently worded. In the UMAP projections, you will notice how they are separated from the rest of the clusters. This is also the smallest cluster.\n\n### Cluster 3: Google Cloud and models in production\n\nThis cluster heavily utilizes Google Cloud Products: anything from GC AutoML to Google Cloud SQL. They use Tensorflow (85%) and Keras (73%), but also PyTorch (63%). These users also apply various autoML tools to their tasks, such as auto-sklearn. \n\nSimilar to cluster 0, these participants apply deep learning techiques to both CV and NLP problems. About 46% of the users say they do research to advance ML, which is the highest across all clusters! In addition, many cluster 3 members focus on putting models in production: 58% build and run the data infrastructure, and 57% build and run ML services. The distribution of job roles is also similar to that of cluster 0, where most users are Data Scientists, ML engineers, Data Analysts or Software Engineers. About 40% of the users are employed in small companies of size less than 50.\n\n### Cluster 4: ML on tabular data\n\nMost participants in Cluster 4 use regularly SQL (more than 75%) to query various databases and warehouses such as PostgresSQL, Redshift, Google BigQuery. They use ML models that are typically used on tabular data, such as gradient boosting (59%) or regression (85%) and the scikit-learn library (80%). They are cloud users (AWS at 72%), but do not utilize cloud ML services. They do not use specialized hardware like GPUs / TPUs. This cluster includes the most experienced coders: more than 65% of the users have more than 5 years of coding experience. \n\nThese are some of the most experienced users in terms of using ML methods and writing code - about 90% have more than 3 years of writing code. They tend to be employed in large companies, nearly half of them are employed in a company with more than 1000 people. Curiously, this is the group with the largest proportion of Data Science job roles (nearly half). The primary focus is analyzing data to influence business decisions (81%) and building ML prototypes in new areas (65%).\n\n### Cluster 5: Computer vision\n\nMore than 90% of the users in this cluster use Convolutional Neural Networks to tackle various problems in image classification, object detection and segmentation. They are most likely using Tensorflow (78%) and/or Keras (73%). Interestingly, in addition to Python, more than a quarter of the users utilize C++, presumably for high-performance computer vision applications. These users utilize GPUs, but more than half do not use a cloud platform.\n\nThis is the group with highest proportion of PhDs (25%) and Research Scientists as the job role (19%). Other common job roles are Data Scientist, ML Engineer, Software Engineer. In terms of ML experience, this cluster falls in the middle, with about half the users having less than 2 years of experience with ML models. Users in this cluster are more likely to do research to advance ML methods compared to other segments. \n\n### Cluster 6: R Users\n\nThis is the R Cluster! Nearly all participants in this cluster use R (in RStudio) and standard R packages such as ggplot2, Caret as well as Shiny (40%) for building and deploying apps. \n\nThis cluster has a higher percentage of Business and Data Analysts (nearly 30% in total) which is in line with the fact that many analysts use R. Perhaps not surpisingly, the majority of the Statisticians in the dataset fall under this cluster (but still represent a small proportion at 12%). The majority of users (more than 80%) focus heavily on analyzing data at work. Curiously, this is the cluster with the highest percentage of women (about 20% vs. 13% in the whole \"professionals\" dataset). \n\n### Cluster 7: Azure and R and more\n\nJust like Cluster 3 uses Google Cloud, Cluster 7 participants are more likely to use a set of tools from the Microsoft Azure platform, such as SQL Server (46%) and Power BI (52%). Interestingly, more than half of the participants in this cluster use R regularly. This might be related to the fact that Microsoft provides good support for R. In terms of ML experience, this cluster falls in the middle, with roughly half of the participants relatively new to ML methods. \n\nUsers in this cluster are involved in a variety of activities, with the most common being \"analyzing data to influence decisions\" (80%), and \"building ML prototypes to explore new areas\" (60%). In addition, nearly half of them also run data infrastructure. This is the largest cluster and also perhaps the most diverse in terms of work activities. So I further split it into two subgroups  - it is very easy to do by applying the functions defined above only on this cluster. I found that one subgroup is more focused on ML model research (Tensorboard, PyTorch, CNNs, autoML), whereas the other subgroup is focused on data analysis (R, RStudio, Azure Notebooks, ggplot2).\n\n### Clusters 8 and 9: Getting started with data\n\nI decided to merge these clusters together. It is challenging to infer much about these clusters since participants here selected \"None\" to many of the questions. A few exceptions: many participants selected \"using basic statistical tool (e.g. Excel)\", as well as \"regularly using SQL\". It is likely that many of these users have non-data-science primary work responsibilities, and are trying to incorporate some data analysis / data science techniques into their work. \n\nIn terms of experience with ML methods, most of the participants in these clusters are beginners (and slightly more so in cluster 8).  Cluster 8 has the highest percentage of software engineers across clusters (above 30%), which might explain why Javascript was often picked as one of the regularly-used languages.    "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}