{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> New York Taxi Fares Prediction </center>\n## <center> Supervised Learning with XGBoost </center>"},{"metadata":{},"cell_type":"markdown","source":"![NY taxis](https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iaFilM3php7g/v1/-1x-1.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nHi Kagglers,\n\nHere I come with another notebook. This time trying to predict the New York taxi rides fares, given the pickup, dropoff locations, and some other features. We will stick with the XGBoost algorithm, which gives very good predictions for this specific data after we perform some interesting feature engineering and exploration of the dataset. Check it out!\n\nTaxicabs are, and always will be, an iconic part of New York. The history of their characteristic yellow color goes as follows: owners of cab companies painted their fleets a distinct signature color, resulting in cabs ranging from brown, white, red, and even checker ones. And some were yellow. After a few years, two big cab companies decided that yellow was the way to go, with both ultimately contributing to the tradition of yellow cabs in New York City. These companies were the Yellow Cab Company, started by John Hertz in Chicago in 1910, and the Yellow Taxicab Company, which was incorporated in New York by Albert Rockwell in 1912. More information on this can be found [here](https://untappedcities.com/2017/07/12/nyc-fun-facts-why-are-most-nyc-taxi-cabs-yellow/).\n\nMy notebook has been inspired by others on the same topic, which I enlist hereunder. My recognition to them and recommendation to take them a look:\n\n- Ravi tanwar's Data Cleaning + Eda + Modelling: https://www.kaggle.com/ravijoe/data-cleaning-eda-modelling\n\n- Jes√∫s Ros' XGBoost'ing Taxi Fares: https://www.kaggle.com/gunbl4d3/xgboost-ing-taxi-fares\n\n- Vinod R's EDA + XGBoost For Predicting Fare Amount: https://www.kaggle.com/vinodsunny1/eda-xgboost-for-predicting-fare-amount\n\n- AlexS2020's Rapdis and XG Boost running on GPU: https://www.kaggle.com/alexs2020/rapdis-and-xg-boost-running-on-gpu\n\n- Nicapotato's Taxi Rides Time Analysis and OOF LGBM: https://www.kaggle.com/nicapotato/taxi-rides-time-analysis-and-oof-lgbm\n\nRemember to upvote if you really enjoyed it. And feel free to comment, suggest or even complain in the comment section. Check my other notebooks, which are also great.\n\nCheers!"},{"metadata":{},"cell_type":"markdown","source":"## Index\n\n[The data](#section0)\n\n1. [Load the libraries](#section1)\n2. [Load the dataset](#section2)\n3. [Basic exploration](#section3)\n4. [Dataset Cleaning](#section4)\n5. [Feature engineering](#section5)\n6. [Further EDA](#section6)\n7. [Model training](#section7)\n8. [Predictions](#section8)"},{"metadata":{},"cell_type":"markdown","source":"## <a id=secion0>The data</a>"},{"metadata":{},"cell_type":"markdown","source":"The data is from a Kaggle's Playground Prediction Competition, it can be found [here](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data).\n\nThe main signature of this dataset is its massive size, counting with 55 million observations in its train set. Therefore, it is vital to handle this in some way, in order to build a decent model and don't convert your computer into a toaster at the same time. Particularly we will subset the set with 7 million rows, an adequate and more than enough amount of data to get a good performance.\n\nApart from this, the data is extremely cleaned and simple in their dimensions, having only 6 features, an id column, and the dependent variable of the fare amount. These are the attributes:\n\n    pickup_datetime - timestamp value indicating when the taxi ride started. \n    pickup_longitude - float for longitude coordinate of where the taxi ride started.\n    pickup_latitude - float for latitude coordinate of where the taxi ride started.\n    dropoff_longitude - float for longitude coordinate of where the taxi ride ended.\n    dropoff_latitude - float for latitude coordinate of where the taxi ride ended.\n    passenger_count - integer indicating the number of passengers in the taxi ride. "},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section1\">1. Load the libraries</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For processing the data\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Visualization tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"white\") # set style for seaborn plots\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Ignore warnings\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Time-related functions\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section2\">2. Load the dataset</a>\n\nWe will load our train dataset from the downloaded csv file. Since, as stated, this dataset has 55 million rows, we will set the `nrows` parameter to 7M to prevent memory issues and speed up everything. Considering there is no order among the observations there is no need to randomize this selection of rows.\n\nFeel free to adapt the `nrows` in accordance with your computer capabilities or the performance of the model you want to attain, if you only want to read and learn something from the notebook you can low it down to 1M for example."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/new-york-city-taxi-fare-prediction/train.csv\", nrows = 7000000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section3\">3. Basic exploration</a>\n\nNow we will explore the loaded data and see whether we find problems that require some sort of fixing, such as missing values, imbalanced data, inconsistent observations, outliers, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimensions of our training set: \", data.shape)\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have the first 5 rows of our data. With them, we can become familiar with all the features,  their values, and have a solid representation of how our set looks like."},{"metadata":{},"cell_type":"markdown","source":"> "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have the distribution of our dependent variable:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(8,5))\nsns.distplot(data[\"fare_amount\"], kde=True, color=\"#fdb813\")\nplt.xlim(0, 700)\nplt.ylim(0, 0.08)\nplt.title(\"Distribution of the target: fare_amount\")\nplt.xlabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We won't remove anything for now, but it will be positive to know if whether or not we have outliers in our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"q1  = data['fare_amount'].quantile(0.25)\nq3  = data['fare_amount'].quantile(0.75)\niqr =  q3 - q1\nprint(\"Fare Amount lower bound : \", q1 - (1.5 * iqr), \n      \"Fare Amount upper bound : \", q3 + (1.5 * iqr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total null values:\\n\", data.isnull().sum())\nprint(\"Percentage of null values:\\n\",\n      data[[\"dropoff_longitude\", \"dropoff_latitude\"]].isnull().sum() / data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have very few rows with null values, only 47 of the 7M, less than a 0.0007%. With this in mind, it is safe to just remove them. We can be sure that we won't lose any valuable information from them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(how='any', axis='rows', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having solved the missing values issue, let us look at how the coordinates are distributed in a scatter plot. Given that the taxi rides are placed in New York, they should be clustered around specific longitude and latitude values. We will only plot the first rows which should be sufficient to show what we want."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(16, 5))\nsns.scatterplot(x=\"pickup_longitude\", y=\"pickup_latitude\", data=data.iloc[:10000], \n                color=\"#fdb813\", ax=ax[0])\nsns.scatterplot(x=\"dropoff_longitude\", y=\"dropoff_latitude\", data=data.iloc[:10000], \n                color=\"#fdb813\", ax=ax[1])\nax[0].set_title(\"Pickup Coordinates\")\nax[1].set_title(\"Dropoff Coordinates\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Latitude and longitude coordinates of New York City are around the values 40.730610, and -73.935242 respectively. But the values of our set are much different from the actual NY latitude and longitude. It can be true that some taxis have gone outside the city to dropoff some commuters, but it is deceptive that they could go that far and that many times. What should we believe? That those observations with 0 latitude values went to pickup or dropoff the passenger to the Earth's Equator?\n\nWe will remove points, not near these coordinates in the future. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the descriptive statistics of the data, we find some interesting yet problematic insights: \n\n- Some `fare_amount` values are negative. Or the taxi driver is a modern Robin Hood, or they are errors that should be removed. \n\n- On the other side, the `passenger_count` has also unrealistic quantities, ranging from 0 to 200. Maybe I miss something but the maximum number of passengers here in the European Union, which I know better, are 7 for the biggest cars: 2 rows of 3 back seats, and the two pilot and co-driver front seats.\n\nSo we will deal with these problems in the next section."},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section4\">4. Dataset Cleaning</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cleaned(df):\n    return df[(df.fare_amount > 0) &\n              (df.pickup_latitude > 35) & (df.pickup_latitude < 45) &\n              (df.pickup_longitude > -80) & (df.pickup_longitude < -68) &\n              (df.dropoff_latitude > 35) & (df.dropoff_latitude < 45) &\n              (df.pickup_longitude > -80) & (df.dropoff_longitude < -68) &\n              (df.passenger_count > 0) & (df.passenger_count < 8)]\n\ndata = get_cleaned(data)\nprint(len(data))\nprint(\"Data lost after the cleaning process: \", 7000000 - len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have lost 169.718 observations due to incongruous data, a fairly high number but with minimum impact considering our 6.830.282 rows dataset."},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section5\">5. Feature engineering</a>\n\nNow that we have cleaned our data, we will follow by adding some interesting features. These new variables ideas have been drawn from all the other notebooks cited above."},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Distance measurement from pickup to dropoff\n\nThe haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes. Let us calculate it for each observation, in other words, let us calculate the distance along great radius between pickup and dropoff coordinates for each individual ride."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sphere_dist(pick_lat, pick_lon, drop_lat, drop_lon):\n    R_earth = 6371 # Earth radius (in km)\n    # Convert degrees to radians\n    pick_lat, pick_lon, drop_lat, drop_lon = map(np.radians, [pick_lat, pick_lon,\n                                                              drop_lat, drop_lon])\n    # Compute distances along lat, lon dimensions\n    dlat = drop_lat - pick_lat\n    dlon = drop_lon - pick_lon\n    \n    # Compute haversine distance\n    a = np.sin(dlat/2.0)**2 + np.cos(pick_lat) * np.cos(drop_lat) * np.sin(dlon/2.0)**2\n    return 2 * R_earth * np.arcsin(np.sqrt(a))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.2 Distance to the city airports\n\nTrips from or to the airports of New York have a fixed price, so it would be nice if we state this fact."},{"metadata":{"trusted":true},"cell_type":"code","source":"def airport_dist(df):\n    \"\"\"\n    JFK: John F. Kennedy International Airport\n    EWR: Newark Liberty International Airport\n    LGA: LaGuardia Airport\n    \"\"\"\n    jfk_coord = (40.639722, -73.778889)\n    ewr_coord = (40.6925, -74.168611)\n    lga_coord = (40.77725, -73.872611)\n    \n    pick_lat = df['pickup_latitude']\n    pick_lon = df['pickup_longitude']\n    drop_lat = df['dropoff_latitude']\n    drop_lon = df['dropoff_longitude']\n    \n    pickup_jfk = sphere_dist(pick_lat, pick_lon, jfk_coord[0], jfk_coord[1])\n    dropoff_jfk = sphere_dist(jfk_coord[0], jfk_coord[1], drop_lat, drop_lon) \n    pickup_ewr = sphere_dist(pick_lat, pick_lon, ewr_coord[0], ewr_coord[1])\n    dropoff_ewr = sphere_dist(ewr_coord[0], ewr_coord[1], drop_lat, drop_lon) \n    pickup_lga = sphere_dist(pick_lat, pick_lon, lga_coord[0], lga_coord[1]) \n    dropoff_lga = sphere_dist(lga_coord[0], lga_coord[1], drop_lat, drop_lon)\n    \n    df['jfk_dist'] = pd.concat([pickup_jfk, dropoff_jfk], axis=1).min(axis=1)\n    df['ewr_dist'] = pd.concat([pickup_ewr, dropoff_ewr], axis=1).min(axis=1)\n    df['lga_dist'] = pd.concat([pickup_lga, dropoff_lga], axis=1).min(axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3 Information from datetime (day of the week, month, hour, day). \n\nTaxi fares change day/night or on weekdays/holidays in most of the cities."},{"metadata":{"trusted":true},"cell_type":"code","source":"def datetime_info(df):\n    #Convert to datetime format\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    df['hour'] = df.pickup_datetime.dt.hour\n    df['day'] = df.pickup_datetime.dt.day\n    df['month'] = df.pickup_datetime.dt.month\n    df['weekday'] = df.pickup_datetime.dt.weekday\n    df['year'] = df.pickup_datetime.dt.year\n    \n    return df\n\n\ndata = datetime_info(data)\ndata = airport_dist(data)\ndata['distance'] = sphere_dist(data['pickup_latitude'], data['pickup_longitude'], \n                               data['dropoff_latitude'], data['dropoff_longitude'])\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section6\">6. Further EDA</a>\n\nNow that we have our brand new features let us continue with further exploration of the set."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.lineplot(x=\"year\", y=\"fare_amount\", data=data, color=\"#fdb813\")\nplt.title(\"Fare among years\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fares have steadily increased over the years. This is important information that our model should take into account."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(12,5))\nax[0].hist(data[\"passenger_count\"], bins=7, color=(\"#fdb813\"))\nax[0].set_title(\"Number of passengers frequency\")\nax[0].set_xlabel('No. of Passengers')\nax[0].set_ylabel('Frequency')\n\nax[1].scatter(x=data[\"passenger_count\"], y=data[\"fare_amount\"], s=1.5, \n              color=(\"#3D2C05\"))\nax[1].set_title(\"Fare amount by number of passengers\")\nax[1].set_xlabel('No. of Passengers')\nax[1].set_ylabel('Fare');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graphs, we can see that single passengers are by far the most frequent, and the highest fare also seems to come from cabs which carry just one commuter."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 3, figsize=(16,5))\nax[0].hist(data[\"hour\"], bins=24, color=\"#fdb813\")\nax[0].set_title(\"Frequency of rides by Hour of the day\")\nax[0].set_xlabel('Hour of the day')\nax[0].set_ylabel('Frequency')\n\nax[1].scatter(x=data[\"hour\"], y=data[\"fare_amount\"], s=1.5, c=\"#3D2C05\")\nax[1].set_title(\"Fares by Hour of the day\")\nax[1].set_xlabel('Hour of the day')\nax[1].set_ylabel('Fare')\n\nsns.barplot(x=\"hour\", y=\"fare_amount\", data=data, ax=ax[2], color=\"#fdb813\")\nax[2].set_title(\"Mean Fares by Hour of the day\")\nax[2].set_xlabel('Hour of the day')\nax[2].set_ylabel('Mean fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we focus our attention on the time of the day, it seems that the fares are higher between 22 and 5h., and 14 to 16h."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.barplot(x='weekday', y=\"fare_amount\", data=data, palette=(\"#fdb813\", \"#3D2C05\"))\nplt.ylim(0, 14)\nplt.title(\"Mean Fares among Days of the week\")\nplt.xlabel('Day of Week')\nplt.ylabel('Mean fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest fares seem to be on a Sunday. But the differences are minimal, and I would say that they are even non-significative."},{"metadata":{},"cell_type":"markdown","source":"Does the distance affect the fare? This is a no-brainer. I am confident enough to bet good money that the distance would affect the fare by a great deal. Let us visualize this phenomenon:"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(16, 5))\nsns.regplot(x=\"distance\", y=\"fare_amount\", data=data, color=\"#fdb813\", ax=ax[0])\nsns.regplot(x=\"distance\", y=\"fare_amount\", data=data, color=\"#fdb813\", ax=ax[1])\nax[1].set_xlim(0, 1000)\nax[1].set_ylim(0, 300)\nplt.title(\"Positive relation between distance and fare\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(14, 8))\nsns.heatmap(data.corr(), annot=True, linewidths=0.2, cmap=\"viridis\")\nplt.title(\"Correlation Heatmap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As suspected, the distance feature is highly correlated with `fare_amount`. Its importance in the prediction will be capital."},{"metadata":{"trusted":true},"cell_type":"code","source":"dropoff_longitude = data['dropoff_longitude'].to_numpy()\ndropoff_latitude = data['dropoff_latitude'].to_numpy()\n\nplt.figure(figsize=(12,8))\nplt.scatter(dropoff_longitude, dropoff_latitude,\n                color=\"#fdb813\", \n                s=.02, alpha=.2)\nplt.title(\"Dropoffs through the city\")\n# Borders of the city\nplt.xlim(-74.03, -73.75)\nplt.ylim(40.63, 40.85)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to drop the columns that we will not use to train our model.\n- `key`: Independent variable with no information at all for the fare. Its function was merely for identification purposes.\n- `pickup_datetime`: We divided this variable into multiple ones. Once done this, it is detrimental to keep it in the set, we would be counting date-related information twice."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=[\"key\", \"pickup_datetime\"], inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section7\">7. Model training</a>\n\nNow that we have the dataframe that we wanted we can start to train the XGBoost model. First, we will split the dataset into train (95%) and test (5%). With this amount of data 10% should be enough to test performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"fare_amount\"]\ntrain = data.drop(columns=[\"fare_amount\"])\n\nx_train, x_test, y_train, y_test = train_test_split(train, y, random_state=2666, test_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through tunning with CV we know the optimal parameters that we record in the next dictionary for training the model in the future:"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"max_depth\": 7,\n    \"subsample\": 0.9,\n    \"eta\": 0.03,\n    \"colsample_bytree\": 0.9,\n    \"random_state\": 2666,\n    \"objective\": \"reg:linear\",\n    \"eval_metric\": \"rmse\",\n    \"silent\": 1\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGBmodel(x_train, x_test, y_train, y_test, params):\n    matrix_train = xgb.DMatrix(x_train, label=y_train)\n    matrix_test = xgb.DMatrix(x_test, label=y_test)\n    model = xgb.train(params=params,\n                      dtrain=matrix_train,num_boost_round=5000, \n                      early_stopping_rounds=10,evals=[(matrix_test,'test')])\n    return model\n\nstart_time = time.time()\nmodel = XGBmodel(x_train, x_test, y_train, y_test, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_taken = time.time() - start_time\ntime_taken","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This took ages... but at least we have a pretty good model. Let us finally make the predictions of the test set and prepare the submission file."},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"section8\">8. Predictions</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"test =  pd.read_csv('../input/new-york-city-taxi-fare-prediction/test.csv')\ntest = datetime_info(test)\ntest = airport_dist(test)\ntest['distance'] = sphere_dist(test['pickup_latitude'], test['pickup_longitude'], \n                               test['dropoff_latitude'] , test['dropoff_longitude'])\ntest_key = test['key']\nx_pred = test.drop(columns=['key', 'pickup_datetime']) \n\n#Predict from test set\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit=model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create submission file\nsubmission = pd.DataFrame({\n        \"key\": test_key,\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('taxi_fare_submission.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}