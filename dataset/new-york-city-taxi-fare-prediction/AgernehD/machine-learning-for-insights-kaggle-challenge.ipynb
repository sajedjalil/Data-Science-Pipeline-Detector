{"cells":[{"metadata":{"_uuid":"9fbcaace85beac8a2be04989a4f1d65cfb2e6650"},"cell_type":"markdown","source":"<h1> Machine Learning for Insights</h1>\n\n<h2> What are the values of these insights</h2> \n- Model debugging\n- Feature engineering\n- Directing furure data collection\n- Informing humman decision-making, for example advertisiers use data insight to help shoppers make purchasing decision \n- Building trust by verifying basic facts about the underlying data"},{"metadata":{"_uuid":"3e07927832632b4f881cc290b959337a96fb2618"},"cell_type":"markdown","source":"<h2>Permutation Importance</h2>\n\nSome of the questions that can be answered by analyzing feature impotance are:\n- Identify important features that will impact model predicition\n- How did each feature in the dataset affect particular prediction accuracy\n- How does each features affect the overall model's predictions\n\n**Feature importance is calculated after a model has been fitted:  **\n- We will ask, if we randomly shuffle a single column of the validation data, leaving the target and all other columns in place, how would that affect the accuracy of prediction in the shuffled data?\n- Randomly re-ordering a single column should cause less accurate predictions, since the resulting data no longer corresponds to anything observed in the real world. \n- Model accuracy especially suffers if we shuffle a column that the model relied on heavily for precition-- this indicates the column is of high importanc\n\n** Process **\n \n 1 Get a trained model\n \n2 Shuffle the values in a single columns, make predictions using the resulting dataset. Calculate the loss function using these predicitons and target values. The preformance deterioration measures the importance of the variable that is being shuffled\n\n3 Undo the shuffling and replete the process for other columns\n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndf = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (df['Man of the Match'] == 'Yes') # convert from string \"Yes/No\" to binary\nfeature_names = [i for i in df.columns if df[i].dtype in [np.int64]]\nX = df[feature_names]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Get feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\npermutation = PermutationImportance(my_model, random_state=1).fit(valid_X, valid_y)\neli5.show_weights(permutation, feature_names = feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a793d14b77973ea7b7fd18b73525a5854d4e7fff"},"cell_type":"markdown","source":"** Interpreting Permutation Importances**\n- Values with high `Weight` are the most impotant features\n- The first number in each row shows how much model performance decreased with a random shffuling (in this case, using \"accuracy\" as the preformance metric)\n- The number after `Â±` measures how performance varied from one-reshuffling to the next\n- Occasionally value of permutation importance could be negative and this indicates that the prediciton on the shuffled data happened to be more accurace that the real data. This will happen when feature didn't matter, but random chance. \n"},{"metadata":{"_uuid":"9dcc35445108b2b7071b4db101575af907719df8"},"cell_type":"markdown","source":"**Calculate Permutatio for Taxi Fare Prediction**"},{"metadata":{"trusted":true,"_uuid":"29e5d839306b062926af53e9dbdd769a19dc6c22"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n# Load data\ndf_taxi = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5da4c936ce9f1a42fc106ce3f6ce39ac84631a1c"},"cell_type":"code","source":"# Remove data with extreme outlier coordinates ot negative fares\ndf_taxi = df_taxi.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +\n                      'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +\n                      'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +\n                      'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +\n                      'fare_amount > 0'\n                       )\ny = df_taxi.fare_amount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d240c922e12ceba20793c6150ecb7d8571223eb"},"cell_type":"code","source":"# Model \nbase_features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n                 'dropoff_latitude', 'passenger_count']\nX = df_taxi[base_features]\n\ntrain_taxi_X, valid_taxi_X, train_taxi_y, valid_taxi_y = train_test_split(X, y, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_taxi_X, train_taxi_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fa09ae4aa41c1f1d15d11d45a96c0656048a406"},"cell_type":"code","source":"train_taxi_X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc6ba0fdc24975a9996f7869de581eb5307d33f2"},"cell_type":"code","source":"train_taxi_y.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b4b66a5ec7a571465f010ce84f9834c4b3f9874"},"cell_type":"markdown","source":" For the first model, which variables seem potentially useful for prediciting taxi fares?"},{"metadata":{"trusted":true,"_uuid":"6592375c4ce6c19d3f67ccda45619540f3acd514"},"cell_type":"code","source":"# Permutation performance\nperm = PermutationImportance(first_model, random_state=1).fit(valid_taxi_X, valid_taxi_y)\neli5.show_weights(perm, feature_names=valid_taxi_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"356100ccfe08bea3e5dd46fd9e9cc29824c39f34"},"cell_type":"markdown","source":"Observation:\n-  Dropoff and pickup lat and long are important features.\n- On average, the lattitude features matter more than the longitude features"},{"metadata":{"_uuid":"59cc8d996e0172c5804292c58952ef7643e588ca"},"cell_type":"markdown","source":"A good next step is to disentangle the effect of being in certain parts of the city from the effect of total distance traveled.  "},{"metadata":{"trusted":true,"_uuid":"a869d4b6f27d35f9c148144121476790aecec23c"},"cell_type":"code","source":"# Create new features\ndf_taxi['abs_lon_change'] = abs(df_taxi.dropoff_longitude - df_taxi.pickup_longitude)\ndf_taxi['abs_lat_change'] = abs(df_taxi.dropoff_latitude - df_taxi.pickup_latitude)\n\n# Add the new featues to the base features\nfeatures_2  = ['pickup_longitude',\n               'pickup_latitude',\n               'dropoff_longitude',\n               'dropoff_latitude',\n               'abs_lat_change',\n               'abs_lon_change']\n\nX = df_taxi[features_2]\nnew_train_taxi_X, new_valid_taxi_X, new_train_taxi_y, new_valid_taxi_y = train_test_split(X, y, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_train_taxi_X, new_train_taxi_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5ed1a34265ac390bf55ddd029066791201bb9ea"},"cell_type":"code","source":"# Create a PermutationImportance object on second_model and fit it with the new valid data\nperm_2 = PermutationImportance(second_model, random_state=1).fit(new_valid_taxi_X, new_valid_taxi_y)\n# Show the weights for the permutation importance \neli5.show_weights(perm_2, feature_names=features_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37abeeef0f09f06d302ca865f3f4aaf3531fceb9"},"cell_type":"markdown","source":"Distance traveled seems far more importanct than any location effect. Possible reasons latitude feature are more important than longitude features\n    - latitudinal distances in the dataset tend to be larger\n    - it is more expensive to travel a fixed latitudinal distance\n    "},{"metadata":{"_uuid":"f5cbb489ac7717e2559bfa72597879669ac2761f"},"cell_type":"markdown","source":"**Conclusion**: Permutation importance is useful for debugging, understanding mode, and communicating a high-level  overview from model. "},{"metadata":{"trusted":true,"_uuid":"0b5ee4be6519e87a08fb46d8cca9af1df151d969"},"cell_type":"markdown","source":"Partial Dependence Plots\n- It shows how a feature affects predictions. \n- Useful to answer questions such as\n    - How would similart size house would priced in different areas?\n    - Could predicted difference due to one feature or another\n- It is calculated after a model has been fit-- similar to PermutationImportance, but we **repeatedly alter the value for one variable** to make a series of predictions. "},{"metadata":{"_uuid":"319dfeca1dbecb5b1ceec5d8d03987b8499533c0"},"cell_type":"markdown","source":"For FIFA 2018 dataset explore features using Decision Tree"},{"metadata":{"trusted":true,"_uuid":"c04eca32dc757d203a3a0b6769acfcba3451ab3c"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\n\n# Create tree_based model\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\n# Visualize tree structure\ngraphviz.Source(tree_graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cddf763f35a278cd7c39ad548373b7ee1bc56cd1"},"cell_type":"markdown","source":"Create Partial Dependence Plot"},{"metadata":{"trusted":true,"_uuid":"3218ed37920ffb9c15ab21ac902bdaf4dd706a2c"},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the dataset for plotting\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=valid_X, model_features=feature_names, feature='Goal Scored')\n\n# Plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88196150a90165d938d16a0765469a93e686af21"},"cell_type":"markdown","source":"Observation  and how to read the ppdp plot:\n- The y-axis is interpreted as **change in the prediction** from what it would be predicted as the baseline or leftmost value\n- A blue shaded area indicates level of confidence\nFrom the graph we see that scoring a goal substantially increase chance of winning `Player of The Game`. However, extra goals beyond that appears to have little impact on predicitions."},{"metadata":{"trusted":true,"_uuid":"7089ce8a1bcc6b6187529c1a7d658f0af899ddbb"},"cell_type":"code","source":"# Another example plot\nfeature_to_plot = 'Distance Covered (Kms)'\npdp_dist = pdp.pdp_isolate(model=tree_model, dataset=valid_X, model_features=feature_names, feature=feature_to_plot)\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a971a4a93a9803bdf779bcdfa1c8206e9a020bab"},"cell_type":"markdown","source":"The graph is simple because of the undelying model used. The sample plot with advanced model "},{"metadata":{"trusted":true,"_uuid":"946037e82cafbccc087a718be2acb225314732fb"},"cell_type":"code","source":"# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\npdp_dist_2 = pdp.pdp_isolate(model=rf_model, dataset=valid_X,\n                            model_features=feature_names, feature=feature_to_plot)\npdp.pdp_plot(pdp_dist_2, feature_to_plot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8d836b4cc23f0ea3819caea4ac5156d4102b937"},"cell_type":"markdown","source":"Random Forest based model suggests that player are more likely to win `PLayer of The Game` if the player run a total of 100km over the course of the game. Running much more than that causes lower predicitions\n\nIn general, the smooth shape of this curve seems more plausible than the steep function from the Decision Tree model. "},{"metadata":{"_uuid":"8194e4d603fa90cc4d148a7bc93c464537785fb5"},"cell_type":"markdown","source":"<h2>2D Partial Dependence Plots</h2>\n\n- It is used for understanding about interactions between features\n"},{"metadata":{"trusted":true,"_uuid":"594ea73ec60a4925f9883907b240e00fcc07d83c"},"cell_type":"code","source":"# Use pdp_interact and pdp_interact_plot instead of pdp_isolate and pdp_isolate_plot, respectively\nfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\ninter_1 = pdp.pdp_interact(model=tree_model, dataset=valid_X, \n                           model_features=feature_names, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=inter_1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b1d43f4e3520712798fe8fc682e9d31b389a153"},"cell_type":"markdown","source":"This contour plot shows predicitions for any combination of Goals Scored and Distance covered. For example, the highest predictions is when a team scores at least 1 goal and they run a total distance close to 100km. \n"},{"metadata":{"trusted":true,"_uuid":"84f2d8fd9d7a2304a928473e7f0c8ee789030efe"},"cell_type":"code","source":"# Partial dependece plot for pickup_longitude\nfeat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=valid_taxi_X, model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc3f46f24ee002f0ec47cf9fa77516201c5145a5"},"cell_type":"code","source":"from plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10fb6ef4aeccf369394f39d6d22fa6b1b88916a0"},"cell_type":"code","source":"# Create all partial plots for NYC taxi-fare\ndef plot_pdp():\n#     fig, axes = plt.subplots(nrows=3, ncols=2, \n#                              figsize=(13, 16))\n    \n    for feat_name in base_features:\n        pdp_dist = pdp.pdp_isolate(model=first_model, dataset=valid_taxi_X, \n                                  model_features=base_features, feature=feat_name,n_jobs=3)\n        pdp.pdp_plot(pdp_dist, feat_name)\n        \n#     plt.subplots_adjust(top=0.9)\n#     plt.show()\n    return None\nplot_pdp()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b4d6c0505b5f84f029f0cb08ea788b9834a547"},"cell_type":"markdown","source":"Observations:\n- Since we don't have distance measure, coordinate features, such as pickup_longitude, capture the effect of distance.\n- Being picked up near the center of the longitude value lowers predicted fares on average, because it means shorter trip (on average)"},{"metadata":{"trusted":true,"_uuid":"01c20b3e9a62a08e31347870490cf7e8c7d3b346"},"cell_type":"code","source":"# 2D partial plot for NYC taxi fare\nfnames = ['pickup_longitude', 'dropoff_longitude']\nlongitude_pdp = pdp.pdp_interact(model = first_model, dataset=valid_taxi_X,\n                                model_features = base_features, features = fnames)\npdp.pdp_interact_plot(pdp_interact_out=longitude_pdp, feature_names=fnames, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a88b9c0af5b65d9efd1e9fee664d2adb742de019"},"cell_type":"markdown","source":"Observations from 2PDP:\n- Countour running along a diagonal line, since we a pair of longitudes for pickup and dropoff indicating shorter trips\n- Fare increases as we go further from the central diagonal \n- Fare also increase as we go further to the upper-right of the graph, including staying near the 45-degree line"},{"metadata":{"_uuid":"b1a60b914b287440eb3633350b2536a24c887696"},"cell_type":"markdown","source":"Add direct distance measures\n"},{"metadata":{"trusted":true,"_uuid":"9ac2ce61d131983f7a024b3138cb514b69ae56d3"},"cell_type":"code","source":"# PDP for pickup_longitude without absolute difference features\nfeat_name = 'pickup_longitude'\npdp_dist_original = pdp.pdp_isolate(model=first_model, dataset=valid_taxi_X, \n                                    model_features=base_features, feature=feat_name)\npdp.pdp_plot(pdp_dist_original, feat_name)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4b6f95523e5fcb08cc7f1b83062250a7037811f"},"cell_type":"code","source":"feat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=new_valid_taxi_X, model_features=features_2, feature=feat_name)\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23dc33aa3f3b8b34938faa4d0c6980565c3e3e41"},"cell_type":"markdown","source":"Observations:\n- Adding absolute distance reduced the partial dependence plot of `pickup_longitude`\n- Accounting for the absolute distance traveled reduced the impct of `pickup_longitude` by about 1.5 (max)"},{"metadata":{"_uuid":"900d5c6836a7b7ae7a067b1499e50ff60050547e"},"cell_type":"markdown","source":"Modify the initialization of `y` so that our PDP plot has a positive slope in the range [-1,1], and a negative slope everywhere else"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"25323401e6b7506f32f2f7df240092fe9a8bc28f"},"cell_type":"code","source":"from numpy.random import rand\nn_sample = 20000\n\n#  Creates two features, `X1` and `X2`, having random values in the range [-2, 2].\nX1 = 4 * rand(n_sample) - 2\nX2 = 4 * rand(n_sample) - 2\n\n# Creates a target variable `y`, which is always 1.\ny = -2 * X1 * (X1<-1) + X1 -2 * X1 * (X1 > 1) - X2\n# Trains a `RandomForestRegressor` model to predict `y` given `X1` and `X2`\nmy_df = pd.DataFrame({'X1':X1, 'X2':X2, 'y':y})\npredictors_df = my_df.drop(['y'], axis=1)\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n# Creates a PDP plot for `X1` and a scatter plot of `X1` vs. `y`\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\n# Visualize results\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79b57e134b3f155969a13061b445c27668486b72"},"cell_type":"markdown","source":"Create a dataset with 2 features and a target, such that the pdp of the first feature is flat, but its permutation importance is high.  We will use a RandomForest for the model."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f76b4ecf709aa4df2c12d01a18a8365d9745630e"},"cell_type":"code","source":"# Create array holding predictive feature\nX1 = 4 * rand(n_sample) - 2\nX2 = 4 * rand(n_sample) - 2\n\n# Create y\ny =  X1 * X2\n# create dataframe because pdp_isolate expects a dataFrame as an argument\nmy_df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\npredictors_df = my_df.drop(['y'], axis=1)\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f095f0f3291a9765cb9465a299ed0a7199a99175"},"cell_type":"code","source":"perm = PermutationImportance(my_model).fit(predictors_df, my_df.y)\n# show the weights for the permutation importance you just calculated\neli5.show_weights(perm, feature_names = ['X1', 'X2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b08d2495c15401f0c2f3b82e0bd8687b7237b125"},"cell_type":"markdown","source":"<h2>SHAP Values</h2>\n- SHapely Additive exPlanations-- break down a predicition to show the impact of each featue\n- Example 1: a model says a bank shouldn't loan someone money, and the bacnk is legally required to explanin the basis for each loan rejection\n- Example 2: a healthcare provider wants to identify what factors are driving each patient's risk of some diseaces fo the van directly address those risk factors with targeted health interventions\n"},{"metadata":{"trusted":true,"_uuid":"814d0c2427d666775e9b649f5338d36af5cf4fe9"},"cell_type":"markdown","source":"To predict whether a ream would have a player win the Man of the Game awrd, we could ask \n   - How much was a prediction driven by the fact that the team scored 3 goals? or we can restate this as\n   - How much was a predicition driven by the fact that the team scored 3 goals, **instead of some baceline number of goals**\n  If we answer the question for `number of goals`, we could repeat the process for all other features\n   - SHAP value perform this in a way that guarantees a nice property. When we make prediction\n   - `sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values`"},{"metadata":{"trusted":true,"_uuid":"0347e58386d5940a004e82428f27f7aea4dec800"},"cell_type":"code","source":"df = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (df['Man of the Match'] == 'Yes') # convert from string \"Yes/No\" to binary\nfeature_names = [i for i in df.columns if df[i].dtype in [np.int64]]\nX = df[feature_names]\nX_fifa = X\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state=1)\nmy_model_fifa = RandomForestClassifier(random_state=0).fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67e963249396363af78925eb63f5567b4d1ba4b7"},"cell_type":"code","source":"# Package used to calculate Shap values\nimport shap\nrow_to_show = 5\ndata_for_prediction = valid_X.iloc[row_to_show]    # use 1 row of data \ndata_for_predicition_array = data_for_prediction.values.reshape(1, -1)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model_fifa)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eba18d0c23ec34693f5fae15e62a2c693f197ed0"},"cell_type":"markdown","source":"The shap_values is a list with two arrays\n- The first array in the list is the SHAP values for negative outcome\n- The second array is the list of SHAP values for positive outcome"},{"metadata":{"trusted":true,"_uuid":"e1a33082a8b2b89ce6b856c7ee6bbc1d5ad6f589"},"cell_type":"code","source":"my_model_fifa.predict_proba(data_for_predicition_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c65370158baa0456defd5e51352d7f8077d89af"},"cell_type":"markdown","source":"The team is 70% likely to have a player win the award. "},{"metadata":{"trusted":true,"_uuid":"0307880456184a09f4551330ef952ad27022b34e"},"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36ee58678dd41290b558c968f6a92f8a7b655a4f"},"cell_type":"code","source":"# Example using KernelExplainer \nk_explainer = shap.KernelExplainer(my_model_fifa.predict_proba, train_X)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbcfbbcd53147cfd523e35f218cb4a4c2859deeb"},"cell_type":"markdown","source":"## The Scenario\nA hospital has struggled with \"readmissions,\" where they release a patient before the patient has recovered enough, and the patient returns with health complications. \n\nThe hospital wants your help identifying patients at highest risk of being readmitted. Doctors (rather than your model) will make the final decision about when to release each patient; but they hope your model will highlight issues the doctors should consider when releasing a patient.\n"},{"metadata":{"_uuid":"be54bd7ab30526b59775b450e27d7aa6401b2bc2"},"cell_type":"markdown","source":"<h2>Step 1</h2>\n   - A simple model is built, but the doctor said, model user, doesn't know how to evaluate a model. \n   - S/He would like further evidence that what the model is performing is in line with thier medical intution. \n   - To address this issue, we need to create a condensed overview of result supported by graphics"},{"metadata":{"trusted":true,"_uuid":"41a75a4a18b951ba0b4826b9269e2b724a3b7040"},"cell_type":"code","source":"hosp_re_data = pd.read_csv('../input/hospital-readmissions/train.csv')\ny = hosp_re_data.readmitted\nbase_features_hosp = [c for c in hosp_re_data.columns if c != 'readmitted']\n# Split data into training and validation set\nX = hosp_re_data[base_features_hosp]\ntrain_X_hosp, valid_X_hosp, train_y_hosp, valid_y_hosp = train_test_split(X, y, random_state=1)\n# Create model \nmodel_hosp = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X_hosp, train_y_hosp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4220ff3cbeb23e2e11eb202faf6de97c2ff5e5b"},"cell_type":"code","source":"# Prepate the condensed exhibits for the doctor\n# Use permutation importance as a suucinct model summary\nperm = PermutationImportance(model_hosp, random_state=1).fit(valid_X_hosp, valid_y_hosp)\neli5.show_weights(perm, feature_names = valid_X_hosp.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b0e547fa2c3a95ae1ea56d4d0599f798607e42b"},"cell_type":"markdown","source":"<h2>Step 2 </h2>\n- From permutation importance, it appears that the `number_inpatient` is an importanct feature. The doctor would like to know more about it. \n- Create a graphic exhibit using PDP to show how `number_inpatient` affect the model preformace"},{"metadata":{"trusted":true,"_uuid":"fd02126d2baf9fc97dface1841d54e1e45240d89"},"cell_type":"code","source":"# Using PDP for number_inpatient feature\nfeature_name = 'number_inpatient'\n# Create the data for ploting\nmy_pdp = pdp.pdp_isolate(model=model_hosp, dataset=valid_X_hosp, model_features=valid_X_hosp.columns, feature=feature_name)\n# plot\npdp.pdp_plot(my_pdp, feature_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0482d45b72bab92d16dd965435820b0940f9c6c3"},"cell_type":"markdown","source":"<h2>Step 3</h2>\n- The doctor thinkns it's a good sign that increasing the number of inpatient procedures leads to increased predicition. \n- From the plot, one can not tell whether the change in the plot is big or small. Add `time_in_hospital` to see hot it compares"},{"metadata":{"trusted":true,"_uuid":"ac2ba7786728b7c5fd73a978599cfc1477a8227c"},"cell_type":"code","source":"feature_name = 'time_in_hospital'\n# Create the data for ploting\nmy_pdp = pdp.pdp_isolate(model=model_hosp, dataset=valid_X_hosp, model_features=valid_X_hosp.columns, feature=feature_name)\n# plot\npdp.pdp_plot(my_pdp, feature_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a1269fb57b8358a765435ac11e5770b419b7545"},"cell_type":"markdown","source":"<h2>Step 4</h2>\n- It appears that `time_in_hospital` doesn't matter at all. The difference between the lowest value on the partial dependence plot and the highest value is about 5%\n- If that is what your model concluded, the doctors will believe it. But it seems so low. Could  the data be wrong, or is your model doing something more complex than they expect?  \n- They'd like you to show them the raw readmission rate for each value of `time_in_hospital` to see how it compares to the partial dependence plot.\n\n    - Make that plot. \n    - Are the results similar or different?"},{"metadata":{"trusted":true,"_uuid":"7afd962755c1bd10237701ea742b4b5900dcbf99"},"cell_type":"code","source":"# Get the average readmission rate for each time_in_hospital\n# Do concat to keep validation data separate, rather than using all original data\nall_train_hosp = pd.concat([train_X_hosp, train_y_hosp], axis=1)\nall_train_hosp.groupby(['time_in_hospital']).mean().readmitted.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc50a1fb6077a55d3ba91f5b1c13067be7c3fa62"},"cell_type":"markdown","source":"<h2>Step 5</h2>\n- Now the doctor is convinced that the data is right, and the model overview looked reasonable. To turn this into a finished product that the doctor can use, lets create a function `patient_risk_factors` that does the following\n    - Takes a single row with patient data\n    - Create a visualization showing what features of that patient increased their risk of readmission, what features decreased it, and how much those features mattered\n    "},{"metadata":{"trusted":true,"_uuid":"16c0ce95436a23cb2c3b891c5081abaf66d9df1e"},"cell_type":"code","source":"# Use SHAP \n# Create sample data to test the function\nsample_data_for_prediction = valid_X_hosp.iloc[0].astype(float)\n\n# Create function\ndef patient_risk_factors(model, patient_data):\n    # Create object that can calculate shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient_data)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c14ba7c17f7ca79448fbd730319936e536f2c42"},"cell_type":"code","source":"patient_risk_factors(model_hosp, sample_data_for_prediction)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82898810236f3bec9ab53a3aeeb4b7a228eb3f1f"},"cell_type":"markdown","source":"<h2> Aggregating SHAP values</h2>\n- Aggregating many SHAP values can give more detailed alternatives to permutation importance and partial dependence plots\n- Unlike `Permutation Importance`, SHAP summary plots gives usa bird-eye view od feature importance and what is driving it. "},{"metadata":{"_uuid":"8229bd79692a8519b3aa427714351551f05d8823"},"cell_type":"markdown","source":"**Summary plot using SHAP**\nSummary plot is made up of may dots with the following characterstics\n- Vertical location shows what feature it is depicting\n- Color shows whether that feature was high or low for that row of the dataset\n- Horizontal location shows whether the effect of that value caused a higher or lower prediction\n"},{"metadata":{"trusted":true,"_uuid":"a987a8f856b999fbab7e50c1cbdc8892fd9a88c2"},"cell_type":"code","source":"# Summary SHAP plot for FIFA data\n\n# Create pbbject that can calculate shap values \nexplainer = shap.TreeExplainer(my_model_fifa)\n# Calculate the shap values for all validation data for plotting\nshap_values = explainer.shap_values(valid_X)\n# Make plot\nshap.summary_plot(shap_values[1], valid_X)    # shap_values[1] is for prediction of 'True'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90c08667472b4b702b26b212e2f4a65ebb43444b"},"cell_type":"markdown","source":"**Observations:**\n- The model ignores the `Red` and `Yellow & Red` features\n- Usually yellow card doesnt affect prediction, but there is an extreme case where a high value caused a much lower prediction\n- High values of `Goal scored` caused higher prediction, and low values caused low predicition "},{"metadata":{"_uuid":"926dc313fb5c00c03489aea4ecc72b1e4e9411e9"},"cell_type":"markdown","source":"<h2>SHAP Dependence Contribution Plots</h2>\n- Provide an alternative insight to PDP's, but they add a lot more detail\n- It shows the effect of distribution"},{"metadata":{"trusted":true,"_uuid":"8f6a71156fc64370e0d63f03a6562ff4a8ae1e21"},"cell_type":"code","source":"# Create pbbject that can calculate shap values \nexplainer = shap.TreeExplainer(my_model_fifa)\n\n# Calculate the shap values for all validation data for plotting\nshap_values = explainer.shap_values(X_fifa)\n\n# Make plot\nshap.dependence_plot('Ball Possession %', shap_values[1], X_fifa, interaction_index='Goal Scored')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e884990a53f6547274efc221f488c59c2de758a"},"cell_type":"markdown","source":"**Observations:**\n- Each do represent a row of data\n- Horizontal value is the actual value from the dataset, and\n- Vertival value shows what having that horizontal value did to the prediction. The fact that there is a upward slope indicates that `Ball possession` increases the model's prediction for winning the `Man of the Game` award\n- The spread suggests that other features must be interacting with `Ball possession %`\n**In general, possessing the ball increasse a team's chance of having their player win the award. **\n"},{"metadata":{"_uuid":"08f6ac385463c22150500a3e8b5ee7e30d399b87"},"cell_type":"markdown","source":"<h2>Exercise using the hospotal-readmissions data</h2>\n"},{"metadata":{"trusted":true,"_uuid":"aca530b2d8b0b34e2c6eaf977fe004fbdf746fef"},"cell_type":"code","source":"base_features = ['number_inpatient', 'num_medications', 'number_diagnoses', 'num_lab_procedures', \n                 'num_procedures', 'time_in_hospital', 'number_outpatient', 'number_emergency', \n                 'gender_Female', 'payer_code_?', 'medical_specialty_?', 'diag_1_428', 'diag_1_414', \n                 'diabetesMed_Yes', 'A1Cresult_None']\n\nX_hosp = hosp_re_data[base_features].astype(float)\ny_hosp = hosp_re_data.readmitted\ntrain_X_hosp_2, valid_X_hosp_2, train_y_hosp_2,valid_taxi_y_2 = train_test_split(X_hosp, y_hosp, random_state=1) \n# sample data for speed\nsmall_valid_X_hosp_2 = valid_X_hosp_2[:150]\nmodel_hosp_2 = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X_hosp_2, train_y_hosp_2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dc38df5b78600e79c233bebb5f3331ac4afd0ed"},"cell_type":"code","source":"hosp_re_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48711d9c4e984008111d7283ba35ddcda6d0af95"},"cell_type":"markdown","source":"**Q1: What is the effect of distribution for each feature?**"},{"metadata":{"trusted":true,"_uuid":"3b2f3210f8bc80cff24079def637ccc1b824035f"},"cell_type":"code","source":"explainer = shap.TreeExplainer(model_hosp_2)\nshap_values = explainer.shap_values(small_valid_X_hosp_2)\nshap.summary_plot(shap_values[1], small_valid_X_hosp_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b47fe2564ecf7e6513c0ecf6790279732b79273"},"cell_type":"markdown","source":"**Q2: Which of these featue `diag_1_428`, which has wider range of effect or `payer_code_`?**\nThe width of the effects range is not a reasonable approximation to permutation importance"},{"metadata":{"_uuid":"22771020452fd303842a57e83534d466c20322c5"},"cell_type":"markdown","source":"## Question \nConsider the following SHAP contribution dependence plot. \n\nThe x-axis shows `feature_of_interest` and the points are colored based on `other_feature`.\n\n![Imgur](https://i.imgur.com/zFdHneM.png)\n\nIs there an interaction between `feature_of_interest` and `other_feature`?  \nIf so, does `feature_of_interest` have a more positive impact on predictions when `other_feature` is high or when `other_feature` is low?\n"},{"metadata":{"_uuid":"67d9d4dae69a5784551201f4fb5c1398dbccfe1a"},"cell_type":"markdown","source":"Increasing feature_of_interest has a more positive impact on predictions when other_feature is high."},{"metadata":{"_uuid":"b220965164c0cf1d9c5357f8841818c588d83cf0"},"cell_type":"markdown","source":"Both **num_medications** and **num_lab_procedures** share that jumbling of pink and blue dots.\n\nAside from `num_medications` having effects of greater magnitude (both more positive and more negative), it's hard to see a meaningful difference between how these two features affect readmission risk.  Create the SHAP dependence contribution plots for each variable, and describe what you think is different between how these two variables affect predictions.\n\nAs a reminder, here is the code you previously saw to create this type of plot.\n\n    shap.dependence_plot(feature_of_interest, shap_values[1], val_X)\n    \nAnd recall that your validation data is called `small_val_X`."},{"metadata":{"trusted":true,"_uuid":"4a2596088e422bee5957df4f00a3054edb9c71e4"},"cell_type":"code","source":"shap.dependence_plot('num_lab_procedures', shap_values[1], small_valid_X_hosp_2)\nshap.dependence_plot('num_medications', shap_values[1], small_valid_X_hosp_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93a673437a47adf26017a55e7229017f8c4a25d5"},"cell_type":"markdown","source":"**Observations:**\n- **num_lab_procedures**: The model seems to think this is a relevant feature. One potential next step would be to explore more by coloring it with different other features to search for an interaction.\n- **num_medications** clearly slopes up until a value of about 20, and then it turns back down."},{"metadata":{"_uuid":"a0b02cf286d2ee246440ec7ee7293fd7b07db561"},"cell_type":"markdown","source":"Note: This notebook was based on Kaggle's **Machine Learning for Insights Challenge** by Dan. I organized it in one notebook for future reference. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}