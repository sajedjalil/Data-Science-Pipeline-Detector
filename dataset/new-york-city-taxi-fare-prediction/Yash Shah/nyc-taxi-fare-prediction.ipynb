{"cells":[{"metadata":{"_uuid":"fb096f16d06a665fbf3b2fb69b1140c54c97bef2"},"cell_type":"markdown","source":"# New York Taxi Fare Prediction (Exploratory Data Analysis in iPython)\n\nThis homework investigates doing exploratory data analysis in iPython. It is based on New York Taxi Fare Prediction on Kaggle (https://www.kaggle.com/c/new-york-city-taxi-fare-prediction), revolving around predicting the fare of a taxi ride given a pickup and a drop off location.\n"},{"metadata":{"trusted":true,"_uuid":"6ae13c42ca7c7ada4c95133c6f58a4287766d503"},"cell_type":"code","source":"# Basic Imports\nimport pandas as pd\nimport numpy as np\nimport urllib.request\nimport json\nfrom time import sleep\n\n# Visualization and geo-data imports\nimport matplotlib.pyplot as plt\nimport folium\nfrom folium import plugins\nimport fiona\nfrom shapely.geometry import shape,mapping, Point, Polygon, MultiPolygon\nimport geopandas\nfrom geopandas.tools import sjoin\nimport seaborn as sns\n\n# Modelling and training\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c20cde55832ddefdfa143ceb12d9561b6928bc75"},"cell_type":"markdown","source":"## Load Datasets\n\nIn this part, we will load the entire 55 million records of NYC taxi fare as follows:\n* First we will only load first 10 rows of training data from csv file (check dataframe column types) using pandas.\n\n* We want to efficiently store dataset (optimize on dataframe column datatypes)\n    \n    * float64 > float32 (for GPS coordinates 7 digit precision is enough, more is overkill)\n      (referenced from: https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude/8674)\n    \n    * object > string (pickup_datetime column's datatype converted to string for reading from csv, again converted back to datetime by parsing string)\n\n    * int64 > int8 (passenger_count field since max passengers in single record is less than 2^8=256)\n    \n    * Ignoring key field (not used for training)\n    \n* Read entire training csv in chunks of 10 million rows and storing chunks in single list"},{"metadata":{"trusted":true,"_uuid":"315a80d055138005380993407c8782d62be671af","_kg_hide-output":false},"cell_type":"code","source":"# Sample and visualize first 10 rows by reading part of training data\n\nsample_train_df = pd.read_csv('../input/train.csv', nrows=10)\nsample_train_df.info()\n#display(sample_train_df.head())\n#display(sample_train_df.tail())\ndel sample_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62608cd605bb8a711e883abe406013543a9d228e"},"cell_type":"code","source":"%%time\n\n### Optimize and compress certain feature's data types\n#   1. float32 is enough for upto 7 digit precision (as used in GPS)\n#   2. datetime object is stored as string for reading csv\n#   3. passenger_count field type is changed to uint8 (not more than 256 passenger count in training data)\n#   4. Ignoring key object field\n\ntrain_df_type = {\n    'fare_amount':'float32',\n    'pickup_datetime':'str',\n    'pickup_longitude':'float32',\n    'pickup_latitude':'float32',\n    'dropoff_longitude': 'float32',\n    'dropoff_latitude': 'float32',\n    'passenger_count': 'uint8'\n}\n\n# Read training csv in chunks and storing chunks in single list\n\nchunksize=10**7\ndf_train_list=[]\ncnt=1\nfor chunk in pd.read_csv('../input/train.csv', dtype=train_df_type, usecols=list(train_df_type), chunksize=chunksize):\n    # converting pickup_datetime from string to datetime object\n    chunk['pickup_datetime'] = pd.to_datetime(chunk['pickup_datetime'].str.slice(0,19), utc=True, format='%Y-%m-%d %H:%M:%S')\n    df_train_list.append(chunk)\n    print(cnt, \"chunk appended\")\n    cnt+=1\n    \n# concatenating list of training data to single dataframe\n\ntrain_df = pd.concat(df_train_list)\ndel df_train_list\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5059557053d7ea5223cda7e70d4728b1549f29d1"},"cell_type":"markdown","source":"## Task 1\n\nTake a look at the training data. There may be anomalies in the data that you may need to factor in before you start on the other tasks. Clean the data first to handle these issues. Explain what you did to clean the data (in bulleted form)."},{"metadata":{"_uuid":"e0995701b878254254fa096cc848c811b3d714f4"},"cell_type":"markdown","source":"### Taking Look at Training and Testing Data\n\n* After reading both train and test data, use describe method to find out statistics (e.g. min, max, mean, count of values inside dataset)"},{"metadata":{"trusted":true,"_uuid":"6250f6462d4843d6fd111f46164a5530a37e9961"},"cell_type":"code","source":"# Describe training data statistics\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43ddf6e4aa2edf4378f87ac5a524ce022c2c3fb4"},"cell_type":"code","source":"# read test csv and describe its statistics\n\ntest_df = pd.read_csv('../input/test.csv')\ntest_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e61189716263612746ccb20ae1e88e46281aaf5"},"cell_type":"markdown","source":"### Find Anomalies in Data\n\n* Fare Amount \n    * Fare amount in 254 training records is negative.\n    * Fare amount is 0 in certain training records (In this case, we need to check for unusual higher distance travelled by taxi and may decide to filter those records)\n    * Fare amount is abnormally high in some taxi records with not that big distances covered (may decide to filter taxi records with fare amount more than 10 standard deviations upwards from mean)\n     \n* Passenger Count\n    * Maximum count of passenger in test data is 6.\n    * Number of passengers are 0 in 195416 taxi trips.\n    * In 116 training records, there are more than 6 passengers.\n \n* Coordinates (Pickup and Dropoff)\n    * Test dataset is clean (no NULL / Nan values).\n    * In training set, we find 376 records with NULL values.\n    * Use test dataset min and max coordinates as bounding box for training data. (since we are focused on predicting for same test data only)"},{"metadata":{"trusted":true,"_uuid":"0a42ba7816ce73fc9627eca2a3493dc71ea841b0"},"cell_type":"code","source":"### Observations from description of training and testing data:\n#   1. Minimum fare amount in training data is negative\n#   2. Maximum count of passenger in test data is 6\n\nprint(\"Instances where features of training data is null: \")\ndisplay(train_df.isnull().sum()) # any null values in train data\n\nprint(\"Occurences of negative fare amount: \" + str(len(train_df[train_df['fare_amount']<0]))) # fare amount is zero or negative\nprint(\"Occurences with more than 6 passengers: \" + str(len(train_df[train_df['passenger_count']>6]))) # passenger count is more than 6\nprint(\"Occurences with exactly 0 passengers: \" + str(len(train_df[train_df['passenger_count']==0])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b7db28796d4560e31b6bfb66d6691e3ba555044"},"cell_type":"markdown","source":"### Helper functions to get Distances (Euclidean, Manhattan)"},{"metadata":{"trusted":true,"_uuid":"a2e28e366cd0aff04f53301922c38fd5b63b032c"},"cell_type":"code","source":"def getEuclidean_distance(pickup_long, pickup_lat, dropoff_long, dropoff_lat):\n    return np.sqrt(((pickup_long-dropoff_long)**2) + ((pickup_lat-dropoff_lat)**2))\n\ndef get_manhattan_dist(pickup_long, pickup_lat, dropoff_long, dropoff_lat):\n    return ((dropoff_long - pickup_long).abs() + (dropoff_lat - pickup_lat).abs())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b1631668a1b3eb1c299d9b6fdc3095c858749c2"},"cell_type":"code","source":"%%time\n\n# Add new column (manhattan_dist) to both training and test dataframe.\n\ntrain_df['manhattan_dist'] = get_manhattan_dist(train_df.pickup_longitude, train_df.pickup_latitude,\n                                              train_df.dropoff_longitude, train_df.dropoff_latitude).astype(np.float32)\n\ntest_df['manhattan_dist'] = get_manhattan_dist(test_df.pickup_longitude, test_df.pickup_latitude,\n                                       test_df.dropoff_longitude, test_df.dropoff_latitude).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e358997f92d4ed6e6bc5b39593e8451b45d3779"},"cell_type":"markdown","source":"### Clean Data (Steps)\n\n* Remove any rows with any kind of NULL / Nan data ()\n* Filter based on passenger counts (only consider data with passenger count between 1 and 6)\n* Filter based on fare amount (remove negative fares, or fares with 0 amount but long distances covered)\n* Filter based on outliers (discard records with fare amount more than 10 standard deviations upward from mean)\n* Filter based on test data coordinates boundary\n                "},{"metadata":{"trusted":true,"_uuid":"fe97f29db74a9d5942bc33fa570099ab02997a02"},"cell_type":"code","source":"def clean_data(train_df):\n    \n    print(\"Initial Train dataframe length: \" + str(len(train_df)))\n    \n    # Remove null data\n    train_df=train_df.dropna(how='any',axis='rows')\n    print(\"Train dataframe length after removing NULL values: \" + str(len(train_df)))\n    \n    train_df=train_df[(train_df.passenger_count<=6) & (train_df.passenger_count>=1)]\n    print(\"Train dataframe length after filtering based on passenger counts: \" + str(len(train_df)))\n    \n    train_df=train_df[(train_df.fare_amount>0) | ((train_df.fare_amount==0) & (train_df.manhattan_dist<0.75))]\n    train_df=train_df[(train_df.fare_amount <= train_df.fare_amount.mean()+10*train_df.fare_amount.std())]\n    print(\"Train dataframe length after filtering based on fare amount: \" + str(len(train_df)))\n    \n    train_df=train_df[(train_df.pickup_longitude>=min(test_df.pickup_longitude)) & (train_df.pickup_longitude<=max(test_df.pickup_longitude))]\n    train_df=train_df[(train_df.pickup_latitude>=min(test_df.pickup_latitude)) & (train_df.pickup_latitude<=max(test_df.pickup_latitude))]    \n    train_df=train_df[(train_df.dropoff_longitude>=min(test_df.dropoff_longitude)) & (train_df.dropoff_longitude<=max(test_df.dropoff_longitude))]\n    train_df=train_df[(train_df.dropoff_latitude>=min(test_df.dropoff_latitude)) & (train_df.dropoff_latitude<=max(test_df.dropoff_latitude))]\n    print(\"Train dataframe length after filtering based on test data coordinates boundary: \" + str(len(train_df)))\n    \n    return train_df\n\n\n\ntrain_df=clean_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"284231514feed064373999d49bb31a3e79f8e21c"},"cell_type":"markdown","source":"### Helper functions to get different distances"},{"metadata":{"_uuid":"d4df14f4e400f657533d65116ae5f7ff99b45a87"},"cell_type":"markdown","source":"### Airport Distances\n\n\n* Get coordinates from each of 3 airports:\n     Airport | Longitude      | Latitude      \n     JFK     | -73.78         | 40.64                   \n     LGA     | -73.87         | 40.77         \n     EWR     | -74.175        | 40.69         \n    \n* Calculates euclidean distances from all airports to all data points (both with pickup and dropoff at airports)\n\n* Add total of 6 new columns (each 3 airport pickup and dropoff distances) to both training and test data."},{"metadata":{"trusted":true,"_uuid":"b0d72f72a2bee0dec7d4f588d693bf1e8cb5aaeb"},"cell_type":"code","source":"def getAirport_pickup_distance(dropoff_long, dropoff_lat, airport):\n    pickup_long=0\n    pickup_lat=0\n    if airport == \"JFK\":\n        pickup_long=-73.7822222222\n        pickup_lat=40.6441666667\n    elif airport == \"LGA\":\n        pickup_long=-73.87\n        pickup_lat=40.77\n    elif airport == \"EWR\":\n        pickup_long=-74.175\n        pickup_lat=40.69\n    return np.sqrt(((pickup_long-dropoff_long)**2) + ((pickup_lat-dropoff_lat)**2))\n\ndef getAirport_dropoff_distance(pickup_long, pickup_lat, airport):\n    dropoff_long=0\n    dropoff_lat=0\n    if airport == \"JFK\":\n        dropoff_long=-73.7822222222\n        dropoff_lat=40.6441666667\n    elif airport == \"LGA\":\n        dropoff_long=-73.87\n        dropoff_lat=40.77\n    elif airport == \"EWR\":\n        dropoff_long=-74.175\n        dropoff_lat=40.69\n    return  np.sqrt(((pickup_long-dropoff_long)**2) + ((pickup_lat-dropoff_lat)**2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa6fe8988a866b4afdb800bda7e49cba1faab66d"},"cell_type":"code","source":"%%time\n\ntrain_df['jfk_pickup_dist']=getAirport_pickup_distance(train_df.dropoff_longitude, train_df.dropoff_latitude, \"JFK\").astype(np.float32)\ntrain_df['lga_pickup_dist']=getAirport_pickup_distance(train_df.dropoff_longitude, train_df.dropoff_latitude, \"LGA\").astype(np.float32)\ntrain_df['ewr_pickup_dist']=getAirport_pickup_distance(train_df.dropoff_longitude, train_df.dropoff_latitude, \"EWR\").astype(np.float32)\n\ntrain_df['jfk_dropoff_dist']=getAirport_dropoff_distance(train_df.pickup_longitude, train_df.pickup_latitude, \"JFK\").astype(np.float32)\ntrain_df['lga_dropoff_dist']=getAirport_dropoff_distance(train_df.pickup_longitude, train_df.pickup_latitude, \"LGA\").astype(np.float32)\ntrain_df['ewr_dropoff_dist']=getAirport_dropoff_distance(train_df.pickup_longitude, train_df.pickup_latitude, \"EWR\").astype(np.float32)\n\ntest_df['jfk_pickup_dist']=getAirport_pickup_distance(test_df.dropoff_longitude, test_df.dropoff_latitude, \"JFK\").astype(np.float32)\ntest_df['lga_pickup_dist']=getAirport_pickup_distance(test_df.dropoff_longitude, test_df.dropoff_latitude, \"LGA\").astype(np.float32)\ntest_df['ewr_pickup_dist']=getAirport_pickup_distance(test_df.dropoff_longitude, test_df.dropoff_latitude, \"EWR\").astype(np.float32)\n\ntest_df['jfk_dropoff_dist']=getAirport_dropoff_distance(test_df.pickup_longitude, test_df.pickup_latitude, \"JFK\").astype(np.float32)\ntest_df['lga_dropoff_dist']=getAirport_dropoff_distance(test_df.pickup_longitude, test_df.pickup_latitude, \"LGA\").astype(np.float32)\ntest_df['ewr_dropoff_dist']=getAirport_dropoff_distance(test_df.pickup_longitude, test_df.pickup_latitude, \"EWR\").astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"358461a29a4738d4992d8b5cdf1247bdb8ce70e3"},"cell_type":"markdown","source":"### Euclidean Distances\n\nCompute euclidean distance between pickup and dropoff points"},{"metadata":{"trusted":true,"_uuid":"05b0408c14b4e8d46c954aa7b85a260a5eeb950c"},"cell_type":"code","source":"train_df['euclidean_dist'] = getEuclidean_distance(train_df.pickup_longitude, train_df.pickup_latitude,\n                                                   train_df.dropoff_longitude, train_df.dropoff_latitude).astype(np.float32)\n\ntest_df['euclidean_dist'] = getEuclidean_distance(test_df.pickup_longitude, test_df.pickup_latitude,\n                                       test_df.dropoff_longitude, test_df.dropoff_latitude).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e92cdeff71a67bb53b296efd5dfcea0c199d11b0"},"cell_type":"markdown","source":"### Haversine Distances\n\nCompute haversine distance between pickup and dropoff points\n(Formula referenced from: https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points)"},{"metadata":{"trusted":true,"_uuid":"b82cd2845ed33637469d30cbf11443d986d43092"},"cell_type":"code","source":"%%time\ndef haversine_np(lon1,lat1,lon2,lat2):\n    lon1,lat1,lon2,lat2 = map(np.radians, [lon1,lat1,lon2,lat2])\n    dlon=lon2-lon1\n    dlat=lat2-lat1\n    a=np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n    c=2*np.arcsin(np.sqrt(a))\n    km=6367*c\n    return km\n\ntrain_df['haversine_dist']=haversine_np(train_df.pickup_longitude, train_df.pickup_latitude, train_df.dropoff_longitude, train_df.dropoff_latitude).astype(np.float32)\ntest_df['haversine_dist']=haversine_np(test_df.pickup_longitude, test_df.pickup_latitude, test_df.dropoff_longitude, test_df.dropoff_latitude).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24545a940464d144baa3552b5b8fe467c7cbce1a"},"cell_type":"markdown","source":"### Save / Load Cleaned Data\n\nSave preprocessed cleaned data to feather file (don't need to spend computation time again for cleaning)\n\nFeather is fast, interoperable binary data frame storage for python (much faster than reading from csv)"},{"metadata":{"_uuid":"923b2e5559bbf574f1794f78e865254ea6569422"},"cell_type":"markdown","source":"### Save / Load training data from Feather"},{"metadata":{"trusted":true,"_uuid":"e46d485f2c32bf0ba330ee810a355596389a85c5"},"cell_type":"code","source":"%%time\n# saving training dataframe to feather file\ntrain_df=train_df.reset_index(drop=True)\ntrain_df.to_feather('train.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ba6b62a1bb5deed756aa5a644ae712051daf315"},"cell_type":"code","source":"%%time\n\n# read from feather file (not from original train csv) for fast loading \n\ntrain_df = pd.read_feather('train.feather')\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25560438dc6ccf225c46e0f71b89add3c6873d43"},"cell_type":"markdown","source":"### Save / Load test data from feather"},{"metadata":{"trusted":true,"_uuid":"15dc1e2dcb897b4300397defaeb8f41914ac9fd8"},"cell_type":"code","source":"%%time\n# saving training dataframe to feather file\ntest_df=test_df.reset_index(drop=True)\ntest_df.to_feather('test.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b9d2e59eac56aa6db5b7699f9e4b9820c300571"},"cell_type":"code","source":"%%time\n\n# read from feather file (not from original train csv) for fast loading \n\ntest_df = pd.read_feather('test.feather')\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be6e96ca5957fc2917b03880d96549afa521deb2"},"cell_type":"markdown","source":"## Task 2\n\nCompute the Pearson correlation between the following: (9 pt)\n* Euclidean distance of the ride and the taxi fare\n* time of day and distance traveled\n* time of day and the taxi fare\n"},{"metadata":{"_uuid":"153613b8cd67cb2eb707ed8937985f9afd91dd6e"},"cell_type":"markdown","source":"Since we have computed haversine and manhattan distances in addition to euclidean distance, we might want to get pearson correlation for those as well."},{"metadata":{"trusted":true,"_uuid":"3a8a78e844a9a035a3d582b86115c3bf890e8c27"},"cell_type":"code","source":"# Haversine Correlation\ntrain_df.haversine_dist.corr(train_df.fare_amount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"902ff70dbb0b29a2b128f2afd669504cb4a3168a"},"cell_type":"code","source":"# Manhattan Correlation\ntrain_df.manhattan_dist.corr(train_df.fare_amount)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a38e556799bc69a1e646c65c41c96037c4c21f51"},"cell_type":"markdown","source":"### Correlation between Euclidean distance of the ride and the taxi fare"},{"metadata":{"trusted":true,"_uuid":"7c2c50b15448694a1afc4d2c0c58cc2f433fd7b6"},"cell_type":"code","source":"# Euclidean Correlation\ntrain_df.euclidean_dist.corr(train_df.fare_amount)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"346a1418baee8dd5f794be8b4ac9f987ade0658e"},"cell_type":"markdown","source":"From above we can see that all (Euclidean, Manhattan and Haversine) are strongly correlated with fare amount (each with value more than 0.8)"},{"metadata":{"_uuid":"2c5949201d1bfbe9247ea798c043b45df42f26a3"},"cell_type":"markdown","source":"### Correlation between time of day and distance traveled"},{"metadata":{"_uuid":"d00312d5be6c81c3febeeef3c3dc0460af6cb5e3"},"cell_type":"markdown","source":"For time of day, we have three options:\n    1. Take continuous values of hours (0 to 23)\n    2. Take minutes elapsed (from 12am)\n    3. Take seconds elapsed (from 12am)\n\nHere time of day is calculated as seconds elapsed from midnight"},{"metadata":{"trusted":true,"_uuid":"f199582fc6c475d08201419f26e5f72cb7742793"},"cell_type":"code","source":"%%time\n\ntime_of_day = train_df.pickup_datetime.dt.hour * 3600 + train_df.pickup_datetime.dt.minute * 60 + train_df.pickup_datetime.dt.second","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7743b8b43d6969c2bc4dacf06a9b00dea3ee9cae"},"cell_type":"code","source":"# time of day and distance traveled correlation\ntime_of_day.corr(train_df.euclidean_dist)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d432d555a5a399bd8bc93bc2e977dd8545a8fb8"},"cell_type":"markdown","source":"### Correlation between time of day and the taxi fare"},{"metadata":{"trusted":true,"_uuid":"64ed32c305f4ed1ab335b6366853a9cf82b81ca1"},"cell_type":"code","source":"# time of day and the taxi fare correlation\ntime_of_day.corr(train_df.fare_amount)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0ca8e85c13043d967562bf52f7b4ee062fee45b"},"cell_type":"markdown","source":"As seen, time of day is almost uncorrelated with both distance traveled and fare amount.\n\n### Highest Correlation\n\n* Highest correlation is between Euclidean distance of the ride and the taxi fare"},{"metadata":{"_uuid":"bbf0c6010a1968b97ff59c061c675b29c1672a0e"},"cell_type":"markdown","source":"## Task 3\n\nFor each subtask of (2), create a plot visualizing the relation between the variables. Comment on whether you see non-linear or any other interesting relations. (9 pt)"},{"metadata":{"_uuid":"5ff5de9b52fab9036d15c4ddc37a5cee10c32ade"},"cell_type":"markdown","source":"### Scattered plot between Euclidean distance of the ride and the taxi fare\n\n* For Distance vs Fare graph, some low euclidean distance records have relatively higher fare amount than for certain big distances. (although for distances less than 0.5, it seems strongly correlated with fare.)\n\n* For Fare vs Distance graph, apart from where fare amount is near 0 and distances are unusual high (on top left portion), the other parts show strong correlation i.e. fare is increasing > distance is also increasing\n\nBelow are shown these two graphs."},{"metadata":{"trusted":true,"_uuid":"a1bb07eb7de8d8040bdb10b856f2129e8d11c656"},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.scatter(train_df.euclidean_dist[0:10**3],train_df.fare_amount[0:10**3])\nplt.title('Distance vs Fare')\nplt.xlabel('Distances')\nplt.ylabel('Fare')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1dce40492dc50bf4becf33cf2a98fc7e6d0f2c8"},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.scatter(train_df.fare_amount[0:10**3],train_df.euclidean_dist[0:10**3])\nplt.title('Fare vs Distance')\nplt.xlabel('Fare')\nplt.ylabel('Distances')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4dca08d3eaf11de3620f5373df11f660c752894"},"cell_type":"markdown","source":"### Scattered plot between time of day and distance traveled\n\n* The plot shows that distances traveled is uncorrelated with time of day. (distances traveled is similar across different time)"},{"metadata":{"trusted":true,"_uuid":"5eb5771255d6960e252c2fdf512bfcadc35384d9"},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.scatter(time_of_day[0:10**3],train_df.euclidean_dist[0:10**3])\nplt.title('Time vs Distance')\nplt.xlabel('Seconds elapsed from midnight')\nplt.ylabel('Distances')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"433e9b2a34af1a94d76849a8dcb493490b6f59ee"},"cell_type":"markdown","source":"### Scattered plot between time of day and the taxi fare\n\nThe plot shows that fare is uncorrelated with time of day. (fare paid is distributed similarly across time period)"},{"metadata":{"trusted":true,"_uuid":"b58f9788f0ddc86965f008bb592e76d761d8814c"},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.scatter(time_of_day[0:10**3],train_df.fare_amount[0:10**3])\nplt.title('Time vs Fare')\nplt.xlabel('Seconds elapsed from midnight')\nplt.ylabel('Fare')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2191fc0b6dd4e06441d2ecc7313cc48cf35aa72"},"cell_type":"code","source":"del time_of_day # free up bit memory","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6055e98e22a42c52b115682d6d0ea25e80fa1b4"},"cell_type":"markdown","source":"## Task 4\nCreate an exciting plot of your own using the dataset that you think reveals something very interesting.   Explain what it is, and anything else you learned. (15 pt)\n\nHere are 4 intersting plots."},{"metadata":{"_uuid":"f7f60436d40c763ce1fedf0f12320947e715827f"},"cell_type":"markdown","source":"### Plot between Hour of day vs Number of Rides (during that time)\n\nObservations:\n* 5am (morning period) has least umber of rides taken.\n* 7pm has most number of rides taken.\n* Throughout the work hours, number of taxi rides taken are high (even this number increases during 6pm to 10pm)\n(This may be due to rush hour people going to home from work in addition to people going outside to restaurants, malls, movies or any touristy places in this evening period)"},{"metadata":{"trusted":true,"_uuid":"e9d352cb9c1800524cbb6552445c2f5a6eb60344"},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.hist(train_df.pickup_datetime.dt.hour, bins=100)\nplt.title('Hour vs Number of Rides')\nplt.xlabel('Hour')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acadafaba9afbd330669c89f7152db6f1e071e64"},"cell_type":"markdown","source":"### Plot between Hour of day vs Fare (during that time)\n\nObservations:\n* The fare amount seems similar throughout the entire day. (nothing like spike during rush hours or night time)"},{"metadata":{"trusted":true,"_uuid":"09fa542b48b416400c8ca60952ff3aad6f98644b"},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nplt.scatter(x=train_df.pickup_datetime[0:10**6].dt.hour, y=train_df['fare_amount'][0:10**6], s=1.5)\nplt.title('Hour vs Fare')\nplt.xlabel('Hour')\nplt.ylabel('Fare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"467bb2925707beca11a9f6710077fab7637757a2"},"cell_type":"markdown","source":"### Heatmap of Pickup points on NYC map\n\nReference: https://alysivji.github.io/getting-started-with-folium.html\n\nObservations:\n* Most of pickup points are in Manhattan (more specifically in Lower Manhattan, Midtown, Upper East and Upper West side)\n* Certain area in Brooklyn (Dumbo near Manhattan and Brooklyn Bridge and williamsburg) and Long Island City in Queens contribute to most of pickup points in those two boroughs.\n* Airports (JFK, LGA both in Queens) have very high heatmap (infer people take taxi to city frequently)\n* On zooming in, hotspots like Penn Station, Times Square, Port Authority, Grand Central, South Ferry, Columbus Circle have high pickup counts. (maybe due to commuters taking cabs from penn st, port authority, grand central, south ferry (staten island people :P) and office areas (columbus circle, times square area)."},{"metadata":{"trusted":true,"_uuid":"37755e4efbac00354a701a51bb53f13e7d0717c6"},"cell_type":"code","source":"# initialize map with first row from training data as coordinates\nm = folium.Map([40.721317, -73.844315], zoom_start=11)\n\nfor index, row in train_df[0:1000].iterrows():\n    folium.CircleMarker([row['pickup_latitude'], row['pickup_longitude']],\n                        radius=0.00001,\n                        fill_color=\"#3db7e4\"\n                       ).add_to(m)\n    \n# convert to (n, 2) nd-array format for heatmap\nstationArr = train_df[0:2500][['pickup_latitude', 'pickup_longitude']].as_matrix()\n\n# plot heatmap\nm.add_child(plugins.HeatMap(stationArr, radius=15))\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5636999e9f0d9f61c1e66fd9db2af792e9d5a2ee"},"cell_type":"code","source":"del stationArr\ndel m","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"830aa014b289c6b8dfec655e255717c58192703e"},"cell_type":"markdown","source":"### Heatmap of Dropoff points on NYC map\n\nReference: https://alysivji.github.io/getting-started-with-folium.html\n\nObservations:\n* Unlike pickup points which are mostly concentrated in / near manhattan, dropoff points are spread little farther from Manhattan.Still most of dropoff points are spread throughout in Manhattan. (This time, it includes Harlem, Uptown on western side and part of Bronx as well)\n* Certain area in Brooklyn, this time spread upto Prospect Park (Dumbo near Manhattan and Brooklyn Bridge and williamsburg) and Long Island City plus Jamica in Queens contribute to most of pickup points in those two boroughs. (Jamaica maybe because people take cab to it and then airtrain)\n* Again airports (JFK, LGA both in Queens) and Newark (EWR) in NJ have very high heatmap."},{"metadata":{"trusted":true,"_uuid":"cb7636154994fed60b31db94c37d80c0f9c1a7f8"},"cell_type":"code","source":"m = folium.Map([40.712276, -73.841614], zoom_start=11)\n\nfor index, row in train_df[0:1000].iterrows():\n    folium.CircleMarker([row['dropoff_latitude'], row['dropoff_longitude']],\n                        radius=0.00001,\n                        fill_color=\"#3db7e4\"\n                       ).add_to(m)\n    \n# convert to (n, 2) nd-array format for heatmap\nstationArr = train_df[0:2500][['dropoff_latitude', 'dropoff_longitude']].as_matrix()\n\n# plot heatmap\nm.add_child(plugins.HeatMap(stationArr, radius=15))\nm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"381efe167354f2136d8ab1c706c843934bd718bb"},"cell_type":"code","source":"del stationArr\ndel m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21956b784bdaefee5c51fcdc51fda60314f9e284"},"cell_type":"code","source":"train_df=train_df[0:10**5]\ntrain_df=train_df.drop(['manhattan_dist','haversine_dist'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5762b7590b43617f05d6c5b8b9ab8e52d2856384"},"cell_type":"markdown","source":"## Task 5\n\nGenerate additional features like those from (2) from the given data set. What additional features can you create? (10 pt)\n"},{"metadata":{"_uuid":"8da75bbbfadc3cd9feba4f7b47d6b799dd79bc36"},"cell_type":"markdown","source":"### Additional Features:\n\n* Distance from / to Airports (already added 6 new columns as features as done above while cleaning dataset)\n\n* Cross Borough Travel: Using NYC Open Data for Borough Boundaries (https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm), get the shape file which contains polynomial containing boundary points for each five boroughs. Find out whether pickup and dropoff points for each taxi ride are in same borough or not. (part of external dataset as well)\n\n* Compute actual road driving distance between points using OSRM (Open Source Routing Machine) APIs. (part of external dataset as well)\n\nOSRM and NYC Open Data are used as part of external datasets incorporated."},{"metadata":{"_uuid":"288a48972c1f5965164df9d183acf33f34e0804f"},"cell_type":"markdown","source":"### Feature: Cross Borough Travel"},{"metadata":{"_uuid":"a2b0e024edb5f4bb271550846d69a3ad70d9c42e"},"cell_type":"markdown","source":"Using fiona to read shape file and display each borough shapes"},{"metadata":{"trusted":true,"_uuid":"2b0597a5c1779abbfd2bf25843ea9e04f57d83a3"},"cell_type":"markdown","source":"multipolys = fiona.open(\"../input/nycboundariesshape/borough.shp\")\nprint(multipolys.schema)\n#len(multipolys)"},{"metadata":{"_uuid":"ac43546f88fff50070ff2bd2bd4dd020f711c353"},"cell_type":"markdown","source":"Convert longitude and latitude to point object necessary for checking whether it is inside or outside borough boundary"},{"metadata":{"trusted":false,"_uuid":"7330515a13c6f0e9c382ae0c92585b51c94ddac7"},"cell_type":"markdown","source":"%%time\n\npickup_points = train_df.apply(lambda x: Point((float(x.pickup_longitude), float(x.pickup_latitude))), axis=1)\ndropoff_points = train_df.apply(lambda x: Point((float(x.dropoff_longitude), float(x.dropoff_latitude))), axis=1)\ntrain_df[\"cross_borough\"]=0"},{"metadata":{"trusted":true,"_uuid":"27c386dffd1d283ecf295d54dc826c4ffb643ef2"},"cell_type":"markdown","source":"# Borough name and group number (0,1,2,3,4)\npoly  = geopandas.GeoDataFrame.from_file('../input/nycboundariesshape/borough.shp')\nprint(poly.boro_name)"},{"metadata":{"trusted":false,"_uuid":"ee06830780734838dac607e2cdd03b546f87a632"},"cell_type":"code","source":"# Optimized method to compute whether point is inside any of the polygons or not\n'''\n%%time\n\ntstPT = train_df.apply(lambda x: Point((float(x.pickup_longitude), float(x.pickup_latitude))), axis=1)\ncrs = {'init': 'epsg:27700'}\ngdf = geopandas.GeoDataFrame(train_df, crs=crs, geometry = tstPT)\n\npointInPolys = sjoin(gdf, poly, how='left')\npickup_grouped = pointInPolys.groupby('index_right').groups\n\ntstPT = train_df.apply(lambda x: Point((float(x.dropoff_longitude), float(x.dropoff_latitude))), axis=1)\ncrs = {'init': 'epsg:27700'}\ngdf = geopandas.GeoDataFrame(train_df, crs=crs, geometry = tstPT)\n\npointInPolys = sjoin(gdf, poly, how='left')\ndropoff_grouped = pointInPolys.groupby('index_right').groups\n#display(len(dropoff_grouped[0.0]))\n\n###\nacross_borough=[]\nfor ind, row in train_df.iterrows():\n    in_bronx=(ind in pickup_grouped[1.0]) and (ind in dropoff_grouped[1.0])\n    in_staten=(ind in pickup_grouped[2.0]) and (ind in dropoff_grouped[2.0])\n    in_brooklyn=(ind in pickup_grouped[3.0]) and (ind in dropoff_grouped[3.0])\n    in_queens=(ind in pickup_grouped[4.0]) and (ind in dropoff_grouped[4.0])\n    in_manhattan=(ind in pickup_grouped[0.0]) and (ind in dropoff_grouped[0.0])\n    if in_manhattan==True or in_bronx==True or in_staten==True or in_brooklyn==True or in_queens==True:\n        across_borough.append(0)\n    else:\n        across_borough.append(1)\n        \n###\ntrain_df['across_borough']=across_borough \n\n###\ndel tstPT\ndel crs\ndel gdf\ndel pointInPolys\ndel pickup_grouped\ndel dropoff_grouped\n\nprint(\"Number of rides across borough\")\nlen(train_df[train_df.across_borough==1])\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cb599664af1544445e118144c1d56ee0bf0df9d"},"cell_type":"markdown","source":"Check whether pickup and dropoff points are in same borough or not (1 if in same, else 0)"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0ac23d9fb23187aca9a7bed5401c8c214e81d0e9"},"cell_type":"markdown","source":"# slower method to work on smaller dataset\n%%time\n\nalready_visited=[]\nfor multi in multipolys:\n    shp=shape(multi['geometry'])\n    print(multi[\"properties\"])\n    for i in range(len(pickup_points)):\n        if i%20000==0:\n            print(i)\n        if i in already_visited:\n            continue\n        a_in=pickup_points[i].within(shp)\n        b_in=dropoff_points[i].within(shp)\n        if (a_in is True and b_in is False) or (a_in is False and b_in is True):\n            train_df[\"cross_borough\"][i]=1\n            already_visited.append(i)"},{"metadata":{"_uuid":"838261ec9497fd8914cb16d4aa83568460377ae9"},"cell_type":"markdown","source":"see how many records are for cross borough travel among first 100K records: 11727"},{"metadata":{"trusted":false,"_uuid":"a67e760e0f6083a6516acbadecd48f0dbc722cb9"},"cell_type":"markdown","source":"len(train_df[train_df.cross_borough==1])"},{"metadata":{"trusted":false,"_uuid":"1477e6aed86c29b0fc5437fbc9002df431dae29a"},"cell_type":"markdown","source":"del pickup_points\ndel dropoff_points"},{"metadata":{"_uuid":"8ee9a91a9367c8f617becccbcc0730d609598a5e"},"cell_type":"markdown","source":"Check correlation between cross_borough vs Fare amount: 0.6375859534462518 (positive correlation)"},{"metadata":{"trusted":false,"_uuid":"afa7388ceb0788a7dbd5c25dba887ddbcbe5a3b1"},"cell_type":"markdown","source":"train_df.cross_borough.corr(train_df.fare_amount)"},{"metadata":{"_uuid":"eb71faf92f735599f5a79da098dcf40355fd834e"},"cell_type":"markdown","source":"Create new feature for test data as well (similar process to train set)"},{"metadata":{"trusted":false,"_uuid":"9f25c86abe39f3bda7f16c5a079fc1e1622563ee"},"cell_type":"markdown","source":"multipolys = fiona.open(\"borough.shp\")\nprint(multipolys.schema)\n\n%%time\n\npickup_points = test_df.apply(lambda x: Point((float(x.pickup_longitude), float(x.pickup_latitude))), axis=1)\ndropoff_points = test_df.apply(lambda x: Point((float(x.dropoff_longitude), float(x.dropoff_latitude))), axis=1)\ntest_df[\"cross_borough\"]=0\n\n%%time\n\nalready_visited=[]\nfor multi in multipolys:\n    shp=shape(multi['geometry'])\n    print(multi[\"properties\"])\n    for i in range(len(pickup_points)):\n        if i%2000==0:\n            print(i)\n        if i in already_visited:\n            continue\n        a_in=pickup_points[i].within(shp)\n        b_in=dropoff_points[i].within(shp)\n        if (a_in is True and b_in is False) or (a_in is False and b_in is True):\n            test_df[\"cross_borough\"][i]=1\n            already_visited.append(i)\n            \nlen(test_df[test_df.cross_borough==1])"},{"metadata":{"_uuid":"5dfcdf0c84c3ba04b9ee70fe3873cc22bf820f3b"},"cell_type":"markdown","source":"## Task 7 (Task 6 to follow next)\n\nConsider external datasets that may be helpful to expand your feature set. Give bullet points explaining all the datasets you could identify that would help improve your predictions. If possible, try finding such datasets online to incorporate into your training. List any that you were able to use in your analysis. (10 pt)\n\n\n* NYC Open Data to get borough boundaries shape (https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm) (as shown above to construct new feature: cross borough travel, giving correlation of 0.64 with fare amount)\n\n* OSRM (Open Source Routing Machine) APIs to compute actual road distances between two points. (since API has limits, used here on first 1 million records to compute distance for small data models, not for highest rank achieved model on Kaggle). oSRM achieves correlation of 0.82 with fare amount.\n"},{"metadata":{"_uuid":"9f45ee41ce6ac23cea7731fedc7550609e543e38"},"cell_type":"markdown","source":"Use OSRM curl url giving input as coordinates and query for actual driving distance."},{"metadata":{"trusted":false,"_uuid":"293d3caa72fd2f332f803bc1b475792e549e7b72"},"cell_type":"markdown","source":"%%time\n\ntrain_df[\"actual_dist\"]=0\nfor index, row in train_df.iterrows():\n    if index%10000==0:\n        print(index)\n    url = \"http://router.project-osrm.org/route/v1/driving/\"+str(round(row[\"pickup_longitude\"],6))+\",\"+str(round(row[\"pickup_latitude\"],6))+\";\"+str(round(row[\"dropoff_longitude\"],6))+\",\"+str(round(row[\"dropoff_latitude\"],6))+\"?overview=false\"\n    contents = urllib.request.urlopen(url).read()\n    data = json.loads(contents.decode('utf8'))\n    train_df[\"actual_dist\"][index]=data[\"routes\"][0][\"legs\"][0][\"distance\"]"},{"metadata":{"_uuid":"565e76754f3b76b64a54798a134241c73fcf2142"},"cell_type":"markdown","source":"Correlation between actual road distance obtained from OSRM against fare amount"},{"metadata":{"trusted":false,"_uuid":"81b87c32c9bf06be5a4760d08f4841f182aba490"},"cell_type":"markdown","source":"train_df.actual_dist.corr(train_df.fare_amount)"},{"metadata":{"trusted":false,"_uuid":"0e867167e48c1e8517e90051f8ea96804bd3368b"},"cell_type":"markdown","source":"# dropping now not required geometry field\ntrain_df=train_df.drop(['geometry'],axis=1)"},{"metadata":{"_uuid":"0bad1f2a8c80a7ef5ca5fc59d532940a15b37ba7"},"cell_type":"markdown","source":"Calculate OSRM distances for test data as well"},{"metadata":{"trusted":false,"_uuid":"e18cbe79e3a01be29379ff596672cc4b55028a5b"},"cell_type":"markdown","source":"%%time\n\ntest_df[\"actual_dist\"]=0\n\nfor index, row in test_df.iterrows():\n    if index%1000==0:\n        print(index)\n    url = \"http://router.project-osrm.org/route/v1/driving/\"+str(round(row[\"pickup_longitude\"],6))+\",\"+str(round(row[\"pickup_latitude\"],6))+\";\"+str(round(row[\"dropoff_longitude\"],6))+\",\"+str(round(row[\"dropoff_latitude\"],6))+\"?overview=false\"\n    contents = urllib.request.urlopen(url).read()\n    data = json.loads(contents.decode('utf8'))\n    test_df[\"actual_dist\"][index]=data[\"routes\"][0][\"legs\"][0][\"distance\"]"},{"metadata":{"_uuid":"4635e5076a0b4d310970596884f5d3bb315eb171"},"cell_type":"markdown","source":"### Saving final train / test data to Feather files"},{"metadata":{"trusted":true,"_uuid":"122e64cbb11fa21b53fbbb5654da4743de3d88a4"},"cell_type":"code","source":"%%time\n# saving training dataframe to feather file\ntrain_df=train_df.reset_index(drop=True)\ntrain_df.to_feather('final_train.feather')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"314417b9c89f079a2d1670ad4f9f22891f91a866"},"cell_type":"markdown","source":"Remove not required columns from test data"},{"metadata":{"trusted":true,"_uuid":"fb437f28aba91598a740ff496a017a7a9a0a6155"},"cell_type":"code","source":"test_df=test_df.drop(['manhattan_dist','haversine_dist'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e3444895ca316a08832cf52f0856edf06e76764"},"cell_type":"code","source":"%%time\n# saving training dataframe to feather file\ntest_df=test_df.reset_index(drop=True)\ntest_df.to_feather('final_test.feather')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cdc24301f692c102f4516c1e9957db0024ca33e"},"cell_type":"markdown","source":"Again read train / test data from feather file\n\nNow we will build models using this data."},{"metadata":{"trusted":true,"_uuid":"3eb6e24247c994daa03d7432932ffe9d2abc3e66"},"cell_type":"code","source":"%%time\n\n# read from feather file (not from original train csv) for fast loading \n\ntrain_df = pd.read_feather('final_train.feather')\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fffd1632c321cb6d6ebb7e296e8b4456132c7645"},"cell_type":"code","source":"%%time\n\n# read from feather file (not from original train csv) for fast loading \n\ntest_df = pd.read_feather('final_test.feather')\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89fdc304e682fbef6b0ff227f0aabbb38cffa6d9"},"cell_type":"markdown","source":"Check correlation between features before building models.\n\nHigh positive correlation of fare amount with euclidean distance, cross borough travel and actual distance computed by OSRM."},{"metadata":{"trusted":true,"_uuid":"9eea27041feebed2fe1e3549df61c807b9aa2737"},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = train_df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52e2626ed1723846f003c818d9533733d2c69f31"},"cell_type":"code","source":"train_df.corr(method='pearson', min_periods=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ef779a94d99127095c5ab69ac8b725e32b62df8"},"cell_type":"markdown","source":"Drop certain features from train and test data"},{"metadata":{"trusted":true,"_uuid":"e67b2b037bf47ec9f2fbeb6b2c622d8f2d8d3f31"},"cell_type":"code","source":"key = test_df['key']\ntest_df = test_df.drop(['key','pickup_datetime'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b467cfd2e99dc7f4b5536eb06968bf28ad3b0ef"},"cell_type":"code","source":"y_train_fare=train_df['fare_amount']\ntrain_df=train_df.drop(['fare_amount','pickup_datetime'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"15de19eee2e40be89c9ce030599e1dee470c48f3"},"cell_type":"code","source":"### Drop any more features if you want\n\n#test_df = test_df.drop(['passenger_count','lga_pickup_dist','lga_dropoff_dist','actual_dist'],axis=1)\n\n#train_df=train_df.drop(['passenger_count','lga_pickup_dist','lga_dropoff_dist','actual_dist'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e54cc90e08482d1f9194625e32d3839ca47518e7"},"cell_type":"markdown","source":"## Task 6\nSet up a simple linear regression model to predict taxi fare. Use your generated features from the previous task if applicable. How well/badly does it work? What are the coefficients for your features? Which variable(s) are the most important one? (12 pt)\n"},{"metadata":{"_uuid":"66e2c702835bd6e9e8e2eaad75af838574ba87c6"},"cell_type":"markdown","source":"Split training set into part train and part test for applying linear regression and checking RMSE on training data. Use sklearn's train_test_split library to achieve this."},{"metadata":{"trusted":true,"_uuid":"0a54c44a05fd1bccc190ce8073770958ab1e6953"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_df,y_train_fare, test_size=0.01)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9168fe48e927ab49d6f24a8c314c8f575bad5cbb"},"cell_type":"markdown","source":"### Simple Linear Regression Model"},{"metadata":{"trusted":true,"_uuid":"9eb8da44947548386217c96e9fe2389233aade46"},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train,y_train)\nprint(lm.score(X_train,y_train))\nprint(lm.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd653c4e3cf0770f694efa4c37b1be93c8462733"},"cell_type":"markdown","source":"Model works well (similar as on actual training) on part testset taken from training data.\n\nCoefficients for features are computed and outputted as follows:"},{"metadata":{"trusted":true,"_uuid":"09c4d0542795baba5593c600527e5f9511da3d50"},"cell_type":"code","source":"train_df.info()\n\nprint('Intercept', round(lm.intercept_, 4))\n\nprint('pickup_longitude coef: ', round(lm.coef_[0], 4),\n      '\\npickup_latitude coef:', round(lm.coef_[1], 4), \n      '\\ndropoff_longitude coef:', round(lm.coef_[2], 4),\n      '\\ndropoff_latitude coef:', round(lm.coef_[3], 4), \n      '\\npassenger_count coef:', round(lm.coef_[4], 4), \n      '\\njfk_pickup_dist coef:', round(lm.coef_[5], 4), \n      '\\nlga_pickup_dist coef:', round(lm.coef_[6], 4), \n      '\\newr_pickup_dist coef:', round(lm.coef_[7], 4), \n      '\\njfk_dropoff_dist coef:', round(lm.coef_[8], 4), \n      '\\nlga_dropoff_dist coef:', round(lm.coef_[9], 4), \n      '\\newr_dropoff_dist coef:', round(lm.coef_[10], 4), \n      '\\neuclidean_dist coef:', round(lm.coef_[11], 4), \n      #'\\ncross_borough coef:', round(lm.coef_[12], 4), \n      #'\\nactual_dist coef:', round(lm.coef_[13], 4)\n     )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62246ff8f0ca2a55b4124d7ec0cd1b03aa8525b8"},"cell_type":"markdown","source":"### Important features:\n    euclidean_dist, ewr_pickup_dist, lga_pickup_dist (positive correlated with fare)\n    lga_dropoff_dist, dropoff_longitude, pickup_longitude  (negatively correlated with fare)"},{"metadata":{"_uuid":"9ae5ce3ceee7368a48e6dae0c0708fd5a293a135"},"cell_type":"markdown","source":"Check RMSE error on entire training set with LR"},{"metadata":{"trusted":true,"_uuid":"45d6933582c73b7cdf683cdc50af35d49777ab80"},"cell_type":"code","source":"y_pred = lm.predict(train_df)\nlrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_train_fare))\nlrmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abe67bc354ebf1448def5b3dd5bdc40182615869"},"cell_type":"markdown","source":"Now, let's predict on test data and output to csv file for Kaggle upload"},{"metadata":{"trusted":true,"_uuid":"e89096a111ca16c8d2d3c9409fbece7fb6c20f39"},"cell_type":"code","source":"LinearPredictions = lm.predict(test_df)\nLinearPredictions = np.round(LinearPredictions, decimals=2)\nLinearPredictions\n\nlinear_submission = pd.DataFrame({\"key\": key,\"fare_amount\": LinearPredictions},columns = ['key','fare_amount'])\nlinear_submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce496df4e7f4d6f9f36aaff9451e57091f74631a"},"cell_type":"markdown","source":"## Task 8\n\nNow, try to build a better prediction model that works harder to solve the task. Perhaps it will still use linear regression but with new features. Perhaps it will preprocess features better (e.g. normalize or scale the input vector, convert non-numerical value into float, or do a special treatment of missing values). Perhaps it will use a different machine learning approach (e.g. nearest neighbors, random forests, etc). Briefly explain what you did differently here versus the simple model. Which of your models minimizes the squared error? (10 pt)\n"},{"metadata":{"_uuid":"0c0af20e5a25a4abc055e3fc2d3359c2b9595361"},"cell_type":"markdown","source":"Rerun the model with xgboost and random forest.\n\nBoth gives better result than linear regression. (due to fact that they capture non-linearity in data more properly, maybe linear regression was overfitting due to high amount of data and large features)"},{"metadata":{"_uuid":"ca0fe78e1dd7b67535999863b9f0a268163d0e89"},"cell_type":"markdown","source":"### xgboost\n\nBelow is xgboost with linear regularization, learning rate set to 0,1, evaluation metric is RMSE, ran for 50 rounds\n\nIt provides better execution speed and better results (maybe it solves the problem of vanishing gradients with LR due to gradient boosting)"},{"metadata":{"trusted":true,"_uuid":"badc6f7b21b70e776ecb101c90be641a5cce84df"},"cell_type":"code","source":"dtrain = xgb.DMatrix(train_df, label=y_train_fare)\ndtest = xgb.DMatrix(test_df)\nparams = {'max_depth':7,\n          'eta':1,\n          'silent':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.1\n         }\nnum_rounds = 50\nxb = xgb.train(params, dtrain, num_rounds)\ny_pred_xgb = xb.predict(dtest)\nprint(y_pred_xgb)\nxgb_submission = pd.DataFrame({\"key\": key,\"fare_amount\": y_pred_xgb},columns = ['key','fare_amount'])\nxgb_submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"53086305f236f36edeb8aa2a7af046e5c2e0d90e"},"cell_type":"markdown","source":"### Random Forest\n\nusing sklearn random forest library to build model and predict on test data.\n\nIt takes lots of time to run on entire dataset but gives best accuracy (minimizes RMSE)."},{"metadata":{"trusted":true,"_uuid":"02bb41458b46299bfcdbf84aa9277d6c7636d543"},"cell_type":"code","source":"rf = RandomForestRegressor()\nrf.fit(train_df,y_train_fare)\nrf_predict = rf.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf63d38b2f7ec16fc4b87c54cc635ce7c20ee494"},"cell_type":"code","source":"y_pred = rf.predict(train_df)\nlrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_train_fare))\nlrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a07a2ba1d2f7252bc17eebb5a1f25439ba66f862"},"cell_type":"code","source":"rf_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c989d9227d07ca6f63cb4164a66e9fbea2579530"},"cell_type":"code","source":"rf_submission = pd.DataFrame({\"key\": key,\"fare_amount\": rf_predict},columns = ['key','fare_amount'])\nrf_submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e2071550f37f9d54fa37cb17a1791673976b9693"},"cell_type":"markdown","source":"Random Forest minimizes RMSE on Kaggle. (Best RMSE: 3.35137)"},{"metadata":{"_uuid":"2a361b64151e0d094ab57c13394a0829aa12ba31"},"cell_type":"markdown","source":"## Task 9\n\nPredict all the taxi fares for instances at file sample_submission.csv. Write the result into a csv file and submit it to the website. You should do this for every model you develop. Report the rank, score, number of entries, for your highest rank. Include a snapshot of your best score on the leaderboard as confirmation. (15 pt)\n"},{"metadata":{"_uuid":"9748e449ff6a3eb77470aa8e3cf28135ac36a2df"},"cell_type":"markdown","source":"* Linear Regression: Best RMSE 4.33129\n* Xgboost: Best RMSE 3.68630\n* Random Forest: Best RMSE 3.35137\n\nTotal Number of Entries: 12\n\nProfile Link: https://www.kaggle.com/yashah19\n    \n![Kaggle Rank / RMSE](kaggle.PNG)"},{"metadata":{"trusted":false,"_uuid":"b74f66babef05cfda2fb5bfdf04a39c6ece3a906"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}