{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.413909Z","iopub.execute_input":"2022-01-07T07:42:13.414684Z","iopub.status.idle":"2022-01-07T07:42:13.428609Z","shell.execute_reply.started":"2022-01-07T07:42:13.414588Z","shell.execute_reply":"2022-01-07T07:42:13.427781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\n\nSeed = 42\nnp.random.seed(Seed)\n\n# import sklearn libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.430353Z","iopub.execute_input":"2022-01-07T07:42:13.431099Z","iopub.status.idle":"2022-01-07T07:42:13.9202Z","shell.execute_reply.started":"2022-01-07T07:42:13.431044Z","shell.execute_reply":"2022-01-07T07:42:13.919349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Build Functions","metadata":{}},{"cell_type":"markdown","source":"##### Calculate Time","metadata":{}},{"cell_type":"code","source":"def calculate_time(s_time):\n    seconds = np.round(time.time() - s_time, 0)\n    minutes = hours = 0\n\n    # claculate minutes\n    while seconds > 60:\n        minutes += 1\n        seconds -= 60\n    \n    # calculate hours\n    while minutes > 60:\n        hours += 1\n        minutes -= 60\n\n    print(f'Cell Executed in {hours}h {minutes}m {seconds}s')","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.921823Z","iopub.execute_input":"2022-01-07T07:42:13.922114Z","iopub.status.idle":"2022-01-07T07:42:13.927698Z","shell.execute_reply.started":"2022-01-07T07:42:13.922057Z","shell.execute_reply":"2022-01-07T07:42:13.926804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Save scaled dataset \nwe use it in the future rather than recleaning and saling the data","metadata":{}},{"cell_type":"code","source":"def save_data(data, labels, file_name, path='./'):\n    df = pd.DataFrame(data, columns=labels)\n    df.to_csv(f'{path}/{file_name}.csv', index=False )","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.930312Z","iopub.execute_input":"2022-01-07T07:42:13.930793Z","iopub.status.idle":"2022-01-07T07:42:13.938015Z","shell.execute_reply.started":"2022-01-07T07:42:13.930753Z","shell.execute_reply":"2022-01-07T07:42:13.937203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Save Scaled Data\nsave numpy arrays as csv file for future use","metadata":{}},{"cell_type":"code","source":"\ndef save_data(data, labels, file_name, path='./'):\n    df = pd.DataFrame(data, columns=labels)\n    df.to_csv(f'{path}/{file_name}.csv', index=False )\t\t","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.939489Z","iopub.execute_input":"2022-01-07T07:42:13.940103Z","iopub.status.idle":"2022-01-07T07:42:13.947219Z","shell.execute_reply.started":"2022-01-07T07:42:13.94005Z","shell.execute_reply":"2022-01-07T07:42:13.94645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Remove Nulls","metadata":{}},{"cell_type":"code","source":"def remove_nulls(df, cols):\n    for col in cols:\n        df[col] = df[col].fillna(df[col].mean())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.948577Z","iopub.execute_input":"2022-01-07T07:42:13.948951Z","iopub.status.idle":"2022-01-07T07:42:13.955827Z","shell.execute_reply.started":"2022-01-07T07:42:13.948913Z","shell.execute_reply":"2022-01-07T07:42:13.955159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Drop Features","metadata":{}},{"cell_type":"code","source":"# split the target and drop key and fare_amount from tain data\ndef drop_features(df, features= ['fare_amount', 'key']):\n    df.drop(features, axis = 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.957436Z","iopub.execute_input":"2022-01-07T07:42:13.957851Z","iopub.status.idle":"2022-01-07T07:42:13.966911Z","shell.execute_reply.started":"2022-01-07T07:42:13.957814Z","shell.execute_reply":"2022-01-07T07:42:13.966048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####  split datetime feature","metadata":{}},{"cell_type":"code","source":"# split datetime feature in dummy feature with [year, month, day, hour, minute, seconds]\ndef split_date_time(df, feature_name = 'pickup_datetime'):\n    print('convert feature into datetime')\n    date_feature = pd.to_datetime( df[feature_name])\n\n    print('get year from feature:')\n    year = date_feature.dt.year\n\n    print('get Month from feature:')\n    month = date_feature.dt.month\n\n    print('get day from feature:')\n    day = date_feature.dt.day\n\n    print('get hour from feature:')\n    hour = date_feature.dt.hour\n\n    print('get minute from feature:')\n    minute = date_feature.dt.minute\n\n    print('get second from feature:')\n    second = date_feature.dt.second\n\n    print('add these cols as new features:')\n    df['year'] = year\n    df['month'] = month\n    df['day'] = day\n    df['hour'] = hour\n    df['minute'] = minute\n    df['second'] = second\n","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.970016Z","iopub.execute_input":"2022-01-07T07:42:13.970368Z","iopub.status.idle":"2022-01-07T07:42:13.977686Z","shell.execute_reply.started":"2022-01-07T07:42:13.970329Z","shell.execute_reply":"2022-01-07T07:42:13.977011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load the dataset","metadata":{}},{"cell_type":"code","source":"dir_path = '/kaggle/input/new-york-city-taxi-fare-prediction/'\n\n# Load the dataset\ntrain = pd.read_csv(f'{dir_path}train.csv', nrows=30_000_000)\ntest = pd.read_csv(f'{dir_path}test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:42:13.979189Z","iopub.execute_input":"2022-01-07T07:42:13.979705Z","iopub.status.idle":"2022-01-07T07:43:17.353043Z","shell.execute_reply.started":"2022-01-07T07:42:13.979669Z","shell.execute_reply":"2022-01-07T07:43:17.352139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['pickup_datetime', 'pickup_longitude',\n       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n       'passenger_count']","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:43:17.354499Z","iopub.execute_input":"2022-01-07T07:43:17.354762Z","iopub.status.idle":"2022-01-07T07:43:17.35973Z","shell.execute_reply.started":"2022-01-07T07:43:17.354729Z","shell.execute_reply":"2022-01-07T07:43:17.358955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:43:50.528333Z","iopub.execute_input":"2022-01-07T07:43:50.528617Z","iopub.status.idle":"2022-01-07T07:43:50.550696Z","shell.execute_reply.started":"2022-01-07T07:43:50.528586Z","shell.execute_reply":"2022-01-07T07:43:50.549748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:43:17.387162Z","iopub.execute_input":"2022-01-07T07:43:17.387639Z","iopub.status.idle":"2022-01-07T07:43:17.401934Z","shell.execute_reply.started":"2022-01-07T07:43:17.387602Z","shell.execute_reply":"2022-01-07T07:43:17.401201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the target from train\ntarget = train['fare_amount']\ntarget.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:43:58.798565Z","iopub.execute_input":"2022-01-07T07:43:58.798839Z","iopub.status.idle":"2022-01-07T07:43:58.811529Z","shell.execute_reply.started":"2022-01-07T07:43:58.798809Z","shell.execute_reply":"2022-01-07T07:43:58.810799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove key, fare_amount features\ndrop_features(train)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:43:59.705321Z","iopub.execute_input":"2022-01-07T07:43:59.705907Z","iopub.status.idle":"2022-01-07T07:44:00.458842Z","shell.execute_reply.started":"2022-01-07T07:43:59.705864Z","shell.execute_reply":"2022-01-07T07:44:00.458029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### data Exploratory","metadata":{}},{"cell_type":"markdown","source":"#### Cleaning the Data","metadata":{}},{"cell_type":"code","source":"# detect Null values\ntrain.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:44:04.419854Z","iopub.execute_input":"2022-01-07T07:44:04.42016Z","iopub.status.idle":"2022-01-07T07:44:07.49454Z","shell.execute_reply.started":"2022-01-07T07:44:04.420126Z","shell.execute_reply":"2022-01-07T07:44:07.4938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill null values using the mean of the rest of the feature samples\nnull_features = ['dropoff_longitude', 'dropoff_latitude']\nremove_nulls(train, null_features)\ntrain.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:44:07.496234Z","iopub.execute_input":"2022-01-07T07:44:07.496601Z","iopub.status.idle":"2022-01-07T07:44:11.187357Z","shell.execute_reply.started":"2022-01-07T07:44:07.496551Z","shell.execute_reply":"2022-01-07T07:44:11.18578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_time = time.time()\n# split datetime feature\nsplit_date_time(train, feature_name='pickup_datetime')\n\ndrop_features(train, ['pickup_datetime'])\n\ncalculate_time(s_time)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T07:44:11.188794Z","iopub.execute_input":"2022-01-07T07:44:11.189087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s_time = time.time()\n\n# Split datetime feature in test set\nsplit_date_time(test)\n\n# remove key and pickup_datetime features\ndrop_features(test, ['key', 'pickup_datetime'])\n\ncalculate_time(s_time)\n\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Scale the features","metadata":{}},{"cell_type":"code","source":"s_time = time.time()\n\nsc = StandardScaler()\n\nprint('Scale Training set')\ntrain_scaled = sc.fit_transform(train)\n\nprint('Scale Testing set')\ntest_scaled = sc.transform(test)\n\ncalculate_time(s_time)\n\ntrain_scaled[:5], test_scaled[:5]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Save Train and Test dataset to import directly in future without recleaning","metadata":{}},{"cell_type":"code","source":"labels = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 'hour', 'minute', 'second']\n\n# Save Train data\nprint('Saving Train File....')\nsave_data(train_scaled, labels, 'scaled_train')\n\nprint('Saving Test File....')\n# save Test data\nsave_data(test_scaled, labels, 'scaled_test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Split valid data from train with 20%","metadata":{}},{"cell_type":"code","source":"\nx_train, x_valid , y_train, y_valid = train_test_split(train_scaled, target, test_size=.2)\n\nx_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Build the Models","metadata":{}},{"cell_type":"markdown","source":"##### SVM Regressor","metadata":{}},{"cell_type":"code","source":"s_time = time.time()\nsvr_reg = SVR()\nprint('Fitting the model...')\nsvr_reg.fit(x_train, y_train)\n\nprint('Predicting the model...')\ny_pred = svr_reg.predict(x_valid)\n\nprint('Calculating RMSE....')\nsvr_reg = mean_squared_error(y_valid, y_pred, squared=False)\n\nprint(f'RMSE = {svr_reg}')\n\ncalculate_time(s_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### xgboost Regressor","metadata":{}},{"cell_type":"code","source":"s_time = time.time()\n\nxg_reg = xgboost.XGBRegressor(n_estimators = 100, random_state = Seed)   \n\nprint('Fitting the model...')\nxg_reg.fit(x_train, y_train)     \n\nprint('Predicting the model...')\ny_pred = xg_reg.predict(x_valid)\n\nprint('Calculating RMSE....')\nxg_mse = mean_squared_error(y_valid, y_pred, squared=False)\n\nprint(f'RMSE = {xg_mse}')\n\ncalculate_time(s_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save predicted test data for submission\ny_test_pred = xg_reg.predict(test_scaled)\n\nsubmit = pd.read_csv('sample_submission.csv')\n\nsubmit['fare_amount'] = y_test_pred\n\nsubmit.to_csv('submission_xgboost.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### RandomForest Regressor","metadata":{}},{"cell_type":"code","source":"rnd_reg = RandomForestRegressor(n_estimators=200, max_depth=4, n_jobs=-1, random_state=Seed)\n\nprint('Fitting the model...')\nrnd_reg.fit(x_train, y_train)\n\nprint('Predicting the model...')\ny_pred_rnd = rnd_reg.predict(x_valid)\n\nprint('Calculating RMSE....')\nmse = mean_squared_error(y_valid, y_pred_rnd, squared=False)\n\nprint(f'RMSE = {xg_mse}')\n\ncalculate_time(s_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save predicted test data for submission\ny_test_pred = rnd_reg.predict(test_scaled)\n\nsubmit = pd.read_csv('sample_submission.csv')\n\nsubmit['fare_amount'] = y_test_pred\n\nsubmit.to_csv('submission_randomForest.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rnd[:5], y_valid[:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}