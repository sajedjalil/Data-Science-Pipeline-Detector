{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\n\nThis dataset is a record of taxi trips in New York City, split into training and test datasets.\n\nTraining and test observations both have the same features: passenger count, pickup longitude, pickup latitude, dropoff longitude, dropoff latitude and pickup datetime.\n\nTraining observations are labeled with the fare amount paid.\n\nOur objective is to create a model that will accurately estimate the fare amount of trips in the test dataset. Accuracy is measured by root mean square error.\n\nWe will also be performing exploratory data analysis in an attempt to better understand the dataset.\n\n# Results\n\n### Modelling\n\nOur final model is a weighted average of two separate models:\n\n1. A gradient boosting decision tree model with 94% weight. This model has 28 features, making the most use of the time of year, time of day, trip distance and proximity to JFK airport.\n2. A k-nearest neighbours model with 6% weight. This model uses only pickup and dropoff locations.\n\nThe leaderboard RMSE obtained by this model is 2.93.\n\n### EDA Insights\n\n* Most taxi rides have 1 passenger.\n* Average fare saw a large increase between 2011 and 2013, going from \\\\$10.43 to \\\\$12.58.\n* Fares are highest around 5am, primarily due to long trips leaving the city.\n* People take long trips later in the day on weekends.\n* Airports and the city centre are taxi hotspots.\n* There is an area to the west of the city centre with very high fares per km travelled.\n* There is a location about 90km from the city centre from which long trips come and go with substantially lower fares than we would expect.\n* The grid most streets are laid along can be seen when examining fare vs point to point distance based on the direction of the trip."},{"metadata":{},"cell_type":"markdown","source":"# Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pandas_profiling\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pyproj import Geod\nimport scipy\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.float_format', lambda x: '%.4f' % x)\n\nTRAIN_PATH = '../input/new-york-city-taxi-fare-prediction/train.csv'\nTEST_PATH = '../input/new-york-city-taxi-fare-prediction/test.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Validation and Cleaning\n\nWe load test.csv to check for missing values and see what value range the features lie in."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(TEST_PATH)\nprint('Null values:',test.isnull().sum().sum())\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test dataset is already clean, with no missing or implausible values.\n\nThe training dataset consists of ~55 million rows. A dataset this large will be slow to load and perform operations on, so we load a subset for quick examination."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp = pd.read_csv(TRAIN_PATH, nrows=100000)\nprofile = pandas_profiling.ProfileReport(df_temp, title=\"Profile Report\", minimal=True, progress_bar=False)\nprofile.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The key column is used as an index and can be removed while working with the data.\n\nfare_amount contains some negative values, we will remove these rows and set a plausible upper limit.\n\npassenger_count contains some trips with zero passengers, we will remove these rows and set a plausible upper limit.\n\nThe longitude/latitude for locations in NYC should be around -73/40. The minimum latitude in this sample is -74 while the maximum longitude is 40. It seems possible that these values have been switched in some rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp[df_temp['pickup_longitude']>0].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This confirms the suspicion that some rows have reversed longitude/latitude values. We will switch these values back to the correct orientation then remove rows that do not contain longitude/latitude values within NYC.\n\nThe functions below will load the dataset and apply the outlined changes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_df(df):\n    df['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 15)\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n    \n    #reverse incorrectly assigned longitude/latitude values\n    df = df.assign(rev=df.dropoff_latitude<df.dropoff_longitude)\n    idx = (df['rev'] == 1)\n    df.loc[idx,['dropoff_longitude','dropoff_latitude']] = df.loc[idx,['dropoff_latitude','dropoff_longitude']].values\n    df.loc[idx,['pickup_longitude','pickup_latitude']] = df.loc[idx,['pickup_latitude','pickup_longitude']].values\n    \n    #remove data points outside appropriate ranges\n    criteria = (\n    \" 0 < fare_amount <= 500\"\n    \" and 0 < passenger_count <= 6 \"\n    \" and -75 <= pickup_longitude <= -72 \"\n    \" and -75 <= dropoff_longitude <= -72 \"\n    \" and 40 <= pickup_latitude <= 42 \"\n    \" and 40 <= dropoff_latitude <= 42 \"\n    )\n    df = (df\n          .dropna()\n          .query(criteria)\n          .reset_index()\n          .drop(columns=['rev', 'index'])          \n         )\n    return df\n\ndef load_df(nrows=None, features=None):\n    #load dataframe in chunks if the number of rows requested is high (currently only using 1 million rows for faster training)\n    cols = [\n        'fare_amount', 'pickup_datetime','pickup_longitude', 'pickup_latitude',\n        'dropoff_longitude', 'dropoff_latitude', 'passenger_count'\n    ]\n    df_as_list = []\n    for df_chunk in pd.read_csv(TRAIN_PATH, usecols=cols, nrows=nrows, chunksize=5000000):\n        df_chunk = clean_df(df_chunk) \n        if features == 'explore':\n            df_chunk = exploration_features(df_chunk)\n        elif features == 'model':\n            df_chunk = modelling_features(df_chunk)\n        else:\n            df_chunk = df_chunk.drop(columns='pickup_datetime')\n        df_as_list.append(df_chunk)\n    df = pd.concat(df_as_list)\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"to keep run times short we will only be using 10 million rows of the data at most. We load this subset below, and see that all values are now valid and in a reasonable range."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = load_df(10000000)\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Baseline Models\nGiven the non-linearity of the relationship I expect to exist between location and fare, more flexible models should outperform the linear regression, which will have little to work with before further features are extracted. \n\nFares have a standard deviation of $9.63. This is the first score any model we make should beat, but before further exploring the data set and creating more features, let's get a better baseline we can look to improve on by seeing the RMSE obtained by training some models on just the initial features.\n\nWe will use 4 models: Linear regression, KNN, gradient boosted trees, and an ensemble of the former."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_split_sets(train):\n    x = train.drop(columns=['fare_amount'])\n    y = train['fare_amount'].values\n    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=0)\n    return x_train, x_val, y_train, y_val\n\n\ndef lin_model(x_train, x_val, y_train, y_val):\n    model = LinearRegression()\n    model.fit(x_train, y_train)\n    pred = model.predict(x_val)\n    rmse = np.sqrt(mean_squared_error(y_val, pred))\n    return model, rmse, pred\n\n\ndef knn_model(x_train, x_val, y_train, y_val, neighbors):\n    min_rmse = 1000\n    for n in neighbors:\n        knn = KNeighborsRegressor(n_neighbors=n)\n        knn.fit(x_train, y_train)\n        pred = knn.predict(x_val)\n        rmse = np.sqrt(mean_squared_error(y_val, pred))\n        if rmse < min_rmse:\n            min_rmse = rmse\n            model = knn\n            best_pred = pred\n        print('Neighbours', n, 'RMSE', rmse)\n    return model, min_rmse, best_pred\n\n\ndef lgbm_model(params,x_train, x_val, y_train, y_val):\n    lgbm_train = lgbm.Dataset(x_train, y_train, silent=True)\n    lgbm_val = lgbm.Dataset(x_val, y_val, silent=True)\n    model = lgbm.train(params=params, train_set=lgbm_train, valid_sets=lgbm_val, verbose_eval=100)\n    pred = model.predict(x_val, num_iteration=model.best_iteration)\n    rmse = np.sqrt(mean_squared_error(y_val, pred))\n    return model, rmse, pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = load_df(1000000)\nx_train, x_val, y_train, y_val= get_split_sets(train)\ntest = pd.read_csv(TEST_PATH)\nx_test = test.drop(columns=['key'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_init_model, lin_init_rmse, lin_init_pred = lin_model(x_train, x_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_choices = [10,20,30,40,50,60]\nknn_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']\nknn_init, knn_init_rmse, knn_init_pred = knn_model(x_train[knn_cols], x_val[knn_cols], y_train, y_val, k_choices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {\n    'objective': 'regression',\n    'boosting': 'gbdt',\n    'num_leaves': 400,\n    'learning_rate': 0.1,\n    'max_bin': 3000,\n    'num_rounds': 5000,\n    'early_stopping_rounds': 100,\n    'metric' : 'rmse'\n}\nlgbm_init_model, lgbm_init_rmse, lgbm_init_pred = lgbm_model(lgbm_params, x_train, x_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm.plot_importance(lgbm_init_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Linear Regression RMSE', lin_init_rmse)\nprint('KNN RMSE', knn_init_rmse)\nprint('LightGBM RMSE', lgbm_init_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear regression is predictably our worst performer by far, with a validation RMSE of 8.3020.\n\nKNN regression achieves a validation RMSE of 4.0210, a huge improvement on the linear regression model.\n\nThe LightGBM model obtains a validation RMSE of 4.0261, almost identical to KNN regression. We see in the feature importance graph that the model makes relatively little use of passenger count."},{"metadata":{"trusted":true},"cell_type":"code","source":"init_preds_ave = (lgbm_init_pred+knn_init_pred)/2\nrmse = np.sqrt(mean_squared_error(y_val, init_preds_ave))\nprint('Combined RMSE: ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance of the linear regression model was poor, so we left it out of the ensemble. The averaged prediction of the KNN and LightGBM models produces the lowest RMSE of these initial attempts, with a validation RMSE of 3.958."},{"metadata":{},"cell_type":"markdown","source":"# Exploration and Feature Extraction\n\nWe create a function to extract more features which will be helpful in visualising the information in this dataset.\n\n**Created Features**\n\nDatetime: year, day of the year, day of the week, time of day.  \n\nGeographic: Trip distance, distance between pickup/dropoff and other locations, direction of the trip, binned direction, and distances from the city centre, airports and a hotspot of long distance trips. Our longitude range is 3 degrees and our latitude range is 2 degrees, we bin these values into ~100m wide squares using the fact that a degree of latitude is ~111km and a degree of longitude is ~85km at New York's latitude. We also create 1km square bins. \n\nPrice: Fare per KM."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def distance(lon1,lat1,lon2,lat2):\n    az12,az21,dist = Geod(ellps='WGS84').inv(lon1,lat1,lon2,lat2)\n    return dist\ndef direction(lon1,lat1,lon2,lat2):\n    az12,az21,dist = Geod(ellps='WGS84').inv(lon1,lat1,lon2,lat2)\n    return az12\n\ndef shared_features(df):\n    \"\"\"adds features that will be used by both the modelling and EDA dataframes\"\"\"\n    rows = len(df)\n    #these long/lat values are needed as lists to hand to the distance function\n    nyc_long, nyc_lat = [-74.001541]*rows, [40.724944]*rows    \n    jfk_long, jfk_lat = [-73.785937]*rows, [40.645494]*rows\n    lga_long, lga_lat = [-73.872067]*rows, [40.774071]*rows\n    nla_long, nla_lat = [-74.177721]*rows, [40.690764]*rows\n    chp_long, chp_lat = [-73.137393]*rows, [41.366138]*rows\n    exp_long, exp_lat = [-74.0375]*rows, [40.736]*rows\n    pickup_long = df.pickup_longitude.tolist()\n    pickup_lat = df.pickup_latitude.tolist()\n    dropoff_long = df.dropoff_longitude.tolist()\n    dropoff_lat = df.dropoff_latitude.tolist()\n    \n    #add features to the data\n    df = df.assign(\n        #time features\n        year=df.pickup_datetime.dt.year,\n        dayofyear=df.pickup_datetime.dt.dayofyear,\n        weekday=df.pickup_datetime.dt.dayofweek,\n        time=(df.pickup_datetime.dt.hour+df.pickup_datetime.dt.minute/5),\n        \n        #distance between pickup and dropoff, and bearing from pickup to dropoff\n        distance=distance(pickup_long, pickup_lat, dropoff_long, dropoff_lat),\n        direction=direction(pickup_long, pickup_lat, dropoff_long, dropoff_lat),\n        \n        #distance from locations\n        pickup_dist_nyc=pd.Series(distance(pickup_long, pickup_lat, nyc_long, nyc_lat)),\n        dropoff_dist_nyc=pd.Series(distance(dropoff_long, dropoff_lat, nyc_long, nyc_lat)),\n        pickup_dist_jfk=pd.Series(distance(pickup_long, pickup_lat, jfk_long, jfk_lat)),\n        dropoff_dist_jfk=pd.Series(distance(dropoff_long, dropoff_lat, jfk_long, jfk_lat)),\n        pickup_dist_lga=pd.Series(distance(pickup_long, pickup_lat, lga_long, lga_lat)),\n        dropoff_dist_lga=pd.Series(distance(dropoff_long, dropoff_lat, lga_long, lga_lat)),\n        pickup_dist_nla=pd.Series(distance(pickup_long, pickup_lat, nla_long, nla_lat)),\n        dropoff_dist_nla=pd.Series(distance(dropoff_long, dropoff_lat, nla_long, nla_lat)),\n        pickup_dist_chp=pd.Series(distance(pickup_long, pickup_lat, chp_long, chp_lat)),\n        dropoff_dist_chp=pd.Series(distance(dropoff_long, dropoff_lat, chp_long, chp_lat)),\n        pickup_dist_exp=pd.Series(distance(pickup_long, pickup_lat, exp_long, exp_lat)),\n        dropoff_dist_exp=pd.Series(distance(dropoff_long, dropoff_lat, exp_long, exp_lat))\n    )\n    return df\n\n\ndef exploration_features(df):\n    \"\"\"adds features for use in the EDA section\"\"\"\n    df = shared_features(df)\n    df = (\n        df\n        .assign(\n            hour=df.pickup_datetime.dt.hour,\n            close_to_airport='No',\n            fare_per_km=df.fare_amount*1000/df.distance,\n            direction_bucket = pd.cut(df.direction, np.linspace(-180, 180, 37)),\n\n            #small location buckets\n            pickup_long_bucket=pd.cut(df.pickup_longitude, bins=2550, labels=False),\n            pickup_lat_bucket=pd.cut(df.pickup_latitude, bins=2200, labels=False),\n            dropoff_long_bucket=pd.cut(df.dropoff_longitude, bins=2550, labels=False),\n            dropoff_lat_bucket=pd.cut(df.dropoff_latitude, bins=2200, labels=False),\n\n            #large location buckets\n            pickup_long_bucket_big=pd.cut(df.pickup_longitude, bins=255, labels=False),\n            pickup_lat_bucket_big=pd.cut(df.pickup_latitude, bins=220, labels=False),\n            dropoff_long_bucket_big=pd.cut(df.dropoff_longitude, bins=255, labels=False),\n            dropoff_lat_bucket_big=pd.cut(df.dropoff_latitude, bins=220, labels=False)\n        )\n        .drop(columns='pickup_datetime')\n        .query(\"0 < distance\")\n    )\n    df.loc[((df['pickup_dist_jfk']<1500) | (df['dropoff_dist_jfk']<1500)), 'close_to_airport'] = 'JFK'\n    df.loc[((df['pickup_dist_lga']<1500) | (df['dropoff_dist_lga']<1500)), 'close_to_airport'] = 'LaGuardia'\n    df.loc[((df['pickup_dist_nla']<1500) | (df['dropoff_dist_nla']<1500)), 'close_to_airport'] = 'Newark'  \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We load 5 million rows of the dataset and take a look at passenger counts."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = load_df(5000000, features='explore')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(15,5))\nsns.countplot(train.passenger_count, ax=ax[0])\nax[0].set_xlabel('Passenger Count')\nax[0].set_ylabel('Frequency')\nax[0].set_title('Distribution of Passenger Count')\nsns.barplot(train.passenger_count, train.fare_amount, ax=ax[1], ci=None)\nax[1].set_xlabel('Passenger Count')\nax[1].set_ylabel('Fare ($)')\nax[1].set_title('Average Fare by Passenger Count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most taxi rides only have 1 passenger, but passenger count doesn't show any relationship to the fare amount.\n\nLet's check how the trip distance relates to fare."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 5))\nsns.scatterplot(x='distance', y='fare_amount', data=train).set_title('Fare vs Distance')\nplt.xlabel('Distance (m)')\nplt.ylabel('Fare ($)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see an expected relationship, with fares increasing as distance increases. There are, however, a large number of data points that seem to hug each axis. \n\n1. Many taxi rides with close to 0 distance between the pickup and dropoff locations have much higher fares than we would expect, with the fare distribution of very low distance trips seeming similar to the distribution of all other trips. Could this have some unknown but real cause, or is it erroneous data?\n\n2. As distance goes above ~75km the average fare actually decreases due to a mass of points around 80-110km with fares below $50. Are these trips on routes with less traffic? Fixed price trips? Or is it erroneous data?\n\n**Answering Q1**\n\nIf we assumed that all trips with a pickup to dropoff point distance of less than 50m have had location data entered incorrectly, then we would expect fares for this group to come from the same distribution as properly recorded trips. A confounding factor we should look out for is the potential for some true very short trips to coexist with misrecorded data. \n\nWe will explore the distributions of these two subsets by using a Q-Q plot and the Kolmogorov-Smirnov test."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"percs = np.linspace(0,99,34)\nshort = np.percentile(train[train['distance']<=50].fare_amount, percs)\nlong = np.percentile(train[train['distance']>50].fare_amount, percs)\nsns.scatterplot(x=short, y=long)\nx = np.linspace(np.min((short.min(),long.min())), np.max((short.max(),long.max())))\nplt.plot(x,x, color=\"k\", ls=\"--\")\nplt.title('')\nplt.xlabel('Short Trip Fare ($)')\nplt.ylabel('Long Trip Fare ($)')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ks = scipy.stats.ks_2samp(\n    train.where(train.distance > 50).dropna()['fare_amount'],\n    train.where(train.distance <= 50).dropna()['fare_amount']\n)\nprint('p-value:', ks[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tiny p-value output by the Kolmogorov-Smirnov test and the shape of the QQ-plot mean we can be confident that these subsets follow different distributions. It is tough to say what the cause of the large fare values is but there is some information encoded here, so we will not drop these rides from the dataset or impute the fare amount.\n\n**Answering Q2**\n\nLet's see how many trips are over 75km long."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"long_trips = train[train.distance>75000].fare_amount.count()\nprint(long_trips, 'trips over 75km.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 788 trips made with a pickup to dropoff point distance of over 75km.\n\nNow let's see what the average fare is for these trips compared to shorter trips, 50-75km long."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Average fare for distance over 75km:', train[train.distance>75000].fare_amount.mean())\nprint('Average fare for distance 50-75km:', train.query('50000 < distance < 75000').fare_amount.mean())\nsns.barplot(['50-75km', '>75km'],[train.query('50000 < distance < 75000').fare_amount.mean(),train[train.distance>75000].fare_amount.mean()])\nplt.title('Fare by Trip Distance')\nplt.ylabel('Fare ($)')\nplt.xlabel('Distance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This confirms the huge decrease in fare that is happening for the longest trips. We will plot these trips to see if we can narrow down the cause."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_long_trips(df):\n    rows=len(df)\n    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n    for i in range(rows):\n        plt.plot([df.pickup_longitude[i],df.dropoff_longitude[i]], [df.pickup_latitude[i], df.dropoff_latitude[i]], marker='o', color='b', alpha=0.1)\n    plt.title('Linked Pickup and Dropoff Points for Trips longer than 75km')\n    plt.ylabel('Latitude')\n    plt.xlabel('Longitude')\n    plt.show()    \n\nplot_long_trips(train[train.distance>75000].reset_index())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear source of most of the long distance trips, but is this location responsible for the low fares?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(train[train.distance>75000].query('41.36 < pickup_latitude < 41.37 or 41.36 < dropoff_latitude < 41.37')\n      .fare_amount.count(), f'of the {long_trips} trips with distance>75km start or end in this area')\n\nprint(f'The average fare of these trips is',\n      train[train.distance>75000].query('41.36 < pickup_latitude < 41.37 or 41.36 < dropoff_latitude < 41.37')\n      .fare_amount.mean())\n\nprint(f'The average fare of long trips starting and ending elsewhere is',\n      train[train.distance>75000].query('(41.36 > pickup_latitude or pickup_latitude > 41.37) and (41.36 > dropoff_latitude or dropoff_latitude > 41.37)')\n      .fare_amount.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though not explaining it entirely, we have found a huge contributor to the massive drop in fares we are seeing for long trips. Checking the coordinates on a map, I'm not sure if these data points are incorrect as there does not seem to be anything there. Regardless, with it having such a big impact on fares in this set we will create a feature for our model that measures distance from this area.\n\nNow we look at how time features interact with fares."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.pivot_table('fare_amount', index='year').plot(figsize=(15,2))\nplt.title('Fare Paid by Year')\nplt.ylabel('Fare ($)')\nplt.xlabel('Year')\ntrain.pivot_table('fare_amount', index='dayofyear').plot(figsize=(15,2))\nplt.title('Fare Paid by Day of Year')\nplt.ylabel('Fare ($)')\nplt.xlabel('Day')\ntrain.pivot_table('fare_amount', index='weekday').plot(figsize=(15,2))\nplt.title('Fare Paid by Weekday (Monday-Sunday)')\nplt.ylabel('Fare ($)')\nplt.xlabel('Day')\ntrain.pivot_table('fare_amount', index='time').plot(figsize=(15,2))\nplt.ylabel('Fare ($)')\nplt.xlabel('Time')\nplt.title('Fare Paid by Time of Day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There has been a steady increase in the average fare by year between 2009 and 2015, with a clear jump between 2012 and 2013. \n\nThroughout the year the average fare fluctuates as one would expect given the smaller sample size per data point, but clear seasonality is present.\n\nFares are lowest on Saturdays and highest on Sundays, but are overall quite stable throughout the week (note the scale of the y-axis).\n\nFares show a very large spike in pricing around 5am, commutes into the city in heavy traffic potentially? Let's see how distance from the city centre interacts with pickup time."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train.query('50 < distance').pivot_table('fare_per_km', index='hour', columns='weekday').plot(figsize=(15,2))\nplt.title('$ per KM vs Time of Day')\nplt.ylabel('$ per KM')\nplt.xlabel('24hr Time')\ntrain.pivot_table('distance', index='time', columns='weekday').plot(figsize=(15,2))\nplt.ylabel('Meters')\nplt.xlabel('24hr Time')\nplt.title('Trip Distance vs Time of Day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the spike in average fare around 5am is probably people commuting for the day, and is a result of longer trip distances rather than a high price per km. Price per km is actually at its highest during standard work hours from 9am-5pm.\n\nInterestingly, we can also see that the peak in trip distance occurs later in the day on Saturday and Sunday, evidence of what one would assume - that people take long trips slightly later in the day on the weekend."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(15, 4))\nplt.plot(train.groupby('time').time.unique(), train.groupby('time')['pickup_dist_nyc'].mean()/1000, label='Pickup Distance')\nplt.plot(train.groupby('time').time.unique(), train.groupby('time')['dropoff_dist_nyc'].mean()/1000, label='Dropoff Distance')\nplt.legend(loc=\"upper right\")\nplt.xlabel('24hr Time')\nplt.ylabel('Distance (km)')\nplt.legend()\nplt.title('Distance from City Centre vs Time of Day')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I was expecting the long trips to be people coming in to the city, but it turns out the longer trip distances in the morning are caused by people travelling outwards from the city centre."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def density_heatmap(direction):\n    df = np.log(\n        train\n        .query(f'-74.3 < {direction}_longitude < -73.6')\n        .query(f'40.4 < {direction}_latitude < 41')\n        .groupby([f'{direction}_long_bucket_big',f'{direction}_lat_bucket_big'])\n        .distance\n        .count()\n        .unstack(level=0)\n        .iloc[::-1]\n    )\n    sns.heatmap(df, cmap=\"plasma\", vmax=8, cbar_kws={'label':f'log({direction}s per sq. km)'}, ax=ax)\n    plt.title(f'{direction} density')\n    plt.ylabel('Latitude Bucket')\n    plt.xlabel('Longitude Bucket')\n    \nfig = plt.figure(figsize=(10, 9))\nfig.subplots_adjust(wspace=0.2, right=1.8)\nax = fig.add_subplot(1, 2, 1)\ndensity_heatmap('pickup')\nax = fig.add_subplot(1, 2, 2)\ndensity_heatmap('dropoff')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most pickups and dropoffs occur in the city centre. Checking the location of the three spots that are brighter than we would expect them to be given their distance from the city centre, we can see that they are Newark Liberty Airport, LaGuardia Airport and JFK Airport."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train['close_to_airport'], y=train['fare_amount'])\nplt.title('Fare Distribution by Proximity to Airports')\nplt.ylim(-1,200)\nplt.xlabel('Close to Airport')\nplt.ylabel('Fare ($)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that fares differ substantially based on whether they are close to an airport, and which airport they are close to. We will include features in the model measuring distance from each of these airports.\n\nWe now see if fares themselves differ based on pickup and dropoff location."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def fare_heatmap(direction):\n    df = (\n        train\n        .groupby([f'{direction}_long_bucket', f'{direction}_lat_bucket'])\n        .fare_amount\n        .mean()\n        .unstack(level=0)\n        .iloc[::-1]\n    )\n    sns.heatmap(df, cmap=\"Blues\", vmin= 0, vmax=30, ax=ax, cbar_kws={'label':'Average Fare ($)'})\n    plt.title(f'Average Fare by {direction} Location')\n    plt.ylabel('Latitude Bucket')\n    plt.xlabel('Longitude Bucket')\n    \nfig = plt.figure(figsize=(10, 9))\nfig.subplots_adjust(wspace=0.2, right=1.8)\nax = fig.add_subplot(1, 2, 1)\nfare_heatmap('pickup')\nax = fig.add_subplot(1, 2, 2)\nfare_heatmap('dropoff')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def dist_heatmap(direction):\n    df = (\n        train\n        .groupby([f'{direction}_long_bucket', f'{direction}_lat_bucket'])\n        .distance\n        .mean()\n        .unstack(level=0)\n        .iloc[::-1]\n    )\n    sns.heatmap(df, cmap=\"Blues\", vmin= 0, vmax=10000, ax=ax, cbar_kws={'label':f'Distance (m)'})\n    plt.title(f'Average Distance by {direction} Location')\n    plt.ylabel('Latitude Bucket')\n    plt.xlabel('Longitude Bucket')\n    \nfig = plt.figure(figsize=(10, 9))\nfig.subplots_adjust(wspace=0.2, right=1.8)\nax = fig.add_subplot(1, 2, 1)\ndist_heatmap('pickup')\nax = fig.add_subplot(1, 2, 2)\ndist_heatmap('dropoff')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These heatmaps show that as you get further away from the city centre, fares and trip distances increase, an expected pattern.\n\nThe largest point of interest is the area directly to the west of the city centre. In this area fares are high, but this is not due to a higher average trip distance as there is no such hotspot on the distance heatmap."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def ratio_heatmap(direction):\n    df = (\n        train\n        .query(f'-74.1 < {direction}_longitude < -73.95')\n        .query(f'40.65 < {direction}_latitude < 40.8')\n        .groupby([f'{direction}_long_bucket', f'{direction}_lat_bucket'])\n        .fare_per_km\n        .mean()\n        .unstack(level=0)\n        .iloc[::-1]\n    )\n    sns.heatmap(df, cmap=\"viridis\", vmin= 0, vmax=80, ax=ax, cbar_kws={'label':f'Fare per km ($)'})\n    plt.title(f'Average Fare Per km by {direction} Location')\n    plt.ylabel('Latitude Bucket')\n    plt.xlabel('Longitude Bucket')\n    \nfig = plt.figure(figsize=(10, 9))\nfig.subplots_adjust(wspace=0.2, right=1.8)\nax = fig.add_subplot(1, 2, 1)\nratio_heatmap('pickup')\nax = fig.add_subplot(1, 2, 2)\nratio_heatmap('dropoff')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This heatmap highlights the area in question, an area of mostly yellow where the fare per km travelled is consistently very high. Could this be due to tolls in the area?\n\nWe'll create a feature for use in our model that measures proximity to this area.\n\nFinally, we examine whether the direction of travel relates to the fare paid."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train.query('distance > 5').pivot_table('fare_per_km', index='direction_bucket', aggfunc='mean').plot(figsize=(15,2))\nplt.ylabel('Fare per km($)')\nplt.xlabel('Direction')\nplt.title('Fare Paid vs Direction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you travel up or down roads along the length of central NYC, you will be travelling on a bearing of ~20 degrees or ~-160 degrees depending on your direction. Fare per km being lowest on these bearings makes sense because we are using a point to point distance, meaning the real distance travelled will be closest to our measure when the journey is directly along a road - on other bearings our distance metric is more heavily underestimating the real distance that had to be travelled.\n\nWe will create an adjusted distance feature based on the bearing of the trip for use in our model."},{"metadata":{},"cell_type":"markdown","source":"# Final Modelling\n\nWe will now add all the features that might prove useful and train the model which we will use for our submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"def modelling_features(df):\n    df = shared_features(df)\n    # using alternative representation of cyclic features\n    df = df.assign(\n        sin_time=np.sin(2*np.pi*df['time']/24),\n        cos_time=np.cos(2*np.pi*df['time']/24),\n        sin_direction=np.sin(2*np.pi*df['direction']/360),\n        cos_direction=np.cos(2*np.pi*df['direction']/360),\n        sin_dayofyear=np.sin(2*np.pi*df['dayofyear']/365),\n        cos_dayofyear=np.cos(2*np.pi*df['dayofyear']/365),\n        sin_weekday=np.sin(2*np.pi*df['weekday']/6),\n        cos_weekday=np.cos(2*np.pi*df['weekday']/6),\n        direction_bucket=pd.cut(df['direction'], bins=37, labels=False)\n        ).drop(columns=['pickup_datetime', 'time', 'direction', 'weekday', 'dayofyear'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = load_df(10000000, features='model')\n\ntest['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 15)\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\ntest = modelling_features(test)\n\ntrain = (train\n    .query(f'{test.pickup_longitude.min()-0.1} <= pickup_longitude <= {test.pickup_longitude.max()+0.1}')\n    .query(f'{test.pickup_latitude.min()-0.1} <= pickup_latitude <= {test.pickup_latitude.max()+0.1}')\n    .query(f'{test.dropoff_longitude.min()-0.1} <= dropoff_longitude <= {test.dropoff_longitude.max()+0.1}')\n    .query(f'{test.dropoff_latitude.min()-0.1} <= dropoff_latitude <= {test.dropoff_latitude.max()+0.1}')\n)\n\nx_train, x_val, y_train, y_val = get_split_sets(train)\n\nx_train['fare_per_km'] = y_train*1000/(x_train.distance+5)\nfares_by_direction = x_train.query('5 < distance').groupby('direction_bucket')['fare_per_km'].mean()\n\nx_train['adj_dist'] = [fares_by_direction[i] for i in x_train.direction_bucket]*x_train.distance/fares_by_direction.max()\nx_val['adj_dist'] = [fares_by_direction[i] for i in x_val.direction_bucket]*x_val.distance/fares_by_direction.max()\ntest['adj_dist'] = [fares_by_direction[i] for i in test.direction_bucket]*test.distance/fares_by_direction.max()\n\nx_train = x_train.drop(columns=['fare_per_km', 'direction_bucket'])\nx_val = x_val.drop(columns=['direction_bucket'])\nx_test = test.drop(columns=['key', 'direction_bucket'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_final_model, lin_final_rmse, lin_final_pred = lin_model(x_train, x_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_cols = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']\nk_choices = [18,24,30,40]\nknn_final_model, knn_final_rmse, knn_final_pred = knn_model(x_train[knn_cols], x_val[knn_cols], y_train, y_val, k_choices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {\n    'objective': 'regression',\n    'boosting': 'gbdt',\n    'reg_sqrt': True,\n    'learning_rate': 0.03,\n    'num_leaves': 1200,\n    'max_depth': -1,\n    'max_bin': 5000,\n    'num_rounds': 5000,\n    'early_stopping_round': 50,\n    'metric': 'rmse'\n}\nlgbm_final_model, lgbm_final_rmse, lgbm_final_pred = lgbm_model(lgbm_params, x_train, x_val, y_train, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm.plot_importance(lgbm_final_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Linear Regression RMSE', lin_final_rmse)\nprint('KNN RMSE', knn_final_rmse)\nprint('LightGBM RMSE', lgbm_final_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features we added have resulted in much better performance by the linear regression model, though the features we extracted were not molded for use with linear regression. Techniques such as target encoding of the time features would probably prove more fruitful if this model was our primary focus.\n\nUsing KNN in a high dimensional space in this notebook will hurt its performance and slow the model fitting to a crawl, so I have rerun it on this larger subset using just the original features.\n\nThe LightGBM model was the primary focus of our feature extraction efforts, and it now achieves a RMSE on the validation set of 3.393. Time of year, time of day, trip distance and proximity to JFK airport are the features this model makes the most use of.\n\nWe will see if ensembling the LightGBM and KNN models still yields any gains now that there is a gap in their individual performances."},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {}\nfor a in np.linspace(0,1,101):\n    final_preds_ave = (lgbm_final_pred*(1-a) + knn_final_pred * a)\n    rmse = np.sqrt(mean_squared_error(y_val, final_preds_ave))\n    d[a] = rmse\nalpha = min(d, key=d.get)\nprint('Best weight to give KNN: ', alpha)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A weighted average ensemble with a weight of 0.06 for the KNN model results in the best performance on the validation set. We will use this weighting for our final submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_test_pred = lgbm_final_model.predict(x_test, num_iteration=lgbm_final_model.best_iteration)\nknn_test_pred = knn_final_model.predict(x_test[['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']])\nsubmission_pred = (lgbm_test_pred*(1-alpha) + knn_test_pred * alpha)\nsubmission = pd.DataFrame({'key': test.key, 'fare_amount': submission_pred})\nsubmission.to_csv('submission_10_10_20_comb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This submission achieves a RMSE of 2.93 on the leaderboard set."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}