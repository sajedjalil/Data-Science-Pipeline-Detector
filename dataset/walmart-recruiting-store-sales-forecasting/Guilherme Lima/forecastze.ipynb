{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div style=\"font-size:15px; font-family:Roboto;background-color:#333333; color: white;\">\n    <img width=\"121\" height=\"40\" src=\"https://courier-images-web.imgix.net/static/img/white-logo.png?auto=compress,format&amp;fit=max&amp;w=undefined&amp;h=undefined&amp;dpr=2&amp;fm=png\" alt=\"ZÃ© delivery logo\" class=\"css-0\">  \n    <h1 style=\"font-size:15px; font-family:Roboto;background-color:#333333; color: white;\"><center><b>Walmart Store Sales Forecasting</b> | <a href=\"https://www.linkedin.com/in/guimarotto/\" target=\"_blank\" style=\"font-family:Roboto; color: #ffcd02;\">Guilherme Lima</a></center></h1>\n"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"font-size:20px; font-family:Roboto; background-color:#F5F5F5; color: black;\">\n    <h1>Summary</h1>\n\n * <a href=\"#ch1\" target=\"_blank\" style=\"font-family:Roboto; color: #ffcd02;\"><b>Abstract</b></a>\n * <a href=\"#ch2\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Exploratory Data Analysis</b></a>\n     * <a href=\"#ch2_1\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Data Ingestion and Preparation</b></a>\n     * <a href=\"#ch2_1\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Exploratory Analysis using Profile</b></a>\n * <a href=\"#ch3\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Feature Engineering</b></a>\n     * <a href=\"#ch3_1\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Feature Preparation</b></a>\n     * <a href=\"#ch3_3\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Feature Selection</b></a>  \n * <a href=\"#ch4\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Model Development</b></a>\n     * <a href=\"#ch4_1\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Parameter Optimization</b></a>\n     * <a href=\"#ch4_2\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Model Training and Tuning</b></a>\n     * <a href=\"#ch4_3\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Model Validation</b></a>\n * <a href=\"#ch5\" target=\"_blank\" style=\"font-family:verdana; color: #ffcd02;\"><b>Results</b></a>\n \n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch1\"></a>\n<h1 id=\"ch1\" style=\"font-family:Roboto;background-color:#333333; color: white;\"> \n    <center><b><br>Abstract</b></center>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">This notebook presents a experiment that forecast store sales based on Walmart historical data. The final Features selected were:</p>\n    <ol style=\"font-size:15px; font-family:verdana; line-height: 1.7em\"><li>\n    <ol>Input Features\n        <li>Store</li>\n        <li>Dept</li>\n        <li>IsHoliday</li>\n        <li>Type</li>\n        <li>Size</li>\n        <li>Week</li>\n        <li>Year</li>\n    </ol>\n</li>\n<li>Target\n    <ol>\n        <li>Weekly_Sales</li>\n    </ol>\n</li>\n</ol>\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n        The final model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=2, min_samples_leaf=1)   <br> The final RMSE error was 1171.018166.\n    </p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch1_1\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\n        Configuration and Import\n    </h1>\n    <br>\n<br>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom tpot import TPOTRegressor\nimport xgboost as xgb\n\nenable_pandas_profilling = False\nenable_tpot = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch2\"></a>\n<h1 id=\"basics\" style=\"font-family:Roboto;background-color:#333333; color: white;\"> \n    <center><b><br>Exploratory Data Analysis</b></center>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch2_1\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\n        Data Ingestion and Preparation\n    </h1>\n    <ol>\n        <li>Data Ingestion\n            <ol>\n                <li>Features Dataset</li>\n                <li>Stores Dataset</li>\n                <li>Training Dataset</li>\n                <li>Testing Dataset</li>\n                <li>Submission Dataset</li>\n            </ol>\n        </li>\n        <li>Data Preparation\n            <ol>\n                <li>Join Features and Stores</li>\n                <li>Verify datetypes</li>\n                <li>Date represents Days or Weeks?</li>\n                <li>Enrich Train and Test Datasets with Features/Stores Information</li>\n            </ol>\n        </li>\n</ol>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ndf_train = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ndf_stores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\ndf_test = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\ndf_sample_submission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat_stores = df_features.merge(df_stores, how='inner', on='Store')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'Type_Feat_Store': df_feat_stores.dtypes,'Type_Train': df_train.dtypes, 'Type_Test': df_test.dtypes})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n        After the ingestion, the field 'Date' on each dataset is a 'object' (string) type. We need to convert them to datetime for them to be better used in the future.\n    </p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat_stores.Date = pd.to_datetime(df_feat_stores.Date)\ndf_train.Date = pd.to_datetime(df_train.Date)\ndf_test.Date = pd.to_datetime(df_test.Date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_feat_stores.Date.unique())/52","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n        Since we verified that the 'Date' field doesn't represent the day itself, but each week, ending every friday, from 2010-02-05 to 2012-11-01, we will create to new features: (1) 'Week' and (2) 'Year'. \n    </p>\n</div>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feat_stores['Week'] = df_feat_stores.Date.dt.week \ndf_feat_stores['Year'] = df_feat_stores.Date.dt.year","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n      By now, the training and test datasets will be the inner join with repectively, train and test data enriched with features and store dataframe.\n    </p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats = df_train.merge(df_feat_stores, how='inner', on=['Store','Date','IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)\ndf_test_feats = df_test.merge(df_feat_stores, how='inner', on=['Store','Date','IsHoliday']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_feat_stores, df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch2_2\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\n        Data Preparation\n    </h1>\n    <ol>\n        <li>Missing Values\n            <ol>\n                <li>Drop cols with more than 60% of missing values</li>\n            </ol>\n        </li>\n        <li>Person Correlation - Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite.\n            <ol>\n                <li>0: no correlation</li>\n                <li>from 0 to +/-0.3: weak correlation</li>\n                <li>from +/-0.3 to +/-0.7: moderate correlaton</li>\n                <li>from +/-0.7 to +/-1: strong correlation</li>\n            </ol>\n        </li>\n</ol>\n</div>\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n      Ideally we want to use features with moderate and strong positive correlations with 'Weekly_sales' and with less than 60% missing values.\n    </p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate the report. We would use the mpg dataset as sample, title parameter for naming our report, and explorative parameter set to True for Deeper exploration.\nif enable_pandas_profilling:\n    profile_train = ProfileReport(df_train_feats, title='Training Dataset Report', explorative = True)\n    profile_train.to_file(output_file='profile_train.html')\n    profile_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* MarkDown1 has 270889 (64.3%) missing values\n* MarkDown2 has 310322 (73.6%) missing values\n* MarkDown3 has 284479 (67.5%) missing values\n* MarkDown4 has 286603 (68.0%) missing values\n* MarkDown5 has 270138 (64.1%) missing values\n\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n        Since MarkDown' 1 to 5 have more than 60% of missing values, aren't strong correlated to 'Weekly_Sales' and they are anonymized, i.e. it's difficult to know what they mean, we can drop them.\n    </p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats = df_train_feats.drop(columns=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\ndf_test_feats = df_test_feats.drop(columns=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\ncorr = df_train_feats.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nplt.title('Correlation Matrix', fontsize=18)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['Weekly_Sales'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n'Fuel_Price', 'Temperature', 'Date', 'CPI', 'Unemployment' have week negative correlation with 'Weekly_sales', so they will be dropped. \n<br>Also, 'Fuel_Price' is strong correlated to 'Year'. We will keep 'Year', and drop 'Fuel_price', because it differentiate same Weeks Numbers for Sales.</p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats = df_train_feats.drop(columns=['Fuel_Price', 'Temperature','Date', 'CPI', 'Unemployment'])\ndf_test_feats = df_test_feats.drop(columns=['Fuel_Price', 'Temperature','Date', 'CPI', 'Unemployment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"white\")\ncorr = df_train_feats.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nplt.title('Correlation Matrix', fontsize=18)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr['Weekly_Sales'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch3\"></a>\n<h1 id=\"basics\" style=\"font-family:Roboto;background-color:#333333; color: white;\"> \n    <center><b><br>Feature Engineering</b></center>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch3_1\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\n        Feature Preprocessing\n    </h1>\n    <ol>\n        <li>Holidays Analysis\n            <ol>\n                <li>When did the holydays happened?</li>\n                <li>IsHolyday covers all US Holydays?</li>\n            </ol>\n        </li>\n        <li>Features types\n            <ol>\n                <li>Can we have only numerical features?</li>\n                <li>What Type stands for?</li>\n            </ol>\n        </li>\n</ol>\n</div>\n<div style=\"color:white;\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n      Ideally we want to use numerical features.\n    </p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales_2010_weekly = df_train_feats[df_train_feats.Year==2010]['Weekly_Sales'].groupby(df_train_feats['Week']).sum()\ndf_sales_2011_weekly = df_train_feats[df_train_feats.Year==2011]['Weekly_Sales'].groupby(df_train_feats['Week']).sum()\ndf_sales_2012_weekly = df_train_feats[df_train_feats.Year==2012]['Weekly_Sales'].groupby(df_train_feats['Week']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.lineplot(df_sales_2010_weekly.index, df_sales_2010_weekly.values)\nsns.lineplot(df_sales_2011_weekly.index, df_sales_2011_weekly.values)\nsns.lineplot(df_sales_2012_weekly.index, df_sales_2012_weekly.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.title('Overall Weekly Sales - Per Year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats[df_train_feats.IsHoliday].Week.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks.\n\nIn 2010 is in Week 13\nIn 2011, Week 16\nWeek 14 in 2012\nand, finally, Week 13 in 2013 for Test set\nSo, we can change to 'True' these Weeks in each Year."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats.loc[(df_train_feats.Year==2010) & (df_train_feats.Week==13), 'IsHoliday'] = True\ndf_train_feats.loc[(df_train_feats.Year==2011) & (df_train_feats.Week==16), 'IsHoliday'] = True\ndf_train_feats.loc[(df_train_feats.Year==2012) & (df_train_feats.Week==14), 'IsHoliday'] = True\ndf_test_feats.loc[(df_test_feats.Year==2013) & (df_test_feats.Week==13), 'IsHoliday'] = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekly_sales_mean = df_train_feats['Weekly_Sales'].groupby(df_train_feats['Week']).mean()\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['Mean'], loc='best', fontsize=16)\nplt.title('Mean Weekly Sales', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.axvline(6, color='red')\nplt.axvline(13, color='red')\nplt.axvline(14, color='red')\nplt.axvline(36, color='red')\nplt.axvline(47, color='red')\nplt.axvline(52, color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats.Type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,8))\ngs = GridSpec(1,2)\nsns.boxplot(y=df_train_feats.Weekly_Sales, x=df_train_feats.Type, ax=fig.add_subplot(gs[0,0]))\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Type', fontsize=16)\nsns.stripplot(y=df_train_feats.Weekly_Sales, x=df_train_feats.Type, ax=fig.add_subplot(gs[0,1]))\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Type', fontsize=16)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nominal_to_ordinal(x):\n    if x == 'A':\n        return 3\n    elif x == 'B':\n        return 2\n    else:\n        return 1\n    \ndf_train_feats.Type = df_train_feats.Type.apply(nominal_to_ordinal)\ndf_test_feats.Type = df_test_feats.Type.apply(nominal_to_ordinal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats.Type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_feats.Type.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,8))\ngs = GridSpec(1,2)\nsns.boxplot(y=df_train_feats.Weekly_Sales, x=df_train_feats.Type, ax=fig.add_subplot(gs[0,0]))\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('TypeOrd', fontsize=16)\nsns.stripplot(y=df_train_feats.Weekly_Sales, x=df_train_feats.Type, ax=fig.add_subplot(gs[0,1]))\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('TypeOrd', fontsize=16)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bool_to_num(x):\n    if x:\n        return 1\n    else:\n        return 0\n    \ndf_train_feats.IsHoliday = df_train_feats.IsHoliday.apply(bool_to_num)\ndf_test_feats.IsHoliday = df_test_feats.IsHoliday.apply(bool_to_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch3_3\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\n        Feature Selection\n    </h1>\n</div>\n<div style=\"\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n            <ol><li>Features\n            <ol>\n                <li>Store</li>\n                <li>Dept</li>\n                <li>IsHoliday</li>\n                <li>Type</li>\n                <li>Size</li>\n                <li>Week</li>\n                <li>Year</li>\n            </ol>\n        </li>\n        <li>Target\n            <ol>\n                <li>Weekly_Sales</li>\n            </ol>\n        </li>\n</ol><br></p>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feats.IsHoliday.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch4\"></a>\n<h1 id=\"basics\" style=\"font-family:Roboto;background-color:#333333; color: white;\"> \n    <center><b><br>Model Development</b></center>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch4_1\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">Parameter Optimization</h1>\n</div>\n<div style=\"\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n            In order to get a better Parameter Optimization and also select the model algorithm that better suits our features I ran TPOT Regressor.</p>\n</div>\n\n<img  src=\"https://github.com/EpistasisLab/tpot/raw/master/images/tpot-pipeline-example.png\" alt=\"Tpot mechanism\" class=\"css-0\">\nfrom  <a href=\"https://github.com/EpistasisLab/tpot\" target=\"_blank\" style=\"font-family:Roboto; color: #ffcd02;\">https://github.com/EpistasisLab/tpot</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train_feats.drop('Weekly_Sales', axis= 1).values\ny_train = df_train_feats.Weekly_Sales.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train), len(y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPOT takes 10hours to get a good parameters estimation so it is set False here.\nif enable_tpot:\n    X_tpot_train, X_tpot_test, y_tpot_train, y_tpot_test = train_test_split(X_train,  y_train, train_size=0.75, test_size=0.25, random_state=42)\n    tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\n    tpot.fit(X_tpot_train, y_tpot_train)\n    print(tpot.score(X_tpot_test, y_tpot_test))\n    tpot.export('tpot_pipeline.py')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n            TPOT Regressor estimation delivered the following params: n_estimators=100, max_depth=27, max_features=6, min_samples_split=8, min_samples_leaf=1.</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch4_2\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\nModel Training and Tuning\n    </h1>\n    <br>\n    <p>\n        From Tpot estimation I will try to tune the model in order to reduce the RMSE.<br>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=100, max_depth=27, max_features=6, min_samples_split=8, min_samples_leaf=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_train)\nrmse_all = np.sqrt(mean_squared_error(y_train, preds))\nprint(\"RMSE: %f\" % (rmse_all))\ndel model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=8, min_samples_leaf=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_train)\nrmse_all = np.sqrt(mean_squared_error(y_train, preds))\nprint(\"RMSE: %f\" % (rmse_all))\ndel model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=4, min_samples_leaf=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_train)\nrmse_all = np.sqrt(mean_squared_error(y_train, preds))\nprint(\"RMSE: %f\" % (rmse_all))\ndel model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_train)\nrmse_all = np.sqrt(mean_squared_error(y_train, preds))\nprint(\"RMSE: %f\" % (rmse_all))\ndel model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=2, min_samples_leaf=1)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_train)\nrmse_all = np.sqrt(mean_squared_error(y_train, preds))\nprint(\"RMSE: %f\" % (rmse_all))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div style=\"\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n            The final model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=2, min_samples_leaf=1)</p>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch4_3\"></a>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <h1 id=\"italic\">\nModel Validation\n    </h1>\n    <br>\n    <p>\n        Let's use the Test Dataset and build our submission sample.<br>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_test_feats), len(df_sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_feats.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = model.predict(df_test_feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_submission['Weekly_Sales'] = predicted\ndf_sample_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"ch5\"></a>\n<h1 id=\"ch5\" style=\"font-family:Roboto;background-color:#333333; color: white;\"> \n<center><b><br>Results</b></center>\n</h1>"},{"metadata":{},"cell_type":"markdown","source":"<div style=\"\n       display:fill;\n       border-radius:5px;\n       background-color:#ffcd02;\n       font-size:110%;\n       font-family:Verdana;\n       letter-spacing:0.5px\">\n    <p style=\"padding: 10px;\n          color:#333337;\">\n            The final model = RandomForestRegressor(n_estimators=50, max_depth=27, max_features=6, min_samples_split=2, min_samples_leaf=1)   <br> The final RMSE error was 1171.018166 </p>\n</div>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}