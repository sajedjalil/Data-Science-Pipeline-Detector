{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# Kaggle environments.\n!git clone https://github.com/Kaggle/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.5 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.5.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will define this magic to run and write the cell at the same time\n# This will facilitate the generation of submission file\nfrom IPython.core.magic import register_cell_magic\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nfrom kaggle_environments import make\nfrom typing import Tuple, Any\nfrom tensorflow.keras import backend as K\nimport os\nimport threading\nimport uuid\nfrom queue import Queue\nimport dill\nfrom scipy.signal import lfilter\nfrom threading import Lock\nimport json\nfrom typing import List\nimport multiprocessing as mp\nimport time\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! mkdir -p /kaggle_simulations/agent/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nimport zlib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## PPO Agent\n\nPPO is known for its ease of use and good results, so I hope it will know how to play football.\n\nWe're going to build 3 modules:\nA more convenient environment wrapper to support parallel episodes collection.\nA transformer with and multi-head self-attention Layers that will help to embed players' units and at last, the PPO agent.\n\n## Agent structure\n\n![diagrams](https://raw.githubusercontent.com/tchaye59/GRFootball/main/diagrams.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## FootEnv : custom environment Wrapper\nWe use this wrapper to preprocess the GFootball environment players_raw data.\n\n* **units** : Information on both left and right side players is parsed as units. \n* **scalars** : Contain other information."},{"metadata":{"trusted":true},"cell_type":"code","source":"right_agent_path = '/kaggle/input/gfootball-template-bot/submission.py'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# FootEnv: \nclass FootEnv(gym.Env):\n    metadata = {'render.modes': ['human']}\n\n    def __init__(self, right_agent=right_agent_path, env_id=0):\n        super(FootEnv, self).__init__()\n        self.env_id = env_id\n        self.agents = [None, right_agent]# We will step on the None agent\n        self.env = make(\"football\", configuration={\"save_video\": False,\n                                                   \"scenario_name\": \"11_vs_11_kaggle\",\n                                                   \"running_in_notebook\": True})\n        self.trainer = None\n\n\n    def step(self, action):\n        obs, reward, done, info = self.trainer.step([action])\n        obs = obs['players_raw'][0]\n        state,(l_score,r_score,custom_reward) = OBSParser.parse(obs)\n        info['l_score'] = l_score\n        info['r_score'] = r_score\n        return state, custom_reward, done, info\n\n    def reset(self):\n        self.trainer = self.env.train(self.agents)\n        obs = self.trainer.reset()\n        obs = obs['players_raw'][0]\n        state,_ = OBSParser.parse(obs)\n        return state\n\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n\n    def close(self):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\n# OBSParser : used to parse observation\nclass OBSParser(object):\n\n    @staticmethod\n    def parse(obs):\n        # parse left players units\n        l_units = [[x[0] for x in obs['left_team']], [x[1] for x in obs['left_team']],\n                   [x[0] for x in obs['left_team_direction']], [x[1] for x in obs['left_team_direction']],\n                   obs['left_team_tired_factor'], obs['left_team_yellow_card'],\n                   obs['left_team_active'], obs['left_team_roles']\n                  ]\n\n        l_units = np.r_[l_units].T\n\n        # parse right players units\n        r_units = [[x[0] for x in obs['right_team']], [x[1] for x in obs['right_team']],\n                   [x[0] for x in obs['right_team_direction']], [x[1] for x in obs['right_team_direction']],\n                   obs['right_team_tired_factor'],\n                   obs['right_team_yellow_card'],\n                   obs['right_team_active'], obs['right_team_roles']\n                  ]\n\n        r_units = np.r_[r_units].T\n        # combine left and right players units\n        units = np.r_[l_units, r_units].astype(np.float32)\n\n        # get other information\n        game_mode = [0 for _ in range(7)]\n        game_mode[obs['game_mode']] = 1\n        scalars = [*obs['ball'],\n                   *obs['ball_direction'],\n                   *obs['ball_rotation'],\n                   obs['ball_owned_team'],\n                   obs['ball_owned_player'],\n                   *obs['score'],\n                   obs['steps_left'],\n                   *game_mode,\n                   *obs['sticky_actions']]\n\n        scalars = np.r_[scalars].astype(np.float32)\n        # get the actual scores and compute a reward\n        l_score,r_score = obs['score']\n        reward = l_score - r_score\n        reward_info = l_score,r_score,reward\n        return (units[np.newaxis, :], scalars[np.newaxis, :]),reward_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just creat and return an environment. Useful when we run multiples threads to collect experiences.\ndef env_fn(env_id=1,right_agent=right_agent_path):\n    return FootEnv(env_id=env_id,right_agent=right_agent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"!! Let test **FootEnv** and **OBSParser**"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = env_fn() \nstate = env.reset()\ndone = False\ni = 0\nwhile not done and i <5:\n    i+=1\n    state, reward, done, info = env.step(5)\n    print('reward ', reward, info)\nprint(f\"Units shape {state[0].shape}, Scalars shape {state[1].shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TransformerBlock & MultiHeadSelfAttention:\n\nI'm trying to replicate the Entity encoder part of the paper [Grandmaster level in StarCraft II using multi-agent reinforcement learning](https://www.nature.com/articles/s41586-019-1724-z.epdf).\nThey use a Transformer architecture to embed the StarCraft game entities. We will do the same for players' units.\n\n\n\n[The core idea behind the Transformer model is self-attention‚Äîthe ability to attend to different positions of the input sequence to compute a representation of that sequence.  \nIt make no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example,StarCraft units).](https://www.tensorflow.org/tutorials/text/transformer)"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\n\"\"\"\n## Implement multi head self attention as a Keras layer\n\"\"\"\nclass MultiHeadSelfAttention(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, **kwargs):\n        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = keras.layers.Dense(embed_dim)\n        self.key_dense = keras.layers.Dense(embed_dim)\n        self.value_dense = keras.layers.Dense(embed_dim)\n        self.combine_heads = keras.layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def scaled_dot_product_attention(self, q, k, v, mask):\n        # (..., seq_len_q, seq_len_k)\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n        # scale matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\n        # add up to 1.\n        attention_weights = tf.nn.softmax(\n            scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n        return output, attention_weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(\n            x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs, training=None, mask=None):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.scaled_dot_product_attention(\n            query, key, value, mask)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n\n    def get_config(self):\n        config = super(MultiHeadSelfAttention, self).get_config()\n        config.update({\n            \"num_heads\": self.num_heads,\n            \"embed_dim\": self.embed_dim,\n            \"projection_dim\": self.projection_dim,\n            \"query_dense\": self.query_dense,\n            \"key_dense\": self.key_dense,\n            \"value_dense\": self.value_dense,\n            \"combine_heads\": self.combine_heads,\n\n        })\n        return config\n\n\n\"\"\"\n## Implement a Transformer block as a layer\n\"\"\"\nclass TransformerBlock(keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n        super(TransformerBlock, self).__init__(**kwargs)\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [keras.layers.Dense(ff_dim, activation=\"relu\"),\n             keras.layers.Dense(embed_dim), ]\n        )\n        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = keras.layers.Dropout(rate)\n        self.dropout2 = keras.layers.Dropout(rate)\n\n    def call(self, inputs, training=None, mask=None):\n        attn_output = self.att(inputs, training, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n    def get_config(self):\n        config = super(TransformerBlock, self).get_config()\n        config.update({\n            \"att\": self.att,\n            \"ffn\": self.ffn,\n            \"layernorm1\": self.layernorm1,\n            \"layernorm2\": self.layernorm2,\n            \"dropout1\": self.dropout1,\n            \"dropout2\": self.dropout2,\n\n        })\n        return config","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## UnitsEncoder\nNow we prepare the units UnitsEncoder layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\nclass UnitsEncoder(keras.layers.Layer):\n    def __init__(self, embed_dim=2 ** 7, num_heads=8, ff_dim=128, rate=0.0, name=None, **args):\n        super(UnitsEncoder, self).__init__(name=name, *args)\n        self.supports_masking = True\n        self.embed_dim = embed_dim\n        self.dense = keras.layers.Dense(embed_dim)\n        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, rate)\n        self.layernorm = keras.layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs, training=None, mask=None):\n        if mask is not None and mask.dtype != inputs.dtype:\n            mask = tf.cast(mask, inputs.dtype)\n        inputs = self.layernorm(inputs)\n        inputs = self.dense(inputs)\n        return self.transformer_block(inputs, training, mask=mask)\n\n    def get_output_shape_for(self, input_shape):\n        return input_shape[0], input_shape[1], self.embed_dim\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[1], self.embed_dim\n\n    def get_config(self):\n        config = super(UnitsEncoder, self).get_config()\n        config.update({\n            \"dense\": self.dense,\n            \"transformer_block\": self.transformer_block,\n            \"layernorm\": self.layernorm,\n            'embed_dim': self.embed_dim\n        })\n        return config","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define used variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\n\nLOSS_CLIPPING = 2  # Only implemented clipping for the surrogate loss, paper said it was best\nENTROPY_LOSS = 5e-3\nGAMMA = 0.99\nN_ACTIONS = 19\nLR = 0.0001\nBATCH_SIZE = 1024\nEPOCHS = 10\nGAMMA = 0.99\nLAMBDA = 0.95","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"restore_path = '/kaggle/input/data' # if we want to restore the previous checkpoint\ndata_path = ''\nlock = Lock()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Actor&Critic"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\n\n# This code is shared by both actor and critic\ndef build_shared(units, scalars) -> keras.layers.Dense:\n    scalars = keras.layers.LayerNormalization()(scalars)\n\n    # units_encoder\n    units_encoder = UnitsEncoder(embed_dim=2 ** 7, num_heads=4, ff_dim=128, name='entities_encoder')(units)\n\n    # scalars encoder\n    scalars_encoder = keras.layers.LayerNormalization()(scalars)\n    scalars_encoder = keras.layers.Dense(64, activation='relu', name='scalars_encoder1')(scalars_encoder)\n    scalars_encoder = keras.layers.Dense(64, activation='relu', name='scalars_encoder2')(scalars_encoder)\n    # combine scalars and units\n    scalars_encoder = keras.layers.RepeatVector(units_encoder.shape[1])(scalars_encoder)\n    encoder = keras.layers.concatenate([units_encoder, scalars_encoder], axis=-1)\n    encoder = keras.layers.Dense(128, activation='relu', )(encoder)\n    encoder = keras.layers.MaxPooling1D(2)(encoder)\n    encoder = keras.layers.Dense(128, activation='relu', )(encoder)\n    encoder = keras.layers.MaxPooling1D(2)(encoder)\n    encoder = keras.layers.Dense(128, activation='relu', )(encoder)\n    encoder = keras.layers.MaxPooling1D(2)(encoder)\n    encoder = keras.layers.Flatten()(encoder)\n    encoder = keras.layers.Dense(256, activation='relu', )(encoder)\n    return encoder\n\n#build actor\ndef build_actor(verbose=True, lr=1e-4):\n    n_actions = 19\n    # create the model architecture\n\n    # inputs\n    units_input = keras.layers.Input(shape=(22, 8), name='units_input')\n    scalars_input = keras.layers.Input(shape=(31,), name='scalars_input')\n\n    # advantage and old_prediction inputs\n    advantage = keras.layers.Input(shape=(1,), name='advantage')\n    old_action = keras.layers.Input(shape=(n_actions,), name='old_action')\n    action_lbl = keras.layers.Input(shape=(n_actions,), name='action_lbl')\n\n    # build_shared\n    encoder = build_shared(units_input, scalars_input)\n\n    # outputs\n    action = keras.layers.Dense(n_actions, activation=keras.activations.softmax)(encoder)\n    inputs = [units_input, scalars_input, advantage, old_action, action_lbl]\n\n    model = keras.models.Model(inputs, action)\n    model.add_loss(ppo_loss(action_lbl, action, advantage, old_action))\n\n    model.compile(optimizer=keras.optimizers.Adam(lr))\n    if verbose: model.summary()\n    return model\n\n#build critic\ndef build_critic(verbose=True, lr=1e-4):\n    # inputs\n    units_input = keras.layers.Input(shape=(22, 8), name='units_input')\n    scalars_input = keras.layers.Input(shape=(31,), name='scalars_input')\n\n    # build_shared\n    encoder = build_shared(units_input, scalars_input)\n\n    # outputs\n    value_dense = keras.layers.Dense(1, name='value')(encoder)\n    inputs = [units_input, scalars_input]\n\n    model = keras.models.Model(inputs, value_dense)\n\n    model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr))\n    if verbose: model.summary()\n    return model\n\n\ndef ppo_loss(label_layer, prediction_layer, advantage, old_prediction, clip=True):\n    prob = label_layer * prediction_layer\n    old_prob = label_layer * old_prediction\n    r = prob / (old_prob + 1e-10)\n    clipped = r\n    if clip:\n        clipped = K.clip(r, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING)\n    return -K.mean(K.minimum(r * advantage,clipped* advantage) + \n                   ENTROPY_LOSS * (prob * K.log(prob + 1e-10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Actor:\")\nbuild_actor()\nprint(\"Critic:\")\nbuild_critic()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Memory\nTo store states, rewards ... for each episode"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Memory:\n    def __init__(self):\n        # inputs\n        self.units = []\n        self.scalars = []\n        # action\n        self.actions_matrix = []\n        self.actions_probs = []\n        # rewards\n        self.rewards = []\n        # dones\n        self.terminal = []\n\n    def isEmpty(self):\n        return len(self.rewards) == 0\n\n    def store(self, obs, actions, reward, done):\n        # inputs\n        units, saclars = obs\n        self.units.append(units)\n        self.scalars.append(saclars)\n\n        # actions\n        _, actions_matrix, actions_probs = actions\n        if actions_matrix is not None: self.actions_matrix.append(actions_matrix)\n        if actions_probs is not None: self.actions_probs.append(actions_probs)\n        # reward\n        self.rewards.append(reward)\n        self.terminal.append(done)\n\n    def discount(self, x, gamma=GAMMA):\n        return lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\n    def discount_rewards(self, GAMMA=0.99):\n        return self.discount(self.rewards, GAMMA)\n\n    def normalize(self, x):\n        mean = np.mean(x)\n        std = np.std(x)\n        return (x - mean) / np.maximum(std, 1e-6)\n\n    def compute_advantages(self, pred_value, GAMMA=0.99, LAMBDA=0.95, normalize=True):\n        # Computes GAE (generalized advantage estimations (from the Schulman paper))\n        rewards = np.array(self.rewards, dtype=np.float32)\n        pred_value_t = pred_value\n        pred_value_t1 = np.concatenate([pred_value[1:], [0.]])\n        pred_value_t1[self.terminal] = 0\n        advantage = rewards + GAMMA * pred_value_t1 - pred_value_t\n        advantage = self.normalize(self.discount(advantage, GAMMA * LAMBDA))\n        return np.array(self.discount_rewards(), dtype=np.float32), \\\n               advantage.astype(np.float32)\n\n    def compute_normal_advantages(self, pred_value, GAMMA=0.99):\n        rewards = np.array(self.discount_rewards(GAMMA), dtype=np.float32)\n        advantage = rewards - pred_value\n        return rewards.astype(np.float32), advantage.astype(np.float32)\n\n    def get_all_as_tensors(self):\n        rewards = np.array(self.discount_rewards(), dtype=np.float32)\n        units = tf.concat(self.units, axis=0)\n        scalars = tf.concat(self.scalars, axis=0)\n\n        actions_matrix = tf.convert_to_tensor(self.actions_matrix, dtype=tf.float32)\n        actions_probs = tf.convert_to_tensor(self.actions_probs, dtype=tf.float32)\n        dones = np.array(self.terminal, dtype=np.float32)\n        return (units, scalars), actions_matrix, actions_probs, rewards, dones","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PPOPolicy"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%write_and_run -a /kaggle_simulations/agent/main.py\n\n# The policy class is straightforward, easy to understand\nclass PPOPolicy:\n    def __init__(self, val=False):\n        self.actor = build_actor(lr=LR,verbose=0)\n        self.critic = build_critic(lr=LR*10,verbose=0)\n\n        self.val = val # Validation or trainning\n\n    def get_values(self, X):\n        return self.critic.predict(X).flatten()\n\n    def get_action(self, X):\n        action_prob = self.actor.predict(X)\n        action_prob = action_prob[0]\n        # action_probs = np.nan_to_num(action_probs[0])\n        n_actions = action_prob.size\n        if self.val:\n            action = np.argmax(action_prob, axis=-1)\n        else:\n            action = np.random.choice(n_actions, p=action_prob)\n\n        # matrix\n        action_matrix = np.zeros(n_actions, np.float32)\n        action_matrix[action] = 1\n\n        return action, action_matrix, action_prob\n\n    def train(self, memories):\n        if not memories:\n            return [],[]\n        actor_ds, critic_ds = None, None\n        # prepare dataset\n        # process and combine memories in actor_ds and critic_ds(tf.data.Dataset objects) \n        for i, memory in enumerate(memories):\n            print(f\"Add Memory {i + 1}/{len(memories)}\")\n            inputs, actions_matrix, actions_probs, rewards, dones = memory.get_all_as_tensors()\n            c_inputs = inputs\n            pred_values = self.get_values(c_inputs)\n\n            # Generalized Advantage Estimation\n            rewards, advantage = memory.compute_advantages(pred_values)\n            rewards = rewards[:, np.newaxis]\n            advantage = advantage[:, np.newaxis]\n\n            labels = actions_matrix\n            a_inputs = *inputs, advantage, actions_probs, labels\n\n            if actor_ds is None:\n                actor_ds = tf.data.Dataset.from_tensor_slices((a_inputs, labels))\n            else:\n                actor_ds = actor_ds.concatenate(tf.data.Dataset.from_tensor_slices((a_inputs, labels)))\n            if critic_ds is None:\n                critic_ds = tf.data.Dataset.from_tensor_slices((c_inputs, rewards))\n            else:\n                critic_ds = critic_ds.concatenate(tf.data.Dataset.from_tensor_slices((c_inputs, rewards)))\n\n        # train\n        print(\"Updating...\")\n        actor_ds = actor_ds.shuffle(100).batch(BATCH_SIZE).prefetch(2)\n        critic_ds = critic_ds.shuffle(100).batch(BATCH_SIZE).prefetch(2)\n\n        s = time.time()\n        a_losses = self.actor.fit(actor_ds, epochs=EPOCHS, verbose=False)\n        a_time = time.time() - s\n        print(f\">>>Actor updated: {a_time}ms\")\n        s = time.time()\n        c_losses = self.critic.fit(critic_ds, epochs=EPOCHS, verbose=False)\n        c_time = time.time() - s\n        print(f\">>>Critic updated: {c_time}ms\")\n        print(f\"Total Duration: {a_time + c_time}\")\n\n        return a_losses.history['loss'], c_losses.history['loss']\n\n    def save(self, path):\n        self.actor.save_weights(path + '.actor.h5')\n        self.critic.save_weights(path + '.critic.h5')\n\n    def load(self, path):\n        if os.path.exists(path + '.actor.h5') or os.path.exists(path + '.critic.h5'):\n            self.actor.load_weights(path + '.actor.h5')\n            self.critic.load_weights(path + '.critic.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EpisodeCollector & ParallelEpisodeCollector\n\n* EpisodeCollector : run a single environment \n* ParallelEpisodeCollector : run a multiple environments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# EpisodeCollector\nclass EpisodeCollector(threading.Thread):\n    n_episode = 1\n    reward_sum = 0\n    max_episode = 0\n\n    def __init__(self, env: FootEnv, policy: PPOPolicy, result_queue=None, replays_dir=None):\n        super().__init__()\n        self.result_queue = result_queue\n        self.env = env\n        self.policy = policy\n        self.replays_dir = replays_dir\n        self.n_episode = -1\n\n    def clone(self):\n        obj = EpisodeCollector(self.env, self.policy)\n        obj.result_queue = self.result_queue\n        obj.replays_dir = self.replays_dir\n        obj.n_episode = self.n_episode\n        return obj\n\n    def run(self):\n        self.result_queue.put(self.collect(1))\n\n    def collect(self, n=1):\n        n = max(n, self.n_episode)\n        return [self.collect_() for _ in range(n)]\n\n    def collect_(self):\n        memory = Memory()\n        done = False\n        EpisodeCollector.n_episode += 1\n        obs = self.env.reset()\n        i = 0\n        total_reward = 0\n        while not done:\n            actions = self.policy.get_action(obs)\n            #action,action_matrix,action_prob = actions\n            new_obs, reward, done, info = self.env.step(actions[0])\n            # store data\n            memory.store(obs, actions, reward, done)\n\n            if done or i % 1000 == 0:\n                with lock:\n                    print(\n                        f\"Episode: {EpisodeCollector.n_episode}/{EpisodeCollector.max_episode} | \"\n                        f\"Step: {i} | \"\n                        f\"Env ID: {self.env.env_id} | \"\n                        f\"Reward: {reward} | \"\n                        f\"Total Rewards: {EpisodeCollector.reward_sum} | \"\n                        f\"{info}\"\n                    )\n\n            obs = new_obs\n            i += 1\n        EpisodeCollector.reward_sum += info['l_score'] # count the total goal scored by the agent\n        if self.replays_dir:\n            with open(os.path.join(self.replays_dir, f'replay-{uuid.uuid4().hex}.dill'), 'wb') as f:\n                dill.dump(memory, f)\n        return memory\n\n# ParallelEpisodeCollector\nclass ParallelEpisodeCollector:\n\n    def __init__(self, env_fn, n_jobs, policy: PPOPolicy, replays_dir=None, ):\n        self.n_jobs = n_jobs\n        self.policy: Policy\n        self.envs = []\n        self.result_queue = Queue()\n        self.replays_dir = replays_dir\n        for i in range(n_jobs):\n            self.envs.append(env_fn(env_id=i))\n        self.collectors = [EpisodeCollector(env,\n                                            policy=policy,\n                                            result_queue=self.result_queue,\n                                            replays_dir=replays_dir) for env in self.envs]\n\n    def collect(self, n_steps=1):\n        if not n_steps: n_steps = 1\n        result_queue = self.result_queue\n        for i, collector in enumerate(self.collectors):\n            collector = collector.clone()\n            self.collectors[i] = collector\n            collector.n_episode = max(1, int(n_steps / len(self.collectors)))\n            print(\"Starting collector {}\".format(i))\n            collector.start()\n        tmp = []\n        for _ in self.collectors:\n            res = result_queue.get()\n            tmp.extend(res)\n        [collector.join() for collector in self.collectors]\n        return tmp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### init "},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_logs_path = os.path.join(data_path, 'tf_log') # For tensorboard\ninfo_path = os.path.join(data_path, 'info.json')\nwriter = tf.summary.create_file_writer(tf_logs_path)\nos.makedirs(tf_logs_path, exist_ok=True)\n\n# Policy\npolicy_path = os.path.join('/kaggle_simulations/agent/', 'model')\nval_policy_path = os.path.join('/kaggle_simulations/agent/', 'model_val')\npolicy = PPOPolicy()\n\n\n# restore previous training state\nbest_reward = float('-inf')\nbest_val_reward = 0.\nn_episodes = 0\nrewards=[]\nif os.path.exists(os.path.join(restore_path, 'info.json')):\n    with open(os.path.join(restore_path, 'info.json'), 'r') as f:\n        info = json.load(f)\n        best_reward = info['best_reward']\n        best_val_reward = info['best_val_reward']\n        n_episodes = info['n_episodes']\n    policy.load(os.path.join(restore_path, 'model'))\n\n# Define the episode collector\nPARALLEL_COLLECTOR = False # ParallelEpisodeCollector No working since last update of kaggle-environment\ncollector = None\nn_collect = 5 #collect 5 episodes each step\nif PARALLEL_COLLECTOR:\n    collector = ParallelEpisodeCollector(env_fn, mp.cpu_count(), policy)\nelse:\n    collector = EpisodeCollector(env_fn(), policy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(steps):\n    global best_reward, best_val_reward,rewards,n_episodes\n\n    EpisodeCollector.max_episode = steps\n    EpisodeCollector.n_episode = n_episodes\n    i = 0\n    while EpisodeCollector.n_episode < EpisodeCollector.max_episode:\n        print(\"Collect episodes...\")\n        memories = collector.collect(n_collect)\n        print(\"Updating the policy...\")\n        losses = policy.train(memories)\n        \n        reward = record(memories, EpisodeCollector.n_episode, losses)\n        # Save the best best policy\n        if reward >= best_reward:\n            best_reward = reward\n            print(\"Saving best policy...\")\n            policy.save(policy_path)\n        print(\n            f\"Episode: {n_episodes} | \"\n            f\"Reward: {int(reward)} | \"\n            f\"Best Reward: {int(best_reward)} | \"\n            f\"Episode Rewards: {[mem.rewards[-1] for mem in memories]} | \"\n        )\n\n        # Validation\n        if i % 10 == 0 and i != 0:\n            print(\"Agent validation...\")\n            policy.val = True\n            memories = collector.collect(2)\n            policy.val = False\n\n            EpisodeCollector.n_episode -= len(memories)\n            rew = sum([mem.rewards[-1] for mem in memories if not mem.isEmpty()]) / len(memories)\n            print(f\"Validation reward : {rew}\")\n            with writer.as_default():\n                tf.summary.scalar(\"val_reward\", rew, step=EpisodeCollector.n_episode)\n                writer.flush()\n            if rew >= best_val_reward:\n                best_val_reward = rew\n                print(\"Saving best validation policy...\")\n                policy.save(val_policy_path)\n        i += 1\n\n\ndef record(memories: List[Memory], current_step, losses):\n    global n_episodes, info_path,rewards\n    if not memories:\n        return 0\n    n_episodes += len(memories)\n    reward = sum([memory.rewards[-1] for memory in memories]) / len(memories)\n    rewards.append(reward)\n\n    with writer.as_default():\n        if losses[0] is not None: tf.summary.scalar(\"Actor loss\", sum(losses[0]) / len(losses[0]), step=current_step)\n        if losses[1] is not None: tf.summary.scalar(\"Critic loss\", sum(losses[1]) / len(losses[1]), step=current_step)\n        tf.summary.scalar(\"best_reward\", best_reward, step=current_step)\n        writer.flush()\n\n    with open(info_path, 'w') as f:\n        json.dump({\n            'best_reward': best_reward,\n            'n_episodes': n_episodes,\n            'best_val_reward': best_val_reward,\n        }, f)\n    return reward","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Start training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start training\nn_collect = 1\ntrain(20) # The training is long. I'll just show a few episodes and continue on my laptop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Plot rewards')\n# smooth first\ndef exponential_average(old, new, b1=0.99):\n    return old * b1 + (1 - b1) * new\n\nrewards_ = []\nold = 0\nfor r in rewards:\n    old = exponential_average(old,r)\n    rewards_.append(old)\n\nsns.lineplot(range(len(rewards)),rewards_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all that remains is to train the agent, hoping that it will converge. Except that train, this agent from scratch will be a very hard task. In my next notebook, I will show how to train it to imitate a rule base one  which will then serve as a base."},{"metadata":{"trusted":true},"cell_type":"code","source":"#build_actor().save_weights('/kaggle_simulations/agent/model.actor.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#! rm /kaggle_simulations/agent/main.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build submission.tar.gz"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile -a /kaggle_simulations/agent/main.py\n\nimport time\n\n\n# set weights in policy\nactor = build_actor(verbose=False)\n# Now that we can submit the tar.gz file, I'm still having a timeout issue. \nactor.load_weights('/kaggle_simulations/agent/model.actor.h5')\nactor.predict([np.zeros((1,22,8)),np.zeros((1,31))])\n\n\ndef agent(obs):\n    s = time.time()\n\n    obs = obs['players_raw'][0]\n    state,_ = OBSParser.parse(obs)\n    action = np.argmax(actor.predict(state),axis=-1)[0]\n    #print(f'Action : {action} | Duration : {time.time()-s}')\n    return [int(action)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle_simulations/agent/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! cd /kaggle_simulations/agent/ && tar -czvf submission.tar.gz  main.py model.actor.h5\n! mv  /kaggle_simulations/agent/submission.tar.gz /kaggle/working/submission.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Test the agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys \nsys.path.append(os.path.abspath(\"/kaggle_simulations/agent/\"))\nimport main","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is a time out issue. It needs time to load libraries like TensorFlow... and the model weights. \n# I'm still looking for a workaround\nfrom kaggle_environments import make\nenv = make(\"football\", \n           debug=True,\n           configuration={\"save_video\": True, \n                          \"scenario_name\": \"11_vs_11_kaggle\", \n                          \"running_in_notebook\": True,\n                          #\"actTimeout\": 30,\n                         })  \noutput = env.run([main.agent, \"/kaggle_simulations/agent/main.py\"])[-1]\nprint('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"!!! **Facing two problems:**\n\nParallelEpisodeCollector getting locked and the timeout issue"},{"metadata":{},"cell_type":"markdown","source":"I think I'm done. My goal was to build an easily customizable agentüòÖÔ∏è, not training(no resources), Still, I'll be happy to see agents based on this one at the top of the LB. The code can be adapted to any A2C like an algorithm: you just have to change the ppo_loss and the advantage function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf football\n!rm -rf kaggle-environments\n!rm -rf kaggle-football","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}