{"cells":[{"metadata":{},"cell_type":"markdown","source":"Along with the Google Research Football competition, I'm trying to implement key algorithms of reinforcement learning in [https://github.com/yamatokataoka/reinforcement-learning-replications](https://github.com/yamatokataoka/reinforcement-learning-replications).\n\nI've implemented VPG, TRPO and PPO so far.\n\nIn this notebook, you'll train a TRPO agent with the Reinforcement Learning Replications (rl-replicas).\n\nIf you're not familiar with TRPO or RL, I highly recomend to read through the [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/user/introduction.html)"},{"metadata":{},"cell_type":"markdown","source":"install gfootball required tools"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"# Install:\n# Kaggle environments.\n!git clone -q https://github.com/Kaggle/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y -qq > /dev/null\n!apt-get install -y -qq libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -q -b v2.8 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n\n!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip install .","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"install rl-replicas"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install rl-replicas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train with rl-replicas"},{"metadata":{},"cell_type":"markdown","source":"In the rl-replicas, you'll set up the TRPO algorithm with `Policy` and `ValueFunction`. Those two core components have their own neural network and optimizer.\n\nThe trained model is saved in `./trpo/model.pt`.\nYou'll use this for test run."},{"metadata":{},"cell_type":"markdown","source":"##### Note\nTRPO uses `ConjugateGradientOptimizer` as an optimizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm_name = 'trpo'\nenvironment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'\nepochs = 5\nsteps_per_epoch = 4000\npolicy_network_architecture = [64, 64]\nvalue_function_network_architecture = [64, 64]\nvalue_function_learning_rate = 1e-3\noutput_dir = './trpo'\n\nenv: gym.Env = gym.make(environment_name)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]\n)\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())\n)\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]\n)\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters(), lr=value_function_learning_rate)\n)\n\nmodel: TRPO = TRPO(policy, value_function, env, seed=0)\n\nprint('an experiment to: {}'.format(output_dir))\n\nprint('algorithm:           {}'.format(algorithm_name))\nprint('epochs:              {}'.format(epochs))\nprint('steps_per_epoch:     {}'.format(steps_per_epoch))\nprint('environment:         {}'.format(environment_name))\n\nprint('value_function_learning_rate: {}'.format(value_function_learning_rate))\nprint('policy network:')\nprint(policy.network)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.learn(\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    output_dir=output_dir,\n    model_saving=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run inference with rl-replicas\n\nTo run inference, you'll initialize `Policy` with the trained network. The optimizer is uncessary here."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ./agent.py\nimport time\n\nimport torch\nimport gfootball\nimport gym\nfrom gfootball.env.wrappers import Simple115StateWrapper\n\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.torch_net import mlp\n\nstart_setup_time: float = time.time()\n\nnum_observation = 115\nnum_action = 19\npolicy_network_architecture = [64, 64]\nmodel_location = './trpo/model.pt'\nmodel = torch.load(model_location)\n\npolicy_network = mlp(\n    sizes = [num_observation]+policy_network_architecture+[num_action]\n)\n\npolicy_network.load_state_dict(model['policy_state_dict'])\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = None\n)\n\ncurrent_step: int = 0\n\nprint('Set up Time: {:<8.3g}'.format(time.time() - start_setup_time))\n\ndef agent(observation):\n    global policy\n    global current_step\n\n    start_time: float = time.time()\n    current_step += 1\n\n    raw_observation = observation['players_raw']\n    simple_115_observation = Simple115StateWrapper.convert_observation(raw_observation, fixed_positions=False)\n    observation_tensor: torch.Tensor = torch.from_numpy(simple_115_observation).float()\n\n    action = policy.predict(observation_tensor)\n    \n    if (current_step%100) == 0:\n        print('Current Step: {}'.format(current_step))\n\n    one_step_time = time.time() - start_time\n    if one_step_time >= 0.2:\n        print('One Step Time exceeded 0.2 seconds: {:<8.3g}'.format(one_step_time))\n\n    return [action.item()]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test run"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"football\", \n           configuration={\n             \"save_video\": True, \n             \"scenario_name\": \"11_vs_11_kaggle\",\n             \"running_in_notebook\": True,\n           })\n\noutput = env.run([\"./agent.py\", \"do_nothing\"])[-1]\n\nprint('Left player: reward = {}, status = {}, info = {}'.format(output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: reward = {}, status = {}, info = {}'.format(output[1]['reward'], output[1]['status'], output[1]['info']))\n\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}