{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Install:\n# Kaggle environments.\n!git clone https://github.com/Kaggle/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install rl-replicas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm = \"trpo\"\nenvironment = \"GFootball-11_vs_11_kaggle-simple115v2-v0\"\nepochs = 5\nsteps_per_epoch = 4000\npolicy_architecture = [64, 64]\nvalue_function_architecture = [64, 64]\nvalue_function_learning_rate = 1e-3\noutput_dir = './trpo'\n\nenv: gym.Env = gym.make(environment)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_architecture+[env.action_space.n])\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params = policy_network.parameters()))\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_architecture+[1])\n\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters(), lr = value_function_learning_rate))\n\nmodel: TRPO = TRPO(policy, value_function, env, seed = 0)\n\nprint(\"an experiment to: {}\".format(output_dir))\n\nprint(\"algorithm:           {}\".format(algorithm))\nprint(\"epochs:              {}\".format(epochs))\nprint(\"steps_per_epoch:     {}\".format(steps_per_epoch))\nprint(\"environment:         {}\".format(environment))\n\nprint(\"value_function_learning_rate: {}\".format(value_function_learning_rate))\nprint(\"policy network:\")\nprint(policy.network)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.learn(\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    output_dir = output_dir,\n    model_saving = True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ./agent.py\nimport time\n\nimport torch\nimport gfootball\nimport gym\nfrom gfootball.env.wrappers import Simple115StateWrapper\n\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.torch_net import mlp\n\nstart_setup_time: float = time.time()\n\nnum_observation = 115\nnum_action = 19\npolicy_network_architecture = [64, 64]\nmodel_location = \"./trpo/model.pt\"\nmodel = torch.load(model_location)\n\npolicy_network = mlp(\n    sizes = [num_observation] + policy_network_architecture + [num_action])\n\npolicy_network.load_state_dict(model[\"policy_state_dict\"])\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = None\n)\n\ncurrent_step: int = 0\n\nprint(\"Set up Time: {:<8.3g}\".format(time.time() - start_setup_time))\n\ndef agent(observation):\n    global policy\n    global current_step\n\n    start_time: float = time.time()\n    current_step += 1\n\n    raw_observation = observation[\"players_raw\"]\n    simple_115_observation = Simple115StateWrapper.convert_observation(raw_observation, fixed_positions = False)\n    observation_tensor: torch.Tensor = torch.from_numpy(simple_115_observation).float()\n\n    action = policy.predict(observation_tensor)\n    \n    if (current_step%100) == 0:\n        print(\"Current Step: {}\".format(current_step))\n\n    one_step_time = time.time() - start_time\n    if one_step_time >= 0.2:\n        print(\"One Step Time exceeded 0.2 seconds: {:<8.3g}\".format(one_step_time))\n\n    return [action.item()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\nfrom kaggle_environments.envs.football.helpers import *\n\n# @human_readable_agent wrapper modifies raw observations \n# provided by the environment:\n# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations\n# into a form easier to work with by humans.\n# Following modifications are applied:\n# - Action, PlayerRole and GameMode enums are introduced.\n# - 'sticky_actions' are turned into a set of active actions (Action enum)\n#    see usage example below.\n# - 'game_mode' is turned into GameMode enum.\n# - 'designated' field is removed, as it always equals to 'active'\n#    when a single player is controlled on the team.\n# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.\n# - Action enum is to be returned by the agent function.\n@human_readable_agent\n\ndef agent(obs):\n    # Make sure player is running.\n    if Action.Sprint not in obs[\"sticky_actions\"]:\n        return Action.Sprint\n    # We always control left team (observations and actions\n    # are mirrored appropriately by the environment).\n    controlled_player_pos = obs[\"left_team\"][obs[\"active\"]]\n    # Does the player we control have the ball?\n    if obs[\"ball_owned_player\"] == obs[\"active\"] and obs[\"ball_owned_team\"] == 0:\n        # Shot if we are 'close' to the goal (based on 'x' coordinate).\n        if controlled_player_pos[0] > 0.5:\n            return Action.Shot\n        # Run towards the goal otherwise.\n            return Action.Right\n    else:\n        # Run towards the ball.\n        if obs[\"ball\"][0] > controlled_player_pos[0] + 0.05:\n            return Action.Right\n        if obs[\"ball\"][0] < controlled_player_pos[0] - 0.05:\n            return Action.Left\n        if obs[\"ball\"][1] > controlled_player_pos[1] + 0.05:\n            return Action.Bottom\n        if obs[\"ball\"][1] < controlled_player_pos[1] - 0.05:\n            return Action.Top\n        # Try to take over the ball if close to the ball.\n        return Action.Slide","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"football\", \n           configuration = {\n             \"save_video\": True, \n             \"scenario_name\": \"11_vs_11_kaggle\",\n             \"running_in_notebook\": True,\n           })\n\noutput = env.run([\"./agent.py\", \"do_nothing\"])[-1]\n\nprint(\"Left player: reward = {}, status = {}, info = {}\".format(output[0][\"reward\"], output[0][\"status\"], output[0][\"info\"]))\nprint(\"Right player: reward = {}, status = {}, info = {}\".format(output[1][\"reward\"], output[1][\"status\"], output[1][\"info\"]))\n\nenv.render(mode = \"human\", width = 800, height = 600)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}