{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploring GFootball environment\n\nUpdate 13/10/2020\nI'm adding a section on running the enviroment using the Kaggle API. This is important for evulatuion, and validating the agent locally (with debugging).\n\n\n\nI'm new to the GFootball environment and these are my notes on it's usage and behaviour. I plan to try it out (and train) using the Gym API first as I have some familiarity with it, then move on to understanding how the Kaggle environment wrappers work. Hopefully this will be useful for anyone not familiar with the OpenAI Gym API.\n\nI'm also working on another couple of notebooks concurrently with this one:  \n - [Deep Q-learner start code](https://www.kaggle.com/garethjns/deep-q-learner-starter-code) - Building a deep Q-learner from scratch for the Simple115 version of this environment  \n - [Convolutional deep Q-learner](https://www.kaggle.com/garethjns/convolutional-deep-q-learner) - Upgrading the deep Q-learner to use a convolutional model with the SMM version of this environment  \n\nContents:\n  1. Setup  \n  2. Alternative setup\n  3. Gym API   \n    4. GFootball observation space and wrappers  \n    5. GFootball action space and wrappers  \n  4. Kaggle API\n    1. Debugging agents\n  6. Example with gym compatible q-learner  \n  7. Convert example agent to Kaggle-compatible submission\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pprint\nimport glob \nimport imageio\nimport pathlib\nimport numpy as np\nfrom typing import Tuple\nfrom tqdm import tqdm\nimport os\nimport sys\nfrom IPython.display import Image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup\n(as per https://www.kaggle.com/piotrstanczyk/gfootball-template-bot)\n\nThis downloads a pre-compiled game engine for the GFootball package, which we'll use here."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https://github.com/google-research/football.git\n!mkdir -p football/third_party/gfootball_engine/lib\n!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# Some helper code\n!git clone https://github.com/garethjns/kaggle-football.git\nsys.path.append(\"/kaggle/working/kaggle-football/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Alternative setup \n\nThis doesn't work in the Kaggle Kernel, but gfootball can be compiled and pip installed in Ubuntu 20 (tested in Python 3.8) with the following additional depencies. This might be useful if anyone has trouble running it locally."},{"metadata":{},"cell_type":"markdown","source":"```Bash\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev libsdl2-image-dev\n\n# Dependencies for PyGame\n!apt-get install -y \\\n  python-dev \\\n  python-numpy \\\n  subversion \\\n  ffmpeg \\\n  libsdl1.2-dev \\\n  libsdl-image1.2-dev \\\n  libsdl-mixer1.2-dev \\\n  libsdl-ttf2.0-dev \\\n  libavcodec-dev \\\n  libavformat-dev \\\n  libportmidi-dev \\\n  libsmpeg-dev \\\n  libswscale-dev \\\n\n!pip install gfootball\n```"},{"metadata":{},"cell_type":"markdown","source":"# Gym interface\n\nThe GFootball package defines an API following the OpenAI Gym spec: https://gym.openai.com/docs/. This allows for standardised interaction between agent and environments, for example (with a random \"agent\"):\n\n```python\nimport gym\n\nenv = gym.make('CartPole-v0')\nobs = env.reset()\nfor _ in range(1000):\n    env.render()\n    new_obs, reward, done, info = env.step(env.action_space.sample())\nenv.close()\n```\n\nNote that before using any GymEnv the .reset() must be called. This returns an observation.\n\nenv.step() takes an action to apply to the environment, typically this comes from an Agent. In the above example it is just randomly sampled from the environmentâ€™s action space.\n\nIt then returns a few things:\n - new observations (which are typically passed to the agent on the next loop iteration)\n - any reward from the environment (can be negative)\n - a done flag indicating if the environment has reached a terminal state\n - \"info\" - a dict containing anything else, this isn't supposed to be used by the agent\n"},{"metadata":{},"cell_type":"markdown","source":"## Registering and creating Gym enviroments"},{"metadata":{},"cell_type":"markdown","source":"Gym environments need to be registered in order to be created with gym.make(). 4 versions of multiple environments are automatically registered on import of the GFootball package (see https://github.com/google-research/football/blob/master/gfootball/__init__.py).\n\nThe scenario relevant for this competition is (presumably) [11_vs_11_kaggle.py](https://github.com/google-research/football/blob/master/gfootball/scenarios/11_vs_11_kaggle.py). This defines the of players, difficulty, etc.\n\nThe 4 versions return different observations (see https://github.com/google-research/football/blob/master/gfootball/doc/observation.md for details of each).\n - simple115 (bugged, don't use)\n - simple115_v2\n - extracted\n - pixels/pixels_gray \n \n These can be created with the following commands (note names are case sensitive):\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nimport gfootball  # Required as envs registered on import\n\nsimple_env = gym.make(\"GFootball-11_vs_11_kaggle-simple115v2-v0\")\npixels_env = gym.make(\"GFootball-11_vs_11_kaggle-Pixels-v0\")\nsmm_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\n\nprint(f\"simple115v2:\\n {simple_env.__str__()}\\n\")\nprint(f\"Pixels:\\n {pixels_env.__str__()}\\n\")\nprint(f\"SMM:\\n {smm_env.__str__()}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each of these environments include various wrappers that modify the interaction of the agent with the base environment by intercepting actions and observations. For example the wrapper Simple115StateWrapper modifies the \"raw\" output of the base environment to match the spec described in https://github.com/google-research/football/blob/master/gfootball/doc/observation.md"},{"metadata":{},"cell_type":"markdown","source":"## The base environment\n\nThe base environment is defined in gfootball.env.football_env:FootballEnv and can be registered for use with gym.make:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gfootball.env.football_env import FootballEnv\n\nenv_name = \"GFootballBase-v0\"\ngym.envs.register(id=env_name,\n                  entry_point=\"gfootball.env.football_env:FootballEnv\",\n                  max_episode_steps=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that FootballEnv can also be instantiated directly by importing the Python class as usual.\n\nIn either case, creating it requires a Config object (these are already defined for the pre-registered envs/scenarios)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gfootball.env.config import Config\n\nbase_env = gym.make(env_name, config=Config())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that this sets a default Config which may differ from the Kaggle scenario, although on cursory inspection it appears to have 11 players on each side. See below to use the make function to handle config setting for different scenarios."},{"metadata":{},"cell_type":"markdown","source":"## Observation space\n\n\n### Raw\n\nWith the base env it's possible to see all the obervation details."},{"metadata":{"trusted":true},"cell_type":"code","source":"obs = base_env.reset()\n\npprint.pprint(obs[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### simple115_v2\n\nWith the Simple115V2 wrapper, this is converted to a single array:"},{"metadata":{"trusted":true},"cell_type":"code","source":"obs = simple_env.reset()\n\nprint(obs.shape)\n\npprint.pprint(obs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMMWrapper\n\nThe SMMWrapper returns a visual representation of the game."},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_football.viz import generate_gif, plot_smm_obs\n\nsmm_env = gym.make(\"GFootball-11_vs_11_kaggle-SMM-v0\")\nprint(smm_env.reset().shape)\n\ngenerate_gif(smm_env, n_steps=200)\nImage(filename='smm_env_replay.gif', format='png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PixelsStateWrapper\n\nThe pixels wrapper requires rendering before obtaining an observation. This appears to crash the kernel??\n\n```python\npixels_env.render()\nobs = pixels_env.reset()\n```"},{"metadata":{},"cell_type":"markdown","source":"# Action space\n\nhttps://github.com/google-research/football/blob/master/gfootball/doc/observation.md#actions\n"},{"metadata":{},"cell_type":"markdown","source":"# Make\n\nThe gfootball package has its own enviroment builder function, this is useful for setting the enviroment configuration for scenarios. It also adds a couple of useful wrappers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gfootball.env import create_environment\n\n# (These are the args set by the kaggle_environments package)\nCOMMON_KWARGS = {\"stacked\": False, \"representation\": 'raw', \"write_goal_dumps\": False,\n                 \"write_full_episode_dumps\": False, \"write_video\": False, \"render\": False,\n                 \"number_of_left_players_agent_controls\": 1, \"number_of_right_players_agent_controls\": 0}\n\ncreate_environment(env_name='11_vs_11_kaggle')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rewards\n\nBy default the enviroments awards the agent -1 on conceding a goal and +1 for scoring. \n\nCheckpoint rewards can be added when creating the enviroment:"},{"metadata":{"trusted":true},"cell_type":"code","source":"chk_reward_env = create_environment(env_name='11_vs_11_kaggle', rewards='scoring,checkpoints')\n\n_ = chk_reward_env.reset()\nfor s in range(100):\n    _, r, _, _ = chk_reward_env.step(5)\n    if r > 0:\n        print(f\"Step {s} checkpoint reward recieved: {r}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Academy environments\n\n[Multuple scenarious are define for GFootball](https://github.com/google-research/football/tree/master/gfootball/scenarios). These include academy enviroments which are much simplair that the full game. They can be created by specifying the name, for example:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_to_score_env = create_environment(env_name='academy_run_to_score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kaggle API"},{"metadata":{},"cell_type":"markdown","source":"## Running agents\n\nThe Kaggle API for evaluation runs agents defined in files. For example, a compatible random agent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random_agent.py\n  \nfrom typing import Any\nfrom typing import List\n\nimport numpy as np\n\n\nclass RandomAgent:\n    def get_action(self, obs: Any) -> int:\n        return np.random.randint(19)\n\n\nAGENT = RandomAgent()\n\n\ndef agent(obs) -> List[int]:\n    return [AGENT.get_action(obs)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This agent can be run against a built in AI, or another agent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make  \nenv = make(\"football\", configuration={\"save_video\": True,\n                                      \"scenario_name\": \"11_vs_11_kaggle\"})\n\n# Define players\nleft_player = \"random_agent.py\"  # A custom agent, eg. random_agent.py or example_agent.py\nright_player = \"run_right\"  # eg. A built in 'AI' agent\n\n# Run the whole sim\n# Output returned is a list of length n_steps. Each step is a list containing the output for each player as a dict.\n# steps\noutput = env.run([left_player, right_player])\n\nfor s, (left, right) in enumerate(output):\n    \n    # Just print the last few steps of the output\n    if s > 2990:\n        print(f\"\\nStep {s}\")\n\n        print(f\"Left player ({left_player}): \\n\"\n              f\"actions taken: {left['action']}, \"\n              f\"reward: {left['reward']}, \"\n              f\"status: {left['status']}, \"\n              f\"info: {left['info']}\")\n\n        print(f\"Right player ({right_player}): \\n\"\n              f\"actions taken: {right['action']}, \"\n              f\"reward: {right['reward']}, \"\n              f\"status: {right['status']}, \"\n              f\"info: {right['info']}\\n\")\n\nprint(f\"Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}\")\n\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output of ```env.run()``` contains a list of output from each step."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(output[-1][0].keys())\nprint(f\"Left player: {output[-1][0]['status']}: {output[-1][0]['info']}\")\nprint(f\"Right player: {output[-1][0]['status']}: {output[-1][1]['info']}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile broken_agent.py\n  \nfrom typing import Any\nfrom typing import List\n\nclass DeliberateException(Exception):\n    pass\n\n\nclass BrokenAgent:\n    def get_action(self, obs: Any) -> int:\n        raise DeliberateException(f\"I am broken.\")\n\n\nAGENT = BrokenAgent()\n\n\ndef agent(obs) -> List[int]:\n    return [AGENT.get_action(obs)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that if the agent fails, the full traceback isn't returned unless debug mode is specified. \n\nFor example, dubug off with a broken agent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"football\", configuration={\"save_video\": True,\n                                      \"scenario_name\": \"11_vs_11_kaggle\"})\n\noutput = env.run([\"random_agent.py\", \"broken_agent.py\"])\n\nprint(len(output))\nprint(f\"Left player: {output[-1][0]['status']}: {output[-1][0]['info']}\")\nprint(f\"Right player: {output[-1][0]['status']}: {output[-1][1]['info']}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not so useful, but with debugging on:"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"football\", debug=True,\n           configuration={\"save_video\": True,\n                          \"scenario_name\": \"11_vs_11_kaggle\"})\n\ntry:\n    output = env.run([\"random_agent.py\", \"broken_agent.py\"])\nexcept DeliberateException as e:\n    print(e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much more useful!\n\n## Breakpoints / interactive debugging\n\nI don't think it's possible to set breakpoints and debug into the agent loaded by ```env.run()```, even with the ```debug=True```, however it is possible it is possible by stepping the env manually ([you're **seriously** missing out of you're not using a decent IDE like PyCharm to do this!](https://www.youtube.com/watch?v=QJtWxm12Eo0))."},{"metadata":{"trusted":true},"cell_type":"code","source":"from random_agent import agent  \n\n\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\"})\nenv.reset()\n\n# This is the observation that is passed to agent function\nobs_kag_env = env.state[0]['observation']\n\nfor _ in range(3000):\n    action = agent(obs_kag_env)\n\n    # Environment step is list of agent actions, ie [[agent_1], [agent_2]], \n    # here there is 1 action per agent.\n    other_agent_action = [0]\n    full_obs = env.step([action, other_agent_action])\n    obs_kag_env = full_obs[0]['observation']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example with a Gym-compatible Q-learning agent\n\nThis section the GFootball environment with a agent designed to run in Gym environments. The agent is a linear Q agent from the [reinforcement_learning_keras](https://github.com/garethjns/reinforcement-learning-keras), which I've been working on recently to learn about RL in general. It also includes a deep Q learner, which will hopefully make a more interesting example in the future.\n\nThis linear Q agent uses only sklearn (not Keras or TensorFlow, so there's less to go wrong). It's simple, it works great on cart-pole, but don't expect it to do well here!\n\nThis agent has a .train method that is defined [here](https://github.com/garethjns/reinforcement-learning-keras/blob/master/reinforcement_learning_keras/agents/agent_base.py), internally this runs multiple episodes which run the familiar [Gym training loop](https://github.com/garethjns/reinforcement-learning-keras/blob/01b1e7e4e827e7816dae796ebefa9211a558ae7b/reinforcement_learning_keras/agents/q_learning/linear_q_agent.py#L138):\n\n```python\n    ...\n\n    def _play_episode(self, max_episode_steps: int = 500,\n                      training: bool = False, render: bool = True) -> Tuple[float, int]:\n        \"\"\"\n        Play a single episode and return the total reward.\n        :param max_episode_steps: Max steps before stopping, overrides any time limit set by Gym.\n        :param training: Bool to indicate whether or not to use this experience to update the model.\n        :param render: Bool to indicate whether or not to call env.render() each training step.\n        :return: The total real reward for the episode.\n        \"\"\"\n        self.env._max_episode_steps = max_episode_steps\n        obs = self.env.reset()\n        total_reward = 0\n        for frame in range(max_episode_steps):\n            action = self.get_action(obs, training=training)\n            prev_obs = obs\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n\n            if render:\n                self.env.render()\n\n            if training:\n                self.update_model(s=prev_obs, a=action, r=reward, d=done, s_=obs)\n\n            if done:\n                break\n\n        return total_reward, frame\n\n    ...\n```\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install reinforcement_learning_keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gym\nfrom reinforcement_learning_keras.agents.components.history.training_history import TrainingHistory\nfrom reinforcement_learning_keras.agents.q_learning.exploration.epsilon_greedy import EpsilonGreedy\nfrom reinforcement_learning_keras.agents.q_learning.linear_q_agent import LinearQAgent\nfrom sklearn.exceptions import DataConversionWarning\n\nimport warnings\n\n\nagent = LinearQAgent(name=\"linear_q\",\n                     env_spec=\"GFootball-11_vs_11_kaggle-simple115v2-v0\",\n                     eps=EpsilonGreedy(eps_initial=0.9, decay=0.001, eps_min=0.01, \n                                       decay_schedule='linear'),\n                     training_history=TrainingHistory(agent_name='linear_q', \n                                                      plotting_on=True, plot_every=25, \n                                                      rolling_average=1))\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', DataConversionWarning)\n    agent.train(verbose=True, render=False,\n                n_episodes=25, max_episode_steps=2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Creating a submission\n\nThe RandomAgent defined above is submittable as it is. See also [GFootball Template Bot](https://www.kaggle.com/piotrstanczyk/gfootball-template-bot) for a similar example using model that doesn't require saved weights, like a hand crafted bot.\n\n\nMore complex models that use learned weights are a bit trickier; see [Deep Q-learner start code](https://www.kaggle.com/garethjns/deep-q-learner-starter-code) and [Convolutional deep Q-learner](https://www.kaggle.com/garethjns/convolutional-deep-q-learner) for examples of creating submissions for models that use external data (liked learned neural network weights, or the weights learned by the LinearQ learner above).\n\n\nI'm also compiling examples and notes here https://github.com/garethjns/kaggle-football\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}