{"nbformat_minor":0,"metadata":{"language_info":{"name":"python","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"metadata":{"_execution_state":"idle","_cell_guid":"e64d5b95-66eb-46aa-9ccd-45e654d19b4e","_uuid":"ba5959f9bf1742c8a36caf01293218525bd09b84","collapsed":false},"execution_count":null,"cell_type":"markdown","outputs":[],"source":"Just a quick look at duplicates in the training data itself.\n\nThere is roughly a 10% of entries with exactly the same 'X' features, with just different 'ID' and 'y' fields, sometimes diverging quite a lot. A good example of this are rows 3070 and 3133.\n\nUntil now I just ignored this, but it might be worth reducing \"dimensionality\" in the sample space."},{"metadata":{"_execution_state":"idle","_cell_guid":"2cde9f9a-5f50-43f9-9e9e-59de41e72aa4","_uuid":"49b6ce872c1991198b34b419ab8fd63f81a2cb7b","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"},{"metadata":{"_execution_state":"idle","_cell_guid":"89d138c3-477b-4cdb-9213-7d4f77557bb1","_uuid":"b5506644159742512da5e3b9cbc53c40271cef2a","collapsed":false,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')"},{"metadata":{"_execution_state":"idle","_cell_guid":"6e9b6044-0523-4ff3-bd9e-889e22bb1d52","_uuid":"d4bac5a6d926b5416a6085b5a7f371ce73a5fe75","collapsed":false,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"feature_columns = train_data.columns[2:]\nfeature_columns"},{"metadata":{"_execution_state":"idle","_cell_guid":"9457afe3-f809-4877-a27d-a60ffa8752ea","_uuid":"a38396157cab4868e69dbba75f08739c1ed5a1b6","collapsed":false,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"label_columns = []\nfor dtype, column in zip(train_data.dtypes, train_data.columns):\n    if dtype == object:\n        label_columns.append(column)\nlabel_columns"},{"metadata":{"_execution_state":"idle","_cell_guid":"116347ef-af5e-40b7-aafc-0ab1eba557ad","_uuid":"6508622f1b23ad5841655e219a43a321c82f6c94","collapsed":false,"trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"print(\"{} duplicate entries in training, out of {}, a {:.2f} %\".format(\n    len(train_data[train_data.duplicated(subset=feature_columns, keep=False)]),\n    len(train_data),\n    100 * len(train_data[train_data.duplicated(subset=feature_columns, keep=False)]) / len(train_data)\n    ))\ntrain_data[train_data.duplicated(subset=feature_columns, keep=False)].sort_values(by=label_columns)"},{"cell_type":"markdown","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"0ca09df2-b657-4b6d-8db9-2abd312053e9","_uuid":"05cee8db81e0fcaaaf43adc29dd529110f8f4a2a","collapsed":false},"outputs":[],"source":"Let's take a quick look at the standard deviation of the 'y' for each duplicate group."},{"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"6ccedc80-2f92-45ef-ae6a-2e9d306fcf91","_uuid":"4fd7ac27b107c4d5e41b5b405d32cadf436778b6","collapsed":false,"trusted":false},"outputs":[],"source":"duplicate_std = train_data[train_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values))['y'].aggregate(['std', 'size']).reset_index(drop=True)\n\nduplicate_std.sort_values(by='std', ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"531499f0-f601-4d75-8767-4759b50499d9","_uuid":"e915dd470712c16fd2a3fd3ccaf02ff1f33cba85","collapsed":false,"trusted":false},"outputs":[],"source":"print(\"{} duplicate groups in training\".format(\n    len(train_data[train_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values)).size().reset_index())))\n\n    \ntrain_data[train_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values)).size().reset_index()"},{"metadata":{"_execution_state":"idle","_cell_guid":"e3c738f1-08ce-4361-be49-999adea77dc8","_uuid":"ce14a9abcec55b1e10e90695ed9e512183718882","collapsed":false},"execution_count":null,"cell_type":"markdown","outputs":[],"source":"Is the same the case with test_data?"},{"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"92b7f650-3538-4ee5-829e-9e9efb9064e2","_uuid":"25f52a15d71ad80c6d8424ffd7f87234660bf1c3","collapsed":false,"trusted":false},"outputs":[],"source":"print(\"{} duplicate entries in test, out of {}, a {:.2f} %\".format(\n    len(test_data[test_data.duplicated(subset=feature_columns, keep=False)]),\n    len(test_data),\n    100 * len(test_data[test_data.duplicated(subset=feature_columns, keep=False)]) / len(test_data) \n    ))\ntest_data[test_data.duplicated(subset=feature_columns,\n                               keep=False)].groupby(label_columns, axis=0).count()[['ID']]"},{"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"130cae92-2286-4451-af88-8758e0b57b29","_uuid":"8d074c9c53938552bdcadbd0482eb8bf08a6db07","collapsed":false,"trusted":false},"outputs":[],"source":"print(\"{} duplicate groups in test\".format(\n    len(test_data[test_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values)).size().reset_index())))\n\ntest_data[test_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values)).size().reset_index()"},{"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"59245519-57ee-4b70-93ad-37136192d5c4","_uuid":"b416f15c9a9e3d06a1b8f3780321b70b45770fa6","collapsed":false,"trusted":false},"outputs":[],"source":"all_data = pd.concat((train_data.drop('y', axis=1), test_data))\nprint(\"{} duplicate entries in total, out of {}, a {:.2f} %\".format(\n    len(all_data[all_data.duplicated(subset=feature_columns, keep=False)]),\n    len(all_data),\n    100 * len(all_data[all_data.duplicated(subset=feature_columns, keep=False)])/ len(all_data)\n    ))\n\nprint(\"{} duplicate groups in total\".format(\n    len(all_data[all_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values)).size().reset_index())))\n\nall_data[all_data.duplicated(subset=feature_columns,\n                             keep=False)].groupby(list(feature_columns.values)).size().reset_index()"},{"cell_type":"markdown","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"42315533-fe09-4638-a6a9-14f6832387ba","_uuid":"4e34815bf7145417470e29bbf5a3ae1cb99cebd4","collapsed":false},"outputs":[],"source":"As a quick analysis, we can see that the sample data is noisy. Also, given the divergence in the value 'y' for the same inputs (except for the 'ID'), a similar situation must be expected in the test data. Even worse, it could be the case the test data 'y' is the one diverging from the results modelled on the training."},{"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"4d8d9a4a-cba9-4d0b-8c0a-f6fbac94c1f9","_uuid":"e8856c2b11e331513becfc41148ba7eb0b35aa4b","collapsed":false,"trusted":false},"outputs":[],"source":""}],"nbformat":4}