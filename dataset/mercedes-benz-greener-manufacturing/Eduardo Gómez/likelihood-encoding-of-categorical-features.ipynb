{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","file_extension":".py","nbconvert_exporter":"python","version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":0,"cells":[{"execution_count":null,"cell_type":"markdown","metadata":{"collapsed":false,"_uuid":"f3fdf191aa6b5d1e4f9e52a6f49e0318a03626a2","_execution_state":"idle"},"outputs":[],"source":"This is just a simple implementation of likelihood encoding (also known as impact coding or target coding) for the categorical features in python.\n\nJust trying to understand how it works.\n\nI'm pretty certain code can be improved, specially regarding the creation of the mapping and its application, but I preserved the same implementation I used in which application to the training data, and creation of the encoding map are two different steps."},{"execution_count":null,"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"015ef170304529aa29067f6248c6b822c04351bb","_cell_guid":"89108e57-7e3e-4b0a-903d-1f8be4c6d6fa","trusted":false},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import KFold\n"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"_uuid":"fe137e7ed2c873ef4b516adeb59740079c26dbd0","_execution_state":"idle"},"outputs":[],"source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"_uuid":"3ccb7319abded5de2cb1d1d0eff0ca4140af7712","_execution_state":"idle"},"outputs":[],"source":"features = train_data.columns[2:]\n\nnumeric_features = []\ncategorical_features = []\n\nfor dtype, feature in zip(train_data.dtypes[2:], train_data.columns[2:]):\n    if dtype == object:\n        #print(column)\n        #print(train_data[column].describe())\n        categorical_features.append(feature)\n    else:\n        numeric_features.append(feature)\ncategorical_features"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"_uuid":"71ca0f929a71151bceec484c0c45d6a6fa1df20f","_execution_state":"idle"},"outputs":[],"source":"# This way we have randomness and are able to reproduce the behaviour within this cell.\nnp.random.seed(13)\n\ndef impact_coding(data, feature, target='y'):\n    '''\n    In this implementation we get the values and the dictionary as two different steps.\n    This is just because initially we were ignoring the dictionary as a result variable.\n    \n    In this implementation the KFolds use shuffling. If you want reproducibility the cv \n    could be moved to a parameter.\n    '''\n    n_folds = 20\n    n_inner_folds = 10\n    impact_coded = pd.Series()\n    \n    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n    kf = KFold(n_splits=n_folds, shuffle=True)\n    oof_mean_cv = pd.DataFrame()\n    split = 0\n    for infold, oof in kf.split(data[feature]):\n            impact_coded_cv = pd.Series()\n            kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)\n            inner_split = 0\n            inner_oof_mean_cv = pd.DataFrame()\n            oof_default_inner_mean = data.iloc[infold][target].mean()\n            for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):\n                # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n                oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n                impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(\n                            lambda x: oof_mean[x[feature]]\n                                      if x[feature] in oof_mean.index\n                                      else oof_default_inner_mean\n                            , axis=1))\n\n                # Also populate mapping (this has all group -> mean for all inner CV folds)\n                inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n                inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n                inner_split += 1\n\n            # Also populate mapping\n            oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n            oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n            split += 1\n            \n            impact_coded = impact_coded.append(data.iloc[oof].apply(\n                            lambda x: inner_oof_mean_cv.loc[x[feature]].mean()\n                                      if x[feature] in inner_oof_mean_cv.index\n                                      else oof_default_mean\n                            , axis=1))\n\n    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n\n# Apply the encoding to training and test data, and preserve the mapping\nimpact_coding_map = {}\nfor f in categorical_features:\n    print(\"Impact coding for {}\".format(f))\n    train_data[\"impact_encoded_{}\".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f)\n    impact_coding_map[f] = (impact_coding_mapping, default_coding)\n    mapping, default_mean = impact_coding_map[f]\n    test_data[\"impact_encoded_{}\".format(f)] = test_data.apply(lambda x: mapping[x[f]]\n                                                                         if x[f] in mapping\n                                                                         else default_mean\n                                                               , axis=1)"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"_uuid":"1b5fba5a60249adda5d90c8fee7d227508242763","_execution_state":"idle"},"outputs":[],"source":"train_data[['y', 'X0'] + list(train_data.columns[-8:])]"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"_uuid":"14df643218d40546036b652d52e2925ad6118f05","_execution_state":"idle"},"outputs":[],"source":""}],"nbformat":4}