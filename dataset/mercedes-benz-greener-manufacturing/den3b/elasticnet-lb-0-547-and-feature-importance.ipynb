{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ab037723-587e-a537-97c7-fcdf577f40af"},"source":"Hello there everyone!\n\nI am new to this competition but it looks like this dataset leads to overfitting problems. In addition, it seems like Mercedes is interested in reducing the dataset to a few meaningful variables. Therefore, I thought it was a good idea to try one of the best linear models I know for tackling overfitting that works also as a feature selection method: **Elastic Nets**.\n\nElastic Nets are essentially a **Lasso/Ridge** hybrid, that entails the minimization of an objective function that includes both **L1** (Lasso) and **L2** (Ridge) norms. You can find more about ElasticNets [here][1].  \n\nFor the sake of this notebook it is important to notice that Elastic nets depends on two parameters: \n\n* the **l1_ratio**, i.e. the tradeoff between the two norms (l1_ratio = 0 --> Ridge,  l1_ratio = 1 --> Lasso, 0<l1_ration<1 --> Mix of the two);\n* **alpha**, that regulates the amount of penalty applied.\n\nIt is important to know that minimizing the L1 norm will force some coefficients to shrink to zero, and that's why Elastic Nets can be used as feature selection techniques. Besides, when there's a high degree of collinearity in the data, the cross-validation procedure used to determine these two parameters will return low l1_ratio since Ridge tends to outperform Lasso in these cases.\n\nOk... let's use scikit-learn and see how these methods perfom and which features they select!\n\n  [1]: http://www.onthelambda.com/2015/08/19/kickin-it-with-elastic-net-regression/"},{"cell_type":"markdown","metadata":{"_cell_guid":"daa3a95f-f785-c953-0dd0-25b5108166de"},"source":"# Initialization"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c25d1c7-7d5d-23a5-f48e-1ca72e6065c4"},"outputs":[],"source":"# load modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15316c87-5ea9-3c9b-4d55-5a254a9d06ef"},"outputs":[],"source":"# load data\ntrain_df  = pd.read_csv('../input/train.csv')\ntest_df  = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e39bfe7d-2054-024d-c349-288104bde187"},"outputs":[],"source":"# get train_y, test ids and unite datasets to perform\ntrain_y = train_df['y']\ntrain_df.drop('y', axis = 1, inplace = True)\ntest_ids = test_df.ID.values\nall_df = pd.concat([train_df,test_df], axis = 0)\n\n# ...one hot encoding of categorical variables\ncategorical =  [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]\nfor f in categorical:\n    dummies = pd.get_dummies(all_df[f], prefix = f, prefix_sep = '_')\n    all_df = pd.concat([all_df, dummies], axis = 1)\n\n# drop original categorical features\nall_df.drop(categorical, axis = 1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f88fb001-8efc-bd59-f34b-ba4b5a06b7f9"},"outputs":[],"source":"# get feature dataset for test and training        \ntrain_X = all_df.drop([\"ID\"], axis=1).iloc[:len(train_df),:]\ntest_X = all_df.drop([\"ID\"], axis=1).iloc[len(train_df):,:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c322d7e-774b-93e4-b600-d7d76d44332a"},"outputs":[],"source":"print(train_X.head())\nprint(test_X.head())"},{"cell_type":"markdown","metadata":{"_cell_guid":"fccc4896-c86a-25ce-1133-1ff4a363f665"},"source":"# Model development and testing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f132b2ad-edd0-4456-5fd8-739463066d77"},"outputs":[],"source":"# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\n\ncv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b19e6690-9303-d270-66e5-4b2ae164592c"},"outputs":[],"source":"cv_model.fit(train_X, train_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7ba4139-bdec-6d31-87fc-d6a0ab051893"},"outputs":[],"source":"print('Optimal alpha: %.8f'%cv_model.alpha_)\nprint('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\nprint('Number of iterations %d'%cv_model.n_iter_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7459711f-92f4-b381-a006-53c9ff327471"},"source":"**l1_ratio = 1**, that means we are just using Lasso."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c84f21b4-7b47-1df5-e3d5-e579b97101d9"},"outputs":[],"source":"# train model with best parameters from CV\nmodel = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\nmodel.fit(train_X, train_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d4a9cb7-7b13-0144-91da-32f7eac249cd"},"outputs":[],"source":"# r2 score on training dataset\nprint(r2_score(train_y, model.predict(train_X)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"27142b96-4827-7d39-74d9-babf075bb6c7"},"source":"Uncomment below if you want the predictions on the test dataset (LB 0.547+)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5eeac2c9-ee1c-2c79-80af-059a273361c7"},"outputs":[],"source":"# preds = model.predict(test_X)\n# df_sub = pd.DataFrame({'ID': test_ids, 'y': preds})\n# df_sub.to_csv('elnet_submission_dummies.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"465a432e-5ae8-44e0-6ffe-9df98ead799e"},"source":"# Feature importance\nLet's see the importance of each feature based on the absolute value of their coefficients "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"add0d507-8417-48df-fb03-33014339a073"},"outputs":[],"source":"feature_importance = pd.Series(index = train_X.columns, data = np.abs(model.coef_))\n\nn_selected_features = (feature_importance>0).sum()\nprint('{0:d} features, reduction of {1:2.2f}%'.format(\n    n_selected_features,(1-n_selected_features/len(feature_importance))*100))\n\nfeature_importance.sort_values().tail(30).plot(kind = 'bar', figsize = (18,6))"},{"cell_type":"markdown","metadata":{"_cell_guid":"be991a61-5efb-b616-f645-003bb1820758"},"source":"## It's nice to see how these features compares with those selected by xgboost or other nonlinear methods. Anyway, 88.26% features reduction (with respect to dataset with dummies) looks nice. Besides, the performance on the LB of this linear method seems to be close to those of more sophisticated ones. \n\nVote this notebook if you liked it :P\n\nCheers"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}