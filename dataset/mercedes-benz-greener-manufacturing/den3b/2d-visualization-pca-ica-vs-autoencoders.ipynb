{"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"cells":[{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"83a3729b473c315f7bc2c66640768092ccb798c4","_cell_guid":"bdd82027-9d53-4d05-b6ec-cf95300cbaaa"},"cell_type":"markdown","execution_count":null,"source":"Hello there!\n\n*Autoencoders* (AE) are neural networks which are trained to learn a compressed representation of the inputs they are fed. For instance, they can be employed for compressing images... although they are no match for specific algorithms such as JPEG. I employed them successfully in the past for anomaly detection in real-time signal processing. \n\nAE have a symmetric hour-glass shape. The two symmetric parts are known as the **encoder** and than **decoder**, respectively (..oh really?). The encoder takes the original inputs and compress them into **codes**. The decoder tries to reconstruct the original inputs from these codes. AE are trained by minimizing a *reconstruction error*, i.e. some measure of the difference between the original patterns and the reconstructed ones.\n\nIn this notebook I will show you how **the dimensionality reduction performed by a simple AE (made using Keras) is more interesting than that of PCA or ICA**, at least for visualization purposes. In particular, I will be comparing the codes of an AE with 1 hidden layer of 2 neurons (yeah... that corresponds to a massive compression, over 150x) against the first 2 principal components of the Mercedes dataset many of you are already familiar with.\n\nIt won't take long, so bear with me for a while and upvote this notebook if you liked it. If you are interested in Autoencoders I strongly suggest you to have a look at [this Keras blog][1]. \n\n\n  [1]: https://blog.keras.io/building-autoencoders-in-keras.html"},{"outputs":[],"source":"# import base modules\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline","cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"bb7bfc89-f3f8-4ff7-b60e-aaec7b0fee90","_uuid":"ada647bbccdd5afc7c026999c420e3b290f40098","trusted":false}},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"4746f0bf4999e8d8e986dce7186246e5984a4737","trusted":false,"_cell_guid":"56af20f4-e184-43e9-b869-f2cf60c97be4"},"cell_type":"code","execution_count":null,"source":"# load training data\nprint('Load data...')\ndf_train = pd.read_csv('../input/train.csv')"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"7d0cc326bf65ceab0998e28fe74b0586ea678108","trusted":false,"_cell_guid":"8d19401b-1d95-419b-b771-cef013b961ef"},"cell_type":"code","execution_count":null,"source":"# let's keep just the boolean features\ntrain = df_train.iloc[:,10:]\n# ... and the y\ny = df_train.y"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e552d5681d092b00d62d19052ba2b902ce87d03c","trusted":false,"_cell_guid":"57261003-ceeb-4e13-aeef-8815648d6359"},"cell_type":"code","execution_count":null,"source":"# Let's quickly build an Autoencoder with 1 hidden layer and only 2 hidden neurons. \nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\nencoding_dim = 2\ninput_layer = Input(shape=(train.shape[1],))\nencoded = Dense(encoding_dim, activation='relu')(input_layer)\ndecoded = Dense(train.shape[1], activation='sigmoid')(encoded)\n\n# let's create and compile the autoencoder\nautoencoder = Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"fad30237b93b0a0c59bb23d017562af625e8ca08","trusted":false,"_cell_guid":"b8e2507d-a15a-42bb-a220-4d8c4241e7da"},"cell_type":"code","execution_count":null,"source":"# let's train the autoencoder, checking the progress on a validation dataset \nfrom sklearn.model_selection import train_test_split\nX1, X2, Y1, Y2 = train_test_split(train, train, test_size=0.2, random_state=42)\n\n# these parameters seems to work for the Mercedes dataset\nautoencoder.fit(X1.values, Y1.values,\n                epochs=300,\n                batch_size=200,\n                shuffle=False,\n                verbose = 2,\n                validation_data=(X2.values, Y2.values))"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"0dba21ec6a2643ea2c3b468ce21d7af3e980d6e3","trusted":false,"_cell_guid":"4c90c16e-c58d-4906-9519-1d7a0ad11482"},"cell_type":"code","execution_count":null,"source":"# now let's evaluate the coding of the initial features\nencoder = Model(input_layer, encoded)\npreds = encoder.predict(train.values)"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"d850b3ec10c079adb041b6d9c3b828ea3ce67644","trusted":false,"_cell_guid":"7429cfb4-a2d3-4810-8341-87359e433716"},"cell_type":"code","execution_count":null,"source":"#... and let's plot the two components of the compression on a scatter plot that also shows \n# the y value associated to each point. PCA decomposition is provided as well for comparison\nplt.figure(figsize = (17,5))\nplt.subplot(131)\nplt.scatter(preds[:,0],preds[:,1],  c = y, cmap = \"RdGy\", \n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.colorbar()\nplt.title('AE Scatter Plot')\n\n\n# ICA and PCA (first 2 components)\nfrom sklearn.decomposition import PCA, FastICA # Principal Component Analysis module\nica = FastICA(n_components=2)\nica_2d = ica.fit_transform(train.values)\n\nplt.subplot(132)\nplt.scatter(ica_2d[:,0],ica_2d[:,1],  c = y, cmap = \"RdGy\",\n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.colorbar()\nplt.title('ICA Scatter Plot')\n\npca = PCA(n_components=2)\npca_2d = pca.fit_transform(train.values)\n\nplt.subplot(133)\nplt.scatter(pca_2d[:,0],pca_2d[:,1],  c = y, cmap = \"RdGy\",\n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.colorbar()\nplt.title('PCA Scatter Plot')\n\nplt.show()"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"7371966fec5affa8e646da5fe85017341464155a","_cell_guid":"cd2bf21a-38f0-417f-9116-e1c59c26b77a"},"cell_type":"markdown","execution_count":null,"source":"### Can you see how the cluster with very low y is clearer when considering the AE compressed representation? In the same way, the AE representation helps identify at least another cluster with low-to-medium y."},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"96d73819c5cf208411abac5dc2cadd8efa38a352","_cell_guid":"f79ad1ee-86f6-4f5e-9bf2-2b689b7fe900"},"cell_type":"markdown","execution_count":null,"source":"### Btw, that cluster is easily easily identified by variable X232... but I think many of you already know this. If not, see below..."},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"a943f6ba0fb69b94b4d602b22bc6641406247df8","trusted":false,"_cell_guid":"4e64813a-af14-4f1b-ac00-f65412ade005"},"cell_type":"code","execution_count":null,"source":"var = df_train.X232\nplt.figure(figsize = (15,5))\nplt.subplot(131)\nplt.scatter(preds[:,0],preds[:,1],  c = y, cmap = \"RdGy\", \n            edgecolor = \"None\", alpha=1, vmin = 75, vmax = 150)\nplt.title('AE Scatter Plot')\n\nplt.subplot(132)\nplt.scatter(preds[:,0],preds[:,1],  c = var, cmap = \"jet\", \n            edgecolor = \"None\", alpha=1, vmin = 0, vmax = 1)\nplt.title('X232')\n\nplt.subplot(133)\nbins = np.linspace(75, 275, 51)\nplt.hist(y[var==0], bins, alpha=0.5, label='0', color = plt.cm.jet(0))\nplt.hist(y[var==1], bins, alpha=0.75, label='1', color = plt.cm.jet(255))\nplt.title('X232')\nplt.legend(loc='upper right')\n\nplt.show()"},{"outputs":[],"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"283538a11cfa0a08abe6a3dd164cab1473bc3051","trusted":false,"_cell_guid":"a98be400-3d26-4084-90ab-0c516ef27529"},"cell_type":"code","execution_count":null,"source":""}]}