{"nbformat_minor":0,"metadata":{"language_info":{"name":"python","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.6.1","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"cells":[{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"2004c4dea5def7d9161805e049b0d444ca91b450","_cell_guid":"705173e3-1dff-49fb-9d05-6a9e89171ddd"},"execution_count":null,"cell_type":"markdown","outputs":[],"source":"### Introduction\nThis is my first approach with stacking (using [StackingRegressor][1]). First I will try stacking with some basic models.\nI also made a little bit of feature engineering (removing some features, factorizing the categorical ones etc.)\n\n\n  [1]: https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/"},{"metadata":{"_execution_state":"idle","_cell_guid":"579bf829-2340-4197-9cff-25d4d8f6d123","_uuid":"e81e692b6d138d354ef4cec4c59e6256a4f924ae","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# import the libraries\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"18ac1986aec31fd438bdb7821c35c532a9587a43","_cell_guid":"eaa69719-b219-45a4-baa3-91de6fffd790","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# read the data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"1a46e27f8940dba1ae1a7486b8a281aa7deed13c","_cell_guid":"7a6dd05b-0385-40ea-b857-f8d9ed6eb2cf","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# store the target variable\ny_train_all = df_train.y\nid_test = df_test.ID\n\n# drop the target variable and the ids before the combination\ndf_train.drop(['ID', 'y'], axis=1, inplace=True)\ndf_test.drop(['ID'], axis=1, inplace=True)\n\n# Build df_all = (df_train+df_test)\nnum_train = len(df_train)\ndf_all = pd.concat([df_train, df_test])\nprint(df_all.shape)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"f1b63333a81e1586430f632ab52385e2d97b2386","_cell_guid":"9be77dcb-3cb2-43db-944b-c133836fba72"},"execution_count":null,"cell_type":"markdown","outputs":[],"source":"### Some feature engineering\nI tried adding some new features but none of them helped so far.  But removing some features did. What I removed:\n\n* features with all the same values (for example only ones)\n* I run an XGBoost model and eliminated the lowest scored features\n* I also implemented a naive script that removes each feature one by one and checks the r2_score of the LassoCV predictions on the reduced dataset"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"d7418b5a9ea31b827706d6a5ba5ee92d93d8aaf1","_cell_guid":"eccae0e9-7e4a-44ab-b62f-61f38cfea468","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# drop the 30 lowest XGB scored features\nlowest_scored_thirty = ['X344', 'X20','X117','X109','X378','X45','X362','X161','X164','X61',\n 'X65','X380','X154', 'X300','X77', 'X114', 'X85', 'X321', 'X195','X209', 'X206', 'X283', 'X343', 'X340', 'X376',\n 'X36', 'X375', 'X264', 'X250', 'X329']\ndf_all = df_all.drop(lowest_scored_thirty, axis=1)\ndf_all.shape"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"128d66558c1fe725821562a47f2ce1cfe7a80c97","_cell_guid":"6229bc8a-5ef5-4bfd-aa0c-771af70c4ed9","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# these are the features that were eliminated using LassoCV\nlasso_eliminated_features = ['X3', 'X0', 'X314', 'X350', 'X315', 'X180', 'X27', 'X261', \n                             'X220', 'X321', 'X355', 'X29', 'X136']\n\nto_eliminate = list(set(lasso_eliminated_features) - set(lowest_scored_thirty))\n\ndf_all = df_all.drop(to_eliminate, axis=1)\ndf_all.shape"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"09682a4e6dcc85efb88c61e8ce12690a408d61a5","_cell_guid":"99faedb4-878b-4149-8ef0-7fdef0ba5729","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# factorize the categorical values\ndf_numeric = df_all.select_dtypes(exclude=['object'])\ndf_obj = df_all.select_dtypes(include=['object']).copy()\nprint(df_obj.shape)\nprint(df_numeric.shape)\n\n# drop the numeric features where the column contains only one unique value\nfor col in df_numeric:\n    cardinality = len(np.unique(df_train[col]))\n    if cardinality == 1:\n        df_numeric = df_numeric.drop(col, axis=1)\n        \nfor col in df_obj:\n    df_obj[col] = pd.factorize(df_obj[col])[0]\n\ndf_values = pd.concat([df_numeric, df_obj], axis=1)\nprint(df_values.shape)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"09a9d8d8d0a9626635499df220dd607cf7f0d55e","_cell_guid":"87bc1402-5978-4b4e-8639-b93e929d4d74","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# now convert to numpy values\nX_all = df_values.values\nprint(X_all.shape)\n\n# the complete training set\nX_train_all = X_all[:num_train]\n\n# create the validation sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=4242)\n\nX_test = X_all[num_train:]\n\ndf_columns = df_values.columns"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"971573ca00332afd2c46e611f5b8d97b99d3bed9","_cell_guid":"9f3dc98c-9b65-42fa-a14c-770bc126d223"},"execution_count":null,"cell_type":"markdown","outputs":[],"source":"### Stacking\nI used a RandomForest, an ElasticNet and a Lasso model for stacking with an ExtraTrees model as the meta regressor. I'm currently working on tuning mode models (especially neural network models) but this combination gave the best result so far."},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"792708f344545cd4fc3f7aff7ad797c375d32c36","_cell_guid":"9a61ee9f-4eff-43bb-a621-e5535805499d","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nrandom_forest = RandomForestRegressor(n_estimators=20, oob_score=True, bootstrap=True, max_depth=3)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"98629815efb2804a831054dbd9d7d884f0de0acc","_cell_guid":"e2976e61-fe4c-4435-8ce6-c7a1a6a2b96b","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.linear_model import ElasticNetCV\nmodel_elastic = ElasticNetCV(l1_ratio=[.1, .4, .5, .6, .7, .8, .9, .95, .99, 1], cv=5)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"6c2722bf504d271747b273c8f8ab32bec164f939","_cell_guid":"4d4c06d0-e253-4f88-a867-a7a9ffaf88c5","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.linear_model import LassoCV\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0001, 0.0005], cv=5)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"646b923ab3c0a27f53cb508d4c4989e13918f728","_cell_guid":"92c7b284-b6a9-4f89-8d51-4bd4acb85e93","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# the meta regressor\nfrom sklearn.ensemble import ExtraTreesRegressor\ntree_model = ExtraTreesRegressor(n_estimators=20, oob_score=True, bootstrap=True, max_depth=5)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"d3f16b3148d2663e3c25b9eb4024dac31adcfacd","_cell_guid":"2fc43639-8142-4a28-9162-e263d0314633","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# this is the stacking part\nfrom mlxtend.regressor import StackingRegressor\nstregr = StackingRegressor(regressors=[random_forest, model_lasso, model_elastic], \n                           meta_regressor=tree_model)\n"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"607d4cbb8d18acf4a7ffca2cd3ffbc0e4933eb11","_cell_guid":"691b4c70-1f03-4cc1-af29-89f0b2ac65b4","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"# fit the stacked model\nstregr.fit(X_train, y_train)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"fb783a7727612d0df2107193272590b0c75da11a","_cell_guid":"31bfbd88-78e2-4139-a705-116f765b18f4","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"from sklearn.metrics import r2_score\n\n# check the r2 value of the stacked model predictions\npredict_val = stregr.predict(X_val)\n\nr2_score(y_val, predict_val)"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"8b577204dafbb985c98b310442e819d497f58705","_cell_guid":"d8869c7c-b906-4d4d-8423-c5ae863d08a9"},"execution_count":null,"cell_type":"markdown","outputs":[],"source":"### Conclusion\nWith this configuration I could improve my LB score to 0.55797 (which is not a very decent value but I just joined this competition a few days ago). I see a lot of room for improvement with more feature engineering and finer tuned models (I already have a better performing model in my local environment which is mostly based on this one)."},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"d75b2519f9fc6919b7b35ec6e6c6d01c0884f0d0","_cell_guid":"1d561980-8323-4a84-90c2-e558eb9c73bb","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":""}],"nbformat":4}