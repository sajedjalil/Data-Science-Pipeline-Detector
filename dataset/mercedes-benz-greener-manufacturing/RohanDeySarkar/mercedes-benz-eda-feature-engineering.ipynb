{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Objective:**\n\nThis dataset contains an anonymized set of variables that describe different Mercedes cars. The ground truth is labeled 'y' and represents the time (in seconds) that the car took to pass testing."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn import ensemble\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"../input/mercedes-benz-greener-manufacturing\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/train.csv.zip\")\ntest_df = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/test.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train Shape: {train_df.shape}')\nprint(f'Test Shape: {test_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"y\" is the variable we need to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_limit = 180\ntrain_df.loc[train_df['y'] > upper_limit, 'y'] = upper_limit\n\nplt.figure(figsize=(8, 6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(train_df.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.dtypes.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Column\", \"Column Type\"]\ndtype_df['Column Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\")[\"Count\"].count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So majority of the columns are integers with 8 categorical columns and 1 float column (target variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtype_df.loc[:10, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"X0 to X8 are the categorical columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum(axis=0).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count'] > 0]\nmissing_df = missing_df.sort_values(by=\"missing_count\")\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_values_dict = {}\n\nfor col in train_df.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        unique_value = str(np.sort(train_df[col].unique()).tolist())\n        if unique_value not in unique_values_dict:\n            unique_values_dict[unique_value] = [col]\n        else:\n            unique_values_dict[unique_value].append(col)\n\nfor unique_val in unique_values_dict:\n    print(\"Columns containing the unique values : \",unique_val)\n    print(unique_values_dict[unique_val])\n    print(\"--------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After Integer Columns Analysis we can see that all the integer columns are binary with some columns have only one unique value 0"},{"metadata":{},"cell_type":"markdown","source":"Now we have to visualize categorical cols against 'y' col"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['X1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_name = \"X1\"\n\nplt.figure(figsize=(12,6))\nsns.stripplot(x=var_name, y='y', data=train_df, order= train_df[var_name].value_counts().index)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_name = \"X2\"\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x=var_name, y='y', data=train_df, order= train_df[var_name].value_counts().index)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_name = \"X3\"\n\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order= train_df[var_name].value_counts().index)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now exploring Binary Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_count_list = []\nzero_count_list = []\n\ncols_list = unique_values_dict['[0, 1]']\n\n# Now to store total no. of 0's & 1's in each col\nfor col in cols_list:\n    zero_count_list.append((train_df[col] == 0).sum())\n    one_count_list.append((train_df[col] == 1).sum())\n\nN = len(cols_list)\nind = np.arange(N)\nwidth = 0.35\n\nplt.figure(figsize=(6,100))\np1 = plt.barh(ind, zero_count_list, width, color='red')\np2 = plt.barh(ind, one_count_list, width, left=zero_count_list, color=\"green\")\nplt.yticks(ind, cols_list)\nplt.legend((p1, p2), ('Zero count', 'One Count'))\nplt.title(\"Count Distribution\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking mean 'y' value for 0 & 1, for each binary cols"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_mean_list = []\none_mean_list = []\n\ncols_list = unique_values_dict['[0, 1]']\n\nfor col in cols_list:\n    zero_mean_list.append(train_df.loc[train_df[col] == 0, 'y'].mean())\n    one_mean_list.append(train_df.loc[train_df[col] == 1, 'y'].mean())\n\ntemp_df = pd.DataFrame({\"column_name\": cols_list + cols_list, \"value\": [0]*len(cols_list) + [1]*len(cols_list), \"y_mean\": zero_mean_list + one_mean_list})\ntemp_df = temp_df.pivot(index = 'column_name', columns = 'value', values = 'y_mean')\ntemp_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now to check if the 'y' mean values of 1s and 0's are almost same or diff wrt each col"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 80))\nsns.heatmap(temp_df, cmap=\"YlGnBu\")\nplt.title(\"Mean of y val across binary variables\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binary variables which shows a good color difference in the above graphs between 0 and 1 are likely to be more predictive given the the count distribution is also good between both the classes"},{"metadata":{},"cell_type":"markdown","source":"Now we will look into the 'ID' col which will give an idea of how the splits are done across train and test (random or id based) and also to help see if ID has some potential prediction capability (probably not so useful for business)"},{"metadata":{"trusted":true},"cell_type":"code","source":"var_name = \"ID\"\n\nplt.figure(figsize=(12,6))\nsns.regplot(x = var_name, y = 'y', data = train_df, scatter_kws = {'alpha':0.5, 's':30})\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The regplot performs a simple linear regression and there seems to be a slight decreasing trend with respect to ID variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['eval_set'] = \"train\"\ntest_df['eval_set'] = \"test\"\ntrain_df['eval_set'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df = pd.concat([train_df[[\"ID\", \"eval_set\"]], test_df[[\"ID\", \"eval_set\"]]])\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Let's see how the IDs are distributed across train and test."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.stripplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('ID', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('ID', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us run and xgboost model to get the important variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \ntrain_y = train_df['y'].values\ntrain_X = train_df.drop([\"ID\", \"y\", \"eval_set\"], axis=1)\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.getLabel()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100, feval=xgb_r2_score, maximize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical occupy the top spots followed by binary variables.\n\nLet us also build a Random Forest model and check the important variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_names = train_X.columns.values\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binary is now on the top spot. \nSo based on diff model there is a significant difference in important features"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}