{"nbformat_minor":0,"cells":[{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"1baf944f-0a93-4f4a-babb-9c58cd4e78c9","collapsed":false,"_uuid":"b90dae453f1bc0d53ff0080c51a3a4fbbf712b2a"},"execution_count":null,"source":"the Claim of the competition is we have to 'shorten the test time'  Y-pred !\n----\n\nforked john, since he has made wonderful graphs\nnow the mirakle is when you make a kind of nearly perfect prediction, you get a score LB 0.48\n----\n\nRemark the current solutions don't minimize the time for mercedes..\nTo minimize the time we have to do other math then regressions","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"83158ee7-602f-4b5c-89f8-4f9438d06dfc","collapsed":false,"_uuid":"69f55f2a1d3e594834856d997e5d9049d26df7ff"},"execution_count":null,"source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport xgboost as xgb \nfrom sklearn.metrics import r2_score\n\n%matplotlib inline\n\nfrom IPython.display import display, HTML\n# Shows all columns of a dataframe\ndef show_dataframe(X, rows = 2):\n    display(HTML(X.to_html(max_rows=rows)))","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"d599a9d3-d2cb-4dc1-b9ca-e2cb89abd63d","collapsed":false,"_uuid":"001bd35fb90497e6c7aeb374920f9cf8b6ca50a3"},"execution_count":null,"source":"# Datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"54cfa8dc-4dd7-4bcc-aef6-9ef0d3d3462d","collapsed":false,"_uuid":"520e308bdc6a7eb8a42ecef23b7d1c364a7ecc02"},"execution_count":null,"source":"The combined X0 - X5 combination labellizing improves forecast\n----\nevidently, the more parameters the better the regression fit, thats no miracle\n","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"d29b4cd8-5339-4239-987d-9d34c5f822b3","collapsed":false,"_uuid":"ace0a1497c1adc835c9ef8a00f04a24578309198"},"execution_count":null,"source":"#add X0-8 combinations\nfor xi in ['X1','X2','X3','X4','X5','X6','X8']:\n    nieuwveld='X0'+xi\n    train[nieuwveld]=train['X0']+'-'+train[xi]\n    test[nieuwveld]=test['X0']+'-'+test[xi]\n\n# Categorical features\ncat_cols = []\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        cat_cols.append(c)\nprint('Categorical columns:', cat_cols)\n\n# Dublicate features\nd = {}; done = []\ncols = train.columns.values\nfor c in cols: d[c]=[]\nfor i in range(len(cols)):\n    if i not in done:\n        for j in range(i+1, len(cols)):\n            if all(train[cols[i]] == train[cols[j]]):\n                done.append(j)\n                d[cols[i]].append(cols[j])\ndub_cols = []\nfor k in d.keys():\n    if len(d[k]) > 0: \n        # print k, d[k]\n        dub_cols += d[k]        \nprint('Dublicates:', dub_cols)\n\n# Constant columns\nconst_cols = []\nfor c in cols:\n    if len(train[c].unique()) == 1:\n        const_cols.append(c)\nprint('Constant cols:', const_cols)","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"f698f747-e1c2-46aa-b5c0-4dce42c01af0","collapsed":false,"_uuid":"48db05580ee3e100ac8065d1f1272ffcbbd80f81"},"execution_count":null,"source":"Figures below show categorical features (on the left) sorted by means of **y**'s grouped by labels. On the right there are corresponding **mean**'s, **std**'s (filled blue), **max**'s (green line) and **min**'s (red line).","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"d7f20244-dc3d-475c-822f-3f56920188fc","collapsed":false,"_uuid":"ca969e8c37ed6d21c2806bd21e2cb72c8715bdb2"},"execution_count":null,"source":"","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"134404c2-d1ea-4d39-ba10-5ae6326a3ced","collapsed":false,"_uuid":"87732c13a13692c926bc445460cad7dbae024b7f"},"execution_count":null,"source":"# Glue train + test\ntrain['eval_set'] = 0; test['eval_set'] = 1\ndf = pd.concat([train, test], axis=0, copy=True)\n# Reset index\ndf.reset_index(drop=True, inplace=True)","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"09f31dc8-a5d9-43e6-a661-2fd2cc9db634","collapsed":false,"_uuid":"203b4e78e6b03e589ea7d4b143a8266a9e4adb68"},"execution_count":null,"source":"### Categorical feature encoding\nIn the next cell for every categorical column from **cat_cols** we'll find **mean** of **y's** for every label using **.groupby()**. Then we sort labels by values of **means**. Now, when labels are sorted, they can be encoded by numbers from *0* to *numbers of labels - 1*.\n\nI use here a quantile ranking\n----","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"5428c9c8-8a5c-462d-a19e-43f0fd5119d2","collapsed":false,"_uuid":"6b0320bf70db6a0d581943410b3fa57eb6029cc0"},"execution_count":null,"source":"def add_new_col(x):\n    if x not in new_col.keys(): \n        # set n/2 x if is contained in test, but not in train \n        # (n is the number of unique labels in train)\n        # or an alternative could be -100 (something out of range [0; n-1]\n        return int(len(new_col.keys())/2)\n    return new_col[x] # rank of the label\n\nfor c in cat_cols:\n    # get labels and corresponding means\n    new_col = train.groupby(c).y.quantile(q=0.5).sort_values().reset_index()\n    # make a dictionary, where key is a label and value is the rank of that label\n    new_col = new_col.reset_index().set_index(c).drop('y', axis=1)['index'].to_dict()\n    # add new column to the dataframe\n    df[c + '_new'] = df[c].apply(add_new_col)\n\n# drop old categorical columns\ndf_new = df.drop(cat_cols, axis=1)\n\n# show the result\nshow_dataframe(df_new, 5)","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","_cell_guid":"d6c69d70-e78d-4c79-9fa8-bb3715c31bad","collapsed":false,"_uuid":"9e57c7ce586a95f0702914a379fd4e2fc9ae152f"},"execution_count":null,"source":"### Train-test split","cell_type":"markdown"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"424034b6-9710-4c9d-b0a3-c062a19efbe7","collapsed":false,"_uuid":"822c0ef3a44266e882ed62fb58f251ad1a37c3d3"},"execution_count":null,"source":"from scipy.stats import signaltonoise\nX = df.drop(list((set(const_cols) | set(dub_cols) | set(cat_cols))), axis=1)\n\n# Train\nX_train = X[X.eval_set == 0]\ny_train = X_train.pop('y'); \nX_train = X_train.drop(['eval_set', 'ID'], axis=1)\n\n# Test\nX_test = X[X.eval_set == 1]\nX_test = X_test.drop(['y', 'eval_set', 'ID'], axis=1)\n\n# Base score\n#y_mean = y_train.sum() #q0.01 LB 0.5546 q=0.25 0.5549  #permut categ LB 0.51\ny_mean = 1/signaltonoise(train['y'])\n# Shapes\n\nprint('Shape X_train: {}\\nShape X_test: {}'.format(X_train.shape, X_test.shape))","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"3e99cb6e-fe17-4431-aa88-faccc5c49741","collapsed":false,"_uuid":"47b3a6bda8cc705adb41d599cdee125b79ed6063"},"execution_count":null,"source":"from sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nkolom=X_train.columns\nkolom=[w for w in kolom if w not in ['y']]\n\nn_comp = 50\n\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(X_train[kolom])\npca2_results_test = pca.transform(X_test[kolom])\n\n\n#save columns list before adding the decomposition components\n\nusable_columns = list(set(train.columns) - set(['y']))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    X_train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    X_test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"0a919200-4dd4-4418-8640-32b9c48082a8","collapsed":false,"_uuid":"ab64bea8868f097869bcb44fe8b95cd823ce9b51"},"execution_count":null,"source":"### Regressor\n\n# prepare dict of params for xgboost to run with\nxgb_params = {\n    'n_trees': 100, \n    'eta': 0.005,\n    'max_depth': 3,\n    'subsample': 0.95,\n    'colsample_bytree': 0.6,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean,\n    'silent': 1\n}\n\n# form DMatrices for Xgboost training\ndtrain = xgb.DMatrix(X_train, np.log(y_train))\ndtest = xgb.DMatrix(X_test)\n\n# evaluation metric\ndef the_metric(y_pred, y):\n    y_true = y.get_label()\n    return 'r2', r2_score(y_true, y_pred)\n# evaluation metric\ndef euclid(y_pred,y):\n    y_true = y.get_label()\n    xa=y_pred\n    ya=y_true\n    eu0=np.sqrt(sum(pow(a-b,2) for a, b in zip(xa, ya)))\n    xb=y_pred\n    yb=y_true\n    eu1=np.sqrt(sum(pow(a-b,2) for a, b in zip(xb, yb)))\n    return 'r2',eu0*eu1\n\n# xgboost, cross-validation\ncv_result = xgb.cv(xgb_params, \n                   dtrain, \n                   num_boost_round=3000, # increase to have better results (~700)\n                   nfold = 3,\n                   early_stopping_rounds=50,\n                   #feval=the_metric,\n                   feval=euclid,\n                   verbose_eval=100, \n                   show_stdv=False\n                  )\n\nnum_boost_rounds = len(cv_result)\nprint('num_boost_rounds=' + str(num_boost_rounds))\n\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n\n# Predict on trian and test\ny_train_pred = np.exp(model.predict(dtrain))\ny_pred = np.exp(model.predict(dtest))\n\nprint('First 5 predicted test values:', y_pred[:5])\n\n","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"c0c07f56-c213-42b3-b77b-649ca9271f4c","_uuid":"6059bab5e30666c22897fd21df78a8cfa9a6806e","collapsed":false},"execution_count":null,"source":"plt.figure(figsize=(16,4))\n\nplt.subplot(1,4,1)\n#train_scores = cv_result['train-rmse']\n#train_stds = cv_result['train-r2-std']\n#plt.plot(train_scores, color='green')\n#plt.fill_between(range(len(cv_result)), train_scores - train_stds, train_scores + train_stds, alpha=0.1, color='green')\n#test_scores = cv_result['train-rmse']\n#test_stds = cv_result['test-r2-std']\n#plt.plot(test_scores, color='red')\n#plt.fill_between(range(len(cv_result)), test_scores - test_stds, test_scores + test_stds, alpha=0.1, color='red')\nplt.title('Train and test cv scores (R2)')\n\nplt.subplot(1,4,2)\nplt.title('True vs. Pred. train')\nplt.plot([80,265], [80,265], color='g', alpha=0.3)\nplt.scatter(x=y_train, y=y_train_pred, marker='.', alpha=0.5)\nplt.scatter(x=[np.mean(y_train)], y=[np.mean(y_train_pred)], marker='o', color='red')\nplt.xlabel('Real train'); plt.ylabel('Pred. train')\n\nplt.subplot(1,4,3)\nsns.distplot(y_train, kde=False, color='g')\nsns.distplot(y_train_pred, kde=False, color='r')\nplt.title('Distr. of train and pred. train')\n\nplt.subplot(1,4,4)\nsns.distplot(y_train, kde=False, color='g')\nsns.distplot(y_pred, kde=False, color='b')\nplt.title('Distr. of train and pred. test')\n\n\n\nplt.figure(figsize=(18,1))\nplt.plot(y_train_pred[:200], color='r', linewidth=0.7)\nplt.plot(y_train[:200], color='g', linewidth=0.7)\nplt.title('First 200 true and pred. trains')\n\nprint('Mean error =', np.mean(y_train - y_train_pred))\nprint('Train r2 =', r2_score(y_train, y_train_pred))","cell_type":"code"},{"outputs":[],"metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"124bb6ee-474e-448e-a1cc-9f3aa2f2a1bd","collapsed":false,"_uuid":"92da3e03044f7daebfd36ed9b52b9c828a77ba54"},"execution_count":null,"source":"output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\noutput.to_csv('subm.csv', index=False)\nfig = plt.figure(figsize=(15,10))\nsns.jointplot(y_train, y_train_pred, kind=\"reg\")","cell_type":"code"}],"metadata":{"language_info":{"name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4}