{"nbformat_minor":0,"nbformat":4,"cells":[{"source":"36 cars per hour are tested per working person.\n----\n175.375 cars per hour are tested in the sample \n----\n\n\nThe train data is stratified sampled has been proven somewhere (correcting the spell errors;-)\nHence you can simply make a 36 car sum  time and create 120 groups\nOr we create a 175.375 24 grouping ...\n\nWhat we see is the difference IMHO of the morning team - day team  -  the night team ?\n\nGlimp on the website i count\n---\n32 models\n5 aandrijvingen : diesel/benzine/aardgas/elektrisch/hybride\n25 motoren : 160 180 200 220 220M 250 250M 250-4M 300 400 AMG43 AMG63 AMG63S 350 180d 200d 220d 250d 350d 350d-4M 300h 350e 400-4M 500-4M 560S-4M\n6 versnelling: 6 7 9 AMG AMGplus AMG-mct\n\nthere could be more, or less, since i know some models are not made in Germany but in US or Austrich, \n\nIf you look at the photo\n---\nyou see a speed/kwh or pk Test. Since they talk about cutting the emission, we talk about testing the kWh power test of a car on the testbank ?\n\nDon't vote for me, i like to stay below the radar\n---\n\n","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"91d316b23e9f01094d4afe3de7f98b1207d9fea4","_execution_state":"idle","collapsed":false}},{"source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#read data\ntrain = pd.read_csv('../input/train.csv')\nprint(len(train))\ntrain['counter']=0\ntrain['group']=0\ntel=0\nuur=0\nfor xi in range(len(train)):\n    tel+=1\n    train.ix[xi,'counter']=tel\n    train.ix[xi,'group']=uur\n    if xi/175.375-uur>1:\n        tel=0\n        uur+=1\n\ngroup_sum=pd.pivot_table(train, values='y', index='group', columns='counter', aggfunc='sum') \nX5_sum=pd.pivot_table(train, values='y', index='X5', columns='counter', aggfunc='sum') \nX0_sum=pd.pivot_table(train, values='y', index='X0', columns='counter', aggfunc='sum')  \nX0_count=pd.pivot_table(train, values='y', index='X0', columns='counter', aggfunc='count')  \nX1_sum=pd.pivot_table(train, values='y', index='X1', columns='counter', aggfunc='sum')  \nX1_count=pd.pivot_table(train, values='y', index='X1', columns='counter', aggfunc='count')  \nX2_sum=pd.pivot_table(train, values='y', index='X2', columns='counter', aggfunc='sum')  \nX2_count=pd.pivot_table(train, values='y', index='X2', columns='counter', aggfunc='count')  \nX3_sum=pd.pivot_table(train, values='y', index='X3', columns='counter', aggfunc='sum')  \nX3_count=pd.pivot_table(train, values='y', index='X3', columns='counter', aggfunc='count')  \nX4_sum=pd.pivot_table(train, values='y', index='X4', columns='counter', aggfunc='sum')  \nX4_count=pd.pivot_table(train, values='y', index='X4', columns='counter', aggfunc='count')  \nX6_sum=pd.pivot_table(train, values='y', index='X6', columns='counter', aggfunc='sum')  \nX6_count=pd.pivot_table(train, values='y', index='X6', columns='counter', aggfunc='count')  \nX8_sum=pd.pivot_table(train, values='y', index='X8', columns='counter', aggfunc='sum')  \nX8_count=pd.pivot_table(train, values='y', index='X8', columns='counter', aggfunc='count')  \n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"fd619c03eab87a22a9dc7f33a2972c6348cac2bc","_execution_state":"idle","trusted":false}},{"source":"import matplotlib.pyplot as plt\ngcs=pd.DataFrame( group_sum.sum(axis=1) )\ngcs.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs.mean())\n# time per 36 cars... approx 3600seconds or 1 hour... and here you see a nice trend..","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"25b927cc7b7e64f8f6ede2636c9a61e47cd826b3","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":"Another look\n-----\nSuppose X5 is one person\nThen this analysis shows the total time people are testing per shift...if the shift is 8hours, one is testing 6hours, he has 2 hours between two tests to pick a car and do a test.","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"b058798f676d3daba1caa524e71356aff61ef11f","_execution_state":"idle","collapsed":false}},{"source":"print(len(X5_sum))\ngcs=pd.DataFrame( X5_sum.sum(axis=1) )\ngcs.plot()\nplt.show()\nprint('hours/X5',gcs/3600)\nprint(gcs/gcs.mean())\n# time per 33type of X5... max 23000seconds /3600 second/hour => 6.4 hours worked per X5 group suppose this is a person;..\n# unexplainable there are people doing only 1 test... or they are doing a reengineering-reparation ?\n# its also the only category that follows the ID... so probably this is a working shift of one person\n\n# another theorie: since we have 33models, suppose there are 29 models made and balanced over the production.. Usually the new models are peaking so it would be bizar this is topping this way.\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"0b33fc4e8be0ca1cbccf39db9331c2c2aa233901","_execution_state":"idle","collapsed":false}},{"source":"print(len(X0_sum))\ngcs=pd.DataFrame( X0_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X0_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\ntimepertypeofcar.plot()\nminutespertypeofcar2=(gcs/60)\nminutespertypeofcar2.plot()\nplt.show()\nprint('hours/X0',gcs/3600)\nprint(gcs/gcs2)\n# time per X0 category... suppose this is a type of car... there is only one type of car taking 150seconds on average, the rest is 110seconds ore 90seconds\n#this collides with the peaks we have in the forecast. \n#But there are only 33 type of cars and nothing has 47 types...  \n#except if you count every engine  - model combination as a car. So probably this collides with the models\n# the peaky behaviour of the number of time spend on the different models could collid with this.\n# the dominant behaviour of XO in the stats could explain equally this relation with the models","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"83f1300252b8431bba2807d6955374f7ce279d25","_execution_state":"idle","collapsed":false}},{"source":"print(len(X1_sum))\ngcs=pd.DataFrame( X1_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X1_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\nminutspertypeofcar=(gcs/60)\ntimepertypeofcar.plot()\nminutspertypeofcar.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs2)\n# time per X1 category... average time per type of category X1\n# this could be very well be the models... and if so this should 'correlate with X0\n# or this are the 'engines'","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"c844866b0d3a8649c85a0b2fb337ea1a055485a2","_execution_state":"idle","collapsed":false}},{"source":"print(len(X2_sum))\ngcs=pd.DataFrame( X2_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X2_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\ntimepertypeofcar.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs2)\n# time per X2 category... average time per type of category X2","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"a449b611f185c9393f82924ad7f586a9a74b98f8","_execution_state":"idle","collapsed":false}},{"source":"print(len(X3_sum))\ngcs=pd.DataFrame( X3_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X3_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\nminutespertypeofcar=(gcs/60)\ntimepertypeofcar.plot()\nminutespertypeofcar.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs2)\n# time per X3 category... average time per type of category X3\n# 7 types of shift gears /4matic combination ? an AMG should gear faster then a manual gear...","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"ae45f516fe91592ffdbf5deb7340fd8bca2c9ea9","_execution_state":"idle","collapsed":false}},{"source":"print(len(X4_sum))\ngcs=pd.DataFrame( X4_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X4_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\nminutespertypeofcar=(gcs/60)\ntimepertypeofcar.plot()\nminutespertypeofcar.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs2)\n# this is an exceptional category, 4 cars in abc, and the rest is d... i don't know what this is, but its not important","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"d2410b379ed32a7e99e1150244a96d6937b5c4dc","_execution_state":"idle","collapsed":false}},{"source":"print(len(X6_sum))\ngcs=pd.DataFrame( X6_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X6_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\nminutespertypeofcar=(gcs/60)\ntimepertypeofcar.plot()\nminutespertypeofcar.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs2)\n# time per X6 category... average time per type of category X6\n# what has 12 types and 3 types running good ? engine cores ?","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"2b1ad42bb1b0b4494bc065382b3934b66ca636f7","_execution_state":"idle","collapsed":false}},{"source":"print(len(X8_sum))\ngcs=pd.DataFrame( X8_sum.sum(axis=1) )\ngcs2=pd.DataFrame( X8_count.sum(axis=1) )\ntimepertypeofcar=(gcs/gcs2)\nminutespertypeofcar=(gcs/60)\ntimepertypeofcar.plot()\nminutespertypeofcar.plot()\nplt.show()\nprint(gcs)\nprint(gcs/gcs2)\n# time per X8category... average time per type of category X8\n# 25 types, but very bisar balanced and swiping between 400 - 200 minutes.","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"3c3aaab1d0211d09851884aeb937617f3aa242b8","_execution_state":"idle","collapsed":false}},{"source":"Arima forecast.\n----\nfirst search best model\nsecond plot arima forecast","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"8a7471e5f69987b07a5ed45f440c2e249ff2406c","_execution_state":"idle","collapsed":false}},{"source":"import statsmodels.api as sm\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nimport datetime\nfrom scipy.stats import norm\nimport statsmodels.api as sm\n\ngcs.columns=['cars']\ngcs['time']=datetime.datetime.now()\nday_time=datetime.datetime.now()\n\nfor xi in range(len(gcs)):\n    seconde=gcs.ix[xi,'cars']\n    #print(seconde)\n    day_time=day_time + datetime.timedelta(hours=0, minutes=0, seconds=seconde)\n    gcs.ix[xi,'time']=day_time\n\nprint(gcs.head())\n\ny__ = pd.Series(gcs['cars'].values, index=gcs.time)\nif len(y__)>0:\n    dta_full = y__\n    aic_full = pd.DataFrame(np.zeros((6,6), dtype=float))\n    warnings.simplefilter('ignore')\n\n    # Iterate over all ARMA(p,q) models with p,q in [0,6]\n    for p in range(6):\n        for q in range(6):\n            if p == 0 and q == 0:\n                continue\n            \n            # Estimate the model with no missing datapoints\n            mod = sm.tsa.statespace.SARIMAX(dta_full, order=(p,0,q), enforce_invertibility=False)\n            try:\n                res = mod.fit(disp=False)\n                aic_full.iloc[p,q] = res.aic\n            except:\n                aic_full.iloc[p,q] = np.nan\n        \n    print(aic_full)\n    mod = sm.tsa.statespace.SARIMAX(dta_full, order=(1,0,1))\n    res = mod.fit(disp=False)\n    print(res.summary())","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"9a6ad5d8de25086dc1ef466cbdffe117b2e99b6f","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":">                 0           1           2           3           4           5\n>     0    0.000000  525.967009  524.787082  468.327770  541.305056  513.702812\n>     1  338.545794  335.952972  337.941255  339.911934         NaN  343.891949\n>     2  336.841863  337.938235  339.952946         NaN         NaN         NaN\n>     3  338.122461  340.799413  341.933967         NaN  343.574326         NaN\n>     4  340.084871  342.097760  343.883114         NaN         NaN         NaN\n>     5  341.917544  344.082478  343.194213         NaN         NaN         NaN\n>                                Statespace Model Results                           \n>     ==============================================================================\n>     Dep. Variable:                      y   No. Observations:                   24\n>     Model:               SARIMAX(2, 0, 0)   Log Likelihood                -165.421\n>     Date:                Mon, 19 Jun 2017   AIC                            336.842\n>     Time:                        09:58:03   BIC                            340.376\n>     Sample:                    06-19-2017   HQIC                           337.779\n>                              - 06-20-2017                                         \n>     Covariance Type:                  opg                                         \n>     ==============================================================================\n>                      coef    std err          z      P>|z|      [0.025      0.975]\n>     ------------------------------------------------------------------------------\n>     ar.L1          0.6187      0.226      2.739      0.006       0.176       1.061\n>     ar.L2          0.3812      0.225      1.691      0.091      -0.061       0.823\n>     sigma2      3.882e+04   1.35e+04      2.867      0.004    1.23e+04    6.54e+04\n>     ===================================================================================\n>     Ljung-Box (Q):                       13.47   Jarque-Bera (JB):                 2.57\n>     Prob(Q):                              0.94   Prob(JB):                         0.28\n>     Heteroskedasticity (H):               0.58   Skew:                             0.80\n>     Prob(H) (two-sided):                  0.45   Kurtosis:                         3.05\n>     ===================================================================================\n>     \n>     Warnings:\n>     [1] Covariance matrix calculated using the outer product of gradients (complex-step).\n>     In [148]:\n>     \n>     res.predict(start=gcs.ix[0,'time'])\n>     ​","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"eebac9eee935e7e40872f813dfb67462c883847f","_execution_state":"idle","collapsed":false}},{"source":"Lets apply what we learned here...\n----","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"f5f30b1b6b08ec581bc46c6f2b988edaea1522d0","_execution_state":"idle","collapsed":false}},{"source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntrain = pd.read_csv('../input/train.csv')\ntest  = pd.read_csv('../input/test.csv')\n#create ID series\ntrain_test = train.append(test)\ny_m=pd.DataFrame(train_test[['y','ID']])\n\n        \n#labelencode        \nfor c in train_test.columns:\n    if train_test[c].dtype == 'object':\n        tempt = train_test[['y',c]]\n        temp=tempt.groupby(c).median().sort('y')\n        templ=temp.index\n        print(templ)\n        aant=len(templ)\n        train_test[c].replace(to_replace=templ, value=[x for x in range(0,aant)], inplace=True, method='pad', axis=1)\n        \ntrain_test_ma = pd.rolling_mean(train_test,window=175)\nprint(train_test_ma[174:])\nprint(train['y'])","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"b34844d2ac21861e073a798a121727cd333d0476","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":"ID does not have any predictive power anymore... right\n----","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"5ec714856e0fd77bf4548a5b1b75cb917e0778b8","_execution_state":"idle","collapsed":false}},{"source":"import statsmodels.formula.api as sm\nres = sm.ols(formula=\"y ~ ID +X0+X1+X2+X3+X4+X5+X6+X8 +X47\", data=train_test_ma[174:]).fit()\nprint(res.summary())\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"93d0662ec04a6c2fc8bef8dcbc2ec4899a9750c3","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":"import statsmodels.formula.api as sm\nres = sm.ols(formula=\"y ~ X0+X3+X5\", data=train_test_ma[174:]).fit()\nprint(res.summary())\n\nprint('Predicted values: ', res.predict())\nbeta=res.params\n\nbeta_=pd.DataFrame(beta)\nbeta_T=pd.DataFrame(beta.T)\nprint(beta_T)\n\n# forecast trainingdata\nX_=train_test[train_test['y']>0 ]\nX_['Intercept']=1.0\nX_r=X_[['Intercept','X0','X3','X5']]\nprint(X_r.shape)\nprint(beta_T.shape)\ntrain['y_pred']=X_r.dot(beta_T)\n#print(train[['y','y_pred']])\n# look at the scatterplot, still some unforecasted trouble but nicer already\nplt.scatter(train['y'],train['y_pred'])\nplt.show()\n\ntest  = pd.read_csv('../input/test.csv')\nXt_=train_test[train_test['y'].isnull() ]\nXt_['Intercept']=1.0\nXt_r=Xt_[['Intercept','X0','X3','X5']]\nprint(Xt_r.head())\ntest['y_pred']=Xt_r.dot(beta_T)\nprediction=test[['y_pred','ID']]\nprediction.to_csv('submission_Arima_PL_parameter.csv', index=False)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"3bcc558ce462174ebbb61b45ac24a2c2472a6333","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":"import statsmodels.formula.api as sm\nres = sm.ols(formula=\"y ~ X0+X1+X2+X3+X4+X5+X6+X8 +X47\", data=train_test_ma[174:]).fit()\nprint(res.summary())\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"defc31ecaae638e12129d805c07fee1b89c1b9ce","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":"Now we have a real correlation of 93%\n----\n\nthe test regression gets LB 0.15, now evidently i am forecasting more the 'moving average' then the real number\nits possible i have to shift '175/2 position' to get the right prediction, have to think about it ","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"a3528fbcfe395de056cbf9c2a78213c285d0f1fc","_execution_state":"idle","collapsed":false}},{"source":"import statsmodels.formula.api as sm\nres = sm.ols(formula=\"y ~ ID+X0 +X1 +X5 +X6 + X8 +X47 +X2 +X3 +X77+X105 +X345 +X3 +X142 +X26 +X322+X46+X267+X151+X240+X342+X287+X152+X140+X65+X95+X70+X116+X265+X354+X177+X362+X52+X383+X273+X58+X157+X156+X64+X131+X355+X173+X73+X31+X338+X225+X271+X230+X71+X174+X27+X163+X141+X327+X127+X51+X292\", data=train_test_ma[174:]).fit()\nprint(res.summary())\n\nprint('Predicted values: ', res.predict())\nbeta=res.params\n\nbeta_=pd.DataFrame(beta)\nbeta_T=pd.DataFrame(beta.T)\nprint(beta_T)\n\n# forecast trainingdata\nX_=train_test[train_test['y']>0 ]\nX_['Intercept']=1.0\nX_r=X_[['Intercept','ID','X0','X1','X5','X6','X8','X47','X2','X3','X77','X105','X345','X142','X26','X322','X46','X267','X151','X240','X342','X287','X152','X140','X65','X95','X70','X116','X265','X354','X177','X362','X52','X383','X273','X58','X157','X156','X64','X131','X355','X173','X73','X31','X338','X225','X271','X230','X71','X174','X27','X163','X141','X327','X127','X51','X292']]\nprint(X_r.shape)\nprint(beta_T.shape)\ntrain['y_pred']=X_r.dot(beta_T)\n#print(train[['y','y_pred']])\n# look at the scatterplot, still some unforecasted trouble but nicer already\nplt.scatter(train['y'],train['y_pred'])\nplt.show()\n\ntest  = pd.read_csv('../input/test.csv')\nXt_=train_test[train_test['y'].isnull() ]\nXt_['Intercept']=1.0\nXt_r=Xt_[['Intercept','ID','X0','X1','X5','X6','X8','X47','X2','X3','X77','X105','X345','X142','X26','X322','X46','X267','X151','X240','X342','X287','X152','X140','X65','X95','X70','X116','X265','X354','X177','X362','X52','X383','X273','X58','X157','X156','X64','X131','X355','X173','X73','X31','X338','X225','X271','X230','X71','X174','X27','X163','X141','X327','X127','X51','X292']]\nprint(Xt_r.head())\ntest['y_pred']=Xt_r.dot(beta_T)\nprediction=test[['y_pred','ID']]\nprediction.to_csv('submission_Arima_PL.csv', index=False)\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"ca1aec0d6f6dc054c0649cfeb634c2b9bfdd1fd9","_execution_state":"idle","trusted":false,"collapsed":false}},{"source":"Now we have a real correlation of 98%\n----\nLB -0.25 fantastic hé. The better we forecast the real time they have to spend on the mercedes test, the lower the ranking here ...","cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_uuid":"8a1219f1e665befe9d789c7a497b463a028a53da","_execution_state":"idle","collapsed":false}},{"source":"kolom=train_test.columns\nkolom=[k for k in kolom if k not in ['y']]\nformul=\"y ~ 1\"\nfor xi in kolom:\n    formul+=\"+\"+xi\n    \nprint(formul)\nres = sm.ols(formula=formul, data=train_test_ma[174:]).fit()\nprint(res.summary())\n\nbeta=res.params\n\nbeta_=pd.DataFrame(beta)\nbeta_T=pd.DataFrame(beta.T)\nprint(beta_T)\n\n# forecast trainingdata\nX_=train_test[train_test['y']>0 ]\nX_['Intercept']=1.0\nX_r=X_[['Intercept']+kolom]\nprint(X_r.shape)\nprint(beta_T.shape)\ntrain['y_pred']=X_r.dot(beta_T)\n#print(train[['y','y_pred']])\n# look at the scatterplot, still some unforecasted trouble but nicer already\nplt.scatter(train['y'],train['y_pred'])\nplt.show()\n\nXt_=train_test[train_test['y'].isnull() ]\nXt_['Intercept']=1.0\nXt_r=Xt_[['Intercept']+kolom]\nprint(Xt_r.head())\ntest['y_pred']=Xt_r.dot(beta_T)\nprediction=test[['y_pred','ID']]\nprediction.to_csv('submission_Arima_PL2.csv', index=False)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_uuid":"0763c05ab2c23f8da56ef560039661654fe93649","_execution_state":"idle","collapsed":false}}],"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}