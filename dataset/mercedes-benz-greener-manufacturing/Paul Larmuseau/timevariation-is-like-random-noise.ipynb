{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","file_extension":".py","version":"3.6.0","nbconvert_exporter":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python"}},"cells":[{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"895ed980d6f66c41a32ed833aa484f56e299bbb2"},"source":"The time variation is random noise...\n----\n\nsubstracting the mean, plotting the histogram you see a slight kurtosis\nthis means: its not predictable...\n\nhttp://machinelearningmastery.com/white-noise-time-series-python/\n\n","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"b688fc0d561e76396cab65b085f432566ff4047a"},"source":"What system behaves identical ?\n---\nthe stockmarket: am I a genius at work ? probably i learned some math most datamasters here didn't learn...","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"4678c7ef3456233947276beb9e3fba32bba6ed8c"},"source":"Nobody has questions ? i presume\n----","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"08f2f8728f8df5d5206790b4ef6f0b12a91e3027"},"source":"","cell_type":"code","execution_count":1},{"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7e787ec1ebfd125df945055cfe30cf6db2e08286"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom random import gauss\nfrom random import seed\nfrom pandas import Series\nfrom pandas.tools.plotting import autocorrelation_plot\nfrom matplotlib import pyplot\n# seed random number generator\n# create white noise series\ntrain = pd.read_csv('../input/train.csv')\n\nseries = train['y']-train['y'].mean()\nseries = Series(series)\n# summary stats\nprint(series.describe())\n# line plot\nseries.plot()\npyplot.show()\n# histogram plot\nseries.hist()\npyplot.show()\n# autocorrelation\nautocorrelation_plot(series)\npyplot.show()","cell_type":"code","execution_count":2},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"49feab7dcab5d342998effcc973117225f8bed77"},"source":"#Stock market behaves identical..\n#-----\ntrain = pd.read_csv('../input/train.csv')\ny = np.array(train['y'])\ntest = pd.read_csv('../input/test.csv')\n\n# Analyze all features ; modify categorical features\n\ntrain['y_mean'] = train.copy().groupby('X0')['y'].transform('mean')\ntrain['y_max'] = train.copy().groupby('X0')['y'].transform('max')\ntrain['y_min'] = train.copy().groupby('X0')['y'].transform('min')\ntrain['y_median'] = train.copy().groupby('X0')['y'].transform('median')\n#print(train)\ntrain_test = train.append(test)\ntrain_test= train_test.sort_values(by='ID')\n\ntrain_test=train_test[['y','ID','X0','X314','X5','X8']]\nlabels_X0=set( train['X0'] )\n#print(train_test)\nn_assets=len(labels_X0)\ndef rand_weights(n):\n    k = np.random.rand(n)\n    return k / sum(k)\n\ntrain_test['tijdzone']=round(train_test['ID']/48,0)\ntrain_pos=train_test[train_test['y']>0]\n#print(train_pos)\ntr_te_pima = pd.pivot_table(train_pos, values='y', index=['X0'],columns=['tijdzone'], aggfunc=np.max)\ntr_te_pimi = pd.pivot_table(train_pos, values='y', index=['X0'],columns=['tijdzone'], aggfunc=np.min)\n\nri_re_pl=pd.DataFrame([])\n\nri_re_pl['re']=tr_te_pima.sum()\nri_re_pl['ri']=tr_te_pima.std()\nri_re_pl['nr']=tr_te_pima.count()\nri_re_pl['ti']=ri_re_pl['re']/ri_re_pl['nr']\nri_re_pli=pd.DataFrame([])\nri_re_pli['re']=tr_te_pimi.sum()\nri_re_pli['ri']=tr_te_pimi.std()\nri_re_pli['nr']=tr_te_pima.count()\nri_re_pli['ti']=ri_re_pli['re']/ri_re_pli['nr']\n\n#print(ri_re_pl)\nprint('Plotting time spend max (blue) - min (orange) time per 48 cars versus variability')\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.scatter(ri_re_pl['ri'],ri_re_pl['ti'], alpha=0.5)\nplt.scatter(ri_re_pli['ri'],ri_re_pli['ti'], alpha=0.5)\nplt.show()","cell_type":"code","execution_count":3},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"1d833ce6cd73ada700f93df96dc02b655493ff57"},"source":"How do you minimize risk\n---\nWell in the stockmarket we call this 'portfolio analysis' , the headbanger who invented stabilizing portofolio's is Markowitz... A Markowitz optimization minizes risks ,maximizes returns\nIts not exactly this solution we want, we want to minimize the time spend on the cars...\n\n**Well mercedes has to RESHUFFLE his production as to minimize... the time spend and minimize that variation...**\nNow the toolbox to calculate this is not installed, but it doesn't matter, we can solve this with lagrangian relaxation.. Yes...\n\nSo the difference is less dramatic now, we talk about 15 seconds\n----\nbut the graph shows the same group the min/max time spend on a car.  Its clear for me the less variation the less time is spend...","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"006e26c881d0e2c0ebdf613e97181b682f668a1c"},"source":"import cvxopt as opt\nfrom cvxopt import blas, solvers\n\n\ndef optimal_portfolio(returns):\n    n = len(returns)\n    returns = np.asmatrix(returns)\n    \n    N = 100\n    mus = [10**(5.0 * t/N - 1.0) for t in range(N)]\n    \n    # Convert to cvxopt matrices\n    S = opt.matrix(np.cov(returns))\n    pbar = opt.matrix(np.mean(returns, axis=1))\n    \n    # Create constraint matrices\n    G = -opt.matrix(np.eye(n))   # negative n x n identity matrix\n    h = opt.matrix(0.0, (n ,1))\n    A = opt.matrix(1.0, (1, n))\n    b = opt.matrix(1.0)\n    \n    # Calculate efficient frontier weights using quadratic programming\n    portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n                  for mu in mus]\n    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n    returns = [blas.dot(pbar, x) for x in portfolios]\n    risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios]\n    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n    m1 = np.polyfit(returns, risks, 2)\n    x1 = np.sqrt(m1[2] / m1[0])\n    # CALCULATE THE OPTIMAL PORTFOLIO\n    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n    return np.asarray(wt), returns, risks\n\nweights, returns, risks = optimal_portfolio(ri_re_pl)\n\nplt.plot(stds, means, 'o')\nplt.ylabel('mean')\nplt.xlabel('std')\nplt.plot(risks, returns, 'y-o')","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"7450151b2258135dc4f8f94d6eb9a3f5a29ca5b5"},"source":"Lets Glimpse in the database \n----\nand search for the existing combination that minimize time spend","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"bc96a35cfe757e577ce28c5198f153352c34c3c5"},"source":"# fast group\nprint(ri_re_pli[ri_re_pl['ti']<98])\n#slow group\nprint(ri_re_pli[ri_re_pl['ti']>109])","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"741684fdd7187579c367ca95d3cd3ced03eee1b9"},"source":"some will get here inspiration to find forecast better the time-relation. \nThats very well possible. But it does not matter. This is IMHO not the solution for Mercedes\nThe real solution is reshuffling the production as to minimize the variation and hence the 'wasted' time\nThe time waste is caused IMHO due to the variability and the time between the cars\n\nexamples of fast groups\n----","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"e7d5d6191684ae3bcad61e1806d711f2ddb6e169"},"source":"import seaborn as sns\n#list of cars with short time\nfast=train_test[train_test['tijdzone']==82]\nfast=fast.groupby(['X0']).count()\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=fast.index, y=\"ID\", data=fast)\n","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"d4c2a0740aa8dc1bd17fd6d4ea7354807c74da1f"},"source":"fast=train_test[train_test['tijdzone']==144]\nfast=fast.groupby(['X0']).count()\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=fast.index, y=\"ID\", data=fast)\n","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"ba56bb813ac0ad87905ec2267578a601dc82b66b"},"source":"\nfast=train_test[train_test['tijdzone']==61]\nfast=fast.groupby(['X0']).count()\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=fast.index, y=\"ID\", data=fast)\n","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"54cc305cc95d5fe43fbaf5f330e70bda3547c7b8"},"source":"examples of slow groups\n----","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"de03949476433b09904aa4709e42f4d66bd20550"},"source":"#list of cars with long time\nfast2=train_test[train_test['tijdzone']==68]\nfast2=fast2.groupby(['X0']).count()\naax = sns.barplot(x=fast2.index, y=\"ID\", data=fast2)\n","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"7565b7df74f9e9d6d32423ef93876086667a02a7"},"source":"#list of cars with long time\nfast2=train_test[train_test['tijdzone']==104]\nfast2=fast2.groupby(['X0']).count()\naax = sns.barplot(x=fast2.index, y=\"ID\", data=fast2)","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"67bcfc82b1dd6c6808f5b3e83143e7a38d450915"},"source":"#list of cars with long time\nfast2=train_test[train_test['tijdzone']==114]\nfast2=fast2.groupby(['X0']).count()\naax = sns.barplot(x=fast2.index, y=\"ID\", data=fast2)","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"bfb2b6aa45e44293cd719bbfe0cc241a8441d58f"},"source":"How could we find an ideal mix ?\n----\n\n - construct a variance covariance matrix of the different X0's, i have to think abit how we could construct this, since we have alot of NAN's...\n - minimize that variance covariance i know how to do that, thats the simplest part\n\n    \n    ","cell_type":"markdown","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"2458add38e9f3ea105127a4ff599df46c065001f"},"source":"#print(train_test)\npivot=pd.pivot_table(train_test[train_test['y']>0], values='y', index=['X0'],columns=['tijdzone'], aggfunc=np.median)\n# not 1OO% happy with that ffill solution, but for the time being that proofs the model\nprint(pivot.fillna( method='ffill', axis=1, inplace=True) )\nprint(pivot.fillna( method='bfill', axis=1, inplace=True) )\n# here is the variance covariance matrix, but some are '0.0' should be dropped\nvcm=pivot.T.cov()\nvcm=vcm.drop(['ab','ac','g'])\nvcm=vcm.drop(['ab','ac','g'],axis=1)\nvcm= vcm.append(pd.DataFrame ( [1 for x in range (0,len(vcm))],index=vcm.index,columns=['mark'] ).T ) \nvcm['mark']=-1.0\nprint(vcm)\n# set the corner to a target value...\n\n\nvcm.ix['mark','mark']=100.0\nprint(vcm.ix['mark','mark'])\n\n# invert\ndf_sol = pd.DataFrame(np.linalg.pinv(vcm.values), vcm.columns, vcm.index)\n# look at last column\nprint(df_sol['mark']*480)\n\n#ok i forgot this one, the optimisation with the matrixinversion gives negatives values, that are in portfolio theorie shorting positions, but in real life i wonder how Benz could short a car?\n\n#give me some time to solve this one, i have to read abit","cell_type":"code","execution_count":null},{"outputs":[],"metadata":{"collapsed":false,"_execution_state":"idle","_uuid":"4b23af3d461c53ce0aa8c312a5e06be9725b5d3f"},"source":"def rand_weight(m,n):\n    s = np.random.normal(5,2, int(n/2))\n    s = s.round()\n    r = np.random.rand(m)*m\n    r = r.round()\n    d = [t ]\n    return r,s    \n\nprint(rand_weight(28,20))\nprint(rand_weight(28,20))","cell_type":"code","execution_count":8}],"nbformat":4,"nbformat_minor":0}