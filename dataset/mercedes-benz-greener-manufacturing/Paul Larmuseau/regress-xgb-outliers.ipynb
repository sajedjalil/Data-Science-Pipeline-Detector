{"cells":[{"cell_type":"markdown","outputs":[],"source":"thats the model  LB 0.5, the second model LB 0.56853\n____\nDon't know LB i can submit only 5 times aday: regressions with R 0.99 has LB 0.00... \n\n 1. Find regression R0.99 with a serie of 'X10+' parameters> thats the forecasteable part, using the predicted y_ as variable for next regression gives after 4 times a kind of stable prediction\n\n 2. substract the prediction from the y-values and we remain with the 1% unforecasteable part.\n\n 3. the same mill, giving PCA, XGB\n\nwhen you look at the graphs\n----\nthe X0, X5 remain the strong forecasters for that restvalue, but since we are forecasting a remaining part, we reconstruct the prediction by first prediction y_ with the regression part, and adding the prediction from the restvalue.\n\n\n\n","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"bba4fe8231ef13756ad5429ab3842fab48733af2"}},{"cell_type":"code","outputs":[],"source":"import numpy as np\nimport xgboost as xgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LinearRegression, TheilSenRegressor,RANSACRegressor,HuberRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n\ntotal = train.append(test)\n\n\n# process columns, apply LabelEncoder to categorical features\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder() \n        lbl.fit(list(train[c].values) + list(test[c].values)) \n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\n\n# find simply the 'autocorrelation' or say the basic regressive explained mean variation \nestimators = [\n              ('TSR', TheilSenRegressor(random_state=42)), #0.11\n              ('OLS', LinearRegression()),   # 0.53\n              ('RSR', RANSACRegressor(random_state=42)),   #0.509 AR#0.03\n              ('HBR', HuberRegressor())]   #AR# 0.01  \nlw = 2\n#cellen=['X'+str(w) for w in range(10,385) if str(w) not in ['25','72','121','149','188','193','303','381']]\ncellen=['X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137']\ncellen=['X47','X95','X314','X315','X232','X119','X311','X76','X329','X238','X340','X362','X137','ID'] # with ID\nX_=train[cellen]\ny_=train['y']\nfor name, estimator in estimators:\n    answ=estimator.fit(X_, y_)\n    print(name,estimator.score(X_,y_),estimator.get_params())\n    train[name]=estimator.predict(X_)\n    y_=estimator.predict(X_)\n        \n# now we simply substract the explained variation and stay with the 'rest' \n#we will try to forecast with xgboost\ntest['TSR']=estimator.predict(test[cellen])\ntrain_mem=train\nkolom=train.columns\nkolom=[k for k in kolom if k not in ['y','OLS','HBR','RSR','TSR','rest']+cellen]  #'ID'\ntrain['rest']=train['y']-train['TSR']  #print(train['rest'].T)\n\n#we use that noise detecting cluster analysis\nfor adc in range (8,9):\n    train=train_mem\n    print('n_comp',adc)\n    ##Add decomposed components: PCA / ICA etc.\n    n_comp = adc\n\n    # PCA\n    pca = PCA(n_components=n_comp, random_state=420)\n    pca2_results_train = pca.fit_transform(train[kolom])\n    pca2_results_test = pca.transform(test[kolom])\n    \n    # ICA\n    ica = FastICA(n_components=n_comp, random_state=420)\n    ica2_results_train = ica.fit_transform(train[kolom])\n    ica2_results_test = ica.transform(test[kolom])\n\n    # GRP\n    grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n    grp_results_train = grp.fit_transform(train[kolom])\n    grp_results_test = grp.transform(test[kolom])\n\n    # SRP\n    srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n    srp_results_train = srp.fit_transform(train[kolom])\n    srp_results_test = srp.transform(test[kolom])\n\n    # Append decomposition components to datasets\n    pcakol=[]\n    icakol=[]\n    grpkol=[]\n    srpkol=[]    \n    for i in range(1, n_comp+1):\n        train['pca_' + str(i)] = pca2_results_train[:,i-1]\n        test['pca_' + str(i)] = pca2_results_test[:, i-1]\n        pcakol+=['pca_' + str(i)]\n        \n        train['ica_' + str(i)] = ica2_results_train[:,i-1]\n        test['ica_' + str(i)] = ica2_results_test[:, i-1]\n        icakol+=['ica_' + str(i)]\n\n        train['grp_' + str(i)] = grp_results_train[:,i-1]\n        test['grp_' + str(i)] = grp_results_test[:, i-1]\n        grpkol+=['grp_' + str(i)] \n        \n        train['srp_' + str(i)] = srp_results_train[:,i-1]\n        test['srp_' + str(i)] = srp_results_test[:, i-1]\n        srpkol+=['srp_' + str(i)]\n        \n    y_train = train['rest']\n    y_mean = np.mean(y_train)\n    ### Regressor\n    import xgboost as xgb\n\n    # prepare dict of params for xgboost to run with\n    xgb_params = {\n        'n_trees': 1500, \n        'eta': 0.0045,\n        'max_depth': 5,\n        'subsample': 0.93,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'base_score': y_mean, # base prediction = mean(target)\n        'silent': 1\n    }\n\n\n    # form DMatrices for Xgboost training\n    totkol=kolom+pcakol+icakol+grpkol+srpkol\n    #print(totkol)\n    dtrain = xgb.DMatrix(train[totkol], train['rest'])\n    dtest = xgb.DMatrix(test[totkol])\n\n    num_boost_rounds = 1250\n    # train model\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n\n\n    # check f2-score (to get higher score - increase num_boost_round in previous cell)\n\n    print(r2_score(dtrain.get_label(),model.predict(dtrain)))\n\n    # make predictions and save results\n    y_pred = model.predict(dtest)\n    print(y_pred)\n\noutput = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\noutput.to_csv('superScript AR_ %f.csv' % adc, index=False)\n    \nfig, ax = plt.subplots(figsize=(10,50))\nxgb.plot_importance(model, height=0.9, ax=ax)\nplt.show()\n    \nprint(r2_score(dtrain.get_label(),model.predict(dtrain)))\n\n# reconstructing the prediction with  the combination of TSR + XGB\n# make predictions and save results #print(dtest.feature_names) #print(dtrain.feature_names)\ny_pred = model.predict(dtest)+test['TSR']\noutput = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\noutput.to_csv('superScript ARenPCA_XGB_ %f.csv' % adc, index=False)\noutput = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\nplt.figure(figsize=(12,8))\nsns.distplot(output.y.values, bins=50, kde=False)\nplt.xlabel('Predicted AVG Time on Test platform', fontsize=12)\nplt.show()\n","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"1ab1c811a8979bda3acc3d94b5c62568ebaeb961"}},{"cell_type":"code","outputs":[],"source":"import numpy as np\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import r2_score\n\n\n\nclass StackingEstimator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed\n\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\n\nstacked_pipeline = make_pipeline(\n    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n    LassoLarsCV()\n    \n)\nkolom=train.columns\ntrain['y_pred']=train['y']\nkolom=[w for w in kolom if w not in ['y']]\nfor lp in range(0,4):\n    stacked_pipeline.fit(train[kolom],train['y_pred'])\n    train['y_pred'] = stacked_pipeline.predict(train[kolom])\ntest['y_pred'] = stacked_pipeline.predict(test[kolom])\nprint(test['y_pred'].T)\ntrain['y_diff']=train['y']-train['y_pred']\nprint(train['y_diff'].T)\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train[kolom])\ntsvd_results_test = tsvd.transform(test[kolom])\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train[kolom])\npca2_results_test = pca.transform(test[kolom])\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train[kolom])\nica2_results_test = ica.transform(test[kolom])\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train[kolom])\ngrp_results_test = grp.transform(test[kolom])\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train[kolom])\nsrp_results_test = srp.transform(test[kolom])\n\n#save columns list before adding the decomposition components\n\nusable_columns = list(set(train.columns) - set(['y']))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n\n#usable_columns = list(set(train.columns) - set(['y']))\n\ny_train = train['y_diff'].values\ny_mean = np.median(y_train)\nid_test = test['ID'].values\n#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \n#finaltrainset = train[usable_columns].values\n#finaltestset = test[usable_columns].values\n\n\n'''Train the xgb model then predict the test data'''\n\nxgb_params = {\n    'n_trees': 1000, # 520 boost 2000 > lb 0.55  #520 boost 1250 LB 0.56\n    'eta': 0.0045,\n    'max_depth': 4,\n    'subsample': 0.93,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean, # base prediction = mean(target)\n    'silent': 1\n}\n# NOTE: Make sure that the class is labeled 'class' in the data file\nkolom=train.columns\ntrain['y_pred']=train['y']\nkolom=[w for w in kolom if w not in ['y','y_pred','y_diff']]\n\ndtrain = xgb.DMatrix(train[kolom], y_train)\ndtest = xgb.DMatrix(test[kolom])\n\nnum_boost_rounds = 750\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\ny_pred = model.predict(dtest)\n\nfig, ax = plt.subplots(figsize=(10,50))\nxgb.plot_importance(model, height=0.9, ax=ax)\nplt.show()\n    \nprint(r2_score(dtrain.get_label(),model.predict(dtrain)))\n\nsub = pd.DataFrame()\nsub['ID'] = id_test\nsub['y'] = y_pred + test['y_pred']\nsub.to_csv('stacked-models.csv', index=False)\nplt.figure(figsize=(12,8))\nsns.distplot(sub.y.values, bins=50, kde=False)\nplt.xlabel('Predicted AVG Time on Test platform', fontsize=12)\nplt.show()\n","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"f64b93022e88c21bb66b44cfefb3650673ccc71a"}},{"cell_type":"code","outputs":[],"source":"","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"f5491b282c4605391a80c42c601cb1da60ceedf7"}},{"cell_type":"code","outputs":[],"source":"","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"478eeec1e22dfd1709264256ba4e0f40c0d382de"}}],"nbformat_minor":0,"metadata":{"language_info":{"mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.0"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4}