{"cells":[{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"66cbe259-1289-47ea-8a12-b17ef777b223","_execution_state":"idle","_uuid":"d978d108a30a95cc5a929835a5d9555f10a25dcc"},"cell_type":"markdown","execution_count":null,"source":"There are four clusters when one plots train 'y' with train prediction. In Scirpus' [__original script__][1] this is pretty clear, yet the color-coding for y-values doesn't show all that well.\n\n\n  [1]: https://www.kaggle.com/scirpus/four-blob-tsne \"original script\""},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"a9924fce-3d76-4c0c-8134-aac8a34f17e1","_execution_state":"idle","trusted":false,"_uuid":"ff5d87f81a051dfa5be4ecd055186914f499b3da"},"cell_type":"code","execution_count":null,"source":"import numpy as np \nimport pandas as pd \nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"0716f5b5-8d6b-476f-a400-ca56d0ba2fe4","_execution_state":"idle","_uuid":"b66770648cb1ac3485f4af14520575b42d03a827"},"cell_type":"markdown","execution_count":null,"source":"Different color map.\n"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"753acfa0-d165-4c6f-ad38-aec8fb701b0e","_execution_state":"idle","trusted":false,"_uuid":"841dbcea467eaa1cb8162c4aa5fccaef7b07d1c3"},"cell_type":"code","execution_count":null,"source":"cm = plt.cm.get_cmap('RdYlBu')"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"6f75e2df-874c-4ecb-8688-3104be32766c","_execution_state":"idle","_uuid":"0809c2f42578e2d241b930c2526bfc88ee800797"},"cell_type":"markdown","execution_count":null,"source":"Five new features were added: X29, X48, X232, X236 and X263. All of them were found by genetic programming, just like the original set of Scirpus' features. I think this is justified as the scores will be better in the end. __X48 and X236 were subsequently removed.__"},{"outputs":[],"metadata":{"_cell_guid":"3e840080-98cb-4711-8344-96f854884c1d","_uuid":"2c4e6d1e77862d63e53a7868eb8545b43614e521","_execution_state":"idle","trusted":false},"cell_type":"code","execution_count":null,"source":"features = ['X118',\n            'X127',\n            'X47',\n            'X315',\n            'X311',\n            'X179',\n            'X314',\n### added by Tilii\n            'X232',\n            'X29',\n            'X263',\n###\n            'X261']"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"481f8bfa-433a-4d1c-8011-f14d9235b59f","_execution_state":"idle","_uuid":"71a48abb76eb5d71fa2d92f256fece150e3592c8"},"cell_type":"markdown","execution_count":null,"source":"In Scirpus' [__original script__][1] the whole y-range is used, so the color-coding gets stretched because of the >250 outlier. Therefore, most of the y-values end up in the bottom half of the color range and the whole plot is just various shades of blue that are difficult to tell apart. In this script I clip y-values so that everything above 130 will be the same shade of BLUE.\n\n\n  [1]: https://www.kaggle.com/scirpus/four-blob-tsne \"original script\""},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"b09d48c6-ac76-4f77-883f-fd37fa500479","_execution_state":"idle","trusted":false,"_uuid":"6517d2975bd87e242cf59b0c2439b91a2342707f"},"cell_type":"code","execution_count":null,"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ny_clip = np.clip(train['y'].values, a_min=None, a_max=130)"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"d7397551-1c29-46cb-a77c-c43fd6b56253","_execution_state":"idle","trusted":false,"_uuid":"9a4c29fc611ebec3825d1de01b54d5e0ba2ede1b"},"cell_type":"code","execution_count":null,"source":"tsne = TSNE(random_state=2016,perplexity=50,verbose=2)\nx = tsne.fit_transform(pd.concat([train[features],test[features]]))"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"2a342186-b38a-4fff-971f-0cba65163763","_execution_state":"idle","trusted":false,"_uuid":"e47da2eee068d140bc6caa48b87b8e88cb5b06c8"},"cell_type":"code","execution_count":null,"source":"plt.figure(figsize=(12,10))\n# plt.scatter(x[train.shape[0]:,0],x[train.shape[0]:,1], cmap=cm, marker='.', s=15, label='test')\ncb = plt.scatter(x[:train.shape[0],0],x[:train.shape[0],1], c=y_clip, cmap=cm, marker='o', s=15, label='train')\nplt.colorbar(cb)\nplt.legend(prop={'size':15})\n#plt.title('t-SNE embedding of train & test data', fontsize=20)\nplt.title('t-SNE embedding of train data', fontsize=20)\nplt.savefig('four-blob-tSNE-01.png')\n"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"02336a75-e4ce-4d71-8d02-7411dd61ea3f","_execution_state":"idle","_uuid":"fbf579015d40d97c92fb61caf80d5a86d712553c"},"cell_type":"markdown","execution_count":null,"source":"Pretty sure that the plot in this notebook will not look the same as my local plot. Not being paranoid - this is based on my experience with t-SNE Kaggle implementation from this [__script__][1]. Anyway, this is how my local plot looks like using the same script as in this notebook.\n\n![__t-SNE on raw data__][2]\n\nGiven that this t-SNE embedding was done on raw data, I think it compares quite nicely with the t-SNE embedding below which was done after neural network training. Four clusters should be obvious in both plots. One could even argue that there is a small 5th cluster.\n\n![__t-SNE after neural network training__][3]\n\n\n  [1]: https://www.kaggle.com/tilii7/you-want-outliers-we-got-them-outliers\n  [2]: https://i.imgur.com/JpmDztu.png\n  [3]: https://i.imgur.com/wRuOZkO.png"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"c652415f-1b97-4b0b-9c7e-54edd4fa12a4","_execution_state":"idle","trusted":false,"_uuid":"6b662af8ed92fa82c37b5b6805a58e439f06f017"},"cell_type":"code","execution_count":null,"source":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import KFold"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"18c17935-b787-44cd-8b2e-91a31b2c5ba3","_execution_state":"idle","trusted":false,"_uuid":"b5a321691f70ab9ab46988f59d5f21d44e38593f"},"cell_type":"code","execution_count":null,"source":"score = 0\nsplits = 5\nkf = KFold(n_splits=splits)\ny = train.y.ravel()\nfor train_index, test_index in kf.split(range(train.shape[0])):\n    blind = x[:train.shape[0]][test_index]\n    vis = x[:train.shape[0]][train_index]\n    knn = KNeighborsRegressor(n_neighbors=80,weights='uniform',p=2)\n    knn.fit(vis,y[train_index])\n    score +=(r2_score(y[test_index],(knn.predict(blind))))\nprint(score/splits)"},{"outputs":[],"metadata":{"collapsed":false,"_cell_guid":"c67313ca-2117-43d3-806f-38b69b823988","_execution_state":"idle","trusted":false,"_uuid":"2966485e38be7bb46895a5843353d193966f8725"},"cell_type":"code","execution_count":null,"source":"score = 0\nsplits = 5\nkf = KFold(n_splits=splits)\ny = train.y.ravel()\nfor train_index, test_index in kf.split(range(train.shape[0])):\n    blind = train[features].loc[test_index]\n    vis = train[features].loc[train_index]\n    knn = KNeighborsRegressor(n_neighbors=80,weights='uniform',p=2)\n    knn.fit(vis,y[train_index])\n    score +=(r2_score(y[test_index],(knn.predict(blind))))\nprint(score/splits)"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"version":"3.6.1","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"nbformat":4,"nbformat_minor":0}