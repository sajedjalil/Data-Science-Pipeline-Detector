{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#importing libraries\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib  \nimport statsmodels.formula.api as smf    \nimport statsmodels.api as sm  \nfrom sklearn.preprocessing import robust_scale\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer    \nfrom lime import lime_tabular\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing.data import QuantileTransformer\nfrom sklearn.preprocessing.data import PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import skew\nfrom sklearn.tree import DecisionTreeRegressor\npd.set_option('display.max_rows', 1000)\n## for data\nimport pandas as pd\nimport numpy as np\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 8, 5\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nSEED=2020\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkf = KFold(n_splits = 5, random_state = 2)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFECV\n## for statistical tests\nimport scipy\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer\nfrom lime import lime_tabular\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\nfrom collections import Counter\nfrom numpy import mean\nfrom IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"\nimport xgboost as xgb\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## importing and taking a look for dataset\ntrain= pd.read_csv('/kaggle/input/mercedes-benz-greener-manufacturing/train.csv.zip')\ntrain.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\nprint(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DF before tuning\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Some preliminary conclusions:\n  - no missing values in dataset\n  - 8 object features, rest - binary\n  - 12 features with 1 unique value"},{"metadata":{"trusted":true},"cell_type":"code","source":"## drop features with 1 unique value\n# train.drop(['ID','X11','X107', 'X233', 'X235', 'X268','X289', 'X290','X293', 'X297', 'X330', 'X347'], axis=1, inplace=True)\ntrain.drop(one_value_cols, axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## drop features with low variabiliry\nvariance_treshold = 0.9\nlow_var_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > variance_treshold]\ntrain=train.drop(low_var_cols, axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add some additional features, thanks for this script to @ Vitalii Mokin\ndef feature_creation(df):\n    for i in ['X0', 'X1', 'X2', 'X3', 'X5', 'X6', 'X8']:\n        for j in ['X0', 'X1', 'X2', 'X3', 'X5', 'X6', 'X8']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n\n    return df\n\n\ntrain = feature_creation(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## encode object features to numeric\nlencoders = {}\nfor col in train.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    train[col] = lencoders[col].fit_transform(train[col])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and drop duplicates\ntrain = train.drop_duplicates()\n# train.T.drop_duplicates().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## target with outliers\nsns.boxplot((train.y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete some outliers from target feature\ntrain = train[(train['y'] <= 136)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## target without outliers\nsns.boxplot((train.y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target normalization between 0 and 1 values\ntrain[\"y\"]=((train[\"y\"]-train[\"y\"].min())/(train[\"y\"].max()-train[\"y\"].min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## result\ntrain.y.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's reduce memory for clear mind\n\ndef reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold for removing correlated variables, thanks to @ Vitalii Mokin\nthreshold = 0.9  ## optimal level 0.9\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Select columns with Pearson's correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = train.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best = []\nfeatures_best.append(features_filtered.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# updated dataframe\ntrain=train[features_best[0]]\ntrain.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('y', axis=1)\ny = train.y\nX['n0'] = (X == 0).sum(axis=1) ## one add feature\n# scale features between 1 and 0\nscaler = preprocessing.MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\nX.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=SEED)\n# prepare the model with target scaling\nrf = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=SEED)\n# evaluate model\ncv = KFold(n_splits=10, shuffle=True, random_state=SEED)\nscores = cross_val_score(rf, X, y, scoring='r2', cv=cv, n_jobs=-1)\n# summarize the result\ns_mean = mean(scores)\nprint('Mean R2: %.3f' % (s_mean))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Feature ranking with recursive feature elimination and cross-validated selection of the best number of features\n# regressor =RandomForestRegressor(random_state=SEED)\n# regressor = DecisionTreeRegressor()\nregressor = xgb.XGBRegressor()\nselector = RFECV(regressor, step = 1, cv=cv, n_jobs=-1,verbose=1,  scoring='r2')\nselector.fit(X, y)\nprint('The optimal number of features is {}'.format(selector.n_features_))\nfeatures_rfecv = [f for f,s in zip(X, selector.support_) if s]\nprint('The selected features are:')\nprint ('{}'.format(features_rfecv)) ## optimal features list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (R2)\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.savefig('feature_auc_nselected.png', bbox_inches='tight', pad_inches=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let's compare RF after feature selection\nrf = RandomForestRegressor(max_depth=3, n_estimators=10, random_state=SEED)\nX_rfe = X[features_rfecv]\nrf.fit(X_rfe, y)\n# acc_log_train = round(rf.score(X_train_rfe, y_train)*100,2) \n# acc_log_test = round(rf.score(X_test[features_rfecv],y_test)*100,2)\nscores = cross_val_score(rf, X_rfe, y, cv=cv, scoring = 'r2')\n# print(\"Training Accuracy: % {}\".format(acc_log_train))\n# print(\"Testing Accuracy: % {}\".format(acc_log_test))\nprint(\"RF based on selected dataset\")\nprint(\"FR CV Accuracy Score after selection:\", scores.mean().round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we got list of 4 best features with the same model score"},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature importance with eli5 \nimport eli5 \nimport shap \nfrom eli5.sklearn import PermutationImportance\nrf.fit(X_rfe, y)\nperm = PermutationImportance(rf, random_state=SEED).fit(X_rfe, y)\neli5.show_weights(perm, feature_names = X_rfe.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_rfe)\nshap.summary_plot(shap_values, X_rfe, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nshap.summary_plot(shap_values, X_rfe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can spend our time taking in account only this 4 features during car tests"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate mean absolute error\ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))\n\nbaseline_guess = np.median(y)\n\nprint('The baseline guess based on Y-median value %0.2f' % baseline_guess)\nprint(\"Baseline Performance based on Y-median value: MAE = %0.4f\" % mae(y_test, baseline_guess))\nprint(\"Baseline Performance based on Random Forest RFECV-model: MAE = %0.4f\" % mae(y_test, rf.predict(X_test[features_rfecv])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}