{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"abf82a49-eb9d-5a1f-a802-f69fe7a3133e"},"source":"#This is my first Kernel!! \n##Performing first level data exploration which I've used to sketch my game plan. \n\nComments welcome! \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"adb84317-435e-87f1-e7ce-510858407497"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy.stats import spearmanr\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c2d9e6e-a07e-0895-5912-e75b92610522"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"a160d8be-4d73-1a22-edcc-6e9c13777cec"},"source":"Rudimentary Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0368b2d5-6763-1a4e-010f-4b742885d339"},"outputs":[],"source":"print(\"Are there NaNs in the dataset?: \",train.isnull().values.any())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2279c7c-53b7-8c64-7804-5c8aec687af5"},"outputs":[],"source":"binary = []\ncate = []\nfor c in train:\n    if c == 'ID' or c == 'y':\n        pass\n    else:\n        if train[[c]].isin([0,1]).all().values :\n            binary.append(c)\n        else:\n            cate.append(c)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8f6a283-b348-e9ef-37a3-0e54ff05a760"},"outputs":[],"source":"print(\"Number of Binary features      \",len(binary))\nprint(\"Number of Categorical features \",len(cate))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d12bf8ce-da42-7ed2-cc20-c33f053ff102"},"outputs":[],"source":"qq = [ np.unique(train[[x]].values) for x in cate]\nxq = [i for xqq in qq for i in xqq] # flattening\nw = np.unique(xq,return_counts=True)\nif np.all(w[1] == 1):\n    print(\"All of the entries in categorical features are unique.\")\nelse:\n    print(\"There is a repetition of entries across the categorical features.\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"5ce4dab7-9928-4c13-4062-8d17c6df1ead"},"source":"#Trends"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"068a5c46-4a88-2b32-0e94-f737502029f0"},"outputs":[],"source":"sr = []\nfor x in binary:\n    try :\n        sr.append(spearmanr(train[[x]].values,train[['y']])[0])\n    except:\n        sr.append(0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"639e9e10-bf71-3a4f-b2e5-fb70b80c0441"},"outputs":[],"source":"plt.figure(figsize=(40,20))\nplt.plot(sr,'-r',lw=1)\nplt.grid(True)\nplt.xticks(np.arange(len(sr)),binary,rotation=90)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4688bd8d-a91e-00ee-3476-2d0d284dcfc3"},"source":"##Mutual Information\n\nThe code used here is taken from MIFS(Mutual Information Feature Selection)[MIFS][1] and from [GIST][2]. \nWhile the original code kind of returns NaNs for large datasets, I used Stirling approximation so that the gamma function doesn't return NaNs.\n\nThere is still a major trouble here and that is, the mutual information is returning negative.  Will be a great help if you can explain me why it fails!!\n  [1]: https://github.com/danielhomola/mifs/\n  [2]: https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45dd1223-e2a1-e8fc-04e0-35961f9986b7"},"outputs":[],"source":"from sklearn.neighbors import NearestNeighbors\nfrom scipy.special import gamma,psi\nfrom scipy import ndimage\nfrom scipy.linalg import det\nfrom numpy import pi"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd692b70-d360-cdda-99a7-95ce4b46bbf5"},"outputs":[],"source":"def MI_DC(x, y, k):\n    \"\"\"\n    Calculates the mututal information between a continuous vector x and a\n    disrete class vector y.\n    This implementation can calculate the MI between the joint distribution of\n    one or more continuous variables (X[:, 1:3]) with a discrete variable (y).\n    Thanks to Adam Pocock, the author of the FEAST package for the idea.\n    Brian C. Ross, 2014, PLOS ONE\n    Mutual Information between Discrete and Continuous Data Sets\n    \"\"\"\n    y = y.flatten()\n    n = x.shape[0]\n    classes = np.unique(y)\n    knn = NearestNeighbors(n_neighbors=k)\n    # distance to kth in-class neighbour\n    d2k = np.empty(n)\n    # number of points within each point's class\n    Nx = []\n    for yi in y:\n        Nx.append(np.sum(y == yi))\n\n    # find the distance of the kth in-class point\n    for c in classes:\n        mask = np.where(y == c)[0]\n        knn.fit(x[mask, :])\n        d2k[mask] = knn.kneighbors()[0][:, -1]\n\n    # find the number of points within the distance of the kth in-class point\n    knn.fit(x)\n    m = knn.radius_neighbors(radius=d2k, return_distance=False)\n    m = [i.shape[0] for i in m]\n\n    # calculate MI based on Equation 2 in Ross 2014\n    MI = psi(n) - np.mean(psi(Nx)) + psi(k) - np.mean(psi(m))\n    return MI\n\n\ndef MI_CC(variables, k=1):\n    \"\"\"\n    Returns the mutual information between any number of variables.\n    Here it is used to estimate MI between continuous X(s) and y.\n    Written by Gael Varoquaux:\n    https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429\n    \"\"\"\n\n    all_vars = np.hstack(variables)\n    return (sum([Entropy(X, k=k) for X in variables]) -\n            Entropy(all_vars, k=k))\n\n\ndef Nearest_Distance(X, k=1):\n    '''\n    X = array(N,M)\n    N = number of points\n    M = number of dimensions\n    returns the distance to the kth nearest neighbor for every point in X\n    '''\n    knn = NearestNeighbors(n_neighbors=k)\n    knn.fit(X)\n    d, _ = knn.kneighbors(X) # the first nearest neighbor is itself\n    return d[:, -1] # returns the distance to the kth nearest neighbor\n\ndef Entropy(X, k=1):\n    ''' Returns the entropy of the X.\n    Parameters\n    ===========\n    X : array-like, shape (n_samples, n_features)\n        The data the entropy of which is computed\n    k : int, optional\n        number of nearest neighbors for density estimation\n    Notes\n    ======\n    Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy\n    of a random vector. Probl. Inf. Transm. 23, 95-101.\n    See also: Evans, D. 2008 A computationally efficient estimator for\n    mutual information, Proc. R. Soc. A 464 (2093), 1203-1215.\n    and:\n    Kraskov A, Stogbauer H, Grassberger P. (2004). Estimating mutual\n    information. Phys Rev E 69(6 Pt 2):066138.\n    '''\n\n    # Distance to kth nearest neighbor\n    r = Nearest_Distance(X, k) # squared distances\n    n, d = X.shape\n#     volume_unit_ball = (pi**(.5*d)) / gamma(.5*d + 1)\n    ge = .5*d + 1\n    lv = 0.5*d*np.log(pi) - .5*np.log(2*pi*ge) - ge*np.log(ge) + ge - d*np.log(2)\n    '''\n    F. Perez-Cruz, (2008). Estimation of Information Theoretic Measures\n    for Continuous Random Variables. Advances in Neural Information\n    Processing Systems 21 (NIPS). Vancouver (Canada), December.\n    return d*mean(log(r))+log(volume_unit_ball)+log(n-1)-log(k)\n    '''\n    return (d*np.mean(np.log(r + np.finfo(np.float).eps))\n            + lv + psi(n) - psi(k))\n\n\ndef MI(variables, k=1):\n    '''\n    Returns the mutual information between any number of variables.\n    Each variable is a matrix X = array(n_samples, n_features)\n    where\n      n = number of samples\n      dx,dy = number of dimensions\n    Optionally, the following keyword argument can be specified:\n      k = number of nearest neighbors for density estimation\n    Example: mutual_information((X, Y)), mutual_information((X, Y, Z), k=5)\n    '''\n    if len(variables) < 2:\n        raise AttributeError(\n                \"Mutual information must involve at least 2 variables\")\n    all_vars = np.hstack(variables)\n    return (sum([Entropy(X, k=k) for X in variables])\n            - Entropy(all_vars, k=k))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6fdea4d-e31a-630e-7495-cb57bdf96e4c"},"outputs":[],"source":"midc = []\nfor x in binary:\n    try :\n        z = MI_CC([train[['y']].values,train[[x]].values],3)\n        midc.append(z)\n    except:\n        print(\"x\",x)\n        midc.append(0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"860be3ab-66e3-373c-7a01-0c9ea749c3c8"},"outputs":[],"source":"plt.figure(figsize=(40,20))\nplt.plot(midc,'-r',lw=1)\nplt.grid(True)\nplt.xticks(np.arange(len(midc)),binary,rotation=90)\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}