{"nbformat_minor":2,"cells":[{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"7886bdecd1e08ff4e2cd80d1cbbca4be90e5c7b2","_cell_guid":"7ccfc60c-4921-4ac8-802b-e12c70c81132"},"source":"# 1. Introduction #\n\n## 1.1 Project Summary ##\n\nTo ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines. \n\n##  1.2 Objective ##\nThe objective is to tackle the curse of dimensionality and reduce the time cars spend on test bench.\n\n## 1.3 Data Dictionary ##\n\nThe dataset contains variables that are anonymus each representing a custome feature in a Mercedes car. The ground truth is labeled ‘y’ and represents the time (in seconds) that the car took to pass testing for each variable.\n\nVariables with letters are categorical. Variables with 0/1 are binary values.\n\n* train.csv - the training set\n* test.csv - the test set, you must predict the 'y' variable for the 'ID's in this file\n* sample_submission.csv - a sample submission file in the correct format\n\n\n## 1.4 Steps ##\nWe will be performing various steps to achieve the objective\n\n* Loading data\n* Exploratory data analysis\n* Data wrangling\n* Modeling and Predicting\n* Formatting output\n"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"db0a175b5b52ec60e9068e679733249c73768658","_cell_guid":"07f4dfd6-7126-4f6e-b4e2-f9a55705fee7"},"source":"# 2. Loading data #\n\n## 2.1 Loading libraries ##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"13931a0122474f4f2bf226513e31ae56e458cea5","trusted":false,"_cell_guid":"8137c83e-92f1-4f7f-af17-bdf5636416a1"},"source":"\"\"\" importing required packages \"\"\"\n%matplotlib inline\n\n\"\"\" packages for data manipulation, munging and merging \"\"\"\nimport pandas as pd\nimport numpy as np\n\n\"\"\" packages for visualiztion and exploratory analysis \"\"\"\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\", color_codes=True)\n\n\"\"\" packages for running machine learning algorithms \"\"\"\nfrom time import time\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR, LinearSVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"98e843530467135e84d9f47e321bda7ebea2e4cd","_cell_guid":"60777031-cb7c-4b2d-b18b-397a7afdc1d4"},"source":"## Loading data ##\n\nWe will be loading training and test datasets using pandas read_csv function."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"50c7c9432a302dbbcd1965b79bded359bbd259b9","trusted":false,"_cell_guid":"d337ef24-2d6b-4a73-b7c2-8bed4d8205fb"},"source":"\"\"\" file paths \"\"\"\ntrain_file = '../input/train.csv'\ntest_file = '../input/test.csv'\n\n\"\"\" function to read training data into pandas dataframe \"\"\"\ndef read_training_data(filepath):\n    train_df = pd.read_csv(filepath)\n    return train_df\n\n\"\"\" function to read test data into pandas dataframe \"\"\"\ndef read_test_data(filepath):\n    test_df = pd.read_csv(filepath)\n    return test_df\n\n\"\"\" calling function to read training data into a dataframe \"\"\"\ntrain_df = read_training_data(train_file)\ntest_df = read_test_data(test_file)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"4b82340b41c4e082a09dab325a805397b5f0c32f","trusted":false,"_cell_guid":"45e43f6b-5fd4-4f37-b9c8-04e7311b4080"},"source":"def data_summary(df):\n    print(df.shape)\n    print(df.info())   \n    print(df.head(5))\n\n\"\"\" calling functions to print summary statistics of training data \"\"\"\ndata_summary(train_df)\ndata_summary(test_df)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"f883848fd4cd6743ad529279b6244f80145b085c","_cell_guid":"c6fc4377-8402-4ddf-864e-e7e7ef129b3c"},"source":"In the training dataset, there are 4209 rows with 378 columns.\n\n* ground truth varible y is of type float\n* X0,X1,X2,X3,X4,X5,X6,X8 are of type object\n* rest of the columns are int type\n\nWe will convert [X0,X1,X2,X3,X4,X5,X6,X8] to categorical types and plot to see the distribution of values.\n"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"3e97efd61db0092e1c55f76d89ae85bf36707dd3","_cell_guid":"1ef56020-25ce-47ff-b8a6-5851c3e9441c"},"source":"# Imputing missing values #\n\nWe will check if there are any missing values in the training and testing datasets. If there are any we will use suitable methods to impute missing values."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"c9aa0e67645c2e5c9df1e1557849afcebc34a26b","trusted":false,"_cell_guid":"d7413f12-9e1b-4e27-a162-6884fabfaf04"},"source":"def check_missing_values(df):\n    if df.isnull().any().any():\n        print(\"There are missing values in the dataframe\")\n    else:\n        print(\"There are no missing values in the dataframe\")\n        \n\"\"\" calling functions to check missing values on training and test datasets \"\"\"\ncheck_missing_values(train_df)\ncheck_missing_values(test_df)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"2bb302cec03b667cfb57e051fbea73dc54dbafbb","_cell_guid":"dda38aa1-73bb-4033-98d1-71c2a583f58b"},"source":"Looks like there are no missing values in the dataframe."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"3ef7107e2d9bb4b6d45997e6f9e45881637f5712","trusted":false,"_cell_guid":"c9736000-574b-46ce-8770-fa36983c0075"},"source":"def initial_datatype_conversion(df):\n    cols = ['X0','X1','X2','X3','X4','X5','X6','X8']\n    for col in cols:\n        df[col] = df[col].astype('category')\n    return df\n\n\"\"\" datatype conversion \"\"\"\nret_train_df = initial_datatype_conversion(train_df)\nret_test_df = initial_datatype_conversion(test_df)\n\n\"\"\" combining categorical attributes from training and test datasets \"\"\"\ntrain_df_cat = ret_train_df.loc[:,['X0','X1','X2','X3','X4','X5','X6','X8']]\ntest_df_cat = ret_test_df.loc[:,['X0','X1','X2','X3','X4','X5','X6','X8']]\ntrain_df_cat = train_df_cat.add_prefix('train_')\ntest_df_cat = test_df_cat.add_prefix('test_')\ncombined = train_df_cat.append(test_df_cat, ignore_index=True)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"ae821af82d609f5c2eb0bde0c5c2c97af3cddc39","_cell_guid":"1010f748-388e-44c0-bad4-7557561b8e5b"},"source":"# Exploratory Data Analysis #"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"b59a47de589e750cd1aae4b6f54152a72aebc133","trusted":false,"_cell_guid":"0f319dfd-adce-48c7-b683-40326f04f7a4"},"source":"def visualize_categories(df, **kwargs):\n    row = kwargs.get('row',None)\n    col = kwargs.get('col',None)\n    hue = kwargs.get('hue',None)\n    \n    df_types = ['train_','test_']\n    y_val = ['X0','X1','X2','X3','X4','X5','X6','X8']\n    for df_type in df_types:\n        for val in y_val:\n            yval = df_type + val\n            plt.figure()\n            sns.countplot(y=yval, data=df, color=\"c\");\n            plt.show()\n\nvisualize_categories(combined)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"0f6b4639ea5a4632a410ad4f0c2ad95099a531c5","_cell_guid":"604067e1-c560-4220-bc6a-f9252751bd4e"},"source":"The plots reveal that values among X0, X1, X2, X5, X6 and X8 are fairly distributed, where as values in X3 is moderately distributed. X4 seems to have only one value 'd'. If X4 is part of the training features dataset, the model outcomes may be skewed. In order to avoid that, we will de dropping it from the training and test datasets that will be passed to modeling step.\n\nThe character values in X0, X1, X2, X5, X6 and X8 are converted to numerical values so that we can use them in modeling steps."},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"db62d03d2c425585013478c04de257dfa084860c","trusted":false,"_cell_guid":"414148d4-a012-4d37-9ee6-dfb235b52d6a"},"source":"\"\"\" converting training dataset object categorical values to numerical categorical types \"\"\"\nle = LabelEncoder()\ncols = ['X0', 'X1', 'X2','X3','X4','X5','X6','X8']\n#for col in cols:\n#    ret_train_df[col] = ret_train_df[col].astype('category')\n#    ret_test_df[col] = ret_test_df[col].astype('category')\n#    \n#    ret_train_df[col] = le.fit_transform(ret_train_df[col])\n#    ret_test_df[col] = le.fit_transform(ret_test_df[col])\n    \nret_train_df = pd.get_dummies(ret_train_df, columns=['X0', 'X1', 'X2','X3','X4','X5','X6','X8'], prefix=['X0', 'X1', 'X2','X3','X4','X5','X6','X8'])\nret_test_df = pd.get_dummies(ret_test_df, columns=['X0', 'X1', 'X2','X3','X4','X5','X6','X8'], prefix=['X0', 'X1', 'X2','X3','X4','X5','X6','X8'])"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"da9e6d61c06ad50263de73e3cae0a2aa169ae949","_cell_guid":"d181e894-4f7e-4f7c-aa16-5901c04b032c"},"source":"# Modeling and Prediction #"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"83920d9d02acb1018ff0b0f7fcaddda2ca950fe5","trusted":false,"_cell_guid":"e48e2f08-81e2-4c5d-9973-62403e32638a"},"source":"\"\"\" data preparation for modeling and prediction steps \"\"\"\ncols = ret_test_df.filter(like='_bb').columns\ntrain_X = ret_train_df.drop(['ID','y'], axis=1)\ntrain_Y = ret_train_df['y']\ntrain_Y = train_Y.values\ntest_X = ret_test_df.drop(['ID'],axis=1)\ntest_X = test_X.drop(cols, axis=1)\n\n#train_X = train_X.loc[:, train_X.var()>0.01]\n#test_X = test_X.loc[:, test_X.var()>0.01]\n\nmatching_cols = train_X.columns.intersection(test_X.columns)\nmatching_cols_list = matching_cols.tolist()\n\ntest_X = test_X[matching_cols_list]\ntrain_X = train_X[matching_cols_list]\n\n#test_X = test_X.loc[:, 'X0_a':]\n#train_X = train_X.loc[:, 'X0_a':]\n\nX_train, X_test, y_train, y_test = train_test_split(train_X, train_Y, test_size=0.33, random_state=42)\n\nprint(X_train.head())\nprint(X_test.head())\nprint(y_train)\nprint(y_test)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"b09ba8931c148aec26d00fc8aa924ee26b4e5778","_cell_guid":"481d34ab-8a44-4e4b-b7f9-93f5ca33a9df"},"source":"## Model Functions ##\n\n### Support Vector Regressor ###"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"a35bfb19d6da6defd38ce830c1aa87177e3aca3d","trusted":false,"_cell_guid":"ce2afd5b-8451-4d0e-90ab-35f4412a651a"},"source":"def perform_svc(train_X, train_Y, test_X, test_Y):\n    svr_clf = SVR()\n    svr_clf.fit(X=train_X, y=train_Y)\n    pred_Y = svr_clf.predict(test_X)\n    r2_score_svc = round(r2_score(test_Y, pred_Y),3)\n    accuracy = round(svr_clf.score(train_X, train_Y) * 100, 2)\n    returnval = {'model':'SVR', 'r2_score':r2_score_svc}\n    return returnval\n#perform_svc(train_X, train_Y, test_X)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"b62ce8ac9638c1699e3e4bc21588cfbdf611000e","_cell_guid":"953a3dd9-49b1-4b18-92d4-984bd9d62a13"},"source":"### Random Forest Regressor ###"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"0b343f76ac85eacc4163cb4d20025927f27159ff","trusted":false,"_cell_guid":"8b29bab5-8964-4f3b-a679-032ec15aeb0c"},"source":"def perform_rfc(df_X, df_Y, test_df_X, test_Y):\n\n    rfr_clf = RandomForestRegressor(n_estimators = 100 ,oob_score=True, max_features=\"auto\")\n    rfr_clf.fit(df_X, df_Y)\n    pred_Y = rfr_clf.predict(test_df_X)\n    r2_score_rfc = round(r2_score(test_Y, pred_Y),3)\n    accuracy = round(rfr_clf.score(df_X, df_Y) * 100, 2)\n    returnval = {'model':'RandomForestRegressor','r2_score':r2_score_rfc}\n    return returnval\n#perform_rfc(train_X, train_Y, test_X)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"de3d252086fc009ae84b625d5bba9537f19247bf","_cell_guid":"14239bfd-d2a1-4f58-ac22-18b0147ef419"},"source":"### K-Nearest Neighbors Regressor ###"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"ce427a5ded29235a058214a7008666c96a269a3e","trusted":false,"_cell_guid":"ba90821e-3904-4667-aae7-6493e942c00f"},"source":"def perform_knn(df_X, df_Y, test_df_X, test_Y):\n    knn = KNeighborsRegressor(n_neighbors=5)\n    knn.fit(df_X, df_Y)\n    pred_Y = knn.predict(test_df_X)\n    r2_score_knn = round(r2_score(test_Y, pred_Y),3)\n    accuracy = round(knn.score(df_X, df_Y) *100,2)\n    returnval = {'model':'KNeighborsRegressor','r2_score':r2_score_knn}\n    #print (returnval)\n    return returnval\n#perform_knn(train_X, train_Y, test_X)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"ad1a73609e57bcc7a41d4f92ad3607d970fd90fc","_cell_guid":"ddeac59d-24b8-4119-9e94-502db183373c"},"source":"## Linear Regression ##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"31e584dff9044c3c65b6c83b652a02e54b5c85f1","trusted":false,"_cell_guid":"e47856c0-8c37-452a-8da9-583bd86f1f8b"},"source":"def perform_linear_regression(df_X,df_Y, test_X, test_Y):\n    regr = LinearRegression()\n    regr.fit(df_X, df_Y)\n    pred_Y = regr.predict(test_X)\n    r2_score_lr = round(r2_score(test_Y, pred_Y),3)\n    accuracy = round(regr.score(df_X, df_Y) *100,2)\n    returnval = {'model':'LinearRegressor', 'r2_score':r2_score_lr}\n    return returnval\n#perform_linear_regression(train_X, train_Y, test_X)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"63624748027f9701f05b2d927487fa3f8d05d648","_cell_guid":"83cdd82e-88ec-4719-89cd-d4643bc15744"},"source":"## Linear Model (Lasso) ##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"b437587c84fdf9ada88896fae744495433f446d3","trusted":false,"_cell_guid":"efb9d2d0-93d3-4c3a-b581-3a1e9c83f571"},"source":"def perform_linear_lasso(df_X, df_Y, test_X, test_Y):\n    clf = Lasso(alpha=1.0)\n    clf.fit(df_X, df_Y)\n    pred_Y = clf.predict(test_X)\n    r2_score_ll = round(r2_score(test_Y, pred_Y),3)\n    accuracy = round(clf.score(df_X, df_Y) *100,2)\n    returnval = {'model':'Lasso','r2_score':r2_score_ll}\n    return returnval\n#perform_linear_lasso(train_X, train_Y, test_X)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"7fdb4868f09c74a7adaa37243b9a4b73fdb57bde","_cell_guid":"cd7130d9-c7eb-4912-b764-98f1fb276b1a"},"source":"## Ridge Regression ##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"166428256f62e927bcc8bfbda54874d3e366db18","trusted":false,"_cell_guid":"46beb851-4ffa-431e-983e-09e9c3313c1f"},"source":"def perform_ridge_regression(df_X, df_Y, test_X, test_Y):\n    clf = Ridge(alpha=1.0)\n    clf.fit(df_X, df_Y)\n    pred_Y = clf.predict(test_X)\n    r2_score_rr = r2_score(test_Y, pred_Y)\n    accuracy = round(clf.score(df_X, df_Y) *100,3)\n    returnval = {'model':'RidgeRegression','r2_score':r2_score_rr}\n    return returnval\n#perform_ridge_regression(train_X, train_Y, test_X)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"eca7202ada4f52dd3f70f1bb8416faf1049c182d","trusted":false,"_cell_guid":"159efa80-1490-433c-bb1b-3e911d925942"},"source":"def perform_elastinet_regression(df_X, df_Y, test_X, test_Y):\n    clf = ElasticNet(alpha=0.1, l1_ratio=0.7)\n    clf.fit(df_X, df_Y)\n    pred_Y = clf.predict(test_X)\n    r2_score_rr = round(r2_score(test_Y, pred_Y),3)\n    accuracy = round(clf.score(df_X, df_Y) *100,2)\n    returnval = {'model':'ElasticNet','r2_score':r2_score_rr}\n    return returnval"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"d517c726a35ba5bd008da9be11241621e260731b","_cell_guid":"7c420171-4adc-4c69-bf64-d7aaee1ae335"},"source":"## Dimensionality Reduction Techniques ##\n\n### Principal component analysis (PCA) ###\n\nLinear dimensionality reduction using Singular Value Decomposition (SVD) of the data to project it to a lower dimensional space. There are 4 types of SVD solvers\n\n* auto : the solver is selected by a default policy based on X.shape and n_components: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient ‘randomized’ method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards\n\n* full: run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing\n\n* arpack: run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. It requires strictly 0 < n_components < X.shape[1]\n\n* randomized: run randomized SVD by the method of Halko et al.\n\nWe will run principal component analysis for multiple n_component values to see how the models get affected.\n\nReference: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"eec97542e26d976986382e624617e934020c7166","trusted":false,"_cell_guid":"c027734d-5162-4c22-b1d6-6506ca9a4af8"},"source":"def perform_pca(train_X, train_Y, test_X, test_Y):\n    pca_model_accuracies = pd.DataFrame()\n    pca = PCA(n_components = 10,svd_solver='randomized',whiten=True)\n    pca.fit(train_X)\n    train_X_pca = pca.transform(train_X)\n    test_X_pca = pca.transform(test_X)\n\n    svc_acc_val = perform_svc(train_X_pca, train_Y, test_X_pca, test_Y)\n\n    rfc_acc_val = perform_rfc(train_X_pca, train_Y, test_X_pca, test_Y)\n\n    knn_acc_val = perform_knn(train_X_pca, train_Y, test_X_pca, test_Y)\n    \n    lr_acc_val = perform_linear_regression(train_X_pca, train_Y, test_X_pca, test_Y)\n    lc_acc_val = perform_linear_lasso(train_X_pca, train_Y, test_X_pca, test_Y)\n    rr_acc_val = perform_ridge_regression(train_X_pca, train_Y, test_X_pca, test_Y)\n    enet_acc_val = perform_elastinet_regression(train_X_pca, train_Y, test_X_pca, test_Y)\n    \n    pca_model_accuracies = pca_model_accuracies.append([svc_acc_val,rfc_acc_val,knn_acc_val,lr_acc_val,\n                                                       lc_acc_val,rr_acc_val,enet_acc_val])\n    cols = list(pca_model_accuracies.columns.values)\n    cols = cols[-1:] + cols[:-1]\n    pca_model_accuracies = pca_model_accuracies[cols]\n    pca_model_accuracies = pca_model_accuracies.sort_values(by='r2_score')\n    return pca_model_accuracies"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"791ccc0c03f57a35333740a78d009909dca80c6a","_cell_guid":"59d17c87-b972-4046-b6f7-44f8eaa4d147"},"source":"### Feature Agglomeration ###\n\nSimilar to AgglomerativeClustering, but recursively merges features instead of samples.\n\nDefault connectivity value is None, i.e, the hierarchical clustering algorithm is used which is unstructured. "},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"4953a25814c3892567c6561dd9a194cc6b2e455c","trusted":false,"_cell_guid":"4a7e0d39-cb3b-4572-bdfb-5c73ca1d6c19"},"source":"def perform_feature_agglomeration(train_X, train_Y, test_X, test_Y):\n    n_clusters = [32]\n    fagg_model_accuracies = pd.DataFrame()\n    for n_cluster in n_clusters:\n        agglo = FeatureAgglomeration(connectivity=None, n_clusters=n_cluster)\n        agglo.fit(train_X)\n        train_X_reduced = agglo.transform(train_X)\n        test_X_reduced = agglo.transform(test_X)\n        \n        svc_acc_val = perform_svc(train_X_reduced, train_Y, test_X_reduced, test_Y)\n                \n        rfc_acc_val = perform_rfc(train_X_reduced, train_Y, test_X_reduced, test_Y)\n                \n        knn_acc_val = perform_knn(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n        lr_acc_val = perform_linear_regression(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n        lc_acc_val = perform_linear_lasso(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n        rr_acc_val = perform_ridge_regression(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n        enet_acc_val = perform_elastinet_regression(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n        fagg_model_accuracies = fagg_model_accuracies.append([svc_acc_val,rfc_acc_val,knn_acc_val,\n                                                              lr_acc_val,lc_acc_val,rr_acc_val,enet_acc_val])\n        cols = list(fagg_model_accuracies.columns.values)\n        cols = cols[-1:] + cols[:-1]\n        fagg_model_accuracies = fagg_model_accuracies[cols]\n        fagg_model_accuracies = fagg_model_accuracies.sort_values(by='r2_score')\n    return fagg_model_accuracies"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"4d9463b7be4b5754259002b779569b1117fa6bf3","_cell_guid":"b1d5ad93-fd15-47ca-9363-0dcd17696e45"},"source":"### Variance Threshold ###"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"08b8c4211a7f9449ee4b3b32be3f5f4317799f92","trusted":false,"_cell_guid":"aacffa72-08cc-4d17-821a-ba3bdc2fa158"},"source":"def perform_variance_threshold(train_X, train_Y, test_X, test_Y):\n    vt_model_accuracies = pd.DataFrame()\n    selector = VarianceThreshold(threshold=0.03)\n    selector.fit(train_X)\n    train_X_reduced = selector.transform(train_X)\n    test_X_reduced = selector.transform(test_X)\n    \n    svc_acc_val = perform_svc(train_X_reduced, train_Y, test_X_reduced, test_Y)\n                \n    rfc_acc_val = perform_rfc(train_X_reduced, train_Y, test_X_reduced, test_Y)\n                \n    knn_acc_val = perform_knn(train_X_reduced, train_Y, test_X_reduced, test_Y)\n    \n    lr_acc_val = perform_linear_regression(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n    lc_acc_val = perform_linear_lasso(train_X_reduced, train_Y, test_X_reduced, test_Y)\n        \n    rr_acc_val = perform_ridge_regression(train_X_reduced, train_Y, test_X_reduced, test_Y)\n    \n    enet_acc_val = perform_elastinet_regression(train_X_reduced, train_Y, test_X_reduced, test_Y)\n                \n    vt_model_accuracies = vt_model_accuracies.append([svc_acc_val,rfc_acc_val,knn_acc_val,\n                                                     lr_acc_val,lc_acc_val,rr_acc_val,enet_acc_val])\n    cols = list(vt_model_accuracies.columns.values)\n    cols = cols[-1:] + cols[:-1]\n    vt_model_accuracies = vt_model_accuracies[cols]\n    vt_model_accuracies = vt_model_accuracies.sort_values(by='r2_score')\n    return vt_model_accuracies"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_uuid":"d044288fae7901b08c31852e2846b9009baeb0b0","_cell_guid":"f7332c93-89a6-40c6-af18-b78a956616c9"},"source":"#  Output #"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"f19c6d204fed9a2a6bd33f5796eece5bcc4300fa","_cell_guid":"def9182a-befc-402a-9f84-d209d712d1a5"},"source":"## PCA Results##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"c319ad4d395a8393b9dd4afd2d4b9e6b5ec3613e","trusted":false,"_cell_guid":"1590cde6-b64b-41af-bb03-745a1da891c1"},"source":"\"\"\" calling function to perform principal component analysis \"\"\"\npca_model_accuracies = perform_pca(X_train, y_train, X_test, y_test)\nprint(pca_model_accuracies)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e80cf3ccfba4adb954ff8f8e48a8e06abf65cf9c","_cell_guid":"eba6b2e0-848e-4fd9-ba33-eea53211b262"},"source":"## Feature Agglomeration Results ##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e43426a35489cca090f79e39bb32a2a7778f1745","trusted":false,"_cell_guid":"17ee69e6-a23c-4c56-9880-a021438d23f4"},"source":"\"\"\" calling function to perform feature agglomeration \"\"\"\nfagg_model_accuracies = perform_feature_agglomeration(X_train, y_train, X_test, y_test)\nprint(fagg_model_accuracies)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"211566761e54089d38843f2de697b55759ef469d","_cell_guid":"83ed0180-7bfe-441a-9591-097bef78194f"},"source":"## Variance Threshold ##"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"856203b08b951cfd2ef90de161caabf156966fe9","trusted":false,"_cell_guid":"be9dfab5-7891-44ee-a7ac-ac7211c7470b"},"source":"\"\"\" calling function to perform variance threshold analysis \"\"\"\nvt_model_accuracies = perform_variance_threshold(X_train, y_train, X_test, y_test)\nprint(vt_model_accuracies)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_execution_state":"idle","_uuid":"bacd8ef1fb925225704bb94293174d6be4203b57","trusted":false,"_cell_guid":"1d416c8e-73e3-4f31-af1b-c8c37dfc788a"},"source":"def write_to_csv(train_X, train_Y, test_X, test_df):\n    \n    print(train_X.shape)\n    print(test_X.shape)\n    \n    #pca = PCA(n_components = 10,svd_solver='randomized')\n    #pca.fit(train_X)\n    #train_X_reduced = pca.transform(train_X)\n    #test_X_reduced = pca.transform(test_X)\n    \n    #agglo = FeatureAgglomeration(connectivity=None, n_clusters=32)\n    #agglo.fit(train_X)\n    #train_X_reduced = agglo.transform(train_X)\n    #test_X_reduced = agglo.transform(test_X)\n    \n    selector = VarianceThreshold(threshold=0.03)\n    selector.fit(train_X)\n    train_X_reduced = selector.transform(train_X)\n    test_X_reduced = selector.transform(test_X)\n    \n    #clf = RandomForestRegressor(n_estimators = 100 ,oob_score=True, max_features=None)\n    #clf.fit(train_X_reduced, train_Y)\n    #pred_Y = rfr_clf.predict(test_X_reduced)\n    \n    clf = Ridge(alpha=1.0)\n    clf.fit(train_X_reduced, train_Y)\n    pred_Y = clf.predict(test_X_reduced)\n    \n    pred_Y_list = pred_Y.tolist()\n    test_X['y'] = pred_Y_list\n    test_X['ID'] = test_df.loc[:,'ID']\n    final_df = test_X.loc[:,['ID','y']]\n    final_df.to_csv('submission_1.csv',sep=',',index=False)\nwrite_to_csv(train_X, train_Y, test_X, ret_test_df)"},{"execution_count":null,"outputs":[],"cell_type":"markdown","metadata":{"_execution_state":"idle","_uuid":"0b00445a2814d221b8639afb33c5cb4ad6c3136a","_cell_guid":"1260bab0-1412-40d4-b730-59b9d3f06fd5"},"source":"\n# Summary #\n-------\n\nThis is my first competition besides titanic machine learning exercise. I had a great time working on this competition. Since I am still trying to get a grip of machine learning concepts and techniques, I request fellow kagglers to provide constructive feedback to improve myself.\n\nNote: I will add proper notes to the relevant sections when I am done with the problem. So please be advised that I might not be entirely correct with the explanations provided.\n\n## References ##\n* [Unsupervised learning](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)\n* [Feature Agglomeration](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration)\n* [Principal component analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n* [Random Projection](http://scikit-learn.org/stable/modules/random_projection.html#random-projection)\n* [Variance Threshold](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold)\n* [Dimensionality Reduction Techniques](https://www.knime.org/blog/seven-techniques-for-data-dimensionality-reduction)"}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1","name":"python","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}}}}