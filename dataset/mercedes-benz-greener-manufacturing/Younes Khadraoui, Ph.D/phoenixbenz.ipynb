{"nbformat":4,"nbformat_minor":0,"cells":[{"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, minmax_scale\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.linear_model import LassoLarsCV, SGDRegressor\n\nfrom sklearn.svm import SVR, LinearSVC\n\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_execution_state":"idle","_cell_guid":"aa64880f-8e35-4e00-a7dc-df0a828a7767","trusted":false,"_uuid":"ea8d6ac98b4194c132ca0658ce4342b2ceb8f13b"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\ntrain_y = train_df['y']\ntrain_id = train_df['ID']\ntrain_df = train_df.drop(\"y\", 1)\ntrain_df = train_df.drop(\"ID\", 1)\n\ntest_id = test_df['ID']\ntest_df = test_df.drop(\"ID\", 1)\n\nnum_train = len(train_df)\n\ndf_all = pd.concat([train_df, test_df])\ndf_all = pd.get_dummies(df_all, drop_first=True)\n\ntrain_df = df_all[:num_train]\ntest_df = df_all[num_train:]\n\n#############################\n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train_df)\ntsvd_results_test = tsvd.transform(test_df)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train_df)\npca2_results_test = pca.transform(test_df)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train_df)\nica2_results_test = ica.transform(test_df)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train_df)\ngrp_results_test = grp.transform(test_df)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train_df)\nsrp_results_test = srp.transform(test_df)\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp+1):\n    train_df['pca_' + str(i)] = pca2_results_train[:,i-1]\n    test_df['pca_' + str(i)] = pca2_results_test[:, i-1]\n    \n    train_df['ica_' + str(i)] = ica2_results_train[:,i-1]\n    test_df['ica_' + str(i)] = ica2_results_test[:, i-1]\n\n    train_df['tsvd_' + str(i)] = tsvd_results_train[:,i-1]\n    test_df['tsvd_' + str(i)] = tsvd_results_test[:, i-1]\n    \n    train_df['grp_' + str(i)] = grp_results_train[:,i-1]\n    test_df['grp_' + str(i)] = grp_results_test[:, i-1]\n    \n    train_df['srp_' + str(i)] = srp_results_train[:,i-1]\n    test_df['srp_' + str(i)] = srp_results_test[:, i-1]\n\nX_dtrain, X_test, y_dtrain, y_test = train_test_split(train_df, train_y, random_state=7, test_size=0.3)","metadata":{"_uuid":"656eb37d1365551eff6402c59ddcbf4289e4a18c","collapsed":false,"_cell_guid":"f2df1ba0-ad18-4984-80d6-db4af31d3652","trusted":false,"_execution_state":"idle"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"model_rfr = RandomForestRegressor(n_estimators=600, max_depth=3, min_samples_split=4, min_samples_leaf=60)\n\n# Let's see the feature importance for this model\nimportances = model_rfr.fit(train_df, train_y).feature_importances_\nfeatures = pd.DataFrame()\nfeatures['feature'] = train_df.columns\nfeatures['importance'] = importances\n\ntodrop = features.loc[features['importance'] == 0].feature\nnew_train_df = train_df.drop(todrop, 1)","metadata":{"_uuid":"6867a13e3abae59d7def50272bc36b5060a60554","collapsed":false,"_cell_guid":"f9e42b01-9339-44a1-bba0-2340dfcfd8c1","trusted":false,"_execution_state":"idle"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"model_ls = KNeighborsRegressor(n_neighbors=15, weights='distance', algorithm='auto', p=1)\n\n\nparam_test1 = {\n \"p\": (1, 2)\n}\n\ngsearch1 = GridSearchCV(estimator = model_ls, \n param_grid = param_test1, scoring='r2',iid=False, cv=5)","metadata":{"_uuid":"f05ac56139864300565b2b6e4fc3d28546f56171","collapsed":false,"_cell_guid":"defbe0e6-474c-49bb-98b5-8fe1ff18921a","trusted":false,"_execution_state":"idle"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"\ngsearch1.fit(new_train_df,train_y)\ngsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_","metadata":{"_uuid":"759bd575ed4f02d01200b84b60db0c8de43bb616","collapsed":false,"_cell_guid":"3f2e2cfd-e39e-47e9-be24-77851b2af012","trusted":false,"_execution_state":"idle"},"cell_type":"code","outputs":[]}],"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.1","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3}},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}