{"nbformat":4,"nbformat_minor":0,"cells":[{"outputs":[],"cell_type":"markdown","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"70c23b1f20552dd9e3013f392a82aeffc98ecdf9","collapsed":false},"source":"\nI tried to see if we can try all different possible row and column combinations to extract the right feature set and right data ( row set).  The total number of combinations is too much The total number of combinations will be\n\n[ row * ( row +1)/2] * [ column * ( column +1)/2]\n\nThe total number of rows = 4209. columns = 580.\n\nThe number of combination will be 1492812133050.\n\nI don't think this is a feasible approach\n\nI tried it in jumps of 50 and found that columns 139 to 554 and rows 5 to 2824 has the maximum impact"},{"outputs":[],"cell_type":"code","execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv('input/train.csv')\ntest = pd.read_csv('input/test.csv')\n\n#subset out datasets to an intial train and test by input and output variables\ntrain_y = train['y']\ntrain_x = train.drop(['y'], axis=1)\ntest_x = test\n\n\ntrain_x['Source'] = 'train'\ntest_x['Source'] = 'test'\ntraintest_x = train_x.append(test_x)\ntraintest_x = pd.get_dummies(traintest_x) \n#Split back data into train_x and test_x now that multicategory split is done\ntrain_x = traintest_x[traintest_x['Source_train']==1]\ndel train_x['Source_train']\ndel train_x['Source_test']\ntest_x = traintest_x[traintest_x['Source_test']==1]\ndel test_x['Source_train']\ndel test_x['Source_test']\n\n\n# Start to make pipeline\nfrom sklearn.pipeline import make_pipeline\n# Import Elastic Net, Ridge Regression, and Lasso Regression\nfrom sklearn.linear_model import ElasticNet, Ridge, Lasso\n\n# Import Random Forest and Gradient Boosted Trees\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\npipelines = {\n    'lasso' : make_pipeline(Lasso(random_state=123)),\n   \n}\n\n\n# Set hyperparameters for each model that will use to fit each model.  This will allow for finding an optimial model\n\n# Lasso hyperparameters \nlasso_hyperparameters = { \n    'lasso__alpha' : [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10] \n}\n\n# Create hyperparameters dictionary\nhyperparameters = {\n    \n    'lasso' : lasso_hyperparameters,\n   \n}\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Create empty dictionary called fitted_models\nfitted_models = {}\n\n#split the train set into a train and val dataset since the test dataset doesn't have label.  start with test size of 0.2\nfrom sklearn.cross_validation import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n\nlen = X_train.shape[1]\nrow = X_train.shape[0]\n\nfor x in range(len):\n    X_train2 = X_train.iloc[:,0:x]\n    X_val2 = X_val.iloc[:,0:x]\n    \n    for name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n        model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n        # Fit model on X_train, y_train\n        model.fit(X_train2, y_train)\n    \n        # Store model in fitted_models[name] \n        #fitted_models[name] = model\n        pred = model.predict(X_val2)\n    \n        # Print '{name} has been fitted'\n        print(\"x value is \", x)\n        #print(name, 'has been fitted.')\n        print( 'R^2:', r2_score(y_val, pred ))\n\n\nX_train2 = X_train.iloc[:,139:554]\nX_val2 = X_val.iloc[:, 139:554]\n\nbestR2Value = -100\n\n\n\nfor a in range(5, row,500):\n   \n    for y in range( a + 15,row):\n        if y > a:\n            X_train2 = X_train.iloc[a:y,139:554]\n            X_val2 = X_val.iloc[a:y,139:554]\n            y_train2 = y_train.iloc[a:y]\n            y_val2 = y_val.iloc[a:y]\n    \n            for name, pipeline in pipelines.items():\n    # Create cross-validation object from pipeline and hyperparameters\n                model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n    \n        # Fit model on X_train, y_train\n                model.fit(X_train2, y_train2)\n    \n        # Store model in fitted_models[name] \n        #fitted_models[name] = model\n                pred = model.predict(X_val2)\n                \n                r2 = r2_score(y_val2, pred )\n    \n        # Print '{name} has been fitted'\n                print(\"a value is\",a)\n                print(\"y value is \", y)\n        #print(name, 'has been fitted.')\n                print( 'R^2:', r2)\n                        \n                if r2 > bestR2Value:\n                    bestR2Value = r2\n                    a1 = a\n                    y1 = y\n                    print(\"bestR2Value is\", bestR2Value)\n                    print(\"a value is\", a)\n                    print(\"y value is\", y)\n\nprint(\"bestR2Value is\", bestR2Value)\nprint(\"a1 value is\", a1)\nprint(\"y1 value is\", y1)\n","metadata":{"trusted":false,"_cell_guid":"f87ce46c-0e0c-4018-a04d-59b0aa1159cf","_uuid":"c062db9c1f90bfc0bfb441ef975f4ee4ec73e421"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"}}}