{"cells":[{"cell_type":"markdown","outputs":[],"source":"# XGBoost with a single feature: X0","execution_count":null,"metadata":{"_uuid":"95901da73f16cb9ec615bec56412494b2af2ba85","_cell_guid":"fcbd123c-bc66-2763-69ef-8197f63a8af0"}},{"cell_type":"markdown","outputs":[],"source":"## Scenario\nWe will try to use only one feature: X0. It is the categorical feature with the highest cardinality, so there is a high probability that it is related somehow to the car model. The problem is: some categories of X0 are present in the test dataset but absent from the training dataset.\n\nWhat we are going to do to overcome this issue is transform X0 into a continuous feature. We hope the model can then interpolate the missing categories.  All other features will be ignored for this experiment.\n\nRegression using XGBoost and locally evaluated by a 10-fold cross validation.","execution_count":null,"metadata":{"_uuid":"8b08364fbd7b5e21a75670ee45ee73442f5e1271","_cell_guid":"b4babaf6-7d2f-43b1-e262-de5c42cdf935"}},{"cell_type":"markdown","outputs":[],"source":"## Load the data","execution_count":null,"metadata":{"_uuid":"95d677e9b21d420590daffe826a19e2ee2355126","_cell_guid":"d3049f9c-3c1c-3114-ad29-05ecdf89843f"}},{"cell_type":"code","outputs":[],"source":"import pandas as pd\n\ndef load_data(file):\n    return pd.read_csv(file, index_col='ID')","metadata":{"_execution_state":"idle","_uuid":"41b75ffc0d6a03008962f83c2795d46720c969c4","_cell_guid":"f053fc8d-76b6-9c43-f85e-fa51e3877720"},"execution_count":1},{"cell_type":"code","outputs":[],"source":"train_df = load_data('../input/train.csv')\nprint(\"Train dataset has {} samples.\".format(len(train_df)))\ntest_df = load_data('../input/test.csv')\nprint(\"Test dataset has {} samples.\".format(len(test_df)))\ntrain_df.head()","metadata":{"_execution_state":"idle","_uuid":"95e802098d37162bef199a1a28a84829d84658d5","_cell_guid":"90516118-912a-ac02-2e65-797e7ab5090b"},"execution_count":2},{"cell_type":"markdown","outputs":[],"source":"### Encoding X0 as a continuous feature","execution_count":null,"metadata":{"_uuid":"ad8f5087783e75c91c4fd80543361266b57c74c5","_cell_guid":"5e333083-099b-d94c-fc8a-2c6d685d9d66"}},{"cell_type":"markdown","outputs":[],"source":"X0 is originally a categorical feature","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"1408abb6d56f9e9144a3de4243291ed69235cfa7"}},{"cell_type":"code","outputs":[],"source":"print(train_df['X0'].unique())","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"05d6359ba4fd3986848677236fc8a858f00a6fc8"}},{"cell_type":"markdown","outputs":[],"source":"Since XGBoost works only with numerical data, categorical columns will not work. We will use a transformation that converts the codes used by Mercedes Benz into a continuous space.  We could have used a LabelEncoder, but in this case we would have to use the combination of train and test dataset (remember: some categories do not exist in the training set).","execution_count":null,"metadata":{"_uuid":"bcab78038050bf8b93666909bab7518db3809b6c","_cell_guid":"7e3a51dc-9bf7-4edb-1e6a-56932180125a"}},{"cell_type":"code","outputs":[],"source":"def mercedes_code_to_int(code):\n    vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', \n             'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', \n             'u', 'v', 'w', 'x', 'y', 'z','aa','ab','ac','ad',\n            'ae','af','ag','ah','ai','aj','ak','al','am','an', \n            'ao','ap','aq','ar','as','at','au','av','aw','ax', \n            'ay','az','ba','bb','bc','bd','be','bf','bg','bh',  \n    ]\n    return vocab.index(code)","metadata":{"_execution_state":"idle","_uuid":"9ceb538b567d57640b8cb8e7d96698f98b503110","_cell_guid":"8b1d48ce-e90f-f63c-1547-9dd8cabeb417"},"execution_count":4},{"cell_type":"code","outputs":[],"source":"def extract_feature_matrix(df):\n    return df['X0'].apply(mercedes_code_to_int).values.reshape(-1, 1)","metadata":{"_execution_state":"idle","_uuid":"5abf2c4784fcc288857958ea8f3fed60081fc333","_cell_guid":"6e39acab-980f-c289-017f-ada119db01e8"},"execution_count":5},{"cell_type":"code","outputs":[],"source":"import numpy as np\ntrain_X = extract_feature_matrix(train_df)\nprint(np.unique(train_X))\n\ntrain_y = train_df['y'].values\nprint(train_X.shape)\nprint(train_y.shape)","metadata":{"_execution_state":"idle","_uuid":"c4a7e215560fede0d67eabb644d72aa3d39ee7a5","_cell_guid":"11fc5d9f-48d0-21fe-dff0-a4c4ab73f305"},"execution_count":6},{"cell_type":"markdown","outputs":[],"source":"Good! X0 was converted to a continuous feature. Do you see the gaps 30, 32, 33, 39, 43, 47 and 53? Some of them occur only in the test dataset. Some of them are absent from both datasets.","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"57e96e3716f8f7d68b6a69c39f5014d2ca4409f3"}},{"cell_type":"markdown","outputs":[],"source":"## Creating a Model\nWe will be using XGBoost","execution_count":null,"metadata":{"_uuid":"02121083b30b1bab6b2f0cf0a976d7df6b06e09a","_cell_guid":"4f93f4d8-c0ad-d0c3-257d-ccb7cb8d726d"}},{"cell_type":"code","outputs":[],"source":"import xgboost as xgb","metadata":{"_execution_state":"idle","_uuid":"4554909a517498708985d43b1d67fd4cc6bca804","collapsed":true,"_cell_guid":"d9a8a78e-6832-c62f-fcdb-9a853affee84"},"execution_count":7},{"cell_type":"markdown","outputs":[],"source":"The parameters max_depth=2 and eta=0.1 are not default. The change on max_depth intends to prevent overfitting and eta produces smaller steps, to get closer to the optimal result.","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"7de133f003986700135eaa51547647cf6558a3f7"}},{"cell_type":"code","outputs":[],"source":"dtrain = xgb.DMatrix(data=train_X, label=train_y)\nparam = {'objective':'reg:linear', 'max_depth': 2, 'eta': 0.1}","metadata":{"_execution_state":"idle","_uuid":"1113089ed475856942121bd7254f55da0f9fcea3","collapsed":true,"_cell_guid":"7265c09c-9abc-1240-8514-461d7d4d8574"},"execution_count":8},{"cell_type":"markdown","outputs":[],"source":"We will use R² score as local evaluation for the xgboost model. It is the same scoring used by Kaggle for this competition to evaluate submissions.","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e4d33c3a64095836f216dd0ed62db53628868e48"}},{"cell_type":"code","outputs":[],"source":"from sklearn.metrics import r2_score\ndef kaggle_eror_eval(preds, dtrain):\n    return 'r^2', r2_score(y_pred=preds, y_true=dtrain.get_label())","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"60f787a69fb633ff81dd4ee6e9d90c4177de9676"}},{"cell_type":"markdown","outputs":[],"source":"Evaluation will be based on a 10-fold cross validation. It will take a while because of the small steps we are taking (see eta above). It can be accelerated by increasing eta, at the expense of the evaluation score.","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"638b0a646f5876605e682569cb0f2af3b4b6765e"}},{"cell_type":"code","outputs":[],"source":"cv_results = xgb.cv(param, dtrain, 1000, nfold=10, verbose_eval=False, feval=kaggle_eror_eval,\n                    maximize=False, early_stopping_rounds=20, seed=42, as_pandas=True)\ncv_results.tail()","metadata":{"_execution_state":"idle","_uuid":"83e8a3776a064d87871a9afa12a1d1f34b61b270","_cell_guid":"156f69b4-a823-c929-f66a-dc933fee42b4"},"execution_count":10},{"cell_type":"markdown","outputs":[],"source":"Since we have now a reference for the number of rounds and the R² score, we will train using all the available training data.","execution_count":null,"metadata":{"_uuid":"951e4b534f5b65e35d280221a956ccc71778ed8f","_cell_guid":"424ba78c-a490-a6a8-469f-9bebf4f5d2ac"}},{"cell_type":"code","outputs":[],"source":"bst = xgb.train(param, dtrain, num_boost_round=len(cv_results))\nbst","metadata":{"_execution_state":"idle","_uuid":"874a1a567b11deddffc38d51e7ad9ba7234f96e2","_cell_guid":"550ac5fc-9a11-8ba0-677d-80a2bf8c2ea3"},"execution_count":11},{"cell_type":"markdown","outputs":[],"source":"## Create a Submission\nAfter training our model, we need to create a final submission file based on the test dataset.\n\nLet's start by extracting the features from the dataframe, using the same approach we used for the training dataset.","execution_count":null,"metadata":{"_uuid":"3575f6d46eb1a3e2818f1fed90f118832b270e20","_cell_guid":"4f9d1bac-8726-1bec-74e7-1a2dd3fc92db"}},{"cell_type":"code","outputs":[],"source":"test_X = extract_feature_matrix(test_df)\nprint(np.unique(test_X))\nprint(test_X.shape)","metadata":{"_execution_state":"idle","_uuid":"92a5dc1a61c1db3f277439ca7e3a6cd59fd9bb4c","_cell_guid":"f42a3af6-16db-1fa4-1909-f5fbda426334"},"execution_count":12},{"cell_type":"markdown","outputs":[],"source":"Now that we have the features, we will just make the predictions using the previously trained model.","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"f31505f030b63a59cf3da701f838cf5444b1af9d"}},{"cell_type":"code","outputs":[],"source":"dtest = xgb.DMatrix(data=test_X)\npredictions = bst.predict(dtest)","metadata":{"_execution_state":"idle","_uuid":"858837decaf06344bff33cc66bb9536cea77a97c","collapsed":true,"_cell_guid":"8148651f-8864-ecfc-8b9e-e5c0e7b7f69c"},"execution_count":13},{"cell_type":"markdown","outputs":[],"source":"The predictions are being stored as a pandas Dataframe. Later, it can be used to generate the submission file.","execution_count":null,"metadata":{"_uuid":"cb1a48f8179a0bcdde5168680d82058db44c6a45","_cell_guid":"64af4607-dd51-3625-03a7-2072dbec26f6"}},{"cell_type":"code","outputs":[],"source":"submission = pd.DataFrame(index=test_df.index,\n                          data={'y': predictions})\nsubmission","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"088a335be124a6a9665830e2c6c026c34655403a"}}],"nbformat_minor":0,"metadata":{"_change_revision":0,"_is_fork":false,"language_info":{"mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.0"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4}