{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8455c544-a7fc-bf71-d182-04787d106120"},"source":"test Keras"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b74a8003-7d3b-4844-5f0d-a049fe2880c4"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA, FastICA\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras.backend as K\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport os\n\nfrom sphinx.addnodes import highlightlang\n\n#os.environ['PATH'] = os.environ['PATH'] + ';C:\\\\Users\\\\Roman\\\\Anaconda3\\\\Library\\mingw-w64\\\\bin\\\\'\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b0d295e-5d95-2704-57e3-2f50f9d0a623"},"outputs":[],"source":"def prepareInputData(train_df, test_df):\n    n = train_df.shape[1] - 2  # count of X columns in input data\n\n    X = pd.concat([train_df[train_df.columns[-n:]], test_df[test_df.columns[-n:]]], ignore_index=True)\n    X = pd.get_dummies(X)\n\n    train_X = X.head(train_df.shape[0])\n    test_X = X.tail(test_df.shape[0])\n\n    # usable_columns = list(set(X.columns))\n    #\n    # for column in usable_columns:\n    #     cardinality = len(np.unique(train_X[column]))\n    #     if cardinality == 1:\n    #         train_X.drop(column, axis=1)  # Column with only one value is useless so we drop it\n    #         test_X.drop(column, axis=1)\n\n    train_X = train_X.as_matrix()\n    test_X = test_X.as_matrix()\n\n    train_Y = train_df[\"y\"].as_matrix()\n    test_id = test_df[\"ID\"].as_matrix()\n\n    return train_X, train_Y, test_X, test_id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d06627d7-b516-b911-50ac-d5ecefd082a3"},"outputs":[],"source":"#def r2_keras(y_true, y_pred):\n#    SS_res =  K.mean(K.square( y_true - y_pred ))\n#    SS_tot = K.mean(K.square( y_true - K.mean(y_true) ))\n#    return ( 1 - SS_res/(SS_tot) )\n\ndef r2_keras(y_true, y_pred):\n    return K.sum(K.square(y_true - y_pred))\n\ndef r2_keras_neg(y_true, y_pred):\n    SS_res =  K.sum(K.square( y_true - y_pred ))\n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ))\n    return -( 1 - SS_res/(SS_tot) )\n\ndef r2_my(y_true, y_pred):\n    SS_res =  np.sum(np.square( y_true - y_pred ))\n    SS_tot = np.sum(np.square( y_true - np.mean(y_true) ))\n    #return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n    return ( 1 - SS_res/(SS_tot) )\n\ndef makeNnEnsemble(X, Y):\n    models = []\n    predicted_true = np.empty(0)\n    predicted_y = np.empty(0)\n\n    scaler_X = MinMaxScaler(feature_range=(-0.5, 0.5))\n    scaler_Y = MinMaxScaler(feature_range=(-0.5, 0.5))\n    X = scaler_X.fit_transform(X)\n    Y = scaler_Y.fit_transform(Y.reshape(-1, 1)).reshape(len(Y))\n\n    kf = KFold(n_splits=5, shuffle=False)\n    for train_index, test_index in kf.split(X):\n        x_train, x_valid = X[train_index], X[test_index]\n        y_train, y_valid = Y[train_index], Y[test_index]\n\n        x_valid = x_train\n        y_valid = y_train\n\n        model = Sequential()\n        model.add(Dense(10, input_dim=X.shape[1], kernel_initializer='normal', activation='linear'))\n        model.add(Dense(1, kernel_initializer='normal', activation='linear'))\n        # Compile model\n        model.compile(loss='mean_squared_error', optimizer='adam', metrics=[r2_keras])\n\n        checkpoint = ModelCheckpoint('t.hdf5', save_best_only=True)\n        s = model.fit(x_train, y_train, epochs=20, verbose=2, validation_data=(x_valid, y_valid), callbacks=[checkpoint], batch_size=64)\n        model.load_weights(\"t.hdf5\")\n        ind = np.argmin(s.history[\"val_loss\"])\n        print(\"best validation loss=\", s.history[\"val_loss\"][ind], \" val_r2_keras=\", s.history[\"val_r2_keras\"][ind],\n              \" train loss=\", s.history[\"loss\"][ind], \" [\", ind, \"]\")\n\n        predicted_true = np.append(predicted_true, y_valid)\n        a = model.predict(x_valid).reshape(len(y_valid))\n        predicted_y = np.append(predicted_y, a)\n        print(\"a.shape: \" + str(a.shape))\n        print(\"x_valid.shape: \" + str(x_valid.shape))\n        print(\"y_valid.shape: \" + str(y_valid.shape))\n        print(\"r2: \" + str(r2_score(y_valid, a)))\n        print(\"r2 my: \" + str(r2_my(y_valid.reshape(len(y_valid)), a)))\n        print(\"r2 keras: \" + str(K.eval(r2_keras(K.variable(y_valid), K.variable(a)))) )\n        print(\"mse: \" + str(mean_squared_error(y_valid, a)) )\n\n\n    print(\"validation r2: \" + str(r2_score(predicted_true, predicted_y)))\n    \n\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.head()\n\ntest_df = pd.read_csv(\"../input/test.csv\")\ntest_df.head()\n\ncols = train_df.shape[1]\n\ntrain_X, train_Y, test_X, test_id = prepareInputData(train_df, test_df)\n\nprint(\"rows before averaging: \" + str(train_X.shape[0]))\n# train_X, train_Y = averageEqualRows(train_X, train_Y)\nprint(\"rows after averaging: \" + str(train_X.shape[0]))\n\nmakeNnEnsemble(train_X.astype(np.float32), train_Y)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}