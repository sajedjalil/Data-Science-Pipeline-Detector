{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f11df408-2522-154b-1f3c-d6c462738a86"},"source":"#Ensemble kernel\n##Consist of xgboost, lstm network, ensemble algorithms\n###Using weights voting"},{"cell_type":"markdown","metadata":{"_cell_guid":"12f333e5-4fe4-cf3a-47af-2be1f5cdcd85"},"source":"###Import all libs, set seed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21fe097e-77c7-ae87-5387-c3aadad8d148"},"outputs":[],"source":"import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\nwarnings.filterwarnings('ignore')\n\nseed = 7\nnp.random.seed(seed)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c38456c-d218-e596-babf-ef1f71943eef"},"source":"###Add features function (PCA, ICA...)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d9655bf-d710-4633-af2e-5d2aa231a8f2"},"outputs":[],"source":"def add_features(train, test):    \n    n_compute = 20\n\n    tsvd = TruncatedSVD(n_components=n_compute, random_state=seed)\n    tsvd_results_train = tsvd.fit_transform(train)\n    tsvd_results_test = tsvd.transform(test)\n\n    pca = PCA(n_components=n_compute, random_state=seed)\n    pca2_results_train = pca.fit_transform(train)\n    pca2_results_test = pca.transform(test)\n\n    ica = FastICA(n_components=n_compute, random_state=seed)\n    ica2_results_train = ica.fit_transform(train)\n    ica2_results_test = ica.transform(test)\n    \n    grp = GaussianRandomProjection(n_components=n_compute, eps=0.1, random_state=seed)\n    grp_results_train = grp.fit_transform(train)\n    grp_results_test = grp.transform(test)\n\n    srp = SparseRandomProjection(n_components=n_compute, dense_output=True, random_state=seed)\n    srp_results_train = srp.fit_transform(train)\n    srp_results_test = srp.transform(test)\n\n    for i in range(1, n_compute + 1):\n        train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n        test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n        \n        train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n        test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n        train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n        test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n        train['grp_' + str(i)] = grp_results_train[:, i - 1]\n        test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n        train['srp_' + str(i)] = srp_results_train[:, i - 1]\n        test['srp_' + str(i)] = srp_results_test[:, i - 1]"},{"cell_type":"markdown","metadata":{"_cell_guid":"cb62336c-35db-10e3-6c27-175552264030"},"source":"###Inverse scale function (Scale function include next cell)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec59be56-66dc-6094-7b75-e8b49f1e91fc"},"outputs":[],"source":"scale_const = 1\n\ndef inverse_scale(predict_value):  \n    return scale_const * predict_value"},{"cell_type":"markdown","metadata":{"_cell_guid":"44ee2115-9326-fd9a-60b4-ab460cad0706"},"source":"##Main load data function. Incude some useful features from regressor + dummy coding."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64c2b400-204c-5ae2-079a-98ed8cdb1a8d"},"outputs":[],"source":"def load_data(path='../input/'):\n    df_train = pd.read_csv(path.__add__('train.csv'))\n    df_test = pd.read_csv(path.__add__('test.csv'))\n    \n    num_train = len(df_train)\n    \n    id_test = df_test['ID'].values\n    \n    y_train = df_train['y'].values.astype(np.float32)\n    \n    df_train_dummies = pd.get_dummies(df_train, drop_first=True)\n    df_test_dummies = pd.get_dummies(df_test, drop_first=True)\n\n    df_train_dummies = df_train_dummies.drop(['ID','y'], axis=1)\n    df_test_dummies = df_test_dummies.drop('ID', axis=1)\n    \n    df_temp = pd.concat([df_train_dummies, df_test_dummies], join='inner')\n    \n    df_train = df_temp[:num_train]\n    df_test = df_temp[num_train:]\n    \n    add_features(df_train, df_test)\n\n    clf = ExtraTreesRegressor(n_estimators=250, max_depth=4, random_state=seed)\n\n    clf.fit(df_train, y_train)\n\n    features = pd.DataFrame()\n    features['feature'] = df_train.columns\n    features['importance'] = clf.feature_importances_\n    features.sort_values(by=['importance'], ascending=True, inplace=True)\n    features.set_index('feature', inplace=True)\n\n    model = SelectFromModel(clf, prefit=True)\n    train_reduced = model.transform(df_train)   \n\n    test_reduced = model.transform(df_test.copy())\n    \n    df_train = pd.concat([df_train, pd.DataFrame(train_reduced)], axis=1)\n    df_test = pd.concat([df_test, pd.DataFrame(test_reduced)], axis=1)\n        \n    df_all = pd.concat([df_train, df_test])\n    \n    x_train, x_test = df_all.values[:num_train], df_all.values[num_train:]\n                                   \n    y_train /= scale_const\n    \n    return id_test, x_train, y_train, x_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"c33e5dc5-475c-5bcd-c216-5d5473cbcec2"},"source":"#Useful ensemble class with mixture of experts mode "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d0f822c-2741-318d-6aa9-f87001e12ecc"},"outputs":[],"source":"def softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\n\nclass Ensemble(object):\n    def __init__(self, stack, weights='mean', bias=None):\n        self.stack = stack\n        self.weights = weights\n        self.bias = bias\n        self.x = None\n        \n    def fit(self, x_train, y_train, lr_moe=0.001, n_epochs_moe=10):\n        len_s = len(self.stack)\n        \n        for b in self.stack:\n            b.fit(x_train, y_train)\n            print('\\n')\n        \n        if self.weights == 'mean':\n            self.weights = np.full(len_s, 1./len_s)\n        elif self.weights == 'moe':\n            print('Train moe algorithm on {}\\n'.format(x_train.shape[0]))\n            \n            self.x = np.random.uniform(low=0, high=1, size=len_s)\n            self.bias = np.random.uniform(low=0, high=5, size=len_s)\n            \n            for t in range(1, n_epochs_moe + 1):                \n                y_predict = np.vstack(self.stack_predict(x_train))\n                prob = softmax(self.x)        \n                \n                for k in range(x_train.shape[0]):\n                    dG_dx = []\n                    dG_db = []\n                    \n                    for i in range(len_s):                     \n                        dG_dx.append(0.5 * prob[i] * (1 - prob[i]) * (y_train - y_predict[i][k] + self.bias[i])**2)\n                        dG_db.append(prob[i] * (y_train - y_predict[i][k] + self.bias[i]))\n                             \n                        self.x[i] -= lr_moe * dG_dx[i][k]\n                        self.bias[i] -= lr_moe * dG_db[i][k]\n                    \n                print('Epoch {}; weights {}; alpha {}'.format(t, softmax(self.x), self.bias))\n                \n            self.weights = softmax(self.x)\n        \n    def stack_predict(self, x_valid):\n        b = [] \n        for b_ in self.stack:\n            b.append(b_.predict(x_valid))\n            \n        return b\n    \n    def predict(self, x_valid):              \n        predict = np.average(self.stack_predict(x_valid), axis=0, weights=self.weights)\n        \n        if self.bias != None:\n            predict += np.average(self.bias, weights=self.weights, axis=0)\n        return predict\n  \n\nprint('Ensemble class succsessful build')"},{"cell_type":"markdown","metadata":{"_cell_guid":"12bcec70-52b2-dd75-6fd8-c710bbccab23"},"source":"##R2 keras metric"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18299de1-a1d1-464a-5ff3-534ddb957d27"},"outputs":[],"source":"def r2_score_keras(y_true, y_pred):\n    SS_res = K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    \n    return (1 - SS_res / (SS_tot + K.epsilon()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5c8eca9f-5052-39fb-406e-d8e2ff48e6a8"},"source":"#Built models\n##Import all libs and load data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"baa12750-90ec-d938-3a40-7401984d5a14"},"outputs":[],"source":"from keras import backend as K\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, LSTM, Dropout, Reshape, Activation, BatchNormalization\nfrom keras.optimizers import sgd\n\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.linear_model import LassoLarsCV\n\nimport xgboost as xgb\n\nid_test, x_train, y_train, x_test = load_data()\n\ninput_shape = (x_test.shape[1], )\nre_input_shape = (1, x_test.shape[1]) \n\nx_train, x_valid, y_train, y_valid = train_test_split(\n    x_train, \n    y_train, \n    test_size=0.2, \n    random_state=seed\n)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3681fc4d-501e-07ac-dd8b-bbb5e7641fbf"},"source":"###Models"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b40dc221-6d31-ccce-d1f5-0508937d71fa"},"outputs":[],"source":"def nn_model():       \n    model = Sequential()\n    \n    model.add(Reshape(re_input_shape, input_shape=input_shape))\n    model.add(LSTM(100, return_sequences=False))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(250))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(1, activation='linear'))\n    \n    model.compile(optimizer='adam',\n                  loss='mse', \n                  metrics=[r2_score_keras])\n   \n    return model\n\nnn_regressor = KerasRegressor(build_fn=nn_model, \n                           epochs=150,  \n                           batch_size=32,\n                           validation_data=(x_valid, y_valid), \n                           shuffle=True,\n                           verbose=2)\n\ngbr = GradientBoostingRegressor(learning_rate=0.001, \n                                loss=\"huber\", \n                                max_depth=3, \n                                max_features=0.55, \n                                min_samples_leaf=18, \n                                min_samples_split=14, \n                                subsample=0.7)\n\nlasso = LassoLarsCV(normalize=True)\n\nrfr = RandomForestRegressor(n_estimators=250, \n                           min_samples_leaf=25,\n                           min_samples_split=25,\n                           n_jobs=4,\n                           max_depth=4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f1ae4aa-e923-8a0a-0d5d-2f14ac03dcc9"},"outputs":[],"source":"# Xgboost\nimport xgboost as xgb\n\nxgb_params = {\n    'n_trees': 1500, \n    'eta': 0.006,\n    'max_depth': 6,\n    'subsample': 0.93,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(y_train),\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\ndvalid = xgb.DMatrix(x_valid)\n\nmodel_xgb = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=750)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2457b771-5bbd-cb3e-837b-63e67fe997e0"},"outputs":[],"source":"model = Ensemble(stack=[nn_regressor, gbr, rfr, lasso], weights='mean')\nmodel.fit(x_train, y_train)\n\nprint('Done')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2992067-6f12-784e-05db-fc9018de6829"},"outputs":[],"source":"y_predict = 0.25 * inverse_scale(model.predict(x_test)) + 0.75 * inverse_scale(model_xgb.predict(dtest)) \ny_predict_test = 0.25 * inverse_scale(model.predict(x_train)) + 0.75 * inverse_scale(model_xgb.predict(dtrain))\ny_predict_valid = 0.25 * inverse_scale(model.predict(x_valid)) + 0.75 * inverse_scale(model_xgb.predict(dvalid))\n\n\ndf = pd.DataFrame({'ID':id_test, 'y':y_predict.ravel()})\ndf.to_csv('stack_test.csv', index=False)\n\nprint('R2 score test', r2_score(inverse_scale(y_train), y_predict_test))\nprint('R2 score test', r2_score(inverse_scale(y_valid), y_predict_valid))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}