{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.1","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"nbformat":4,"nbformat_minor":0,"cells":[{"execution_count":null,"execution_state":"idle","outputs":[],"metadata":{"_cell_guid":"f56612b7-5e56-a982-e444-2e803d037696","_uuid":"a78b735faf783956212fc8fe0470f7dd5d7bfe51","collapsed":false,"trusted":false,"_active":false},"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\nwarnings.filterwarnings('ignore')\n\nseed = 7\nnp.random.seed(seed)\n\nscale_const = 1"},{"metadata":{"_uuid":"86796791614c94c9eeb3d62f5e8361ba3655c985","_active":false,"_cell_guid":"9b0c5167-5916-1295-6c9a-dd1f6398c2ce"},"outputs":[],"cell_type":"markdown","source":"### Import","execution_count":null},{"execution_count":null,"execution_state":"idle","outputs":[],"metadata":{"trusted":false,"_uuid":"6fbfcfe44ddaf0f75eb427dcf490096bf86df96d","_active":true,"_cell_guid":"c1860d05-f575-5b2b-af7e-53478e209504"},"cell_type":"code","source":"def load_data(path='../input/'):\n    df_train = pd.read_csv(path.__add__('train.csv'))\n    df_test = pd.read_csv(path.__add__('test.csv'))\n    \n    num_train = len(df_train)\n    \n    id_test = df_test['ID'].values\n    \n    y_train = df_train['y'].values.astype(np.float32)\n    \n    df_train_dummies = pd.get_dummies(df_train, drop_first=True)\n    df_test_dummies = pd.get_dummies(df_test, drop_first=True)\n\n    df_train_dummies = df_train_dummies.drop(['ID','y'], axis=1)\n    df_test_dummies = df_test_dummies.drop('ID', axis=1)\n    \n    df_temp = pd.concat([df_train_dummies, df_test_dummies], join='inner')\n    \n    df_train = df_temp[:num_train]\n    df_test = df_temp[num_train:]\n    \n    add_pca_ica_features(df_train, df_test)\n\n    clf = ExtraTreesRegressor(n_estimators=300, max_depth=4, random_state=seed)\n\n    clf.fit(df_train, y_train)\n\n    features = pd.DataFrame()\n    features['feature'] = df_train.columns\n    features['importance'] = clf.feature_importances_\n    features.sort_values(by=['importance'], ascending=True, inplace=True)\n    features.set_index('feature', inplace=True)\n\n    model = SelectFromModel(clf, prefit=True)\n    train_reduced = model.transform(df_train)   \n\n    test_reduced = model.transform(df_test.copy())\n    \n    df_train = pd.concat([df_train, pd.DataFrame(train_reduced)], axis=1)\n    df_test = pd.concat([df_test, pd.DataFrame(test_reduced)], axis=1)\n        \n    df_all = pd.concat([df_train, df_test])\n    \n    x_train, x_test = df_all.values[:num_train], df_all.values[num_train:]  \n                                   \n    y_train /= scale_const\n    \n    return id_test, x_train, y_train, x_test\n    \n           \ndef add_pca_ica_features(train, test):    \n    n_compute = 10\n\n    # tSVD\n    tsvd = TruncatedSVD(n_components=n_compute, random_state=seed)\n    tsvd_results_train = tsvd.fit_transform(train)\n    tsvd_results_test = tsvd.transform(test)\n\n    # PCA\n    pca = PCA(n_components=n_compute, random_state=seed)\n    pca2_results_train = pca.fit_transform(train)\n    pca2_results_test = pca.transform(test)\n\n    # ICA\n    ica = FastICA(n_components=n_compute, random_state=seed)\n    ica2_results_train = ica.fit_transform(train)\n    ica2_results_test = ica.transform(test)\n\n    # GRP\n    grp = GaussianRandomProjection(n_components=n_compute, eps=0.1, random_state=seed)\n    grp_results_train = grp.fit_transform(train)\n    grp_results_test = grp.transform(test)\n\n# SRP\n    srp = SparseRandomProjection(n_components=n_compute, dense_output=True, random_state=seed)\n    srp_results_train = srp.fit_transform(train)\n    srp_results_test = srp.transform(test)\n\n#save columns list before adding the decomposition components\n\n# Append decomposition components to datasets\n    for i in range(1, n_compute + 1):\n        train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n        test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n        \n        train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n        test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n        train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n        test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n        train['grp_' + str(i)] = grp_results_train[:, i - 1]\n        test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n        train['srp_' + str(i)] = srp_results_train[:, i - 1]\n        test['srp_' + str(i)] = srp_results_test[:, i - 1]\n\n\ndef inverse_scale(predict_value):  \n    return scale_const * predict_value\n\nid_test, x_train, y_train, x_test = load_data()"},{"execution_count":null,"execution_state":"busy","outputs":[],"metadata":{"_cell_guid":"29b158b3-ce67-f979-31d4-11103dd0b968","_uuid":"fc1c612936849604ae7e21f4a156d879dcbe1d1c","collapsed":false,"trusted":false,"_active":false},"cell_type":"code","source":" # mmm, xgboost, loved by everyone ^-^\nimport xgboost as xgb\n\n# prepare dict of params for xgboost to run with\nxgb_params = {\n    'n_trees': 1000, \n    'eta': 0.001,\n    'max_depth': 8,\n    'subsample': 0.95,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(y_train), # base prediction = mean(target)\n    'silent': 1\n}\n\n# form DMatrices for Xgboost training\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\n# xgboost, cross-validation\ncv_result = xgb.cv(xgb_params, \n                   dtrain, \n                   num_boost_round=500, # increase to have better results (~700)\n                   early_stopping_rounds=50,\n                   verbose_eval=50, \n                   show_stdv=False\n                  )\n\nnum_boost_rounds = len(cv_result)\nprint(num_boost_rounds)\n\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"},{"execution_count":null,"execution_state":"busy","outputs":[],"metadata":{"trusted":false,"_uuid":"b4f9646204c68358921f533cf006ac36bd36f277","_active":false,"_cell_guid":"5f634dac-4ad8-7c09-9603-5fa817140136"},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\n# now fixed, correct calculation\nprint(r2_score(dtrain.get_label(), model.predict(dtrain)))"},{"execution_count":null,"execution_state":"busy","outputs":[],"metadata":{"_cell_guid":"169562ea-efbe-92b1-6f13-08f34b6597a5","_uuid":"c417ffbcebbdae053ca7c75d3dcc11a977ff705d","collapsed":false,"trusted":false,"_active":false},"cell_type":"code","source":"# make predictions and save results\ny_pred = inverse_scale(model.predict(dtest))\noutput = pd.DataFrame({'id': id_test, 'y': y_pred.ravel()})\noutput.to_csv('xgboost-depth{}-pca-ica.csv'.format(xgb_params['max_depth']), index=False)"}]}