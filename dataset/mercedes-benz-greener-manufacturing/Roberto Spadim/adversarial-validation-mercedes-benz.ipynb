{"cells":[{"source":"FROM https://www.kaggle.com/rspadim/adversarial-validation-porto-seguro\n\n\n---\n","cell_type":"markdown","metadata":{"_uuid":"b69ebf0d696a8185c2cfcd69c3909ca3e599c90e","_cell_guid":"788f05ff-541a-4ebb-98e9-d09422109955"}},{"source":"[FROM OTHER CONTEST]\n\nLike probably everyone else in this contest, I've been scratching my head: what to do about validation? The usual approaches don't work (obviously regular cross validation is a bad idea due to the time dimension [NOT IN PORTO SEGURO CONTEST], the score on last year of data is nowhere near the LB). Solution? Adversarial validation, inspired by FastML:\n\n[Adversarial validation][1]\n\nThe general idea is to check the degree of similarity between training and tests in terms of feature distribution: if they are difficult to distinguish, the distribution is probably similar and the usual validation techniques should work. It does not seem to be the case, so we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0/1 labels (0 - train, 1-test) and evaluating a binary classification task.\n\n\n  [1]: http://fastml.com/adversarial-validation-part-two/","cell_type":"markdown","metadata":{"_uuid":"1d734049717370542a597437f417e0437221c609","_cell_guid":"b1511075-2040-1acf-08f8-db4ab6bed4e1"}},{"outputs":[],"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","cell_type":"code","metadata":{"_uuid":"4863a6e30358fbb65fe3ef2d6a4045dc1ae034a6","collapsed":true,"_cell_guid":"bcbf5e1a-6442-47ba-5435-7053db612903"}},{"outputs":[],"execution_count":null,"source":"# We start by loading the training / test data and combining them with minimal preprocessing necessary\nxtrain = pd.read_csv('../input/train.csv')\nxtrain.drop(['ID', 'y'], axis = 1, inplace = True)\nxtest = pd.read_csv('../input/test.csv')\nxtest.drop(['ID'], axis = 1, inplace = True)\n\n# add identifier and combine\nxtrain['istrain'] = 1\nxtest['istrain'] = 0\nxdat = pd.concat([xtrain, xtest], axis = 0)\n\n# convert non-numerical columns to integers\ndf_numeric = xdat.select_dtypes(exclude=['object'])\ndf_obj = xdat.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \nxdat = pd.concat([df_numeric, df_obj], axis=1)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)","cell_type":"code","metadata":{"_uuid":"45aad642ad15eebf87376343bd6421aa5bbe5156","collapsed":true,"_cell_guid":"3f4ffc92-a776-6715-b7f3-925f1547d4fc"}},{"source":"[FROM OTHER CONTEST]\n\nDefine a split and the model (xgboost, what else :-)","cell_type":"markdown","metadata":{"_uuid":"ea6f7afc0e25ce0ebafe785f568e5c1a72b6f026","_cell_guid":"d0d09367-f024-ae7e-82ce-e75d53780274"}},{"outputs":[],"execution_count":null,"source":"skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 44) # why stratified k fold?\nxgb_params = { # is this parameters ok?\n        'learning_rate': 0.05, 'max_depth': 4,'subsample': 0.9,\n        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n        'silent': 1, 'n_estimators':100, 'gamma':1,\n        'min_child_weight':4, 'n_jobs':-1\n        }   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)     ","cell_type":"code","metadata":{"_uuid":"e7bdea3e061b85af7eba75d5a5e0e407ba2d8285","collapsed":true,"_cell_guid":"891a4cdc-d085-b67f-45e7-dcdc3ca0f8b2"}},{"source":"[FROM OTHER CONTEST]\n\nCalculate the AUC for each fold","cell_type":"markdown","metadata":{"_uuid":"6ffde62c36939f3f7c3c6db97ba8257fede6b19c","_cell_guid":"e103b1ab-479c-777d-ee19-a9f3a572a7c4"}},{"outputs":[],"execution_count":null,"source":"for train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10) # it takes ~ 80 rounds to fit\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))\n        \n#final dataset:\nclf.fit(xdat, y, eval_set=[(x1, y1)],\neval_metric='logloss', verbose=False,early_stopping_rounds=10) # it takes ~ 80 rounds to fit\n\nprval = clf.predict_proba(xdat)[:,1]\nprint(roc_auc_score(y,prval))","cell_type":"code","metadata":{"_uuid":"c460000353cffc78554d772be84707b5e3e48be2","collapsed":true,"_cell_guid":"223a095a-712f-fa90-707a-ea34fa091fc0"}},{"source":"[FROM OTHER CONTEST]\n\nAs we can see, the separation is almost perfect - which strongly suggests that the train / test rows are very easy to distinguish even for an xgboost\n\n---\n\nmaybe not... 0.499538408032 (not tunned parameters) of log loss vs 0.992604547897 from  Sberbank Russian Housing Market\n\nfinal (all dataset) 0.5213","cell_type":"markdown","metadata":{"_uuid":"7aa9c2629e7f3b59b4d5ea01c50ebe9a15f35e65","_cell_guid":"394561c3-a9d5-d3ad-31a0-801a8e7e2c42"}},{"source":"---\nwhat about KNN?","cell_type":"markdown","metadata":{"_uuid":"9bfef4d19fc2a2592488f800ae0e9473059b719c","collapsed":true,"_cell_guid":"9e501264-042f-47fb-b2a9-24f099f0880a"}},{"outputs":[],"execution_count":null,"source":"from sklearn.neighbors import KNeighborsClassifier\nknn_params={\n    'n_neighbors':5, # first try value\n    'weights':'distance',\n    'metric':'manhattan' #i like this name =)\n    \n    #distances to test: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n    #float:            \n    #   euclidean, manhattan, chebyshev, minkowski, wminkowski, seuclidean, mahalanobis\n    #integers: \n    #   hamming, canberra, braycurtis\n}\n\nclf = KNeighborsClassifier(**knn_params)      #good bye xgboost","cell_type":"code","metadata":{"_uuid":"c94e626e2ba8fec3474141db5626e37c16e3d3b9","collapsed":true,"_cell_guid":"0e64fb58-681e-4525-8b40-f0f28697e9d9"}},{"outputs":[],"execution_count":null,"source":"for train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0) # very easy parameters :)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))\n        \n#final dataset:\nclf.fit(xdat, y)\n\nprval = clf.predict_proba(xdat)[:,1]\nprint(roc_auc_score(y,prval))","cell_type":"code","metadata":{"_uuid":"f275eadc856cf70ddd87c1df2cf8f09a0827efbe","collapsed":true,"_cell_guid":"1398c3c9-9d08-4dbf-941f-3e6a5b08473f"}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"_change_revision":0,"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}},"_is_fork":false},"nbformat_minor":1}