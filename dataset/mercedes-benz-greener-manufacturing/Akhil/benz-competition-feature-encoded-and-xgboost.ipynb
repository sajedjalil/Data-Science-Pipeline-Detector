{"cells":[{"metadata":{"_uuid":"64d7c79b088314e8d589e12dd76cba2d80bf3239"},"cell_type":"markdown","source":"## Mercedes Benz competition"},{"metadata":{"_uuid":"abdbb2794669c11248ec8746b6b56df30ee003a6"},"cell_type":"markdown","source":"The objective of this competition is to predict the testing time for a Mercedes car based on mulitple combinations of features. While exploring the data, the most important part that i found interesting is the feature engineering part. The technique that i have used for feature engineering involves the use of target variable for encoding the categorical variable. This will help in identifying the patterns in the categorical variables and assign them ordinal variables that will capture more information."},{"metadata":{"trusted":true,"_uuid":"c50df5f3d28d4c59c87612a30dec10a9005806c8"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39c29e6fda310886883cd51fd633c4461881f6d4"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a408171790bdcc16c161f7991f030b853f298d6"},"cell_type":"markdown","source":"## 1.Exploratory Data Analysis"},{"metadata":{"_uuid":"8ca5e8308b89368f21737fa123afcec267e3348e"},"cell_type":"markdown","source":"### i.Checking the distribution of the target variable in the training dataset"},{"metadata":{"trusted":true,"_uuid":"3db33a979d33353d955cb398530765fb45fa59f5"},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(train['y'])\nplt.xlabel('value')\nplt.ylabel('frequency')\nplt.title('Distribution of target variable')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ae4d3ce5d2231e052d61febedd00e24fedd16c4"},"cell_type":"markdown","source":"We can observe that the target is skewed towards the left. We can improve the distribution by taking a log transformation or proceed with the analysis further and perform a baseline model"},{"metadata":{"_uuid":"862c31087b596f5548e6b622a5badf5a14e4f642"},"cell_type":"markdown","source":"### ii. Checking the summary statistics of all the variables in the data"},{"metadata":{"_uuid":"17b1af48de6ecbde0b1320ee36baf1bc76e2f0e0"},"cell_type":"markdown","source":"Getting the categorical values in the data"},{"metadata":{"trusted":true,"_uuid":"d9692699088f4138a9934a5a18eaea3d34f854b1"},"cell_type":"code","source":"train_cols = train.columns\ntrain_cols_num = train._get_numeric_data().columns\ncat_cols = list(set(train_cols) - set(train_cols_num))\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14511ae8f3f237a09ec06893382105cfd7bccd8a"},"cell_type":"markdown","source":"Checking the number of distinct values in each categorical column"},{"metadata":{"trusted":true,"_uuid":"50340fffe1d1f143e70632fe8cfc823123eb222e"},"cell_type":"code","source":"uniq_vals = {}\nfor col in cat_cols:\n    uniq_vals[col] = len(train[col].unique())\ncat_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc9d443d3e30cdc92262cd2b6634b60789cc9bab"},"cell_type":"code","source":"train_desc = train.describe()\n\n# Checking for the presence of negative values in the dataset\n# - There are no negative values in the dataset\nmin(train_desc.loc['min',:])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9ea33ecd7c08bb8a08b26c40ea22f21718a46ff"},"cell_type":"markdown","source":"**Insights:**  \nThere are 4209 rows in the training dataset.  \nThe maximum testing time is 265.32 secs and the minimum testing time is 72.11 secs. As we have already observed the data skewed towards the left which makes the mean as 100.66   \n"},{"metadata":{"_uuid":"a5843728e0909058a2a4ea3a9452f0daaf0aaa41"},"cell_type":"markdown","source":"### iii.Removing the outliers in the dataset  \nData at the longtails will be removed to remove any outliers from the dataset to reduce the distortions in the dataset"},{"metadata":{"_uuid":"7553f14fb7f77c42ac638a0d86c2e03a6e0d36b7"},"cell_type":"markdown","source":"Checking for outliers in the y variable"},{"metadata":{"trusted":true,"_uuid":"a2f882c02222da1bcaf5f5e93f67a7f55393d285"},"cell_type":"code","source":"\ndef limits(k):\n    upper_limit = k.mean() + 2*k.std()\n    lower_limit = k.mean() - 2*k.std()\n    std = k.std()\n    return (lower_limit,upper_limit)\n\noutlier_indices = []\nmask = (train['y'] < limits(train['y'])[0]) | (train['y'] > limits(train['y'])[1])\noutlier_indices.extend(train['y'][mask].index.values)\ntrain_cleaned = train.drop(train.index[list(set(outlier_indices))])\ntrain_cleaned","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55e77f20564fbbf623fed7c1d4a02f37fea7f296"},"cell_type":"markdown","source":"There are about 175 outliers in the data.\nAs a first pass, we will remove the outliers from the data and perform the analysis and at a later stage, we can check if adding these outliers improve the predictive power of the model.\nBut it is worth noting that decision trees and neural networks can handle outliers"},{"metadata":{"_uuid":"b527643b360a46c8eba027564d9020d8c0310b2b"},"cell_type":"markdown","source":"### iv. Encoding the categorical variables using the mean of the target variables"},{"metadata":{"trusted":true,"_uuid":"4e921cd49cbcfa7af423663ca8d8243a2189c91d"},"cell_type":"code","source":"import seaborn as sns\n\nfig,(ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8) = plt.subplots(8,1,figsize = (20,25))\nsns.boxplot(data = train_cleaned,x = 'X0', y = 'y',ax = ax1)\nsns.boxplot(data = train_cleaned,x = 'X1', y = 'y',ax = ax2)\nsns.boxplot(data = train_cleaned,x = 'X2', y = 'y',ax = ax3)\nsns.boxplot(data = train_cleaned,x = 'X3', y = 'y',ax = ax4)\nsns.boxplot(data = train_cleaned,x = 'X4', y = 'y',ax = ax5)\nsns.boxplot(data = train_cleaned,x = 'X5', y = 'y',ax = ax6)\nsns.boxplot(data = train_cleaned,x = 'X6', y = 'y',ax = ax7)\nsns.boxplot(data = train_cleaned,x = 'X8', y = 'y',ax = ax8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79b03d14ebdb19cbfdf0784c87f2d2cf793db910"},"cell_type":"code","source":"# Glue train + test\ntrain['eval_set'] = 0; test['eval_set'] = 1\ndf = pd.concat([train, test], axis=0, copy=True,sort = True)\n# Reset index\ndf.reset_index(drop=True, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"940e44c9700c31c7fa6f3f97958e928e3cf15fb0"},"cell_type":"code","source":"def add_new_col(x):\n    if x not in new_col.keys(): \n        # set n/2 x if is contained in test, but not in train \n        # (n is the number of unique labels in train)\n        # or an alternative could be -100 (something out of range [0; n-1]\n        return int(len(new_col.keys())/2)\n    return new_col[x] # rank of the label\n\nfor c in cat_cols:\n    # get labels and corresponding means\n    new_col = train_cleaned.groupby(c).y.mean().sort_values().reset_index()\n    # make a dictionary, where key is a label and value is the rank of that label\n    new_col = new_col.reset_index().set_index(c).drop('y', axis=1)['index'].to_dict()\n    # add new column to the dataframe\n    df[c + '_new'] = df[c].apply(add_new_col)\n\n# # drop old categorical columns\ndf_new = df.drop(cat_cols, axis=1)\n\n# # show the result\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2faa1ee131e58bf2173acfb39598a4caa2366457"},"cell_type":"markdown","source":"### Train test split"},{"metadata":{"trusted":true,"_uuid":"b7b234bc51c397ef3ff933404ad40e7be2aca4ff"},"cell_type":"code","source":"X = df.drop(list(set(cat_cols)), axis=1)\n\n# Train\nX_train = X[X.eval_set == 0]\ny_train = X_train.pop('y'); \nX_train = X_train.drop(['eval_set', 'ID'], axis=1)\n\n# Test\nX_test = X[X.eval_set == 1]\nX_test = X_test.drop(['y', 'eval_set', 'ID'], axis=1)\n\n# Base score\ny_mean = y_train.mean()\n# Shapes\n\nprint('Shape X_train: {}\\nShape X_test: {}'.format(X_train.shape, X_test.shape))\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"426ef8c2b00b7733facdfc539c7b3aa3881ee34b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daf388c7a24802d7006245b40d648e3c34dbf6e7"},"cell_type":"markdown","source":"## 2. Prediction using different models"},{"metadata":{"_uuid":"a12c3b94eea1bb5efa0ba822725c33b3a37e4d53"},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true,"_uuid":"25c98e66886e102631a74f9b4cfb49eaa018389f"},"cell_type":"code","source":"# from sklearn.linear_model import LassoCV\n# from sklearn.linear_model import Lasso\n# from sklearn.linear_model import Ridge\n# from sklearn.model_selection import KFold\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import mean_absolute_error,r2_score\n\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, Y_train, Y_test = train_test_split(pd.DataFrame(X_train),y_train,test_size = 0.30, random_state=50)\n\n# lasso = Lasso(random_state=0)\n# alphas = 10**np.linspace(1.2,-3,50)\n\n# k_fold = KFold(5)\n\n# lasso_r2_score = []\n# for i in range(0,len(alphas)):\n#     print(i)\n#     r2_score_k = []\n#     for k, (train, val) in enumerate(k_fold.split(X_train, Y_train)):\n#         clf = Lasso(alpha=alphas[i])\n#         clf.fit(X_train.iloc[train], Y_train.iloc[train])\n#         y_pred_val = clf.predict(X_train.iloc[val])\n#         k = r2_score(Y_train.iloc[val],y_pred_val)\n#         r2_score_k.append(k)\n#     m = np.mean(r2_score_k)\n#     lasso_r2_score.append(m)\n\n# l = pd.DataFrame(lasso_r2_score)\n# l['alphas'] = alphas\n# l.columns = ['lasso_r2_score','alphas']\n# print('Best chosen alpha value on cross validation(Lasso) is :',alphas[l['lasso_r2_score'].idxmax()])\n\n# # Lasso\n# from sklearn.metrics import r2_score\n# lasso = Lasso(alpha= 0.023 )\n# lasso.fit(X,Y)\n\n# #train\n# y_pred_test = lasso.predict(X_actual_test)\n# print('r2_score : %0.2f'%r2_score(Y,y_pred_test))\n\n\n# Test\n# Make predictions using the testing set\n# y_pred_test = lasso.predict(X_actual_test)\n# final_sub = pd.DataFrame(test['ID'])\n# final_sub['y'] = list(y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7b8791018b08de1e73eda16f50c49cf068a1b55"},"cell_type":"markdown","source":"### XGboost"},{"metadata":{"trusted":true,"_uuid":"0d2ae4909eaef151d6148c0f1c3d60f27cebc934"},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\n# xgb_preds = []\n\n# sub = pd.DataFrame()\n# test_id = test['ID']\n# sub['id'] = test['ID']\n# sub['y'] = np.zeros_like(test_id)\n\n# K = 5\n# kf = KFold(n_splits = K, random_state = 1, shuffle = True)\n\n# # X_new = new_features\n\n# for i,(train_index,val_index) in enumerate(kf.split(X_train,np.array(y_train))):\n#     print('Fold : %d'%i)\n# #     print(train_index)\n# #     print(val_index)\n    \n#     x_train, x_valid = X_train[train_index], X_train[val_index]\n#     y_train, y_valid = y_train[train_index], y_train[val_index]\n\n#     xgb_params= {\n#                     'learning_rate': 0.03,\n#                     'objective' : 'reg:linear',\n#                     'max_depth' : 4,\n#                     'metric': 'rmse',\n#                     'subsample': 0.9,\n#                     'colsample_bytree': 0.9,\n#                     'random_state': 1,\n#                     'num_leaves': 15\n#                  }\n    \n#     d_train = xgb.DMatrix(x_train, y_train)\n#     d_valid = xgb.DMatrix(x_valid, y_valid)\n#     d_test = xgb.DMatrix(np.array(X_test))\n\n#     def xgb_r2_score(preds, dtrain):\n#         labels = dtrain.get_label()\n#         return 'r2', r2_score(labels, preds)\n    \n#     watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n#     mdl = xgb.train(xgb_params, d_train, 1000, watchlist, early_stopping_rounds=70, feval=xgb_r2_score, maximize=True, verbose_eval=1)\n\n#     print('[Fold %d/%d Prediciton:]' % (i + 1, K))\n# #     # Predict on our test data\n#     p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n#     sub['y'] += p_test/K  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03527b58e4dfca9098b387e07975ff2a63ebac88"},"cell_type":"code","source":"xgb_params = {\n    'n_trees': 500, \n    'eta': 0.005,\n    'max_depth': 3,\n    'subsample': 0.9,\n    'colsample_bytree': 0.6,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.log(y_mean),\n    'silent': 1\n}\n\n# form DMatrices for Xgboost training\ndtrain = xgb.DMatrix(X_train, np.array(np.log(y_train)))\ndtest = xgb.DMatrix(X_test)\n\n# evaluation metric\ndef the_metric(y_pred, y):\n    y_true = y.get_label()\n    return 'r2', r2_score(y_true, y_pred)\n\n# xgboost, cross-validation\ncv_result = xgb.cv(xgb_params, \n                   dtrain, \n                   num_boost_round=2000, \n                   nfold = 3,\n                   early_stopping_rounds=50,\n                   feval=the_metric,\n                   verbose_eval=100, \n                   show_stdv=False\n                  )\n\nnum_boost_rounds = len(cv_result)\nprint('num_boost_rounds=' + str(num_boost_rounds))\n\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n\n# Predict on trian and test\ny_train_pred = np.exp(model.predict(dtrain))\ny_pred = np.exp(model.predict(dtest))\n\nprint('First 5 predicted test values:', y_pred[:5])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2e3be8740539eb4919b00f13cdf01ddae470c4f"},"cell_type":"code","source":"output = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\noutput.to_csv('sub_16_encoded.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dca72a2cac3dc9dcd1d2daf28cf602ca030da2ef"},"cell_type":"markdown","source":"### Light XGB"},{"metadata":{"trusted":true,"_uuid":"a9bce7d8ef6dd7456fde2e296dcad5c8508a5f3f"},"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n\n# lgb_params = {\n#     'learning_rate': 0.03,\n#     'metric': 'rmse',\n#     'subsample': 0.9,\n#     'colsample_bytree': 0.9,\n#     'random_state': 1,\n#     'num_leaves': 31\n# }\n# X_train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f053529f9b923fa8a4852a801ef280d6cc465eb0"},"cell_type":"code","source":"# sub.to_csv('sub14_xgb_cv.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}