{"nbformat":4,"nbformat_minor":0,"cells":[{"outputs":[],"cell_type":"markdown","metadata":{"_cell_guid":"3760953a-ab3f-485b-ab32-ab25227584f5","_execution_state":"idle","collapsed":false,"_uuid":"6c75f4e7d94045fe42e3827cc418dbaf1f32bef7"},"source":"Other competitors provided lot of insights of Mercedes data already, see e.g. [here][1]  and [here][2]. I propose yet another. Two couples of very interesting graphs are at bottom of this notebook. Don't miss!   :)\n\n  [1]: http://www.kaggle.com/robertoruiz/the-only-ones\n  [2]: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/35382","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"f4a82694-b5a7-4c03-a6c9-b5edf00cf777","_execution_state":"idle","collapsed":false,"_uuid":"b105f4634e44452d1d19cdb041f5a87e0da29bde"},"source":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport numpy as np \nimport pandas as pd \nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.neighbors import KNeighborsRegressor\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":10},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"462592ee-6ef6-4181-ac99-a826d0c88a8c","_execution_state":"idle","collapsed":false,"_uuid":"6ce17ea82ee6a043c8aaf8bb34a2b2b15d10a07a"},"source":"# To be sure that R2 is calculated properly\ndef r2(y,f):\n    SS_res = ((f - y)**2).sum()\n    SS_tot = ((y - y.mean())**2).sum()\n    R2 = 1 - SS_res / SS_tot\n    return SS_res, SS_tot, R2","execution_count":5},{"outputs":[],"cell_type":"code","execution_count":null,"source":"# Loading data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","metadata":{"trusted":false,"_cell_guid":"a877a84d-cd55-4b31-b7ef-61107f209a79","_execution_state":"idle","_uuid":"171579a1616182d9f2dfbe434cfef47e084f2b39"}},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"2403760c-3f64-4547-a75f-f64f79fd6071","_execution_state":"idle","collapsed":false,"_uuid":"b92f9fb228bef9084724cf2b2f12621abe2f424d"},"source":"# features with only one values\nzero_features =['X11','X93','X107','X233','X235','X268','X289','X290','X293','X297','X330','X347']","execution_count":6},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"333e7a81-3071-4d9e-9aef-1a7a9f7df74a","_execution_state":"idle","collapsed":false,"_uuid":"99bd77bd2bcb304330ffd341de95c87bfce5cdf0"},"source":"# preparing data\ny_train = df_train[df_train.y < 250]['y']\nx_train = df_train[df_train.y < 250].drop(['ID','y'] + zero_features,axis =1)\nx_test = df_test.drop(['ID'] + zero_features,axis = 1)\n\nprint(x_train.shape, y_train.shape, x_test.shape)","execution_count":7},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"db04636f-323b-4349-bfb3-50b4641e635e","_execution_state":"idle","collapsed":false,"_uuid":"9f107e2aed84b4cda173fb6f16871c2a311c4e81"},"source":"# dealing with categorical variables\nnum_train = len(x_train)\nx_all = pd.concat([x_train, x_test])\n\nfor c in x_all.columns:\n    if x_all[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(x_all[c].values))\n        x_all[c] = lbl.transform(list(x_all[c].values))\n\nx_train = x_all[:num_train]\nx_test = x_all[num_train:]\n\nprint(x_train.shape, y_train.shape, x_test.shape)","execution_count":8},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"385c6a5e-b4a6-4b6e-9cda-1c00bd3cbec0","_execution_state":"idle","collapsed":false,"_uuid":"342c5cd8f2c178a30af22313d3bd5bbc028af4f2"},"source":"# preparing xgboost. Parameters are defined by cross validation (xgb.cv)\n\nxgb_params = {\n    'eta': 0.01,\n    'max_depth': 2,\n    'subsample': 0.8,\n    'colsample_bytree': 1.0,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)","execution_count":18},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"45e49f05-fb34-4567-8b5f-fe97fa9ef1c3","_execution_state":"idle","collapsed":false,"_uuid":"1949b91c6fc5987c0644fe95ba4e82c88c2c4dd5"},"source":"# run xgboost\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round = 672)","execution_count":21},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"24d3834f-02e1-466e-9393-e71406ac9709","_execution_state":"idle","collapsed":false,"_uuid":"09f6aad0b915df0754f8559a8a489620f8b1ba55"},"source":"# Look on data. Seems all is OK\ny_predict = model.predict(dtest)\nsub = pd.DataFrame({'id': df_test['ID'], 'y': y_predict})\nsub.head()","execution_count":22},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"7f5b4270-3886-45b3-9d06-ee6b9ab13278","_execution_state":"idle","collapsed":false,"_uuid":"18d1175aca5a6bef7a413eaf1172532ebbb39dee"},"source":"# But check them on train set\nz_train = df_train[df_train.y < 250].copy()\nz_train['err'] = y_train - model.predict(dtrain)\nz_train['y'] = y_train\nz_train['predict'] = model.predict(dtrain)\nfig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(15, 5)\nz_train.plot.scatter(x = 'y',y = 'predict', ax=axes[0], label = 'prediction')\nz_train.plot.scatter(x = 'y',y = 'y', color = 'Red',ax=axes[0], label = 'actual')\nz_train.plot.scatter(x = 'y',y = 'err', ax=axes[1], label = 'error')","execution_count":23},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"af19fdfb-dbdf-447f-bd5f-adae940d9f6d","_execution_state":"idle","collapsed":false,"_uuid":"77e8e85533b78578f9efe0868f2bcfe5ebecc571"},"source":"","execution_count":null},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8263e6f9-7361-426a-bdaa-ea10732f7c37","_execution_state":"idle","collapsed":false,"_uuid":"1016c98e25c50bb6e2296ea9b9640fcf03cb79d2"},"source":"Strange things. It looks as there are few clusters. Some kagglers found four [see here][1] . I found six :) But it is another story. Let check R2 on train data\n\n\n  [1]: https://www.kaggle.com/tilii7/four-blob-tsne-with-legal-supplements","cell_type":"markdown"},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"8d9b2cb0-17be-4def-afbc-4843c6b9408d","_execution_state":"idle","collapsed":false,"_uuid":"b469271da68d5971286447b2788aac6b37bf7d06"},"source":"SS_res, SS_tot, R2 = r2(y_train,model.predict(dtrain))\nprint('SS_res: %.0f  SS_tot: %.0f  R2: %0.5f' %(SS_res, SS_tot, R2))","execution_count":24},{"outputs":[],"cell_type":"markdown","metadata":{"_cell_guid":"4561a705-d773-4493-b2d5-0c5ebecae406","_execution_state":"idle","collapsed":false,"_uuid":"90c877ebfeabfe8598fa91b63e1822587b799168"},"source":"R2 looks well, but I believe this submission will have LB ~ 0.53..0.55 (you can check it yourself :)","execution_count":null},{"outputs":[],"cell_type":"markdown","metadata":{"_cell_guid":"b42c46f4-4384-4f17-a1ac-f37b870a0bdf","_execution_state":"idle","collapsed":false,"_uuid":"a3c80c960298fecfb930a402b9788d6716dd233e"},"source":"So here we can conclude that usage of tree-based estimators is not good idea. We need something else. KNeighborsRegressor with distance weights is good candidate.","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"d4f80416-579e-46fe-b979-6c0fa644c8a8","_execution_state":"idle","collapsed":false,"_uuid":"a4178fcee0928ac4515e8a6b605367893626d710"},"source":"knr = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n          metric_params=None, n_jobs=1, n_neighbors=19, p=2, weights='distance')\nknr.fit(x_train,y_train)\ny_pred = knr.predict(x_test)\nsub = pd.DataFrame({'id': df_test['ID'], 'y': y_pred})\nsub.head()","execution_count":25},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"0802ad9b-5d5a-442d-8cd2-9ac5b26c8e86","_execution_state":"idle","collapsed":false,"_uuid":"b54de4a37bb0095fa3fd88109eff8669d876d9a0"},"source":"# Let's look on predicting on train set\nz_train = df_train[df_train.y < 250].copy()\nz_train['err'] = y_train - knr.predict(x_train)\nz_train['y'] = y_train\nz_train['predict'] = knr.predict(x_train)\nfig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(15, 5)\nz_train.plot.scatter(x = 'y',y = 'predict', ax=axes[0], label = 'prediction')\nz_train.plot.scatter(x = 'y',y = 'y', color = 'Red',ax=axes[0], label = 'actual')\nz_train.plot.scatter(x = 'y',y = 'err', ax=axes[1], label = 'error')","execution_count":27},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"94e4e45d-b43a-45fb-9aa2-1f6af038704b","_execution_state":"idle","collapsed":false,"_uuid":"822567d04ce960bd1eb131d74f506d4d558f9e27"},"source":"SS_res, SS_tot, R2 = r2(y_train,knr.predict(x_train))\nprint('SS_res: %.0f  SS_tot: %.0f  R2: %0.5f' %(SS_res, SS_tot, R2))","execution_count":28},{"outputs":[],"cell_type":"code","metadata":{"trusted":false,"_cell_guid":"e60591f0-7073-4a27-8731-4033ffb50c5b","_execution_state":"idle","collapsed":false,"_uuid":"55a05cfe1712dc4a3d905d7f1af6220eb8bffb65"},"source":"sub.to_csv(\"knr.csv\", index = False)","execution_count":29},{"outputs":[],"cell_type":"markdown","metadata":{"_cell_guid":"304d5ebc-f476-4cdc-b9ea-b7a72a680d7a","_execution_state":"idle","collapsed":false,"_uuid":"37a90e47f6376583a6221c9c086c18aa7386ca32"},"source":"Bingo! But this submission has only LB = 0.35545  :(\n\nAny ideas? KNR is overfitted? Or there are external factors?  ","execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"}}}