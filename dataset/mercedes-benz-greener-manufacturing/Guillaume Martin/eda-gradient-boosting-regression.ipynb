{"cells":[{"cell_type":"markdown","outputs":[],"source":"# Mercedes-Benz Greener Manufacturing\n\nThis is a regression problem. We have to forecast the time spent testing from a set of variables","execution_count":null,"metadata":{"_uuid":"7cc42c890ffe62ff6c0c5354553501558fc0df8f"}},{"cell_type":"code","outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\n\n%matplotlib inline","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"28ef890d607ef53c39b428514c30d1109922736f"},"execution_count":1},{"cell_type":"markdown","outputs":[],"source":"## Load the data","execution_count":null,"metadata":{"_uuid":"fef89d600edd88120faadaa1519625cbce36687f"}},{"cell_type":"code","outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nprint(\"train: {}\".format(train.shape))\nprint(\"test: {}\".format(test.shape))\n\ntrain.head(10)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"2db03486f5d76545ea76a9fa0276c311affff1e1"},"execution_count":2},{"cell_type":"code","outputs":[],"source":"test.head(10)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"43c30e223f446eaf1fa87e20f7708e82e1354b18"},"execution_count":3},{"cell_type":"markdown","outputs":[],"source":"Both datasets have the same number of rows.\nThere might be some time order in the ids.","execution_count":null,"metadata":{"_uuid":"fc884983b3a221802613fd34eb1cf2b7eb828b82"}},{"cell_type":"markdown","outputs":[],"source":"## Target analysis","execution_count":null,"metadata":{"_uuid":"ed4eae6f5165813f85be9ac1b4042da3466ab815"}},{"cell_type":"code","outputs":[],"source":"train[\"y\"].dtype","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"322c0219c7aea1ba12fd762cc546b3295d5e86d0"},"execution_count":4},{"cell_type":"markdown","outputs":[],"source":"How are the values of y distributed?","execution_count":null,"metadata":{"_uuid":"bc78fd52bdaca5007921b9117de9ad5e3122891c"}},{"cell_type":"code","outputs":[],"source":"train[\"y\"].describe()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"a9187448c5390ee87dd80ae0ce272a5e78cfac3d"},"execution_count":5},{"cell_type":"code","outputs":[],"source":"plt.figure()\nplt.boxplot(train[\"y\"])\nplt.ylabel(\"y\")\nplt.show()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"0f856cfa90568ec2d81607dddbdeac1d914f7b2d"},"execution_count":6},{"cell_type":"markdown","outputs":[],"source":"The data seems to be skeewed. We have a lot of outliers.  \nLet's look at the distribution without the outliers.","execution_count":null,"metadata":{"_uuid":"97506dd4e8acc7df5e83b7a6a0ab44089ac077c6"}},{"cell_type":"code","outputs":[],"source":"# Get the IQR\nq75, q25 = np.percentile(train[\"y\"], [75, 25])\niqr = q75 - q25\n\nminimum = q25 - (iqr * 1.5)\nmaximum = q75 + (iqr * 1.5)\n\nprint(\"Minimum = %.2f\" % minimum)\nprint(\"Maximim = %.2f\" % maximum)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"bb1f36f7bc55782fc76bfb740ebe70c055aaac1d"},"execution_count":7},{"cell_type":"code","outputs":[],"source":"plt.figure()\nplt.boxplot(train[\"y\"][(train[\"y\"] >= minimum) & (train[\"y\"] <= maximum)])\nplt.ylabel(\"y\")\nplt.show()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"86da111d32183fedaa29439a39aa6087ea102f2b"},"execution_count":8},{"cell_type":"markdown","outputs":[],"source":"Is there any trend over time?  \nIf the tests are in chronological order, can we identify any trend in the test time?","execution_count":null,"metadata":{"_uuid":"502bfb14c5d043a90fe5750c8c700aedbe9dfa5f"}},{"cell_type":"code","outputs":[],"source":"plt.plot(train[\"ID\"], train[\"y\"])","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"40df3c0ed76460e77ff509c9ea5bbf31928ef174"},"execution_count":9},{"cell_type":"markdown","outputs":[],"source":"We can't see any trend.  \nLet's look at the moving averages to be sure.","execution_count":null,"metadata":{"_uuid":"c566c94ed15f445b8d276ee08af8f432e7dc974a"}},{"cell_type":"code","outputs":[],"source":"periods = [10,20,50,100]\n\nfig = plt.figure(figsize=(20,10))\n\nfor n in periods:\n    col = \"MA\" + str(n)\n    train[col] = train[\"y\"].rolling(window=n).mean()\n\nax1 = fig.add_subplot(411)\nax1.plot(train[\"ID\"], train[\"MA10\"])\n\nax2 = fig.add_subplot(412)\nax2.plot(train[\"ID\"], train[\"MA20\"])\n\nax3 = fig.add_subplot(413)\nax3.plot(train[\"ID\"], train[\"MA50\"])\n\nax4 = fig.add_subplot(414)\nax4.plot(train[\"ID\"], train[\"MA100\"])","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"1911e63566d8e038eafcec5f9b6ab1d9c2fb0c54"},"execution_count":10},{"cell_type":"markdown","outputs":[],"source":"As I increase the period of the MA, a small decreasing trend seems to appear after the ID 5000. \nThere's still a lot of noise and I'm not sure I can get anything from the MAs.  ","execution_count":null,"metadata":{"collapsed":true,"_uuid":"9ac28c6783a2af6a3d91580b377c8754b22df492"}},{"cell_type":"code","outputs":[],"source":"# The first n rows of the MAn columns have NA values. \n# We replace them by the average y\nfor col in [\"MA10\",\"MA20\",\"MA50\",\"MA100\"]:\n    train[col].fillna(train[\"y\"].mean(), inplace=True)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"7329158fc48dd1d2ff9ee54564ab4f6b27f13b9f"},"execution_count":11},{"cell_type":"markdown","outputs":[],"source":"## Features analysis\nThe features are anonymized. Their name starts with an **X** followed by a sequential number.  \nI won't get any information from the name.  \nLet's first look at what kind of data we have.  \nThe 2 following cells are running code form [anoka's kernel](https://www.kaggle.com/anokas/mercedes-eda-xgboost-starter-0-55)","execution_count":null,"metadata":{"_uuid":"ac924e98ae44d48381d4bfb193e12db3b66f0c9a"}},{"cell_type":"code","outputs":[],"source":"cols = [c for c in train.columns if 'X' in c]\nprint('Number of features: {}'.format(len(cols)))\n\nprint('Feature types:')\ntrain[cols].dtypes.value_counts()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"6db2498981e681a549290d63325fb5c35fae8141"},"execution_count":12},{"cell_type":"code","outputs":[],"source":"counts = [[], [], []]\nfor c in cols:\n    typ = train[c].dtype\n    uniq = len(np.unique(train[c]))\n    if uniq == 1: counts[0].append(c)\n    elif uniq == 2 and typ == np.int64: counts[1].append(c)\n    else: counts[2].append(c)\n\nprint('Constant features: {}\\nBinary features: {}\\nCategorical features: {}\\n'.format(*[len(c) for c in counts]))\n\nprint('Constant features:', counts[0])\nprint('Categorical features:', counts[2])","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"c20bbd6d67bd424315533d38615e98e1099e188d"},"execution_count":13},{"cell_type":"markdown","outputs":[],"source":"There are 12 constant variables. They won't bring anyhting to the training, but I'll keep them because they might not be constant in the test set.  \nThe features from \"X0\" to \"X8\" are categorical. We're looking at them first.\n\nThe binary features go from **X10** to **X385**. That's 375 features. However, we find only 356. 19 features have been dropped from the dataset.","execution_count":null,"metadata":{"_uuid":"de0acf9f4b891fa2956f19ffd3d6efa1017dccb2"}},{"cell_type":"markdown","outputs":[],"source":"### Categorical features","execution_count":null,"metadata":{"_uuid":"3d49823b583e9758bd3e9c494d3b63a9df912c7a"}},{"cell_type":"code","outputs":[],"source":"cat_feat = counts[2]\ntrain[cat_feat].head()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"fea8e44caf6d775f237c2e3b4b00d6ecb4aeb963"},"execution_count":14},{"cell_type":"markdown","outputs":[],"source":"The features values are letters that should just be the real values encoded.  \nLet's their relationship with the target   ","execution_count":null,"metadata":{"_uuid":"79e21c71f37ab5e836d54ce92f8a7fdf6398e663"}},{"cell_type":"code","outputs":[],"source":"fig, ax = plt.subplots(8, 1, figsize=(30,40))\nfor c in cat_feat:\n    axis = ax[cat_feat.index(c)]\n    ax2 = axis.twinx()\n    \n    # plot with the outliers\n    # sns.boxplot(x=train[c], y=train[\"y\"], color=\"c\", ax=ax[cat_feat.index(c)])\n    \n    # plot without the outiers\n    sns.boxplot(x=train[c], y=train[\"y\"][(train[\"y\"] >= minimum) & (train[\"y\"] <= maximum)], color=\"c\", ax=axis)\n    sns.countplot(x=train[c], alpha=0.3, color=\"c\", ax=ax2)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"6e16ef43876d0cfa7c86148711300e89ea5e2268"},"execution_count":15},{"cell_type":"markdown","outputs":[],"source":"Here, we have one chart per categorical feature.  \nThe boxplots show us the distribution of the target for each value of the feature.  \nThe bars show us the frequency of each value of the features.  \nMost features have values that are more represented. Only X8 has all values represented.  \nThe distribution of the target values doesn't change a lot across the different values of features X3, X5, X6 and X8. \n","execution_count":null,"metadata":{"_uuid":"f01b64a2fd1ffea9598869af3ef4dd64d2415322"}},{"cell_type":"markdown","outputs":[],"source":"### Binary features","execution_count":null,"metadata":{"_uuid":"24b2e993020b1f603b14be37be8c06faa8f8d6cb"}},{"cell_type":"code","outputs":[],"source":"binary_features = counts[1]\nlen(binary_features)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"ff8673ec9045624518dd81df61266a6b22ada352"},"execution_count":16},{"cell_type":"code","outputs":[],"source":"train[\"total\"] = train[binary_features].sum(axis=1)","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"5820728bdfed73822792b23f2044883796686e59"},"execution_count":17},{"cell_type":"code","outputs":[],"source":"train.head()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"dbb989454c350005b77e451e3a0dcecbc1ae451e"},"execution_count":18},{"cell_type":"code","outputs":[],"source":"print(train[\"total\"].describe())\nplt.boxplot(train[\"total\"])\nplt.show()","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"24f00f019a387091945d60b0c378100b6ce94ecb"},"execution_count":19},{"cell_type":"code","outputs":[],"source":"plt.scatter(train[\"total\"], train[\"y\"], alpha=0.1)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"57fbfe1c6391dc83fc697f69c6a126df8de2bd2a"},"execution_count":20},{"cell_type":"code","outputs":[],"source":"# Try again without the outliers\nplt.scatter(train[\"total\"][(train[\"y\"] >= minimum) & (train[\"y\"] <= maximum)], \n            train[\"y\"][(train[\"y\"] >= minimum) & (train[\"y\"] <= maximum)], \n            alpha=0.1)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"3e1909734d67e4e0cf2a5b10aae519b6d0bedc73"},"execution_count":21},{"cell_type":"code","outputs":[],"source":"# Limit total values to IQR\nplt.scatter(train[\"total\"][(train[\"y\"] >= minimum) & (train[\"y\"] <= maximum) & (train[\"total\"] >= 53) & (train[\"total\"] <= 63)], \n            train[\"y\"][(train[\"y\"] >= minimum) & (train[\"y\"] <= maximum) & (train[\"total\"] >= 53) & (train[\"total\"] <= 63)], \n            alpha=0.1)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"87ba47b6e3cf24ea5697060395a900a63b99fac8"},"execution_count":22},{"cell_type":"markdown","outputs":[],"source":"The number of selected binary features doesn't seem to have an impact on the test time.","execution_count":null,"metadata":{"_uuid":"b9e88954d5ffb98104a8f0d7a10f2d23fbaa0513"}},{"cell_type":"markdown","outputs":[],"source":"## Features preprocessing","execution_count":null,"metadata":{"_uuid":"162a67eb5097ec0c9e8f9a6cfb0675f5336156d4"}},{"cell_type":"code","outputs":[],"source":"test[\"total\"] = test[binary_features].sum(axis=1)\ntrain.drop([\"MA10\", \"MA20\", \"MA50\", \"MA100\"],axis=1, inplace=True)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"7cd7de6b814e6b58056303ff114b55fa78412b0e"},"execution_count":23},{"cell_type":"code","outputs":[],"source":"def dummify(df, columns, drop=True):\n    ''' add dummy variables columns to a dataframe\n    \n    parameters\n    ----------\n    df: dataframe\n        the dataframe that need to be modified\n        \n    columns: list\n        a list of column names for which we'll create dummy variables\n        \n    drop: boolean (default=True)\n        True to drop the original column\n            \n    return\n    ------\n        a dataframe with extra dummy variables\n    '''\n    \n    for column in columns:\n        df_dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df,df_dummies], axis=1)\n        if drop == True:\n            df.drop([column], inplace=True, axis=1)\n    \n    return df\n\n\ndef add_missing_dummy_columns(df, columns):\n    ''' add missing dummy columns to a dataframe\n    \n        If a categorical feature in the test set doesn't\n        have as many values than the same feature in the \n        train set, the two dataframes will not have the\n        same number of dummy columns.\n        \n        This function add the dummy columns that are missing\n        and fill them with zeros\n        \n    parameters\n    ----------\n    df: dataframe\n        The dataframe with missing columns\n    \n    columns: list\n        The complete list of dummy columns\n    '''\n    \n    missing_cols = set(columns) - set(df.columns)\n    for c in missing_cols:\n        df[c] = 0","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"23b2aa53d1aa2058542640de5fc64293f5250f46"},"execution_count":24},{"cell_type":"markdown","outputs":[],"source":"Create dummy variables for categorical features","execution_count":null,"metadata":{"_uuid":"9c92eaa580b63df2f38fa8c1359f8a1681bb348b"}},{"cell_type":"code","outputs":[],"source":"# We save the list of columns before we create new dummy columns\n# to get the list of columns that have been created\nold_col_train = list(train.drop(cat_feat, axis=1).columns)\nold_col_test = list(test.drop(cat_feat, axis=1).columns)\n\n# We create dummy variables for the train and test sets\ntrain = dummify(train, cat_feat, True)\ntest = dummify(test, cat_feat, True)\n\n# We list all the new columns. Those are the dummy columns\n# that should appear in both dataframes\nnew_col_train = [c for c in list(train.columns) if c not in old_col_train]\nnew_col_test = [c for c in list(test.columns) if c not in old_col_test]\n\n# Finally, we add the missing columns in both dataframes\nadd_missing_dummy_columns(test, new_col_train)\nadd_missing_dummy_columns(train, new_col_test)\n","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"abae216f4fb505951226604ff639c4954266923d"},"execution_count":25},{"cell_type":"code","outputs":[],"source":"# We control that both df have the same shape\nprint(\"Train: {}\".format(train.shape))\nprint(\"Test: {}\".format(test.shape))","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"4857b0b7a7ddee2f2f21fcea6907af436ade0e10"},"execution_count":26},{"cell_type":"markdown","outputs":[],"source":"The difference should be the target column","execution_count":null,"metadata":{"_uuid":"0557bf84628cb25d5f5d8a66091ca7147f3d68d4"}},{"cell_type":"markdown","outputs":[],"source":"## Modeling","execution_count":null,"metadata":{"_uuid":"ae329de6e81081ee14553852c061b4a3eddd72e5"}},{"cell_type":"code","outputs":[],"source":"model = GradientBoostingRegressor()","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"2411637459ea0aeeadb5e1ff804824950fba5284"},"execution_count":27},{"cell_type":"code","outputs":[],"source":"X = train.drop([\"ID\",\"y\"], axis=1)\ny = train[\"y\"]","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"242b651d3665e75d4cfd53a7e2b5a56669ec5a9f"},"execution_count":28},{"cell_type":"code","outputs":[],"source":"def plot_learning_curves(estimator, X, y, scoring=\"accuracy\", cv=None, n_jobs=1, train_sizes=np.linspace(0.1,1.0,5)):\n    \"\"\" Generate a plot showing training and test learning curves\n        source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n\n    Parameters\n    ----------\n    estimator: object type\n        the estimator that will be used to implement \"fit\" and \"predict\"\n\n    X: array, shape(n_samples, m_features)\n        Training vector\n\n    y: array, shape(n_samples)\n        Target relative to X\n\n     scoring:string\n        The scoring method   \n\n    cv: int\n        Cross-validation splitting strategy\n\n    n_jobs: int\n        Number of jobs to run in parallel\n\n    train_sizes: array, shape(n_ticks)\n        Number of training examples that will be used to generate\n        the learning curve\n    \"\"\"\n\n    plt.figure()\n    plt.title(\"Learning Curves\\n\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score ({})\".format(scoring))\n    plt.legend(loc=\"best\")\n    plt.grid()\n\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1,\n                     color=\"g\")\n\n    plt.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\",\n             label=\"Training score\")\n\n    plt.plot(train_sizes, test_scores_mean, \"o-\", color=\"g\", \n            label=\"Cross-validation score\")\n\n    plt.show()","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"c6036a198f85ee540ffd1e6b75e894d5677caf4e"},"execution_count":29},{"cell_type":"code","outputs":[],"source":"plot_learning_curves(model, X, y, scoring=\"R2\", cv=10, n_jobs=4)","metadata":{"scrolled":false,"_execution_state":"idle","collapsed":false,"_uuid":"0adeb7c072f6aa6d084ea31d25f8e90135b0571a"},"execution_count":30},{"cell_type":"markdown","outputs":[],"source":"The learning curves look ok. There's no sign of overfitting.  \nWe fit the model and then we can try to do some predictions.","execution_count":null,"metadata":{"_uuid":"05901f9f37c957835e978a2b08632cde6fafdca8"}},{"cell_type":"code","outputs":[],"source":"model.fit(X, y)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"4380aa9e4acdd1010a0801be7d11ef0629abaf25"},"execution_count":31},{"cell_type":"markdown","outputs":[],"source":"# Predict and Submit","execution_count":null,"metadata":{"_uuid":"1d10f140ad539db7ddc1c2619286daffb545b6a0"}},{"cell_type":"code","outputs":[],"source":"y_pred = model.predict(test.drop(\"ID\", axis=1))\n\nsubmission = pd.DataFrame()\nsubmission[\"ID\"] = test[\"ID\"].values\nsubmission[\"y\"] = y_pred\nsubmission.to_csv(\"gbr-2017-06-17.csv\", index=False)","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"4011fa0b4653a29e83cb49febdca0287dc40286f"},"execution_count":32},{"cell_type":"markdown","outputs":[],"source":"Here we are. \nThis is the first kernel I share on Kaggle. Critics are most welcome.\n","execution_count":null,"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"c8d4a4ca598dc5c30fc8d3507cfff53767b1ef1c"}}],"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.0"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4}