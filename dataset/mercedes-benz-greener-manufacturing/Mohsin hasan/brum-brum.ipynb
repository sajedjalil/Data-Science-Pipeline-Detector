{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c0d63495-1122-9e6e-4717-aa4c7d680574"},"source":"Cruise through data :D"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19b8cb04-ae3c-a442-f346-d9bc81069aba"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6816831-4f83-2c95-b6e4-4fc7d6fba616"},"outputs":[],"source":"#Read data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(train.shape, test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dee2f459-0a52-632e-e96c-33f880de2159"},"outputs":[],"source":"pd.set_option('display.max_rows', 500)\ntrain.sample(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c19b0ba1-9fcc-982b-b05c-857ad0cb2eed"},"outputs":[],"source":"cat_data = train.select_dtypes(include=['object'])\ncat_cols = cat_data.columns\nprint(cat_cols)\ncat_data.head()\n#Interesting labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c48c49f-9d7d-e01b-04aa-29d9f55ca93b"},"outputs":[],"source":"#Lets get binary variables\ntrain.apply(lambda x: x.unique().shape[0])\n#All columns from X10 -X385 are binary\n#Basically, all variables are cardinal"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed9becac-59e8-cf7d-8de4-83d104ab1413"},"outputs":[],"source":"#Lets encode categoricals to numerical labels\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nlb = LabelEncoder()\nlb_dict = defaultdict(LabelEncoder)\nall_data = pd.concat([train, test])\n\nfor col in cat_cols:\n    lb_dict[col] = lb.fit(all_data[col])\n    train[col + '_mod'] = lb_dict[col].transform(train[col])\n    test[col + '_mod'] = lb_dict[col].transform(test[col])\n    \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95a4e84a-7a21-4740-c114-059f4efae883"},"outputs":[],"source":"\n#Set train and test arrays\nfeats = [f for f in train.columns if f not in list(cat_cols) + ['y','ID']]\n\nX_train, X_test = train[feats], test[feats]\ny_train = train['y']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a7aa247-2725-bd9f-8d0a-219a3be6b5c3"},"outputs":[],"source":"#Extra tree regressor \n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify=X_train, test_size=0.2, random_state=1)\n\netr = ExtraTreesRegressor(n_estimators=500, n_jobs=-1, max_depth=4, random_state=1)\netr.fit(X_tr, y_tr)\nprint(etr.score(X_val, y_val))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2dddf27-9c5e-792e-1d32-a69febe0232f"},"outputs":[],"source":"importances = etr.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in etr.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1][:50]\n\nplt.figure(figsize=(18,16))\nplt.bar(range(len(indices)), importances[indices], color=\"black\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), np.array(feats)[indices], rotation=90)\nplt.show() #Paint it black"},{"cell_type":"markdown","metadata":{"_cell_guid":"b3ece560-2425-f497-d0b3-44ef5b16401d"},"source":"**X314, X315, seem to be best features** - Will need to do some work to get value out of other binary ones"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0f35f82-7c45-002d-16ec-62cedc4c8a79"},"outputs":[],"source":"sns.boxplot(x='X314', y='y', data=train)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56de0d23-0a2b-af31-badf-4e61ac66e5e4"},"outputs":[],"source":"sns.boxplot(x='X315', y='y', data=train)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b43b3765-4967-7026-ed0d-6c7182da86d8"},"outputs":[],"source":"sns.boxplot(x='X47', y='y', data=train)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"940cbd54-16a3-428d-aacf-3e879d6f286c"},"outputs":[],"source":"plt.hist(train.y, bins=100)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"020fa010-8eab-469a-230d-bb27e12e53de"},"outputs":[],"source":"from sklearn import metrics\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    # return a pair metric_name, result\n    # since preds are margin(before logistic transformation, cutoff at 0)\n    return 'error', metrics.r2_score(labels, preds)\nmodel_xgb = XGBRegressor(n_estimators=1500, learning_rate=0.02, max_depth=4)\n\nmodel_xgb.fit(X_tr, y_tr, verbose=10, eval_set=[(X_val, y_val)], eval_metric=evalerror)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a59c9a02-d65a-83f7-b990-e9f07bd90762"},"outputs":[],"source":"from sklearn.model_selection import StratifiedKFold:\n "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"784aaddf-7fc1-8972-2888-c2d7c742f90b"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}