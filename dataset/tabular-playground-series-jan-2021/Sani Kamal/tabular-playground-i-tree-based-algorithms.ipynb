{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Tabular Playground Series - Jan 2021</h1>"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:#45ffa3;text-align:left;color:#aa45ff\">Contents</h1>\n\n- Basic Data Analysis and Visualization\n- Linear Algorithms\n- Tree Based Algorithms\n    - Decision Tree\n    - Random Forest\n    - Gradient Boosting (GBM)\n    - XGboost"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Importing required libraries</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor,XGBRFRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Reading the data</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv',index_col='id')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv',index_col='id')\ndf_sub = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv',index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# get the number of missing data points per column\ndf_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Bivariate Analysis</h2>\n\n**Bivariate analysis** is the simultaneous analysis of two variables (attributes). It explores the concept of relationship\nbetween two variables, whether there exists an association and the strength of this association, or whether there are\ndifferences between two variables and the significance of these differences."},{"metadata":{},"cell_type":"markdown","source":"### Scatter plot of features vs. target\n\nScatter plot of each feature in train vs. target values."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_feature_target_scatter(df, features):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(5, 3,figsize=(14, 24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5, 3, i)\n        plt.scatter(df[feature], df['target'], marker='+', color='purple')\n        plt.xlabel(feature, fontsize=9)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features = ['cont1', 'cont2','cont3','cont4', 'cont5', 'cont6', 'cont7',\n            'cont8', 'cont9','cont10','cont11', 'cont12', 'cont13', 'cont14']\n\nplot_feature_target_scatter(df_train[::15], features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_feature_distribution(df1, df2, features):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(5, 3,figsize=(14, 24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5, 3,i)\n        sns.distplot(df1[feature],color=\"orange\", kde=True,bins=120, label='train')\n        sns.distplot(df2[feature],color=\"purple\", kde=True,bins=120, label='test')\n        plt.xlabel(feature, fontsize=9); plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_distribution(df_train[::15],df_test[::10], features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features correlation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 16))\nheatmap = sns.heatmap(np.round(df_train[features].corr(), 3), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features correlation', fontdict={'fontsize':10}, pad=10)\nplt.title(\"Spearman correlation - test data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"features_target = features + ['target']\nplt.figure(figsize=(16, 16))\nheatmap = sns.heatmap(np.round(df_train[features_target].corr(), 3), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features correlation', fontdict={'fontsize':10}, pad=10)\nplt.title(\"Spearman correlation - train data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_train.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(df_train, target, train_size=0.60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Linear Regression</h2>\n\nLinear regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables.\n\nIn Multiple linear regression more than one predictor variables are used to predict the response variable."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(15,8)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    sns.scatterplot(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model = LinearRegression(fit_intercept=False)\nlinear_model.fit(X_train, y_train)\ny_linear = linear_model.predict(X_test)\nscore_linear = mean_squared_error(y_test, y_linear, squared=False)\nprint(f'{score_linear:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results('Linear',y_test,y_linear)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Lasso Regression Model</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_model = Lasso(fit_intercept=False)\nlasso_model.fit(X_train, y_train)\ny_lasso = lasso_model.predict(X_test)\nscore_lasso = mean_squared_error(y_test, y_lasso, squared=False)\nprint(f'{score_lasso:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results('Lasso',y_test,y_lasso)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Introduction to Tree Based Algorithms</h2>\n\nTree based algorithms are considered to be one of the best and mostly used supervised learning methods. Tree based algorithms empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. They are adaptable at solving any kind of problem at hand (classification or regression).\nMethods like decision trees, random forest, gradient boosting are being popularly used in all kinds of data science problems. "},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">What is a Decision Tree ?</h2>\n\nDecision tree is a type of supervised learning algorithm that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables."},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Types of Decision Trees</h2>\n\nTypes of decision tree is based on the type of target variable we have. It can be of two types:\n\n1. **Categorical Variable Decision Tree:** Decision Tree which has categorical target variable then it called as categorical variable decision tree.\n2. **Continuous Variable Decision Tree:** Decision Tree has continuous target variable then it is called as Continuous Variable Decision Tree.\n"},{"metadata":{},"cell_type":"markdown","source":"<h3>Basic terminology used with Decision trees:</h3>\n\n- **Root Node:** It represents entire population or sample and this further gets divided into two or more homogeneous sets.\n- **Splitting:** It is a process of dividing a node into two or more sub-nodes.\n- **Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.\n- **Leaf/ Terminal Node:** Nodes do not split is called Leaf or Terminal node.\n- **Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n- **Branch / Sub-Tree:** A sub section of entire tree is called branch or sub-tree.\n- **Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node."},{"metadata":{},"cell_type":"markdown","source":"<h3>Advantages and Disadvantages</h3>\n\n**Advantages**\n\n- Easy to Understand\n- Useful in Data exploration\n- Less data cleaning required\n- Data type is not a constraint\n- Non Parametric Method\n\n**Disadvantages**\n\n- Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning"},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Decision Tree Regressor Model</h2>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dtree_model = DecisionTreeRegressor(random_state=0)\ndtree_model.fit(X_train, y_train)\ny_dtree = dtree_model.predict(X_test)\nscore_dtree = mean_squared_error(y_test, y_dtree, squared=False)\nprint(f'{score_dtree:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results('Decision Tree',y_test,y_dtree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Random Forest Regressor Model</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = RandomForestRegressor(n_estimators=50, n_jobs=-1)\nrf_model.fit(X_train, y_train)\ny_rf = rf_model.predict(X_test)\nscore_dtree = mean_squared_error(y_test, y_rf, squared=False)\nprint(f'{score_dtree:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results('Random Forest',y_test,y_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Gradient Boosting Regressor Model</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_model = GradientBoostingRegressor(n_estimators=100,max_depth=5)\ngb_model.fit(X_train, y_train)\ny_gb = gb_model.predict(X_test)\nscore_gb = mean_squared_error(y_test, y_gb, squared=False)\nprint(f'{score_gb:0.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results('Gradient Boosting',y_test,y_gb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">XGBoost (eXtreme Gradient Boosting)</h2>\n\nXGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. Itâ€™s feature to implement parallel computing makes it at least 10 times faster than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor(n_jobs = -1)\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           cv = 5,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n    return gsearch.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperParameterTuning(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(\n        objective = 'reg:squarederror',\n        colsample_bytree = 0.5,\n        learning_rate = 0.05,\n        max_depth = 6,\n        min_child_weight = 1,\n        n_estimators = 1000,\n        subsample = 0.7)\n\nxgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)], verbose=False)\n\ny_pred_xgb = xgb_model.predict(X_test)\n\nmae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n\nprint(\"MAE: \", mae_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Submission</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dtree_model = DecisionTreeRegressor()\n# dtree_model.fit(df_train, target)\n# df_sub['target'] = dtree_model.predict(df_test)\n# df_sub.to_csv('dtree_submission.csv')\n\n# model = RandomForestRegressor(n_estimators=50, n_jobs=-1)\n# model.fit(df_train, target)\n# df_sub['target'] = model.predict(df_test)\n# df_sub.to_csv('submission.csv')\n\n# model=GradientBoostingRegressor(n_estimators=100,max_depth=5)\n# model.fit(df_train, target)\n# df_sub['target'] = model.predict(df_test)\n# df_sub.to_csv('submission.csv')\n\n\ndf_sub['target'] = xgb_model.predict(df_test)\ndf_sub.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style=\"background-color:#45ffa3;text-align:center;color:#aa45ff\">Reference Notebook</h2>\n\n- [Tabular Playground Series January EDA](https://www.kaggle.com/gpreda/tabular-playground-series-january-eda)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}