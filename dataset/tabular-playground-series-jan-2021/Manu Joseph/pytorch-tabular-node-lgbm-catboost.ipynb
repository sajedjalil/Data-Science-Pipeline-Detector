{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is PyTorch Tabular?\n\n![PyTorch Tabular](https://deepandshallowml.files.wordpress.com/2021/01/pytorch_tabular_logo.png)\n\nPyTorch Tabular is a framework/ wrapper library which aims to make Deep Learning with Tabular data easy and accessible to real-world cases and research alike. The core principles behind the design of the library are:\n\n- Low Resistance Usability\n- Easy Customization\n- Scalable and Easier to Deploy\n\nInstead of starting from scratch, the framework has been built on the shoulders of giants like **PyTorch**(obviously), and **PyTorch Lightning**.\n\nIt also comes with state-of-the-art deep learning models that can be easily trained using pandas dataframes.\n\nThe high-level config driven API makes it very quick to use and iterate. You can just use a **pandas dataframe** and all of the heavy lifting for normalizing, standardizing, encoding categorical features, and preparing the dataloader is handled by the library.\n\nThe `BaseModel` class provides an easy to extend abstract class for implementing custom models and still leverage the rest of the machinery packaged with the library.\nState-of-the-art networks like **Neural Oblivious Decision Ensembles(NODE)** for Deep Learning on Tabular Data, and **TabNet**: Attentive Interpretable Tabular Learning are implemented. See examples from the [documentation](https://pytorch-tabular.readthedocs.io/en/latest/) for how to use them.\n\nBy using PyTorch Lightning for the training, PyTorch Tabular inherits the flexibility and scalability that Pytorch Lightning provides\n\n- GitHub: [https://github.com/manujosephv/pytorch_tabular](https://github.com/manujosephv/pytorch_tabular)\n- Documentation: [https://pytorch-tabular.readthedocs.io/en/latest/](https://pytorch-tabular.readthedocs.io/en/latest/)\n- Accompanying Blog: [PyTorch Tabular â€“ A Framework for Deep Learning for Tabular Data](https://deep-and-shallow.com/2021/01/27/pytorch-tabular-a-framework-for-deep-learning-for-tabular-data/)\n"},{"metadata":{},"cell_type":"markdown","source":"# How to use PyTorch Tabular?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# install PyTorch Tabular first\n!pip install pytorch_tabular\n# This is for a custom optimizer. PyTorch Tabular is flexible enough to use custom optimizers\n!pip install torch_optimizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# NODE and ML tools\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom pytorch_tabular import TabularModel\nfrom pytorch_tabular.models import CategoryEmbeddingModelConfig, NodeConfig, TabNetModelConfig\nfrom pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\nfrom pytorch_tabular.categorical_encoders import CategoricalEmbeddingTransformer\nfrom torch_optimizer import QHAdam\nimport category_encoders as ce\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading and PreProcessing the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load training data\ndf_train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ndisplay(df_train.head())\n# load test data\ndf_test = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\ndisplay(df_test.head())\n\nfeatures = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5',\n            'cont6', 'cont7', 'cont8', 'cont9', 'cont10',\n            'cont11', 'cont12', 'cont13', 'cont14']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Binning the Continuous Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer\nenc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy=\"quantile\")\nenc.fit(df_train[features])\nbinned_df_train = enc.transform(df_train[features])\nbinned_df_test = enc.transform(df_test[features])\nfor i, feature in enumerate(features):\n    df_train[f\"{feature}_binned\"] = binned_df_train[:,i]\n    df_test[f\"{feature}_binned\"] = binned_df_test[:,i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining the configs for the data, training, model, and optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_configs(train):\n    epochs = 25\n    batch_size = 512\n    steps_per_epoch = int((len(train)//batch_size)*0.9)\n    data_config = DataConfig(\n        target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n        continuous_cols=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n           'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'],\n        categorical_cols=['cont1_binned', 'cont2_binned', 'cont3_binned',\n       'cont4_binned', 'cont5_binned', 'cont6_binned', 'cont7_binned',\n       'cont8_binned', 'cont9_binned', 'cont10_binned', 'cont11_binned',\n       'cont12_binned', 'cont13_binned', 'cont14_binned'],\n        continuous_feature_transform=\"quantile_normal\"\n    )\n    trainer_config = TrainerConfig(\n        auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n        batch_size=batch_size,\n        max_epochs=epochs,\n        gpus=1, #index of the GPU to use. 0, means CPU\n    )\n    optimizer_config = OptimizerConfig(lr_scheduler=\"OneCycleLR\", lr_scheduler_params={\"max_lr\":0.005, \"epochs\": epochs, \"steps_per_epoch\":steps_per_epoch})\n    # model_config = CategoryEmbeddingModelConfig(\n    #     task=\"regression\",\n    #     layers=\"200-100\",  # Number of nodes in each layer\n    #     activation=\"ReLU\", # Activation between each layers\n    #     learning_rate = 1e-3,\n    #     batch_norm_continuous_input=True,\n    #     use_batch_norm =True,\n    #     dropout=0.0,\n    #     embedding_dropout=0.0,\n    #     initialization=\"kaiming\"\n    # )\n\n    model_config = NodeConfig(\n        task=\"regression\",\n        num_layers=2, # Number of Dense Layers\n        num_trees=1024, #Number of Trees in each layer\n        depth=6, #Depth of each Tree\n        embed_categorical=True, #If True, will use a learned embedding, else it will use LeaveOneOutEncoding for categorical columns\n        learning_rate = 1e-3,\n        target_range=[(df_train[col].min(),df_train[col].max()) for col in ['target']]\n    )\n    return data_config, trainer_config, optimizer_config, model_config","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validated Bagging Model Run\n\nHere, I am running a 5 fold validation and training the model on these five folds and predicting on the test set.\n\nThe models are:\n1. NODE\n2. LGBM\n3. CatBoost\n\nFor LGBM and CatBoost, **we use the categorical encoding which was trained as part of NODE for categorical binned columns**. This is easily done using **PyTorch Tabular**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# random seeds\nrnd_seed_cv = 1234\nrnd_seed_reg = 1234\n# cross validation\nkf = KFold(n_splits=5, random_state=rnd_seed_cv, shuffle=True)\ndf_train.drop(columns='id', inplace=True)\ndf_test.drop(columns='id', inplace=True)\ndf_test['target'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def node(train, valid, df_test):\n    data_config, trainer_config, optimizer_config, model_config = get_configs(train)\n    tabular_model = TabularModel(\n        data_config=data_config,\n        model_config=model_config,\n        optimizer_config=optimizer_config,\n        trainer_config=trainer_config\n    )\n    # fit model\n    tabular_model.fit(train=train, validation=valid, optimizer=QHAdam, \n                  optimizer_params={\"nus\": (0.7, 1.0), \"betas\": (0.95, 0.998)})\n    result = tabular_model.evaluate(valid)\n    return np.sqrt(result[0][\"test_mean_squared_error\"]), tabular_model.predict(valid)[\"target_prediction\"].values, tabular_model.predict(df_test)[\"target_prediction\"].values, tabular_model\n\ndef lgbm(train, valid, df_test):\n    lgb_model = LGBMRegressor(n_estimators=10000,\n                              learning_rate=0.005,\n                              early_stopping_rounds=50,\n                          feature_pre_filter = False,\n                          num_leaves=102, \n                          min_child_samples=20,\n                          colsample_bytree = 0.4,\n                          subsample = 1,\n                          subsample_freq = 0,\n                          lambda_l1 = 4.6,\n                          lambda_l2 = 1.9,\n                          random_state=42)\n    lgb_model.fit(train.drop(columns='target'), train['target'], eval_set=(valid.drop(columns='target'),valid.loc[:,'target']))\n    lgb_preds = lgb_model.predict(valid.drop(columns='target'))\n    score = mean_squared_error(valid['target'].values, lgb_preds, squared=False)\n    return score, lgb_model.predict(valid.drop(columns='target')), lgb_model.predict(df_test.drop(columns='target'))\n\ndef cb(train, valid, df_test):\n    best_params = {\n    'grow_policy': 'Lossguide', \n    'boosting_type': 'Plain', \n    'depth': 20, \n    'l2_leaf_reg': 3.699746597668451,\n    'min_data_in_leaf': 4,\n    'random_strength': 4.9263987954247455, \n    'rsm': 1.0,\n#     \"eval_metric\": \"RMSE:hints=skip_train~false\",\n    \"n_estimators\": 10000,\n    \"learning_rate\": 0.5,\n    \"od_type\": \"Iter\",\n    \"od_wait\": 50,\n    \n}\n    catboost_model = CatBoostRegressor(**best_params)\n    catboost_model.fit(train.drop(columns='target'), train['target'], eval_set=(valid.drop(columns='target'),valid.loc[:,'target']))\n    cb_preds = catboost_model.predict(valid.drop(columns='target'))\n    score = mean_squared_error(valid['target'].values, cb_preds, squared=False)\n    return score, catboost_model.predict(valid.drop(columns='target')), catboost_model.predict(df_test.drop(columns='target'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train\n\nCV_node = []\nCV_lgb = []\nCV_cb = []\npreds_train_node = []\npreds_train_lgb = []\npreds_train_cb = []\npreds_test_node = []\npreds_test_lgb = []\npreds_test_cb = []\ncross_validated_preds = []\n\nt1 = time.time()\nfor train_index, test_index in kf.split(df_train):\n    train = df_train.iloc[train_index]\n    valid = df_train.iloc[test_index]\n    cv_val = valid.copy()\n    #NODE\n    node_score, node_train_pred, node_test_pred, tabular_model = node(train, valid, df_test)\n    CV_node.append(node_score)\n    cv_val['pred_node'] = node_train_pred\n    preds_train_node.append(node_train_pred)\n    preds_test_node.append(node_test_pred)\n    # Using the trained Embeddings to replace categorical features\n    transformer = CategoricalEmbeddingTransformer(tabular_model)\n    train_transform = transformer.fit_transform(train)\n    val_transform = transformer.transform(valid)\n    df_test_transform = transformer.transform(df_test)\n    #LGBM\n    lgbm_score, lgbm_train_pred, lgbm_test_pred = lgbm(train_transform, val_transform, df_test_transform)\n    CV_lgb.append(lgbm_score)\n    cv_val['pred_lgb'] = lgbm_train_pred\n    preds_train_lgb.append(lgbm_train_pred)\n    preds_test_lgb.append(lgbm_test_pred)\n    #Catboost\n    cb_score, cb_train_pred, cb_test_pred = cb(train_transform, val_transform, df_test_transform)\n    CV_cb.append(cb_score)\n    cv_val['pred_cb'] = cb_train_pred\n    preds_train_cb.append(cb_train_pred)\n    preds_test_cb.append(cb_test_pred)\n    cross_validated_preds.append(cv_val)\nt2 = time.time()\nprint('Elapsed time [s]: ', t2-t1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Validation performance\nprint('CV performance [RMSE]: ', np.mean(CV_node, axis=0))\nprint('CV performance [RMSE]: ', np.mean(CV_lgb, axis=0))\nprint('CV performance [RMSE]: ', np.mean(CV_cb, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_pred_df = pd.concat(cross_validated_preds, sort=False)\ncross_val_pred_df.to_csv(\"cross_val_preds.csv\")\nimport joblib\njoblib.dump(preds_test_node, \"preds_test_node.sav\")\njoblib.dump(preds_test_lgb, \"preds_test_lgb.sav\")\njoblib.dump(preds_test_cb, \"preds_test_cb.sav\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weighted Average the Predictions\n\nThe weights are derived by running Linear Regression on the Cross Validated Predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_cb_pred = np.mean(preds_test_cb, axis=0)\navg_lgb_pred = np.mean(preds_test_lgb, axis=0)\navg_node_pred = np.mean(preds_test_node, axis=0)\npred_test = np.average([avg_node_pred, avg_lgb_pred, avg_cb_pred], axis=0, weights=[-0.15447081,  1.1021915 ,  0.06145868])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission\ndf_sub = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')\ndf_sub.target = pred_test\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save to file for submission\ndf_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}