{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nAs displayed by the EDA per https://www.kaggle.com/gvyshnya/using-autoviz-to-build-a-comprehensive-eda , *cont2* and *cont14* seem to have a nice separation of values into relatively contained clusters vs. the values of *target* in the training set. It leads to the hypothesis on a statiscially meaningful clustering of the observations in the training and testing sets for this competition within the 2-dimentional affinity space of *cont2*X*cont14* \n\n*Notes*: \n- we are going to use KMeans clustering and Euclidian distance metric in *cont2*X*cont14* space to find the optimal clustering break-down\n- there had been experiments with density clustering approach (namely, with *DBSCAN* method) but they did not work well for this dataset"},{"metadata":{},"cell_type":"markdown","source":"# Preparation Activities\n\nFirst of all, we are going to do a few usual preparation steps\n\n- import the packages we need to work with in the course of the current analytical effort\n- read the competion data in memory for future manipulations"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt\nfrom typing import Tuple, List, Dict\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.offline\n\n\n# read data\nin_kaggle = True\n\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '../input/tabular-playground-series-jan-2021/train.csv'\n        test_path = '../input/tabular-playground-series-jan-2021/test.csv'\n        sample_submission_path = '../input/tabular-playground-series-jan-2021/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data/train.csv'\n        test_path = 'data/test.csv'\n        sample_submission_path = 'data/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# get the training set and labels\ntrain_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n\ndf_train = pd.read_csv(train_set_path)\ndf_test = pd.read_csv(test_set_path)\n\nsubm = pd.read_csv(sample_subm_path)\n\n# list of feature columns\nfeature_list = [col for col in df_train.columns if col.startswith('cont')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before moving on with the clustering experiments, we will check the basic info about our training dataset (records count, data types of variables, % of missing values etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KMeans Clustering Experiments\n\nNow, we are ready to create a subset of the training set to use in the clustering experiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Let's cluster the observations \n\nclustering_cols = ['cont2', 'cont14']\n\n# subset of training set for the clustering experiment\nX = df_train.filter(clustering_cols, axis=1)\n#X = StandardScaler().fit_transform(X)\n\ndisplay(X.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reducing the clustering feature space to 2 will allow for more relevant utilization of Euclidian distance-based clustering algorithms (like KMeans clustering we are going to utilize below).\n\nHowever, the weak side of such algorithms is a certain voluntarism of a researcher in specifying the number of target clusters to be calculated by the analytical software before the actual analysis started. Thus the final clustering composition is very sensitive to the decision on the number of clusters to calculate (and thus the real analytical edge of the clustering composition could be less then useful).\n\nTo mitigate such a risk, we are going to put some data-driven ground into selection of a number of clusters to calculate for our current KMeans clustering experiment, using so called 'silhouette analysis' (as explained in https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n\n\nX = StandardScaler().fit_transform(X)\n\nrange_n_clusters = [6, 7, 8, 9, 10, 11, 12]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From reviewing the charts above, it looks like clustering with 6 clusters seems to be the best one in terms of their geometry and the points spread.\n\nNow we are ready to proceed with the actual clustering the observations in the training and test sets, using KMeans clustering with 6 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=6)\nkmeans.fit(X)\n\nclusters = kmeans.predict(X)\ncentroids = kmeans.cluster_centers_\n\ndf_train['cluster'] = clusters\n\n# subset of training set for the clustering experiment\nX_test = df_test.filter(clustering_cols, axis=1)\nX_test = StandardScaler().fit_transform(X_test)\n\nclusters_test = kmeans.predict(X_test)\ndf_test['cluster'] = clusters_test\n\n# drop id column\ndf_train = df_train.drop(['id'], axis=1)\ndf_test = df_test.drop(['id'], axis=1)\n\ndf_train.groupby('cluster').mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the feature variable mean variability across the clusters calculated on the training set, the clusters we calculated are statistically significant and really provide a meaningful grouping of the records of the training set.\n\nLet's count the number of records in each cluster of the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_count = df_train.filter(['cluster', 'cont1'], axis=1)\ndf_count.groupby('cluster').count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are going to check if the clustering calculated above is applicable to the testing set in the equally good manner."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.groupby('cluster').mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the feature variable mean variability across the clusters calculated on the testing set, the clusters are also statistically significant."},{"metadata":{},"cell_type":"markdown","source":"Let's count the number of records in each cluster of the testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_count = df_test.filter(['cluster', 'cont1'], axis=1)\ndf_count.groupby('cluster').count().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n\nYou can find more theory on the methods/techniques used in this experiments per the links below\n\n- Selecting the number of clusters with silhouette analysis on KMeans clustering - https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n- Mukesh Chaudhary, Silhouette Analysis in K-means Clustering - https://medium.com/@cmukesh8688/silhouette-analysis-in-k-means-clustering-cefa9a7ad111"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}