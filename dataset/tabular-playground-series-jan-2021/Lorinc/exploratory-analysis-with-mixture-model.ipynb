{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Exploratory analysis of the 2021 tabular playground target data\n\nDumping the dataset into several ML algorithms then choosing the best one, then run a hyperparameter-tuning and automatic feature engineering do not teach you anything about the problem. \n\nI like to go manual and apply a lot of common sense and experimentation. To help this, I always visualize my hypotheses and findings. And I stare a lot at pairplots, sometimes distorted, coloured by different data aspects, zoomed in-and-out. I do this with my estimations and errors too, down to the individual record."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture as GMM\n\n# suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# reading just the train data, I do not do estimations yet\ntrain = pd.read_csv(\n    '/kaggle/input/tabular-playground-series-jan-2021/train.csv', \n    index_col=0\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the shape of the target distribution\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1, figsize=(20, 5), sharex=True)\nsns.kdeplot(train.target, ax=ax[0])\nsns.boxplot(train.target, ax=ax[1], fliersize=10, **{'flierprops':{'alpha':.2}})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying to find the distributions that make up this curve\nThis looks like a few normal distributions on top of each other, and some outlier at around 0.\nThere is a distribution just for that, called [Gaussian Mixture Model](https://scikit-learn.org/stable/modules/mixture.html). Now I try to model the target distribution (blue) with GMM (red)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting the GMM model to the target data\nclf = (\n    GMM(\n        n_components=5, \n        max_iter=200, \n        random_state = 0\n    )\n    .fit(\n        np.array(train.target)\n        .reshape(-1, 1)\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the actual data, the estimated normal distributions, and their sum"},{"metadata":{"trusted":true},"cell_type":"code","source":"# configuring plot\nplt.figure(figsize=(20,5))\nplt.title('The estimated gaussians behind the multimodal target data')\n\n# plotting the original kde (in blue)\nsns.kdeplot(train.target)\n\n# plotting the estimation (in red)\nxpdf = np.linspace(5,10,100000).reshape((-1,1))\ndensity = np.exp(clf.score_samples(xpdf))\nplt.plot(xpdf, density, '-r', alpha=.5)\n\n# plotting the estimated underlying normal distributions\nfor i in range(clf.n_components):\n    pdf = (\n        clf.weights_[i]\n        * stats.norm(\n            clf.means_[i, 0],\n            np.sqrt(clf.covariances_[i, 0])\n        ).pdf(xpdf)\n    )\n    plt.fill(xpdf, pdf, facecolor='gray',\n             edgecolor='none', alpha=0.3)\n\n# trimming the outlier at 0\nplt.xlim(5, 10)\n\n# putting the 5 distributions GMM has found into a DF\ngaussians = pd.concat(\n    [\n        pd.DataFrame(clf.means_, columns=['means']),\n        pd.DataFrame(clf.covariances_.squeeze(), columns=['covariances']),\n        pd.DataFrame(clf.weights_, columns=['weights'])\n    ],\n    axis=1)\n\nplt.table(\n    cellText=gaussians.values,\n    rowLabels=gaussians.index,\n    colLabels=gaussians.columns,\n    cellLoc = 'right', rowLoc = 'center', loc='bottom',\n    bbox=[.015,.45,.35,.5])\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Honestly, I'm a bit dissapointed. I was hoping for a near-perfect fit from the GMM, but for the sake of the experiment, I push forward with this. So now I will try to find/build features that are strong predictors to one of the 5 gaussians, and see, where that path takes us."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}