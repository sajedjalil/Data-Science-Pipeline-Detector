{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas  as pd\nimport xgboost as xgb\n\n#===========================================================================\n# read in the data\n# Original kernel: https://www.kaggle.com/carlmcbrideellis/very-simple-xgboost-regression\n#===========================================================================\ntrain_data = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest_data  = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\n\n#===========================================================================\n# select some features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\n\n#===========================================================================\n#===========================================================================\nX_train = train_data[features]\ny_train = train_data[\"target\"]\nfinal_X_test = test_data[features]\n\n#===========================================================================\n# XGBoost regression: \n# Parameters: \n# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n#                of boosting rounds.\"\n# learning_rate \"Boosting learning rate (xgb’s “eta”)\"\n# max_depth     \"Maximum depth of a tree. Increasing this value will make \n#                the model more complex and more likely to overfit.\" \n#===========================================================================\n# regressor=xgb.XGBRegressor(n_estimators  = 500,\n#                            learning_rate = 0.1,\n#                            max_depth     = 5)\n# regressor.fit(X_train, y_train)\n\n#===========================================================================\n# To use early_stopping_rounds: \n# \"Validation metric needs to improve at least once in every \n# early_stopping_rounds round(s) to continue training.\"\n#===========================================================================\n# perform a test/train split \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n\n# params for XGB are taked from this great kernel https://www.kaggle.com/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna \n# by Hamza Ghanmi\n\nregressor = xgb.XGBRegressor(\n                 colsample_bytree=0.5,\n                 alpha=0.01563,\n                 #gamma=0.0,\n                 learning_rate=0.01,\n                 max_depth=15,\n                 min_child_weight=257,\n                 n_estimators=4000,                                                                  \n                 #reg_alpha=0.9,\n                 reg_lambda=0.003,\n                 subsample=0.7,\n                 random_state=2020,\n                 metric_period=100,\n                 silent=1)\n\nregressor.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================================\n# use the model XGB to predict the prices for the test data\n#===========================================================================\npredictions = regressor.predict(final_X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This part is taked from this great [kernel](https://www.kaggle.com/bowaka/tps21-optuna-lgb-fast-hyper-parameter-tunning) by [Bowaka](https://www.kaggle.com/bowaka)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data.drop(['id','target'], axis=1)\nXtest = test_data.drop(['id'], axis=1)\ny = train_data['target']\n\ntrain = int(len(X)*0.9)\nXtrain, Xval = X.iloc[:train], X.iloc[train:]\nytrain, yval = y.iloc[:train], y.iloc[train:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params from this kernel https://www.kaggle.com/kailex/tabular-playground\n\nparams={'random_state': 33,'n_estimators':5000,\n 'min_data_per_group': 5,\n 'boosting_type': 'gbdt',\n 'num_leaves': 256,\n 'max_dept': -1,\n 'learning_rate': 0.02,\n 'subsample_for_bin': 200000,\n 'lambda_l1': 1.074622455507616e-05,\n 'lambda_l2': 2.0521330798729704e-06,\n 'n_jobs': -1,\n 'cat_smooth': 1.0,\n 'silent': True,\n 'importance_type': 'split',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.8206341150202605,\n 'min_data_in_leaf': 100,\n 'min_sum_hessian_in_leaf': 0.001,\n 'bagging_freq': 6,\n 'feature_fraction': 0.5,\n 'min_gain_to_split': 0.0,\n 'min_child_samples': 20}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\n\nN_FOLDS = 5\n\nkf = KFold(n_splits = N_FOLDS)\noof = np.zeros(len(y))\noof_vanilla = np.zeros(len(y))\npreds = np.zeros(len(Xtest))\nparams['learning_rate'] = 0.005\nparams['num_iterations'] = 5000\nfor train_ind, test_ind in tqdm(kf.split(X)):\n    Xtrain = X.iloc[train_ind]\n    Xval = X.iloc[test_ind]\n    ytrain = y.iloc[train_ind]\n    yval = y.iloc[test_ind]\n\n    model = LGBMRegressor(**params)\n    vanilla_model = LGBMRegressor()\n    \n    model.fit(Xtrain, ytrain, eval_set = ((Xval,yval)), early_stopping_rounds = 50, verbose = 0)\n    vanilla_model.fit(Xtrain, ytrain)\n    p = model.predict(Xval)\n    p_vanilla = vanilla_model.predict(Xval)\n    oof[test_ind] = p\n    oof_vanilla[test_ind] = p_vanilla\n    \n    preds += model.predict(Xtest)/N_FOLDS\n    \nprint(f'mean square error on training data (vanilla model): {np.round(mean_squared_error(y, oof_vanilla, squared=False),5)}')    \nprint(f'mean square error on training data (with optuna tuning): {np.round(mean_squared_error(y, oof, squared=False),5)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensempling between two predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I taked this submissions from this great kernel https://www.kaggle.com/somayyehgholami/results-driven-tabular-playground-series-201\n# by Somayyeh Gholami\nsub1 = pd.read_csv('../input/resultsdriven-tabular-playground-series-201/submission - 2021-01-15T023916.124.csv')\npredictions1 = sub1['target'].tolist()\nsub2 = pd.read_csv('../input/resultsdriventabularplaygroundseries2011/submission - 2021-01-16T012125.132.csv')\npredictions2 = sub2['target'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = [y*1.0002 for x, y in zip(predictions1, predictions2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"id\":test_data.id, \"target\":results})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}