{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ead5438a-876e-68c1-e142-e3b0b3080882"},"source":"Transfer Learning on Stack Exchange Tags\n----------------------------------------\n\n *Predict tags from models trained on unrelated topics* "},{"cell_type":"markdown","metadata":{"_cell_guid":"47dc42fc-5a0b-b68b-9166-7a8229cf42d0"},"source":"*Qualitative Description of Task*\nWhat does physics have in common with biology, cooking, cryptography, diy, robotics, and travel? If you answered \"all pursuits are governed by the immutable laws of physics\" we'll begrudgingly give you partial credit. If you answered \"all were chosen randomly by a scheming Kaggle employee for a twisted transfer learning competition\", congratulations, we accept your answer and mark the question as solved.\n\nIn this competition, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm learn appropriate physics tags from \"extreme-tourism Antarctica\"? Let's find out.\n\nKaggle is hosting this competition for the data science community to use for fun and education. This dataset originates from the Stack Exchange data dump."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"865dc57e-7923-3e04-411d-87521c1a11e2"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"642e79e1-8654-cc49-030c-5a368d681ff9"},"outputs":[],"source":"\"\"\"\nTo Form a Quantitative Description of the Problem \nLet's See What Output is Expected\n\"\"\"\nprint(\"sample Submission \\n\")\nsubmission_format = {\"sample_submission\": pd.read_csv(\"../input/sample_submission.csv\")}\nprint(submission_format[\"sample_submission\"].iloc[2])\n\nprint(\"\\n Test Format \\n\")\ntest_format = pd.read_csv(\"../input/test.csv\")\ntest_format.head(6)"},{"cell_type":"markdown","metadata":{"_cell_guid":"aaa7adfa-d53b-ec19-7877-df5cf99605ea"},"source":"Our Working Description:\n(source: http://machinelearningmastery.com/how-to-define-your-machine-learning-problem/)\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n\nTask (T): Classify physics questions that has not been seen, with tags learned from E.\nExperience (E): A corpus of tagged stack exchange questions in 6 discrete categories.\nPerformance (P): Classification accuracy, the number of tags predicted correctly out of all questions considered as a percentage.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ed26b85-7b6d-238e-1be4-5fd2f841063b"},"outputs":[],"source":"\"\"\"\"\nLoad data and test the structure of our input data\n\"\"\"\n\ndataframes = {\n    \"biology\": pd.read_csv(\"../input/biology.csv\"),\n    \"cooking\": pd.read_csv(\"../input/cooking.csv\"),\n    \"crypto\": pd.read_csv(\"../input/crypto.csv\"),\n    \"diy\": pd.read_csv(\"../input/diy.csv\"),\n    \"robotics\": pd.read_csv(\"../input/robotics.csv\"),\n    \"travel\": pd.read_csv(\"../input/travel.csv\"), \n}\n\nfrom random import randint, choice\n\nprint(dataframes[choice([\"biology\", \"cooking\", \"crypto\", \"diy\", \"robotics\", \"travel\"])].iloc[randint(0,20)])\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"74c46b14-bc53-27ec-4724-b3818c175447"},"source":"Key Assumptions Check\n=====================\n*assume = to make an 'ass' out of 'u' and 'me'*\n\n Seeing as Transfer Learning is in the title Kaggle wants us to solve this by training a really accurate classifier over the given sites and then transferring that model to the physics domain.\n\n1. I'll probably end up sticking a Softmax Regression as our transfer ![Example Softmax Regression][1]\n1.1. Already this is a problem since I'm assuming our end classes are mutually exclusive of one another, this clearly isn't the case, but maybe there's an amalgam of tags which are (e.g. like how we use super-pixels for classification). Ideally, our softmax is serving as an \"activation\" or \"link\" function, shaping the output of our linear function into whatever form we want (etc. pd distr)\n2. A multiple binary classifier will probably be a good baseline for our final function.\n3. Our base model should probably be extremely feauture rich\n  [1]: https://www.harrisgeospatial.com/docs/html/images/Classification/SoftmaxDiagram.gif"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ff7848a-a421-48b2-684a-d30627476aed"},"outputs":[],"source":"stats = pd.concat(dataframes)\nstats.head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}