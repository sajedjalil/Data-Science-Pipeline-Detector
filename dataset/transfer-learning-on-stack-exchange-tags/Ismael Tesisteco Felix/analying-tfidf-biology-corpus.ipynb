{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f27711ab-64f1-d179-d9b3-bbfd17496994"},"source":"This is my first script, I'm analyzing the biology corpus to extract tags using tfidf score, I increased the value for words that appear in the title, and I'm evaluating the output with the corpus tags, i got a low score: 0.073, I'm going to apply advanced techniques in order to increase the score."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f086258f-b3f0-02f4-ce21-bd0ff1c048a2"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom subprocess import check_output\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"275b65ba-7202-8e6b-0425-f1d98dbb7331"},"outputs":[],"source":"biology = pd.read_csv(\"../input/biology.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4764359-7aeb-dfb8-8f6f-93411191c9b3"},"outputs":[],"source":"biology.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de3aeab9-c684-e411-6729-dee3b2bd17f9"},"outputs":[],"source":"swords1 = stopwords.words('english')\n\npunctuations = string.punctuation\n\ndef data_clean(data):\n    print('Cleaning data')\n    data = data.apply(lambda x: x.lower())\n    data = data.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n    data = data.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n    data = data.apply(lambda i: ''.join(i.strip(punctuations))  )\n    #print('tokenize')\n    data = data.apply(lambda x: word_tokenize(x))\n\n    #Select only the nouns\n    is_noun = lambda pos: pos[:2] == 'NN' \n    for i in range(len(data)):\n        data[i] = [word for (word, pos) in nltk.pos_tag(data[i]) if is_noun(pos)]\n    \n    #print('Remove stopwords')\n    data = data.apply(lambda x: [i for i in x if i not in swords1 if len(i)>2])\n    #print('minor clean some wors')\n    data = data.apply(lambda x: [i.split('/') for i in x] )\n    data = data.apply(lambda x: [i for y in x for i in y])\n    #print('Lemmatizing')\n    wordnet_lemmatizer = WordNetLemmatizer()\n    data = data.apply(lambda x: [wordnet_lemmatizer.lemmatize(i) for i in x])\n    data = data.apply(lambda x: [i for i in x if len(i)>2])\n    return(data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"962ff0f7-a029-2934-2d5a-dc22c5a416bc"},"outputs":[],"source":"#nltk.download()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f8c47b4-ac55-0ff8-2766-cd30e3df7135"},"outputs":[],"source":"def get_frequency(content, title):\n    \n    frequency = []\n    inverse_frequency = {}\n    for i in range(len(content)):\n        word_count = {}\n        important = {}\n        for word in title[i]:\n            if word in word_count:\n                word_count[word] = word_count[word] + 100\n            else:\n                word_count[word] = 10\n                important[word] = True\n        for word in content[i]:\n            if word in word_count:\n                if word in important:\n                    word_count[word] = word_count[word] + 50\n                else:\n                    word_count[word] = word_count[word] + 1\n            else:\n                word_count[word] = 1\n                \n        for word in word_count:\n            if word in inverse_frequency:\n                inverse_frequency[word] = inverse_frequency[word] + 1\n            else:\n                inverse_frequency[word] = 1            \n        frequency.append(word_count)\n    return (frequency, inverse_frequency)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"472ded81-fb73-40d3-3aa8-b3ecb35f349c"},"outputs":[],"source":"content = data_clean(biology.content)\ntitle = data_clean(biology.title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb39af5c-23d9-5d05-5968-f0537ce20d13"},"outputs":[],"source":"frequency, inverse_frequency = get_frequency(content, title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be1fb60e-9495-b191-042e-61cffed1e964"},"outputs":[],"source":"import operator\nfrequency_words = {}\nfor document in frequency:\n    for word in document:\n        if word in frequency_words:\n            frequency_words[word] = frequency_words[word] + document[word]\n        else:\n            frequency_words[word] = document[word]            \nfrequency_words = sorted(frequency_words.values())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe3bd07d-6432-6f24-0729-5097da930db3"},"outputs":[],"source":"print('number of words:',len(frequency_words))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3704022f-8b8d-9234-4b1a-1e14d55b994a"},"outputs":[],"source":"plt.plot(frequency_words)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50dcb082-547b-53f8-6ecf-bc06f84d62f7"},"outputs":[],"source":"plt.plot(np.log(frequency_words))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c142067e-5461-5061-f060-7c56ab632011"},"outputs":[],"source":"tfidf = frequency"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"caa264bc-d6c6-6569-bafe-1c92cdfbfffd"},"outputs":[],"source":"tfidf_distribution = []\nfor document in tfidf:\n    if document == {}:\n        continue\n    max_frequency = sorted(document.items(), key=operator.itemgetter(1), reverse=True)[0][1]\n    for word in document:\n        document[word] = document[word]/(max_frequency + 0.0)*np.log(len(tfidf)/(inverse_frequency[word]+0.))\n        tfidf_distribution.append(document[word])\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e35f2fd-3bf0-1f85-e4cb-358fe4c4984d"},"outputs":[],"source":"index = 0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd67635d-d140-2fc5-6286-e461db056fe5"},"outputs":[],"source":"sorted(tfidf[index].items(), key=operator.itemgetter(1), reverse=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ddec35d-243a-aa61-3817-4d294e8c7d37"},"outputs":[],"source":"print(biology.title[index])\nprint(biology.content[index])\nprint(biology.tags[index])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a65b0d87-bffa-8275-7607-b2b5aa60a1a4"},"outputs":[],"source":"tfidf_distribution = sorted(tfidf_distribution)\nprint(len(tfidf_distribution))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b2dddf2-0512-2c46-0349-fa987df12300"},"outputs":[],"source":"plt.plot(tfidf_distribution)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"096ac1c3-1081-699b-dd32-e40272bc8111"},"outputs":[],"source":"plt.plot(np.log(tfidf_distribution))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cae9e292-3d51-dbd0-a91a-be243a7cf8eb"},"outputs":[],"source":"def getF1(prediction,tags):\n    if len(prediction) == 0 or len(tags) == 0:\n        return 0.0\n    tags = set(tags.split())\n    corrects = 0\n    for p in prediction:\n        if p in tags:\n            corrects = corrects + 1\n    \n    precision = corrects / (len(prediction) + 0.)\n    recall = corrects / (len(tags) + 0.)\n    if precision == 0 or recall == 0:\n        return 0.0     \n    return 2*precision*recall/(precision + recall)\n        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a743d5c-160e-f600-7677-8065400a9e29"},"outputs":[],"source":"top = 3\ncorpusf1 = []\nfor i in range(len(tfidf)):\n    prediction = sorted(tfidf[i], key=tfidf[i].get, reverse=True)[0:top]\n    corpusf1.append(getF1(prediction, biology.tags[i]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b9406b2-8356-8b98-f268-030c1556b135"},"outputs":[],"source":"print(np.average(corpusf1))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5c6aaa63-93c9-c472-9650-f09184f8c569"},"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}