{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4b8ad13-7998-9476-11d4-69db6de1d2b8"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom subprocess import check_output\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5c98bad-801d-0d6c-937f-479400f19680"},"outputs":[],"source":"physic = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c4159dc-aeaa-d1dc-3afb-f553b6457474"},"outputs":[],"source":"physic.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcff54e4-07ee-a085-ed1c-414f713617ae"},"outputs":[],"source":"punctuations = string.punctuation\n\ndef data_clean(data):\n    print('Cleaning data')\n    data = data.apply(lambda x: x.lower())\n    data = data.apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n    data = data.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n    data = data.apply(lambda i: ''.join(i.strip(punctuations))  )\n    #print('tokenize')\n    data = data.apply(lambda x: word_tokenize(x))\n\n    #Select only the nouns\n    is_noun = lambda pos: pos[:2] == 'NN' \n    for i in range(len(data)):\n        data[i] = [word for (word, pos) in nltk.pos_tag(data[i]) if is_noun(pos)]\n    \n    #print('Lemmatizing')\n    wordnet_lemmatizer = WordNetLemmatizer()\n    data = data.apply(lambda x: [wordnet_lemmatizer.lemmatize(i) for i in x])\n    data = data.apply(lambda x: [i for i in x if len(i)>2])\n    return(data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3040199a-6cce-106c-23e7-c3764ff82a2d"},"outputs":[],"source":"#nltk.download()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30b27017-9530-31ea-ae24-90556f1efbd4"},"outputs":[],"source":"def get_frequency(title):\n    \n    frequency = []\n    inverse_frequency = {}\n    for i in range(len(title)):\n        word_count = {}\n\n        for word in title[i]:\n            if word in word_count:    \n                word_count[word] = word_count[word] + 1\n            else:\n                word_count[word] = 1\n                \n        for word in word_count:\n            if word in inverse_frequency:\n                inverse_frequency[word] = inverse_frequency[word] + 1\n            else:\n                inverse_frequency[word] = 1            \n        frequency.append(word_count)\n        \n    return (frequency, inverse_frequency)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7a8d6f6-af03-1da4-8c23-9453904ffa98"},"outputs":[],"source":"title = data_clean(physic.title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d53c0adb-3e29-42ba-502a-d9543659ba65"},"outputs":[],"source":"frequency, inverse_frequency = get_frequency(title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4d92b7a-7bb0-7891-641e-f67218a7432a"},"outputs":[],"source":"import operator\nfrequency_words = {}\nfor document in frequency:\n    for word in document:\n        if word in frequency_words:\n            frequency_words[word] = frequency_words[word] + document[word]\n        else:\n            frequency_words[word] = document[word]            \nfrequency_words = sorted(frequency_words.values())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4f63b4b-574d-ef17-fc28-19506995e5e2"},"outputs":[],"source":"print('number of words:',len(frequency_words))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25e03bc5-6f4c-6307-4467-eddedd0da90f"},"outputs":[],"source":"plt.plot(frequency_words)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"449075bb-9cd8-d813-2fa0-2be8ac453ae7"},"outputs":[],"source":"plt.plot(np.log(frequency_words))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26fa7009-73a5-6453-840a-960442ed3cff"},"outputs":[],"source":"tfidf = frequency"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"098c7900-8a71-47ea-3edb-cb6d6f464d18"},"outputs":[],"source":"tfidf_distribution = []\nfor document in tfidf:\n    if document == {}:\n        continue\n    max_frequency = sorted(document.items(), key=operator.itemgetter(1), reverse=True)[0][1]\n    for word in document:\n        document[word] = document[word]/(max_frequency + 0.0)*np.log(len(tfidf)/(inverse_frequency[word]+0.))\n        tfidf_distribution.append(document[word])\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea725b10-595c-9c26-2a08-47dd590034b7"},"outputs":[],"source":"index = 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7f04a68-53ac-c21a-1bcf-5fc04caf5dad"},"outputs":[],"source":"sorted(tfidf[index].items(), key=operator.itemgetter(1), reverse=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f4fed8b-fd4a-782a-7602-8561ce9db78e"},"outputs":[],"source":"print(physic.title[index])\nprint(physic.content[index])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92fb7ca9-a343-8844-9443-732c26f6ef65"},"outputs":[],"source":"tfidf_distribution = sorted(tfidf_distribution)\nprint(len(tfidf_distribution))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99a25076-4f6b-e856-c218-985797ec05ff"},"outputs":[],"source":"plt.plot(tfidf_distribution)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61686d8c-c0cc-cc1b-5b65-37d2ea198521"},"outputs":[],"source":"plt.plot(np.log(tfidf_distribution))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b185bf6e-9d2e-4251-4b79-074e5a5c0e40"},"outputs":[],"source":"top = 8\noutput = []\nfor i in range(0,len(physic)):\n    prediction = sorted(tfidf[i], key=tfidf[i].get, reverse=True)[0:top]\n    output.append([physic.id[i], ' '.join(prediction)])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7628e338-8c3a-763f-c2e0-fcd52ff23f0f"},"outputs":[],"source":"pd.DataFrame(data=output,columns = ['id','tags']).to_csv('Submission.csv', index=False)       "},{"cell_type":"markdown","metadata":{"_cell_guid":"4460b461-ded3-edac-110c-b8ed7e250ed5"},"source":"This is my first try, i'm going to try another techniques in order to increase the results."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c45e6872-fb4c-f5ca-3144-2ca82c4d6254","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}