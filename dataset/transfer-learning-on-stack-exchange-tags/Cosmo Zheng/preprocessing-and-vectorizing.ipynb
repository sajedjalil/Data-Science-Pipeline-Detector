{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"992f36de-d4d9-4bec-daab-d4520629f8a7"},"outputs":[],"source":"# Imports\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# preprocessing\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57f6d04e-86f6-17b8-6fc1-101a18852bde"},"outputs":[],"source":"dataframes = {\n    \"cooking\": pd.read_csv(\"../input/cooking.csv\"),\n    \"crypto\": pd.read_csv(\"../input/crypto.csv\"),\n    \"robotics\": pd.read_csv(\"../input/robotics.csv\"),\n    \"biology\": pd.read_csv(\"../input/biology.csv\"),\n    \"travel\": pd.read_csv(\"../input/travel.csv\"),\n    \"diy\": pd.read_csv(\"../input/diy.csv\"),\n}\n\ntest = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"308a12a1-e6b1-16e1-f046-4a6380560ef2"},"outputs":[],"source":"# remove html tags and uris from contents\n\nuri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'\n\ndef stripTagsAndUris(x):\n    if x:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(x, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(uri_re, \"\", text)\n    else:\n        return \"\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd9fc15a-8b09-80a6-395b-b9f119e904b6"},"outputs":[],"source":"for df in dataframes.values():\n    df[\"content\"] = df[\"content\"].map(stripTagsAndUris)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0307a53a-5c88-7b7e-a4cd-dffae1a1da45"},"outputs":[],"source":"corpus = []\nvectorizer = CountVectorizer(min_df=1)\nfor df in dataframes.values():\n    for title in df['title']:\n        corpus.append(title)\n    for content in df['content']:\n        corpus.append(content)\nX = vectorizer.fit_transform(corpus)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f965448-287c-ca71-3336-917f05613f93"},"outputs":[],"source":"analyzer = vectorizer.build_analyzer()\ntokenizer = vectorizer.build_tokenizer()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"980a1e01-c152-3050-0ec4-568bf08ed0b2"},"outputs":[],"source":"for df in dataframes.values():\n    df[\"title\"] = df[\"title\"].map(analyzer)\n    df[\"content\"] = df[\"content\"].map(analyzer)\n    df[\"tags\"] = df[\"tags\"].map(tokenizer)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27a26a49-0797-db71-efd3-14f2b2101db5"},"outputs":[],"source":"dataframes['cooking'].head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}