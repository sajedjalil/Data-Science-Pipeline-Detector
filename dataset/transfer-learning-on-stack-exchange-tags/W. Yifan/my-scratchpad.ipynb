{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"99102ce5-87ce-4f73-2971-1ff0b843cbf7"},"source":"LB0.086"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57ebdb31-f7fe-8f00-ea81-1fd00396d402"},"outputs":[],"source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom subprocess import check_output\nfrom nltk.stem import WordNetLemmatizer\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"5a6ac2dd-3269-421a-1fd1-0a6129100c1e"},"source":"# Load Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55c09c00-c430-4fcf-5c8c-f7dbff9e8fc1"},"outputs":[],"source":"bio = pd.read_csv(\"../input/biology.csv\")\ncook = pd.read_csv(\"../input/cooking.csv\")\ncrypto = pd.read_csv(\"../input/crypto.csv\")\ndiy = pd.read_csv(\"../input/diy.csv\")\nrobot = pd.read_csv(\"../input/robotics.csv\")\ntravel = pd.read_csv(\"../input/travel.csv\")\nsample_sub = pd.read_csv(\"../input/sample_submission.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n\nall_dat = [bio,cook,crypto,diy,robot,travel]"},{"cell_type":"markdown","metadata":{"_cell_guid":"d71f6ce8-4993-0413-30c6-e828cde7135e"},"source":"# Title Cleaning"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a56216b3-9a48-3124-a759-9922fe0c17df"},"outputs":[],"source":"swords1 = stopwords.words('english')\n\npunctuations = string.punctuation\n\ndef title_clean(data):\n    title = data.title\n    title = title.apply(lambda x: x.lower())\n    print('Remove Punctuations')\n    # title = [' '.join(word.strip(punctuations) for word in i.split()) for i in title]\n    title = title.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n    title = title.apply(lambda i: ''.join(i.strip(punctuations))  )\n    print('tokenize')\n    title = title.apply(lambda x: word_tokenize(x))\n    print('Remove stopwords')\n    title = title.apply(lambda x: [i for i in x if i not in swords1 if len(i)>2])\n    print('minor clean some wors')\n    title = title.apply(lambda x: [i.split('/') for i in x] )\n    title = title.apply(lambda x: [i for y in x for i in y])\n    print('Lemmatizing')\n    wordnet_lemmatizer = WordNetLemmatizer()\n    title = title.apply(lambda x: [wordnet_lemmatizer.lemmatize(i,pos='v') for i in x])\n    title = title.apply(lambda x: [i for i in x if len(i)>2])\n    return(title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a23a17c6-a9f1-3589-31e3-50d706ef624e"},"outputs":[],"source":"test.title = title_clean(test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a975c45-6ba1-a291-6ee2-01501dd2d691"},"source":"# Content Cleaning"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f7e8269-ca43-4f31-b4ac-8907c84ba3f7"},"outputs":[],"source":"def content_clean(data):\n    content = data.content\n    content = content.apply(lambda x: x.lower())\n    print('Remove <>')\n    content = content.apply(lambda x: re.sub(r'\\<[^<>]*\\>','',x))\n    print('Remove n')\n    content = content.apply(lambda x: re.sub(r'\\n','',x))\n    print('tokenize')\n    content = content.apply(lambda x: word_tokenize(x))\n    print('Remove stopwords')\n    content = content.apply(lambda x: [i for i in x if i not in swords1 if len(i)>2])\n    print('Lemmatizing')\n    wordnet_lemmatizer = WordNetLemmatizer()\n    content = content.apply(lambda x: [wordnet_lemmatizer.lemmatize(i,pos='v') for i in x])\n    content = content.apply(lambda x: [i for i in x if len(i)>2])\n    print('further cleaning')\n    content = content.apply(lambda x: [''.join(j for j in i if j not in punctuations) for i in x])\n    content = content.apply(lambda x: [i for i in x if len(i)>2])\n    return(content)\n        \ntest.content = content_clean(test)\n\n  "},{"cell_type":"markdown","metadata":{"_cell_guid":"ea42fb4e-b444-ad50-a3a1-3fd321c68ec8"},"source":"# Tfidf top terms for content:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ab6219b-0741-f4e9-95b9-03bb9b27ea97"},"outputs":[],"source":"from gensim import corpora\nfrom gensim import models\nimport gensim\nimport numpy as np\ncontent = test.content\ndictionary = corpora.Dictionary(content)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bddb555-0ba0-78c2-356e-9912e46c396c"},"outputs":[],"source":"corpus = [dictionary.doc2bow(text) for text in content]\n\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34dcb6d5-f15c-d79c-abcf-20b3932d11fe"},"outputs":[],"source":"good_corpus = []"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ae6fc5e-d051-98dc-c432-75c639ddfc69"},"outputs":[],"source":"\nfor doc in corpus_tfidf:\n    doc_dat = [(dictionary.get(item[0]),item[1]) for item in doc]\n    good_corpus.append(doc_dat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26a3b391-ea92-bf9c-22cb-49fed978588e"},"outputs":[],"source":"good_corpus2 = [ sorted(i,key=lambda x: x[1],reverse=True) for i in good_corpus]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"463ce542-5cf5-6a63-320c-81018fcc632d"},"outputs":[],"source":"\n\ngood_corpus3 = [term_list[:10] for term_list in good_corpus2]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68404781-5ff1-b80d-0493-b7de83dd7424"},"outputs":[],"source":"good_corpus4 = []\n\nfor item_list in good_corpus3:\n    good_term = [i[0] for i in item_list]\n    good_corpus4.append(good_term)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b56d1eab-4285-d3c0-4aff-e35ac50efe28"},"source":"# POS Tagging Title:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fba3cb5b-2c90-5e1a-1b02-d0a38c2396fd"},"outputs":[],"source":"tags1 = [nltk.pos_tag(x) for x in test.title]\ntags2 = []\nfor taglist in tags1:\n    goodterm = [i[0] for i in taglist if i[1][0] in \"N\"]\n    tags2.append(goodterm)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64004add-5c46-9df6-cd9c-84a50c409e93"},"outputs":[],"source":"tags2[:5]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b10db7e-307a-f3e4-a740-fdce62c80ff7"},"outputs":[],"source":"title_corpus = [\" \".join(terms)  for terms in tags2]\n\nsub_title = pd.DataFrame(\n    {\n        'id': test.id,\n        'tags': title_corpus\n    \n  \n    })\n\nsub_title.to_csv('mysub_title.csv',index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"894de853-1702-51ae-5e4a-b79a20b8ebd4"},"source":"# POS Tagging Content:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"012baa5d-c9dc-918d-fb66-35088f7390a4"},"outputs":[],"source":"tags_c = [nltk.pos_tag(x) for x in good_corpus4]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80b2ae40-28a1-c14e-b8f0-1d08f8b00909"},"outputs":[],"source":"tags_c[:5]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"955bd8f9-75a7-13b2-6e8c-df67672d88b3"},"outputs":[],"source":"tags_c2 = []\nfor taglist in tags_c:\n    goodterm = [i[0] for i in taglist]\n    tags_c2.append(goodterm)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6538402e-74b2-105f-af5f-ea8ad55758f4"},"outputs":[],"source":"good_corpus_cont = [\" \".join(terms)  for terms in tags_c2]\nsub_cont = pd.DataFrame(\n    {\n        'id': test.id,\n        'tags': good_corpus_cont\n    \n  \n    })"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b55e6242-7b46-8faa-5079-586e2ea9b3ef"},"outputs":[],"source":"sub_cont.to_csv('mysub_content.csv',index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e4680bd5-e2b4-6d29-a15f-aff45090d864"},"source":"# Other Submissions:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c14c225-0bd2-01d7-c34d-483563ff5c4b"},"outputs":[],"source":"title_tag = []\n\nfor taglist in tags2:\n    goodterm = [i[0] for i in taglist if i[1][0] in \"N\"]\n    title_tag.append(goodterm)\n    \n    \ngood_corpus5 = [\" \".join(terms)  for terms in tags_c2]\nsub = pd.DataFrame(\n    {\n        'id': test.id,\n        'tags': good_corpus5\n    \n  \n    })"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"793e1930-ceb5-c8c8-03cb-9171f3dfb5bd"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51456805-24a0-b45a-8e12-fc9e0932ab5f"},"outputs":[],"source":"final_res=[]\n\nfor i in range(len(tags2)):\n    res = list(set(tags_c2[i]+tags2[i]))\n    final_res.append(res)\n    \n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3ab9f12-4d46-ea70-0d35-1aeeed1e237a"},"outputs":[],"source":"final_res[:2]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0c9e0a0-05fd-1c05-203f-e4273ea1f49e"},"outputs":[],"source":"good_corpus_all = [\" \".join(terms)  for terms in final_res]\nsub_all = pd.DataFrame(\n    {\n        'id': test.id,\n        'tags': good_corpus_all\n    \n  \n    })"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd133d9b-ff8a-14ab-cd58-c1e8bc3b4d03"},"outputs":[],"source":"sub_all.to_csv('mysub_together.csv',index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e52cf67-211c-5949-5ef6-2ecb655f383a"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}