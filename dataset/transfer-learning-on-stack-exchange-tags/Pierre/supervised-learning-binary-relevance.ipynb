{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e0e479de-b719-1bb8-2b01-8c92540e52e2"},"source":"<h1>Introduction</h1>\nIn this notebook, I will present the method I propose to make of this competition a supervised learning challenge. The only source of data I found and that I think it is relevant is the scientific paper stored in Springer. I used its API to create my training set.<br>\nTo create the model, I used binary relevance: I'm not sure it's the best way but everything in this notebook are open to debate.\n\n<h1>List of the libraries used in this notebook</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1244082-491e-588f-d578-25d6bf3b9ace"},"outputs":[],"source":"import requests\nimport re\nimport csv\nfrom lxml import html\nfrom lxml import etree\n\nimport pandas as pd\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport nltk\nimport pickle\n\nimport copy\nimport math\n\nfrom sklearn import linear_model\nimport os.path\n\nimport csv"},{"cell_type":"markdown","metadata":{"_cell_guid":"63bacafc-c88b-6a36-cf0e-6cd2e16f3785"},"source":"<h1>Creation of the training set</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b36568e-c833-986d-1271-9ff6dbea63e3"},"outputs":[],"source":"# This function uses the Springer API to get the articles related to physics and their keywords\ndef get_training_set():\n\t# To create a free account : https://dev.springer.com/signup\n\tapi_key = \"put your API key here\"\n\thost = \"http://api.springer.com\"\n\tpath = \"/metadata/pam\"\n\n\tquery = \"physics\"\n\n\tarticles = open('./physics_articles.csv', 'w')\n\tcsvwriter = csv.writer(articles)\n\tcsvwriter.writerow(['id', 'title', 'content', 'tags'])\n\n\ti = 0\n\ts = 1\n\tstep = 100\n\t\n\t# The maximum number of articles to download\n\tmaxi = 1000\n\n\twhile True:\n\t\tget_params = {\n\t\t'q': query,\n\t\t's': s,\n\t\t'p': step,\n\t\t'api_key': api_key\n\t\t}\n\n\t\tr = requests.get(host + path, params=get_params)\n\t\tresponse = html.fromstring(r.content)\n\n\t\t# The total number of articles got from the query\n\t\ttotal = int(response.xpath('//result/total/text()')[0])\n\n\t\tdoil = response.xpath('//doi/text()')\n\n\t\t# We have to make one request per articles\n\t\tfor doi in doil:\t\n\t\t\ti += 1\n\t\t\t\n\t\t\tget_params = {\n\t\t\t\t'q': doi,\n\t\t\t\t'api_key': api_key\n\t\t\t}\n\t\n\t\t\tr = requests.get(host + path, params=get_params)\n\t\t\tresponse = html.fromstring(r.content)\n\t\n\t\t\tkeywords = response.xpath('//facet[@name=\"keyword\"]/facet-value/text()')\n\t\n\t\t\tif(len(keywords) == 0):\n\t\t\t\tcontinue\n\t\n\t\t\ttitle = response.xpath('//article/title/text()')\n\t\n\t\t\tif(len(title) == 0):\n\t\t\t\tcontinue\n\t\n\t\t\tabstract = response.xpath('//body/p')\n\t\n\t\t\tif(len(abstract) == 0):\n\t\t\t\tcontinue\n\t\n\t\t\ttitle = title[0]\n\t\t\tabstract = re.sub('<.*?>', '', str(etree.tostring(abstract[0]).decode('utf-8')))\n\t\n\t\t\ttag_list = []\n\n\t\t\tfor k in keywords:\n\t\t\t\tif(re.search('[0-9:\\\\(\\\\)]', k)):\n\t\t\t\t\tcontinue\n\n\t\t\t\tk = re.sub(' ', '-', k.strip()).lower()\n\t\t\t\ttag_list.append(k)\n\t\t\n\t\t\ttags = ' '.join(tag_list)\n\t\n\t\t\tcsvwriter.writerow([i, title, abstract, tags])\n\n\t\tif(s + step >= total or s + step >= maxi):\n\t\t\tbreak\n\t\telse:\n\t\t\ts += step\n\n\tarticles.close()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5b240e1-6eb8-963a-cf24-e6adda5ef43f"},"source":"In this piece of code, we can choose the number of articles we want to download (maxi variable) but we have to be aware that the algorithm dumps the articles without title, abstract or keywords. It's happen more often we can think."},{"cell_type":"markdown","metadata":{"_cell_guid":"d70e58da-56c1-734d-62cd-fdc7e8b212ff"},"source":"<h1>Creation of dictionaries</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b3c2c04-0b84-fe02-c6d1-fc28129bf5e6"},"outputs":[],"source":"# This function creates a list of dictionaries (one per document/articles), a dictionary of all the words in the corpus (dico_global) and one dictionary that counts the number of documents who contains a word (dico_unique).\ndef create_dictionary():\n\ttrain = pd.read_csv('./physics_articles.csv')\n\n\tdico_list = []\n\tdico_global = defaultdict(int)\n\tdico_unique = defaultdict(int)\n\tstemmer = SnowballStemmer(\"english\")\n\tstop = stopwords.words('english')\n\n\tfor i in range(0, train.count()['id']):\n\t\trow = train.loc[i]\n\t\tdico = defaultdict(int)\n\t\n\t\tfor word, token in nltk.pos_tag(nltk.word_tokenize(row['title'].lower())):\n\t\t\tif(word not in stop and (token.startswith(\"NN\") or token.startswith(\"VB\") or token.startswith(\"JJ\"))):\n\t\t\t\tword = re.sub('[^a-z \\\\-]', '', word)\n\t\t\t\tword = stemmer.stem(word)\n\t\t\t\tif(dico[word] == 0):\n\t\t\t\t\tdico_unique[word] += 1\n\t\t\t\tdico[word] += 1\n\t\t\t\tdico_global[word] += 1\n\t\n\t\tfor word, token in nltk.pos_tag(nltk.word_tokenize(re.sub('<.*?>', '', row['content'].lower()))):\n\t\t\tif(word not in stop and (token.startswith(\"NN\") or token.startswith(\"VB\") or token.startswith(\"JJ\"))):\n\t\t\t\tword = re.sub('[^a-z \\\\-]', '', word)\n\t\t\t\tword = stemmer.stem(word)\n\t\t\t\tif(dico[word] == 0):\n\t\t\t\t\tdico_unique[word] += 1\n\t\t\t\tdico[word] += 1\n\t\t\t\tdico_global[word] += 1\n\n\t\tdico_list.append(dico)\n\t\n\tpickle.dump(dico_list, open('./dico_list.bin', 'wb'))\n\tpickle.dump(dico_unique, open('./dico_unique.bin', 'wb'))\n\tpickle.dump(dico_global, open('./dico_global.bin', 'wb'))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a829ab6-6958-8897-cc27-2427c5d21cd8"},"source":"Here, I only keep nouns, verbs and adjectives and I stem every words before creating the dictionaries. I do that in order to decrease the required amount of memory.<br>\nAt the end of each function, I use pickle to dump the variables and free the memory.\n\n<h1>TF-IDF</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01f8d856-d2ef-d094-ea3a-bc385f5c3b94"},"outputs":[],"source":"# Application of TD-IDF on dico_list\n# I also chose to only keep the ten most important words in each document in order to reduce the dictionaries\n# and I removed the words present in more than 10% of the documents and present in only one document\ndef tfidf():\n\ttrain = pd.read_csv('./physics_articles.csv')\n\tdico_list = pickle.load(open('./dico_list.bin', 'rb'))\n\tdico_global = pickle.load(open('./dico_global.bin', 'rb'))\n\tdico_unique = pickle.load(open('./dico_unique.bin', 'rb'))\n\t\n\tdico_list_tfidf = defaultdict(float)\n\tword_to_keep = set()\n\n\tfor i in range(0, train.count()['id']):\n\t\tfor word, occur in dico_list[i].items():\n\t\t\trow = train.loc[i]\n\t\t\ttext = row['title'] + ' ' + re.sub('<.*?>', '', row['content'])\n\t\t\ttf = occur / len(text.split())\n\t\t\tidf = math.log(train.count()['id'] / dico_unique[word], 10)\n\t\t\tdico_list_tfidf[i][word] = tf * idf\n\t\n\t\tword_to_keep.update(set(sorted(dico_list[i], key=dico_list[i].get, reverse=True)[:10]))\n\t\n\t# word_to_keep has to be an ordered list, not a set\n\tword_to_keep = list(word_to_keep)\n\t\n\t# It's not possible to edit the dictionary in a for loop\n\tcopy_dico_global = copy.copy(dico_global)\n\n\tfor word in copy_dico_global:\n\t\tif(word not in word_to_keep or dico_global[word] == 1 or dico_global[word] / train.count()['id'] > 0.1):\n\t\t\tdico_global.pop(word)\n\t\t\n\tdel copy_dico_global\n\n\tpickle.dump(dico_list_tfidf, open('./dico_list_tfidf.bin', 'wb'))\n\tpickle.dump(word_to_keep, open('./word_to_keep.bin', 'wb'))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"5867e967-dd25-c29c-4352-dd6c5d1d884c"},"source":"It's the first time I use TF-IDF, if you see an error, please tell me.<br>\nAgain, I remove some words of the dictionary for memory optimization.\n\n<h1>Creation of the X matrix</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8cf0eeb-60ae-fe87-7a59-01742d7e3223"},"outputs":[],"source":"# This function create the X matrix who will be send to the sklearn algorithm\ndef create_x():\n\tdico_list_tfidf = pickle.load(open('./dico_list_tfidf.bin', 'rb'))\n\tword_to_keep = pickle.load(open('./word_to_keep.bin', 'rb'))\n\t\n\tX = []\n\t\n\tfor i in range(0, train.count()['id']):\n\t\tx = []\n\t\tfor word in word_to_keep:\n\t\t\tx.append(dico_list_tfidf[i][word])\n\t\tX.append(x)\n\n\tpickle.dump(X, open('./X.bin', 'wb'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f27421b-4a48-79fd-dd65-408b3516bda4"},"source":"The X variable is huge! It's possible to store the data in a CSV file for the same result. It's very useful if we want to use a model who can do partial fitting such as SGD.\n\n<h1>Creation of the Y matrix</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c6b01b7-f2f9-74b0-f37d-1cee04021bb2"},"outputs":[],"source":"# This function creates the keywords array who contains all the different tags in the training set\ndef get_keywords():\n\ttrain = pd.read_csv('./physics_articles.csv')\n\tkeywords = []\n\n\tfor i in range(0, train.count()['id']):\n\t\trow = train.loc[i]\n\t\tfor word in row['tags'].split():\n\t\t\tkeywords.append(word)\n\n\tkeywords = list(set(keywords))\n\t\n\tpickle.dump(keywords, open('./keywords.bin', 'wb'))\n\n# This function create the Y matrix who will be send to the sklearn algorithm\n# It's a matrix because we will use binary relevance: each column of the matrix correspond to one tag/keyword\ndef create_y():\n\ttrain = pd.read_csv('./physics_articles.csv')\n\tkeywords = pickle.load(open('./keywords.bin', 'rb'))\n\t\n\tY = []\n\n\tfor keyword in keywords:\n\t\tprint(keyword)\n\t\ty = []\n\t\tfor i in range(0, train.count()['id']):\n\t\t\trow = train.loc[i]\n\t\t\tif(keyword in row['tags'].split()):\n\t\t\t\ty.append(1)\n\t\t\telse:\n\t\t\t\ty.append(0)\n\t\tY.append(y)\n\n\tpickle.dump(Y, open('./Y.bin', 'wb'))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"a283cf45-fa19-9cd2-4644-f14fc101b123"},"source":"It's really important for the variables word_to_keep and keywords to be list and not set because the order of the words will help us to retrieve the corresponding term when we will create the X matrix of the testing set and to link each model to a keyword.\n\n<h1>Creation of the models</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60468d40-fce0-e2ad-27c4-a9d3465891ff"},"outputs":[],"source":"# This function create one model per keyword\ndef create_models():\n\tX = pickle.load(open('./X.bin', 'rb'))\n\tY = pickle.load(open('./Y.bin', 'rb'))\n\t\n\tfor i in range(0, len(Y)):\n\t\t# I added this condition because I got a problem during the creation of the models\n\t\tif(not os.path.isfile('./clf/' + str(i) + '.bin')):\n\t\t\t# You can adjust the C parameter but, with smaller value on smaller training set, I got a very bad in-sample error (almost 100% with C=1)\n\t\t\tclf = linear_model.LogisticRegression(C=100)\n\t\t\tclf.fit(X, Y[i])\n\t\t\n\t\t\tpickle.dump(clf, open('./clf/' + str(i) + '.bin', 'wb'))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8d745a7-dc63-25c7-be98-a5bd01733da5"},"source":"I create a different folder to store all the model dump files. Each file correspond to one classifier who correspond to one keyword.<br>\nI use LogisticRegression because it's fast but I think that hard-margin SVM or Nearest Neighbors can be better.\n\n<h1>Predictions</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79a0370c-c9c5-5aa5-7318-7e6bc00219b2"},"outputs":[],"source":"# This function predicts tags according to our models\ndef predict():\n\ttest = pd.read_csv('./test.csv')\n\tdico_unique = pickle.load(open('./dico_unique.bin', 'rb'))\n\t\n\tpredict_csv = open('./predict.csv', 'w')\n\tcsvwriter = csv.writer(predict_csv)\n\tcsvwriter.writerow(['id', 'tags'])\n\n\tstemmer = SnowballStemmer(\"english\")\n\n\tfor i in range(0, int(test.count()['id'])):\n\t\trow = test.loc[i]\n\t\tdico = defaultdict(float)\n\t\tpredicted_keywords = \"\"\n\t\n\t\tfor word in nltk.word_tokenize(row['title'].lower()):\n\t\t\tword = re.sub('[^a-z \\\\-]', '', word)\n\t\t\tword = stemmer.stem(word)\n\t\t\tif(word in word_to_keep):\n\t\t\t\tdico[word] += 1\n\t\n\t\tfor word in nltk.word_tokenize(row['content'].lower()):\n\t\t\tword = re.sub('[^a-z \\\\-]', '', word)\n\t\t\tword = stemmer.stem(word)\n\t\t\tif(word in word_to_keep):\n\t\t\t\tdico[word] += 1\n\t\n\t\tx = []\n\t\tlength = len(row['content'].split()) + len(row['title'].split())\n\t\n\t\tfor word in word_to_keep:\n\t\t\ttf = dico[word] / length\n\t\t\tidf = math.log(test.count()['id'] / dico_unique[word], 10)\n\t\t\tx.append(tf * idf)\n\t\n\t\tfor j in range(0, len(keywords)):\n\t\t\tclf = pickle.load(open(\"./clf/\" + str(j) + \".bin\", 'rb'))\n\t\t\tres = clf.predict([x])[0]\n\t\t\n\t\t\tif(res == 1):\n\t\t\t\tpredicted_keywords += (\" \" + keywords[j])\n\t\n\t\tcsvwriter.writerow([row['id'], predicted_keywords])\n\n\tpredict_csv.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e0971c3-90bf-5a95-eaeb-dd5b6b3d85b4"},"source":"No need of a lot of RAM in this case but a lot of time is required! (or a good cluster)\n\n<h1>Conclusion</h1>\n\nUnfortunately, I cannot test this script on my good old laptop (Intel Core 2 Duo / 2 GB of RAM) so I want to propose it to you. I hope it can help. There is a lot of optimizations we can do. Your feedbacks are welcome! "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}