{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"15f13016-7583-1d9a-dbb2-c3e2c27faa4a"},"source":"## Sklearn Basic Random Forests (0.586 LB)"},{"cell_type":"markdown","metadata":{"_cell_guid":"84898e14-6057-047a-d85d-e82b52c44a86"},"source":"In the following network I build a RF based on the Sklearn interface. This is a brief summary of the steps I follow to build it:\n\n- Load data\n- Generate location features\n- Feature engineering ('basic_preprocess' function)\n- Normalize features\n- GridSearch on RF parameters to find the optimum\n- Generate predictions on the test dataset\n\nThe generated predictions got a 0.5861 log-loss (LB).\n\nSome of the feature engineering here is based on two previous notebooks:\n\n- [Unsupervised and supervised neighborhood encoding](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/unsupervised-and-supervised-neighborhood-encoding)\n- [Price/Bedrooms/Bathrooms](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths)\n\nAll the preprocessing is the same than in:\n\n- [Neural Network w/ feat. engineering 0.583LB](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/neural-network-w-feat-engineering-0-583lb)\n\n*Any feedback or comment will be appreciated! Upvote if you found it interesting/useful :)\nThanks!*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e58ad318-e1a9-2a8a-f831-73579c3acc28"},"outputs":[],"source":"%matplotlib inline\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport time as time\nfrom sklearn.preprocessing import StandardScaler, Imputer, LabelBinarizer\nfrom sklearn.metrics import log_loss\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.pipeline import make_pipeline\n\ndef get_skf_indexes(df, target, kfold=4):\n    X = df.values\n    y = df[target].values\n    skf = StratifiedKFold(n_splits=4);\n    skf.get_n_splits(X, y);\n    indexes = [[],[]]\n    for train_index, test_index in skf.split(X, y):\n        indexes[0].append(train_index)\n        indexes[1].append(test_index)\n    return indexes\n\n\ndef output_results(clf, x_test, listing, fname):\n    preds = clf.predict_proba(x_test)\n    preds = pd.DataFrame(preds)\n    cols = ['low', 'medium', 'high']\n    preds.columns = cols\n    preds['listing_id'] = listing\n    preds.to_csv(fname, index=None)\n    print(preds[cols].mean().values)\n\n\ndef basic_preprocess(df_train, df_test, n_min=50, precision=3):\n    \n    # Interest: Numerical encoding of interest level\n    df_train['y'] = 0.0\n    df_train.loc[df_train.interest_level=='medium', 'y'] = 1.0\n    df_train.loc[df_train.interest_level=='high', 'y'] = 2.0\n    \n    # Location features: Latitude, longitude\n    df_train['num_latitude'] = df_train.latitude.values\n    df_test['num_latitude'] = df_test.latitude.values\n    df_train['num_longitude'] = df_train.longitude.values\n    df_test['num_longitude'] = df_test.longitude.values\n    x = np.sqrt(((df_train.latitude - df_train.latitude.median())**2) + (df_train.longitude - df_train.longitude.median())**2)\n    df_train['num_dist_from_center'] = x.values\n    x = np.sqrt(((df_test.latitude - df_train.latitude.median())**2) + (df_test.longitude - df_train.longitude.median())**2)\n    df_test['num_dist_from_center'] = x.values\n    df_train['pos'] = df_train.longitude.round(precision).astype(str) + '_' + df_train.latitude.round(precision).astype(str)\n    df_test['pos'] = df_test.longitude.round(precision).astype(str) + '_' + df_test.latitude.round(precision).astype(str)\n    \n    # Degree of \"outlierness\"\n    OutlierAggregated = (df_train.bedrooms > 4).astype(float)\n    OutlierAggregated2 = (df_test.bedrooms > 4).astype(float)\n    OutlierAggregated += (df_train.bathrooms > 3).astype(float)\n    OutlierAggregated2 += (df_test.bathrooms > 3).astype(float)\n    OutlierAggregated += (df_train.bathrooms < 1).astype(float)\n    OutlierAggregated2 += (df_test.bathrooms < 1).astype(float)\n    x = np.abs((df_train.price - df_train.price.median())/df_train.price.std()) > 0.30\n    OutlierAggregated += x.astype(float)\n    x2 = np.abs((df_test.price - df_train.price.median())/df_train.price.std()) > 0.30\n    OutlierAggregated2 += x2.astype(float)\n    x = np.log1p(df_train.price/(df_train.bedrooms.clip(1,3) + df_train.bathrooms.clip(1,2))) > 8.2\n    OutlierAggregated += x.astype(float)\n    x2 = np.log1p(df_test.price/(df_test.bedrooms.clip(1,3) + df_test.bathrooms.clip(1,2))) > 8.2\n    OutlierAggregated2 += x2.astype(float)\n    x = np.sqrt(((df_train.latitude - df_train.latitude.median())**2) + (df_train.longitude - df_train.longitude.median())**2) > 0.30\n    OutlierAggregated += x.astype(float)\n    x2 = np.sqrt(((df_test.latitude - df_train.latitude.median())**2) + (df_test.longitude - df_train.longitude.median())**2) > 0.30\n    OutlierAggregated2 += x2.astype(float)\n    df_train['num_OutlierAggregated'] = OutlierAggregated.values\n    df_test['num_OutlierAggregated'] = OutlierAggregated2.values\n    \n    # Average interest in unique locations at given precision\n    x = df_train.groupby('pos')['y'].aggregate(['count', 'mean'])\n    d = x.loc[x['count'] >= n_min, 'mean'].to_dict()\n    impute = df_train.y.mean()\n    df_train['num_pos'] = df_train.pos.apply(lambda x: d.get(x, impute))\n    df_test['num_pos'] = df_test.pos.apply(lambda x: d.get(x, impute))\n    \n    # Density in unique locations at given precision\n    vals = df_train['pos'].value_counts()\n    dvals = vals.to_dict()\n    df_train['num_pos_density'] = df_train['pos'].apply(lambda x: dvals.get(x, vals.min()))\n    df_test['num_pos_density'] = df_test['pos'].apply(lambda x: dvals.get(x, vals.min()))\n\n    # Building null\n    df_train['num_building_null'] = (df_train.building_id=='0').astype(float)\n    df_test['num_building_null'] = (df_test.building_id=='0').astype(float)\n    \n    # Building supervised\n    x = df_train.groupby('building_id')['y'].aggregate(['count', 'mean'])\n    d = x.loc[x['count'] >= n_min, 'mean'].to_dict()\n    impute = df_train.y.mean()\n    df_train['num_building_id'] = df_train.building_id.apply(lambda x: d.get(x, impute))\n    df_test['num_building_id'] = df_test.building_id.apply(lambda x: d.get(x, impute))\n    \n    # Building frequency\n    d = np.log1p(df_train.building_id.value_counts()).to_dict()\n    impute = np.min(np.array(list(d.values())))\n    df_train['num_fbuilding'] = df_train.building_id.apply(lambda x: d.get(x, impute))\n    df_test['num_fbuilding'] = df_test.building_id.apply(lambda x: d.get(x, impute))\n    \n    # Manager supervised\n    x = df_train.groupby('manager_id')['y'].aggregate(['count', 'mean'])\n    d = x.loc[x['count'] >= n_min, 'mean'].to_dict()\n    impute = df_train.y.mean()\n    df_train['num_manager'] = df_train.manager_id.apply(lambda x: d.get(x, impute))\n    df_test['num_manager'] = df_test.manager_id.apply(lambda x: d.get(x, impute))\n\n    # Manager frequency\n    d = np.log1p(df_train.manager_id.value_counts()).to_dict()\n    impute = np.min(np.array(list(d.values())))\n    df_train['num_fmanager'] = df_train.manager_id.apply(lambda x: d.get(x, impute))\n    df_test['num_fmanager'] = df_test.manager_id.apply(lambda x: d.get(x, impute))\n    \n    # Creation time features\n    df_train['created'] = pd.to_datetime(df_train.created)\n    df_train['num_created_weekday'] = df_train.created.dt.dayofweek.astype(float)\n    df_train['num_created_weekofyear'] = df_train.created.dt.weekofyear\n    df_test['created'] = pd.to_datetime(df_test.created)\n    df_test['num_created_weekday'] = df_test.created.dt.dayofweek\n    df_test['num_created_weekofyear'] = df_test.created.dt.weekofyear\n    \n    # Bedrooms/Bathrooms/Price\n    df_train['num_bathrooms'] = df_train.bathrooms.clip_upper(4)\n    df_test['num_bathrooms'] = df_test.bathrooms.clip_upper(4)\n    df_train['num_bedrooms'] = df_train.bedrooms.clip_upper(5)\n    df_test['num_bedrooms'] = df_test.bedrooms.clip_upper(5)\n    df_train['num_price'] = df_train.price.clip_upper(10000)\n    df_test['num_price'] = df_test.price.clip_upper(10000)\n    bins = df_train.price.quantile(np.arange(0.05, 1, 0.05))\n    df_train['num_price_q'] = np.digitize(df_train.price, bins)\n    df_test['num_price_q'] = np.digitize(df_test.price, bins)\n    \n    # Composite features based on: \n    # https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths\n    df_train['num_priceXroom'] = (df_train.price / (1 + df_train.bedrooms.clip(1, 4) + 0.5*df_train.bathrooms.clip(0, 2))).values\n    df_test['num_priceXroom'] = (df_test.price / (1 + df_test.bedrooms.clip(1, 4) + 0.5*df_test.bathrooms.clip(0, 2))).values\n    df_train['num_even_bathrooms'] = ((np.round(df_train.bathrooms) - df_train.bathrooms)==0).astype(float)\n    df_test['num_even_bathrooms'] = ((np.round(df_test.bathrooms) - df_test.bathrooms)==0).astype(float)\n    \n    # Other features\n    df_train['num_features'] = df_train.features.apply(lambda x: len(x))\n    df_test['num_features'] = df_test.features.apply(lambda x: len(x))\n    df_train['num_photos'] = df_train.photos.apply(lambda x: len(x))\n    df_test['num_photos'] = df_test.photos.apply(lambda x: len(x))\n    df_train['num_desc_length'] = df_train.description.str.split(' ').str.len()\n    df_test['num_desc_length'] = df_test.description.str.split(' ').str.len()\n    df_train['num_desc_length_null'] = (df_train.description.str.len()==0).astype(float)\n    df_test['num_desc_length_null'] = (df_test.description.str.len()==0).astype(float)\n    \n    # Features/Description Features\n    bows = {'nofee': ['no fee', 'no-fee', 'no  fee', 'nofee', 'no_fee'],\n            'lowfee': ['reduced_fee', 'low_fee','reduced fee', 'low fee'],\n            'furnished': ['furnished'],\n            'parquet': ['parquet', 'hardwood'],\n            'concierge': ['concierge', 'doorman', 'housekeep','in_super'],\n            'prewar': ['prewar', 'pre_war', 'pre war', 'pre-war'],\n            'laundry': ['laundry', 'lndry'],\n            'health': ['health', 'gym', 'fitness', 'training'],\n            'transport': ['train', 'subway', 'transport'],\n            'parking': ['parking'],\n            'utilities': ['utilities', 'heat water', 'water included']\n          }\n    for fname, bow in bows.items():\n        x1 = df_train.description.str.lower().apply(lambda x: np.sum([1 for i in bow if i in x]))\n        x2 = df_train.features.apply(lambda x: np.sum([1 for i in bow if i in ' '.join(x).lower()]))\n        df_train['num_'+fname] = ((x1 + x2) > 0).astype(float).values\n        x1 = df_test.description.str.lower().apply(lambda x: np.sum([1 for i in bow if i in x]))\n        x2 = df_test.features.apply(lambda x: np.sum([1 for i in bow if i in ' '.join(x).lower()]))\n        df_test['num_'+fname] = ((x1 + x2) > 0).astype(float).values\n\n    return df_train, df_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"36a523f5-3f68-6188-d405-b87c3a999a35"},"source":"### A. Load and preprocess datasets"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f3ca83f-9625-60a5-af33-51bcd7852d9d"},"source":"Load data:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d66aff1f-8386-0d09-0ce4-6af1243c71bb"},"outputs":[],"source":"df = pd.read_json('../input/train.json')\ndf_test = pd.read_json('../input/test.json')\ndf['created'] = pd.to_datetime(df.created)\ndf_test['created'] = pd.to_datetime(df_test.created)"},{"cell_type":"markdown","metadata":{"_cell_guid":"444fac90-e6de-dd05-1999-9aee2175b973"},"source":"Location encoding based on:\n\n- [Unsupervised and supervised neighborhood encoding](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/unsupervised-and-supervised-neighborhood-encoding)\n- [Price/Bedrooms/Bathrooms](https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"389404b6-0ad7-16dc-6a10-46c3982df469"},"outputs":[],"source":"dftemp = df.copy()\nfor i in ['latitude', 'longitude']:\n    while(1):\n        x = dftemp[i].median()\n        ix = abs(dftemp[i] - x) > 3*dftemp[i].std()\n        if ix.sum()==0:\n            break\n        dftemp.loc[ix, i] = np.nan\ndftemp = dftemp.loc[dftemp[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n\ndfm = DataFrameMapper([(['latitude'], [StandardScaler()]), (['longitude'], [StandardScaler()])])\n\nfor i in [5, 10, 20, 40]:\n    pipe_location = make_pipeline(dfm, KMeans(n_clusters=i, random_state=1))\n    pipe_location.fit(dftemp);\n    df['location_'+str(i)] = pipe_location.predict(df).astype(str)\n    df_test['location_'+str(i)] = pipe_location.predict(df_test).astype(str)\nfor i in df.location_10.unique():\n    df['num_location_10_'+str(i)] = (df.location_10==i).astype(float)\n    df_test['num_location_10_'+str(i)] = (df_test.location_10==i).astype(float)"},{"cell_type":"markdown","metadata":{"_cell_guid":"82cbe6d5-e8f0-db38-4907-7bb6666be015"},"source":"### B. Keep only relevant numerical features and normalize"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4998fc01-ef4a-8e52-b646-385891a10980"},"outputs":[],"source":"# Get relevant features\ndf, df_test = basic_preprocess(df, df_test, n_min=15, precision=3)\nfeats = [i for i in df.columns.values if i.startswith('num_')]\nx_train = df[feats].values\nx_test = df_test[feats].values\nprint(x_train.shape, x_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8fdfe31e-3b3b-50aa-dc16-3a15091b2541"},"outputs":[],"source":"# Normalize\nfor i in range(x_train.shape[1]):\n    x_test[:, i] = (x_test[:, i] - np.mean(x_train[:, i]))/np.std(x_train[:, i])\n    x_train[:, i] = (x_train[:, i] - np.mean(x_train[:, i]))/np.std(x_train[:, i])"},{"cell_type":"markdown","metadata":{"_cell_guid":"91365173-5d50-10e3-aed6-9df2bb5668b5"},"source":"### C. Build and evaluate Random Forest Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c3ef811-298b-a919-d15a-267da17d6de2"},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nclf_rfc = RandomForestClassifier()\nparams = {\"max_depth\": [12],\n              \"n_estimators\": [1000],\n              \"min_samples_split\": [10],\n              \"bootstrap\": [False]}\ngs_rfc = GridSearchCV(clf_rfc, param_grid=params, scoring='neg_log_loss', n_jobs=3, cv=3, verbose=1)\nstart = time.time()\ngs_rfc.fit(x_train, df.y.values)\nprint('Time: %.2f minutes' % ((time.time() - start)/60))\nprint('Best score: %.4f' % gs_rfc.best_score_)\nprint('Best params: %s' % gs_rfc.best_params_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11bab4cf-f8c6-9c72-4f93-aa7e047cebd1"},"outputs":[],"source":"output_results(gs_rfc, x_test, df_test.listing_id.values, 'basic_rf.csv') # 0.586LB"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88df384a-81eb-86a0-f568-ae6253753289"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}