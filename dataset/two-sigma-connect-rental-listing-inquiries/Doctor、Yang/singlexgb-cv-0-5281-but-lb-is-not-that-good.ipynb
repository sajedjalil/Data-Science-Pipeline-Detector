{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1a65030d-fa8f-45f6-4ee1-c43557004640"},"source":"Single XGB cv score 0.5281,but LB only 0.5447,why???"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc916937-201d-d9a3-1059-8b9d205f2710"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport string\nimport datetime\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings(\"ignore\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f64d69e-d0ab-e3ec-344b-bfbf872f32d1"},"outputs":[],"source":"def load_data():\n    train_path=\"./train.json\"\n    test_path=\"./test.json\"\n    train=pd.read_json(train_path)\n    y=train['interest_level'].reset_index(drop=True)\n    y_map = {'low': 0, 'medium': 1, 'high': 2}\n    y = y.apply(lambda x: y_map[x])\n    test=pd.read_json(test_path).reset_index(drop=True)\n    listing_id=test.listing_id\n    return train,test,y,listing_id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd3e4b47-db8a-7b11-9324-9d10f4ad854c"},"outputs":[],"source":"def process_buildingid(data,data1):\n    fea=['building_id','price','manager_id','bathrooms','bedrooms','latitude','longitude']\n    subdata=data[fea].reset_index(drop=True)\n    subdata=pd.concat((subdata,data1[['Month','Wday','Diff_days']]),axis=1)\n    t=subdata[['building_id','price']]\n    t['building_count']=1\n    t=t.groupby(['building_id']).agg('count').reset_index()\n    t.drop('price',axis=1,inplace=True)\n\n    t1=subdata[['building_id','price','bathrooms','bedrooms','latitude','longitude','Month','Wday','Diff_days']]\n    t1['room_size']=t1['bathrooms']+t1['bedrooms']\n    t1.drop(['bathrooms','bedrooms'],axis=1,inplace=True)\n    \n    subdata=pd.merge(subdata,t,on='building_id',how='left')\n    \n    status=['sum','mean','min','max','median']\n    for i,statu in enumerate(status):\n        temp=t1.groupby('building_id').agg(statu).reset_index()\n        temp.rename(columns={'price':'build_price_{}'.format(statu),'latitude':'build_lat_{}'.format(statu),'longitude':'build_lon_{}'.format(statu),\n                             'Month':'build_month_{}'.format(statu),'Wday':'build_Wday_{}'.format(statu),\n                             'Diff_days':'build_daydiff_{}'.format(statu),'room_size':'build_room_{}'.format(statu)},inplace=True)\n        subdata=pd.merge(subdata,temp,on='building_id',how='left')\n\n    t7=data[fea].reset_index(drop=True)\n    t7=pd.concat((t7,data1[['Month','Wday','Diff_days']]),axis=1)\n    t7['room_size']=t7['bathrooms']+t7['bedrooms']\n    t7.drop(['bathrooms','bedrooms'],axis=1,inplace=True)\n\n    t8=t7[['building_id','manager_id']]\n    t8['build_manager_count']=1\n    t8=t8.groupby(['building_id','manager_id']).agg('sum').reset_index()\n    subdata=pd.merge(subdata,t8,on=['building_id','manager_id'],how='left')\n    \n    for i,statu in enumerate(status):\n        temp=t7.groupby(['building_id','manager_id']).agg(statu).reset_index()\n        temp.rename(columns={'price':'bm_price_{}'.format(statu),'latitude':'bm_lat_{}'.format(statu),'longitude':'bm_lon_{}'.format(statu),\n                             'Month':'bm_month_{}'.format(statu),'Wday':'bm_Wday_{}'.format(statu),\n                       'Diff_days':'bm_daydiff_{}'.format(statu),'room_size':'bm_room_{}'.format(statu)},inplace=True)\n        subdata=pd.merge(subdata,temp,on=['building_id','manager_id'],how='left')\n\n    subdata['build_type']=subdata['building_id'].apply(lambda x : 0 if x==0 else 1)\n    subdata.drop(['building_id','manager_id','Month','Wday','Diff_days'],axis=1,inplace=True)\n    return subdata"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d8ce8e1-6edb-ca43-f46d-4aa29424f539"},"outputs":[],"source":"def Day_situation(x):\n    if x<=10:\n        return 1\n    elif x>20:\n        return 3\n    else:\n        return 2\n        \ndef process_created(data):\n    subdata=pd.DataFrame(data['created'])\n    subdata['date']=pd.to_datetime(data['created'])\n    subdata['Month']=subdata.date.dt.month\n    subdata['Season']=1\n    subdata.loc[subdata['Month'].isin([4,5,6]),'Season']=2\n    subdata.loc[subdata['Month'].isin([7,8,9]),'Season']=3\n    subdata.loc[subdata['Month'].isin([10,11,12]),'Season']=4\n    subdata['Day']=subdata.date.dt.day\n    subdata['day_situation']=subdata.Day.apply(Day_situation)\n    subdata['Wday']=subdata.date.dt.dayofweek\n    subdata['isWeekday']=subdata.Wday.apply(lambda x: 1 if x in [0,6] else 0)\n    subdata['Yday']=subdata.date.dt.dayofyear\n    subdata['Hour']=subdata.date.dt.hour\n    subdata['isNight']=subdata.Hour.apply(lambda x:1 if x>12 else 0)\n    subdata['Diff_days']=subdata['created'].apply(lambda x:(datetime.date(2017,03,01)-\n    datetime.date(int(x.split(' ')[0].split('-')[0]),int(x.split(' ')[0].split('-')[1]),int(x.split(' ')[0].split('-')[2]))).days)\n\n    subdata.drop(['created','date'],axis=1,inplace=True)\n    subdata=subdata.reset_index(drop=True)\n    return subdata\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b21ada2-0282-db40-527d-def126cfb28b"},"outputs":[],"source":"def process_managerid(data,data1):\n    fea=['manager_id','price','bathrooms','bedrooms','latitude','longitude']\n    subdata=data[fea].reset_index(drop=True)\n    subdata=pd.concat((subdata,data1[['Month','Wday','Diff_days']]),axis=1)\n    t=subdata[['manager_id','price']]\n    t['manager_count']=1\n    t=t.groupby(['manager_id']).agg('count').reset_index()\n    t.drop('price',axis=1,inplace=True)\n\n    t1=subdata[['manager_id','price','bathrooms','bedrooms','latitude','longitude','Month','Wday','Diff_days']]\n    t1['room_size']=t1['bathrooms']+t1['bedrooms']\n    t1.drop(['bathrooms','bedrooms'],axis=1,inplace=True)\n    subdata=pd.merge(subdata,t,on='manager_id',how='left')\n    \n    status=['sum','mean','min','max','median']\n    for statu in status:\n        temp=t1.groupby('manager_id').agg(statu).reset_index()\n        temp.rename(columns={'price':'manager_price_{}'.format(statu),'latitude':'manager_lat_{}'.format(statu),'longitude':'manager_lon_{}'.format(statu),\n                   'Month':'manager_month_{}'.format(statu),'Wday':'manager_Wday_{}'.format(statu),\n                   'Diff_days':'manager_daydiff_{}'.format(statu),'room_size':'manager_room_{}'.format(statu)},inplace=True)\n        subdata=pd.merge(subdata,temp,on='manager_id',how='left')\n    subdata.drop(['manager_id','price','bathrooms','bedrooms','latitude','longitude','Month','Wday','Diff_days'],axis=1,inplace=True)\n    return subdata\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"716b3763-79f3-8d66-718b-a4524fba055b"},"outputs":[],"source":"def get_word_feature(data):\n    fea=['description','display_address','features','photos','street_address']\n    subdata=data[fea].reset_index(drop=True)\n    feature_transform = CountVectorizer(stop_words='english', max_features=150)\n    #####features########\n    subdata['features'] = subdata[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n#    data1['features'] = data1[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\n    feature_transform.fit(list(subdata['features']))\n    vocabulary=feature_transform.vocabulary_\n    feat_sparse = feature_transform.transform(subdata[\"features\"])\n    fea_counter = pd.DataFrame([pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0])])\n    fea_counter.columns = list(sorted(vocabulary.keys()))\n    subdata['features_count']=subdata['features'].apply(lambda x:len(x))\n    #####description######\n    subdata['description'] = subdata['description'].apply(lambda x: x.replace('<p><a  website_redacted ', ''))\n    subdata['description'] = subdata['description'].apply(lambda x: x.replace('!<br /><br />', ''))\n\n    string.punctuation.__add__('!!')\n    string.punctuation.__add__('(')\n    string.punctuation.__add__(')')\n    remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n\n    subdata['description'] = subdata['description'].apply(lambda x: x.translate(remove_punct_map))\n    subdata['desc_letter_count']=subdata['description'].apply(lambda x:len(x.strip()))\n    subdata['desc_words_count'] = subdata['description'].apply(lambda x: 0 if len(x.strip()) == 0 else len(x.split(' ')))\n    ######adddress########\n    subdata['address1'] = subdata['display_address']\n    subdata['address1'] = subdata['address1'].apply(lambda x: x.lower())\n\n    address_map = {\n        'w': 'west','st.': 'street','ave': 'avenue',\n        'st': 'street','e': 'east','n': 'north','s': 'south'}\n\n    def address_map_func(s):\n        s = s.split(' ')\n        out = []\n        for x in s:\n            if x in address_map:\n                out.append(address_map[x])\n            else:\n                out.append(x)\n        return ' '.join(out)\n\n    subdata['address1'] = subdata['address1'].apply(lambda x: x.translate(remove_punct_map))\n    subdata['address1'] = subdata['address1'].apply(lambda x: address_map_func(x))\n    new_cols = ['street', 'avenue', 'east', 'west', 'north', 'south']\n\n    for col in new_cols:\n        subdata[col] = subdata['address1'].apply(lambda x: 1 if col in x else 0)\n    subdata['other_address'] = subdata[new_cols].apply(lambda x: 1 if x.sum() == 0 else 0, axis=1)\n    ###########photos#################\n    subdata['photos_count'] = subdata['photos'].apply(lambda x: len(str(x).split(',')))\n    subdata.drop(['description','display_address','features','photos','street_address','address1'],axis=1,inplace=True)\n\n    subdata=pd.concat([subdata,fea_counter],axis=1)\n    return subdata"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02d07ebb-9a09-46f6-f6ac-43b20c43aeeb"},"outputs":[],"source":"train,test,y,listing_id=load_data()\ntrain.drop('interest_level',axis=1,inplace=True)\nn=train.shape[0]\ndata=pd.concat([train,test])\n\ndata1=add_room_price(data)\ndata2=process_created(data) \ndata3=process_buildingid(data,data2)\ndata4=process_managerid(data,data2) \ndata5=get_word_feature(data)\ndata_concat=pd.concat([data1,data2,data3,data4,data5],axis=1)\n\nX_train=data_concat.iloc[:n,:]\nX_test=data_concat.iloc[n:,:]\n\nX_training,X_val,y_training,y_val=train_test_split(X_train,y,test_size=0.3,random_state=0)\ndtrain=xgb.DMatrix(data=X_training,label=y_training)\ndval=xgb.DMatrix(data=X_val,label=y_val)\ndtest=xgb.DMatrix(data=X_test)\n\nparams = {\n    'eta':.02,\n    'max_depth':4,\n    'min_child_weight':3,\n    \"n_estimators\":600,\n    'early_stopping_rounds':30,\n    'colsample_bytree':.7,\n    'subsample':.7,\n    'gamma':0.1,\n    'seed':0,\n    'nthread':-1,\n    'objective':'multi:softprob',\n    'eval_metric':'mlogloss',\n    'num_class':3,\n    'silent':1\n    }\n\nwatchlist=[(dtrain,'train'),(dval,'val')]\n#xgb_cv=xgb.cv(params,dtrain, num_boost_round=5600, nfold=4,seed=0)\n#print \"Min_logloss{}\".format(min(xgb_cv['test-mlogloss-mean']))\n#print \"Best_Rounds{}\".format(np.argmin(xgb_cv['test-mlogloss-mean']))\n#best_rounds = np.argmin(xgb_cv['test-mlogloss-mean'])\nbst=xgb.train(params,dtrain,3500,evals=watchlist) #cv 0.5281\npre_test=bst.predict(dtest)\n\ndef prepare_submission(preds):\n    now = datetime.datetime.now()   \n    submission = pd.DataFrame(data = {'listing_id': listing_id})\n    submission['low'] = preds[:, 0]\n    submission['medium'] = preds[:, 1]\n    submission['high'] = preds[:, 2]\n    sub_file = './Submission/'+'Submission_' + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n    submission.to_csv(sub_file,index = False)\nprepare_submission(pre_test)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}