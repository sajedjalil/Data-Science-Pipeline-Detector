{"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","name":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"collapsed":false,"_uuid":"02293ca1545fa94b6502a4a5c18132ff37f6208b","_execution_state":"idle"},"execution_count":null,"outputs":[],"source":"# Initial Analysis of the RentHop Dataset\nCredit to Mitchell Spryn \n\nThe goal of this notebook is to develop an intuitive understanding of the data provided for the renthop kaggle challenge ([link](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries)). We'll examine feature distributions and try training a naive model off of the most promising basic features.","cell_type":"markdown"},{"metadata":{"_cell_guid":"2f7b379d-8030-43b3-a37d-f358dbedd19a","_uuid":"d4ef1353ca4f6bbe58e8976ef02bbb1e00b2ba6c","trusted":false},"execution_count":null,"outputs":[],"source":"# Exploratory Analysis of Renthop Competition\n\n# Credit to Mitchell Spryn \n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix\n\ndef make_auto_pct_string(values):\n    def call_fxn(percent):\n        total = sum(values)\n        value = int(round(percent*total/100.0))\n        return '{p:.2f}%  ({v:d})'.format(p=percent,v=value)\n    return call_fxn\n\ndef pie_by_interest(dataset, title):\n    grouped = dataset.groupby(['interest_level']).size().reset_index(name='count')\n    fig = plt.figure(figsize=(8,8))\n    ax = plt.axes(aspect=1)\n    plt.pie(grouped['count'], labels=grouped['interest_level'], autopct=make_auto_pct_string(grouped['count']))\n    plt.title(title)\n    plt.legend()\n    plt.show()\n    \ndef histogram_by_interest(dataset, fieldname, title, bins=50):\n    sns.distplot(dataset[fieldname], kde=False, rug=False, bins=bins, color='y', label='all')\n    sns.distplot(dataset[dataset['interest_level'] == 'low'][fieldname], kde=False, rug=False, bins=bins, color = 'g', label='low')\n    sns.distplot(dataset[dataset['interest_level'] == 'medium'][fieldname], kde=False, rug=False, bins=bins, color = 'c', label='medium')\n    sns.distplot(dataset[dataset['interest_level'] == 'high'][fieldname], kde=False, rug=False, bins=bins, color='r', label='high')\n    plt.legend()\n    plt.title(title)\n    plt.show()\n    \npie_by_interest(train_data, 'Distribution of Labels in Train Dataset')\n\n\ntrain_data_high = train_data[train_data['interest_level'] == 'high']\ntrain_data_medium = train_data[train_data['interest_level'] == 'medium']\ntrain_data_low = train_data[train_data['interest_level'] == 'low']\n\ndescription_length = train_data.copy()\ndescription_length['description_length'] = description_length.apply(lambda r: len(r['description']), axis=1)\n\n#count number of zero descriptions\nno_description = description_length[description_length['description_length'] == 0]\nprint('Number of data points without description: {0} ({1}%)'.format(\\\n            no_description.shape[0], no_description.shape[0]/description_length.shape[0]))\n\n#Show histogram\nhistogram_by_interest(description_length, 'description_length', 'Description length by interest level')\n\n\npie_by_interest(description_length[description_length['description_length'] < 90], 'Short Description by Label')\n\ndef guid_to_categorical(series, series_name_pfx):\n    guid_to_index = {}\n    index_to_guid = {}\n    new_index = 0\n    number_unique = len(series.unique())\n    output_data = [[0 for i in range(0, number_unique, 1)] for j in range(0, len(series))]\n    for i in range(0, len(series), 1):\n        item = series.iloc[i]\n        if item not in guid_to_index:\n            guid_to_index[item] = new_index\n            index_to_guid[new_index] = item\n            new_index += 1\n        output_data[i][guid_to_index[item]] = 1\n    \n    column_names = ['{0}_{1}'.format(series_name_pfx, index_to_guid[i]) for i in range(0, number_unique, 1)]\n    \n    print('Max index: {0}'.format(new_index))\n    return pd.DataFrame(data = output_data, columns = column_names)\n\ndef features_to_categorical(features_series, series_name_pfx):\n    word_column_index = {}\n    index_to_word = {}\n    new_index = 0\n    for feature_set in features_series:\n        for word in set(feature_set):\n            if word not in word_column_index:\n                word_column_index[word] = new_index\n                index_to_word[new_index] = word\n                new_index += 1\n    \n    out_data = [[0 for i in range(0, new_index, 1)] for j in range(0, len(features_series), 1)]\n    \n    for i in range(0, len(features_series), 1):\n        features = features_series.iloc[i]\n        for j in range(0, len(features), 1):\n            current_feature = features[j]\n            out_data[i][word_column_index[current_feature]] += 1\n    \n    out_data_column_names = ['{0}_{1}'.format(series_name_pfx, index_to_word[i]) for i in range(0, new_index, 1)]\n    \n    print('Max index: {0}'.format(new_index))\n    return pd.DataFrame(data = out_data, columns = out_data_column_names)\n    \n#Create features explored earlier\nprint('Generating features...')\ninput_train_data = train_data.copy()\ninput_train_data['description_length'] = input_train_data.apply(lambda r: len(r['description']), axis=1)\ninput_train_data['no_description'] = input_train_data.apply(lambda r: len(r['description']) == 0, axis=1)\ninput_train_data['number_of_photos'] = input_train_data.apply(lambda r: len(r['photos']), axis=1)\n\n#Create categorical features\nprint('Generating categorical features...')\ncategorical_building_id = guid_to_categorical(input_train_data['building_id'], 'building')\ncategorical_manager_id = guid_to_categorical(input_train_data['manager_id'], 'manager')\ncategorical_features = features_to_categorical(input_train_data['features'], 'feature')\ninput_train_data = pd.concat([input_train_data, categorical_features, categorical_building_id, categorical_manager_id], axis=1)\n\nprint('Deleting excess columns...')\ndel input_train_data['created']\ndel input_train_data['description']\ndel input_train_data['display_address']\ndel input_train_data['listing_id']\ndel input_train_data['photos']\ndel input_train_data['features']\ndel input_train_data['street_address']\ndel input_train_data['building_id']\ndel input_train_data['manager_id']\n\nprint('Generating labels...')\nlabels = input_train_data['interest_level'].apply(lambda r: 0 if r == 'low' else 1 if r == 'medium' else 2)\ndel input_train_data['interest_level']\n\nprint('Splitting data...')\n#Split data using sklearn\nx_train, x_test, y_train, y_test = train_test_split(input_train_data, labels, test_size=0.20, random_state=42, stratify=labels)\n\nprint('Training model...')\n#Train tree model\nmodel = ExtraTreesClassifier()\nmodel = model.fit(x_train, y_train)\n\n#Generate confusion matrix\nprint('Generating confusion matrix and feature importances...')\npredictions = model.predict(x_test)\nprint(\"Confusion matrix.\")\nprint(confusion_matrix(y_test, predictions))\n\n#Plot feature importances\nprint(\"Feature importances\")\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(x_train.shape[1]):\n    print(\"{0}. feature {1} ({2}) ({3})\".format(f + 1, indices[f], importances[indices[f]], x_train.columns[indices[f]]))\n\nplt.figure(figsize=(15, 15))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(x_train.shape[1]), indices)\nplt.xlim([-1, x_train.shape[1]])\n\n\n","cell_type":"code"}]}