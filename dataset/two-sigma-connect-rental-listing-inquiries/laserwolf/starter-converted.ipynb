{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_cell_guid":"5fd3562c-2f0c-97e7-9367-221eedcee26f","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"markdown","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"3a0339ae-6660-105f-2328-d7ed1ba5fa39","_active":false,"collapsed":false},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport scipy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom scipy import sparse\nimport xgboost as xgb\n\nimport os\nimport sys\nimport operator","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"bd3f1dda-3349-4b4a-a2c0-0806c13859e0","_active":false,"collapsed":false},"source":"df=pd.read_json(\"../input/train.json\")","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"11eaccca-8cfb-b981-d94e-741b0e8d2165","_active":false,"collapsed":false},"source":"df['priceperbed']=(df['price']/df['bedrooms'].clip(lower=1))\ndf['created']=df['created'].astype(np.datetime64)\ndf['created_date']=np.array(df.created.values, dtype='datetime64[D]').astype(np.float32)\ndf['created_day']=np.array(df.created.values, dtype='datetime64[D]').astype(np.float32)%7\ndf['created_week']=np.array(df.created.values, dtype='datetime64[W]').astype(np.float32)\ndf['created_month']=np.array(df.created.values, dtype='datetime64[M]').astype(np.float32)\ndf['created_hour']=np.array(df.created.values, dtype='datetime64[h]').astype(np.float32)%24\ndf['desc_count']=df.description.apply(lambda x: len(x.split()))\ndf['features_count']=df.features.apply(lambda x: len(x))\ndf['photos_count']=df.photos.apply(lambda x: len(x))\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"5e9c7281-ed9e-4c18-741e-8bcad36cc13e","_active":false,"collapsed":false},"source":"categorical = [\"display_address\", \"building_id\", \"street_address\", \"manager_id\"]\nfor f in categorical:\n        if df[f].dtype=='object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(df[f].values) + list(df[f].values))\n            df[f] = lbl.transform(list(df[f].values))","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"b1bfb737-1178-fb7a-a2c9-c3721bf17b4b","_active":false,"collapsed":false},"source":"df['features'] = df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\nprint(df[\"features\"].head())\ntfidf = CountVectorizer(stop_words='english', max_features=200)\nfeatures_sparse = tfidf.fit_transform(df[\"features\"])","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e6e45daa-73fc-e217-313f-9157f3c68aa6","_active":false,"collapsed":false},"source":"train_ix, test_ix = train_test_split(range(df.shape[0]), random_state=0)\ntrain_ix, val_ix = train_test_split(train_ix, random_state=0)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"58ad657a-895f-33b8-2849-4553f77b9799","_active":false,"collapsed":false},"source":"temp = pd.concat([df.iloc[train_ix].manager_id,pd.get_dummies(df.iloc[train_ix].interest_level)], \n                 axis = 1).groupby('manager_id').mean()\ntemp.columns = ['high_frac','low_frac', 'medium_frac']\ntemp['count'] = df.iloc[train_ix].groupby('manager_id').count().iloc[:,1]\n\ntemp['manager_skill'] = temp['high_frac']*2 + temp['medium_frac']\nunranked_managers_ixes = temp['count']<8\nranked_managers_ixes = ~unranked_managers_ixes\nmean_values = temp.loc[ranked_managers_ixes, [\n    'high_frac','low_frac', 'medium_frac','manager_skill']].mean()\ntemp.loc[unranked_managers_ixes,[\n    'high_frac','low_frac', 'medium_frac','manager_skill']] = mean_values.values\n\ndf = df.merge(temp.reset_index(),how='left', on='manager_id')\nnew_manager_ixes = df['high_frac'].isnull()\ndf.loc[new_manager_ixes,['high_frac','low_frac', 'medium_frac','manager_skill'\n                            ]] = mean_values.values","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"f0faa73a-e4ae-766d-2bb7-258b1cd2b3e8","_active":false,"collapsed":false},"source":"cols=['price', 'bathrooms', 'bedrooms', 'latitude', 'longitude', 'created_hour', 'created_day',\n      'created_month', 'created_date', 'desc_count', 'photos_count', 'features_count', \n      'priceperbed', 'manager_id']+categorical","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e1ee2c55-172a-d026-9517-33e4a64762d7","_active":false,"collapsed":false},"source":"X = sparse.hstack([df[cols], features_sparse]).tocsr()\n\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ny = np.array(df['interest_level'].apply(lambda x: target_num_map[x]))\nprint(X.shape, y.shape)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"0d8bb7d4-a3ea-94a4-c6d2-086f7a14b95f","_active":false,"collapsed":false},"source":"xgtrain = xgb.DMatrix(data=X[train_ix], label=y[train_ix])\nxgval = xgb.DMatrix(data=X[val_ix], label=y[val_ix])","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"1fa4bddd-cf6f-d5f2-5fdb-8b7312aadec4","_active":false,"collapsed":false},"source":"params = {\n    #'eta':.15,\n    'eta':.15,\n    'max_depth':6,\n    'min_child_weight':3,\n    #'min_child_weight':1,\n    'colsample_bytree':.8,\n    'subsample':.8,\n    #'colsample_bytree':.7,\n    #'subsample':.7,\n    'seed':0,\n    'nthread':16,\n    'objective':'multi:softprob',\n    'eval_metric':'mlogloss',\n    'num_class':3,\n}\n\nclf = xgb.train(params, xgtrain, 189,#1000, \n                #evals=[(xgtrain, 'train'), (xgval, 'val')], \n                #early_stopping_rounds=15, \n                verbose_eval=25)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e0faefda-68af-4957-7ff5-21528b0100a9","_active":true,"collapsed":false},"source":"y_pred = clf.predict(xgtrain)\nscore=log_loss(y[train_ix], y_pred)\nprint(score)\ny_pred = clf.predict(xgval)\nscore=log_loss(y[val_ix], y_pred)\nprint(score)","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"}]}