{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f0587cf-7722-1b77-566f-b2606c9cce94"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom itertools import product\nfrom sklearn.model_selection import StratifiedKFold\n\nimport os\nimport sys\nimport operator\nimport math\nfrom scipy import sparse\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\ndef add_features(df):\n    fmt = lambda s: s.replace(\"\\u00a0\", \"\").strip().lower()\n    df[\"photo_count\"] = df[\"photos\"].apply(len)\n    df[\"street_address\"] = df['street_address'].apply(fmt)\n    df[\"display_address\"] = df[\"display_address\"].apply(fmt)\n    df[\"desc_wordcount\"] = df[\"description\"].apply(str.split).apply(len)\n    df[\"pricePerBed\"] = df['price'] / df['bedrooms']\n    df[\"pricePerBath\"] = df['price'] / df['bathrooms']\n    df[\"pricePerRoom\"] = df['price'] / (df['bedrooms'] + df['bathrooms'])\n    df[\"bedPerBath\"] = df['bedrooms'] / df['bathrooms']\n    df[\"bedBathDiff\"] = df['bedrooms'] - df['bathrooms']\n    df[\"bedBathSum\"] = df[\"bedrooms\"] + df['bathrooms']\n    df[\"bedsPerc\"] = df[\"bedrooms\"] / (df['bedrooms'] + df['bathrooms'])\n\n    df = df.fillna(-1).replace(np.inf, -1)\n    return df\n\n\ndef factorize(df1, df2, column):\n    ps = df1[column].append(df2[column])\n    factors = ps.factorize()[0]\n    df1[column] = factors[:len(df1)]\n    df2[column] = factors[len(df1):]\n    return df1, df2\n\n\ndef designate_single_observations(df1, df2, column):\n    ps = df1[column].append(df2[column])\n    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n    return df1, df2\n\n\ndef hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n    \"\"\"\n    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n    Classification and Prediction Problems\" by Daniele Micci-Barreca\n    \"\"\"\n    hcc_name = \"_\".join([\"hcc\", variable, target])\n\n    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n\n    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n    if r_k: df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n\n    if update_df is None: update_df = test_df\n    if hcc_name not in update_df.columns: update_df[hcc_name] = np.nan\n    update_df.update(df)\n    return\n\n\ndef create_binary_features(df):\n    bows = {\n        \"dogs\": (\"dogs\", \"dog\"),\n        \"cats\": (\"cats\",),\n        \"nofee\": (\"no fee\", \"no-fee\", \"no  fee\", \"nofee\", \"no_fee\"),\n        \"lowfee\": (\"reduced_fee\", \"low_fee\", \"reduced fee\", \"low fee\"),\n        \"furnished\": (\"furnished\",),\n        \"parquet\": (\"parquet\", \"hardwood\"),\n        \"concierge\": (\"concierge\", \"doorman\", \"housekeep\", \"in_super\"),\n        \"prewar\": (\"prewar\", \"pre_war\", \"pre war\", \"pre-war\"),\n        \"laundry\": (\"laundry\", \"lndry\"),\n        \"health\": (\"health\", \"gym\", \"fitness\", \"training\"),\n        \"transport\": (\"train\", \"subway\", \"transport\"),\n        \"parking\": (\"parking\",),\n        \"utilities\": (\"utilities\", \"heat water\", \"water included\")\n    }\n\n    def indicator(bow):\n        return lambda s: int(any([x in s for x in bow]))\n\n    features = df[\"features\"].apply(lambda f: \" \".join(f).lower())   # convert features to string\n    for key in bows:\n        df[\"feature_\" + key] = features.apply(indicator(bows[key]))\n\n    return df\n    \n    \n# Load data\nX_train = pd.read_json(\"../input/train.json\").sort_values(by=\"listing_id\")\nX_test = pd.read_json(\"../input/test.json\").sort_values(by=\"listing_id\")\n\n# Make target integer, one hot encoded, calculate target priors\nX_train = X_train.replace({\"interest_level\": {\"low\": 0, \"medium\": 1, \"high\": 2}})\nX_train = X_train.join(pd.get_dummies(X_train[\"interest_level\"], prefix=\"pred\").astype(int))\nprior_0, prior_1, prior_2 = X_train[[\"pred_0\", \"pred_1\", \"pred_2\"]].mean()\n\n# Add common features\nX_train = add_features(X_train)\nX_test = add_features(X_test)\n\n# Special designation for building_ids, manager_ids, display_address with only 1 observation\nfor col in ('building_id', 'manager_id', 'display_address'):\n    X_train, X_test = designate_single_observations(X_train, X_test, col)\n\n# High-Cardinality Categorical encoding\nskf = StratifiedKFold(5)\nattributes = product((\"building_id\", \"manager_id\"), zip((\"pred_1\", \"pred_2\"), (prior_1, prior_2)))\nfor variable, (target, prior) in attributes:\n    hcc_encode(X_train, X_test, variable, target, prior, k=5, r_k=None)\n    for train, test in skf.split(np.zeros(len(X_train)), X_train['interest_level']):\n        hcc_encode(X_train.iloc[train], X_train.iloc[test], variable, target, prior, k=5, r_k=0.01, update_df=X_train)\n\n# Factorize building_id, display_address, manager_id, street_address\nfor col in ('building_id', 'display_address', 'manager_id', 'street_address'):\n    X_train, X_test = factorize(X_train, X_test, col)\n\n# Create binarized features\nX_train = create_binary_features(X_train)\nX_test = create_binary_features(X_test)\n\n# Save\n\"\"\"\nX_train = X_train.sort_index(axis=1).sort_values(by=\"listing_id\")\nX_test = X_test.sort_index(axis=1).sort_values(by=\"listing_id\")\ncolumns_to_drop = [\"photos\", \"pred_0\",\"pred_1\", \"pred_2\", \"description\", \"features\", \"created\"]\nX_train.drop(columns_to_drop, axis=1, errors=\"ignore\").to_csv(\"data/train_python.csv\", index=False, encoding='utf-8')\nX_test.drop(columns_to_drop, axis=1, errors=\"ignore\").to_csv(\"data/test_python.csv\", index=False, encoding='utf-8')\n\"\"\"    \n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a606883-acb9-1f33-d319-6f89a162c1b5"},"outputs":[],"source":"\ndef runXGB(train_X, train_y, test_X=None, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 6\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = 1\n    param['subsample'] = 0.7\n    param['colsample_bytree'] = 0.7\n    param['seed'] = 8088\n    num_rounds = num_rounds\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_X is not None:\n        if test_y is not None:\n            xgtest = xgb.DMatrix(test_X, label=test_y)\n            watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n            model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=25,\n                             verbose_eval=25)#False)\n        else:\n            xgtest = xgb.DMatrix(test_X)\n            model = xgb.train(plst, xgtrain, num_rounds)\n\n        pred_test_y = model.predict(xgtest)\n        return pred_test_y, model\n    else:\n        evals=xgb.cv(plst, xgtrain, num_rounds, nfold=5, early_stopping_rounds=25,\n                             verbose_eval=25, seed=0)\n        return evals\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82577be0-3265-e3aa-31a4-ed919aaa811d"},"outputs":[],"source":"features_list=['bathrooms', 'bedrooms', 'building_id', 'display_address', 'interest_level', 'latitude', 'listing_id', 'longitude', 'manager_id', 'price', 'street_address', 'pred_0', 'pred_1', 'pred_2', 'photo_count', 'desc_wordcount', 'pricePerBed', 'pricePerBath', 'pricePerRoom', 'bedPerBath', 'bedBathDiff', 'bedBathSum', 'bedsPerc', 'hcc_building_id_pred_1', 'hcc_building_id_pred_2', 'hcc_manager_id_pred_1', 'hcc_manager_id_pred_2', 'feature_dogs', 'feature_cats', 'feature_nofee', 'feature_lowfee', 'feature_furnished', 'feature_parquet', 'feature_concierge', 'feature_prewar', 'feature_laundry', 'feature_health', 'feature_transport', 'feature_parking', 'feature_utilities']\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"365e1c1a-f812-0eb4-83d7-6a06a7512a04"},"outputs":[],"source":"#skf = StratifiedKFold(5)\nkf = KFold(n_splits=5, random_state=0)\n\nscores=[]\nbest_i=[]\n#for train_index, test_index in skf.split(X_train.index, X_train['interest_level']):\nfor train_index, test_index in kf.split(X_train):\n    tr=X_train.loc[train_index, features_list]\n    tr_y=X_train.loc[train_index, ['interest_level']].values\n    te=X_train.loc[test_index, features_list]\n    te_y=X_train.loc[test_index, ['interest_level']].values\n    preds, model = runXGB(tr, tr_y, te, te_y)\n    scores+=[model.best_score]\n    best_i+=[model.best_iteration]\n    print(\"%.6f %d\"%(model.best_score, model.best_iteration))\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}