{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f560b2c7-1cd6-741d-666b-465ce3b756fc"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom math import exp\nimport xgboost as xgb\nfrom sklearn.cluster import Birch\n\nrandom.seed(321)\nnp.random.seed(321)\n\nX_train = pd.read_json(\"../input/train.json\")\nX_test = pd.read_json(\"../input/test.json\")\n\ninterest_level_map = {'low': 0, 'medium': 1, 'high': 2}\nX_train['interest_level'] = X_train['interest_level'].apply(lambda x: interest_level_map[x])\nX_test['interest_level'] = -1\n\n#add features\nfeature_transform = CountVectorizer(stop_words='english', max_features=150)\nX_train['features'] = X_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nX_test['features'] = X_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nfeature_transform.fit(list(X_train['features']) + list(X_test['features']))\n\ntrain_size = len(X_train)\nlow_count = len(X_train[X_train['interest_level'] == 0])\nmedium_count = len(X_train[X_train['interest_level'] == 1])\nhigh_count = len(X_train[X_train['interest_level'] == 2])\n\ndef find_objects_with_only_one_record(feature_name):\n    temp = pd.concat([X_train[feature_name].reset_index(), \n                      X_test[feature_name].reset_index()])\n    temp = temp.groupby(feature_name, as_index = False).count()\n    return temp[temp['index'] == 1]\n\nmanagers_with_one_lot = find_objects_with_only_one_record('manager_id')\nbuildings_with_one_lot = find_objects_with_only_one_record('building_id')\naddresses_with_one_lot = find_objects_with_only_one_record('display_address')\n\nlambda_val = None\nk=5.0\nf=1.0\nr_k=0.01 \ng = 1.0\n\ndef categorical_average(variable, y, pred_0, feature_name):\n    def calculate_average(sub1, sub2):\n        s = pd.DataFrame(data = {\n                                 variable: sub1.groupby(variable, as_index = False).count()[variable],                              \n                                 'sumy': sub1.groupby(variable, as_index = False).sum()['y'],\n                                 'avgY': sub1.groupby(variable, as_index = False).mean()['y'],\n                                 'cnt': sub1.groupby(variable, as_index = False).count()['y']\n                                 })\n                                 \n        tmp = sub2.merge(s.reset_index(), how='left', left_on=variable, right_on=variable) \n        del tmp['index']                       \n        tmp.loc[pd.isnull(tmp['cnt']), 'cnt'] = 0.0\n        tmp.loc[pd.isnull(tmp['cnt']), 'sumy'] = 0.0\n\n        def compute_beta(row):\n            cnt = row['cnt'] if row['cnt'] < 200 else float('inf')\n            return 1.0 / (g + exp((cnt - k) / f))\n            \n        if lambda_val is not None:\n            tmp['beta'] = lambda_val\n        else:\n            tmp['beta'] = tmp.apply(compute_beta, axis = 1)\n            \n        tmp['adj_avg'] = tmp.apply(lambda row: (1.0 - row['beta']) * row['avgY'] + row['beta'] * row['pred_0'],\n                                   axis = 1)\n                                   \n        tmp.loc[pd.isnull(tmp['avgY']), 'avgY'] = tmp.loc[pd.isnull(tmp['avgY']), 'pred_0']\n        tmp.loc[pd.isnull(tmp['adj_avg']), 'adj_avg'] = tmp.loc[pd.isnull(tmp['adj_avg']), 'pred_0']\n        tmp['random'] = np.random.uniform(size = len(tmp))\n        tmp['adj_avg'] = tmp.apply(lambda row: row['adj_avg'] *(1 + (row['random'] - 0.5) * r_k),\n                                   axis = 1)\n    \n        return tmp['adj_avg'].ravel()\n     \n    #cv for training set \n    k_fold = StratifiedKFold(5)\n    X_train[feature_name] = -999 \n    for (train_index, cv_index) in k_fold.split(np.zeros(len(X_train)),\n                                                X_train['interest_level'].ravel()):\n        sub = pd.DataFrame(data = {variable: X_train[variable],\n                                   'y': X_train[y],\n                                   'pred_0': X_train[pred_0]})\n            \n        sub1 = sub.iloc[train_index]        \n        sub2 = sub.iloc[cv_index]\n        \n        X_train.loc[cv_index, feature_name] = calculate_average(sub1, sub2)\n    \n    #for test set\n    sub1 = pd.DataFrame(data = {variable: X_train[variable],\n                                'y': X_train[y],\n                                'pred_0': X_train[pred_0]})\n    sub2 = pd.DataFrame(data = {variable: X_test[variable],\n                                'y': X_test[y],\n                                'pred_0': X_test[pred_0]})\n    X_test.loc[:, feature_name] = calculate_average(sub1, sub2)                               \n\ndef transform_data(X):\n    #add features    \n    feat_sparse = feature_transform.transform(X[\"features\"])\n    vocabulary = feature_transform.vocabulary_\n    del X['features']\n    X1 = pd.DataFrame([ pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0]) ])\n    X1.columns = list(sorted(vocabulary.keys()))\n    X = pd.concat([X.reset_index(), X1.reset_index()], axis = 1)\n    del X['index']\n    \n    X[\"num_photos\"] = X[\"photos\"].apply(len)\n    X['created'] = pd.to_datetime(X[\"created\"])\n    X[\"created_hour\"] = X[\"created\"].dt.hour\n\n    X[\"num_description_words\"] = X[\"description\"].apply(lambda x: len(x.split(\" \")))\n    X['price_per_bed'] = X['price'] / X['bedrooms']    \n    X['price_per_bath'] = X['price'] / X['bathrooms']\n    X['price_per_room'] = X['price'] / (X['bathrooms'] + X['bedrooms'] )\n    \n    X['low'] = 0\n    X.loc[X['interest_level'] == 0, 'low'] = 1\n    X['medium'] = 0\n    X.loc[X['interest_level'] == 1, 'medium'] = 1\n    X['high'] = 0\n    X.loc[X['interest_level'] == 2, 'high'] = 1\n    \n    X['display_address'] = X['display_address'].apply(lambda x: x.lower().strip())\n    X['street_address'] = X['street_address'].apply(lambda x: x.lower().strip())\n    \n    X['pred0_low'] = low_count * 1.0 / train_size\n    X['pred0_medium'] = medium_count * 1.0 / train_size\n    X['pred0_high'] = high_count * 1.0 / train_size\n    \n    X.loc[X['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n          'manager_id'] = \"-1\"\n    X.loc[X['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n          'building_id'] = \"-1\"\n    X.loc[X['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n          'display_address'] = \"-1\"\n          \n    return X\n\ndef normalize_high_cordiality_data():\n    high_cardinality = [\"building_id\", \"manager_id\"]\n    for c in high_cardinality:\n        categorical_average(c, \"medium\", \"pred0_medium\", c + \"_mean_medium\")\n        categorical_average(c, \"high\", \"pred0_high\", c + \"_mean_high\")\n\ndef transform_categorical_data():\n    categorical = ['building_id', 'manager_id', \n                   'display_address', 'street_address']\n                   \n    for f in categorical:\n        encoder = LabelEncoder()\n        encoder.fit(list(X_train[f]) + list(X_test[f])) \n        X_train[f] = encoder.transform(X_train[f].ravel())\n        X_test[f] = encoder.transform(X_test[f].ravel())\n                  \n\ndef remove_columns(X):\n    columns = [\"photos\", \"pred0_high\", \"pred0_low\", \"pred0_medium\",\n               \"description\", \"low\", \"medium\", \"high\",\n               \"interest_level\", \"created\"]\n    for c in columns:\n        del X[c]\n\ndef cluster_latlon(n_clusters, data):  \n    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    data_e=data[~(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    #put it in matrix form\n    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n    \n    brc = Birch(branching_factor=100, n_clusters=n_clusters, threshold=0.01,compute_labels=True)\n\n    brc.fit(coords)\n    clusters=brc.predict(coords)\n    data_c[\"cluster_\"+str(n_clusters)]=clusters\n    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n    data=pd.concat([data_c,data_e])\n    return brc, data\n\ndef cluster_latlon2(brc, data):  \n    #split the data between \"around NYC\" and \"other locations\" basically our first two clusters \n    data_c=data[(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    data_e=data[~(data.longitude>-74.05)&(data.longitude<-73.75)&(data.latitude>40.4)&(data.latitude<40.9)]\n    #put it in matrix form\n    coords=data_c.as_matrix(columns=['latitude', \"longitude\"])\n    \n    clusters=brc.predict(coords)\n    data_c[\"cluster_\"+str(n_clusters)]=clusters\n    data_e[\"cluster_\"+str(n_clusters)]=-1 #assign cluster label -1 for the non NYC listings \n    data=pd.concat([data_c,data_e])\n    return data\n\nprint(\"Starting transformations\")        \nX_train = transform_data(X_train)    \nX_test = transform_data(X_test) \n\nbrc, X_train = cluster_latlon(5, X_train)\nX_test=cluster_latlon2(brc, X_test)\n\ny = X_train['interest_level'].ravel()\n\nprint(\"Normalizing high cordiality data...\")\nnormalize_high_cordiality_data()\ntransform_categorical_data()\n\nremove_columns(X_train)\nremove_columns(X_test)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11553527-4b0d-149f-84a9-d90c9b54efc3"},"outputs":[],"source":"print(\"Start fitting...\")\n\nparam = {}\nparam['objective'] = 'multi:softprob'\nparam['eta'] = 0.02\n#param['eta'] = 0.1\nparam['max_depth'] = 6\nparam['silent'] = 1\nparam['num_class'] = 3\nparam['eval_metric'] = \"mlogloss\"\nparam['min_child_weight'] = 1\nparam['subsample'] = 0.7\nparam['colsample_bytree'] = 0.7\nparam['seed'] = 0\nparam['nthread'] = 8\nnum_rounds = 3000\n\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, y, random_state=0)\n\nxgtrain = xgb.DMatrix(X_tr, label=y_tr)\nxgtest = xgb.DMatrix(X_te, label=y_te)\nwatchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\nclf = xgb.train(param, xgtrain, num_rounds, watchlist, early_stopping_rounds=25,\n\tverbose_eval=25)\n\nprint(\"Fitted\")\n\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}