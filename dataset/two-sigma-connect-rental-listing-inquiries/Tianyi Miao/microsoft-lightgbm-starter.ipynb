{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"93f3e145-a484-3449-e2d9-0d0867f8fdb7"},"source":"**Microsoft [LightGBM][1]** is a powerful, open-source boosted decision tree library similar to xgboost. In practice, it runs even faster than xgboost and achieves better performance in some cases.\n\nTo install LightGBM, follow the [installation guide][2] to get the C++ distribution. The python API can then be easily built with these [instructions][3].\n\nSome useful resources for LightGBM python API and parameter tuning:\n\n**[Python API Documentation][4]:** this page includes all the functions and objects\n\n**[List of Parameters][5]:** all possible parameters for LightGBM functions and classes\n\n**[Parameter Tuning Guide][6]:** the advanced parameter tuning guide for LightGBM. Since most parameters in LightGBM are similar to those in XGBoost, it should be intuitive to follow.\n\n\n  [1]: https://github.com/Microsoft/LightGBM\n  [2]: https://github.com/Microsoft/LightGBM/wiki/Installation-Guide\n  [3]: https://github.com/Microsoft/LightGBM/tree/master/python-package\n  [4]: https://github.com/Microsoft/LightGBM/blob/master/docs/Python-API.md\n  [5]: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md\n  [6]: https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters-tuning.md"},{"cell_type":"markdown","metadata":{"_cell_guid":"7abe71f2-0ad1-4f9a-a86b-987dfe79a6e0"},"source":"## Import Libraries, Preprocessing ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fbbab02-330c-10b1-99af-e546690d46c1"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport json\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport lightgbm as lgbm\n\ndef preprocess_1(train_df, test_df):\n    \"\"\"Just a generic preprocessing function, feel free to substitute it with your custom function\"\"\"\n    # encode target variable\n    train_df['interest_level'] = train_df['interest_level'].apply(lambda x: {'high': 0, 'medium': 1, 'low': 2}[x])\n    test_df['interest_level'] = -1\n    train_index = train_df.index\n    test_index = test_df.index\n    data_df = pd.concat((train_df, test_df), axis=0)\n    del train_df, test_df\n    \n    # add counting features\n    data_df['num_photos'] = data_df['photos'].apply(len)\n    data_df['num_features'] = data_df['features'].apply(len)\n    data_df['num_description'] = data_df['description'].apply(lambda x: len(x.split(' ')))\n    data_df.drop('photos', axis=1, inplace=True)\n    \n    # naive feature engineering\n    data_df['room_difference'] = data_df['bedrooms'] - data_df['bathrooms']\n    data_df['total_rooms'] = data_df['bedrooms'] + data_df['bathrooms']\n    data_df['price_per_room'] = data_df['price'] / (data_df['total_rooms'] + 1)\n    \n    # add datetime features\n    data_df['created'] = pd.to_datetime(data_df['created'])\n    data_df['c_month'] = data_df['created'].dt.month\n    data_df['c_day'] = data_df['created'].dt.day\n    data_df['c_hour'] = data_df['created'].dt.hour\n    data_df['c_dayofyear'] = data_df['created'].dt.dayofyear\n    data_df.drop('created', axis=1, inplace=True)\n    \n    # encode categorical features\n    for col in ['display_address', 'street_address', 'manager_id', 'building_id']:\n        data_df[col] = LabelEncoder().fit_transform(data_df[col])\n       \n    data_df.drop('description', axis=1, inplace=True)\n    \n    # get text features\n    data_df['features'] = data_df['features'].apply(lambda x: ' '.join(['_'.join(i.split(' ')) for i in x]))\n    textcv = CountVectorizer(stop_words='english', max_features=200)\n    text_features = pd.DataFrame(textcv.fit_transform(data_df['features']).toarray(),\n                                 columns=['f_' + format(x, '03d') for x in range(1, 201)],\n                                 index=data_df.index)\n    data_df = pd.concat(objs=(data_df, text_features), axis=1)\n    data_df.drop('features', axis=1, inplace=True)\n    \n    feature_cols = [x for x in data_df.columns if x not in {'interest_level'}]\n    return data_df.loc[train_index, feature_cols], data_df.loc[train_index, 'interest_level'],\\\n        data_df.loc[test_index, feature_cols]\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"bc1f3e65-0a99-9cb6-88c1-c988c1f62a3e"},"source":"## Load Data ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd0afa7c-97ec-c6f4-aaaa-f1ad3cc65e4e"},"outputs":[],"source":"train = pd.read_json(open(\"../input/train.json\", \"r\"))\ntest = pd.read_json(open(\"../input/test.json\", \"r\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"be284d13-56c8-7715-3fb1-b264da276660"},"source":"## Define Hyperparameters for LightGBMClassifier ##"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a596eb31-d895-4386-71bf-551b6918e39b"},"outputs":[],"source":"# the following dictionary contains most of the relavant hyperparameters for our task\n# I haven't tuned them yet, so they are mostly default\nt4_params = {\n    'boosting_type': 'gbdt', 'objective': 'multiclass', 'nthread': -1, 'silent': True,\n    'num_leaves': 2**4, 'learning_rate': 0.05, 'max_depth': -1,\n    'max_bin': 255, 'subsample_for_bin': 50000,\n    'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6, 'reg_alpha': 1, 'reg_lambda': 0,\n    'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 10, 'scale_pos_weight': 1}\n\n# they can be used directly to build a LGBMClassifier (which is wrapped in a sklearn fashion)\nt4 = lgbm.sklearn.LGBMClassifier(n_estimators=1000, seed=0, **t4_params)"},{"cell_type":"markdown","metadata":{"_cell_guid":"aa2c3f9b-21fd-be18-c760-cc337ce7d262"},"source":"## Early Stopping with Cross Validation ##\nSimilar to xgboost, we can use cross validation with early stopping to efficiently determine the optimal \"**n_estimators**\" value."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbdfc5d2-d933-cfc5-acff-817069bbf358"},"outputs":[],"source":"def cross_validate_lgbm(filename_str, preprocess_func=preprocess_1):\n    lgbm_params = t4_params.copy()\n    lgbm_params['num_class'] = 3\n    train_X, train_y, test_df = preprocess_func(train, test)\n    dset = lgbm.Dataset(train_X, train_y, silent=True)\n    cv_results = lgbm.cv(\n        lgbm_params, dset, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='multi_logloss',\n        early_stopping_rounds=100, verbose_eval=50, show_stdv=True, seed=0)\n    # note: cv_results will look like: {\"multi_logloss-mean\": <a list of historical mean>,\n    # \"multi_logloss-stdv\": <a list of historical standard deviation>}\n    json.dump(cv_results, open(filename_str, 'w'))\n    print(filename_str)\n    print('best n_estimators:', len(cv_results['multi_logloss-mean']))\n    print('best cv score:', cv_results['multi_logloss-mean'][-1])\n\n# we simply have to run the following code each time we modify the hyperparameters:\ncross_validate_lgbm('lgbm_1.json')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}