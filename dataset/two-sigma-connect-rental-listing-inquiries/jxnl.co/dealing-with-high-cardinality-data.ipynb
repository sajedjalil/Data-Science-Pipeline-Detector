{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5436ef5b-cd11-edbc-4c67-8302be75e7c6"},"source":"# Mean-Encoding High Cardinality Categoricals without Seeing into the future.\n\nWhen we want to build models that predict `y ~ x` and `x` is a high cardinality categorical variable we often try to do one-hot encoding with a sparse matrix and hope that everything works. However it may not be best to so in terms of model complexity, overfitting, and leakage. Instead we may want to try mean encoding features without seeing into the future. \n\n## Mean Encoding  w/ a empirical prior \n\nOften times we may want to mean encode features. Instead of fitting a model like `y ~ (1 if X=x else 0)` we may want to replace it with a estimate of `p(y | X=x)`. While the MLE may work well if all categories had sufficient sample size to do an estimate, in this data set it is not the case.\n\n> For example if we flipped a coin once and got heads. our MLE suggests\n> that` P(heads|coin1) = 1/1 = 1 `and our model would think that this\n> coin is definitly biases. however if we flipped a join 100 times and\n> got 90 heads, `P(heads| coin2) = 90/100 = .9` although the MLE is\n> smaller we have an idea that our estimate is more confident.\n\nTurns out we can think of adding a prior as the same thing as adding fake data, so say we add 10 fake unbiased coin flips we would expect 5 heads, that would mean our new estimate would look like \n\nP(heads | coin1) = (1 + 5 / 1 + 10) = 55% \nP(heads | coin2) = (90 + 5 / 100 + 10) = 86% \n\n## Without seeing into the future\n\nIf we took just the estimates for each id we could come into situations where future events could be predicting the pass, therefore, we should also make sure that on any given date, we only use historical data.\n\n\n# Method\n\nthat being said our method is quite simple.\n\n\n1. Estimate a empirical prior using MLE of the whole data.\n2. On any given day, for a building_id, we use the historical data to estimate the counts, add in the fake data and compute the posterior.\n\n# Contents\n\nThis notebooks goes over a step by step example of how things work and presents the strip in the last few cells. I hope you've learning something either about pandas or math. Let me know if there are more efficient ways of doing what I did!\n\nThanks!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2e099cf-c153-8746-e121-1a7f7de29bf0"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ntrain_df = pd.read_json(\"../input/train.json\")\ntest_df = pd.read_json(\"../input/test.json\")\ntrain_test = pd.concat([train_df, test_df], 0).set_index(\"listing_id\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"68f39ea1-c77e-f677-1993-183f2ea884f4"},"source":"Here we will use building_id but feel free to fork and run it with manager_id, the script will output a CSV indexed on listing_id so you can join that to the rest of your data and fit a model! "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"541f3c0b-ee1f-3906-1c9e-8b081a6db0d8"},"outputs":[],"source":"# ARGUMENTS\n\ncategory = \"building_id\"\nn_fake_data = 20"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94c50a14-d211-9a0d-7c8f-889743ec01f0"},"outputs":[],"source":"df = train_test[[category, \"created\", \"interest_level\"]].copy()\n\ndf[\"low\"] = (df[\"interest_level\"] == \"low\").astype(int)\ndf[\"medium\"] = (df[\"interest_level\"] == \"medium\").astype(int)\ndf[\"high\"] = (df[\"interest_level\"] == \"high\").astype(int)\n\ndf[\"created\"] = pd.to_datetime(df[\"created\"]).dt.dayofyear\ntrain_test[\"created\"] = df[\"created\"]\n\ndel df[\"interest_level\"]\n\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bbe7974f-01ac-fda0-0501-a999d108c91f"},"source":"## Compute Empirical Prior"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c56bfc38-6577-8724-d772-0f06e16129f9"},"outputs":[],"source":"interests = [\"low\", \"medium\", \"high\"]\npriors = df[interests].sum() / df[interests].sum().sum()\npriors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65b81d64-d70f-f752-869f-4da5b1611033"},"outputs":[],"source":"df = (df.\n      sort_values(\"created\").\n      groupby([category, \"created\"]).\n      agg(sum)[interests].\n      reset_index(\"created\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"86e59c73-a92c-391f-0cba-671b65b6fe4e"},"source":"## Example for single building_id"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bdb66f75-2225-267b-44e6-a0d8b9242ba0"},"outputs":[],"source":"# sort on created in order to make sure we are always using historical data\n# see that each day has its own count data for low/medium/high\n\ntemp = df.loc[[\"0\"]].copy().sort_values(\"created\")\ntemp.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13b01323-cff7-9a24-6afb-9e467b3280b4"},"outputs":[],"source":"# by using cumsum we essentually compute the best counts using historical data\n\ntemp[interests] = temp[interests].cumsum(0)\ntemp.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4dc9fa53-97f9-16a2-50e2-0b0663e647a9"},"outputs":[],"source":"# we then shift in order to make sure that one any given day, \n# we do not know what is happening on that day\n\ntemp[interests] = temp[interests].shift().fillna(0)\ntemp.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"514da677-70b0-496e-0aa8-a092eaa695da"},"outputs":[],"source":"# add the prior aka 'fake data' into the mix \n\ntemp[interests] = temp[interests] + n_fake_data * priors\ntemp.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7db2b05e-89dd-2df5-0c44-8f76a9065acc"},"outputs":[],"source":"# then we compute the MLE with the fake data, (not sure if this counts as MAP)\n\nn = temp[interests].sum(1)\ntemp[interests] = temp[interests].apply(lambda _: _/n)\ntemp.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"213ba19a-a1de-fb31-e91d-1bec6a45163f"},"source":"Now we see that each day has a reasonable probability distribution and it has been smoothed out with the prior. also note that as we collect more data, the  influence of the fake samples will disappear. changing `n_fake_data` will change how that influence spreads out.\n\n# Computing Final Results\n\nThe for loop takes a bit, about 2 minutes. if there is a way to do this more efficiently please let me know!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be603c0c-0ae0-bf38-c8a8-ede652b971b3"},"outputs":[],"source":"nd1 = 1 + n_fake_data \nnpriors = n_fake_data * priors "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f4a8d2d-6a2c-3630-41f0-69aa77275658"},"outputs":[],"source":"idxs = set(df.index)\ntotal = len(idxs)\n\nfor i, idx in enumerate(idxs):\n    temp = df.loc[[idx]].copy()\n    if len(temp) == 1:\n        temp.loc[:,interests] = temp.loc[:,interests].fillna(0) + npriors\n        temp.loc[:,interests] /= nd1\n    else:\n        temp.loc[:,interests] = temp.loc[:,interests].cumsum(0).shift().fillna(0) + npriors\n        n = temp.loc[:,interests].sum(1)\n        temp.loc[:,interests] = temp.loc[:,interests].apply(lambda _: _/n)\n    df.loc[[idx]] = temp\n    \n    if i % 1000 == 0:\n        print(\"completed {}/{}\".format(i, total))\n        \ndf.reset_index(category, inplace=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8eaf248-d775-979e-ddd1-df6c3c3b3b75"},"outputs":[],"source":"features = train_test[[category, \"created\"]].copy()\nfeatures[\"listing_id\"] = train_test.index\nfeatures = pd.merge(df, features, left_on=[category, \"created\"], right_on=[category, \"created\"])\nfeatures = features.set_index(\"listing_id\")[interests]\nfeatures.columns = [category + \"_\" + c for c in features.columns]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fbcce15-3481-280f-f8db-eafc690879c8"},"outputs":[],"source":"features.sample(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6a78e9f-959c-c04c-e625-6046d2b557f6"},"outputs":[],"source":"features.to_csv(\"meancoded_{}.csv\".format(category))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}