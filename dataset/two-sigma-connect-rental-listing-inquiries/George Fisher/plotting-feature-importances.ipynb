{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d5b60d6e-e560-e0b2-18c7-8e054eb217e4"},"source":"# Plotting Feature Importances\n\nTree-based models provide a way to tell which features have the biggest impact     \n\nThis function plots feature importances for sklearn models"},{"cell_type":"markdown","metadata":{"_cell_guid":"597b683d-6d62-1f0c-e858-028bcedd8c3c"},"source":"## Concoct a dataset with 20 features, 6 of which are informative\n\n    * features 0-4 are informative (in X_left)\n    * one unknown feature in 5-19 is also informative (X_right)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc227082-ae44-37c9-d1d3-7befade3a6d6"},"outputs":[],"source":"from sklearn.datasets import make_classification\n\nX_left, y = make_classification(n_samples     = 100, \n                                n_features    = 5, \n                                n_informative = 5, \n                                n_redundant   = 0, \n                                n_repeated    = 0, \n                                n_classes     = 2, \n                                random_state  = 7,\n                                n_clusters_per_class=2)\n\nX_right, _ = make_classification(n_samples     = 100, \n                                 n_features    = 15, \n                                 n_informative = 1, \n                                 n_redundant   = 5, \n                                 n_repeated    = 5, \n                                 n_classes     = 1, \n                                 random_state  = 7,\n                                 n_clusters_per_class=1)\n\nimport numpy as np\nX = np.hstack((X_left, X_right))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6882e611-a8f7-97ed-ca9e-f14f0cd6433c"},"outputs":[],"source":"import warnings\nwarnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)"},{"cell_type":"markdown","metadata":{"_cell_guid":"83942df9-40e8-ba05-a1a9-29a595e3f9b3"},"source":"# plot_feature_importances FUNCTION"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f38bd80-de7b-8707-0e82-f146828251c0"},"outputs":[],"source":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility/extendibility\n              2. complicated models/datasets\n          But for many situations Scikit-plot is the way to go\n          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp"},{"cell_type":"markdown","metadata":{"_cell_guid":"0da6b781-5f0e-4910-57cd-fdcb469b8618"},"source":"# Example"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cef6fb2-ba2f-e61d-62d8-f3b360d1ea8c"},"outputs":[],"source":"import pandas as pd\nX_train = pd.DataFrame(X)\ny_train = pd.DataFrame(y)\n\nfrom xgboost              import XGBClassifier\nfrom sklearn.ensemble     import ExtraTreesClassifier\nfrom sklearn.tree         import ExtraTreeClassifier\nfrom sklearn.tree         import DecisionTreeClassifier\nfrom sklearn.ensemble     import GradientBoostingClassifier\nfrom sklearn.ensemble     import BaggingClassifier\nfrom sklearn.ensemble     import AdaBoostClassifier\nfrom sklearn.ensemble     import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom lightgbm             import LGBMClassifier\n\n\nclfs = [XGBClassifier(),              LGBMClassifier(), \n        ExtraTreesClassifier(),       ExtraTreeClassifier(),\n        BaggingClassifier(),          DecisionTreeClassifier(),\n        GradientBoostingClassifier(), LogisticRegression(),\n        AdaBoostClassifier(),         RandomForestClassifier()]\n\nfor clf in clfs:\n    try:\n        _ = plot_feature_importances(clf, X_train, y_train, top_n=X_train.shape[1], title=clf.__class__.__name__)\n    except AttributeError as e:\n        print(e)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"935e2e0b-2f26-2a3c-998a-d740f0f8fe29"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}