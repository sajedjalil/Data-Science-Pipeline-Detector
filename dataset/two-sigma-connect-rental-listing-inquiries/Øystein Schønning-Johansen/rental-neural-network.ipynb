{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"fa002b5c-33c8-c372-087b-6b5292056862"},"source":"OK, folks! How simple can you have a neural network?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c73c0155-30c4-c831-ebb9-4542a025f687"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\n# Look! No scikit learn!"},{"cell_type":"markdown","metadata":{"_cell_guid":"9616592f-caf1-f97a-3973-760f103a62b0"},"source":"### Read the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff63f89f-a372-8b58-dbc0-e379b2a36249"},"outputs":[],"source":"df_train = pd.read_json(open(\"../input/train.json\", \"r\"))\ndf_train.set_index(\"listing_id\", inplace=True)\ndf_test  = pd.read_json(open(\"../input/test.json\", \"r\"))\ndf_test.set_index(\"listing_id\", inplace=True)\n# We will work with a concatenation of the two, then split after the scaling.\ndf = pd.concat([df_train, df_test])"},{"cell_type":"markdown","metadata":{"_cell_guid":"e973717a-e8a8-52ee-8f0a-b31c4f48386b"},"source":"### Simple feature engineering\nLet's do the same feature engineering as Li Li. However w/o the year. Some years are missing, I believe. Edit: No! all samples are 2016. Then we can safely ignore this."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"120de10c-7fe0-cf9f-78ee-951d5e6b07ae"},"outputs":[],"source":"df[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\n#df[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1eebfc1-4153-5e6f-8ad7-b104844ab0ce"},"source":"Since the distribution of prices are so skewed and also probably an important indicator, we must transform the price in some way. The first thing that pops into my head is logarithm transform. Let's try."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"125a1c2c-bcec-2324-611d-c7c966be5b98"},"outputs":[],"source":"df[\"logprice\"] = np.log(df.price)"},{"cell_type":"markdown","metadata":{"_cell_guid":"61366c79-dddc-3b62-a492-ade080516589"},"source":"One of the samples is listed with 112(!) bathrooms. That must be wrong. I'm adjusting that to 1 bathroom. (This gets a more realistic scaling so it improves the overall result)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9164387-a303-d018-0fba-313e11babdbe"},"outputs":[],"source":"df.loc[df.bathrooms == 112, \"bathrooms\"] = 1"},{"cell_type":"markdown","metadata":{"_cell_guid":"b758f124-d50a-113c-2703-450c1eaed3f9"},"source":"Since we will use a neural network we need to scale the features. How simple can we do that?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6a2bb10-c6d6-bfd1-567e-163a400f7fd1"},"outputs":[],"source":"numeric_feat = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"logprice\",\n             \"num_photos\", \"num_features\", \"num_description_words\",\n              \"created_month\", \"created_day\"]\nfor col in numeric_feat:\n    df[col] -= df[col].min()\n    df[col] /= df[col].max()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53f8e82c-f6c5-5b56-95ca-aaee3db00853"},"outputs":[],"source":"X_train = df.loc[df.interest_level.notnull(), numeric_feat]\ny_train = pd.get_dummies(df_train[[\"interest_level\"]], prefix=\"\")\ny_train = y_train[[\"_high\", \"_medium\", \"_low\"]]  # Set the order according to submission\nX_test  = df.loc[df.interest_level.isnull(), numeric_feat]"},{"cell_type":"markdown","metadata":{"_cell_guid":"aa42b3b5-ab61-ee01-71f9-b0b59f8b274a"},"source":"### Clean up\nClean up the unused data frames."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72d53e79-63e0-0020-607b-db18f88c35a3"},"outputs":[],"source":"del df\ndel df_train\ndel df_test"},{"cell_type":"markdown","metadata":{"_cell_guid":"897cbaff-5cc9-73a4-6130-11b3f5889e32"},"source":"### A really simple neural network\nHere is an implementation of a really simple neural network. This is the kind of neural network you would expect in the late 1990's. There is no weight decay regularisation or dropout or anything fancy, so the only way to prevent overfitting is early stopping, and limiting the capacity by setting the number of hidden units.\n\nAlso note that there is only three layers: input, hidden and output. The output has softmax outputs and the hidden layer has sigmoid activation function. Please try other configurations if you like.\n\n(This is the very same implementation I user in [Ghouls, Goblins, and Ghosts... Boo!][1], no modifications what so ever!)\n\n  [1]: https://www.kaggle.com/oysteijo/ghouls-goblins-and-ghosts-boo/ghosts-n-goblins-n-neural-networks-lb-0-74858"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1167a03-076e-3525-04d6-6df14bb9820f"},"outputs":[],"source":"## A dead simple neural network class in Python+Numpy. Plain SGD, and no regularization.\ndef sigmoid(X):\n    return 1.0 / ( 1.0 + np.exp(-X) )\n\ndef softmax(X):\n    _sum = np.exp(X).sum()\n    return np.exp(X) / _sum\n\nclass neuralnet(object):\n    def __init__(self, num_input, num_hidden, num_output):\n        self._W1 = (np.random.random_sample((num_input, num_hidden)) - 0.5).astype(np.float32)\n        self._b1 = np.zeros((1, num_hidden)).astype(np.float32)\n        self._W2 = (np.random.random_sample((num_hidden, num_output)) - 0.5).astype(np.float32)\n        self._b2 = np.zeros((1, num_output)).astype(np.float32)\n\n    def forward(self,X):\n        net1 = np.matmul( X, self._W1 ) + self._b1\n        y = sigmoid(net1)\n        net2 = np.matmul( y, self._W2 ) + self._b2\n        z = softmax(net2)\n        return z,y\n\n    def backpropagation(self, X, target, eta):\n        z, y = self.forward(X)\n        d2 = (z - target)\n        d1 = y*(1.0-y) * np.matmul(d2, self._W2.T)\n        # The updates are done within this method. This more or less implies\n        # utpdates with Stochastic Gradient Decent. Let's fix that later.\n        # TODO: Support for full batch and mini-batches etc.\n        self._W2 -= eta * np.matmul(y.T,d2)\n        self._W1 -= eta * np.matmul(X.reshape((-1,1)),d1)\n        self._b2 -= eta * d2\n        self._b1 -= eta * d1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1ad24ea-4aa6-a290-a24e-01dbe881a595"},"outputs":[],"source":"# Some hyper-parameters to tune.\nnum_hidden = 17    # I think I get about 1 epoch/sec with this size on the docker instance\nn_epochs   = 100\neta        = 0.01"},{"cell_type":"markdown","metadata":{"_cell_guid":"0181e23f-1f5f-ab27-7406-66028678a088"},"source":"Create the neural network."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eac99d46-70a8-4db5-6275-d20b08cdb39e"},"outputs":[],"source":"nn = neuralnet( X_train.shape[1], num_hidden, y_train.shape[1])"},{"cell_type":"markdown","metadata":{"_cell_guid":"f3399011-6d20-1414-1d26-2f14e563bcaa"},"source":"**New!** Let's have a logloss calculation, such that we're not totally in blindness."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"531de4e5-6a8a-a007-de51-cffc1be9734c"},"outputs":[],"source":"def logloss( nn, X, Y ):\n    err = 0\n    for apartment, target in zip( X, Y ):\n        probs = nn.forward( np.array(apartment, dtype=np.float32))[0][0]\n        err += sum(target*np.log(probs))\n    return -err/X.shape[0]"},{"cell_type":"markdown","metadata":{"_cell_guid":"70790efe-e47e-8d34-a4e7-3db71331e6ad"},"source":"Do the training!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d067736c-45b2-0c4d-b02b-43100854222e"},"outputs":[],"source":"# It's much faster to convert the dataframes to numpy arrays and then iterate.\nX = np.array(X_train, dtype=np.float32)\nY = np.array(y_train, dtype=np.float32)\nfor epoch in range(n_epochs):\n    print(\"Epoch: {:3d} train-error: {}\".format(epoch, logloss(nn, X, Y)), end='\\r')    \n    for apartment, target in zip(X,Y):\n        nn.backpropagation( apartment, target, eta)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32d1a04b-e350-2b34-35bd-0aca7fb3e127"},"outputs":[],"source":"with open('submission-{}-hidden.csv'.format(num_hidden), 'w') as f:\n    f.write(\"listing_id,high,medium,low\\n\")\n    for index, apartment in X_test.iterrows():\n        probs = nn.forward( np.array(apartment, dtype=np.float32))[0][0]\n        f.write(\"{},{},{},{}\\n\".format(index, *probs))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b812d7fa-f7ba-d113-1b73-3fc29e73c3b4"},"source":"### TODO\n\n - Local CV!! (Update: we now have training error, but that does not give us the whole truth.)\n - Parameter tuning\n - Feature engineering\n - Add batch/mini-batch training"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}