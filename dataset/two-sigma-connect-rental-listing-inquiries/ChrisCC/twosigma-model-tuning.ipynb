{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom scipy import sparse\nimport pandas as pd\nimport xgboost as xgb\nimport re\nimport string\nimport time\nimport seaborn as sns\nimport itertools\n\nfrom sklearn import preprocessing, pipeline, metrics, model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/train.json.zip', convert_dates=['created'])\ntest_data = pd.read_json('../input/two-sigma-connect-rental-listing-inquiries/test.json.zip', convert_dates=['created'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = train_data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create target variables\n\nWe need to convert the raw target variable into numeric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['target'] = train_data['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\ntrain_data['low'] = train_data['interest_level'].apply(lambda x: 1 if x=='low' else 0)\ntrain_data['medium'] = train_data['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\ntrain_data['high'] = train_data['interest_level'].apply(lambda x: 1 if x=='high' else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merge training and testing data\nSo we don't have to perform transformations twice","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data=pd.concat([train_data,test_data])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Group variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\ncat_vars = ['building_id','manager_id','display_address','street_address']\ntext_vars = ['description','features']\ndate_var = 'created'\nimage_var = 'photos'\nid_var = 'listing_id'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date/time features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\nfull_data['created_year']=full_data['created_datetime'].apply(lambda x:x.year) ## low variant\nfull_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\nfull_data['created_month']=full_data['created_datetime'].apply(lambda x:x.month)\nfull_data['created_day']=full_data['created_datetime'].apply(lambda x:x.day)\nfull_data['created_dayofweek']=full_data['created_datetime'].apply(lambda x:x.dayofweek)\nfull_data['created_dayofyear']=full_data['created_datetime'].apply(lambda x:x.dayofyear)\nfull_data['created_weekofyear']=full_data['created_datetime'].apply(lambda x:x.weekofyear)\nfull_data['created_hour']=full_data['created_datetime'].apply(lambda x:x.hour)\nfull_data['created_epoch']=full_data['created_datetime'].apply(lambda x:x.value//10**9)\n\ndate_num_vars = ['created_month','created_dayofweek','created_dayofyear'\n                 ,'created_weekofyear','created_hour','created_epoch']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Geolocation features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data[\"geo_area_50\"] = \\\n    full_data[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*50)%50)*50+(int(-x[1]*50)%50),axis=1)                                         \n                         \n\nfull_data[\"geo_area_100\"] = \\\n    full_data[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*100)%100)*100+(int(-x[1]*100)%100),axis=1)                                         \n  \n\nfull_data[\"geo_area_200\"] = \\\n    full_data[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*200)%200)*200+(int(-x[1]*200)%200),axis=1)                                         \n\nimport math\n\n# Financial district\nlat=40.705628\nlon=-74.010278\nfull_data['distance_to_fi'] = full_data[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-lat)**2+(x[1]-lon)**2), axis=1)\n\n# Central park\nlat = 40.785091\nlon = -73.968285\nfull_data['distance_to_cp'] = full_data[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-lat)**2+(x[1]-lon)**2), axis=1)\n\n\ngeo_cat_vars = ['geo_area_50', 'geo_area_100', 'geo_area_200']\n\ngeo_num_vars = ['distance_to_fi', 'distance_to_cp']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numeric features: basic engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \nfull_data['num_of_photos'] = full_data['photos'].apply(lambda x:len(x))\nfull_data['num_of_features'] = full_data['features'].apply(lambda x:len(x))\nfull_data['len_of_desc'] = full_data['description'].apply(lambda x:len(x))\nfull_data['words_of_desc'] = full_data['description'].apply(lambda x:len(re.sub('['+string.punctuation+']', '', x).split()))\n\n\nfull_data['nums_of_desc'] = full_data['description']\\\n        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: len([s for s in x if s.isdigit()]))\n        \nfull_data['has_phone'] = full_data['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\nfull_data['has_email'] = full_data['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n\nfull_data['building_id_is_zero'] = full_data['building_id'].apply(lambda x:1 if x=='0' else 0)\n\nadditional_num_vars = ['rooms','num_of_photos','num_of_features','len_of_desc',\n                    'words_of_desc','has_phone','has_email','building_id_is_zero']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numeric-Numeric interactions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data['avg_word_len'] = full_data[['len_of_desc','words_of_desc']]\\\n                                    .apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n    \nfull_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_feature'] = full_data[['price','num_of_features']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_photo'] = full_data[['price','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_word'] = full_data[['price','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_by_desc_len'] = full_data[['price','len_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n\n\nfull_data['photos_per_room'] = full_data[['num_of_photos','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['photos_per_bedroom'] = full_data[['num_of_photos','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['photos_per_bathroom'] = full_data[['num_of_photos','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n\nfull_data['desc_len_per_room'] = full_data[['len_of_desc','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_bedroom'] = full_data[['len_of_desc','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_bathroom'] = full_data[['len_of_desc','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_word'] = full_data[['len_of_desc','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_numeric'] = full_data[['len_of_desc','nums_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n\nfull_data['features_per_room'] = full_data[['num_of_features','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_bedroom'] = full_data[['num_of_features','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_bathroom'] = full_data[['num_of_features','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_photo'] = full_data[['num_of_features','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_word'] = full_data[['num_of_features','words_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_by_desc_len'] = full_data[['num_of_features','len_of_desc']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n\n\ninteractive_num_vars = ['avg_word_len','price_per_room','price_per_bedroom','price_per_bathroom',\n                        'price_per_feature','price_per_photo','price_per_word','price_by_desc_len',\n                        'photos_per_room','photos_per_bedroom','photos_per_bathroom',\n                        'desc_len_per_room','desc_len_per_bedroom','desc_len_per_bathroom','desc_len_per_word',\n                        'desc_len_per_numeric','features_per_room','features_per_bedroom','features_per_bathroom',\n                        'features_per_photo','features_per_word','features_by_desc_len']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndisplay=full_data[\"display_address\"].value_counts()\nmanager_id=full_data[\"manager_id\"].value_counts()\nbuilding_id=full_data[\"building_id\"].value_counts()\nstreet=full_data[\"street_address\"].value_counts()\nbedrooms=full_data[\"bedrooms\"].value_counts()\nbathrooms=full_data[\"bathrooms\"].value_counts()\ncreated_dayofyear=full_data[\"created_dayofyear\"].value_counts()\ncreated_weekofyear=full_data[\"created_weekofyear\"].value_counts()\n\nfull_data[\"display_count\"]=full_data[\"display_address\"].apply(lambda x:display[x])\nfull_data[\"manager_count\"]=full_data[\"manager_id\"].apply(lambda x:manager_id[x])  \nfull_data[\"building_count\"]=full_data[\"building_id\"].apply(lambda x:building_id[x])\nfull_data[\"street_count\"]=full_data[\"street_address\"].apply(lambda x:street[x])\nfull_data[\"bedrooms_count\"]=full_data[\"bedrooms\"].apply(lambda x:bedrooms[x])\nfull_data[\"bathrooms_count\"]=full_data[\"bathrooms\"].apply(lambda x:bathrooms[x])\nfull_data[\"created_dayofyear_count\"]=full_data[\"created_dayofyear\"].\\\n    apply(lambda x:created_dayofyear[x])\nfull_data[\"created_weekofyear_count\"]=full_data[\"created_weekofyear\"].\\\n    apply(lambda x:created_weekofyear[x])\n\ncount_vars = ['manager_count', 'building_count', 'street_count', 'bedrooms_count',\n       'bathrooms_count', 'created_dayofyear_count', 'created_weekofyear_count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numeric-categorical interactions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cat_vars =[]\nprice_by_manager = full_data.groupby('manager_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\nprice_by_manager.columns = ['manager_id','min_price_by_manager',\n                            'max_price_by_manager','median_price_by_manager','mean_price_by_manager']\nfull_data = pd.merge(full_data,price_by_manager, how='left',on='manager_id')\n\nprice_by_building = full_data.groupby('building_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\nprice_by_building.columns = ['building_id','min_price_by_building',\n                            'max_price_by_building','median_price_by_building','mean_price_by_building']\nfull_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n\n\nfull_data['price_percentile_by_manager']=\\\n            full_data[['price','min_price_by_manager','max_price_by_manager']]\\\n            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n                  axis=1)\nfull_data['price_percentile_by_building']=\\\n            full_data[['price','min_price_by_building','max_price_by_building']]\\\n            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n                  axis=1)\n\n\nnum_cat_vars.append('price_percentile_by_manager')\nnum_cat_vars.append('price_percentile_by_building')\n\nprint (num_cat_vars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Two-way categorical features interactions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for comb in itertools.combinations(cat_vars, 2):\n    comb_var_name = comb[0] +'-'+ comb[1]\n    full_data [comb_var_name] = full_data [ comb[0]].astype(str) +'_' + full_data [ comb[1]].astype(str)\n    cat_vars.append(comb_var_name)\n\ncat_vars    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text features\n\n* Here we are using CountVectorizer but you are encouraged to give TfidfVectorizer a try.\n\n* The parameter of max_features to be tuned\n\n* The outputs are sparse matrices which can be merged with numpy arrays using scipy.stats.sparse.hstack function\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ncntvec = CountVectorizer(stop_words='english', max_features=200)\nfeature_sparse =cntvec.fit_transform(full_data[\"features\"]\\\n                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n\nfeature_vars = ['feature_' + v for v in cntvec.vocabulary_]\n\ncntvec = CountVectorizer(stop_words='english', max_features=100)\ndesc_sparse = cntvec.fit_transform(full_data[\"description\"])\ndesc_vars = ['desc_' + v for v in cntvec.vocabulary_]\n\n\ncntvec = CountVectorizer(stop_words='english', max_features=10)\nst_addr_sparse = cntvec.fit_transform(full_data[\"street_address\"])\nst_addr_vars = ['desc_' + v for v in cntvec.vocabulary_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features - label encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LBL = preprocessing.LabelEncoder()\n\nLE_vars=[]\nLE_map=dict()\nfor cat_var in cat_vars:\n    print (\"Label Encoding %s\" % (cat_var))\n    LE_var=cat_var+'_le'\n    full_data[LE_var]=LBL.fit_transform(full_data[cat_var])\n    LE_vars.append(LE_var)\n    LE_map[cat_var]=LBL.classes_\n    \nprint (\"Label-encoded feaures: %s\" % (LE_vars))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features - one hot encoding\n\nThe output is a sparse matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"OHE = preprocessing.OneHotEncoder(sparse=True)\nstart=time.time()\nOHE.fit(full_data[LE_vars])\nOHE_sparse=OHE.transform(full_data[LE_vars])\n                                   \nprint ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n\n\nOHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n                for var in cat_vars for level in LE_map[var] ]\n\nprint (\"OHE_sparse size :\" ,OHE_sparse.shape)\nprint (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features - mean encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom itertools import product\n\nclass MeanEncoder:\n    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n        \"\"\"\n        :param categorical_features: list of str, the name of the categorical columns to encode\n\n        :param n_splits: the number of splits used in mean encoding\n\n        :param target_type: str, 'regression' or 'classification'\n\n        :param prior_weight_func:\n        a function that takes in the number of observations, and outputs prior weight\n        when a dict is passed, the default exponential decay function will be used:\n        k: the number of observations needed for the posterior to be weighted equally as the prior\n        f: larger f --> smaller slope\n        \"\"\"\n\n        self.categorical_features = categorical_features\n        self.n_splits = n_splits\n        self.learned_stats = {}\n\n        if target_type == 'classification':\n            self.target_type = target_type\n            self.target_values = []\n        else:\n            self.target_type = 'regression'\n            self.target_values = None\n\n        if isinstance(prior_weight_func, dict):\n            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n        elif callable(prior_weight_func):\n            self.prior_weight_func = prior_weight_func\n        else:\n            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n\n    @staticmethod\n    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n        X_train = X_train[[variable]].copy()\n        X_test = X_test[[variable]].copy()\n\n        if target is not None:\n            nf_name = '{}_pred_{}'.format(variable, target)\n            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n        else:\n            nf_name = '{}_pred'.format(variable)\n            X_train['pred_temp'] = y_train  # regression\n        prior = X_train['pred_temp'].mean()\n\n        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg(['mean', 'size']).rename(columns={'size': 'beta'})\n        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n\n        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n\n        return nf_train, nf_test, prior, col_avg_y\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        :param X: pandas DataFrame, n_samples * n_features\n        :param y: pandas Series or numpy array, n_samples\n        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n        \"\"\"\n        X_new = X.copy()\n        if self.target_type == 'classification':\n            skf = StratifiedKFold(self.n_splits)\n        else:\n            skf = KFold(self.n_splits)\n\n        if self.target_type == 'classification':\n            self.target_values = sorted(set(y))\n            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n                                  product(self.categorical_features, self.target_values)}\n            for variable, target in product(self.categorical_features, self.target_values):\n                nf_name = '{}_pred_{}'.format(variable, target)\n                X_new.loc[:, nf_name] = np.nan\n                for large_ind, small_ind in skf.split(y, y):\n                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n                    X_new.iloc[small_ind, -1] = nf_small\n                    self.learned_stats[nf_name].append((prior, col_avg_y))\n        else:\n            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n            for variable in self.categorical_features:\n                nf_name = '{}_pred'.format(variable)\n                X_new.loc[:, nf_name] = np.nan\n                for large_ind, small_ind in skf.split(y, y):\n                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n                    X_new.iloc[small_ind, -1] = nf_small\n                    self.learned_stats[nf_name].append((prior, col_avg_y))\n        return X_new\n\n    def transform(self, X):\n        \"\"\"\n        :param X: pandas DataFrame, n_samples * n_features\n        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n        \"\"\"\n        X_new = X.copy()\n\n        if self.target_type == 'classification':\n            for variable, target in product(self.categorical_features, self.target_values):\n                nf_name = '{}_pred_{}'.format(variable, target)\n                X_new[nf_name] = 0\n                for prior, col_avg_y in self.learned_stats[nf_name]:\n                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n                        nf_name]\n                X_new[nf_name] /= self.n_splits\n        else:\n            for variable in self.categorical_features:\n                nf_name = '{}_pred'.format(variable)\n                X_new[nf_name] = 0\n                for prior, col_avg_y in self.learned_stats[nf_name]:\n                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n                        nf_name]\n                X_new[nf_name] /= self.n_splits\n\n        return X_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_encoder = MeanEncoder(categorical_features=['manager_id','building_id'])\nmean_encoded_train = mean_encoder.fit_transform(train_data, train_data['target'])\nmean_encoded_test = mean_encoder.transform(test_data)\n\nmean_coded_vars = list(set(mean_encoded_train.columns) - set(train_data.columns))\nmean_coded_vars.append('listing_id')\nfull_data = pd.merge(full_data, \n                     pd.concat([mean_encoded_train[mean_coded_vars], mean_encoded_test[mean_coded_vars]]),\n                     how='left',\n                     on='listing_id'\n                    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars+ count_vars + LE_vars + mean_coded_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'created_month', 'created_dayofweek', 'created_dayofyear', 'created_weekofyear', 'created_hour', 'created_epoch', 'rooms', 'num_of_photos', 'num_of_features', 'len_of_desc', 'words_of_desc', 'has_phone', 'has_email', 'building_id_is_zero', 'avg_word_len', 'price_per_room', 'price_per_bedroom', 'price_per_bathroom', 'price_per_feature', 'price_per_photo', 'price_per_word', 'price_by_desc_len', 'photos_per_room', 'photos_per_bedroom', 'photos_per_bathroom', 'desc_len_per_room', 'desc_len_per_bedroom', 'desc_len_per_bathroom', 'desc_len_per_word', 'desc_len_per_numeric', 'features_per_room', 'features_per_bedroom', 'features_per_bathroom', 'features_per_photo', 'features_per_word', 'features_by_desc_len', 'geo_area_50', 'geo_area_100', 'geo_area_200', 'manager_count', 'building_count', 'street_count', 'bedrooms_count', 'bathrooms_count', 'created_dayofyear_count', 'created_weekofyear_count', 'building_id_le', 'manager_id_le', 'display_address_le', 'street_address_le', 'building_id-manager_id_le', 'building_id-display_address_le', 'building_id-street_address_le', 'manager_id-display_address_le', 'manager_id-street_address_le', 'display_address-street_address_le', 'building_id_pred_1', 'manager_id_pred_2', 'building_id_pred_0', 'building_id_pred_2', 'manager_id_pred_0', 'manager_id_pred_1', 'listing_id'])-set(['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'created_month', 'created_dayofweek', 'created_dayofyear', 'created_weekofyear', 'created_hour', 'created_epoch', 'rooms', 'num_of_photos', 'num_of_features', 'len_of_desc', 'words_of_desc', 'has_phone', 'has_email', 'building_id_is_zero', 'avg_word_len', 'price_per_room', 'price_per_bedroom', 'price_per_bathroom', 'price_per_feature', 'price_per_photo', 'price_per_word', 'price_by_desc_len', 'photos_per_room', 'photos_per_bedroom', 'photos_per_bathroom', 'desc_len_per_room', 'desc_len_per_bedroom', 'desc_len_per_bathroom', 'desc_len_per_word', 'desc_len_per_numeric', 'features_per_room', 'features_per_bedroom', 'features_per_bathroom', 'features_per_photo', 'features_per_word', 'features_by_desc_len', 'norm_listing_id', 'building_id_le', 'manager_id_le', 'display_address_le', 'street_address_le', 'building_id-manager_id_le', 'building_id-display_address_le', 'building_id-street_address_le', 'manager_id-display_address_le', 'manager_id-street_address_le', 'display_address-street_address_le', 'building_id_pred_1', 'manager_id_pred_2', 'building_id_pred_0', 'building_id_pred_2', 'manager_id_pred_0', 'manager_id_pred_1', 'listing_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM \n\nTypically, GBDT model converges faster with a larger learning rate (e.g 0.1) than smaller learning rate however the accuracy may not be as promising. We will be using 0.1 as the learning rate for the rest of this notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Large learning rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport lightgbm as lgb\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.1\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n# 0.549718","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### small learning rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## listing ID\nTheoretically ID variable is not supposed to be included in training a model. However, for some reason listing_id appears to be correlated to the created date time and therefore might be a good candidate as a feature:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot('listing_id', 'created_epoch', full_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_listing_id = full_data['listing_id'].min()\nmax_listing_id = full_data['listing_id'].max()\nfull_data['norm_listing_id']=full_data['listing_id'].apply(lambda x:np.float64((x-min_listing_id+1))/(max_listing_id-min_listing_id+1))\nlisting_vars = [ 'norm_listing_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n     + listing_vars\nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.1\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mkt_price = full_data.groupby(['building_id', 'display_address', 'bedrooms', 'bathrooms']).price.mean().reset_index()\nmkt_price = pd.merge(full_data[['building_id', 'display_address', 'bedrooms', 'bathrooms']],\n                     mkt_price, how='left', on=['building_id', 'display_address', 'bedrooms', 'bathrooms']).price\nfull_data['mkt_price'] = mkt_price.values\nfull_data['diff_to_mkt_price'] = full_data['price'] - full_data['mkt_price']\nfull_data['ratio_to_mkt_price'] = full_data['price'] / full_data['mkt_price']\n\nprice_vars = ['diff_to_mkt_price', 'ratio_to_mkt_price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hack \"HopScore\"\n\nThough it may not be 100% correlated it turns out that Renthop uses a system called [\"**HopScore**\"](https://www.renthop.com/agent-guide/the-hopscore) to rank listings. According to the official instruction there are three things to consider to improve HopScore:\n\n* Listing freshness\n* Listing quality\n* Manager performance\n\nThis finding is a breakthrough when I worked on feature engineering for this competition and resulted quite a few fruitful ideas.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Listing freshness and listing quality","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import hashlib\n\n# unique identifer for listings - photo links uniquely identify a listing\nfull_data['photos_str'] = full_data['photos'].astype(str)\nfull_data['listing_uid'] = full_data[['manager_id', 'building_id','photos_str']].apply(lambda x: hashlib.md5((x[0] + x[1] + x[2]).encode()).hexdigest(), axis=1 )\nfull_data['posted_times'] = full_data.groupby('listing_uid').created_datetime.rank(method='first', na_option='top',pct=True)\n\n# Using html tag may improve listing quality\nfull_data['num_of_html_tag']=full_data.description.apply(lambda x:x.count('<'))\n\n# Studies have shown that titles with excessive all caps and special characters give renters the impression \n# that the listing is fraudulent â€“ i.e. BEAUTIFUL***APARTMENT***CHELSEA.\nfull_data['num_of_#']=full_data.description.apply(lambda x:x.count('#'))\nfull_data['num_of_!']=full_data.description.apply(lambda x:x.count('!'))\nfull_data['num_of_$']=full_data.description.apply(lambda x:x.count('$'))\nfull_data['num_of_*']=full_data.description.apply(lambda x:x.count('*'))\nfull_data['num_of_>']=full_data.description.apply(lambda x:x.count('>'))\nfull_data['num_of_puncs']=full_data['num_of_#'] + full_data['num_of_!'] + full_data['num_of_$'] + full_data['num_of_*'] + full_data['num_of_>']\nfull_data['puncs_ratio'] = full_data['num_of_puncs']/full_data['len_of_desc']\nfull_data['upper_char_ratio'] = full_data['description'].apply(lambda x: 0 if sum([s.isalpha() for s in x])==0 else sum([s.isalpha()&s.isupper() for s in x])/ sum([s.isalpha() for s in x]))\n\n# Accuracy of location/ address\nfull_data['disp_is_street'] = (full_data['display_address'] == full_data['street_address'])*1\nfull_data['disp_st_addr_word_ratio'] = full_data.apply(lambda x:len(x['display_address'].split(' '))/len(x['street_address'].split(' ')), axis=1)\n\nlisting_quality_vars = ['disp_is_street', 'num_of_html_tag','num_of_#','num_of_!','num_of_$', 'num_of_*',\n                        'posted_times', 'disp_st_addr_word_ratio','upper_char_ratio']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n    + listing_vars + listing_quality_vars\nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n\n# 0.547453","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Manager performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew, kurtosis\n\ndef p25(x):\n    return np.percentile(x, 25)\ndef p50(x):\n    return np.percentile(x, 50)\ndef p75(x):\n    return np.percentile(x, 75)\ndef nunique(x):\n    return np.size(np.unique(x))\ndef max_min(x):\n    return np.max(x)-np.min(x)\ndef p75_p25(x):\n    return np.percentile(x, 75)-np.percentile(x, 25)\n\n\n\ndef get_group_stats(df, stat_funcs, target_column, group_column, ranking=False, ranking_pct=True):\n    aggr = df.groupby(group_column)[target_column].agg([v for v in stat_funcs.values()]).reset_index()\n    aggr.columns = [group_column] + [  target_column + '_' + k + '_by_' + group_column for k in stat_funcs.keys()]\n    aggr = df[[group_column]].merge(aggr, how='left', on=group_column)\n    \n    #rank\n    if ranking:\n        aggr[target_column + '_rank_by_' + group_column] = df.groupby(group_column)[target_column].rank(method='dense', \n                                                                                                    na_option='top',\n                                                                                                    pct=ranking_pct)\n    return aggr.drop(group_column, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stat_funcs = {\n#     'count_unique': nunique,\n    'mean': np.mean,\n    'min': np.min,\n    'max': np.max,\n    'std': np.std,\n    'p25': p25,\n    'p50': p50,\n    'p75': p75,\n    'skew': skew,\n    'kurtosis': kurtosis,\n    'max_min': max_min,\n    'p75_p25': p75_p25\n}\n\n\nmgr_aggr = pd.DataFrame()\nfor num_var in num_vars + additional_num_vars + listing_quality_vars:\n    mgr_aggr = pd.concat([mgr_aggr,\n                          get_group_stats(full_data, stat_funcs,\n                                          target_column=num_var, group_column='manager_id', ranking=False)\n                          ],\n                         axis=1\n                         )\n    \n## manager activeness\nmgr_aggr = pd.concat([mgr_aggr,\n                      get_group_stats(full_data, {'max_min': max_min, 'p75_p25': p75_p25},\n                                      target_column='created_epoch', group_column='manager_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n\nmgr_aggr = pd.concat([mgr_aggr,\n                      get_group_stats(full_data, {'nunique': nunique},\n                                      target_column='created_dayofyear', group_column='manager_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n\n## Buildings managed by the manager\nmgr_aggr = pd.concat([mgr_aggr,\n                      get_group_stats(full_data, {'nunique': nunique},\n                                      target_column='building_id', group_column='manager_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n\n## Areas \nfor aggr_col in ['geo_area_50', 'geo_area_100', 'geo_area_200']:\n    mgr_aggr = pd.concat([mgr_aggr,\n                          get_group_stats(full_data, {'nunique': nunique},\n                                          target_column=aggr_col, group_column='manager_id', ranking=False)\n                          ],\n                         axis=1\n                         )\n\n## Price fairness    \nfor aggr_col in ['diff_to_mkt_price', 'ratio_to_mkt_price']:\n    mgr_aggr = pd.concat([mgr_aggr,\n                          get_group_stats(full_data, {'mean': np.mean},\n                                          target_column=aggr_col, group_column='manager_id', ranking=False)\n                          ],\n                         axis=1\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars\nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n# Best iteration: 306, best score: 0.531753","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Similar for building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stat_funcs = {\n#     'count_unique': nunique,\n    'mean': np.mean,\n    'min': np.min,\n    'max': np.max,\n    'std': np.std,\n    'p25': p25,\n    'p50': p50,\n    'p75': p75,\n    'skew': skew,\n    'kurtosis': kurtosis,\n    'max_min': max_min,\n    'p75_p25': p75_p25\n}\n\n\nbuilding_aggr = pd.DataFrame()\n\nbuilding_aggr = pd.concat([building_aggr,\n                      get_group_stats(full_data, stat_funcs,\n                                      target_column='price', group_column='building_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n    \n\n## Buildings managed by the manager\nbuilding_aggr = pd.concat([building_aggr,\n                      get_group_stats(full_data, {'nunique': nunique},\n                                      target_column='manager_id', group_column='building_id', ranking=False)\n                      ],\n                     axis=1\n                     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n    + listing_vars + listing_quality_vars\nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr,\n                         building_aggr]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr,\n                       building_aggr]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n# Best iteration: 306, best score: 0.531753","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Location Location Location!!!\n\nNot all listings were created equally. Location is one of the most dominant factors when seeking a place to live. When we think of location we are not only talking about the absolute location but the relative location, e.g. proximity to facilities such as school, transportations and supermarkets. Unfortunately, these information are not provided by the dataset naively but thanks to Kaggler [Farron](https://www.kaggle.com/mmueller) who graciously shared his secret sauce which brilliantly hacked the proximity information and helped him win the second place in this competition. Here's what he did: \n> It consists of kmeans cluster of (latitude, longitude) followed by computing statistics like the ones above and cluster center distances. In order to get some proxies for PoI's in the neighborhood, I created clusters after filtering the dataset based on certain words in the descriptions. That way, I estimated coordinates for things like \"supermarket\", \"shopping\", \"subway\", \"bus\", \"health\", \"fitness\", \"park\" etc. Afterwards I created minimal distances to those locations as well as counts based on different distances cut-offs.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Parks\n\nThe biggest challenge for replicating Faron's great idea is to figure out the appropriate number of clusters for each category. How can we do that?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from geopy.distance import vincenty\n\nfrom sklearn.cluster import KMeans\npark_listings = full_data[full_data[['description', 'features']].apply(lambda x: 'park' in x[0] or 'park' in x[1], axis=1)][['latitude', 'longitude']]\n\npark_n_clusters = 25\nkms = KMeans(n_clusters=park_n_clusters)\nkms.fit(park_listings)\n\npark_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n                              columns = ['dist_to_park_' + str(i) for i in range(park_n_clusters)]\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars +geo_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars  \nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr,\n                        building_aggr,\n                        park_dist_data]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr,\n                       building_aggr,\n                       park_dist_data]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For your reference, we can use the following snippet to fine tune the optimal number clusters\n\n```python\n\nscores = []\nfor park_n_clusters in (10, 15, 20, 25, 30):\n    kms = KMeans(n_clusters=park_n_clusters)\n    kms.fit(park_listings)\n\n    park_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n                                  columns=['dist_to_park_' +\n                                      str(i) for i in range(park_n_clusters)]\n                                 )\n\n    full_num_vars = num_vars + date_num_vars + additional_num_vars + \\\n        interactive_num_vars + listing_vars + listing_quality_vars + magic_vars + \\\n        num_cat_vars + mean_coded_vars + distance_vars\n    full_cat_vars = LE_vars\n    full_vars = full_num_vars + full_cat_vars\n    train_x = sparse.hstack([full_data[full_vars],\n                             feature_sparse,\n                             desc_sparse,\n                             st_addr_sparse,\n                             mgr_aggr,\n                            park_dist_data]).tocsr()[:train_size]\n    train_y = full_data['target'][:train_size].values\n    test_x = sparse.hstack([full_data[full_vars],\n                            feature_sparse,\n                            desc_sparse,\n                            st_addr_sparse,\n                            mgr_aggr,\n                            park_dist_data]).tocsr()[train_size:]\n    test_y = full_data['target'][train_size:].values\n\n    full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n    print(\"training data size: \", train_x.shape,\n          \"testing data size: \", test_x.shape)\n\n    lgb_params = dict()\n    lgb_params['objective'] = 'multiclass'\n    lgb_params['num_class'] = 3\n    lgb_params['learning_rate'] = 0.05\n    lgb_params['num_leaves'] = 63\n    lgb_params['max_depth'] = 15\n    lgb_params['min_gain_to_split '] = 1\n    lgb_params['subsample'] = 0.7\n    lgb_params['colsample_bytree'] = 0.7\n    lgb_params['min_sum_hessian_in_leaf'] = 0.001\n    lgb_params['seed'] = 42\n\n    lgb_cv = lgb.cv(lgb_params,\n                    lgb.Dataset(train_x,\n                                label=train_y\n                                ),\n                    num_boost_round=100000,\n                    nfold=5,\n                    stratified=True,\n                    shuffle=True,\n                    early_stopping_rounds=50,\n                    seed=42,\n                    verbose_eval=100)\n\n    best_score = min(lgb_cv['multi_logloss-mean'])\n    best_iteration = len(lgb_cv['multi_logloss-mean'])\n    print('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n    scores.append([park_n_clusters, best_score])\nscores = np.array(scores)\nbest_park_n_clusters = scores[:, 0][(np.argmin(scores[:, 1]))]\nprint('best number of clusters: %d, best score: %f' % (best_park_n_clusters, np.min(scores[:, 1])))\n\n\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Subways","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subway_listings = full_data[full_data[['description', 'features']].apply(lambda x: 'subway' in x[0] or 'subway' in x[1], axis=1)][['latitude', 'longitude']]\n\nsubway_n_clusters = 400\nkms = KMeans(n_clusters=subway_n_clusters)\nkms.fit(subway_listings)\n\nsubway_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n                              columns = ['dist_to_subway_' + str(i) for i in range(subway_n_clusters)]\n                             )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars \nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr,\n                        building_aggr,\n                        park_dist_data,\n                        subway_dist_data]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr,\n                       building_aggr,\n                       park_dist_data,\n                       subway_dist_data]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## item2vec\n\nWe can use the same idea for word2vec to embed any items","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy ## Spacy is the de-facto NLP tool used by industry\nfrom gensim.models import FastText  \nnlp = spacy.load(\"en_core_web_sm\")\n\n## Tokenize a sentence\ndef seq_to_token(seq, nlp=nlp):\n    doc = nlp(str(seq).lower())\n    tokens = [token.text for token in doc if not ( token.is_space | token.is_stop|token.like_num)]\n    return tokens\n\n## Convert tokens to vector\ndef tokens_to_vec(tokens, model, vec_size=10):\n    if len(tokens)==0:\n        return np.zeors(vec_size)\n    else:\n        return np.array([emb_model[token] for token in tokens]).mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embed building id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 1: Generate \"sentences\"\nbuilding_by_mgr = full_data.groupby('manager_id')['building_id'].apply(list)\nbuilding_by_mgr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 2: Train a fasttext model\nbuilding_model = FastText(size=10, window=3, min_count=1, workers=16)  # instantiate\nbuilding_model.build_vocab(sentences=building_by_mgr)\nbuilding_model.train(sentences=building_by_mgr.values, total_examples=len(building_by_mgr.values), epochs=5)\n\n## Take a look at the embedding for building_id 8a8b08e08888819a3e745005a8cd0408\nbuilding_model['8a8b08e08888819a3e745005a8cd0408']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Step 3: Embed building ids\nbuilding_emb = full_data['building_id'].apply(lambda x:building_model[x]).values\nbuilding_emb = np.array([e.reshape(1,-1) for e in building_emb]).reshape(-1,10)\nbuilding_emb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Embed manager id","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"manager_by_building = full_data.groupby('building_id')['manager_id'].apply(list)  \nmanager_model = FastText(size=10, window=3, min_count=1, workers=16)\nmanager_model.build_vocab(sentences=manager_by_building)\nmanager_model.train(sentences=manager_by_building.values, \n                    total_examples=len(manager_by_building.values), epochs=5)\nmanager_emb = full_data['manager_id'].apply(lambda x:manager_model[x]).values\nmanager_emb = np.array([e.reshape(1,-1) for e in manager_emb]).reshape(-1,10)\nmanager_emb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"manager_by_building = full_data.groupby('building_id')['manager_id'].apply(list)  \nmanager_model = FastText(size=10, window=3, min_count=1, workers=16)\nmanager_model.build_vocab(sentences=manager_by_building)\nmanager_model.train(sentences=manager_by_building.values, \n                    total_examples=len(manager_by_building.values), epochs=5)\nmanager_emb = full_data['manager_id'].apply(lambda x:manager_model[x]).values\nmanager_emb = np.array([e.reshape(1,-1) for e in manager_emb]).reshape(-1,10)\nmanager_emb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars  \nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr,\n                        building_aggr,\n                        park_dist_data,\n                        subway_dist_data,\n                        manager_emb,\n                        building_emb]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr,\n                       building_aggr,\n                       park_dist_data,\n                       subway_dist_data,\n                       manager_emb,\n                    building_emb]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\n\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The magic feature\n\nFirstly mentioned by Grand Master Silogram\nhttps://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31765\n\nDiscovered and made available to public by another Grand Master KazAnova\nhttps://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870\n\nIt may contain the information when the listing was actually created.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_date = pd.read_csv(\"../input/twosigma-magic-feature/listing_image_time.csv\")\n\nimage_date.columns = [\"listing_id\", \"image_time_stamp\"]\nfull_data = pd.merge(full_data, image_date, on=\"listing_id\", how=\"left\")\nmagic_vars = ['image_time_stamp']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars + magic_vars + price_vars\nfull_cat_vars = LE_vars + mean_coded_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr,\n                        building_aggr,\n                        park_dist_data,\n                        subway_dist_data,\n                        manager_emb,\n                        building_emb]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr,\n                       building_aggr,\n                       park_dist_data,\n                       subway_dist_data,\n                       manager_emb,\n                    building_emb]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\n\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"%%time\nlgb_params = dict()\nlgb_params['objective'] = 'multiclass'\nlgb_params['num_class'] = 3\nlgb_params['learning_rate'] = 0.05\nlgb_params['num_leaves'] = 63\nlgb_params['max_depth'] = 15\nlgb_params['min_gain_to_split '] = 1\nlgb_params['subsample'] = 0.7\nlgb_params['colsample_bytree'] = 0.7\nlgb_params['min_sum_hessian_in_leaf'] = 0.001\nlgb_params['seed']=42\n\nlgb_cv = lgb.cv(lgb_params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=True,\n                shuffle=True,\n                early_stopping_rounds=50,\n                seed=42,\n                verbose_eval=50)\n\n\nbest_score = min(lgb_cv['multi_logloss-mean'])\nbest_iteration = len(lgb_cv['multi_logloss-mean'])\nprint ('Best iteration: %d, best score: %f' % (best_iteration, best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text embedding (optional)\n\nThe pretrained FastText embedding can be downloaded and installed using the following commands:\n\n```shell\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n!python -m spacy init-model en ../embedding/crawl-300d-2M --vectors-loc ../embedding/crawl-300d-2M.vec.zip\n```\n\nThen in Python:\n\n```Python\nimport spacy  \nnlp_fasttext = spacy.load(\"../embedding/crawl-300d-2M\")\n# nlp_glove = spacy.load(\"en_core_web_lg\")\ndef seq_to_vec(seq, nlp, dim=300):\n    doc = nlp(str(seq))\n    vec = np.array(\n        [\n            token.vector\n            for token in doc\n            if not ( token.is_space | token.is_oov)\n        ]\n    ).mean(axis=0)\n    if isinstance(vec, np.ndarray):\n        return vec\n    else:\n        return np.zeros((dim))\ndesc_emb = np.array([v for v in full_data[\"description\"].fillna('').apply(lambda x:seq_to_vec(x, nlp_fasttext)).values])\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model tuning\n## LightGBM\n### Manual tuning\nWe will manually tune LightGBM paramters one at a time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\ngreater_is_better = False\n\nlgb_metric = 'multi_logloss'\ndefault_lgb_params = {}\ndefault_lgb_params[\"objective\"] = \"multiclass\"\ndefault_lgb_params[\"num_class\"] = 3\ndefault_lgb_params[\"learning_rate\"] = 0.05\ndefault_lgb_params[\"metric\"] = lgb_metric\ndefault_lgb_params[\"bagging_freq\"] = 1\ndefault_lgb_params[\"seed\"] = 1234\n\nparams_lgb_space = {}\nparams_lgb_space['feature_fraction'] = [0.1, 0.3, 0.5, 0.7, 0.9]\nparams_lgb_space['num_leaves'] = [3, 7, 15, 31, 63, 127]\nparams_lgb_space['max_depth'] = [3, 7, 10, 15, 31, -1]\nparams_lgb_space['min_gain_to_split'] = [0, 0.1, 0.3, 1, 1.5, 2, 3]\nparams_lgb_space['bagging_fraction'] = [0.2, 0.4, 0.6, 0.8, 1]\nparams_lgb_space['min_sum_hessian_in_leaf'] = [0, 0.0001, 0.001, 0.1, 1, 3, 10]\nparams_lgb_space['lambda_l2'] = [0, 0.01, 0.1, 1, 10, 100]\nparams_lgb_space['lambda_l1'] = [0, 0.01, 0.1, 1, 10]\n\n\nbest_lgb_params = copy.copy(default_lgb_params)\n\nfor p in params_lgb_space:\n    print (\"\\n Tuning parameter %s in %s\" % (p, params_lgb_space[p]))\n\n    params = best_lgb_params\n    scores = []    \n    for v in params_lgb_space[p]:\n        print ('\\n    %s: %s' % (p, v), end=\"\\n\")\n        params[p] = v\n        lgb_cv = lgb.cv(params,\n                lgb.Dataset(train_x,\n                            label=train_y\n                            ),\n                num_boost_round=100000,\n                nfold=5,\n                stratified=False,\n                early_stopping_rounds=50,\n                verbose_eval=False)\n        if greater_is_better:\n            best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n        else:\n            best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n        best_lgb_iteration = len(lgb_cv['%s-mean' % (lgb_metric)])\n        print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))\n        scores.append([v, best_lgb_score])\n    # best param value in the space\n    best_param_value = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][0]\n    best_param_score = sorted(scores, key=lambda x:x[1],reverse=greater_is_better)[0][1]\n    best_lgb_params[p] = best_param_value\n    print (\"Best %s is %s with a score of %f\" %(p, best_param_value, best_param_score))\n\nprint ('\\n Best manually tuned parameters:', best_lgb_params)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('\\n Best manually tuned parameters:', best_lgb_params)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Automated tuning with Bayesian Optimization\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\ndef lgb_evaluate(\n                 num_leaves,\n                 max_depth,\n                 min_sum_hessian_in_leaf,\n                 min_gain_to_split,\n                 feature_fraction,\n                 bagging_fraction,\n                 lambda_l2,\n                 lambda_l1\n                 ):\n    params = dict()\n    params['objective'] = 'multiclass'\n    params['num_class'] = 3\n    params['learning_rate'] = 0.05\n    params['seed'] = 1234\n    params['num_leaves'] = int(num_leaves)  \n    params['max_depth'] = int(max_depth) \n    params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n    params['min_gain_to_split'] = min_gain_to_split    \n    params['feature_fraction'] = feature_fraction\n    params['bagging_fraction'] = bagging_fraction\n    params['bagging_freq'] = 1\n    params['lambda_l2'] = lambda_l2\n    params['lambda_l1'] = lambda_l1   \n    params[\"metric\"] = lgb_metric\n\n    lgb_cv = lgb.cv(params,\n            lgb.Dataset(train_x,\n                        label=train_y\n                        ),\n            num_boost_round=100000,\n            nfold=5,\n            stratified=False,\n            early_stopping_rounds=50,\n            verbose_eval=-1)\n    if greater_is_better:\n        best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n    else:\n        best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\n    best_lgb_iteration = len(lgb_cv['%s-mean' % (lgb_metric)])\n    print (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))\n\n    return -best_lgb_score\n\n\nlgb_BO = BayesianOptimization(lgb_evaluate, \n                             {\n                              'num_leaves': (10, 20),\n                              'max_depth': (2, 20),\n                              'min_sum_hessian_in_leaf': (5, 15),\n                              'min_gain_to_split': (0,0),\n                              'feature_fraction': (0.2, 0.4),\n                              'bagging_fraction': (0.8,1),\n                              'lambda_l2': (5, 15),\n                              'lambda_l1': (0.1, 5)\n                             }\n                            )\n## I use 5, 20 to save time but you may want to change it to larger numbers,e.g. 8, 30 \nlgb_BO.maximize(init_points=5, n_iter=20) \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show top 5 best tuned parameters. Comparing to the manually tuned results, which one works better?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_BO_scores = pd.DataFrame([p['params'] for p in lgb_BO.res])\nlgb_BO_scores['score'] = [p['target'] for p in lgb_BO.res]\nlgb_BO_scores = lgb_BO_scores.sort_values(by='score',ascending=False)\nlgb_BO_scores.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Retrain the model with smaller learning rate\nNow let's validate the model again but with a smaller learning rate(0.01 as compared to 0.05) and see the changes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_best_params = lgb_BO_scores.T.to_dict().get(lgb_BO_scores.index.values[0])\nlgb_best_params['objective'] = 'multiclass'\nlgb_best_params['learning_rate'] = 0.01 ## from 0.05 to 0.01\nlgb_best_params['num_class'] = 3\nlgb_best_params['seed'] = 1234\nlgb_best_params['metric'] = lgb_metric\nlgb_best_params['bagging_freq'] = 1\n\nlgb_best_params['num_leaves'] = int(lgb_best_params['num_leaves'])\nlgb_best_params['max_depth'] = int(lgb_best_params['max_depth'])\n\nprint(lgb_best_params)\n\nlgb_cv = lgb.cv(lgb_best_params,\n        lgb.Dataset(train_x,\n                    label=train_y\n                    ),\n        num_boost_round=100000,\n        nfold=5,\n        stratified=True,\n        early_stopping_rounds=50,\n        verbose_eval=100)\nif greater_is_better:\n    best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\nelse:\n    best_lgb_score = min(lgb_cv['%s-mean' % (lgb_metric)])\nbest_lgb_iteration = len(lgb_cv['%s-mean' % (lgb_metric)])\nprint (', best_score: %f, best_iteration: %d' % (best_lgb_score, best_lgb_iteration))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Apparently the model performs better with a smaller learning rate however it also took more iterations (longer time) to converge.\n\nNow let's retrain the model with the learning rate 0.01 as well as the tuned iterations(num_boost_round) and generate the submission.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb.train(lgb_best_params,\n                  lgb.Dataset(train_x,\n                              label=train_y\n                              ),\n                  num_boost_round=best_lgb_iteration)\npreds = model.predict(test_x)\nsub_lgb_df = pd.DataFrame(preds, columns=[\"low\", \"medium\", \"high\"])\nsub_lgb_df[\"listing_id\"] = test_data.listing_id.values\nsub_lgb_df.to_csv(\"../output/sub_lgb_auto_tuned.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}