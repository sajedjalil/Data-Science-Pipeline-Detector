{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"035c16ea-12b9-3a34-60ce-73a3f6962298"},"source":"It seems the current [high scoring script][1] is written in R using H2O. So let us do one in python using XGBoost. \n\nThanks to [this script][2] for feature engineering ideas. \n\nWe shall start with importing the necessary modules\n\n\n  [1]: https://www.kaggle.com/gospursgo/two-sigma-connect-rental-listing-inquiries/h2o-starter-pack/run/835757\n  [2]: https://www.kaggle.com/aikinogard/two-sigma-connect-rental-listing-inquiries/random-forest-starter-with-numerical-features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b30297fd-d7dc-601d-1c3f-5ceb50bf0a8d"},"outputs":[],"source":"import sys\nimport operator\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing, ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport matplotlib.pyplot as plt\n\nfrom xgboost.sklearn import XGBClassifier"},{"cell_type":"markdown","metadata":{"_cell_guid":"29fa9103-bd3b-ad6c-0afd-c2bda6456f80"},"source":"Now let us write a custom function to run the xgboost model."},{"cell_type":"markdown","metadata":{"_cell_guid":"251ffb3b-e0d0-8409-5fc9-260c08ff804e"},"source":"Let us read the train and test files and store it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8dea5f1-e6c1-54d8-15fe-6711825b7e50"},"outputs":[],"source":"data_path = \"../input/\"\ntrain_file = data_path + \"train.json\"\ntest_file = data_path + \"test.json\"\ntrain_df = pd.read_json(train_file)\ntest_df = pd.read_json(test_file)\nprint(train_df.shape)\nprint(test_df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0de6a1e-328d-6177-1e08-60321c259fc9"},"outputs":[],"source":"train_df.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d72ef73f-09bc-d9ef-e4ac-a5bab39a5058"},"outputs":[],"source":"train_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0165afcd-ce08-fd0c-7ac6-3b1f1cd99c77"},"outputs":[],"source":"train_df.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f943207-f45b-84b8-35b0-85f908192165"},"source":"We do not need any pre-processing for numerical features and so create a list with those features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5b5da1a-5066-b665-bf44-03c20558fcf9"},"outputs":[],"source":"features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e18805f-cb26-7181-7c52-1a50e6cca5fe"},"source":"Now let us create some new features from the given features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc2ea8f0-074a-7233-4a26-80dc6870b973"},"outputs":[],"source":"# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# convert the created column to datetime object so as to extract more features \ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n\n# Let us extract some features like year, month, day, hour from date columns #\ntrain_df[\"created_year\"] = train_df[\"created\"].dt.year\ntest_df[\"created_year\"] = test_df[\"created\"].dt.year\ntrain_df[\"created_month\"] = train_df[\"created\"].dt.month\ntest_df[\"created_month\"] = test_df[\"created\"].dt.month\ntrain_df[\"created_day\"] = train_df[\"created\"].dt.day\ntest_df[\"created_day\"] = test_df[\"created\"].dt.day\ntrain_df[\"created_hour\"] = train_df[\"created\"].dt.hour\ntest_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n\n# adding all these new features to use list #\nfeatures_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"},{"cell_type":"markdown","metadata":{"_cell_guid":"bfcfd8e6-484e-5137-d562-b35a4e756aef"},"source":"We have 4 categorical features in our data\n\n - display_address\n - manager_id\n - building_id\n - listing_id\n\nSo let us label encode these features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c15ea019-8eac-ae3f-2d23-8a68b2169b05"},"outputs":[],"source":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc36f663-6122-8727-c466-ed50a04554c9"},"source":"We have features column which is a list of string values. So we can first combine all the strings together to get a single string and then apply count vectorizer on top of it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7e55f87-1de7-0ab2-7b01-6b1d772849b7"},"outputs":[],"source":"train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\nprint(train_df[\"features\"].head())\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train_df[\"features\"])\nte_sparse = tfidf.transform(test_df[\"features\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0a57223-cfc6-ad42-0447-cb33cc13febb"},"outputs":[],"source":"train_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1fc1374a-8c1d-2460-5e00-8513c8a13993"},"source":"Now let us stack both the dense and sparse features into a single dataset and also get the target variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c313f80c-880b-fa4a-907f-637e26acdf3a"},"outputs":[],"source":"train_df[features_to_use].head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15a9e31e-1caf-2860-4f7a-85999bb2a8e6"},"outputs":[],"source":"train_X = train_df[features_to_use]\ntest_X = test_df[features_to_use]\n\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ntrain_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\nprint(train_X.shape, test_X.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbcb6917-d9b6-a71a-de02-73acb4c9eb30"},"outputs":[],"source":"train_X"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e2b5c47-2b14-f56f-a4ae-b85294d8d281"},"outputs":[],"source":"train_y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01143cf0-0529-b836-2fbc-e3c1f46127c7"},"outputs":[],"source":"train_df['interest_level']"},{"cell_type":"markdown","metadata":{"_cell_guid":"bad1197b-dc73-ea6e-9ed4-e944ed300db3"},"source":"Now let us do some cross validation to check the scores. \n\nPlease run it in local to get the cv scores. I am commenting it out here for time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"989e560b-b273-fb03-5c9d-ca0d0782d393"},"outputs":[],"source":"features_to_use"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a599c136-1925-59ac-1a66-5a32e299c007"},"outputs":[],"source":"def modelfit(alg, X,y, useTrainCV=True,\n             cv_fold=5,early_stopping_rounds = 20):\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgb_param['num_class']=3\n        xgtrain = xgb.DMatrix(X,label = y)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=\n                          alg.get_params()['n_estimators'],nfold=cv_fold,\n                          metrics='mlogloss',early_stopping_rounds = early_stopping_rounds)\n        print(cvresult)\n        alg.set_params(n_estimators=cvresult.shape[0])\n        \n    #Fit the algorithm on the data\n    alg.fit(X, y,eval_metric='mlogloss')\n        \n    #Predict training set:\n    y_pred = alg.predict(X)\n    y_predprob = alg.predict_proba(X)\n    print(y_predprob)\n        \n    #Pring model report\n    print (\"\\nModel Report\")\n    #print (\"Accuracy : %.4g\" % accuracy_score(y, \n                                            #y_pred))\n    print (\"logloss score (Train) : %f\" % log_loss(y,  y_predprob))\n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='hbar',title='Feature Importances')\n    plt.ylabel('Feature Importance Score')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7399489a-9d03-b90e-fbe4-f447386aa500"},"outputs":[],"source":"xgb1 = XGBClassifier(\n    learning_rate = 0.1,\n    n_estimators=100,\n    max_depth=5,\n    min_child_weight=1,\n    gamma = 0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective='multi:softprob',\n    nthread =4,\n    scale_pos_weight=1,\n    seed = 0,\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a5a0743-c82a-5f36-6ec8-247ea926c505"},"outputs":[],"source":"modelfit(xgb1, train_X, train_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ae65091-6f1e-1c9c-140a-afb6952d6aa2"},"outputs":[],"source":"xgb1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f02f2e1-f157-7fe4-2bb3-5a00ff1a07b7"},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28aeed48-de25-28da-23ac-826932d4f528"},"outputs":[],"source":"param_test1={\n    'max_depth':range(1,10,2),\n    'min_child_weight':range(1,10,3)\n}\n\ngs1 = GridSearchCV(xgb1,param_grid=param_test1, \n                   scoring='neg_log_loss', n_jobs=-1,iid=False, cv=5)\ngs1.fit(train_X,train_y)\ngs1.grid_scores_, gs1.best_params_,gs1.best_score_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19f667c9-a9a3-f92b-c12a-01dc45a5997b"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afcfb932-2081-21a3-f2af-1296f34236d4","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55f89a90-4ac1-ea20-c07a-c289ffae9304","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77685e7f-2c3f-78b1-7794-44a5f0ff6991","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8165935e-b992-c01d-e903-1d4ca110304e","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5dbdeb44-bf07-8fcb-2534-086f6ec3b82a","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59555bb8-b27e-9c33-fc9c-3e91ce44f5f1","collapsed":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"c29df0a9-ea45-342e-e126-90b7adb8cec3"},"source":"Now let us build the final model and get the predictions on the test set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e599ea34-9295-776b-32de-7b4e46fa13cc"},"outputs":[],"source":"preds, model = runXGB(train_X, train_y, test_X, num_rounds=400)\nout_df = pd.DataFrame(preds)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = test_df.listing_id.values\n#out_df.to_csv(\"xgb2.csv\", index=False)\nout_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"dba1497e-5a5e-f93c-ed98-e122f7df6db7"},"source":"\nHope this helps the python users as a good starting point."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64a38118-3e95-0737-e80b-a6a176b36e80"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}