{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1a740a67-5bc9-4391-5f42-70f4efe40380"},"source":"# You know I am strictly Bayesian Right?\n\nThis is for finding if a Bayesian Framework is going to be better at predicting results compared to a normal model. The results uptill now show significant gains (LB score 11.98 -> 0.74) by employing this framework. I shall be working on implementing multiple models and also on a version that uses `predict_proba` rather than `predict`. I expect to see significant gains wheh I switch to `predict_proba`\n\n## 1. Hypothesis Testing\n\nWe are tasked to generate the probability that a house has `low`, `medium` or `high` interest, given the feature set provided. We need to train a model, and figure out the probabilities:\n\n$$P[low]=?, P[medium]=?, P[high]=?$$\n\nWhat we want to do is to find the best method of coming up with these probabilities. So, how do we do that? What we want to do is to see of there is a method of coming up with a solution that is an improvement over a basic machine learning (ML) estimation. We want to come up with a framework, that will allow us to put ML results as part of a Bayesian inference problem. \n\n### 1.1. Some Definitions\n\nLet us start with the definitions of some terms which will be used for the rest of this report. First, lets write down the preliminaries:\n\n| value | definition\n|-------|---\n| $H_i$ | The hypothesis that we are testing. $i$ is the subscript that is one of $low$, $medium$, $high$. Hence, it will be useful to look at values such as $H_{high}$, $H_{medium}$, and $H_{low}$. $H_{low}$ for example is the *hypothesis* that the house under scrutiny has low interest. \n| $P(H_i)$ | This is the prior probability that the current house has interest level $i$. Hence, $P(H_{low})$ is the probability that the current house has low interest. \n\n### 1.2. Now for the Math\n\nNow, we are also given some features, for each house. Let us call them $\\mathbf{f}$. Let us say that there are $M$ features, which can be written as:\n\n$$\\mathbf{f} = [f_0, f_1, \\ldots, f_{M-1}]$$\n\nTypically, the normal procedure that Bayesian people follow is something like the following: \n\n$$P(H_i|\\mathbf{f}) = P(H_i) \\frac {P(\\mathbf{f}|H_i) } {P(\\mathbf{f})} $$\n\nAs the saying goes, we are *updating* the prior knowledge with new information. Note that $P(H_i|\\mathbf{f})$ is what we are seeking to find in this competition. However, I want to propose a modified version of this method to see if we can gain from ML techniques. \n\n### 1.3. Throwing ML in the Mix\n\nLet us say that we train a model $m$ (for example, a Random Forest model) which makes a prediction of $P(H_i)$, using the features $\\mathbf{f}$. This can be written as:\n\n$$ m(\\mathbf{f}) \\rightarrow \\hat{P}(H_i)$$\n\nWe shall use the symbol $m$ in place of $ m(\\mathbf{f})$ and $\\hat{P}(H_i)$, because they *all* mean the same thing. $\\hat{P}(H_i)$ is one *estimation* of $P(H_i)$. Let us see this is the Bayesian perspective... \n\n$$P(H_i| m) = P(H_i) \\frac {P(m|H_i) } {P(m)} = P(H_i) \\frac {P(m|H_i) } { P(m|H_i) P(H_i) + P(m|\\bar{H_i}) P( \\bar{H_i}) } $$\n\nNote that $\\bar{H_i}$ is the *inverse* of $H_i$. So, $\\bar{H_{low}}$ is the condition that the current house has either a high or medium interest. \n\n### 1.4. Why Stop at One Model?\n\nIt is entirely possible that we train not one model, but two, or three, or even a hundred. People frequently do, and either use some form of voting classifier, or some form if averaging for obtaining the final result. However, why not improve the Bayesian paradigm itself? Let us say that we have a bunch of models $\\mathbf{m} = [m_0, m_1, \\ldots, m_{N-1}]$. Then, each of these models can give us a bunch of predictions on the testing data $\\mathbf{\\hat{P}}(H_i)$. The above equation changes to:\n\n$$P(H_i| \\mathbf{m}) = P(H_i) \\frac {P(\\mathbf{m}|H_i)} {P(\\mathbf{m})}$$\n\nThis is not a trivial solution. However, it is not impossible to solve. We can, for the sake of simplicity, use the naive assumption of model independence. In that case, we can try the following:\n\n$$P(H_i| \\mathbf{m}) = P(H_i) \\frac { \\prod_{j=0}^{N-1} P(m_j|H_i)} { \\prod_{j=0}^{N-1} P(m_j) }$$.\n\nOk, so we have some form of a mathematical construct. \n\n### 1.5. Summary\n\nBayesian inference lends itself naturally in this current setting. The rest of the document will look at some possible ways of approximating the different values in the equations above. If anyone has some good methods to improve the methods already described, do share, so that all of us can benefit :).\n\n\n## 2. The Preliminaries\n\nFirst, let us consider some of the preliminaries. We shall consider what we are doing using what we already know. \n\nThe data from the competition has already been split into the training and the test sets, and the output value and put into its own folder `localFiles\\`. There are three files within this folder:\n\n - `Xtest.csv`  \n - `Xtrain.csv`  \n - `y.csv`\n\n### 2.1. One-Hot-Encoding prediction\n\nThis is relatively simple, and this we shall not delve into this too much. It has already been done and saved in the file `localFiles\\y.csv`.\n\n```python\n>>> import pandas as pd\n>>> ys = pd.read_csv('localFiles\\y.csv')\n>>> ys.head()\n   high  medium  low\n0   0.0     1.0  0.0\n1   0.0     0.0  1.0\n2   1.0     0.0  0.0\n3   0.0     0.0  1.0\n4   0.0     0.0  1.0\n```\n\n## 3. Equations to Programs ...\n\nLet us find out how we can find the different values in the equations above. What do we need to find?\n\n 1. $P(H_i)$, $P(\\bar{H_i})$,\n 2. $P(m|H_i)$, $P(m|\\bar{H_i})$, and $P(m)$\n\nThats it. Only five values. Although, from section 1.3 it would appear that we woule be able to calcualate $P(m)$ from the other values, as we shall see later, the computation of $P(m)$ is relatively straight-forward. So we shall do that exclusively. Under such a circumstance, we don't need to calculate $P(m|\\bar{H_i})$ explicitely. \n\n### 3.1. Finding $P(H_i)$, and $P(\\bar{H_i})$\n\nWithout any information about the houses, we can simply say that there is an equal probability of having a certain amount of interest in a particular house. That will make $P(H_i) = 1$. However, we do know from the training data that there is a significant amount of skew in the data.  $P(H_i)$ \n\n```python\n>>> for c in ys.columns:\n...    print c, sum(ys[c] == 1)*1.0 / len(ys[c]), sum(ys[c] == 0)*1.0 / len(ys[c])\nhigh 0.0777881342195 0.922211865781\nmedium 0.227528772897 0.772471227103\nlow 0.694683092884 0.305316907116\n```\n\nSo, we get out first set of priors:\n\n| $H_i$        | $P(H_i)$        | $P(\\bar{H_i})$ \n|--------------|-----------------|------------------\n| $H_{high}$   | 0.0777881342195 | 0.922211865781\n| $H_{medium}$ | 0.227528772897  | 0.772471227103\n| $H_{low}$    | 0.694683092884  | 0.305316907116\n\nThat was easy! \n\n### 3.2. Finding $P(m|H_i)$, $P(m|\\bar{H_i})$, and $P(m)$\n\nLet us think about what this is. We shall look at $P(m|H_i)$ first. It will soon be clear that $P(m|\\bar{H_i})$ follows trivially from this same example. I'll be honest here. This is not trivial, and at some points, you do need to make leaps of faith.  When you do, I will clearly enunciate them, so you can make your own decisions of how to deal with them. Let's first break this down real quick so it will be easier to follow:\n\n#### 3.2.1. First what is $m$ and how do we obtain it?\n\nWell, as we have seen previously, the definition for $m$ is:\n\n$$ m(\\mathbf{f}) \\rightarrow \\hat{P}(H_i)$$\n\nNow note that we have used $m$ in two different ways. It represents the \n\n 1. *model* $m$ as well as the \n 2. *prediction* that the model gives, given the featureset $\\mathbf{f}$ (i.e. $m(\\mathbf{f})$).\n\nFor convenience, these are used interchangeably, since the use of $m$ is so much easier than its actual counterpart. Which definition of $m$ we are using will generally be clear form the context. In this and the subsequent sections, $m$ will refer to the *prediction*. \n\nSo how do we get a model? Lets take a Random Forest model for example, and train it ...\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nXtest  = pd.read_csv('localFeatures/Xtest.csv')\nXtrain = pd.read_csv('localFeatures/Xtrain.csv')\ny      = pd.read_csv('localFeatures/y.csv')\n\nmodel = RandomForestClassifier()\nmodel.fit(Xtest, y['low'])\n```\n\n> Question: Why U No Use PIP8? (cringe!!!)\n>  \n>  Ans: I find it easier to read my code when it is not linted. \n\n**Note:** *In this case, we are only predicting the values for the case where the interest is low. We need to redo this calculation for each interest level.*\n\nHere `model` represents $m$, our model (not a prediction). The *prediction* of the `i`<sup>th</sup> house in the training dataset $m(\\mathbf{f})$ would then be represented by either `model.predict(Xtrain.ix[ i , :])` or `model.predict_proba(Xtrain.ix[ i , :])[:, 1]`. This is either a `1` or `0` for the discrete case, or a floating point number of the continuous case. We shall look at the discrete one first as this one is easier to wrap out heads around. \n\n#### 3.2.2. The discrete case for $P(m|H_i)$\n\nNow let us suppose that, given a set of features $\\mathbf{f}$, out model *predicts*  a value of `1`. We know (from Section 3.1.) that before the prediction, the probability that the house has low interest is approximately 0.69. Given that our model predicts that this also have low interest, how do we *update* this probability?\n\nFor this, we need two quantities. $P(m|H_i)$. In this specific case, we want to find\n\n$$ P( \\hat{P}(H_i) = 1 | H_{low} = 1 ) $$\n\nFrom the training set, we can easily calculate this quantity ...\n\n```python\nyHat = model.predict(Xtrain.ix[y['low'] == 1, :]) \nP_m_given_H = sum(yHat == 1)*1.0/len(yHat)\n```\n\nIf the current model predicts a `0` on the other hand, the equation that we are interested in evaluating is the following:\n\n$$    P( H_i = 1| \\hat{P}(H_i) = 0 )  =  P(H_i = 1) \\frac {P( \\hat{P}(H_i) = 0 |H_i = 1) } {P( \\hat{P}(H_i) = 0 )} $$\n\nUnder these circumstances, the quantity that we would be interested in is: ${P( \\hat{P}(H_i) = 0 |H_i = 1) }$, the code for which can be described written as:\n\n```python\nyHat = model.predict(Xtrain.ix[y['low'] == 1, :])\nP_m_given_H = sum(yHat == 0)*1.0/len(yHat)\n```\n\nThese two sets of code is essentially the same. The one that we are going to choose is the one that depends upon the prediction made by the current model for the current house. \n\n```python\npred = 1 # This is the current prediction\n\nyHat = model.predict(Xtrain.ix[y['low'] == 1, :])\nP_m_given_H = sum(yHat == pred)*1.0/len(yHat)\n```\n\n\n#### 3.2.3. The discrete case for $P(m)$\n\nThis is much simpler of course. We are looking for the quantities:\n\n - $ P( \\hat{P}(H_i) = 1 ) $, and \n - $ P( \\hat{P}(H_i) = 0 ) $\n\nThis is simply given by the following lines of code:\n\n```python\npred = 1 # This is the current prediction\n\nyHatAll = model.predict(Xtrain) # Predict for all values.\nP_m = sum(yHatAll == pred)*1.0/len(yHatAll)\n```\n\n#### 3.3.4. Summary\n\nFor *every* model that we train, we want to find the values of the following quantities:\n\n - $ P( \\hat{P}(H_i) = 1 | H_{low} = 1 ) $,\n - $ P( \\hat{P}(H_i) = 0 | H_{low} = 1 ) $\n - $ P( \\hat{P}(H_i) = 1 ) $, and\n - $ P( \\hat{P}(H_i) = 0 ) $\n\nNote that these quantities are *independent* of the testing set, and thus can be easily calculated immediately following the training of the data.\n\n## 4. A Simple Test Script\n\nLet us first test to see if a simple test script is going to show improvement in scores using a Bayesing inference methodology.\n\n```python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random, json\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\n\nif __name__ == '__main__':\n\n    print('Reading data ...')\n    Xtest  = pd.read_csv('localFeatures/Xtest.csv')\n    Xtrain = pd.read_csv('localFeatures/Xtrain.csv')\n    y      = pd.read_csv('localFeatures/y.csv')\n    test   = pd.read_json(\"../../data/test.json\" )\n\n    predictClasses = ['high', 'medium', 'low']\n\n    Xtest.fillna( Xtest.mean(), inplace=True)\n\n    print('Fitting a different model for each class ...')\n\n\n    params = {\n        'min_samples_split' : random.randint(4, 10),\n        'max_depth'         : random.randint(15, 50),\n        'n_estimators'      : random.randint(100, 500),\n        'n_jobs'            :  -1,\n    }\n\n    resultNormal   = {}\n    resultBayesian = {}\n\n    for p in predictClasses:\n\n        print 'Training model: ', p\n\n        # Train a random model ...\n        model = RandomForestClassifier(**params)\n        model.fit(Xtrain, y[p])\n\n        # P(m|H)\n        P_m_given_H = {}\n        yHat = model.predict(Xtrain.ix[y[p] == 1, :]) \n        P_m_given_H[0] = sum(yHat == 0)*1.0/len(yHat)\n        P_m_given_H[1] = sum(yHat == 1)*1.0/len(yHat)\n\n        # P(m)\n        P_m = {}\n        yHatAll = model.predict(Xtrain) # Predict for all values.\n        P_m[0] = sum(yHatAll == 0)*1.0/len(yHatAll)\n        P_m[1] = sum(yHatAll == 1)*1.0/len(yHatAll)\n\n        # This is the factor by which a result is going\n        # to be changed, given that a 1 or a 0 is selected\n        Factor = {}\n        Factor[0] = P_m_given_H[0] / P_m[0]\n        Factor[1] = P_m_given_H[1] / P_m[1]\n\n        print 'Factor  :', Factor\n        print 'P(m)    :', P_m\n        print 'P(m|H)  :', P_m_given_H\n\n        Hi = sum(y[p] == 1)*1.0 / len(y[p])\n\n        yHat = model.predict(Xtest)\n\n        resultNormal[p] = yHat\n        resultBayesian[p] = Hi * np.array([ Factor[m] for m in yHat])\n\n    resultBayesian['listing_id'] = test['listing_id']\n    resultBayesian = pd.DataFrame(resultBayesian)[['listing_id', 'high', 'medium', 'low']]\n    \n    resultNormal['listing_id'] = test['listing_id']\n    resultNormal = pd.DataFrame(resultNormal)[['listing_id', 'high', 'medium', 'low']]\n\n    print resultBayesian.head()\n    print resultNormal.head()\n\n    resultBayesian.to_csv('results/bayesian1.csv', index=False)\n    resultNormal.to_csv('results/normal.csv', index=False)\n\n\n\n```\n\n| Method    |  LB Score\n|-----------|---------\n| Normal    | 11.98269\n| Bayesian  | 0.74782\n\n## 5. Multiple Models\n\n[To do] I shall complete this section in a couple of days ...\n\n## 6. Conclusion\n\nWe have seen in this article how we can incorporate machine learning models into a Bayesian Framework. The good news about this framework is that this is going to work well no matter what type of model you use, how different they are. In fact, because of the assumption of independence, the more different your models are, the better off you will be. The other interesting thing about this approach is that this method os going to change your prior *in proportion* to how good they are. So if a particular model is very accurate, then this method should theoretically enhance that. \n\n#### More to come \n\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}