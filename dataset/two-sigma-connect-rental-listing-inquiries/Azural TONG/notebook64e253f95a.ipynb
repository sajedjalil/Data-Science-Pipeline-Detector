{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"313a02e2-0a3f-3c41-7ee7-d8d12b1adda5"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nfrom math import exp\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e574f96c-d4d4-fb7e-377b-c021138002f6"},"outputs":[],"source":"random.seed(321)\nnp.random.seed(321)\n\nX_train = pd.read_json(\"../input/train.json\")\nX_test = pd.read_json(\"../input/test.json\")\n\ninterest_level_map = {'low': 0, 'medium': 1, 'high': 2}\nX_train['interest_level'] = X_train['interest_level'].apply(lambda x: interest_level_map[x])\nX_test['interest_level'] = -1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4ac622b-6748-0a68-9163-65a8f17ec91e"},"outputs":[],"source":"#add features\nfeature_transform = CountVectorizer(stop_words='english', max_features=150)\nX_train['features'] = X_train[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nX_test['features'] = X_test[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.lower().split(\" \")) for i in x]))\nfeature_transform.fit(list(X_train['features']) + list(X_test['features']))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6187c7af-578b-3588-7eeb-1184782d733e"},"outputs":[],"source":"train_size = len(X_train)\nlow_count = len(X_train[X_train['interest_level'] == 0])\nmedium_count = len(X_train[X_train['interest_level'] == 1])\nhigh_count = len(X_train[X_train['interest_level'] == 2])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6298ce7-ed3f-675e-81d0-06b5bc249a56"},"outputs":[],"source":"#find some features that only appears once (Why?)\ndef find_objects_with_only_one_record(feature_name):\n    temp = pd.concat([X_train[feature_name].reset_index(), \n                      X_test[feature_name].reset_index()])\n    temp = temp.groupby(feature_name, as_index = False).count()\n    return temp[temp['index'] == 1]\n\nmanagers_with_one_lot = find_objects_with_only_one_record('manager_id')\nbuildings_with_one_lot = find_objects_with_only_one_record('building_id')\naddresses_with_one_lot = find_objects_with_only_one_record('display_address')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7f7a741-998d-c9b8-ce81-e3a8826104ae"},"outputs":[],"source":"lambda_val = None\nk=5.0\nf=1.0\nr_k=0.01 \ng = 1.0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d044700b-a595-3b6b-0f08-779407d9252c"},"outputs":[],"source":"def categorical_average(variable, y, pred_0, feature_name):\n    def calculate_average(sub1, sub2):\n        s = pd.DataFrame(data = {\n                                 variable: sub1.groupby(variable, as_index = False).count()[variable],                              \n                                 'sumy': sub1.groupby(variable, as_index = False).sum()['y'],\n                                 'avgY': sub1.groupby(variable, as_index = False).mean()['y'],\n                                 'cnt': sub1.groupby(variable, as_index = False).count()['y']\n                                 })\n                                 \n        tmp = sub2.merge(s.reset_index(), how='left', left_on=variable, right_on=variable) \n        del tmp['index']                       \n        tmp.loc[pd.isnull(tmp['cnt']), 'cnt'] = 0.0\n        tmp.loc[pd.isnull(tmp['cnt']), 'sumy'] = 0.0\n\n        def compute_beta(row):\n            cnt = row['cnt'] if row['cnt'] < 200 else float('inf')\n            return 1.0 / (g + exp((cnt - k) / f))\n            \n        if lambda_val is not None:\n            tmp['beta'] = lambda_val\n        else:\n            tmp['beta'] = tmp.apply(compute_beta, axis = 1)\n            \n        tmp['adj_avg'] = tmp.apply(lambda row: (1.0 - row['beta']) * row['avgY'] + row['beta'] * row['pred_0'],\n                                   axis = 1)\n                                   \n        tmp.loc[pd.isnull(tmp['avgY']), 'avgY'] = tmp.loc[pd.isnull(tmp['avgY']), 'pred_0']\n        tmp.loc[pd.isnull(tmp['adj_avg']), 'adj_avg'] = tmp.loc[pd.isnull(tmp['adj_avg']), 'pred_0']\n        tmp['random'] = np.random.uniform(size = len(tmp))\n        tmp['adj_avg'] = tmp.apply(lambda row: row['adj_avg'] *(1 + (row['random'] - 0.5) * r_k),\n                                   axis = 1)\n    \n        return tmp['adj_avg'].ravel()\n     \n    #cv for training set \n    k_fold = StratifiedKFold(5)\n    X_train[feature_name] = -999 \n    for (train_index, cv_index) in k_fold.split(np.zeros(len(X_train)),\n                                                X_train['interest_level'].ravel()):\n        sub = pd.DataFrame(data = {variable: X_train[variable],\n                                   'y': X_train[y],\n                                   'pred_0': X_train[pred_0]})\n            \n        sub1 = sub.iloc[train_index]        \n        sub2 = sub.iloc[cv_index]\n        \n        X_train.loc[cv_index, feature_name] = calculate_average(sub1, sub2)\n    \n    #for test set\n    sub1 = pd.DataFrame(data = {variable: X_train[variable],\n                                'y': X_train[y],\n                                'pred_0': X_train[pred_0]})\n    sub2 = pd.DataFrame(data = {variable: X_test[variable],\n                                'y': X_test[y],\n                                'pred_0': X_test[pred_0]})\n    X_test.loc[:, feature_name] = calculate_average(sub1, sub2)   "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7883fef4-cf63-7547-c6b1-55d66431beba"},"outputs":[],"source":"def transform_data(X):\n    #add features    \n    feat_sparse = feature_transform.transform(X[\"features\"])\n    vocabulary = feature_transform.vocabulary_\n    del X['features']\n    X1 = pd.DataFrame([ pd.Series(feat_sparse[i].toarray().ravel()) for i in np.arange(feat_sparse.shape[0]) ])\n    X1.columns = list(sorted(vocabulary.keys()))\n    X = pd.concat([X.reset_index(), X1.reset_index()], axis = 1)\n    del X['index']\n    \n    X[\"num_photos\"] = X[\"photos\"].apply(len)\n    X['created'] = pd.to_datetime(X[\"created\"])\n    X[\"num_description_words\"] = X[\"description\"].apply(lambda x: len(x.split(\" \")))\n    X['price_per_bed'] = X['price'] / X['bedrooms']    \n    X['price_per_bath'] = X['price'] / X['bathrooms']\n    X['price_per_room'] = X['price'] / (X['bathrooms'] + X['bedrooms'] )\n    \n    X['low'] = 0\n    X.loc[X['interest_level'] == 0, 'low'] = 1\n    X['medium'] = 0\n    X.loc[X['interest_level'] == 1, 'medium'] = 1\n    X['high'] = 0\n    X.loc[X['interest_level'] == 2, 'high'] = 1\n    \n    X['display_address'] = X['display_address'].apply(lambda x: x.lower().strip())\n    X['street_address'] = X['street_address'].apply(lambda x: x.lower().strip())\n    \n    X['pred0_low'] = low_count * 1.0 / train_size\n    X['pred0_medium'] = medium_count * 1.0 / train_size\n    X['pred0_high'] = high_count * 1.0 / train_size\n\n    #interesting????!\n    X.loc[X['manager_id'].isin(managers_with_one_lot['manager_id'].ravel()), \n          'manager_id'] = \"-1\"\n    X.loc[X['building_id'].isin(buildings_with_one_lot['building_id'].ravel()), \n          'building_id'] = \"-1\"\n    X.loc[X['display_address'].isin(addresses_with_one_lot['display_address'].ravel()), \n          'display_address'] = \"-1\"\n          \n    return X\n\ndef normalize_high_cordiality_data():\n    high_cardinality = [\"building_id\"]\n    for c in high_cardinality:\n        categorical_average(c, \"medium\", \"pred0_medium\", c + \"_mean_medium\")\n        categorical_average(c, \"high\", \"pred0_high\", c + \"_mean_high\")\n\ndef transform_categorical_data():\n    categorical = ['building_id', 'manager_id', \n                   'display_address', 'street_address']\n                   \n    for f in categorical:\n        encoder = LabelEncoder()\n        encoder.fit(list(X_train[f]) + list(X_test[f])) \n        X_train[f] = encoder.transform(X_train[f].ravel())\n        X_test[f] = encoder.transform(X_test[f].ravel())\n                  \n\ndef remove_columns(X):\n    columns = [\"photos\", \"pred0_high\", \"pred0_low\", \"pred0_medium\",\n               \"description\", \"low\", \"medium\", \"high\",\n               \"interest_level\", \"created\"]\n    for c in columns:\n        del X[c]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"efd15f12-92d4-1101-afeb-80b382e59170"},"outputs":[],"source":"def clean_feature(X):\n    dup_dict = {'dishwasher': ['_dishwasher_','dishwasher','washer', 'washer_', 'washer_in_unit','unit_washer'],\n    'dryer':['_dryer','dryer','dryer_in_building','dryer_in_unit'],\n    'pets_friendly':['_pets_ok_','pet_friendly', 'pets_on_approval'],\n    'backyard':['backyard','courtyard','patio'],\n    'central_ac':['central_a','central_ac'],\n    'childrens_playroom':['childrens_playroom','children'],\n    'common_parking':['common_parking','parking','parking_space','site_parking', 'site_parking_lot']\n    'common_roof_deck': ['common_roof_deck', 'common_terrace']\n    'concierge':['concierge','doorman','ft_doorman','time_doorman']\n    'fireplace': ['decorative_fireplace','fireplace']\n    'fitness':['fitness','fitness_center','gym','gym_in_building']\n    'garden': ['garden','residents_garden']\n    'hardwood':['hardwood','hardwood_floors']\n    'high_ceiling':['high_ceiling','high_ceilings']\n    'high_speed_internet': ['high_speed_internet', 'speed_internet']\n    'in_super':['in_super','in_superintendent','live_in_super','site_super']\n    'laundry':['laundry','site_laundry']\n    'lounge':['lounge','lounge_room','residents_lounge']\n    'newly_renovated':['newly_renovated','renovated']\n    'outdoor_space':['outdoor_areas', 'outdoor_entertainment_space','outdoor_space']\n    'roof':['roof', 'roof_deck', 'roofdeck','terrace']\n    'wheelchair_access':['wheelchair_access', 'wheelchair_ramp']}\n    \n    def n_logical(list,Y):\n        n = len(list)\n        log_or = 0\n        for i in range(0,n-1):\n            if i == 0:\n                log_or = np.logical_or(Y[list[i]] == 1, Y[list[i+1]] == 1)\n            else:\n                log_or = np.logical_or(Y[list[i]] == 1, log_or)\n        \n        return log_or\n        \n    for (key, feature_list) in dup_dict:\n        X.loc[n_logical(feature_list, X), key] = 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27568176-299f-2ca3-b2fe-7e1d0634340f"},"outputs":[],"source":"def simple_bayes_high_cord(train_df, test_df):\n    index=list(range(train_df.shape[0]))\n    random.shuffle(index)\n    a=[np.nan]*len(train_df)\n    b=[np.nan]*len(train_df)\n    c=[np.nan]*len(train_df)\n\n    for i in range(5):\n        building_level={}\n        for j in train_df['manager_id'].values:\n            building_level[j]=[0,0,0]\n        test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n        train_index=list(set(index).difference(test_index))\n        for j in train_index:\n            temp=train_df.iloc[j]\n            if temp['interest_level']=='low':\n                building_level[temp['manager_id']][0]+=1\n            if temp['interest_level']=='medium':\n                building_level[temp['manager_id']][1]+=1\n            if temp['interest_level']=='high':\n                building_level[temp['manager_id']][2]+=1\n        for j in test_index:\n            temp=train_df.iloc[j]\n            if sum(building_level[temp['manager_id']])!=0:\n                a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n                b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n                c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n    train_df['manager_level_low']=a\n    train_df['manager_level_medium']=b\n    train_df['manager_level_high']=c\n\n\n\n    a=[]\n    b=[]\n    c=[]\n    building_level={}\n    for j in train_df['manager_id'].values:\n        building_level[j]=[0,0,0]\n    for j in range(train_df.shape[0]):\n        temp=train_df.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n\n    for i in test_df['manager_id'].values:\n        if i not in building_level.keys():\n            a.append(np.nan)\n            b.append(np.nan)\n            c.append(np.nan)\n        else:\n            a.append(building_level[i][0]*1.0/sum(building_level[i]))\n            b.append(building_level[i][1]*1.0/sum(building_level[i]))\n            c.append(building_level[i][2]*1.0/sum(building_level[i]))\n    test_df['manager_level_low']=a\n    test_df['manager_level_medium']=b\n    test_df['manager_level_high']=c\n\n    features_to_use.append('manager_level_low') \n    features_to_use.append('manager_level_medium') \n    features_to_use.append('manager_level_high')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f167309-9549-b2b1-b7e4-05c7681b13ba"},"outputs":[],"source":"print(\"Starting transformations\")        \nX_train = transform_data(X_train)    \nX_test = transform_data(X_test) \ny = X_train['interest_level'].ravel()\n\nprint(\"Normalizing high cordiality data...\")\nsimple_bayes_high_cord(X_train, X_test)\nnormalize_high_cordiality_data()\ntransform_categorical_data()\n\nremove_columns(X_train)\nremove_columns(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d43c854e-009a-2f3c-70a1-71b44680a9ec"},"outputs":[],"source":"X_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52fdf657-07d3-955c-af41-36258c5cfb36"},"outputs":[],"source":"cols = X_train.columns\nfor i in range(0,16):\n    print(cols[(10*i):(10*i+10)])\nprint(cols[160:len(cols)])"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}