{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6748f8d1-eb06-039a-9b7a-93b2de5ca8e9"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3cf573cb-3835-a3a1-b7b1-14783d890bbf"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport random\nfrom scipy import sparse\nfrom scipy.sparse import vstack\n\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge, LinearRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, log_loss, classification_report\n\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c4a40a-d7c5-2eb6-b24e-bfa810bd4d5e"},"outputs":[],"source":"data_path = \"../input/\"\ntrain_file = data_path + \"train.json\"\ntest_file = data_path + \"test.json\"\ntrain_df = pd.read_json(train_file)\ntest_df = pd.read_json(test_file)\nprint(train_df.shape)\nprint(test_df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7aa1154-5828-a0a5-800c-17589cf824f5"},"outputs":[],"source":"test_df[[\"listing_id\", \"display_address\",\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]].head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d2aa6d2-131a-9e0b-280d-41ec95db2086"},"outputs":[],"source":"def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 6\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = 1\n    param['subsample'] = 0.7\n    param['colsample_bytree'] = 0.7\n    param['seed'] = seed_val\n    num_rounds = num_rounds\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest)\n    return pred_test_y, model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6704b36-8c1a-46ec-743c-a75164e7a3ab"},"outputs":[],"source":"features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]\nfeatures_for_xgb  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12e75972-ddcf-2e49-8d22-6ee875b1fe8a"},"outputs":[],"source":"# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# convert the created column to datetime object so as to extract more features \ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n\n# Let us extract some features like year, month, day, hour from date columns #\ntrain_df[\"created_year\"] = train_df[\"created\"].dt.year\ntest_df[\"created_year\"] = test_df[\"created\"].dt.year\n\ntrain_df[\"created_month\"] = train_df[\"created\"].dt.month\ntest_df[\"created_month\"] = test_df[\"created\"].dt.month\n\ntrain_df[\"created_day\"] = train_df[\"created\"].dt.day\ntest_df[\"created_day\"] = test_df[\"created\"].dt.day\n\ntrain_df[\"created_hour\"] = train_df[\"created\"].dt.hour\ntest_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n\ntrain_df[\"weekday\"] = train_df[\"created\"].dt.weekday\ntest_df[\"weekday\"] = test_df[\"created\"].dt.weekday\n\n# adding all these new features to use list #\nfeatures_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \"created_day\", \"created_hour\", \"weekday\"])\nfeatures_for_xgb.extend([\"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee949888-3758-31b8-18b7-877df4ba7727"},"outputs":[],"source":"# Building Level\ntrain_df.ix[train_df.building_id == '0', 'new_building_id'] = train_df['building_id'] + train_df['manager_id']\ntrain_df.ix[train_df.building_id != '0', 'new_building_id'] = train_df['building_id']\n\na=[np.nan]*len(train_df)\nbuilding_level={}\n\nfor bid in train_df['new_building_id'].values:\n    building_level[bid]=[0,0,0]\n    \nfor j in range(train_df.shape[0]):\n    rec=train_df.iloc[j]\n    if rec['interest_level']=='low':\n        building_level[rec['new_building_id']][0]+=1\n    if rec['interest_level']=='medium':\n        building_level[rec['new_building_id']][1]+=1\n    if rec['interest_level']=='high':\n        building_level[rec['new_building_id']][2]+=1\n        \nfor j in range(train_df.shape[0]):    \n        rec=train_df.iloc[j]\n        occurance = sum(building_level[rec['new_building_id']])\n        if occurance!=0:\n            a[j]= (building_level[rec['new_building_id']][0]*0.0 + building_level[rec['new_building_id']][1]*1.0 \\\n                   + building_level[rec['new_building_id']][2]*2.0) / occurance\n\ntrain_df['building_level']=a\n\ntest_df.ix[test_df.building_id == '0', 'new_building_id'] = test_df['building_id'] + test_df['manager_id']\ntest_df.ix[test_df.building_id != '0', 'new_building_id'] = test_df['building_id']\n\nb=[]\nfor i in test_df['new_building_id'].values:\n    if i not in building_level.keys():\n        b.append(np.nan)\n    else:\n        occurance = sum(building_level[i])\n        b.append((building_level[i][0]*0.0 + building_level[i][1]*1.0 \\\n                   + building_level[i][2]*2.0) / occurance)\n\ntest_df['building_level']=b\n\ntrain_df = train_df.drop(['new_building_id'], axis=1)\ntest_df = test_df.drop(['new_building_id'], axis=1)\n\nfeatures_to_use.append('building_level')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1949e53-fbf9-90d2-86f7-180ae0b2f197"},"outputs":[],"source":"# Manager Level\nindex=list(range(train_df.shape[0]))\nrandom.shuffle(index)\na=[np.nan]*len(train_df)\nb=[np.nan]*len(train_df)\nc=[np.nan]*len(train_df)\n\nfor i in range(5):\n    building_level={}\n    for j in train_df['manager_id'].values:\n        building_level[j]=[0,0,0]\n    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n    train_index=list(set(index).difference(test_index))\n    for j in train_index:\n        temp=train_df.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n    for j in test_index:\n        temp=train_df.iloc[j]\n        if sum(building_level[temp['manager_id']])!=0:\n            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\ntrain_df['manager_level_low']=a\ntrain_df['manager_level_medium']=b\ntrain_df['manager_level_high']=c\n\n#### Prepare test data\na=[]\nb=[]\nc=[]\nbuilding_level={}\nfor j in train_df['manager_id'].values:\n    building_level[j]=[0,0,0]\nfor j in range(train_df.shape[0]):\n    temp=train_df.iloc[j]\n    if temp['interest_level']=='low':\n        building_level[temp['manager_id']][0]+=1\n    if temp['interest_level']=='medium':\n        building_level[temp['manager_id']][1]+=1\n    if temp['interest_level']=='high':\n        building_level[temp['manager_id']][2]+=1\n\nfor i in test_df['manager_id'].values:\n    if i not in building_level.keys():\n        a.append(np.nan)\n        b.append(np.nan)\n        c.append(np.nan)\n    else:\n        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n        c.append(building_level[i][2]*1.0/sum(building_level[i]))\ntest_df['manager_level_low']=a\ntest_df['manager_level_medium']=b\ntest_df['manager_level_high']=c\n\nfeatures_to_use.append('manager_level_low') \nfeatures_to_use.append('manager_level_medium') \nfeatures_to_use.append('manager_level_high')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c8c5ee4-be7c-c308-92a6-ce63a1a39735"},"outputs":[],"source":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)\n            features_for_xgb.append(f)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f5940ba-8b2d-a54f-a7fc-c63e0955740a"},"outputs":[],"source":"train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train_df[\"features\"])\nte_sparse = tfidf.transform(test_df[\"features\"])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23ed7ac5-2d7c-3236-2ca0-f5129efadfb9"},"outputs":[],"source":"#Train and test set for XGBoost\ntrain_X = sparse.hstack([train_df[features_for_xgb], tr_sparse]).tocsr()\ntest_X = sparse.hstack([test_df[features_for_xgb], te_sparse]).tocsr()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bccb8dd-3aed-de29-fb77-22fddf19d3f2"},"outputs":[],"source":"target_num_map = {'high':2, 'medium':1, 'low':0}\ntrain_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7a6c164-6e92-e0ab-2d0b-6d1dcbc8b65c"},"outputs":[],"source":"#Prepare Train and test sets for Trees\nfill_NaN = Imputer(missing_values=np.nan, strategy='mean', axis=1)\ntrain_imputed = pd.DataFrame(fill_NaN.fit_transform(train_df[features_to_use]))\ntrain_imputed.columns = train_df[features_to_use].columns\ntrain_imputed.index = train_df.index\n\ntest_imputed = pd.DataFrame(fill_NaN.fit_transform(test_df[features_to_use]))\ntest_imputed.columns = test_df[features_to_use].columns\ntest_imputed.index = test_df.index\n\n\ntrain_Xtree = train_imputed\ntest_Xtree = test_imputed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1824537b-2f10-b315-d480-ca20d9ff891d"},"outputs":[],"source":"#Train and test set for XGBoost\nprint(train_X.shape, test_X.shape)\n\n#Train and test set for trees\nprint(train_Xtree.shape, test_Xtree.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"45793f79-035d-b352-882b-33b0b86ab06d"},"source":"## Begin Stacking"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"891419d1-3474-c433-20d8-150ccf2fbbde"},"outputs":[],"source":"NFOLDS = 5\nSEED = 0\ny_train = train_y\n\nntrain = train_Xtree.shape[0]\nntest = test_Xtree.shape[0]\nprint(\"{},{}\".format(ntrain, ntest))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9140a62c-4699-b5bd-5d97-0bb1fb617fd5"},"outputs":[],"source":"class SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict_prb(self, x):\n        return self.clf.predict(x)\n    \n    def predict_proba(self, x):\n        return self.clf.predict_proba(x)  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2584c69-9c4f-c741-5c9c-548dfe57acc0"},"outputs":[],"source":"def get_oof(clf):\n    oof_train = np.zeros((ntrain,3))\n    oof_test = np.zeros((ntest,3))\n    oof_test_skf = np.empty((NFOLDS, ntest, 3))\n\n    i = 0\n    for train_index, test_index in skf.split(x_train, y_train):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        rf1.train(x_tr, y_tr)\n\n        oof_train[test_index]= rf1.predict_proba(x_te)\n        oof_test_skf[i, :, :] = rf1.predict_proba(x_test)\n        i += 1\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train, oof_test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd439a36-2956-5a9d-41f4-a7211109d9b4"},"outputs":[],"source":"train_test = pd.concat((train_Xtree, test_Xtree)).reset_index(drop=True)\nx_train = np.array(train_test.iloc[:ntrain,:])\nx_test = np.array(train_test.iloc[ntrain:,:])\n\nprint(\"{},{},{}\".format(x_train.shape, y_train.shape, x_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42e4b347-42a5-55d6-e287-d95fa4c6cfdf"},"outputs":[],"source":"skf = StratifiedKFold(n_splits=NFOLDS, random_state=SEED, shuffle=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ce4f703-9f5d-6e40-678f-fad8c71f4eff"},"outputs":[],"source":"rf1_params = {\n    'n_jobs': 16,\n    'n_estimators': 10,\n    'criterion' : \"entropy\",\n    'max_features': 0.5,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n}\n\nrf2_params = {\n    'n_jobs': 16,\n    'criterion' : \"gini\",\n    'n_estimators': 1000,\n    'max_features': None,\n    'max_depth': 8,\n    'min_samples_leaf': 1,\n}\n\net1_params = {\n    'n_jobs': 16,\n    'n_estimators': 10,\n    'max_features': \"auto\",\n    'criterion' : \"gini\",\n    'max_depth': 4,\n    'min_samples_leaf': 2,\n}\n\net2_params = {\n    'n_jobs': 16,\n    'n_estimators': 1000,\n    'criterion' : \"entropy\",\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n    'max_features': 0.8,\n}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef651ee2-380c-539a-d15d-17aee17429ac"},"outputs":[],"source":"et1 = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et1_params)\net2 = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et2_params)\n\nrf1 = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf1_params)\nrf2 = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf2_params)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d389fcbd-a632-7470-49b6-1eb0035d2576"},"outputs":[],"source":"gbc1 = GradientBoostingClassifier(n_estimators = 10, max_depth = 4, subsample = 0.5,\n          learning_rate = 0.1, min_samples_leaf = 2, random_state = 0)\n\ngbc2 = GradientBoostingClassifier(n_estimators = 1000, max_depth = 8, \n          learning_rate = 0.5, min_samples_leaf = 1, random_state = 0)\n\ngbc1_oof_train, gbc1_oof_test = get_oof(gbc1)\ngbc2_oof_train, gbc2_oof_test = get_oof(gbc2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb0aa4ea-9779-8bbd-e049-768c4656be17"},"outputs":[],"source":"et1_oof_train, et1_oof_test = get_oof(et1)\net2_oof_train, et2_oof_test = get_oof(et2)\n\nrf1_oof_train, rf1_oof_test = get_oof(rf1)\nrf2_oof_train, rf2_oof_test = get_oof(rf2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1ea91ce-21cb-1319-2c13-23ede025537b"},"outputs":[],"source":"print(\"ET1-CV: {}\".format(log_loss(y_train, et1_oof_train)))\nprint(\"ET2-CV: {}\".format(log_loss(y_train, et2_oof_train)))\n\nprint(\"RF1-CV: {}\".format(log_loss(y_train, rf1_oof_train)))\nprint(\"RF2-CV: {}\".format(log_loss(y_train, rf2_oof_train)))\n\nprint(\"GBC1-CV: {}\".format(log_loss(y_train, gbc1_oof_train)))\nprint(\"GBC2-CV: {}\".format(log_loss(y_train, gbc2_oof_train)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9a98e01-5b31-57df-cbc1-307e59a6d1aa"},"source":"## Prepare XGBoost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64040798-4c8a-a48d-bb08-9b25e677f24f"},"outputs":[],"source":"class XgbWrapper(object):\n    def __init__(self, seed=0, params=None):\n        self.param = params\n        self.param['seed'] = seed\n        self.nrounds = params.pop('nrounds', 250)\n\n    def train(self, x_train, y_train):\n        dtrain = xgb.DMatrix(x_train, label=y_train)\n        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n\n    def predict(self, x):\n        return self.gbdt.predict(xgb.DMatrix(x))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"281a74c2-2a76-8ba3-ab40-a6c5c434cede"},"outputs":[],"source":"xgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.7,\n    'silent': 1,\n    'eta': 0.1,\n    'subsample': 0.7,\n    'learning_rate': 0.075,\n    'objective': 'multi:softprob',\n    'num_class': 3,\n    'max_depth': 7,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'eval_metric': \"mlogloss\",\n    'nrounds': 400\n}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99d759af-45d5-60be-706b-a24ad4e001af"},"outputs":[],"source":"train_test = vstack([train_X, test_X]).toarray()\nx_train = np.array(train_test[:ntrain,:])\nx_test = np.array(train_test[ntrain:,:])\n\nprint(\"{},{},{}\".format(x_train.shape, y_train.shape, x_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6810c2e-8bc3-24ba-ac5e-2cc76819fed6"},"outputs":[],"source":"xg = XgbWrapper(seed=SEED, params=xgb_params)\nxg_oof_train, xg_oof_test = get_oof(xg)\nprint(\"XG-CV: {}\".format(log_loss(y_train, xg_oof_train)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2b722ed-5204-72b2-0d87-b3cfb9b04736"},"outputs":[],"source":"x_train = np.concatenate((xg_oof_train, et1_oof_train, rf2_oof_train, gbc2_oof_train), axis=1)\nx_test = np.concatenate((xg_oof_test, et1_oof_test, rf2_oof_test, gbc2_oof_test), axis=1)\n\nprint(\"{},{}\".format(x_train.shape, x_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69279589-63c0-9341-1d91-fcd353cda6d0"},"outputs":[],"source":"dtrain = xgb.DMatrix(x_train, label=y_train)\ndtest = xgb.DMatrix(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e44c3133-1d3f-831c-6b57-3295502c14b9"},"source":"## Level 2 stacking"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03ef166b-1220-4bd3-ed6b-4cb564344c69"},"outputs":[],"source":"xgb_params_2 = {\n    'seed': 0,\n    'colsample_bytree': 0.8,\n    'eta': 0.1,\n    'silent': 1,\n    'subsample': 0.6,\n    'learning_rate': 0.01,\n    'objective': 'multi:softprob',\n    'num_class': 3,\n    'max_depth': 7,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'eval_metric': 'mlogloss',   \n}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5188f1ef-cf4f-6141-bf93-7b8e467c0133"},"outputs":[],"source":"res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=4, seed=SEED, stratified=False,\n             early_stopping_rounds=25, verbose_eval=10, show_stdv=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6570d3e5-2f09-58cd-0ffd-eb5ae3c0cff7"},"outputs":[],"source":"best_nrounds = res.shape[0] - 1\ncv_mean = res.iloc[-1, 0]\ncv_std = res.iloc[-1, 1]\n\nprint('Ensemble-CV: {0}+{1}'.format(cv_mean, cv_std))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b48dd185-ee69-17f7-4b92-1f4605cd313b"},"outputs":[],"source":"gbdt = xgb.train(xgb_params_2, dtrain, best_nrounds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4577ea9e-703f-3132-ba64-09bab4f0e78d"},"outputs":[],"source":"preds = gbdt.predict(dtest)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84503177-10dd-1daf-141f-ad3b8cafbddd"},"outputs":[],"source":"out_df = pd.DataFrame(preds)\nout_df.columns = [\"low\", \"medium\", \"high\"]\nout_df[\"listing_id\"] = test_df.listing_id.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95d9fa5a-4232-b397-1a19-ce6b9ce4e5c0"},"outputs":[],"source":"out_df.to_csv(\"./stacker_starter_1.csv\", index=False) "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}