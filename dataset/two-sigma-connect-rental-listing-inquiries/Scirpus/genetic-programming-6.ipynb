{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"84508db8-6c14-866c-00b7-ac48fb7e42d4"},"source":"There is a lot of juice to be had combining the raw numeric parameters.  This is for illustrative purposes and scores only 0.664 with a train score of 0.661.  Coming up with decent combos will certainly improve your scores. As will by thinking vertically with rolling parameters based on dates."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5e3fa92-9e2d-7c20-230e-89bb154f3d55"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2a41e87-7b7e-d4a0-5b31-2597fb77aaa2"},"outputs":[],"source":"def Outputs(p):\n    return 1.0/(1.0+np.exp(-p))\n\ndef GPLow(data):\n    p = (np.tanh((((2.275860 + (((2.275860 + (19.666700 + ((data[\"num_photos\"] * 2.0) * 2.0))) + data[\"bedrooms\"]) * data[\"price\"]))/2.0) * 2.0)) +\n         np.tanh((((((((data[\"price\"] * 2.0) * 2.0) * 2.0) - ((data[\"num_features\"] + data[\"bedrooms\"])/2.0)) * 2.0) + (data[\"num_photos\"] * data[\"num_photos\"]))/2.0)) +\n         np.tanh(((((data[\"price\"] + data[\"latitude\"]) * 2.0) * 2.0) - (((data[\"created_hour\"] * 2.0) + ((((data[\"num_description_words\"] + data[\"bedrooms\"])/2.0) + data[\"bedrooms\"])/2.0))/2.0))) +\n         np.tanh((((0.220930 + ((data[\"created_hour\"] + ((-((data[\"created_hour\"] / 2.0))) * (data[\"created_hour\"] / 2.0)))/2.0))/2.0) * (data[\"num_features\"] + data[\"created_hour\"]))) +\n         np.tanh((((data[\"bathrooms\"] - (data[\"bedrooms\"] * 2.0)) * data[\"price\"]) + (data[\"latitude\"] * ((data[\"price\"] + data[\"latitude\"]) * 31.0)))) +\n         np.tanh(((data[\"longitude\"] * (data[\"longitude\"] + ((5.0) + (3.750000 * (data[\"bedrooms\"] * 2.0))))) + (data[\"price\"] + data[\"latitude\"]))) +\n         np.tanh(((0.065574 * (data[\"num_description_words\"] * (((((data[\"created_hour\"] * 2.0) + data[\"num_features\"])/2.0) + (data[\"num_description_words\"] - 1.022220))/2.0))) * 2.0)) +\n         np.tanh(((((data[\"price\"] - (data[\"bathrooms\"] / 2.0)) / 2.0) + (((0.065574 + data[\"price\"]) + data[\"longitude\"]) * 2.0)) + data[\"longitude\"])))\n    return Outputs(p)\n\ndef GPMedium(data):\n    p = (np.tanh(((data[\"num_features\"] + ((data[\"num_features\"] * (3.857140 - data[\"num_features\"])) - (data[\"created_day\"] + 10.0)))/2.0)) +\n         np.tanh((((((data[\"num_photos\"] + data[\"created_hour\"])/2.0) + ((-(data[\"latitude\"])) - (data[\"num_photos\"] * data[\"num_photos\"])))/2.0) - ((data[\"price\"] * 2.0) * 2.0))) +\n         np.tanh((((((((data[\"bedrooms\"] + data[\"num_features\"])/2.0) + data[\"num_features\"])/2.0) + (data[\"bedrooms\"] - 0.591837))/2.0) - (((data[\"price\"] * 2.0) * 2.0) * 2.0))) +\n         np.tanh((((data[\"price\"] * (((7.0) + ((-(data[\"bathrooms\"])) * (data[\"price\"] * (7.0))))/2.0)) * data[\"bedrooms\"]) - 0.220930)) +\n         np.tanh(((((0.090909 - data[\"latitude\"]) * (((data[\"latitude\"] + data[\"num_description_words\"])/2.0) + data[\"bedrooms\"])) - data[\"latitude\"]) - data[\"price\"])) +\n         np.tanh((((data[\"latitude\"] * (data[\"latitude\"] * ((-((10.0 + data[\"bathrooms\"]))) * 2.0))) - data[\"longitude\"]) - (data[\"longitude\"] * 2.0))) +\n         np.tanh((0.090909 * ((data[\"created_hour\"] + 1.169230) + ((-((data[\"num_features\"] + data[\"created_hour\"]))) * (data[\"created_hour\"] + data[\"num_description_words\"]))))) +\n         np.tanh((((data[\"num_photos\"] + (((-(data[\"num_photos\"])) / 2.0) * (data[\"num_photos\"] / 2.0)))/2.0) * ((-1.0 + (data[\"num_photos\"] * data[\"num_photos\"]))/2.0))))\n    return Outputs(p)\n\ndef GPHigh(data):\n    p = (np.tanh((19.666700 * (-1.0 - ((data[\"price\"] * (((((19.666700 + data[\"created_hour\"])/2.0) * 2.0) + 5.764710)/2.0)) / 2.0)))) +\n         np.tanh((-((((1.514290 + (((((5.764710 * 2.0) * 2.0) + data[\"bedrooms\"])/2.0) * data[\"price\"])) * 2.0) * 2.0)))) +\n         np.tanh(((((-((data[\"price\"] * 31.0))) + (-(3.071430))) + (data[\"bedrooms\"] + (data[\"created_hour\"] + data[\"bedrooms\"])))/2.0)) +\n         np.tanh((((data[\"num_features\"] / 2.0) / 2.0) - (data[\"price\"] + ((data[\"price\"] * 2.0) + (((data[\"num_photos\"] * data[\"num_photos\"]) + 0.090909)/2.0))))) +\n         np.tanh((((data[\"latitude\"] * 2.0) + data[\"price\"]) * ((data[\"created_hour\"] - (1.653850 + (31.0 * data[\"latitude\"]))) - 1.653850))) +\n         np.tanh((((((((0.090909 + data[\"price\"])/2.0) * 2.0) * 2.0) * ((-(1.362070)) + (data[\"bedrooms\"] * 2.0))) - data[\"longitude\"]) * 2.0)) +\n         np.tanh((((((-(data[\"price\"])) + (((-1.0 + data[\"num_photos\"])/2.0) - (data[\"longitude\"] * 19.666700)))/2.0) - data[\"price\"]) - data[\"price\"])) +\n         np.tanh((data[\"latitude\"] + (((-(((data[\"created_day\"] + ((((data[\"created_hour\"] + data[\"bedrooms\"])/2.0) * data[\"bedrooms\"]) - data[\"bedrooms\"]))/2.0))) / 2.0) / 2.0))))\n    return Outputs(p)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81b3168d-ef8c-a5c4-0d3b-5afdd6787d00"},"outputs":[],"source":"train = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')\ntrain.loc[train.interest_level=='low','interest_level'] = 0\ntrain.loc[train.interest_level=='medium','interest_level'] = 1\ntrain.loc[train.interest_level=='high','interest_level'] = 2\ntrain.interest_level = train.interest_level.astype(float)\ntrain[\"created\"] = pd.to_datetime(train[\"created\"])\ntest[\"created\"] = pd.to_datetime(test[\"created\"])\ntrain[\"num_photos\"] = train[\"photos\"].apply(len)\ntest[\"num_photos\"] = test[\"photos\"].apply(len)\ntrain[\"num_features\"] = train[\"features\"].apply(len)\ntest[\"num_features\"] = test[\"features\"].apply(len)\ntrain[\"created_year\"] = train[\"created\"].dt.year\ntest[\"created_year\"] = test[\"created\"].dt.year\ntrain[\"created_month\"] = train[\"created\"].dt.month\ntest[\"created_month\"] = test[\"created\"].dt.month\ntrain[\"created_day\"] = train[\"created\"].dt.day\ntest[\"created_day\"] = test[\"created\"].dt.day\ntrain[\"created_hour\"] = train[\"created\"].dt.hour\ntest[\"created_hour\"] = test[\"created\"].dt.hour\ntrain[\"num_description_words\"] = train[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest[\"num_description_words\"] = test[\"description\"].apply(lambda x: len(x.split(\" \")))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"894df8ec-adfd-121e-4326-86df6aa83c2a"},"outputs":[],"source":"actualcolumns = ['bathrooms', 'bedrooms',\n                 'num_photos', 'num_features', 'num_description_words',\n                 'latitude','longitude',\n                 'price',\n                 'created_year',\n                 'created_month',\n                 'created_day',\n                 'created_hour']\nclasses = ['low','medium','high']\nss = StandardScaler()\nss.fit(pd.concat([train[actualcolumns],test[actualcolumns]]))\npredictions  = np.zeros((train.shape[0],3))\ngptrain = train[actualcolumns].copy()\ngptrain[actualcolumns] = ss.transform(train[actualcolumns])\nfor i in range(3):\n    if(i==0):\n        predictions[:,0] = GPLow(gptrain)\n    elif(i==1):\n        predictions[:,1] = GPMedium(gptrain)\n    else:\n        predictions[:,2] = GPHigh(gptrain)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b99c239-3bb5-9c5a-023a-e65c123f62eb"},"outputs":[],"source":"print('Log Loss', log_loss(train.interest_level,predictions))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71661550-1c43-51e0-6ce9-d06880d1e58a"},"outputs":[],"source":"predictions  = np.zeros((test.shape[0],3))\ngptest = test[actualcolumns].copy()\ngptest[actualcolumns] = ss.transform(test[actualcolumns])\nfor i in range(3):\n    if(i==0):\n        predictions[:,0] = GPLow(gptest)\n    elif(i==1):\n        predictions[:,1] = GPMedium(gptest)\n    else:\n        predictions[:,2] = GPHigh(gptest)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4a4abba-4681-de9f-9462-da8fc1ef347c"},"outputs":[],"source":"print('Started producing Submission File')\nout_df = pd.DataFrame(predictions)\nout_df.columns = [\"low\", \"medium\", \"high\" ]\nout_df[[\"low\", \"medium\", \"high\" ]] = out_df[[\"low\", \"medium\", \"high\" ]].div(out_df[[\"low\", \"medium\", \"high\" ]].sum(axis=1), axis=0)\nout_df[\"listing_id\"] = test.listing_id.values\nout_df = out_df[['high', 'medium', 'low','listing_id']]\nout_df.to_csv(\"loo_xgb_starter.csv\", index=False)\nprint('Finished producing Submission File')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}