{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install ../input/timm-pytorch-image-models/pytorch-image-models-master/","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:07.252608Z","iopub.execute_input":"2021-05-29T23:40:07.252997Z","iopub.status.idle":"2021-05-29T23:40:37.968209Z","shell.execute_reply.started":"2021-05-29T23:40:07.25291Z","shell.execute_reply":"2021-05-29T23:40:37.967237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"# import glob\nimport os\nimport random\nimport warnings\nfrom functools import partial\n\n# import colorednoise as cn\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport scipy as sp\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.optim as optim\nfrom pytorch_lightning import LightningDataModule, callbacks\n\n# from pytorch_lightning.utilities import rank_zero_info\nfrom sklearn import model_selection\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchaudio.transforms import AmplitudeToDB, MelSpectrogram\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:37.971443Z","iopub.execute_input":"2021-05-29T23:40:37.971814Z","iopub.status.idle":"2021-05-29T23:40:42.913838Z","shell.execute_reply.started":"2021-05-29T23:40:37.971779Z","shell.execute_reply":"2021-05-29T23:40:42.911838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport audioread\nimport logging\nimport os\nimport random\nimport time\nimport warnings\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\n# from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n# from torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T23:40:42.915298Z","iopub.execute_input":"2021-05-29T23:40:42.915682Z","iopub.status.idle":"2021-05-29T23:40:43.856666Z","shell.execute_reply.started":"2021-05-29T23:40:42.915641Z","shell.execute_reply":"2021-05-29T23:40:43.855877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n    \n    \ndef get_logger(out_file=None):\n    logger = logging.getLogger()\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger.handlers = []\n    logger.setLevel(logging.INFO)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    handler.setLevel(logging.INFO)\n    logger.addHandler(handler)\n\n    if out_file is not None:\n        fh = logging.FileHandler(out_file)\n        fh.setFormatter(formatter)\n        fh.setLevel(logging.INFO)\n        logger.addHandler(fh)\n    logger.info(\"logger set up\")\n    return logger\n    \n    \n@contextmanager\ndef timer(name: str, logger: Optional[logging.Logger] = None):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:43.857949Z","iopub.execute_input":"2021-05-29T23:40:43.858264Z","iopub.status.idle":"2021-05-29T23:40:43.868124Z","shell.execute_reply.started":"2021-05-29T23:40:43.85823Z","shell.execute_reply":"2021-05-29T23:40:43.867107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = get_logger(\"main.log\")\nset_seed(1213)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:43.871356Z","iopub.execute_input":"2021-05-29T23:40:43.87195Z","iopub.status.idle":"2021-05-29T23:40:43.888882Z","shell.execute_reply.started":"2021-05-29T23:40:43.871913Z","shell.execute_reply":"2021-05-29T23:40:43.888206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    ######################\n    # Globals #\n    ######################\n    seed = 1213\n    epochs = 35\n    train = True\n    folds = [0]\n    img_size = 224\n    main_metric = \"epoch_f1_at_05\"\n    minimize_metric = False\n\n    ######################\n    # Data #\n    ######################\n    train_datadir = Path(\"../input/birdclef-2021/train_short_audio\")\n    train_csv = \"../input/birdclef-2021/train_metadata.csv\"\n    train_soundscape = \"../input/birdclef-2021/train_soundscape_labels.csv\"\n\n    ######################\n    # Dataset #\n    ######################\n    transforms = {\n        \"train\": [{\"name\": \"Normalize\"}],\n        \"valid\": [{\"name\": \"Normalize\"}],\n        \"test\": [{\"name\": \"Normalize\"}]\n    }\n    period = 30\n#     period = 5\n    n_mels = 128\n    \n    sample_rate = 32000\n\n    target_columns = [\n        'acafly', 'acowoo', 'aldfly', 'ameavo', 'amecro',\n        'amegfi', 'amekes', 'amepip', 'amered', 'amerob',\n        'amewig', 'amtspa', 'andsol1', 'annhum', 'astfly',\n        'azaspi1', 'babwar', 'baleag', 'balori', 'banana',\n        'banswa', 'banwre1', 'barant1', 'barswa', 'batpig1',\n        'bawswa1', 'bawwar', 'baywre1', 'bbwduc', 'bcnher',\n        'belkin1', 'belvir', 'bewwre', 'bkbmag1', 'bkbplo',\n        'bkbwar', 'bkcchi', 'bkhgro', 'bkmtou1', 'bknsti', 'blbgra1',\n        'blbthr1', 'blcjay1', 'blctan1', 'blhpar1', 'blkpho',\n        'blsspa1', 'blugrb1', 'blujay', 'bncfly', 'bnhcow', 'bobfly1',\n        'bongul', 'botgra', 'brbmot1', 'brbsol1', 'brcvir1', 'brebla',\n        'brncre', 'brnjay', 'brnthr', 'brratt1', 'brwhaw', 'brwpar1',\n        'btbwar', 'btnwar', 'btywar', 'bucmot2', 'buggna', 'bugtan',\n        'buhvir', 'bulori', 'burwar1', 'bushti', 'butsal1', 'buwtea',\n        'cacgoo1', 'cacwre', 'calqua', 'caltow', 'cangoo', 'canwar',\n        'carchi', 'carwre', 'casfin', 'caskin', 'caster1', 'casvir',\n        'categr', 'ccbfin', 'cedwax', 'chbant1', 'chbchi', 'chbwre1',\n        'chcant2', 'chispa', 'chswar', 'cinfly2', 'clanut', 'clcrob',\n        'cliswa', 'cobtan1', 'cocwoo1', 'cogdov', 'colcha1', 'coltro1',\n        'comgol', 'comgra', 'comloo', 'commer', 'compau', 'compot1',\n        'comrav', 'comyel', 'coohaw', 'cotfly1', 'cowscj1', 'cregua1',\n        'creoro1', 'crfpar', 'cubthr', 'daejun', 'dowwoo', 'ducfly', 'dusfly',\n        'easblu', 'easkin', 'easmea', 'easpho', 'eastow', 'eawpew', 'eletro',\n        'eucdov', 'eursta', 'fepowl', 'fiespa', 'flrtan1', 'foxspa', 'gadwal',\n        'gamqua', 'gartro1', 'gbbgul', 'gbwwre1', 'gcrwar', 'gilwoo',\n        'gnttow', 'gnwtea', 'gocfly1', 'gockin', 'gocspa', 'goftyr1',\n        'gohque1', 'goowoo1', 'grasal1', 'grbani', 'grbher3', 'grcfly',\n        'greegr', 'grekis', 'grepew', 'grethr1', 'gretin1', 'greyel',\n        'grhcha1', 'grhowl', 'grnher', 'grnjay', 'grtgra', 'grycat',\n        'gryhaw2', 'gwfgoo', 'haiwoo', 'heptan', 'hergul', 'herthr',\n        'herwar', 'higmot1', 'hofwoo1', 'houfin', 'houspa', 'houwre',\n        'hutvir', 'incdov', 'indbun', 'kebtou1', 'killde', 'labwoo', 'larspa',\n        'laufal1', 'laugul', 'lazbun', 'leafly', 'leasan', 'lesgol', 'lesgre1',\n        'lesvio1', 'linspa', 'linwoo1', 'littin1', 'lobdow', 'lobgna5', 'logshr',\n        'lotduc', 'lotman1', 'lucwar', 'macwar', 'magwar', 'mallar3', 'marwre',\n        'mastro1', 'meapar', 'melbla1', 'monoro1', 'mouchi', 'moudov', 'mouela1',\n        'mouqua', 'mouwar', 'mutswa', 'naswar', 'norcar', 'norfli', 'normoc', 'norpar',\n        'norsho', 'norwat', 'nrwswa', 'nutwoo', 'oaktit', 'obnthr1', 'ocbfly1',\n        'oliwoo1', 'olsfly', 'orbeup1', 'orbspa1', 'orcpar', 'orcwar', 'orfpar',\n        'osprey', 'ovenbi1', 'pabspi1', 'paltan1', 'palwar', 'pasfly', 'pavpig2',\n        'phivir', 'pibgre', 'pilwoo', 'pinsis', 'pirfly1', 'plawre1', 'plaxen1',\n        'plsvir', 'plupig2', 'prowar', 'purfin', 'purgal2', 'putfru1', 'pygnut',\n        'rawwre1', 'rcatan1', 'rebnut', 'rebsap', 'rebwoo', 'redcro', 'reevir1',\n        'rehbar1', 'relpar', 'reshaw', 'rethaw', 'rewbla', 'ribgul', 'rinkin1',\n        'roahaw', 'robgro', 'rocpig', 'rotbec', 'royter1', 'rthhum', 'rtlhum',\n        'ruboro1', 'rubpep1', 'rubrob', 'rubwre1', 'ruckin', 'rucspa1', 'rucwar',\n        'rucwar1', 'rudpig', 'rudtur', 'rufhum', 'rugdov', 'rumfly1', 'runwre1',\n        'rutjac1', 'saffin', 'sancra', 'sander', 'savspa', 'saypho', 'scamac1',\n        'scatan', 'scbwre1', 'scptyr1', 'scrtan1', 'semplo', 'shicow', 'sibtan2',\n        'sinwre1', 'sltred', 'smbani', 'snogoo', 'sobtyr1', 'socfly1', 'solsan',\n        'sonspa', 'soulap1', 'sposan', 'spotow', 'spvear1', 'squcuc1', 'stbori',\n        'stejay', 'sthant1', 'sthwoo1', 'strcuc1', 'strfly1', 'strsal1', 'stvhum2',\n        'subfly', 'sumtan', 'swaspa', 'swathr', 'tenwar', 'thbeup1', 'thbkin',\n        'thswar1', 'towsol', 'treswa', 'trogna1', 'trokin', 'tromoc', 'tropar',\n        'tropew1', 'tuftit', 'tunswa', 'veery', 'verdin', 'vigswa', 'warvir',\n        'wbwwre1', 'webwoo1', 'wegspa1', 'wesant1', 'wesblu', 'weskin', 'wesmea',\n        'westan', 'wewpew', 'whbman1', 'whbnut', 'whcpar', 'whcsee1', 'whcspa',\n        'whevir', 'whfpar1', 'whimbr', 'whiwre1', 'whtdov', 'whtspa', 'whwbec1',\n        'whwdov', 'wilfly', 'willet1', 'wilsni1', 'wiltur', 'wlswar', 'wooduc',\n        'woothr', 'wrenti', 'y00475', 'yebcha', 'yebela1', 'yebfly', 'yebori1',\n        'yebsap', 'yebsee1', 'yefgra1', 'yegvir', 'yehbla', 'yehcar1', 'yelgro',\n        'yelwar', 'yeofly1', 'yerwar', 'yeteup1', 'yetvir']\n\n    ######################\n    # Loaders #\n    ######################\n    loader_params = {\n        \"train\": {\n            \"batch_size\": 64,\n            \"num_workers\": 20,\n            \"shuffle\": True\n        },\n        \"valid\": {\n            \"batch_size\": 64,\n            \"num_workers\": 20,\n            \"shuffle\": False\n        },\n        \"test\": {\n            \"batch_size\": 64,\n            \"num_workers\": 20,\n            \"shuffle\": False\n        }\n    }\n\n    ######################\n    # Model #\n    ######################\n    models_cfg = [\n        {\"resnet34\": \"../input/birdclef20201-checkpoints/checkpoints/exp004/fold0__20210417-17-00-45/lightning_logs/version_0/checkpoints/best_loss.ckpt\",\n},\n        {\"efficientnet_b0\": \"../input/birdclef20201-checkpoints/checkpoints/exp004/fold0__20210418-10-44-57/best_loss.ckpt\",},\n        {\"resnet18\": \"../input/birdclef20201-checkpoints/checkpoints/exp005/fold0__20210420-06-34-46/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"../input/birdclef20201-checkpoints/checkpoints/exp005/fold0__20210421-00-17-35/best_loss.ckpt\",},\n        {\"mixnet_m\": \"../input/birdclef20201-checkpoints/checkpoints/exp005/fold0__20210421-11-01-38/best_loss.ckpt\",},\n        {\"repvgg_b0\": \"../input/birdclef20201-checkpoints/checkpoints/exp005/fold0__20210421-21-55-49/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints/checkpoints/exp005/fold0__20210422-11-13-45/best_loss.ckpt\"},\n        {\"resnest50d_1s4x24d\": \"../input/birdclef20201-checkpoints/checkpoints/exp005/fold0__20210423-04-29-31/best_loss.ckpt\",},\n        \n        {\"repvgg_b0\": \"../input/birdclef20201-checkpoints/checkpoints/exp008/repvgg_b0/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        \n        {\"resnet34\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/resnet34/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"efficientnet_b0\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/efficientnet_b0/fold0/lightning_logs/version_1/checkpoints/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/resnest50d_1s4x24d/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/resnest50d_1s4x24d/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet18\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/resnet18/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/ecaresnet26t/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/ecaresnet26t/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"mixnet_m\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/mixnet_m/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/resnest26d/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/resnest26d/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/resnest26d/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"repvgg_b0\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/repvgg_b0/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"repvgg_b0\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/repvgg_b0/fold3/lightning_logs/version_0/checkpoints/checkpoints/best_loss.ckpt\"},\n        {\"repvgg_b2\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/repvgg_b2/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"repvgg_b2\":\"../input/birdclef20201-checkpoints/checkpoints/exp009/repvgg_b2/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        \n        {\"resnet34\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/resnet34/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"efficientnet_b0\": \"../input/birdclef20201-checkpoints/checkpoints/exp009/efficientnet_b0/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/resnest50d_1s4x24d/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet18\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/resnet18/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/ecaresnet26t/fold0/lightning_logs/version_1/checkpoints/best_loss.ckpt\",},\n        {\"mixnet_m\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/mixnet_m/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/resnest26d/fold2/lightning_logs/version_1/checkpoints/best_loss.ckpt\"},\n        {\"repvgg_b0\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/repvgg_b0/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"repvgg_b2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/repvgg_b2/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        \n        {\"efficientnet_b0\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/efficientnet_b0_ft30/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnest50d_1s4x24d_ft30/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"ecaresnet26t\":\"../input/birdclef20201-checkpoints2/checkpoints/exp010/ecaresnet26t_ft30/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnest26d_ft30/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"repvgg_b2\":\"../input/birdclef20201-checkpoints2/checkpoints/exp010/repvgg_b2_ft30/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        \n        {\"efficientnet_b0\":\"../input/birdclef20201-checkpoints2/checkpoints/exp010/efficientnet_b0_ft30/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"resnest50d_1s4x24d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnest50d_1s4x24d_ft30/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/ecaresnet26t_ft30/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnest26d_ft30/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnest26d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnest26d_ft30/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"repvgg_b2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/repvgg_b2_ft30/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        \n        {\"resnest50d_1s4x24d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/resnest50d_1s4x24d/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet50d\":\"../input/birdclef20201-checkpoints2/checkpoints/exp009/resnet50d/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet50d\":\"../input/birdclef20201-checkpoints2/checkpoints/exp009/resnet50d/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"efficientnet_b2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/efficientnet_b2/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"efficientnet_b2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/efficientnet_b2/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        \n        {\"resnest50d_1s4x24d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnest50d_1s4x24d_ft30/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet50d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnet50d_ft30/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet50d\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/resnet50d_ft30/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\", },\n        {\"efficientnet_b2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/efficientnet_b2_ft30/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"efficientnet_b2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp010/efficientnet_b2_ft30/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        \n        {\"wide_resnet50_2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/wide_resnet50_2/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"wide_resnet50_2\": \"../input/birdclef20201-checkpoints2/checkpoints/exp009/wide_resnet50_2/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        \n        {\"repvgg_b2\": \"../input/birdclef20201-checkpoints3/checkpoints/exp011/repvgg_b2/fold0/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"repvgg_b2\": \"../input/birdclef20201-checkpoints3/checkpoints/exp011/repvgg_b2/fold1/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet50d\":\"../input/birdclef20201-checkpoints3/checkpoints/exp011/resnet50d/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"resnet50d\":\"../input/birdclef20201-checkpoints3/checkpoints/exp011/resnet50d/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\",},\n        {\"wide_resnet50_2\":\"../input/birdclef20201-checkpoints3/checkpoints/exp011/wide_resnet50_2/fold2/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n        {\"wide_resnet50_2\":\"../input/birdclef20201-checkpoints3/checkpoints/exp011/wide_resnet50_2/fold3/lightning_logs/version_0/checkpoints/best_loss.ckpt\"},\n    ]\n    pretrained = False\n    num_classes = 397\n    in_channels = 1\nprint(f\"model_num: {len(CFG.models_cfg)}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:43.891934Z","iopub.execute_input":"2021-05-29T23:40:43.892295Z","iopub.status.idle":"2021-05-29T23:40:43.924813Z","shell.execute_reply.started":"2021-05-29T23:40:43.892259Z","shell.execute_reply":"2021-05-29T23:40:43.923476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"TARGET_SR = 32000\nTEST = (len(list(Path(\"../input/birdclef-2021/test_soundscapes/\").glob(\"*.ogg\"))) != 0)\nif TEST:\n    DATADIR = Path(\"../input/birdclef-2021/test_soundscapes/\")\nelse:\n    DATADIR = Path(\"../input/birdclef-2021/train_soundscapes/\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:43.926653Z","iopub.execute_input":"2021-05-29T23:40:43.927105Z","iopub.status.idle":"2021-05-29T23:40:43.941452Z","shell.execute_reply.started":"2021-05-29T23:40:43.927069Z","shell.execute_reply":"2021-05-29T23:40:43.940789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_audios = list(DATADIR.glob(\"*.ogg\"))\nall_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]\nsubmission_df = pd.DataFrame({\n    \"row_id\": all_audio_ids\n})\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:43.942723Z","iopub.execute_input":"2021-05-29T23:40:43.943123Z","iopub.status.idle":"2021-05-29T23:40:43.969315Z","shell.execute_reply.started":"2021-05-29T23:40:43.943084Z","shell.execute_reply":"2021-05-29T23:40:43.968604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model","metadata":{}},{"cell_type":"code","source":"def gem_freq(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), 1)).pow(1.0 / p)\n\n\nclass GeMFreq(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super().__init__()\n        self.p = torch.nn.Parameter(torch.ones(1) * p)\n        self.eps = eps\n\n    def forward(self, x):\n        return gem_freq(x, p=self.p, eps=self.eps)\n\n\nclass NormalizeMelSpec(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, X):\n        mean = X.mean((1, 2), keepdim=True)\n        std = X.std((1, 2), keepdim=True)\n        Xstd = (X - mean) / (std + self.eps)\n        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n            (norm_max - norm_min)\n        )\n        V = torch.zeros_like(Xstd)\n        if fix_ind.sum():\n            V_fix = Xstd[fix_ind]\n            norm_max_fix = norm_max[fix_ind, None, None]\n            norm_min_fix = norm_min[fix_ind, None, None]\n            V_fix = torch.max(\n                torch.min(V_fix, norm_max_fix),\n                norm_min_fix,\n            )\n            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n            V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n            V[fix_ind] = V_fix\n        return V\n\n\nclass AttHead(nn.Module):\n    def __init__(\n        self, in_chans, p=0.5, num_class=397, train_period=15.0, infer_period=5.0\n    ):\n        super().__init__()\n        self.train_period = train_period\n        self.infer_period = infer_period\n        self.pooling = GeMFreq()\n\n        self.dense_layers = nn.Sequential(\n            nn.Dropout(p / 2),\n            nn.Linear(in_chans, 512),\n            nn.ReLU(),\n            nn.Dropout(p),\n        )\n        self.attention = nn.Conv1d(\n            in_channels=512,\n            out_channels=num_class,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n        self.fix_scale = nn.Conv1d(\n            in_channels=512,\n            out_channels=num_class,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n\n    def forward(self, feat):\n        feat = self.pooling(feat).squeeze(-2).permute(0, 2, 1)  # (bs, time, ch)\n\n        feat = self.dense_layers(feat).permute(0, 2, 1)  # (bs, 512, time)\n        time_att = torch.tanh(self.attention(feat))\n        assert self.train_period >= self.infer_period\n        if self.training or self.train_period == self.infer_period:\n\n            clipwise_pred = torch.sum(\n                torch.sigmoid(self.fix_scale(feat)) * torch.softmax(time_att, dim=-1),\n                dim=-1,\n            )  # sum((bs, 24, time), -1) -> (bs, 24)\n            logits = torch.sum(\n                self.fix_scale(feat) * torch.softmax(time_att, dim=-1),\n                dim=-1,\n            )\n        else:\n            framewise_pred_long = torch.sigmoid(self.fix_scale(feat))\n            clipwise_pred_long = torch.sum(framewise_pred_long * torch.softmax(time_att, dim=-1), dim=-1) \n            \n            feat_time = feat.size(-1)\n            start = (\n                feat_time / 2 - feat_time * (self.infer_period / self.train_period) / 2\n            )\n            end = start + feat_time * (self.infer_period / self.train_period)\n            start = int(start)\n            end = int(end)\n            feat = feat[:, :, start:end]\n            att = torch.softmax(time_att[:, :, start:end], dim=-1)\n#             print(feat_time, start, end)\n#             print(att_a.sum(), att.sum(), time_att.shape)\n            framewise_pred = torch.sigmoid(self.fix_scale(feat))\n            clipwise_pred = torch.sum(framewise_pred * att, dim=-1) \n            logits = torch.sum(\n                self.fix_scale(feat) * att,\n                dim=-1,\n            )\n            time_att = time_att[:, :, start:end]\n        return (\n            logits,\n            clipwise_pred,\n            self.fix_scale(feat).permute(0, 2, 1),\n            time_att.permute(0, 2, 1),\n            clipwise_pred_long,\n        )\n\n\nclass AttModel(nn.Module):\n    def __init__(\n        self,\n        backbone=\"resnet34\",\n        p=0.5,\n        n_mels=224,\n        num_class=397,\n        train_period=15.0,\n        infer_period=5.0,\n        in_chans=1,\n    ):\n        super().__init__()\n        self.n_mels = n_mels\n        self.logmelspec_extractor = nn.Sequential(\n            MelSpectrogram(\n                32000,\n                n_mels=n_mels,\n                f_min=20,\n                n_fft=2048,\n                hop_length=512,\n                normalized=True,\n            ),\n            AmplitudeToDB(top_db=80.0),\n            NormalizeMelSpec(),\n        )\n\n        self.backbone = timm.create_model(\n            backbone, features_only=True, pretrained=False, in_chans=in_chans\n        )\n        encoder_channels = self.backbone.feature_info.channels()\n        dense_input = encoder_channels[-1]\n        self.head = AttHead(\n            dense_input,\n            p=p,\n            num_class=num_class,\n            train_period=train_period,\n            infer_period=infer_period,\n        )\n\n    def forward(self, input):\n#         img = self.logmelspec_extractor(input)[\n#             :, None\n#         ]  # (batch_size, 1, mel_bins, time_steps)\n        feats = self.backbone(input)\n        return self.head(feats[-1])\n    \nclass Model(nn.Module):\n    def __init__(\n        self,\n        backbone=\"resnet34\",\n        p=0.5,\n        n_mels=224,\n        num_class=397,\n        train_period=15.0,\n        infer_period=5.0,\n        in_chans=1,\n    ):\n        super().__init__()\n        self.model = AttModel(backbone, p, n_mels, num_class, train_period, infer_period, in_chans)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:43.970507Z","iopub.execute_input":"2021-05-29T23:40:43.970846Z","iopub.status.idle":"2021-05-29T23:40:43.999103Z","shell.execute_reply.started":"2021-05-29T23:40:43.97082Z","shell.execute_reply":"2021-05-29T23:40:43.998248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class TestDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, clip: np.ndarray, train_period=30, \n                 waveform_transforms=None):\n        self.df = df\n        self.clip = np.concatenate([clip, clip, clip])\n        self.train_period = train_period\n        self.waveform_transforms=waveform_transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        row_id = sample.row_id\n\n        end_seconds = int(sample.seconds)\n        start_seconds = int(end_seconds - 5)\n        \n        end_index = int(SR * (end_seconds + (self.train_period - 5) / 2) + len(self.clip) // 3)\n        start_index = int(SR * (start_seconds - (self.train_period - 5) / 2) + len(self.clip) // 3)\n        \n        y = self.clip[start_index:end_index].astype(np.float32)\n\n        y = np.nan_to_num(y)\n\n        if self.waveform_transforms:\n            y = self.waveform_transforms(y)\n\n        y = np.nan_to_num(y)\n        \n        return y, row_id","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.000357Z","iopub.execute_input":"2021-05-29T23:40:44.00088Z","iopub.status.idle":"2021-05-29T23:40:44.011801Z","shell.execute_reply.started":"2021-05-29T23:40:44.000842Z","shell.execute_reply":"2021-05-29T23:40:44.011137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms(phase: str):\n    transforms = CFG.transforms\n    if transforms is None:\n        return None\n    else:\n        if transforms[phase] is None:\n            return None\n        trns_list = []\n        for trns_conf in transforms[phase]:\n            trns_name = trns_conf[\"name\"]\n            trns_params = {} if trns_conf.get(\"params\") is None else \\\n                trns_conf[\"params\"]\n            if globals().get(trns_name) is not None:\n                trns_cls = globals()[trns_name]\n                trns_list.append(trns_cls(**trns_params))\n\n        if len(trns_list) > 0:\n            return Compose(trns_list)\n        else:\n            return None\n\n\ndef get_waveform_transforms(config: dict, phase: str):\n    return get_transforms(config, phase)\n\n\ndef get_spectrogram_transforms(config: dict, phase: str):\n    transforms = config.get('spectrogram_transforms')\n    if transforms is None:\n        return None\n    else:\n        if transforms[phase] is None:\n            return None\n        trns_list = []\n        for trns_conf in transforms[phase]:\n            trns_name = trns_conf[\"name\"]\n            trns_params = {} if trns_conf.get(\"params\") is None else \\\n                trns_conf[\"params\"]\n            if hasattr(A, trns_name):\n                trns_cls = A.__getattribute__(trns_name)\n                trns_list.append(trns_cls(**trns_params))\n            else:\n                trns_cls = globals().get(trns_name)\n                if trns_cls is not None:\n                    trns_list.append(trns_cls(**trns_params))\n\n        if len(trns_list) > 0:\n            return A.Compose(trns_list, p=1.0)\n        else:\n            return None\n\n\nclass Normalize:\n    def __call__(self, y: np.ndarray):\n        max_vol = np.abs(y).max()\n        y_vol = y * 1 / max_vol\n        return np.asfortranarray(y_vol)\n\n\nclass NewNormalize:\n    def __call__(self, y: np.ndarray):\n        y_mm = y - y.mean()\n        return y_mm / y_mm.abs().max()\n\n\nclass Compose:\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, y: np.ndarray):\n        for trns in self.transforms:\n            y = trns(y)\n        return y\n\n\nclass AudioTransform:\n    def __init__(self, always_apply=False, p=0.5):\n        self.always_apply = always_apply\n        self.p = p\n\n    def __call__(self, y: np.ndarray):\n        if self.always_apply:\n            return self.apply(y)\n        else:\n            if np.random.rand() < self.p:\n                return self.apply(y)\n            else:\n                return y\n\n    def apply(self, y: np.ndarray):\n        raise NotImplementedError\n\n\nclass NoiseInjection(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n        super().__init__(always_apply, p)\n\n        self.noise_level = (0.0, max_noise_level)\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        noise_level = np.random.uniform(*self.noise_level)\n        noise = np.random.randn(len(y))\n        augmented = (y + noise * noise_level).astype(y.dtype)\n        return augmented\n\n\nclass GaussianNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))\n\n        white_noise = np.random.randn(len(y))\n        a_white = np.sqrt(white_noise ** 2).max()\n        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n        return augmented\n\n\nclass PinkNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal / (10 ** (snr / 20))\n\n        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n        a_pink = np.sqrt(pink_noise ** 2).max()\n        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n        return augmented\n\n\nclass PitchShift(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_range=5, sr=32000):\n        super().__init__(always_apply, p)\n        self.max_range = max_range\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        n_steps = np.random.randint(-self.max_range, self.max_range)\n        augmented = librosa.effects.pitch_shift(y, self.sr, n_steps)\n        return augmented\n\n\nclass TimeStretch(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n        super().__init__(always_apply, p)\n        self.max_rate = max_rate\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        rate = np.random.uniform(0, self.max_rate)\n        augmented = librosa.effects.time_stretch(y, rate)\n        return augmented\n\n\ndef _db2float(db: float, amplitude=True):\n    if amplitude:\n        return 10**(db / 20)\n    else:\n        return 10 ** (db / 10)\n\n\ndef volume_down(y: np.ndarray, db: float):\n    \"\"\"\n    Low level API for decreasing the volume\n    Parameters\n    ----------\n    y: numpy.ndarray\n        stereo / monaural input audio\n    db: float\n        how much decibel to decrease\n    Returns\n    -------\n    applied: numpy.ndarray\n        audio with decreased volume\n    \"\"\"\n    applied = y * _db2float(-db)\n    return applied\n\n\ndef volume_up(y: np.ndarray, db: float):\n    \"\"\"\n    Low level API for increasing the volume\n    Parameters\n    ----------\n    y: numpy.ndarray\n        stereo / monaural input audio\n    db: float\n        how much decibel to increase\n    Returns\n    -------\n    applied: numpy.ndarray\n        audio with increased volume\n    \"\"\"\n    applied = y * _db2float(db)\n    return applied\n\n\nclass RandomVolume(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, limit=10):\n        super().__init__(always_apply, p)\n        self.limit = limit\n\n    def apply(self, y: np.ndarray, **params):\n        db = np.random.uniform(-self.limit, self.limit)\n        if db >= 0:\n            return volume_up(y, db)\n        else:\n            return volume_down(y, db)\n\n\nclass OneOf:\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, y: np.ndarray):\n        n_trns = len(self.transforms)\n        trns_idx = np.random.choice(n_trns)\n        trns = self.transforms[trns_idx]\n        y = trns(y)\n        return y\n\n\nclass CosineVolume(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, limit=10):\n        super().__init__(always_apply, p)\n        self.limit = limit\n\n    def apply(self, y: np.ndarray, **params):\n        db = np.random.uniform(-self.limit, self.limit)\n        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n        dbs = _db2float(cosine * db)\n        return y * dbs\n\n\ndef drop_stripes(image: np.ndarray, dim: int, drop_width: int, stripes_num: int):\n    total_width = image.shape[dim]\n    lowest_value = image.min()\n    for _ in range(stripes_num):\n        distance = np.random.randint(low=0, high=drop_width, size=(1,))[0]\n        begin = np.random.randint(\n            low=0, high=total_width - distance, size=(1,))[0]\n\n        if dim == 0:\n            image[begin:begin + distance] = lowest_value\n        elif dim == 1:\n            image[:, begin + distance] = lowest_value\n        elif dim == 2:\n            image[:, :, begin + distance] = lowest_value\n    return image\n\n\nclass TimeFreqMasking(ImageOnlyTransform):\n    def __init__(self,\n                 time_drop_width: int,\n                 time_stripes_num: int,\n                 freq_drop_width: int,\n                 freq_stripes_num: int,\n                 always_apply=False,\n                 p=0.5):\n        super().__init__(always_apply, p)\n        self.time_drop_width = time_drop_width\n        self.time_stripes_num = time_stripes_num\n        self.freq_drop_width = freq_drop_width\n        self.freq_stripes_num = freq_stripes_num\n\n    def apply(self, img, **params):\n        img_ = img.copy()\n        if img.ndim == 2:\n            img_ = drop_stripes(\n                img_, dim=0, drop_width=self.freq_drop_width, stripes_num=self.freq_stripes_num)\n            img_ = drop_stripes(\n                img_, dim=1, drop_width=self.time_drop_width, stripes_num=self.time_stripes_num)\n        return img_","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.013294Z","iopub.execute_input":"2021-05-29T23:40:44.013838Z","iopub.status.idle":"2021-05-29T23:40:44.058945Z","shell.execute_reply.started":"2021-05-29T23:40:44.0138Z","shell.execute_reply":"2021-05-29T23:40:44.05793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get model","metadata":{}},{"cell_type":"code","source":"from torch.nn.modules.batchnorm import _BatchNorm\n\ndef prepare_model_for_inference(model, path: Path):\n    if not torch.cuda.is_available():\n        ckpt = torch.load(path, map_location=\"cpu\")\n    else:\n        ckpt = torch.load(path)\n    model.load_state_dict(ckpt[\"state_dict\"])\n    model.eval()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.060299Z","iopub.execute_input":"2021-05-29T23:40:44.060819Z","iopub.status.idle":"2021-05-29T23:40:44.066925Z","shell.execute_reply.started":"2021-05-29T23:40:44.060781Z","shell.execute_reply":"2021-05-29T23:40:44.065793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_for_clip(test_df: pd.DataFrame, \n                        clip: np.ndarray, \n                        models, \n                        threshold=0.05, \n                        threshold_long=None):\n\n    dataset = TestDataset(df=test_df, \n                          clip=clip,\n                          train_period = CFG.period, \n                          waveform_transforms=get_transforms(phase=\"test\"))\n    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n#     [model.eval() for model in models]\n    prediction_dict = {}\n    for image, row_id in tqdm(loader):\n        row_id = row_id[0]\n        image = image.to(device)\n\n        with torch.no_grad():\n            image = models[0].logmelspec_extractor(image)[:, None]\n            probas = []\n            probas_long = []\n            for model in models:\n                with torch.cuda.amp.autocast():\n                    _, clipwise_pred, _, _, clipwise_pred_long = model(image)\n                probas.append(clipwise_pred.detach().cpu().numpy().reshape(-1))\n                probas_long.append(clipwise_pred_long.detach().cpu().numpy().reshape(-1))\n            probas = np.array(probas)\n            probas_long = np.array(probas_long)\n#             probas = np.array([model(image)[1].detach().cpu().numpy().reshape(-1) for model in models])\n        if threshold_long is None:\n            events = probas.mean(0) >= threshold\n        else:\n            events = ((probas.mean(0) >= threshold).astype(int) \\\n                      + (probas_long.mean(0) >= threshold_long).astype(int)) >= 2\n        labels = np.argwhere(events).reshape(-1).tolist()\n#         labels = labels[:2]\n        if len(labels) == 0:\n            prediction_dict[row_id] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[row_id] = label_string\n    return prediction_dict","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.069823Z","iopub.execute_input":"2021-05-29T23:40:44.070118Z","iopub.status.idle":"2021-05-29T23:40:44.081758Z","shell.execute_reply.started":"2021-05-29T23:40:44.070085Z","shell.execute_reply":"2021-05-29T23:40:44.080824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(backbone_name, weight_path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = Model(\n        backbone_name,\n        p=0.5,\n        n_mels=CFG.n_mels,\n        num_class=CFG.num_classes,\n        train_period=CFG.period,\n        infer_period=5,\n    )\n    model = prepare_model_for_inference(model, weight_path).to(device)\n    model = model.model\n    return model\n\ndef prediction(test_audios,\n               models_cfg,\n               threshold=0.05, \n               threshold_long=None):\n    \n    models = [load_model(list(models_cfg.keys())[0], list(models_cfg.values())[0]) for models_cfg in models_cfg]\n    warnings.filterwarnings(\"ignore\")\n    prediction_dfs = []\n    for audio_path in test_audios:\n        with timer(f\"Loading {str(audio_path)}\", logger):\n            clip, _ = sf.read(audio_path)\n\n        seconds = []\n        row_ids = []\n        for second in range(5, 605, 5):\n            row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n            seconds.append(second)\n            row_ids.append(row_id)\n            \n        test_df = pd.DataFrame({\n            \"row_id\": row_ids,\n            \"seconds\": seconds\n        })\n        with timer(f\"Prediction on {audio_path}\", logger):\n            prediction_dict = prediction_for_clip(test_df,\n                                                  clip=clip,\n                                                  models=models,\n                                                  threshold=threshold, threshold_long=threshold_long)\n        row_id = list(prediction_dict.keys())\n        birds = list(prediction_dict.values())\n        prediction_df = pd.DataFrame({\n            \"row_id\": row_id,\n            \"birds\": birds\n        })\n        prediction_dfs.append(prediction_df)\n    \n    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n    return prediction_df","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.083212Z","iopub.execute_input":"2021-05-29T23:40:44.083918Z","iopub.status.idle":"2021-05-29T23:40:44.096574Z","shell.execute_reply.started":"2021-05-29T23:40:44.083881Z","shell.execute_reply":"2021-05-29T23:40:44.095793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"threshold = 0.025\nthreshold_long = 0.05","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.0981Z","iopub.execute_input":"2021-05-29T23:40:44.098578Z","iopub.status.idle":"2021-05-29T23:40:44.107259Z","shell.execute_reply.started":"2021-05-29T23:40:44.098541Z","shell.execute_reply":"2021-05-29T23:40:44.106496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # site_mask = pd.read_csv(\"../input/site-month-mask/site_mask.csv\")\n# # site_mask[\"all\"] = site_mask.drop('other', axis=1).T.sum()\n# # rare_threshold = 10\n# # common_threshold = 200\n# # print(\"common, rare: \", site_mask[\"all\"][site_mask[\"all\"].values>common_threshold].sum(), site_mask[\"all\"][site_mask[\"all\"].values<rare_threshold].sum())\n# # print(f\"common classes:\", (site_mask[\"all\"].values>common_threshold).sum(), \"rare classes: \", (site_mask[\"all\"].values<rare_threshold).sum())\n# # threshold = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold, np.ones_like(site_mask[\"all\"].values) * threshold * 4)\n# # threshold = np.where((site_mask[\"all\"].values>common_threshold), np.ones_like(site_mask[\"all\"].values) * threshold / 2, np.ones_like(site_mask[\"all\"].values) * threshold)\n\n# # threshold_long = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold_long, np.ones_like(site_mask[\"all\"].values) * threshold_long * 4)\n# # threshold_long = np.where((site_mask[\"all\"].values>common_threshold), np.ones_like(site_mask[\"all\"].values) * threshold_long / 2, np.ones_like(site_mask[\"all\"].values) * threshold_long)\n\n# site_mask = pd.read_csv(\"../input/site-month-mask/site_mask.csv\")\n# site_mask[\"all\"] = site_mask.drop('other', axis=1).T.sum()\n# rare_threshold = 10\n# print(\"common, rare: \", site_mask[\"all\"][site_mask[\"all\"].values>rare_threshold].sum(), site_mask[\"all\"][site_mask[\"all\"].values<rare_threshold].sum())\n# print(\"common classes: \", (site_mask[\"all\"].values>rare_threshold).sum())\n# threshold = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold, np.ones_like(site_mask[\"all\"].values) * threshold * 4)\n# threshold_long = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold_long, np.ones_like(site_mask[\"all\"].values) * threshold_long * 4)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.109068Z","iopub.execute_input":"2021-05-29T23:40:44.109564Z","iopub.status.idle":"2021-05-29T23:40:44.134451Z","shell.execute_reply.started":"2021-05-29T23:40:44.109459Z","shell.execute_reply":"2021-05-29T23:40:44.133608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# month_mask = pd.read_csv(\"../input/site-month-mask/month_mask.csv\")\n\n# month_mask[\"all\"] = month_mask.T.sum()\n# rare_threshold = 100\n# print(\"common, rare: \", month_mask[\"all\"][month_mask[\"all\"].values>rare_threshold].sum(), month_mask[\"all\"][month_mask[\"all\"].values<rare_threshold].sum())\n# print(\"common classes: \", (month_mask[\"all\"].values>rare_threshold).sum())\n# threshold = np.where((month_mask[\"all\"].values>rare_threshold), np.ones_like(month_mask[\"all\"].values) * threshold, np.ones_like(month_mask[\"all\"].values) * 0.1)\n# threshold_long = np.where((month_mask[\"all\"].values>rare_threshold), np.ones_like(month_mask[\"all\"].values) * threshold_long, np.ones_like(month_mask[\"all\"].values) * 0.2)\n# month_mask[\"all\"].plot()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.135512Z","iopub.execute_input":"2021-05-29T23:40:44.135831Z","iopub.status.idle":"2021-05-29T23:40:44.140719Z","shell.execute_reply.started":"2021-05-29T23:40:44.135799Z","shell.execute_reply":"2021-05-29T23:40:44.139238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = prediction(test_audios=all_audios,\n                        models_cfg=CFG.models_cfg,\n                        threshold=threshold, \n                        threshold_long=threshold_long)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:40:44.142311Z","iopub.execute_input":"2021-05-29T23:40:44.143254Z","iopub.status.idle":"2021-05-29T23:43:57.489953Z","shell.execute_reply.started":"2021-05-29T23:40:44.143212Z","shell.execute_reply":"2021-05-29T23:43:57.48776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# birds2id = {b : i for i, b in enumerate(CFG.target_columns)}\n# month_mask = pd.read_csv(\"../input/site-month-mask/month_mask.csv\")\n# site_mask = pd.read_csv(\"../input/site-month-mask/site_mask.csv\")\n# index = 0\n# remove_num = 0\n# for audio_path in all_audios:\n#     site = audio_path.name.split(\"_\")[1]\n#     month = audio_path.name.split(\"_\")[2][4:6]\n#     m = month_mask[month]\n#     for second in range(5, 605, 5):\n#         row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n#         birds = submission.iloc[index, 1].split(\" \")\n#         birds_removed = []\n#         for b in birds:\n#             if b != \"nocall\":\n#                 if site == \"COR\" or site == \"COL\":\n#                     if site_mask.loc[birds2id[b], \"COR\"] == 0 and site_mask.loc[birds2id[b], \"COL\"] == 0:\n#                         remove_num += 1\n#                     else:\n#                         if month_mask.loc[birds2id[b], month] == 0:\n#                             remove_num += 1\n#                         else:\n#                             birds_removed.append(b)\n#                 else:\n#                     if site_mask.loc[birds2id[b], site] == 0:\n#                         remove_num += 1\n#                     else:\n#                         if month_mask.loc[birds2id[b], month] == 0:\n#                             remove_num += 1\n#                         else:\n#                             birds_removed.append(b)\n#         birds_removed = list(set(birds_removed))\n#         if len(birds_removed) == 0:\n#             birds_removed.append(\"nocall\")\n#         submission.iloc[index, 1] = \" \".join(birds_removed)\n#         index += 1\n# print(remove_num)\n# # print(submission)\n# submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:43:57.491224Z","iopub.status.idle":"2021-05-29T23:43:57.49206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:43:57.493189Z","iopub.status.idle":"2021-05-29T23:43:57.493947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics(s_true, s_pred):\n    s_true = set(s_true.split())\n    s_pred = set(s_pred.split())\n    n, n_true, n_pred = len(s_true.intersection(s_pred)), len(s_true), len(s_pred)\n    \n    prec = n/n_pred\n    rec = n/n_true\n    f1 = 2*prec*rec/(prec + rec) if prec + rec else 0\n    \n    return {\"f1\": f1, \"prec\": prec, \"rec\": rec, \"n_true\": n_true, \"n_pred\": n_pred, \"n\": n}","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:43:57.495044Z","iopub.status.idle":"2021-05-29T23:43:57.495793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TARGET_PATH = None\nTEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/test_soundscapes\")\nSAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\nif not len(list(TEST_AUDIO_ROOT.glob(\"*.ogg\"))):\n    TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/train_soundscapes\")\n    SAMPLE_SUB_PATH = None\n    # SAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\n    TARGET_PATH = Path(\"../input/birdclef-2021/train_soundscape_labels.csv\")\n    \nif TARGET_PATH:\n    sub_target = pd.read_csv(TARGET_PATH)\n    sub_target = sub_target.merge(submission, how=\"left\", on=\"row_id\")\n    \n    print(sub_target[\"birds_x\"].notnull().sum(), sub_target[\"birds_x\"].notnull().sum())\n    assert sub_target[\"birds_x\"].notnull().all()\n    assert sub_target[\"birds_y\"].notnull().all()\n    \n    df_metrics = pd.DataFrame([get_metrics(s_true, s_pred) for s_true, s_pred in zip(sub_target.birds_x, sub_target.birds_y)])\n    \n    print(df_metrics.mean())","metadata":{"execution":{"iopub.status.busy":"2021-05-29T23:43:57.496939Z","iopub.status.idle":"2021-05-29T23:43:57.497683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}