{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello you all, I hope you are doing well ?\n\n**Context 1**:\nIn this competition we can have simultaneously several birds or no birds in an audio sample. So it is a multi-labels classification over 397 species.\n\n**Context 2**:\nAccording to scikit-learn:\nmicro:\nCalculate metrics globally by counting the total true positives,\nfalse negatives and false positives.\n\n**Context 3**:\nSo when I refer to: Submissions will be evaluated based on their row-wise micro averaged F1 score\n\n\nBasing on those contexts I propose: \nNB:\n*  Tell me what you think about this implementation (good, bad, other proposition)\n*  Upvote (if you want). \n    \n**Thanks**\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef row_wise_micro_averaged_f1_score(y_true, y_pred):\n    \"\"\"\n        Calculate f1-score globally by counting the total true positives,\n        false negatives and false positives.\n        \n        Parameters\n        ----------\n        y_true: numpy.ndarray\n                The ground truth binary labels with dimension n_samples * n_birds_species.\n        y_pred: numpy.ndarray\n                Our model's binary label predictions with dimension n_samples * n_birds_species.\n    \"\"\"\n    # False positive means it is False but the model says it is true\n    condition_fp = (np.logical_and(y_true==False , y_pred==True))\n    false_positive = np.where(condition_fp, True, False).sum()\n    # False negative means it is True but the model says it is False\n    condition_fn = (np.logical_and(y_true==True , y_pred==False))\n    false_negative = np.where(condition_fp, True, False).sum()\n    # True positive means it is True and the model says it is True\n    condition_tp = (np.logical_and(y_true==True , y_pred==True))\n    True_positive = np.where(condition_tp, True, False).sum()\n    # Compute the f1_score globally  f1 = tp / (tp + 0.5 * (fp + fn))\n    micro_averaged_f1_score = True_positive / (True_positive + 0.5*(false_positive+false_negative) +1e-15)\n    \n    return micro_averaged_f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}