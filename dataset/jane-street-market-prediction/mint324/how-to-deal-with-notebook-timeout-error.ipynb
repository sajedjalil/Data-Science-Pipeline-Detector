{"cells":[{"metadata":{},"cell_type":"markdown","source":"I'm biginner of kaggle competition.\n\nI made this basic Notebook and running it. \nAfter I ran my model, I could make my submission.csv which written 0 or 1. \nAfter I submitted my submission file, however the public score wasn't calculated. \nThis Error name is \"Notebook Timeout error\". \nBecause of my few nowledge, now I can't solve this Notebook Timeout error.\n\nI want to know what is wrong and how to fix it.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport janestreet\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss, accuracy_score\n\n#input data\npath = \"../input/jane-street-market-prediction/\"\ntrain = pd.read_csv(path + \"train.csv\",nrows=10000)\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns})\n\ncolumn = ['feature_0', 'feature_1', 'feature_2',\n       'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7',\n       'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12',\n       'feature_13', 'feature_14', 'feature_15', 'feature_16',\n       'feature_17', 'feature_18', 'feature_19', 'feature_20',\n       'feature_21', 'feature_22', 'feature_23', 'feature_24',\n       'feature_25', 'feature_26', 'feature_27', 'feature_28',\n       'feature_29', 'feature_30', 'feature_31', 'feature_32',\n       'feature_33', 'feature_34', 'feature_35', 'feature_36',\n       'feature_37', 'feature_38', 'feature_39', 'feature_40',\n       'feature_41', 'feature_42', 'feature_43', 'feature_44',\n       'feature_45', 'feature_46', 'feature_47', 'feature_48',\n       'feature_49', 'feature_50', 'feature_51', 'feature_52',\n       'feature_53', 'feature_54', 'feature_55', 'feature_56',\n       'feature_57', 'feature_58', 'feature_59', 'feature_60',\n       'feature_61', 'feature_62', 'feature_63', 'feature_64',\n       'feature_65', 'feature_66', 'feature_67', 'feature_68',\n       'feature_69', 'feature_70', 'feature_71', 'feature_72',\n       'feature_73', 'feature_74', 'feature_75', 'feature_76',\n       'feature_77', 'feature_78', 'feature_79', 'feature_80',\n       'feature_81', 'feature_82', 'feature_83', 'feature_84',\n       'feature_85', 'feature_86', 'feature_87', 'feature_88',\n       'feature_89', 'feature_90', 'feature_91', 'feature_92',\n       'feature_93', 'feature_94', 'feature_95', 'feature_96',\n       'feature_97', 'feature_98', 'feature_99', 'feature_100',\n       'feature_101', 'feature_102', 'feature_103', 'feature_104',\n       'feature_105', 'feature_106', 'feature_107', 'feature_108',\n       'feature_109', 'feature_110', 'feature_111', 'feature_112',\n       'feature_113', 'feature_114', 'feature_115', 'feature_116',\n       'feature_117', 'feature_118', 'feature_119', 'feature_120',\n       'feature_121', 'feature_122', 'feature_123', 'feature_124',\n       'feature_125', 'feature_126', 'feature_127', 'feature_128',\n       'feature_129']\n\n#train data\ntrain['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 )   ).astype('int')\ntrain.fillna(0,inplace=True)\ntrain.drop(columns = [\"resp_1\",\"resp_2\",\"resp_3\",\"resp_4\",\"resp\",'date','weight','ts_id'],inplace=True)\ntrain_y = train[[\"action\"]].copy()\ntrain_x = train.copy()\ntrain_x.drop(\"action\",axis = 1,inplace = True)\n\n#学習させる\ndef create_model(df):\n    model = Sequential()\n    model.add(BatchNormalization())\n    model.add(Dense(1024, activation='relu',input_shape=(df.shape[1],)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())    \n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer=\"adam\",\n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    return model\n\nscore_logloss = []\nbatch_size = 1024\nepochs = 250\n\n#learning\nscore,score_logloss = [],[]\nkf = KFold(n_splits=4,shuffle = True,random_state=71)\nfor tr_idx,va_idx in kf.split(train_x):\n    tr_x,va_x = train_x.iloc[tr_idx],train_x.iloc[va_idx]\n    tr_y,va_y = train_y.iloc[tr_idx],train_y.iloc[va_idx]\n    model = create_model(train_x)\n        \n    history = model.fit(tr_x.values,tr_y.values,batch_size = batch_size,epochs = epochs,\n                        verbose = 1,validation_data=(va_x,va_y))\n    \n    va_pred = model.predict(va_x)\n    score = log_loss(va_y,va_pred)\n    score_logloss.append(score)\n\n    \nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df.fillna(0,inplace = True)\n    X_test = test_df.loc[:, column].values\n    y_preds = model.predict(X_test)\n    sample_prediction_df.action = np.where(y_preds >= 0.5, 1, 0).astype(int)\n    env.predict(sample_prediction_df)\n\nprint(\"X_test\",X_test.shape,X_test)\n\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}