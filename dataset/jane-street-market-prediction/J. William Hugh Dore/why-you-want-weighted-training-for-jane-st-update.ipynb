{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Why You want Weighted Training for Jane Street\n**Intro**"},{"metadata":{},"cell_type":"markdown","source":"When the accuracy of the model is not the final objective but a solution to a real world problem is, weighted training is often a valuable tool."},{"metadata":{},"cell_type":"markdown","source":"In many notebooks i have seen the score correlated to Wegiht* Resp, and calculation of that then used as a Metric. \nBut i see few use it in the training process. Weighted training is extremly usefull in particular situations, where a misclassification can be particularrly damaging ( a high loss in this case ) or particularly beneficial ( high profit). \nSince Jane street fits into this application and it was often not used i set out to see it's  effectivness in this competition.\n\nI use the LGBMClassifier from lightgbm witch supports weight training.\nI have not applied any feature engineering at all nor parametrization of the lgmb as the score it achieves is not important. The increment in score achievable with this technic is the focus.\nIt gets shown to be a must have in the pipeline.\ni create a fertile ground for more work.\n\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"I am sure the people high up on the leader board are using this is some form,but it is often overlooked or obscure, and this is one of it's perfect applicationd !\nSee how many of you it benefit !  if you like it please Upvote ;) . \n\nI use and thank [@Luis Moneda](https://www.kaggle.com/lgmoneda)for some snabbits of code from ( maybe check it out !) [this notebook](https://www.kaggle.com/lgmoneda/jane-temporal-feature-selection-with-shap/comments) for the evaluation metrics,data split, graph (adapted) and submission code."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport math\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport janestreet\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 4)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# And for those who don't know what weighted training is ..\n\nIts theoretical way of working is fairly simply. To Each Label will be attributed a weight, and this weight will be used  as a way of sizing the different errors of misclassification while computing the loss function.\nIt such a way it can call the attention more onto what matters in a scenario. When the precision of the model is not the main objective but a real world problem is.\nSince we are trying to make money, the important $$ trades should matter more. Weighted training makes the models we create see what matters. \nBut how best to do that ?\n"},{"metadata":{},"cell_type":"markdown","source":"# Metrics\nThis is the formula witch i see going around. \n( DISCALIMER - i have not seen or checked if this reflects the actual scoring sistem created by the Competition creators. it is just the most widespread method) \npi= ∑j(weightij∗respij∗actionij)\n\nt= ∑pi/∑p**2i * √250|i|\n\n(i being the number of unique dates in test set ).\n\nutitily = min(max(t,0),6)∑pi.\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u\n\ndef jane_utility(data, action_column=\"action\"):\n    return utility_score_numba(data[\"date\"].values, \n                               data[\"weight\"].values, \n                               data[\"resp\"].values, \n                               data[action_column].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train and Test Split + keeping back some of the last days of data ( 100) labled -out of time\n- just as the competition data will be from an unseen future. "},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/train.csv\")\n\nTIME_SPLIT= 400\nin_time= train_data.loc[train_data['date']<TIME_SPLIT].copy()\nout_of_time= train_data.loc[train_data['date']>=TIME_SPLIT].copy()\n\ntrain, test = train_test_split(in_time, \n                           test_size=0.2, \n                           random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#clean up \nimport gc \ndel train_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Some minor settings\nTARGET='action'\nfeatures = [f'feature_{x}' for x in range(130)] \n\n#Setting my will do do only Finiancially positive trades\ntrain['action']= train['resp']>0\ntest['action']= test['resp']>0\nout_of_time['action']= out_of_time['resp']>0\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Workshop,different simple methods of calculating weights for the labels.\n\nworking with different distributions of weights, \nthis is the playground."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Different simple methods of calculating weights for the labels\n#Create a column for each different distribution\ntrain['none']= np.ones(train.shape[0],int)\ntrain['weight']=abs(train['weight'])\ntrain['mix']= abs(train['resp'])*train['weight']\ntrain['res']= abs(train['resp'])\n\n# \" And if that doesn't work, try the root or the log\"\n#- Andrew Ng.\ntrain['mix1']= abs(train['resp'])*(train['weight'].transform('sqrt'))\ntrain['mix2']= abs(train['resp'])*(train['weight'].transform('log').clip(lower=0.01, upper=None)*train['weight'].transform('sqrt'))\ntrain['mix3']= abs(train['resp'])*(train['weight'].transform('sqrt').transform('sqrt'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Baseline')\nfor weight_method in ['none','weight','mix','res','mix1','mix2','mix3']:\n    print('\\nTraining now with {0} as method for calculating training weights'.format(weight_method))\n    model = LGBMClassifier()\n    model.fit(train[features], train[TARGET],sample_weight= train[weight_method])\n\n\n    test[\"challenger\"] = model.predict_proba(test[features])[:, 1]\n    out_of_time[\"challenger\"] = model.predict_proba(out_of_time[features])[:, 1]\n\n    test[\"{}_action\".format(weight_method)] = model.predict(test[features])\n    out_of_time[\"{}_action\".format(weight_method)] = model.predict(out_of_time[features])\n\n    print(\"Test AUC (in time): {:.6f}\".format(roc_auc_score(test[TARGET], test[\"challenger\"])))\n    print(\"Out of time AUC: {:.6f}\".format(roc_auc_score(out_of_time[TARGET], out_of_time[\"challenger\"])))\n    print(\"Test Jane Utility (in time): {:.2f}\".format(jane_utility(test, \"{}_action\".format(weight_method))))\n    print(\"Out of time Jane Utility: {:.2f}\".format(jane_utility(out_of_time, \"{}_action\".format(weight_method))))\n    print(\"-----------\")\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What this shows**:\nto start off the main advantage of weighted training is showing the main gool (utility score) is improving when the accuracy is not. ( even doing slightly worse) \n\nIf we pay more attention to the Big Trades, weighted by - Weight we miss out on a lot of opportunities. Score pretty low in comparison to the bench mark. It is not a reasonable practice. \n\nIf we give more attention to the highly $$ profitable trades, weighted by Weight*resp , \non the witheld Test data we make significantly more Dough ! \nIt seems not to generalize well into the more distant future - days we haven't seen (out_of[training]_time ) \n\nAttention to being profitable with Resp seems to be a good trade off !! All round increase. \n\nBut things really get exicting with Mix3 !\n\na 20% increase on the future for free just there\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"none_action\"])).rolling(60).mean().plot(label=\"No weights (benchmark)\", color=\"purple\")\n# pd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"weight_action\"])).rolling(60).mean().plot(label=\"weight\")\n# pd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"mix_action\"])).rolling(60).mean().plot(label=\"Weight*resp \", color=\"green\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"res_action\"])).rolling(60).mean().plot(label=\"resp\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"mix3_action\"])).rolling(60).mean().plot(label=\"mix3\")\nxmin, xmax, ymin, ymax = plt.axis()\nplt.vlines(TIME_SPLIT, ymin, ymax, linestyle=\"dotted\", color=\"red\", label=\"Out of time split\")\nplt.legend(bbox_to_anchor=(1.05, 1.0))\nplt.title(\"Performance moving average of 60 periods window for both test and out of time periods\", pad=16)\nplt.ylabel(\"sum(Weight * Resp * Action)\")\nplt.xlabel(\"Date\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the graphs shows some days it pays to be more focused on on thing a some days on another. Trends that some Moveing Average and some feature engineering may undercover and help take advantage out of.\n\nHere simply focusing the attention on certain trades with a more noticeable resp made for more constant and higher gains than the model trained normaly. \n\nAs you can see from the graph it seems a bit more constant & stable. **Slow and steady wins the race**?  \nNot so sure, sometimes going big helps ! \nlook at Mix3 ! up 20+% !\n- to be continued and developed\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#clean up before restarting\nimport gc \ndel out_of_time,in_time ,test,train,model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** \n# On the Lb \nThe best mothod for weight training appears to be the Resp, ( +170 above the baseline for this minimalistic lgbm) \nSo the validation set i chose (days after 400) may have been overfitted with Mix3. \n\nI set out to do some cross validation and find a more realistic( akin to the Lb) validation set.\nAny knowledge about how many days that contains ?  "},{"metadata":{},"cell_type":"markdown","source":"Any tips,ideas, upvotes and comments are greatly appreciated \n"},{"metadata":{},"cell_type":"markdown","source":"# **UPDATE**\n\nI aimed to make my train and validation set more reflect the Competition Data distribution beetween Training data and the private leader board Test set.\nGiving the High score on the board and the lentgh of time it took to score i thought we were only given a small part of the data and we were being tested many days into the future. \n( this was supported by what i read [here](https://www.kaggle.com/c/jane-street-market-prediction/discussion/200495) about the time period. \n\nSo below i re-scored all the algorithms to reflect what this might change. The validation split is now done at day 100( the realationship beetween time seen at time tested over is key). Here we can see how poorly some weighted trainging made us do in the future. ( probably due to some overfitting of outsiders) \nThis new division shows us a more realistic view into the benefits of wighted training in the competition and also reflects why using just resp scores better than other formulas on the LeaderBoard.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/train.csv\")\n\nTIME_SPLIT= 100\nin_time= train_data.loc[train_data['date']<TIME_SPLIT].copy()\nout_of_time= train_data.loc[train_data['date']>=TIME_SPLIT].copy()\n\ntrain, test = train_test_split(in_time, \n                           test_size=0.2, \n                           random_state=42)\n\n#Some minor settings\nTARGET='action'\nfeatures = [f'feature_{x}' for x in range(130)] \n\n#Setting my will do do only Finiancially positive trades\ntrain['action']= train['resp']>0\ntest['action']= test['resp']>0\nout_of_time['action']= out_of_time['resp']>0\n\n#Different simple methods of calculating weights for the labels\n#Create a column for each different distribution\ntrain['none']= np.ones(train.shape[0],int)\ntrain['weight']=abs(train['weight'])\ntrain['mix']= abs(train['resp'])*train['weight']\ntrain['res']= abs(train['resp'])\n\n# \" And if that doesn't work, try the root or the log\"\n#- Andrew Ng.\ntrain['mix1']= abs(train['resp'])*(train['weight'].transform('sqrt'))\ntrain['mix2']= abs(train['resp'])*(train['weight'].transform('log').clip(lower=0.01, upper=None)*train['weight'].transform('sqrt'))\ntrain['mix3']= abs(train['resp'])*(train['weight'].transform('sqrt').transform('sqrt'))\n\n\nprint('Baseline')\nfor weight_method in ['none','weight','mix','res','mix1','mix2','mix3']:\n    print('\\nTraining now with {0} as method for calculating training weights'.format(weight_method))\n    model = LGBMClassifier()\n    model.fit(train[features], train[TARGET],sample_weight= train[weight_method])\n\n\n    test[\"challenger\"] = model.predict_proba(test[features])[:, 1]\n    out_of_time[\"challenger\"] = model.predict_proba(out_of_time[features])[:, 1]\n\n    test[\"{}_action\".format(weight_method)] = model.predict(test[features])\n    out_of_time[\"{}_action\".format(weight_method)] = model.predict(out_of_time[features])\n\n    print(\"Test AUC (in time): {:.6f}\".format(roc_auc_score(test[TARGET], test[\"challenger\"])))\n    print(\"Out of time AUC: {:.6f}\".format(roc_auc_score(out_of_time[TARGET], out_of_time[\"challenger\"])))\n    print(\"Test Jane Utility (in time): {:.2f}\".format(jane_utility(test, \"{}_action\".format(weight_method))))\n    print(\"Out of time Jane Utility: {:.2f}\".format(jane_utility(out_of_time, \"{}_action\".format(weight_method))))\n    print(\"-----------\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, Mix 3 and res were still good but here, with this realistic distribution of ( seen and not seen time )\nMix1 emerged as a Winner. i am happy to say this validation method was backed up by the Lb.\nand the simple lgbm now scores with weight training with Mix1 4294. Vs the baseline of 3807-\na 13% percent increase in score ont the baseline. \nSince in this book we had a + 40 % increase above the baseline, while going in right direction with the more unfair split in train and test data i still do not think i am close to having a really functional valdiation set. \n\nMathematically i am not supprised Mix 1 won. It was a good runner all along and mathematically it also makes sense, the realtionship between resp and weight seemed to be inservly proportinal in an exponential way as the grafic below shows.\nSince resp x Weight is in the evaulation metric it made sense to have both in the weight distribution, eliminating the esponential likeness int the relationship avoids in the training giving a disproportionate weight to a few points. Flatening it out in training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#mix\nimport seaborn as sns\ntrain_data['t_weight']=abs(train_data['resp'])*train_data['weight']\nsns.scatterplot(x=train_data['resp'],y=train_data['weight'],hue=train_data['t_weight'])\nplt.title(\"Distribution of training weights calc. using : weight* resp  (aka Mix) \", pad=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Mix1\nimport seaborn as sns\ntrain_data['t_weight']=abs(train_data['resp'])*train_data['weight'].transform('sqrt')\nsns.scatterplot(x=train_data['resp'],y=train_data['weight'],hue=train_data['t_weight'])\nplt.title(\"Distribution of training weights calc. using : sqrt(weight)* resp  ( aka Mix1)\", pad=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the important area at a 45 degree angle is much better distributed than above ! \nthe difference is in the formula beetween the winning mix1 below and resp x Weight\nbelow the how well the  models faired in the time line, when trained with the new train and validation set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"none_action\"])).rolling(60).mean().plot(label=\"No weights (benchmark)\", color=\"purple\")\n# pd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"weight_action\"])).rolling(60).mean().plot(label=\"weight\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"mix3_action\"])).rolling(60).mean().plot(label=\"mix3 \", color=\"green\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"res_action\"])).rolling(60).mean().plot(label=\"resp\")\npd.concat([test, out_of_time]).groupby(\"date\").apply(lambda x: np.sum(x[\"resp\"] * x[\"weight\"] * x[\"mix1_action\"])).rolling(60).mean().plot(label=\"mix1\")\nxmin, xmax, ymin, ymax = plt.axis()\nplt.vlines(TIME_SPLIT, ymin, ymax, linestyle=\"dotted\", color=\"red\", label=\"Out of time split\")\nplt.legend(bbox_to_anchor=(1.05, 1.0))\nplt.title(\"Performance moving average of 60 periods window for both test and out of time periods\", pad=16)\nplt.ylabel(\"sum(Weight * Resp * Action)\")\nplt.xlabel(\"Date\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very different from above right ? that is beacuase of the time lapse, models traidneed in the past rarely reflect with accuracy- or have problems in doing so- the current state. This downward trend is Why Luis Moneda did his goodnotebook i linked. - and why normally you would want to use an up to date model for stock trading.\nNote when things go badly as in figure - day 450 ish it pays to have just paid attention to resp.\ntoo bad we have no real time resp information in the competition."},{"metadata":{},"cell_type":"markdown","source":"# So in practice, import the data  + 4 lines of code \nand submit and score + 4000"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#clean up before restarting\nimport gc \ndel out_of_time,in_time,test,train,model,train_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Download all the data\ndata = pd.read_csv(\"/kaggle/input/jane-street-market-prediction/train.csv\")\n\n#-5 lines of code\n#create Weights\ndata['mix1']= abs(data['resp'])*data['weight'].transform('sqrt')\n#set Features, target and classifications\ndata['TARGET']= data['resp']>0\nfeatures = [f'feature_{x}' for x in range(130)] \n#train Model\nmodel = LGBMClassifier()\nmodel.fit(data[features], data['TARGET'],sample_weight= data['mix1'])\n\n\n#submission\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df[\"action\"] = model.predict(test_df[features]).astype(int)\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take away: \ni hope i have convinced you of the use for Weighted Training, and as indicated above\n( abs(resp) x sqrt(Weight) ) seems the way to go as a weight distribution. (aka Mix1)\n\nEnjoy your day !"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}