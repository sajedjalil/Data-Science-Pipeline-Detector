{"cells":[{"metadata":{},"cell_type":"markdown","source":"There is one model parameter which is common to almost all models in this competition which no one seems to really be talking about.  For example given this Neural Network model (thanks to Yirun Zhang: https://www.kaggle.com/gogo827jz/jane-street-neural-network-starter):"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport dask.dataframe as dd\nimport matplotlib.pyplot as plt                                                                      \n%matplotlib inline   \n\ntf.random.set_seed(42)\n\ntrain = dd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv').compute().sort_values('ts_id')\nfeatures = [c for c in train.columns if \"feature\" in c]\n\n\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\n\nbatch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [\n    0.10143786981358652,\n    0.19720339053599725,\n    0.2703017847244654,\n    0.23148340929571917,\n    0.2357768967777311,\n]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nnum_models = 2\n\nmodels = []\nfor i in range(num_models):\n    clf = create_mlp(\n        len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n    clf.load_weights(f\"../input/js-nn-models/JSModel_{i}.hdf5\")\n    models.append(clf)\n\nf_mean = np.load('../input/js-nn-models/f_mean.npy')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get different scores by changing `opt_th` to different values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_th = 0.5025","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This parameter controls the ratio of ones and zeros in the prediction.  If the market is going up, then you will want to predict more ones than zeros.  \n\nHowever it's a little more complicated than this as you need to take the weights into consideration and not just the direction of the whole market:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['trend'] = train['resp'].cumsum()                                                              \ntrain['weighted_trend'] = (train['weight']*train['resp']).cumsum()                                   \ntrain.plot(x='ts_id', y=['trend', 'weighted_trend'])                                                 \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the training data the overall market is going up, but the weighted returns are trending downwards so you would want to predict fewer ones for this time period."},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\n\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\n\nfor test_df, pred_df in env_iter:\n    if test_df[\"weight\"].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = 0.0\n        for clf in models:\n            pred += clf(x_tt, training=False).numpy().item() / num_models\n        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Values I've tested and their scores are:\n* 0.49  6704                                                                                          \n* 0.496 6871                                                                                          \n* 0.498 6782                                                                                          \n* 0.5   6876                                                                                          \n* 0.505 6781                                                                                          \n* 0.503 6920                                                                                          \n* 0.508 6370  \n\nObviously `0.053` does well on the public test set but there is no guarantee it will do well on the private test set."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}