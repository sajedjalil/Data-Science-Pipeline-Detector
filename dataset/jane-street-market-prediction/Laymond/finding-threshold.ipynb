{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Description"},{"metadata":{},"cell_type":"markdown","source":"### Upgrade \"Finding Best Threshold\" from this code https://www.kaggle.com/a763337092/searching-best-threshold    "},{"metadata":{},"cell_type":"markdown","source":"* **You can find best threshold and ensemble ratio.**  \n* **FYI I only used tensorflow for faster inference time.**  \n* **Additionally TensorflowLite.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport time\nimport pickle\nimport random\nfrom random import choices\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import namedtuple\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\n\nimport warnings\nwarnings.filterwarnings (\"ignore\")\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nDATA_PATH = '../input/jane-street-market-prediction/'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n        \ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_list = [False, False] # Model1, Model2\nscale_feature = True\n\ndef seed_everything(seed=1111):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(seed=1111)\n\ntrain = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntrain = train.query('date > 85').reset_index(drop = True) \n\nfeat_cols = [f'feature_{i}' for i in range(130)]\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain['action'] = (train['resp'] > 0).astype('int')\ntrain['action_1'] = (train['resp_1'] > 0).astype('int')\ntrain['action_2'] = (train['resp_2'] > 0).astype('int')\ntrain['action_3'] = (train['resp_3'] > 0).astype('int')\ntrain['action_4'] = (train['resp_4'] > 0).astype('int')\n\ntrain[feat_cols[1:]] = train[feat_cols[1:]].fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(train, scale=True):\n    '''\n        Standardize features & manipulate\n    '''\n    if scale:\n        scaler = StandardScaler()\n        f_41 = scaler.fit_transform(train['feature_41'].values.reshape(-1,1))\n        f_42 = scaler.fit_transform(train['feature_42'].values.reshape(-1,1))\n        f_43 = scaler.fit_transform(train['feature_43'].values.reshape(-1,1))\n        cross_41_42_43 = f_41.reshape(-1) + f_42.reshape(-1) + f_43.reshape(-1)\n\n        f_1 = scaler.fit_transform(train['feature_1'].values.reshape(-1,1))\n        f_2 = scaler.fit_transform(train['feature_2'].values.reshape(-1,1))\n        cross_1_2 = f_1.reshape(-1) / ( f_2.reshape(-1) / 1e-5 )\n    else:\n        cross_41_42_43 = train['feature_41'].values + train['feature_42'].values + train['feature_43'].values\n        cross_1_2 = train['feature_1'].values / ( train['feature_2'].values / 1e-5 )\n        \n    return cross_41_42_43, cross_1_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_cols_130 = [f'feature_{i}' for i in range(130)]\nfeat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n# Feature Engineering : Standardize\ntrain['cross_41_42_43'], train['cross_1_2'] = feature_engineering(train, scale=scale_feature)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert Tensorflow-Lite model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LiteModel:\n    \n    @classmethod\n    def from_file(cls, model_path):\n        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n    \n    @classmethod\n    def from_keras_model(cls, kmodel):\n        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n        tflite_model = converter.convert()\n        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n    \n    def __init__(self, interpreter):\n        self.interpreter = interpreter\n        self.interpreter.allocate_tensors()\n        input_det = self.interpreter.get_input_details()[0]\n        output_det = self.interpreter.get_output_details()[0]\n        self.input_index = input_det[\"index\"]\n        self.output_index = output_det[\"index\"]\n        self.input_shape = input_det[\"shape\"]\n        self.output_shape = output_det[\"shape\"]\n        self.input_dtype = input_det[\"dtype\"]\n        self.output_dtype = output_det[\"dtype\"]\n        \n    def predict(self, inp):\n        inp = inp.astype(self.input_dtype)\n        count = inp.shape[0]\n        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n        for i in range(count):\n            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n            self.interpreter.invoke()\n            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n        return out\n    \n    def predict_single(self, inp):\n        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n        inp = np.array([inp], dtype=self.input_dtype)\n        self.interpreter.set_tensor(self.input_index, inp)\n        self.interpreter.invoke()\n        out = self.interpreter.get_tensor(self.output_index)\n        return out[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1 : ResNet Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(keras.Model):\n    def __init__(self, drop_prob, hidden_units, num_classes=5):\n        super(Model, self).__init__()\n        self.bnorm0 = BatchNormalization()\n        self.dropout0 = Dropout(drop_prob)\n\n        self.bnorm1 = BatchNormalization()\n        self.dropout1 = Dropout(drop_prob)\n        self.dense1 = Dense(hidden_units)\n        \n        self.bnorm2 = BatchNormalization()\n        self.dropout2 = Dropout(drop_prob)\n        self.dense2 = Dense(hidden_units)\n        \n        self.bnorm3 = BatchNormalization()\n        self.dropout3 = Dropout(drop_prob)\n        self.dense3 = Dense(hidden_units)\n        \n        self.bnorm4 = BatchNormalization()\n        self.dropout4 = Dropout(drop_prob)\n        self.dense4 = Dense(hidden_units)\n        \n        self.dense5 = Dense(num_classes)\n        self.LeakyReLU = tf.keras.layers.LeakyReLU(alpha=0.01)\n        \n    def call(self, x, training=False):\n        x = self.bnorm0(x, training=training)\n        x = self.dropout0(x, training=training)\n        # Dense 1    \n        x1 = self.dense1(x)\n        x1 = self.bnorm1(x1, training=training)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1, training=training)\n        x = tf.keras.layers.concatenate([x, x1], axis=-1)\n        # Dense 2\n        x2 = self.dense2(x)\n        x2 = self.bnorm2(x2, training=training)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2, training=training)\n        x = tf.keras.layers.concatenate([x1, x2], axis=-1)\n        # Dense 3\n        x3 = self.dense3(x)\n        x3 = self.bnorm3(x3, training=training)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3, training=training)\n        x = tf.keras.layers.concatenate([x2, x3], axis=-1)\n        # Dense 4\n        x4 = self.dense4(x)\n        x4 = self.bnorm4(x4, training=training)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4, training=training)\n        x = tf.keras.layers.concatenate([x3, x4], axis=-1)\n        # Dense 5\n        x = self.dense5(x)\n        x = tf.keras.activations.sigmoid(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_smoothing = 5e-3\nlearning_rate = 1e-3\ndrop_prob = 0.2\nhidden_units = 256\nnum_classes = 5\nNFOLDS = 5\n\nsample_input = np.random.randn(1, 132)\n\nmodel_list1 = []\nfor _fold in range(NFOLDS):\n    tf.keras.backend.clear_session()\n\n    model1 = Model(drop_prob=drop_prob, hidden_units=hidden_units, num_classes=num_classes)\n    model1.load_weights(f'../input/groupkmodel/model_1/JSModel_{_fold}.tf')\n    model1.predict(sample_input.astype(np.float32)) # To assert input_shape\n    tflite_model1 = LiteModel.from_keras_model(model1)\n    model_list1.append(tflite_model1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_list1[4].predict(sample_input.astype(np.float32)))\nprint(model1.predict(sample_input.astype(np.float32)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 : Simple MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=[\n            tf.keras.metrics.AUC(name=\"AUC\"),\n            tf.keras.metrics.Precision(name=\"Precision\"),\n        ]\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_smoothing = 5e-3\nlearning_rate = 1e-3\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\ntf.keras.backend.clear_session()\n\nsample_input = np.random.randn(1, 130)\n\nmodel_list2 = []\nfor _fold in range(NFOLDS):\n    tf.keras.backend.clear_session()\n\n    model2 = create_mlp(len(feat_cols_130), 5, hidden_units, dropout_rates, label_smoothing, learning_rate)\n    model2.load_weights(f'../input/groupkmodel/model_2/JSModel_{_fold}.tf')\n    model2.predict(sample_input.astype(np.float32)) # To assert input_shape\n    tflite_model2 = LiteModel.from_keras_model(model2)\n    model_list2.append(tflite_model2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_list2[4].predict(sample_input.astype(np.float32)))\nprint(model2.predict(sample_input.astype(np.float32)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Valid Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid = train.loc[(train.date >= 450) & (train.date < 500)].reset_index(drop=True)\n# del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_pred1 = np.zeros((len(valid), len(target_cols)))\nfor model in model_list1:\n    valid_pred1 += model.predict(valid[feat_cols].values) / len(model_list1)\n    \nvalid_pred1 = np.median(valid_pred1, axis=1)\n\nvalid_pred2 = np.zeros((len(valid), len(target_cols)))\nfor model in model_list2:\n    valid_pred2 += model.predict(valid[feat_cols_130].values) / len(model_list2)\n    \nvalid_pred2 = np.median(valid_pred2, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in range(40, 60):\n    best_threshold, best_u_score = 0.5, 0\n    alpha = float(a) / 100\n    print(f'alpha : {alpha} 1-alpha: {1-alpha}')\n    for i in range(4500, 5500):\n        thres = float(i) / 10000\n\n        valid_pred = valid_pred1*alpha + valid_pred2*(1-alpha)\n        slice_valid_pred = valid_pred.copy()\n        slice_valid_pred = np.where(slice_valid_pred >= thres, 1, 0).astype(int)\n        valid_u_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,\n                                               resp=valid.resp.values, action=slice_valid_pred)\n#         print(f'thresold={thres:.4f}, valid_u_score={valid_u_score:.4f}')\n\n        if valid_u_score >= best_u_score:\n            best_u_score = valid_u_score\n            best_threshold = thres\n    print(f'Best thresold={best_threshold:.4f}, best valid u score={best_u_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in range(60, 70):\n    best_threshold, best_u_score = 0.5, 0\n    alpha = float(a) / 100\n    print(f'alpha : {alpha} 1-alpha: {1-alpha}')\n    for i in range(4500, 5500):\n        thres = float(i) / 10000\n\n        valid_pred = valid_pred1*alpha + valid_pred2*(1-alpha)\n        slice_valid_pred = valid_pred.copy()\n        slice_valid_pred = np.where(slice_valid_pred >= thres, 1, 0).astype(int)\n        valid_u_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,\n                                               resp=valid.resp.values, action=slice_valid_pred)\n#         print(f'thresold={thres:.4f}, valid_u_score={valid_u_score:.4f}')\n\n        if valid_u_score >= best_u_score:\n            best_u_score = valid_u_score\n            best_threshold = thres\n    print(f'Best thresold={best_threshold:.4f}, best valid u score={best_u_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in range(70, 90):\n    best_threshold, best_u_score = 0.5, 0\n    alpha = float(a) / 100\n    print(f'alpha : {alpha} 1-alpha: {1-alpha}')\n    for i in range(4500, 5500):\n        thres = float(i) / 10000\n\n        valid_pred = valid_pred1*alpha + valid_pred2*(1-alpha)\n        slice_valid_pred = valid_pred.copy()\n        slice_valid_pred = np.where(slice_valid_pred >= thres, 1, 0).astype(int)\n        valid_u_score = utility_score_bincount(date=valid.date.values, weight=valid.weight.values,\n                                               resp=valid.resp.values, action=slice_valid_pred)\n#         print(f'thresold={thres:.4f}, valid_u_score={valid_u_score:.4f}')\n\n        if valid_u_score >= best_u_score:\n            best_u_score = valid_u_score\n            best_threshold = thres\n    print(f'Best thresold={best_threshold:.4f}, best valid u score={best_u_score:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best Setting\n# alpha : 0.81 1-alpha: 0.18999999999999995\n# Best thresold=0.4893, best valid u score=3291.1995","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}