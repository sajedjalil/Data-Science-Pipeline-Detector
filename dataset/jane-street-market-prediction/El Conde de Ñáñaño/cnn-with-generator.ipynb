{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CNN1D With Generator\nIn this notebook I will create a basic 1D convolutional network and train it on a generator.  1D convolutional networks can be a useful tool if \nthe present observation's outcome is in some way dependant on the previous datapoints.  Note that cnns are not limited to data in this format, as seen by @baosenguo's genius [2nd place solution][1] in the MOA competition.  \n\nThe primary takeaway from this kernel is the generator, in my opinion.  Creating a cnn is relatively simple in whatever framework you are using.  Getting the data into the appropriate format is much more difficult.  Samples for a 1D cnn are not just 1 ts_id with its features (1,130).  A sample for a 1D cnn is the current ts_id with the n previous ts_ids (1+n,130).  This notebook's 1Dcnn looks at the previous 2 ts_ids, so its samples are of size (1+2,130).  How do we reorder the data to be this dimension?  \n\n## Data Preparation\nA simple way to prepare the data for the model is to divide the data into chunks of size 4.  As the train has 2.4 million observations, this will create 800k\nsamples of size (3,130).  The advantage of this approach is the simplicity of implementation.  The disadvantages is the reduction of training samples.  Looking at the 2 previous observations reduces the sample size by a factor of 3.  Looking at the previous 1000 observations reduces the training samples by a factor of around 1000!  \n\nAn alternative is to duplicate the data such that each individual observation has its 2 preceeding observations.  This yields 2.4 million samples of size (3,130), effectively quadrupling the memory footprint of the data.  If you have the memory, go for it!  Otherwise, this might not be feasable.\n\nOne last alternative is to create a generator to create samples on the fly for training.  A generator cycles through each observation during runtime, then creates a sample by combining it with the previous 2 observations to form a sample of size (3,130).  The advantage of a generator is that you can use all data points in the train to train you model while not increasing the memory footprint in doing so.   The disadvantage is that samples are generated during training, leading to somewhat slower training time.  For this competition, the drawback is relatively small; all 5 folds were trained in under 1 hour.  To me, the biggest disadvantage of generators is coding them.  Perhaps I am a n00b, but coding a generator can be rather difficult.  If you wish to use a generator, it is my sincere hope that this code can help you get started.  If you want a more in depth at creating generators, I strongly recommend this [Stanford blog post][3].  I used that post to create the generator for this notebook.\n\n\n[1]: https://www.kaggle.com/c/lish-moa/discussion/202256\n[3]: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport janestreet\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nSUBMIT = True\nPATH='../input/cnn1d-pretrain4/model_4_finetune.hdf5'\nif SUBMIT== True and PATH == '':\n    1/0\nSAVED_MODEL_DIR = None #only for submissions.  They use trained models.  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters for the Model\n\nBelow are the hyperparameters for the model.  They are \n1. ***NUM_FOLDS***: the number of cross val folds for the model\n1. **window_size**: the number of observations in a sample. window_size = current_observation + num_previous_observations  \n1. **num_features**: the number of features your model will use in the data.  \n1. **filters**: number of neurons in hidden layers of 1Dcnn.\n1. **bottleneck_layer**: Dense layer preceeding convolutions.  <=0 if no bottleneck used, >0 for number of neurons in Dense layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"HYPERPARAMS = dict( NUM_FOLDS = 5,\n                    window_size = 3,\n                    num_features = 130,\n                    filters = [64,64,64],\n                    bottleneck_layer= 64, #put <=0 for no bottleneck layer\n                    dropout_rate = .1,\n                    kernel_size = 3,\n                    batch_size = 4096,\n                    learning_rate = .001,\n                    label_smoothing = 1e-2,\n                    pretraining = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def nontimeseries_split(df,val_len = 720000, NUM_FOLDS = HYPERPARAMS['NUM_FOLDS']):\n    tot_len = df.shape[0]\n    \n    val_starts = [int((tot_len - val_len)/(NUM_FOLDS -1)*i) for i in range(NUM_FOLDS)]\n    folds = []#list of (train_idx, val_idx)\n    for val_start in val_starts:\n        dates_val =  df.loc[val_start:(val_start + val_len -1), 'date'].unique()\n        dates_train= list(set(df.date.unique()) - set(dates_val) )\n        \n        val_idx = df[df.date.isin(dates_val)].index\n        train_idx = df[df.date.isin(dates_train)].index\n        \n        folds.append((train_idx, val_idx))\n    return folds\n\n\n#From Yirun Zhang: https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function\nfrom numba import njit\n@njit\ndef fillna_npwhere_njit(array, values= 0):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array\n\ndef Feature_Engineering(X, inference = False):\n    X = X.loc[:, X.columns.str.contains('feature')]\n    X = fillna_npwhere_njit(X.values, 0)\n    return X\n\n#Loading the data\ntrain = pd.read_parquet('../input/parquet/train.parquet')\n\nFEATURES = [feat for feat in train.columns if 'feature' in feat]\nf_mean = train[FEATURES].mean()\ntrain[FEATURES] = train[FEATURES].fillna(f_mean)\ntrain['action'] = (train['resp'] > 0).astype('int')\nf_mean = train[FEATURES].mean().values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The 1D Convolutional Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################################################################\n#Shape: (window_size, num_features)\n#dropout_rate: (float) the dropout rate for network.\n#filters: (list of floats) the filter sizes for the conv blocks. This \n    #is effectively the size of the network\n#kernel_size: (int) the size of the kernel\n######################################################################\ndef makeConv(shape = (HYPERPARAMS['window_size'], HYPERPARAMS['num_features']), \n             filters = HYPERPARAMS['filters'], dropout_rate = HYPERPARAMS['dropout_rate'],\n             learning_rate = HYPERPARAMS['learning_rate'], label_smoothing =HYPERPARAMS['label_smoothing'],\n             kernel_size = HYPERPARAMS['kernel_size']):\n    inp = tf.keras.layers.Input(shape = shape)\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    \n    #Bottleneck Layer\n    if HYPERPARAMS['bottleneck_layer'] >0:\n        x = tf.keras.layers.Dense(HYPERPARAMS['bottleneck_layer'])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout_rate)(x)\n    \n    \n    #Conv Blocks\n    for filter_ in filters:\n        x = tf.keras.layers.Conv1D(filters=filter_, kernel_size=kernel_size, strides=1, \n                   padding='causal')(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        \n\n    \n    #Dense output\n    x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.Dense(100, activation='relu', name='last_dense')(x)\n    out = tf.keras.layers.Dense(1, activation='sigmoid', name = 'output')(x)\n    \n   \n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model\n    \nmodel = makeConv()\ntf.keras.utils.plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"###############################################################################\n#Generator: used to generate (minibatch, window_size, num_features)\n#INPUTS:\n    #train: (pandas df) the train with all feature engineering done. DO NOT TAKE OUT WEIGHT==0 ts_id DAYS!!! THIS IS IMPORTANT! MAYBE!!!\n    #features: (list) all features to be trained on\n    #TARGET: (list) all target features \n    #batch_size: (int) size of each minibatch\n    #window_size: (int) number of ts_id that the CNN will look at per sample\n#STEPS:\n    #1.  Decides minibatch label\n    #2.  Picks MINIBATCH_SIZE paired embeddings\n###############################################################################\n\nclass Generator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, train, features, TARGET, batch_size=32, window_size=100, shuffle = True):\n        'Initialization'\n        self.features = features\n        self.TARGET = TARGET\n        self.batch_size = batch_size\n        self.window_size = window_size\n        \n        \n        #Getting the indices that have the correct window specs\n        train['date_day'] = train['ts_id'] - train['date'].map(train.groupby('date')['ts_id'].min())\n        self.idx = train.index[(train.date_day>=(self.window_size-1)) & (train.weight !=0)].to_numpy()\n        \n        self.date_weight_resp = train.loc[self.idx, ['date','weight','resp']]\n        \n        self.train = train[self.features].values\n        self.labels = train[self.TARGET].values\n        self.num_features = len(self.features)\n        self.num_labels = len(TARGET)\n        \n        if shuffle==True:\n            self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return len(self.idx) // self.batch_size\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indices_for_minibatch = self.idx[(index*self.batch_size):(self.batch_size * (index+1))]\n        train, target = self.get_minibatch(indices_for_minibatch)\n        \n        return train, target\n\n    def on_epoch_end(self):\n        'Shuffles the order of the minibatches'\n        np.random.shuffle(self.idx)\n        gc.collect()\n            \n    def get_minibatch(self, indices_for_minibatch):\n        'Generates one minibatch and its val'\n        train_minibatch = np.zeros(shape= (len(indices_for_minibatch), self.window_size, self.num_features))\n        val_minibatch = np.zeros(shape= (len(indices_for_minibatch), self.num_labels))\n        for minibatch_index, train_index in enumerate(indices_for_minibatch):\n            train_minibatch[minibatch_index] = self.train[(train_index - (self.window_size - 1)):(train_index+1)]\n            val_minibatch[minibatch_index] =  self.labels[train_index]\n            \n        return train_minibatch, val_minibatch\n    \n    def get_labels(self):\n        return self.labels\n    \n    def get_date_weight_resp(self):\n        'Only is useful if shuffle is False'\n        return self.date_weight_resp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################\n#Pretraining\n###################\nif SUBMIT ==False and HYPERPARAMS['pretraining']:\n    \n    #Creating the model trained from resp_n; n:1-4\n    model = makeConv()\n    new_model = tf.keras.Model(inputs= model.input, outputs=model.get_layer(name='last_dense').output)\n    out = tf.keras.layers.Dense(4, activation='sigmoid')(new_model.output)\n    newest_model = tf.keras.Model(inputs=new_model.input, outputs = out)\n    newest_model.compile(optimizer=tf.optimizers.Adam(), loss='mse')\n    \n    #Creating the data\n    TARGET = [f'resp_{i}' for i in range(1,5)]\n    train_gen = Generator(train= train.copy(), features = [feat for feat in train.columns if 'feature' in feat], \n            TARGET= TARGET, batch_size = HYPERPARAMS['batch_size'], \n            window_size= HYPERPARAMS['window_size'])\n\n    #Pretraining Model\n    newest_model.fit(train_gen, epochs = 3)\n    \n    #Saving the model_weights\n    ckp_path = f'pretraining.hdf5'\n    model.save_weights(ckp_path)\n    \n    del train_gen; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if SUBMIT ==False:\n    for fold_num, (tr_idx, val_idx) in enumerate(nontimeseries_split(train)):\n    #for fold_num, (tr_idx, val_idx) in enumerate(date_ts_split(train)):\n        start = time()\n        tf.keras.backend.clear_session()\n        print(f'Starting fold {fold_num+1} of {HYPERPARAMS[\"NUM_FOLDS\"]}')\n        \n        \n        #Preparing the generators\n        train_fold = train.iloc[tr_idx].copy()\n        train_fold.reset_index(drop=True,inplace=True)\n        val_fold = train.iloc[val_idx].copy()\n        val_fold.reset_index(drop=True, inplace=True)\n        train_gen = Generator(train= train_fold, features = [feat for feat in train.columns if 'feature' in feat], \n                TARGET=['action'], batch_size = HYPERPARAMS['batch_size'], \n                window_size= HYPERPARAMS['window_size'])\n        val_gen = Generator(train= val_fold, features = [feat for feat in train.columns if 'feature' in feat], \n                TARGET=['action'], batch_size = HYPERPARAMS['batch_size'], \n                window_size= HYPERPARAMS['window_size'], shuffle=False)\n        del train_fold, val_fold; gc.collect()\n        \n        \n        #Preparing Callbacks\n        ckp_path = f'model_{fold_num}.hdf5'\n        rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n                             min_delta = 1e-4, mode = 'max')\n        ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                           save_best_only = True, save_weights_only = True, mode = 'max')\n        es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n                        baseline = None, restore_best_weights = True, verbose = 0)\n        \n        #Creating and Training Model\n        model = makeConv()\n        if HYPERPARAMS['pretraining']:\n            model.load_weights('pretraining.hdf5')\n        H = model.fit(train_gen, validation_data = val_gen, epochs = 1, callbacks = [rlr, ckp, es])\n        \n        #Plotting the models\n        df = pd.DataFrame(H.history)\n        df.plot(y=['loss','val_loss'], kind='line')\n        df.plot(y=['AUC', 'val_AUC'], kind='line')\n        \n        \n        #Training the model for 3 more epochs on only the val at very weak lr\n        model = makeConv(learning_rate = HYPERPARAMS['learning_rate']/100)\n        model.load_weights(ckp_path)\n        model.fit(val_gen, epochs = 3)\n        model.save_weights(f'model_{fold_num}_finetune.hdf5')\n        \n        #Freeing memory\n        del train_gen, val_gen; gc.collect()\n\nelse:\n    model = makeConv()\n    model.load_weights(PATH)\n    env = janestreet.make_env()\n    env_iter = env.iter_test()\n    \n    tmp = np.zeros((1, 4000000, HYPERPARAMS['num_features']))\n    opt_th = 0.5\n    for index, (test_df, pred_df) in enumerate(tqdm(env_iter)):\n        #Cleaning the row and putting it in the numpy container\n        row = Feature_Engineering(test_df)\n        tmp[0, index, :] = row\n\n        if index < HYPERPARAMS['window_size']-1:\n            pred_df.action = 0\n        elif test_df['weight'].item() > 0:\n            pred = model(tmp[0:1, (index - HYPERPARAMS['window_size'] +1):index+1, :])\n            pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# When you love TensorFlow but TensorFlow doesn't love you back\nIf you have trained a network, you have undoubtedly noticed that the RAM is near maxed out.  After all this talk about generator memory efficiency, why are we almost out of memory after running the models?  Let's investigate.\n```\ntrain_gen = Generator(train= train.copy(), features = [feat for feat in train.columns if 'feature' in feat], \n                TARGET=['action'], batch_size = HYPERPARAMS['batch_size'], \n                window_size= HYPERPARAMS['window_size'])\nfor index in range(len(train_gen)):\n    a = train_gen.__getitem__(index)\ndel train_gen; gc.collect()\n```\nThe above code creates a generator from all the train, cycles through the data creating batches shape (batch_size, window_size, num_features).\nDeleting the generator frees all memory.  No problem memory problems are created when using the generator to cycle through all the data.\n```\n#Ludicrous callback to delete and gc.collect() every minibatch after creating via the generator\n#Unnecessary, but done to remove all doubt that this is a tf problem.\nclass Ludicrous_Memory_Freer(tf.keras.callbacks.Callback):\n    def on_train_batch_begin(self, batch, logs=None):\n        del batch; gc.collect()\n        \ntrain_gen = Generator(train= train.copy(), features = [feat for feat in train.columns if 'feature' in feat], \n                TARGET=['action'], batch_size = HYPERPARAMS['batch_size'], \n                window_size= HYPERPARAMS['window_size'])\nmodel = makeConv()\nmodel.fit(train_gen, callbacks=[Ludicrous_Memory_Freer()])\ndel train_gen, model; gc.collect()\ntf.keras.backend.clear_session()\n```\nThe above code runs creates a generator, cycles through the data creating batches shape (batch_size, window_size, num_features), runs the batches through a tf model, deletes the batches immediately after consumption, deletes everything else, then clears the tf.session.  \n\nUnlike the previous block, the RAM does not go down after this code.  Both blocks ran a generator through all data, yet only the first freed up memory.  I believe tf has a memory leak under the hood.  The good news is that the program runs and that tf is usually pretty good with dealing with bugs.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}