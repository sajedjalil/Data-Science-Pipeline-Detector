{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport logging\nimport math\nimport numpy as np\nimport pandas as pd\nfrom numba import jit, njit\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.metrics.functional.classification import auroc\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils import checkpoint\nfrom torch.utils.data import Dataset\nfrom transformers.configuration_utils import PretrainedConfig\nimport random\nfrom pytorch_lightning import loggers as pl_loggers\n\nlogger = logging.getLogger(__name__)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport os\nimport warnings\nfrom typing import List, Optional, Tuple\nfrom torch.optim.optimizer import Optimizer\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import ModelOutput\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\nfrom transformers.modeling_utils import (\n    Conv1D,\n    PreTrainedModel,\n    SequenceSummary,\n    find_pruneable_heads_and_indices,\n    prune_conv1d_layer,\n)\nfrom transformers.utils import logging\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"GPT2Config\"\n_TOKENIZER_FOR_DOC = \"GPT2Tokenizer\"\n\nGPT2_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"gpt2\",\n    \"gpt2-medium\",\n    \"gpt2-large\",\n    \"gpt2-xl\",\n    \"distilgpt2\",\n    # See all GPT-2 models at https://huggingface.co/models?filter=gpt2\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv', nrows=20000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nemb_dim=384\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndicts = torch.load('../input/janestreet-filesv1/transformation_dicts_021921n.pth', map_location='cpu')\ntransformations = dicts['transformations_dict']\n\ndef get_lists(dicts):\n    floats = dicts['floats']\n    strings = dicts['strings']\n    dep_var = dicts['dep_var']\n    str_dicts = dicts['keys_dict']\n    float_dicts = dicts['float_keys']\n    f_mean = dicts['f_mean']\n    return floats, strings, dep_var, str_dicts, float_dicts, f_mean\n\nfloat_cols, string_cols, dep_var, str_dicts, float_dicts, f_mean = get_lists(dicts)\n\ndef normalize_df(df_in):\n    df_new = df_in.copy()\n    df_new['decoded_weight'] = df_new['weight']\n    with tqdm(total=len(float_cols)) as pbar:\n        transformation_dict = dicts['transformations_dict']\n        for col in float_cols:\n            mean = transformation_dict[col]['mean']\n            std = transformation_dict[col]['std']\n            df_new[col] = df_new[col].apply(lambda x: (x-mean)/std)\n            pbar.update(1)\n    return df_new\n\n\ndef fill_nans(df_in):\n    df_newer = df_in.copy()\n    with tqdm(total=len(float_cols)) as pbar:\n        transformation_dict = dicts['transformations_dict']\n        f_means = dicts['f_mean']\n        for n, col in enumerate(float_cols[1:]):\n            mean = transformation_dict[col]['mean']\n            std = transformation_dict[col]['std']\n            f_mean = f_means[n+1]\n            new_num = (f_mean-mean)/std\n            df_newer[col] = df_newer[col].fillna(new_num)\n            pbar.update(1)\n    return df_newer\n\n\ndef get_y_vals(df_in):\n    df_new = df_in.copy()\n    df_new['action_0'] = (df_new['resp_1'] > 1.5e-3).astype(int)\n    df_new['action_1'] = (df_new['resp_2'] > 1.5e-3).astype(int)\n    df_new['action_2'] = (df_new['resp_3'] > 1.5e-3).astype(int)\n    df_new['action_3'] = (df_new['resp'] > 1.5e-3).astype(int)\n    df_new['action_4'] = (df_new['resp_4'] > 1.5e-3).astype(int)\n    return df_new\n\n\ntrain_df = train_df.sample(frac=.1)\ntrain_df = normalize_df(train_df)\ntrain_df = fill_nans(train_df)\ntrain_df = get_y_vals(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating dataset to test on valid data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StructuredDataset(Dataset):\n    def __init__(self,\n                 df,\n                 float_cols,\n                 string_cols,\n#                  dep_col,\n                 dicts):\n        \n        self.df = df\n        self.float_cols = float_cols\n        self.string_cols = string_cols\n#         self.dep_col = dep_col\n        self.dicts = dicts\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def lookup_floats(self, floats_list):\n        encoded_list = [self.dicts['float_keys'][np.round(y, 1)] for x, y in list(zip(self.float_cols, floats_list))]\n        return torch.LongTensor(encoded_list)\n\n    def lookup_strs(self, strings):\n        encoded_list = [self.dicts['keys_dict'][x]['word2key'][int(y)] for x, y in list(zip(self.string_cols, strings))]\n        return torch.LongTensor(encoded_list)\n\n    def __getitem__(self, idx):\n        strings = self.df.iloc[idx, :][self.string_cols].tolist()\n        floats = self.df.iloc[idx, :][self.float_cols].to_numpy()\n        floats[np.isnan(floats)] = 999.0\n\n#         dep_var = torch.Tensor(self.df.iloc[idx, :][self.dep_col].astype(int).tolist())\n        \n        float_cols_array = self.lookup_floats(floats)\n        str_cols_array = self.lookup_strs(strings)\n        \n        attention_mask = torch.ones(132).to(torch.int64)\n        return str_cols_array.to(torch.int64), float_cols_array.to(torch.int64), attention_mask\n    \n\n# LOAD DATASET AND DATALOADER FOR TESTING\nbs=128\nvalidation_data = pd.read_hdf('../input/janestreet-filesv1/valid_feather_z12_only.hdf')\nvalidation_data = validation_data.iloc[:-(validation_data.shape[0]%bs)]\n\nds = StructuredDataset(validation_data,\n                       float_cols,\n                       string_cols,\n                       dicts)\n\ndataloader = torch.utils.data.DataLoader(ds, \n                                         batch_size=128, \n                                         shuffle=False,\n                                         drop_last=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading neural net - converted to torch script to speed up inference\n\ntraced_model_bs128 = torch.jit.load('../input/janestreet-filesv1/traced_model_size128.pt', map_location='cpu')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_outs_list = []\n\ntraced_model_bs128.to(device)\ntraced_model_bs128.eval()\n\nwith tqdm(total=len(ds)//128) as pbar:\n    for n, batch in enumerate(dataloader):\n        x, y, z = batch\n        x, y, z = x.to(device), y.to(device), z.to(device)\n        outs = traced_model_bs128(x, y, z)\n        model_outs_list.append(outs.detach().cpu().numpy())\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATING DATAFRAME WITH INFERENCES ON VALIDATION SET\nth = 0.5\nouts_array = np.concatenate(model_outs_list)\nouts_array =  np.where(outs_array > th, 1, 0)\n\nanswers = pd.DataFrame(outs_array).rename({0: 'action_0a', 1: 'action_1a', 2: 'action_2a', 3: 'action_3a', 4: 'action_4a'}, axis=1)\nanswers['keys'] = answers.index\nanswers.to_hdf('/kaggle/working/validation_inference.hdf', key='keys')\n\nval_inference = pd.concat([validation_data, answers], axis=1)\ncols = val_inference.columns.tolist()\n\n# GET COMPARISON COLUMNS\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\nval_actions = ['action_0', 'action_1', 'action_2', 'action_3', 'action_4']\ninference_cols = ['action_0a', 'action_1a', 'action_2a', 'action_3a', 'action_4a']\n\n# Convert matches to bools\ncompare1 = val_inference[val_actions].copy()\ncompare2 = val_inference[inference_cols].copy()\naccuracy_df = pd.DataFrame(compare1.to_numpy()==compare2.to_numpy())\n\n# Get accuracy on validation set \nprint(str(round(((((val_inference[inference_cols].sum(axis=1)>=3) == (val_inference[val_actions].sum(axis=1)>=3)).sum())/val_inference.shape[0] * 100),2)) + '% accuracy on validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scores for Each Resp are > 0 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scores for All Resp Cols are > 0 - then why wouldn't kaggle give me a score?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility Score on Validation Set\nval_inference['resp_mean'] = val_inference[resp_cols].mean(axis=1)\nval_inference['resp_sum'] = val_inference[resp_cols].sum(axis=1)\nfor resp in resp_cols + ['resp_mean', 'resp_sum']:\n    val_inference['buy_sell_signal'] = val_inference[inference_cols].sum(axis=1)>=3\n    val_inference['pi'] = val_inference['decoded_weight']*val_inference[resp]*val_inference['buy_sell_signal']\n\n    days_in_val = validation_data['date'].unique().shape[0]\n    t_score = ((val_inference['pi'].sum())/((val_inference['pi']**2).sum())**(1/2))*((250/days_in_val)**(1/2))\n    utility_score = min(max(t_score,0),6)*val_inference['pi'].sum()\n    print('{} utility score: '.format(resp) + str(round(utility_score, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Below is the code I implemented to pre-process the streams from the competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Processing for data from competition stream\n\ndef get_col_transforms(transformations=transformations, float_cols=float_cols):\n    mean_array = np.array([transformations[col]['mean'] for col in float_cols], dtype=np.float64)\n    std_array = np.array([transformations[col]['std'] for col in float_cols], dtype=np.float64)\n    return mean_array, std_array\n\nmean_array, std_array = get_col_transforms(transformations=transformations, float_cols=float_cols)\n\n@jit(nopython=True)\ndef transform_test_df(tensor, mean_array=mean_array, std_array=std_array):\n    out1 = np.divide(np.subtract(tensor,mean_array),std_array)\n    return out1\n\ndef lookup_floats(floats_list, dicts=dicts, float_cols=float_cols):\n    encoded_list = [dicts['float_keys'][np.round(y,1)] for x,y in list(zip(float_cols, floats_list))]\n    return torch.LongTensor(encoded_list)\n    \ndef lookup_strs(strings_list, dicts=dicts, string_cols=string_cols):\n    encoded_list = [dicts['keys_dict'][x]['word2key'][int(y)] for x,y in list(zip(string_cols, strings_list))]\n    return torch.LongTensor(encoded_list)\n\n\ndef process_data(df, mean_array=mean_array, std_array=std_array, string_cols=string_cols, float_cols=float_cols, dicts=dicts, transformations=transformations, f_mean=f_mean):\n    \n    strings = df.iloc[0,:][string_cols].tolist()\n    floats = df.iloc[0][float_cols].replace([np.inf, -np.inf], np.nan).to_numpy()\n    \n    floats[np.isnan(floats)] = f_mean[np.isnan(floats)]\n    floats_list = transform_test_df(floats)\n\n    float_cols_array = lookup_floats(floats_list)\n    float_cols_array = float_cols_array.unsqueeze(0)\n    \n    str_cols_array = lookup_strs(strings)\n    str_cols_array = str_cols_array.unsqueeze(0) \n    \n    attention_mask = torch.ones(132).to(torch.int64)\n    attention_mask = attention_mask.unsqueeze(0).to(device)\n    \n    return str_cols_array, float_cols_array, attention_mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Code used to submit model outputs to kaggle:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\n\ntraced_model = torch.jit.load('../input/janestreet-filesv1/traced_model_size1.pt', map_location='cpu')\n\nth = 0.5\ntraced_model.eval()\ntraced_model.to(device)\nmean_array, std_array = get_col_transforms(transformations=transformations, float_cols=float_cols)\n\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    try:\n        if test_df.weight.iloc[0] > 0:\n            x, y, z = process_data(test_df, mean_array, std_array)\n            out2 = traced_model(*[x.to(device), y.to(device), z.to(device)])\n            pred = out2.detach().cpu().numpy()\n            pred_df.action = np.where(np.where(pred > .5, 1, 0).sum() >= 3, 1, 0).astype(int)\n\n        else:\n            pred_df.action = 0\n        \n        env.predict(pred_df)\n    \n    except:\n        pred_df.action = 0\n        env.predict(pred_df)\n        print('exception')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}