{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1st, I would like to say thank you to  \n \n@snippsy\n@code1110\n\nand eveyone share an ideas helped to make this code","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of the adavantages of DenseNet is the ability to have a deep narrow network without loss in performance.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\ntrain = train.query('weight>0').reset_index(drop=True)\ntrain.fillna(train.median(),inplace=True)\ntrain['feature_stock_id_sum'] = train['feature_41'] + train['feature_42'] + train['feature_43']\ntrain['feature_1_2_cross'] = train['feature_1']/(train['feature_2']+1e-5)\nNUM_TRAIN_EXAMPLES = len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [c for c in train.columns if 'feature' in c]\nf_mean = np.nanmean(train[features[1:]].values,axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PATH = \"../input/\"\nPATH ='../input/neutralizing2'\n#PATH = '../input/densenetneu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resp_cols = [c for c in train.columns if 'resp' in c]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\ndef mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})\n\ndef create_model(input_shape):\n    \n    inp = tf.keras.layers.Input(input_shape)\n    tmp = tf.keras.layers.BatchNormalization()(inp)\n    xs = [tmp]\n    for _ in range(10):\n        if len(xs) > 1:\n            tmp = tf.keras.layers.Concatenate(axis=-1)(xs)\n        else:\n            tmp = xs[0]\n        tmp = tf.keras.layers.Dense(64,activation='mish')(tmp)\n        tmp = tf.keras.layers.BatchNormalization()(tmp)\n        tmp = tf.keras.layers.Dropout(0.2)(tmp)\n        xs.append(tmp)\n    \n    output = tf.keras.layers.Dense(len(resp_cols),activation='sigmoid')(tf.keras.layers.Concatenate()(xs))\n    model = tf.keras.models.Model(inp,output)\n    optimizer = tfa.optimizers.RectifiedAdam(1e-3)\n    model.compile(optimizer, loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n                    metrics=[tf.keras.metrics.BinaryAccuracy(name='binary_accuracy')])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport os\ndef set_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See https://www.kaggle.com/c/jane-street-market-prediction/discussion/215305","metadata":{}},{"cell_type":"code","source":"#Designed to do all features at the same time, but Kaggle kernels are memory limited.\nclass NeutralizeTransform:\n    def __init__(self,proportion=1.0):\n        self.proportion = proportion\n    \n    def fit(self,X,y):\n        self.lms = []\n        self.mean_exposure = np.mean(y,axis=0)\n        self.y_shape = y.shape[-1]\n        for x in X.T:\n            scores = x.reshape((-1,1))\n            exposures = y\n            exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n            \n            transform = np.linalg.lstsq(exposures, scores, rcond=None)[0]\n            self.lms.append(transform)\n            \n    def transform(self,X,y=None):\n        out = []\n        for i,transform in enumerate(self.lms):\n            x = X[:,i]\n            scores = x.reshape((-1,1))\n            exposures = np.repeat(self.mean_exposure,len(x),axis=0).reshape((-1,self.y_shape))\n            exposures = np.concatenate([exposures,np.array([np.mean(scores)] * len(exposures)).reshape((-1,1))],axis=1)\n            correction = self.proportion * exposures.dot(transform)\n            out.append(x - correction.ravel())\n            \n        return np.asarray(out).T\n    \n    def fit_transform(self,X,y):\n        self.fit(X,y)\n        return self.transform(X,y)\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nif TRAINING:\n    mask = train[features].isna()\n    train.fillna(0,inplace=True)\n    for feature in features:\n        nt = NeutralizeTransform(proportion=0.25)\n        train[feature] = nt.fit_transform(train[feature].values.reshape((-1,1)),\n                                          train['resp'].values.reshape((-1,1)))\n        pd.to_pickle(nt,f'NeutralizeTransform_{feature}.pkl')\n    train[mask] = np.nan\n    \nelse:\n    nts = []\n    for feature in features:\n        nt = pd.read_pickle(f'{PATH}/NeutralizeTransform_{feature}.pkl')\n        nts.append(nt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_tr = train.query('date<440')[features].values\n# y_tr = (train.query('date<440')[resp_cols].values > 0).astype(int)\n    \n# X_val = train.query('date>460')[features].values\n# y_val = (train.query('date>460')[resp_cols].values > 0).astype(int)\n\nX_tr = train.query('date<350')[features].values\ny_tr = (train.query('date<350')[resp_cols].values > 0).astype(int)\n    \nX_val = train.query('date>400')[features].values\ny_val = (train.query('date>400')[resp_cols].values > 0).astype(int)\n\ndel train\ngc.collect()\n\n\nif TRAINING:\n    metric = {}\n    \n    #for seed in [6, 28, 496, 8128]:\n    for seed in [2020,1982]:\n        set_all_seeds(seed)\n\n        model = create_model(X_tr.shape[-1])\n        hist = model.fit(X_tr,y_tr,\n                         validation_data=(X_val,y_val),\n                         epochs=200,\n                         batch_size=8192,\n                         callbacks=[tf.keras.callbacks.EarlyStopping('val_binary_accuracy',mode='max',patience=20,restore_best_weights=True),\n                                   tf.keras.callbacks.ReduceLROnPlateau('val_binary_accuracy',mode='max',patience=10,cooldown=5)])\n        \n        model.save_weights(f'model_{seed}.tf')\n        metric[seed] = max(hist.history['val_binary_accuracy'])\n        \n        tf.keras.backend.clear_session()\n        \n    print(metric)        \nelse:\n    models = []\n    #for seed in [6, 28, 496, 8128]:\n    #for seed in [6,8128]:\n    for seed in [2020,1982]:\n        model = create_model(X_tr.shape[-1])\n        model.load_weights(f'{PATH}/model_{seed}.tf')\n        model.call = tf.function(model.call, experimental_relax_shapes=True)\n        models.append(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df_columns = ['weight'] + [f'feature_{i}' for i in range(130)] + ['date']\n# features = [c for c in train.columns if 'feature' in c]\n# #features = feat_cols\n# index_features = [n for n,col in enumerate(test_df_columns) if col in features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nif not TRAINING:\n    f = np.median\n    import janestreet\n    #janestreet.competition.make_env.__called__ = False\n    env = janestreet.make_env()\n    #th = 0.50\n    th = 0.495\n    for (test_df, pred_df) in tqdm(env.iter_test()):\n        #if test_df['weight'].item() > 0:\n        if test_df['weight'].values[0] > 0:\n            \n            test_df['feature_stock_id_sum'] = test_df['feature_41'] + test_df['feature_42'] + test_df['feature_43']\n            test_df['feature_1_2_cross'] = test_df['feature_1']/(test_df['feature_2']+1e-5)\n            \n            x_tt = test_df.loc[:, features].values\n            #x_tt = test_df.values[0][index_features].reshape(1, -1)\n            if np.isnan(x_tt[:, 1:].sum()):\n                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n \n            for i in range(len(nts)):\n                x_tt[:,i] = nts[i].transform(np.expand_dims(x_tt[:,i],0))\n            \n            p = f(np.mean([model(x_tt,training=False).numpy() for model in models],axis=0))\n    \n            pred_df.action = np.where(p > th, 1, 0).astype(int)\n        else:\n            pred_df[\"action\"].values[0] = 0\n        env.predict(pred_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}