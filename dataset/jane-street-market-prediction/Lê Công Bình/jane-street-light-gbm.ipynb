{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Single Neural Network version: https://www.kaggle.com/binhlc/jane-street-tensorflow-dense\n* Highest Scored: 7736\n* Time: 2.5 hours on GPU"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport gc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\ntrain = reduce_mem_usage(train)\nfeatures = [c for c in train.columns if 'feature' in c]\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float16').columns}) \ntrain = train.fillna(train.mean())\nf_mean = np.mean(train[features[1:]].values,axis=0)\ntrain = train.query('date > 85').reset_index(drop = True)\ntrain = train[train.weight != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nseed = 42\n\nScaleData = False\nif (ScaleData):\n    scaler = MinMaxScaler(feature_range = (0, 1)).fit(train[features])\n    train.loc[:,features] = scaler.transform(train[features])\n\nX = train[features]\ny = (train['resp'].values > 0).astype(int)\n\n#del train\n#gc.collect()\n\n#skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n#train_index, test_index = next(skf.split(X, y))\n#X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n#y_train, y_test = y[train_index], y[test_index]\n\n\nX_train = train[train['date'] < 400][features]\nX_test = train[train['date'] >= 400][features]\ntrain['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')\n\ny_train = train[train['date'] < 400]['action']\ny_test = train[train['date'] >= 400]['action']\n\nX_train = train[features]\ny_train = train['action']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.histogram(y_train, bins=2)\nunique, counts = np.unique(y_test, return_counts=True)\ndict(zip(unique, counts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\nNUM_ROUND = 2000\nparams = {\n    'objective': 'cross_entropy',\n    \"metric\": \"auc\",\n    \"random_state\": seed,\n    \"learning_rate\": 0.05,\n    #\"max_depth\": 16,\n    #\"max_bin\": 255,\n    \"num_leaves\": 511,\n    \"feature_fraction\": 0.8,\n    \"min_data_in_leaf\": 20,\n    \"min_data_per_group\": 90,\n    \"lambda_l1\": 20,\n    \"lambda_l2\": 5,\n    \"device\": \"gpu\"\n    \n}\n\ntrainData = lgb.Dataset(X, label = y, free_raw_data = False)\n#validData = lgb.Dataset(X_test, label = y_test, free_raw_data = False)\nmodel = lgb.train(params, trainData, num_boost_round = NUM_ROUND, \n                  #early_stopping_rounds = 30, \n                  #valid_sets = [validData, trainData], verbose_eval=50\n                 )\n\n'''\nimport xgboost as xgb\n\nparams = {\n    #'objective': 'binary:logistic',\n    'learning_rate': 0.05,\n    \"eval_metric\": \"auc\",\n    #'tree_method': 'gpu_hist',\n    #'gpu_id': 0,\n    \"random_state\": seed\n}\n\nNUM_ROUND = 500\ntrainData = xgb.DMatrix(X_train, label = y_train)\nvalidData = xgb.DMatrix(X_test, label = y_test)\nevallist = [(trainData, 'train'),(validData, 'eval')]\nmodel = xgb.train(params, trainData, NUM_ROUND, evallist, early_stopping_rounds = 20, verbose_eval = 50)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission\nimport janestreet\nfrom tqdm.notebook import tqdm\njanestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    if (test_df.iloc[0].weight > 0): \n        if (ScaleData):\n            test_df[features] = scaler.transform(test_df[features])\n        test_np = test_df.loc[:, features].values\n        if np.isnan(test_np[:, 1:].sum()):\n            test_np[:, 1:] = np.nan_to_num(test_np[:, 1:]) + np.isnan(test_np[:, 1:]) * f_mean\n            test_df[features] = test_np\n        #action = model.predict(xgb.DMatrix(test_df[features]))[0]\n        action = model.predict(test_df[features])[0]\n        if (action > 0.5):\n            sample_prediction_df.action = 1\n        else:\n            sample_prediction_df.action = 0                            \n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}