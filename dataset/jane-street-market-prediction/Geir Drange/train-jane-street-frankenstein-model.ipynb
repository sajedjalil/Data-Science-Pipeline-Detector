{"cells":[{"metadata":{},"cell_type":"markdown","source":"![logo](https://www.goodrebels.com/wp-content/uploads/2018/12/181219_frankenstein_WP-1.png)"},{"metadata":{},"cell_type":"markdown","source":"# The Frankenstein model\nA lot of the public notebooks in this competition are using a DNN to achieve superior overfitting. While there has been very little (public) feature engineering. Feature engineering can be tedious work, so why not let the model itself find the best feature crossings? [This paper](https://arxiv.org/pdf/1708.05123.pdf) describes the concept.  \n\nThere hasn't been many 2D CNN models either (maybe for a good reason?). There is nothing (technically) stopping us from feeding a 1D feature vector into a 2D CNN model - it is just a matter of reshaping the 1D feature vector to 2D. So in this notebook we will combine all into a Deep and Cross and Convolutional Network - a Frankenstein model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape, Activation\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input data\nThe training data has been preprocessed in [this notebook](https://www.kaggle.com/mistag/jane-street-data-preprocessing)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all = pd.read_pickle('../input/jane-street-preprocessing/train_data.pkl')\ntrain_all['action'] = (train_all.resp > 0).astype(int).to_list()\nfeatures = [c for c in train_all.columns if \"feature\" in c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stratification\nDuring training we will run a cross-validation scheme, and some stratification is done here on the data. A new feature column based on binned dates and feature_0 is created as the statification column."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all['date_bin'] = (pd.qcut(train_all['date'], q=4, labels=False)+1)*train_all['feature_0'] # stratify column\nfeatures = [c for c in train_all.columns if \"feature\" in c]\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nX_train = train_all.loc[:, train_all.columns.str.contains('feature|date_bin')]\ny_train = pd.DataFrame(np.stack([(train_all[c] > 0).astype('int') for c in resp_cols]).T, columns = resp_cols)\n\ndel train_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model building\nHere we build the combined model of feature crossing, 2D CNN and DNN. Note the reshaping operation for the CNN. With 130 features, we have several options for 2D shape: 13x10, 10x13, 5x26, 26x5. The CNN model is inspired from [this notebook](https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist).   \n\nThere are several ways to put together these models. We can do sort of wide and deep with all three in parallel, or we can feed the output of the feature crossings to the CNN and DNN. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dropout_rate = 0.2\ncross_units = [130, 130, 130]\nhidden_units = [130, 130, 130, 130]\n\ndef create_frankenstein_model(num_columns, x_shape, y_shape):\n\n    x0 = tf.keras.layers.Input(shape=(num_columns,))   \n    \n    # feature crossing model\n    cross = x0\n    for _ in cross_units:\n        units = cross.shape[-1]\n        x = layers.Dense(units)(cross)\n        cross = x0 * x + cross\n    cross = layers.BatchNormalization()(cross)\n    cross = Dropout(dropout_rate)(cross)\n\n    # 2D CNN model \n    cnn = tf.keras.layers.Reshape((x_shape, y_shape, 1))(x0)  \n    cnn = Conv2D(32,kernel_size=3,activation='relu',input_shape=(x_shape, y_shape, 1),padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Conv2D(32,kernel_size=3,activation='relu',padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Dropout(dropout_rate)(cnn)\n    cnn = Conv2D(64,kernel_size=3,activation='relu',padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Conv2D(64,kernel_size=3,activation='relu',padding='same')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu')(cnn)\n    cnn = BatchNormalization()(cnn)\n    cnn = Dropout(dropout_rate)(cnn)\n    cnn = Flatten()(cnn)\n    cnn = Dense(128, activation='relu')(cnn)\n    cnn = Dropout(dropout_rate)(cnn)\n    \n    # DNN model\n    deep = x0\n    for _ in hidden_units:\n        deep = Dense(units)(deep)\n        deep = BatchNormalization()(deep)\n        deep = Activation(tf.keras.activations.swish)(deep)\n        deep = Dropout(dropout_rate)(deep)\n\n    # merging the 3 models\n    merged = layers.concatenate([cross, cnn, deep])\n    x = Dense(len(resp_cols))(merged)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n    model = keras.Model(inputs=x0, outputs=out)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"MNAME = 'Frank'\n\ndef get_callbacks(idx):\n    mc = ModelCheckpoint(MNAME+\"-{}.h5\".format(idx), save_best_only=True)\n    rp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n    return [mc, rp, es]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\n\nhistory = []\nepochs = 200\nbatch_size = 4096\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nskf = StratifiedKFold(n_splits=NFOLDS, shuffle = True, random_state = 42)\n\nfor i in range(NFOLDS):\n    start = time.time()\n    result = next(skf.split(X_train, X_train.date_bin), None)\n    X_tr = X_train.iloc[result[0]].reset_index(drop=True)\n    X_tr.drop(labels='date_bin', axis = 1, inplace=True)\n    y_tr = y_train.iloc[result[0]].reset_index(drop=True)\n    X_val = X_train.iloc[result[1]].reset_index(drop=True)\n    X_val.drop(labels='date_bin', axis=1, inplace=True)\n    y_val = y_train.iloc[result[1]].reset_index(drop=True)\n    del result\n    if i == NFOLDS - 1:\n        del X_train, y_train\n    \n    tf.random.set_seed(42*i)\n    model = create_frankenstein_model(len(features), 13, 10)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n    callbacks = get_callbacks(i)\n    history.append(model.fit(X_tr, y_tr, epochs=epochs, batch_size=batch_size, validation_data=(X_val,y_val), callbacks=callbacks, verbose=0))\n    \n    del model, X_tr, y_tr, X_val, y_val # save precious memory\n    print('fold {} training: {}'.format(i, time.strftime('%H:%M:%S', time.gmtime(time.time() - start))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the training curves:"},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,10))\nfor i in range(NFOLDS):\n    plt.plot(history[i].history['AUC'], linestyle='-', color=colors[i], label='Train Fold {}'.format(str(i)))\nfor i in range(NFOLDS):\n    plt.plot(history[i].history['val_AUC'], linestyle='--', color=colors[i], label='Validation Fold {}'.format(str(i)))\nplt.title('Model AUC')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,10))\nfor i in range(NFOLDS):\n    plt.plot(history[i].history['loss'],linestyle='-', color=colors[i], label='Train Fold {}'.format(str(i)))\nfor i in range(NFOLDS):\n    plt.plot(history[i].history['val_loss'],linestyle='--', color=colors[i], label='Validation Fold {}'.format(str(i)))\nplt.title('Model Loss')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save training results\nThe training loss and AUC is saved here for later use during inference."},{"metadata":{"trusted":true},"cell_type":"code","source":"res = []\nfor i in range(NFOLDS):\n    res.append([i, np.min(history[i].history['val_loss']), np.max(history[i].history['val_AUC'])])\nrdf = pd.DataFrame(res, columns=['Model', 'Loss', 'AUC']).sort_values(by=['AUC'], ascending=False)\nrdf.to_pickle('results.pkl')\nrdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}