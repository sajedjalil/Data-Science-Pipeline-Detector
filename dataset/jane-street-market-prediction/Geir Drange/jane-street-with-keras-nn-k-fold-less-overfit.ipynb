{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/tf-model-garden-official-models/TF.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook is based on [Jane Street with Keras NN overfit](https://www.kaggle.com/code1110/jane-street-with-keras-nn-overfit), which itself is based on [OWN Jane Street with Keras NN](https://www.kaggle.com/tarlannazarov/own-jane-street-with-keras-nn).  \n\nThe changes I have made in this version are:\n* Add stratified K-Fold data splitting\n* Add learning curve plotting\n* Add early stopping etc.\n* Improve inference speed as described in [this notebook](https://www.kaggle.com/tocha4/20210204-speed-up-your-prediction) and [this notebook](https://www.kaggle.com/gogo827jz/optimise-speed-of-filling-nan-function)\n* Tidy up code somewhat  \n\nThe primary motivation is to make the model more general (so less likely to top the public overfit leaderboard in this competition). The model itself has intentionally been kept unchanged to see how the PB score compares with the original, but there are plenty of improvements that could be made to the model itself of course. "},{"metadata":{},"cell_type":"markdown","source":"# Import training data\nThe data has lot's of NaNs, and they are imputed here like in the original notebook with mean values. Mean values are very likely to change over time, so a bias is introduced here which might impact inference on future data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\n\ntrain_all = pd.read_csv('../input/jane-street-market-prediction/train.csv')\ntrain_all = train_all[train_all.date > 85].reset_index(drop = True) \ntrain_all = train_all[train_all['weight'] != 0]\ntrain_all.fillna(train_all.mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stratification\nWhen splitting data into K-folds cross-validation sets, it is important that the splits contain equal percentage of desired features. The NN model here does not use any time series information whatsoever, so we can split and shuffle the data as we like. Let's take a look at date distribution for example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\ntrain_all['date'].plot(kind='hist');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trading frequency increase with time, and we want each fold to have equal distribution of trades from different times. So we bin the dates into 4. But we will also like to have equal amounts of sell/buy data, so we multiply the date bins with feature_0 to get our stratification variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_all['date_bin'] = (pd.qcut(train_all['date'], q=4, labels=False)+1)*train_all['feature_0'] # stratify column\nfeatures = [c for c in train_all.columns if \"feature\" in c]\nf_mean = np.mean(train_all[features[1:]].values,axis=0)\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\nX_train = train_all.loc[:, train_all.columns.str.contains('feature|date_bin')]\ny_train = pd.DataFrame(np.stack([(train_all[c] > 0).astype('int') for c in resp_cols]).T, columns = resp_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the distribution after splitting:"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=NFOLDS, shuffle = True, random_state = 42)\nresult = next(skf.split(X_train, X_train.date_bin), None)\ntrain = train_all.iloc[result[0]].reset_index(drop=True)\nvalid = train_all.iloc[result[1]].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date'].plot(kind='hist');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid['date'].plot(kind='hist');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The date distributions in train/validation sets are identical as expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, valid, train_all, result # save precious memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"MNAME = 'model'\n\ndef get_callbacks(idx):\n    mc = ModelCheckpoint(MNAME+\"-{}.h5\".format(idx), save_best_only=True)\n    rp = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n    return [mc, rp, es]\n\ndef create_dnn(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=NFOLDS, shuffle = True, random_state = 42)\nhistory = []\n\nfor i in range(NFOLDS):\n    print('fold {}'.format(i))\n    result = next(skf.split(X_train, X_train.date_bin), None)\n    X_tr = X_train.iloc[result[0]].reset_index(drop=True)\n    X_tr.drop(labels='date_bin', axis = 1, inplace=True)\n    y_tr = y_train.iloc[result[0]].reset_index(drop=True)\n    X_val = X_train.iloc[result[1]].reset_index(drop=True)\n    X_val.drop(labels='date_bin', axis=1, inplace=True)\n    y_val = y_train.iloc[result[1]].reset_index(drop=True)\n    \n    np.random.seed(42*i)\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(42*i)\n\n    # fit\n    batch_size = 8192\n    hidden_units = [160, 160, 160]\n    dropout_rates = [0.2, 0.2, 0.2, 0.2]\n    label_smoothing = 1e-2\n    learning_rate = 1e-3\n\n    clf = create_dnn(len(features), y_train.shape[1], hidden_units, dropout_rates, label_smoothing, learning_rate)\n    clf.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n    callbacks = get_callbacks(i)\n\n    epochs = 200\n\n    history.append(clf.fit(X_tr, y_tr, epochs=epochs, batch_size=batch_size, validation_data=(X_val,y_val), callbacks=callbacks, verbose=0))\n    \n    del clf, X_tr, y_tr, X_val, y_val, result # save precious memory","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learning curves\nPlot train and validation loss/AUC to check our training."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_lrc(hist_list):\n    x = np.arange(1,epochs+1)\n    fig, ax = plt.subplots(2,1,figsize=(16,16))    \n    for i in range(len(hist_list)):\n        if i == 0:\n            ax[0].plot(x[0:len(history[i].history['loss'])], history[i].history['loss'], \n                   color='tab:blue', label='Train loss')\n            ax[0].plot(x[0:len(history[i].history['val_loss'])], history[i].history['val_loss'], \n                   color='tab:orange', label='Validation loss')\n        else:\n            ax[0].plot(x[0:len(history[i].history['loss'])], history[i].history['loss'], \n                   color='tab:blue')\n            ax[0].plot(x[0:len(history[i].history['val_loss'])], history[i].history['val_loss'], \n                   color='tab:orange')\n    ax[0].set_xlabel('Epoch', fontsize=10)\n    ax[0].set_ylabel('Loss', fontsize=10)    \n    ax[0].legend()\n    for i in range(len(hist_list)):\n        if i == 0:\n            ax[1].plot(x[0:len(history[i].history['AUC'])], history[i].history['AUC'], \n                   color='tab:blue', label='Train AUC')\n            ax[1].plot(x[0:len(history[i].history['val_AUC'])], history[i].history['val_AUC'], \n                   color='tab:orange', label='Validation AUC')\n        else:\n            ax[1].plot(x[0:len(history[i].history['AUC'])], history[i].history['AUC'], \n                   color='tab:blue')\n            ax[1].plot(x[0:len(history[i].history['val_AUC'])], history[i].history['val_AUC'], \n                   color='tab:orange')        \n    ax[1].set_xlabel('Epoch', fontsize=10)\n    ax[1].set_ylabel('AUC', fontsize=10)    \n    ax[1].legend()\n    plt.suptitle('Training curves', fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_lrc(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The curves for the different folds are very close, as expected."},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nfrom numba import njit\n\nenv = janestreet.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@njit\ndef fillna_npwhere_njit(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"th = 0.501\nclf0 = tf.keras.models.load_model(\"model-0.h5\")\n#clf1 = tf.keras.models.load_model(\"model-1.h5\")\nclf2 = tf.keras.models.load_model(\"model-2.h5\")\n#clf3 = tf.keras.models.load_model(\"model-3.h5\")\nclf4 = tf.keras.models.load_model(\"model-4.h5\")\nmodels = [clf0, clf2, clf4]\n\ntest_df_columns = ['weight'] + [f'feature_{i}' for i in range(130)] + ['date']\nindex_features = [n for n,col in enumerate(test_df_columns) if col in features]\n\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].values[0] > 0:\n        x_tt = test_df.values[0][index_features].reshape(1,-1)\n        x_tt[:, 1:] = fillna_npwhere_njit(x_tt[:, 1:][0], f_mean)\n        pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in models],axis=0))\n        pred_df.action = int(pred >= th)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}