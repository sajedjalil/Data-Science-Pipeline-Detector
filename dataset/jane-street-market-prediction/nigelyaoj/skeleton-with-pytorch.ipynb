{"cells":[{"metadata":{},"cell_type":"markdown","source":"## TODO\n1. add features\n2. change targets\n3. model\n4. loss_fn\n5. tune the parameters"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## packages and settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\nimport os\nimport copy\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\nif not os.path.exists(\"results\"):\n    os.mkdir(\"results\")\n\nTRAINING = True\nread_path = '/kaggle/input/jane-street-market-prediction/train.csv'\n# model_path = \"/kaggle/input/skeleton-with-pytorch/results/123/best_model\"\nsave_path = os.path.join(\"results\", str(seed))\n\ndevice = torch.device(\"cuda:0\")\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\n    \n# train = pd.read_csv(read_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## preprocess the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(read_path)\n# train = train.query('date > 85').reset_index(drop = True) \nfeatures = [c for c in train.columns if 'feature' in c]\n\n# preprocess the features\nf_mean = train[features].mean()\ntrain = train.loc[train.weight > 0].reset_index(drop = True)\ntrain[features] = train[features].fillna(f_mean)\n\ntrain = train.astype(\"float32\")\ntrain['action'] = (train['resp'] > 0).astype('int')\ntrain['action1'] = (train['resp_1'] > 0).astype('int')\ntrain['action2'] = (train['resp_2'] > 0).astype('int')\ntrain['action3'] = (train['resp_3'] > 0).astype('int')\ntrain['action4'] = (train['resp_4'] > 0).astype('int')\n\ntargets = ['resp']\ntargets = ['action', 'action1', 'action2', 'action3', 'action4']\n\n\ndef add_features(df, features):\n    new_features = copy.deepcopy(features)\n    \n    # todo\n    df[\"cross_1_2\"] = df[\"feature_1\"] / (df[\"feature_2\"] + 1e-5)\n    df[\"cross_41_42_43\"] = df[\"feature_41\"] + df[\"feature_42\"] + df[\"feature_43\"]\n    new_features.extend([\"cross_1_2\", \"cross_41_42_43\"])\n\n    return df, new_features\n\ntrain, train_features = add_features(train, features)\n\n# to do: update the mean online\n# f_mean = f_mean.values\n# np.save(os.path.join(save_path, 'f_mean.npy'), f_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = train.loc[(train.date >= 450) & (train.date < 500)].reset_index(drop=True)\n# train = train.loc[(train.date<450)].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"date: 1~499\n\nvalid: 440~469\n\ntest: 470~499"},{"metadata":{"trusted":true},"cell_type":"code","source":"# n1 = np.sum(train['date']<440)\n# n2 = np.sum(train['date']<470)\n# tr = list(range(n1))\n\n# # test using the last 60 days data\n# te = list(range(n1, train.shape[0]))\n# test = train.iloc[te, :]\n\n# # another test set with gap (30 days)\n# te_with_gap = list(range(n2, train.shape[0]))\n# test_with_gap = train.iloc[te, :]\n\n\n# train = train.iloc[tr, :]\n# # print(te[-1], train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## the dataset and model(resnet)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset:\n    def __init__(self, df, features, targets):\n        self.features = df[features].values\n        \n        # preprocess the labels\n        # self.labels = (df[targets] > 0).astype('int').values\n        self.labels = df[targets].values\n        self.weights = df['weight'].values\n        \n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        feat_ = torch.tensor(self.features[idx], dtype=torch.float)\n        label_ = torch.tensor(self.labels[idx], dtype=torch.float)\n        weight_ = torch.tensor(self.weights[idx], dtype=torch.float)\n        \n        return feat_, label_, weight_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, all_feat_cols, target_cols):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.1)\n\n        dropout_rate = 0.1\n        hidden_size = 512\n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x\n    \n    \nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets, weights=None):\n        # print(inputs.shape, targets.shape)\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets, weights)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\n    \nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved. Saving model!')\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{},"cell_type":"markdown","source":"### utility"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for feature, label, weight in dataloader:\n        feature = feature.to(device)\n        label = label.to(device)\n        weight = weight.to(device)\n        optimizer.zero_grad()\n        outputs = model(feature)\n        \n        loss = loss_fn(outputs.reshape(-1, 1), label.reshape(-1, 1)) #  weight.reshape(-1,1)\n        loss.backward()\n        \n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n    return final_loss\n\n    \ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    labels = []\n    final_loss = 0\n\n    for feature, label, weight in dataloader:\n        feature = feature.to(device)\n        label = label.to(device)\n        with torch.no_grad():\n            outputs = model(feature)\n            loss = loss_fn(outputs.reshape(-1, 1), label.reshape(-1, 1)) #  weight.reshape(-1,1)\n            preds.append(outputs.sigmoid().cpu().numpy())\n            labels.append(label.cpu().numpy())\n            \n        final_loss += loss.item()\n    \n    final_loss/= len(dataloader)\n    preds = np.concatenate(preds, axis=0)\n    labels = np.concatenate(labels, axis=0)\n\n    return preds, labels, final_loss\n    \n    \ndef utility_score(date, weight, resp, action):    \n    values = weight * resp * action\n    to_bincount = {}\n\n    for d, v in zip(date, values):\n        to_bincount.setdefault(d, []).append(v)\n\n    Pi = []\n    for val in to_bincount.values():\n        Pi.append(np.sum(val))\n    Pi = np.array(Pi)\n    count_i = len(np.unique(date))\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    \n    return u\n\n    \ndef loss_mse(preds, targets):\n    \n    return ((preds-targets)**2).mean()\n\n\ndef loss_ce(preds, targets, weight=None):\n    \n    return F.binary_cross_entropy_with_logits(preds, targets, weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import sys\n# f_handler=open('out.txt', 'w')\n# sys.stdout=f_handler\n\nif TRAINING:\n    batch_size = 1024\n    label_smoothing = 1e-2\n    learning_rate = 1e-3\n\n    import time\n    start_time = time.time()\n    oof = np.zeros(len(train['action']))\n    gkf = GroupKFold(n_splits = 5)\n    models = []\n    for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n        train_set = MyDataset(train.loc[tr], train_features, targets)\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n        valid_set = MyDataset(train.loc[te], train_features, targets)\n        valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=1)\n\n        torch.cuda.empty_cache()\n        device = torch.device(\"cuda:0\")\n        model = Model(train_features, targets)\n        model.to(device)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n\n        ckp_path = f'JSModel_{fold}.pth'\n\n        es = EarlyStopping(patience=100, mode=\"min\")\n\n        for epoch in range(100):\n            if epoch > 0 & (epoch % 5 == 0):\n                learning_rate *= 0.9\n            train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n            valid_pred, true_valid_labels, valid_loss = inference_fn(model, valid_loader, device)\n            # logloss_score = log_loss((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n            \n            auc_score = roc_auc_score(true_valid_labels.reshape(-1, 1), valid_pred.reshape(-1, 1))\n            valid_pred = np.median(valid_pred, axis=1)\n            valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n            u_score = utility_score(date=train.loc[te].date.values.reshape(-1),\n                                    weight=train.loc[te].weight.values.reshape(-1),\n                                    resp=train.loc[te].resp.values.reshape(-1),\n                                    action=valid_pred.reshape(-1))\n            u_score_max = utility_score(date=train.loc[te].date.values.reshape(-1),\n                                        weight=train.loc[te].weight.values.reshape(-1),\n                                        resp=train.loc[te].resp.values.reshape(-1),\n                                        action=train.loc[te].action.values.reshape(-1))\n\n            # print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f},max_u_score:{u_score_max:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n                  # f\"time: {(time.time() - start_time) / 60:.2f}min\")\n            print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, valid_loss:{valid_loss:.5f}, auc:{auc_score:.5f},u_score:{u_score:.5f},max_u_score:{u_score_max:.5f}\"\n                  f\"time: {(time.time() - start_time) / 60:.2f}min\")\n\n\n            es(valid_loss, model, model_path=ckp_path)\n            if es.early_stop:\n                print(\"Early stopping\")\n                break\n            \n        models.append(model)\n        \n\n        # if using test data with gap; using test_with_gap dataset instead\n        test_set = MyDataset(test, train_features, targets)\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=1)\n        test_preds, true_test_labels, test_loss = inference_fn(model, test_loader, device)\n        test_preds = np.median(test_preds, axis=1)\n        test_preds = np.where(test_preds >= 0.5, 1, 0).astype(int)\n\n        auc_score = roc_auc_score(test['action'].values.reshape(-1, 1), test_preds)\n        print(\"auc on test set:\", auc_score)\n        u_score = utility_score(date=test.date.values.reshape(-1),\n                                weight=test.weight.values.reshape(-1),\n                                resp=test.resp.values.reshape(-1),\n                                action=test_preds.reshape(-1))\n        print(\"utility score on test set:\", u_score)\n        u_score2 = utility_score(date=test.date.values.reshape(-1),\n                                weight=test.weight.values.reshape(-1),\n                                resp=test.resp.values.reshape(-1),\n                                action=test.action.values.reshape(-1))\n        print(\"max utility score on test set(if using true action):\", u_score2)\n        \n        break # only train 1 model for fast, you can remove it to train 5 folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ckp_path = \"./JSModel_0.pth\"\n# model2 = Model(train_features, targets)\n# model2.to(device)\n# model2.load_state_dict(torch.load(ckp_path))\n\n# test_set = MyDataset(test, train_features, targets)\n# test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n# test_preds, true_test_labels, test_loss = inference_fn(model2, test_loader, device)\n# test_preds = np.median(test_preds, axis=1)\n# test_preds = np.where(test_preds >= 0.5, 1, 0).astype(int)\n\n# auc_score = roc_auc_score(test['action'].values.reshape(-1, 1), test_preds)\n# print(\"auc on test set:\", auc_score)\n# u_score = utility_score(date=test.date.values.reshape(-1),\n#                         weight=test.weight.values.reshape(-1),\n#                         resp=test.resp.values.reshape(-1),\n#                         action=test_preds.reshape(-1))\n# print(\"utility score on test set:\", u_score)\n# u_score2 = utility_score(date=test.date.values.reshape(-1),\n#                         weight=test.weight.values.reshape(-1),\n#                         resp=test.resp.values.reshape(-1),\n#                         action=test.action.values.reshape(-1))\n# print(\"max utility score on test set(if using true action):\", u_score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## evaluate our model using test data (we define)"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not TRAINING:\n    models = []\n    for i in [0, 1, 2, 3, 4]: # for fast inference, you can change 1-->5 to get higher score\n        torch.cuda.empty_cache()\n        device = torch.device(\"cuda:0\")\n        model = Model(train_features, targets)\n        model.to(device)\n        model.eval()\n        ckp_path = f'/kaggle/input/js-models/JSModel_{i}.pth'\n        # ckp_path = f'/kaggle/input/mlp012003weights/online_model{i}.pth'\n        model.load_state_dict(torch.load(ckp_path))\n        models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAINING:\n    models = []\n    for i in [1, 3]: # for fast inference, you can change 1-->5 to get higher score\n        torch.cuda.empty_cache()\n        device = torch.device(\"cuda:0\")\n        model = Model(train_features, targets)\n        model.to(device)\n        model.eval()\n\n        ckp_path = f'./JSModel_{i}.pth'\n        # ckp_path = f'/kaggle/input/mlp012003weights/online_model{i}.pth'\n        model.load_state_dict(torch.load(ckp_path))\n        models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以在下面用50天的test数据集调试"},{"metadata":{"trusted":true},"cell_type":"code","source":"# models = [models[1], models[3], models[2]]\n\nbatch_size = 4096\nlabel_smoothing = 1e-2\nloss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n\ntest_pred = np.zeros((len(test), len(targets)))\ntest_set = MyDataset(test, train_features, targets)\ntest_loader = DataLoader(test_set, batch_size=4096, shuffle=False, num_workers=4)\nfor id_, model in enumerate(models):\n    test_pred_, _, __ = inference_fn(model, test_loader, device) \n    test_pred += test_pred_ / len(models)\n    test_pred_ = np.median(test_pred_, axis=1)\n    test_pred_ = np.where(test_pred_ >= 0.50, 1, 0).astype(int)\n    \n    auc_score = roc_auc_score(test['action'].values.reshape(-1, 1), test_pred_)\n    \n    u_score = utility_score(date=test.date.values.reshape(-1),\n                            weight=test.weight.values.reshape(-1),\n                            resp=test.resp.values.reshape(-1),\n                            action=test_pred_.reshape(-1))\n    print(\"model\", id_, \"auc:\", auc_score, \"u_score:\", u_score)\n\ntest_preds = np.median(test_pred, axis=1)\ntest_preds = np.where(test_preds >= 0.50, 1, 0).astype(int)\n\n\nauc_score = roc_auc_score(test['action'].values.reshape(-1, 1), test_preds)\nu_score = utility_score(date=test.date.values.reshape(-1),\n                        weight=test.weight.values.reshape(-1),\n                        resp=test.resp.values.reshape(-1),\n                        action=test_preds.reshape(-1))\nprint(auc_score, u_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\n\nth = 0.5\n\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        \n        if np.isnan(x_tt.sum()):\n            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean.values.reshape(1, -1)\n\n        feature_inp = pd.DataFrame(x_tt)\n        feature_inp.columns = features\n        feature_inp, _ = add_features(feature_inp,features)\n        feature_inp = torch.tensor(feature_inp.values, dtype=torch.float).to(device)\n        \n        pred = np.zeros((1, len(targets)))\n        for model in models: \n            pred += model(feature_inp).detach().cpu().numpy()\n        pred /= len(models)\n        \n        pred = pred.mean(axis=1).item()\n        pred_df.action = int(pred >= 0)\n        \n    else:\n        pred_df.action = 0\n        \n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}