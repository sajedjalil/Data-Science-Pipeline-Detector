{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook was implemented using the official documentation of Catboost\n\nhttps://catboost.ai/"},{"metadata":{},"cell_type":"markdown","source":"# Kaggle Code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sys\n\nThe used cmds are designed for a GPU env, and I have collected them from various notebooks.\n\nKaggle provides free access to NVidia K80 GPUs in kernels.\n\nA GPU Kernel will give you Tesla P100 16gb VRAM as GPU, with 13gb RAM + 2-core of Intel Xeon as CPU. No-GPU option will give you 4-cores + 16gb RAM, hence more CPU power."},{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess\nfrom ast import literal_eval\nimport multiprocessing\nfrom pynvml import *\nimport torch\n\n\n\ndef run(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    print(out.decode('utf-8').strip())\n    \n\n    \nprint('### CPU ###')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^model name\"')\nprint(\"cpu count       :\", multiprocessing.cpu_count())\n#run('cat /proc/cpuinfo | egrep -m 1 \"^cpu MHz\"')\nrun('cat /proc/cpuinfo | egrep -m 1 \"^cpu cores\"')\n\n\n\n# These codes executes only in a GPU env\nprint('\\n### RAM ###')\n#run('cat /proc/meminfo | egrep \"^MemTotal\"')\nnvmlInit()\nh = nvmlDeviceGetHandleByIndex(0)\ninfo = nvmlDeviceGetMemoryInfo(h)\nprint(f'Total    : {info.total / 1073741824} GB')\nprint(f'Used     : {info.used  / 1073741824} GB')\nprint(f'Free     : {info.free  / 1073741824} GB')\n\n\n\nprint('\\n### OS ###')\nrun('uname -a')\n\n\n\nprint('\\n### GPU ###')\n#run('lspci | grep VGA')\n# setting device on GPU if available, else CPU\nprint('Device name      :', torch.cuda.get_device_name(0))\nprint('Memory Allocated :', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\nprint('Memory Cached    :', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Libs"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n\nimport cudf as cu\nfrom sklearn.metrics import classification_report\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.feature_selection import SelectFromModel\nfrom tqdm.notebook import tqdm\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata_cudf = cu.read_csv('/kaggle/input/jane-street-market-prediction/train.csv') # cudf is sensitivee about tyoe and use at and iat instead of loc and iloc\ndata = data_cudf.to_pandas()\ndel data_cudf\n\n#backup = data\n#train = backup_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Nan"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # drop the colomns that have +10% na\n\n# print(train.columns.shape)\n# missing_val = pd.DataFrame(train.isna().sum().sort_values(ascending=True)*100/train.shape[0],columns=['missing %'])[:138-14]\n# missing_val.style.background_gradient(cmap='Oranges_r')\n# features = missing_val.index\n# print(features.shape)\n\n# train = train[features]\n# print(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [c for c in data.columns if 'feature' in c]\nfor i in features:\n    x = data[i].mean()     \n    data[i] = data[i].fillna(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\n\n# prepare the data before splitting to train and test\ndata = data[data['weight']!=0]\ndata['action'] = (data['resp']>0)*1\nprint(data.shape)\n#features = data.columns.str.contains('feature')\n\n# with we will create the model\ntrain = data.sample(frac = 0.75, random_state = 73)\nprint(train.shape)\n#train = data.reset_index(drop=True) # if we keep it here it will raise an error in the valid set, cause if the change of the index\n\nvalid = data.drop(train.index) # take the data that does not exist in train\nprint(valid.shape)\n\n\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\n\n# it will help us track the model\n#test  = data.tail(500).reset_index(drop=True)\n\n# split the train set\nX_train = train.loc[:,features] \ny_train = train.loc[:, 'action']\n\n# split the test set\n#X_test = test.loc[:, features]\n#y_test = test.loc[:, 'action']\n\n\nX_valid = valid.loc[:, features]\ny_valid = valid.loc[:, 'action']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n### using CatBoostClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nselector = CatBoostClassifier(thread_count = -1, task_type = \"GPU\", devices = '-1', random_seed = 73).fit(Pool(X_train, y_train), verbose = 100) # without task_type='GPU' the fit time is 42min with GPU is 46s\nlist_of_tuples = list(zip(X_train.columns.values, selector.get_feature_importance())) \n\ndf = pd.DataFrame(list_of_tuples).sort_values(by = [1]).reset_index(drop = True).rename(columns = {0: 'feat_labels', 1: 'feature_importances'})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Update Splitted data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.tail(111) # features importance > 0.003\nfeatures = list(df1[\"feat_labels\"])\n\nX_train = X_train.loc[:, features]\n#X_test = X_test.loc[:, features]\n#print(train.shape)\nprint(X_train.shape)\n#print(X_test.shape)\n\nX_valid = X_valid.loc[:, features]\nprint(X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper Pram tuning\n### using CatBoostClassifier Randomized Search\n##### dont run this it takes alot of time. The result is used in the next cell.\n### V1: I have changed some grid params"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n\n#model = CatBoostClassifier(\n#                thread_count = -1, task_type = \"GPU\", devices = '-1', random_seed = 73, \n#                bootstrap_type = 'Poisson', verbose = 10, name = 'V1')\n\n# my core number is 4\n# I have only one GPU, so no need to -1 for activating all the gpu's but instead 0:1\n# save_snapshot=True, snapshot_file=\"V0\", snapshot_interval=600 not supported for randomized search\n\n#grid = {  'depth'           :[3, 1, 2, 6, 4, 5, 7, 8, 9, 10],\n#          'iterations'      :[1000, 250, 100, 500],\n#          'learning_rate'   :[0.03, 0.001, 0.01, 0.1, 0.2, 0.3],\n#          'l2_leaf_reg'     :[3, 1, 5, 10, 100],\n#          'border_count'    :[32, 5, 10, 20, 50, 100, 200],\n#          }\n# 'loss_function'   :['Logloss', 'CrossEntropy'], currently not supported in grid search\n# 'thread_count'    :4 error non iterable\n# 'ctr_border_count':[50,5,10,20,100,200] error not a map\n\n#grid_search_result = model.randomized_search(  grid, \n#                                               Pool(X_train, y_train), # X = X_train, y = y_train\n#                                               cv         = 3,\n#                                               n_iter     = 10,\n#                                               refit      = False,\n#                                               shuffle    = True,\n#                                               stratified = True,\n#                                               train_size = 0.75,\n#                                               plot       = True)\n# search_by_train_test_split = Split the source dataset into train and test parts. \n# Models are trained on the train part, while parameters are compared by the loss function score on the test dataset.\n\n\n#print(grid_search_result['params'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# {'border_count': 200, 'depth': 10, 'l2_leaf_reg': 1, 'iterations': 250, 'learning_rate': 0.5}\n\n# Dataset processing.The fastest way to pass the features data to the Pool\ntrain_data = Pool(data = X_train,\n                  label = y_train,\n                  ) #weight=[0.1, 0.2...]\n\nvalid_data = Pool(data = X_valid,\n                  label = y_valid)\n\nmodel = CatBoostClassifier(border_count = 32, depth = 5, l2_leaf_reg = 3.5, \n                           thread_count = -1,iterations = 100, learning_rate = 0.5, \n                           task_type = \"GPU\",devices = '0:1', bootstrap_type = 'Poisson', \n                           random_seed = 73, verbose = 100, name = 'V3', \n                           use_best_model=True, loss_function= 'Logloss', eval_metric='AUC',) \n# rsm=0.98 not supported on GPU\n# use_best_model=True This option requires a validation dataset to be provided. \n# Use the validation dataset to identify the iteration with the optimal value of the metric specified in  --eval-metric (eval_metric).\n\nmodel.fit(train_data, eval_set = valid_data) # eval_set=(X_test, y_test) Pool(X_train, y_train)\n#y_pred = model.predict(X_test)\n\n#print(\"\\n\\n\",model.predict(X_test))\n#print(\"\\n\\n\",classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save and Load"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.save_model(fname = \"/model_v2\",\n#                   format=\"cbm\",\n#                   export_parameters=None, # Additional format-dependent parameters for Apple CoreML ONNX-ML PMML\n#                   pool=None) # This parameter is required if the model contains categorical features and the output format is cpp, python, or JSON.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = CatBoostClassifier()      \n#model.load_model(\"/model_v1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    \n    X_test = test_df.loc[:, features]\n    \n    for i in features:\n        x = X_test[i].mean()     \n        X_test[i] = X_test[i].fillna(x)\n\n    y_preds = model.predict(X_test)\n    \n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}