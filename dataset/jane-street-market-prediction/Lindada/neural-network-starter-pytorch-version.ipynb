{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Neural Network Starter Pytorch Version\n\nThis kernel is a pytorch version of yirun's https://www.kaggle.com/gogo827jz/jane-street-neural-network-starter kernel.\nIf you think this kernel is helpful, please upvote!!"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc, random, time\n# import cudf\nimport pandas as pd\nimport numpy as np\n# import cupy as cp\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"print('Loading...')\ntrain = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nprint('Filling...')\nf_mean = train[features[1:]].mean()\ntrain = train.loc[train.weight > 0].reset_index(drop = True)\ntrain[features[1:]] = train[features[1:]].fillna(f_mean)\ntrain['action'] = (train['resp'] > 0).astype('int')\n\nprint('Converting...')\n# train = train.to_pandas()\nf_mean = f_mean.values#.get()\nnp.save('f_mean.npy', f_mean)\n\nprint('Finish.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.10143786981358652)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 384)\n        self.batch_norm1 = nn.BatchNorm1d(384)\n        self.dropout1 = nn.Dropout(0.19720339053599725)\n\n        self.dense2 = nn.Linear(384, 896)\n        self.batch_norm2 = nn.BatchNorm1d(896)\n        self.dropout2 = nn.Dropout(0.2703017847244654)\n\n        self.dense3 = nn.Linear(896, 896)\n        self.batch_norm3 = nn.BatchNorm1d(896)\n        self.dropout3 = nn.Dropout(0.23148340929571917)\n\n        self.dense4 = nn.Linear(896, 394)\n        self.batch_norm4 = nn.BatchNorm1d(394)\n        self.dropout4 = nn.Dropout(0.2357768967777311)\n\n        self.dense5 = nn.Linear(394, 1)\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout4(x)\n\n        x = self.dense5(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MarketDataset:\n    def __init__(self, df):\n        self.features = df[features].values\n\n        self.label = (df['resp'] > 0).astype('int').values.reshape(-1, 1)\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return {\n            'features': torch.tensor(self.features[idx], dtype=torch.float),\n            'label': torch.tensor(self.label[idx], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        features = data['features'].to(device)\n        label = data['label'].to(device)\n        outputs = model(features)\n        loss = loss_fn(outputs, label)\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n\n    return final_loss\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        features = data['features'].to(device)\n\n        with torch.no_grad():\n            outputs = model(features)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds).reshape(-1)\n\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    # print('weight: ', weight)\n    # print('resp: ', resp)\n    # print('action: ', action)\n    # print('weight * resp * action: ', weight * resp * action)\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 4096\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nstart_time = time.time()\noof = np.zeros(len(train['action']))\ngkf = GroupKFold(n_splits = 5)\nfor fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n    train_set = MarketDataset(train.loc[tr])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n    valid_set = MarketDataset(train.loc[te])\n    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    torch.cuda.empty_cache()\n    device = torch.device(\"cuda:0\")\n    model = Model()\n    model.to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n    \n    ckp_path = f'JSModel_{fold}.pth'\n    \n    es = EarlyStopping(patience=3, mode=\"max\")\n    for epoch in range(10):\n        train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n        valid_pred = inference_fn(model, valid_loader, device)\n        auc_score = roc_auc_score((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n        logloss_score = log_loss((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n        u_score = utility_score_bincount(date=train.loc[te].date.values, weight=train.loc[te].weight.values, resp=train.loc[te].resp.values, action=valid_pred)\n\n        print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n              f\"time: {(time.time() - start_time) / 60:.2f}min\")\n        \n        es(auc_score, model, model_path=ckp_path)\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n#     break # only train 1 model for fast, you can remove it to train 5 folds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Models\n\nJust use three models to reduce running time."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nfor i in range(5): # for fast inference, you can change 1-->5 to get higher score\n    torch.cuda.empty_cache()\n    device = torch.device(\"cuda:0\")\n    model = Model()\n    model.to(device)\n    model.eval()\n    \n    ckp_path = f'JSModel_{i}.pth'\n    model.load_state_dict(torch.load(ckp_path))\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_mean = np.load('./f_mean.npy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env()\nenv_iter = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"opt_th = 0.5\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = 0.\n        \n        for i, clf in enumerate(models):\n            if i == 0:\n                pred = model(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / len(models)\n            else:\n                pred += model(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / len(models)\n        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Just upvoting if it helps!!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}