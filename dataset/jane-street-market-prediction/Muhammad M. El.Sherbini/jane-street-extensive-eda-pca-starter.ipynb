{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# <font color='darkblue'>Jane Street Market Prediction</font>\n### This is an extensive data analysis for the jane street market dataset, this notebook will go through the train and features csv's for an extensive exploratory data analysis, Also some data cleaning and preprocessing will be done along the way.\n### <font color='darkred'>Please note the following:</font> \n- this is a relatively large notebook with a lot of exhaustive analytics as it meant to be that way to give a comprehensive understanding of the dataset so if you are to copy and run it please note that it will take a considerable amount of time to run ( around 50 minutes) so be patient.\n- this is an exploratory data analysis and not explanatory one so some figures will be kind of complicated, long and full of data as this notebook meant to help data scientists here on kaggle in their model building process.\n- I didn't discuss the hypothesis of the data being synthetic as this point was addressed by other kaggler in detail, and personally I think the pattern in features are because the fact that many financial metrics are correlated with each other.\n- The PCA / Clustering part is just a starter for exploring the possible alternatives to reduce the features space size and find some patterns in the dadataset. \n\n\n## [The EDA](#eda) will be devided to three main parts:\n      \n\n## 1- [General EDA](#general)\n  >- [Resp Data Analysis](#resp)\n  >- [Date](#date)\n  >- [Weight](#weight)\n\n## 2 - [Features Data Analysis](#features)\n  >- [Null Values](#nulls)\n  >- [Cumulative growth](#growth)\n  >- [Multicollinearity](#multicollinearity)\n  >- [Outliers](#outlier)\n  >- [Feature 0](#f0)\n\n\n## 3 - [PCA & Clustering](#pcacls)\n  >- [PCA](#pca)\n  >- [Clustering](#cluster)"},{"metadata":{},"cell_type":"markdown","source":"<h1><center>Let's dive right in!</center></h1>\n\n![](https://media1.tenor.com/images/ed3ccde29b0efef4a88e13353f6923ba/tenor.gif)\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport sys\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 140)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\nsns.set_style('darkgrid')\n\nfrom sklearn.preprocessing import StandardScaler as scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import k_means\nfrom sklearn.model_selection import train_test_split as split \nfrom sklearn.model_selection import GridSearchCV as Grid\nimport xgboost as xgb\nfrom sklearn.metrics import (roc_auc_score, precision_score, recall_score, f1_score,\n                             confusion_matrix, accuracy_score, roc_curve, auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Loading Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/jane-street-market-prediction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving the original dataset length\norg_len = len(df)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying to cut corners to save some memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.feature_0 = df.feature_0.astype(np.int8)\ndf.date= df.date.astype(np.int16)\ndf.ts_id = df.ts_id.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following code loop through the dataframe to change float64 columns to float32 only if there is a really few amount of data (a very conservative threshold of 0.1%) between -.0001:.0001 to avoid hurting accuracy of small values columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df:\n    if df[i].dtype == np.float64:\n        if (((df[i] < .0001) & (df[i] > -.0001)).mean()) > .001:\n            print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df:\n    if df[i].dtype == np.float64:\n        if (((df[i] < .0001) & (df[i] > -.0001)).mean()) < .001:\n            df[i] = df[i].astype(np.float32)\n            gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### we can see a noticeable difference in memory usage"},{"metadata":{},"cell_type":"markdown","source":"### Assuring that Data is stored by date"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by= ['date','ts_id'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding target"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['action'] = np.where(df['resp'] > 0,1,0)\ndf.action = df.action.astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eda\"></a>\n# **EDA**"},{"metadata":{},"cell_type":"markdown","source":"<a id='general'></a>\n# **General Data Analysis**"},{"metadata":{},"cell_type":"markdown","source":"<a id='resp'></a>\n## **Resp Data Analysis** "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,6))\nax = plt.subplot(1,1,1)\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']].sum().cumsum().plot(ax=ax)\nplt.title('Cumulative Sum of Different RESP\\'s',fontsize=18)\nplt.xlabel('Date',fontsize=14)\nplt.axvspan(0,92,linestyle=':',linewidth=2,label='first 92 days',color='darkorange',alpha=.2)\nplt.legend(fontsize=12,ncol=3,loc=2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It can be noticed the there were more gains in the first 92 days which leads to the insight mentioned by many kaggler that it may be a good idea to drop the observations before this point,\n> ### We can also notice that resp_4 has the highest cumulative sum on the other hand resp_1 has the smallest cumulative sum."},{"metadata":{},"cell_type":"markdown","source":"### Now we plot the average of each Resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(df.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean(),\n              x= df.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4','resp']].mean().index,\n              y= ['resp_1', 'resp_2', 'resp_3', 'resp_4','resp'],\n              title= '\\naverage Resp per day')\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Avg Resp'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,((ax11,ax12,ax13),(ax21,ax22,ax23),(ax31,ax32,ax33),(ax41,ax42,ax43),(ax51,ax52,ax53)) = plt.subplots(5,3,figsize=(18,24))\nplt.subplots_adjust(hspace=0.35)\nax11.hist(df.resp,bins=150, color='darkblue',alpha=.6)\nax11.axvline(df.resp.mean()+df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\nax11.axvline(df.resp.mean()-df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp.plot.hist(bins= 150,ax=ax12,color='darkblue',alpha=.6)\nax12.axvline(df.resp.mean()+df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\nax12.axvline(df.resp.mean()-df.resp.std(),color='darkorange',linestyle=':',linewidth=2)\nax12.set_xlim(-.08,.08)\nax13.hist(df.resp,bins=150, color='darkblue',alpha=.6)\nax13.set_yscale('log')\nskew= round(df.resp.skew(),4)\nkurt= round(df.resp.kurtosis())\nstd1= round((((df.resp.mean()-df.resp.std()) < df.resp ) & (df.resp < (df.resp.mean()+df.resp.std()))).mean()*100,2)\nprops = dict(boxstyle='round', facecolor='white', alpha=0.5)\nax11.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp.mean(),4),round(df.resp.std(),4),skew,kurt,std1),\n         transform=ax11.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax11.set_title('Resp Hist Normal scale',fontsize=14)\nax12.set_title('Resp Hist normal scale zoomed',fontsize=14)\nax13.set_title('Resp Hist with freq on a log scale',fontsize=14);\nax11.set_xlabel('')\nax11.set_ylabel('')\nax12.set_xlabel('')\nax12.set_ylabel('')\nax13.set_xlabel('')\nax13.set_ylabel('')\nax21.hist(df.resp_1,bins=150,color='darkblue',alpha=.6)\nax21.axvline(df.resp_1.mean()+df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\nax21.axvline(df.resp_1.mean()-df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_1.plot.hist(bins= 150,ax=ax22,color='darkblue',alpha=.6)\nax22.axvline(df.resp_1.mean()+df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\nax22.axvline(df.resp_1.mean()-df.resp_1.std(),color='darkorange',linestyle=':',linewidth=2)\nax22.set_xlim(-.08,.08)\nax23.hist(df.resp_1,bins=150,color='darkblue',alpha=.6)\nax23.set_yscale('log')\nskew= round(df.resp_1.skew(),4)\nkurt= round(df.resp_1.kurtosis())\nstd1= round((((df.resp_1.mean()-df.resp_1.std()) < df.resp_1 ) & (df.resp_1 < (df.resp_1.mean()+df.resp_1.std()))).mean()*100,2)\nax21.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_1.mean(),4),round(df.resp_1.std(),4),skew,kurt,std1),\n         transform=ax21.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax21.set_title('Resp_1 Hist Normal scale',fontsize=14)\nax22.set_title('Resp_1 Hist normal scale zoomed',fontsize=14)\nax23.set_title('Resp_1 Hist with freq on a log scale',fontsize=14);\nax21.set_xlabel('')\nax21.set_ylabel('')\nax22.set_xlabel('')\nax22.set_ylabel('')\nax23.set_xlabel('')\nax23.set_ylabel('')\nax31.hist(df.resp_2,bins=150,color='darkblue',alpha=.6)\nax31.axvline(df.resp_2.mean()+df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\nax31.axvline(df.resp_2.mean()-df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_2.plot.hist(bins= 150,ax=ax32,color='darkblue',alpha=.6)\nax32.axvline(df.resp_2.mean()+df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\nax32.axvline(df.resp_2.mean()-df.resp_2.std(),color='darkorange',linestyle=':',linewidth=2)\nax32.set_xlim(-.08,.08)\nax33.hist(df.resp_2,bins=150, color='darkblue',alpha=.6)\nax33.set_yscale('log')\nskew= round(df.resp_2.skew(),4)\nkurt= round(df.resp_2.kurtosis())\nstd1= round((((df.resp_2.mean()-df.resp_2.std()) < df.resp_2 ) & (df.resp_2 < (df.resp_2.mean()+df.resp_2.std()))).mean()*100,2)\nax31.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_2.mean(),4),round(df.resp_2.std(),4),skew,kurt,std1),\n         transform=ax31.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax31.set_title('Resp_2 Hist Normal scale',fontsize=14)\nax32.set_title('Resp_2 Hist normal scale zoomed',fontsize=14)\nax33.set_title('Resp_2 Hist with freq on a log scale',fontsize=14);\nax31.set_xlabel('')\nax31.set_ylabel('')\nax32.set_xlabel('')\nax32.set_ylabel('')\nax33.set_xlabel('')\nax33.set_ylabel('')\nax41.hist(df.resp_3, color='darkblue',alpha=.6,bins=150)\nax41.axvline(df.resp_3.mean()+df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\nax41.axvline(df.resp_3.mean()-df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_3.plot.hist(bins=150, color='darkblue',alpha=.6,ax=ax42)\nax42.axvline(df.resp_3.mean()+df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\nax42.axvline(df.resp_3.mean()-df.resp_3.std(),color='darkorange',linestyle=':',linewidth=2)\nax42.set_xlim(-.08,.08)\nax43.hist(df.resp_3, color='darkblue',alpha=.6,bins=150)\nax43.set_yscale('log')\nskew= round(df.resp_3.skew(),4)\nkurt= round(df.resp_3.kurtosis())\nstd1= round((((df.resp_3.mean()-df.resp_3.std()) < df.resp_3 ) & (df.resp_3 < (df.resp_3.mean()+df.resp_3.std()))).mean()*100,2)\nax41.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_3.mean(),4),round(df.resp_3.std(),4),skew,kurt,std1),\n         transform=ax41.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax41.set_title('Resp_3 Hist Normal scale',fontsize=14)\nax42.set_title('Resp_3 Hist normal scale zoomed',fontsize=14)\nax43.set_title('Resp_3 Hist with freq on a log scale',fontsize=14);\nax41.set_xlabel('')\nax41.set_ylabel('')\nax42.set_xlabel('')\nax42.set_ylabel('')\nax43.set_xlabel('')\nax43.set_ylabel('')\nax51.hist(df.resp_4,bins=150, color='darkblue',alpha=.6)\nax51.axvline(df.resp_4.mean()+df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\nax51.axvline(df.resp_4.mean()-df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\ndf.resp_4.plot.hist(bins= 150,color='darkblue',alpha=.6,ax=ax52)\nax52.axvline(df.resp_4.mean()+df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\nax52.axvline(df.resp_4.mean()-df.resp_4.std(),color='darkorange',linestyle=':',linewidth=2)\nax52.set_xlim(-.08,.08)\nax53.hist(df.resp_4,bins=150,color='darkblue',alpha=.6)\nax53.set_yscale('log')\nskew= round(df.resp_4.skew(),4)\nkurt= round(df.resp_4.kurtosis())\nstd1= round((((df.resp_4.mean()-df.resp_4.std()) < df.resp_4 ) & (df.resp_4 < (df.resp_4.mean()+df.resp_4.std()))).mean()*100,2)\nax51.text(.02,.96,'μ = {}\\nstd = {}\\nskewness = {}\\nkurtosis = {}\\n% values in 1 std = {}%'.format(round(df.resp_4.mean(),4),round(df.resp_4.std(),4),skew,kurt,std1),\n         transform=ax51.transAxes, verticalalignment='top',bbox=props,fontsize=10)\nax51.set_title('Resp_4 Hist Normal scale',fontsize=14)\nax52.set_title('Resp_4 Hist normal scale zoomed',fontsize=14)\nax53.set_title('Resp_4 Hist with freq on a log scale',fontsize=14)\nax51.set_xlabel('')\nax51.set_ylabel('')\nax52.set_xlabel('')\nax52.set_ylabel('')\nax53.set_xlabel('')\nax53.set_ylabel('')\nfig.suptitle('RESPs Historgrams on Different Scales',fontsize=18,y=.92);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']],corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that Resp is highly related to Resp_4\nalso Resp_1 and Resp_2 are highly relted to each other \nfrom the relation shown in this figure and the standard deviation and distribution shown in the figure before this one we can assume that Resp is more related to longer time horizon invest as longer time horizon are associated with more return and higher risk\n\n### [The Basics of Investment Time Horizons](https://www.investopedia.com/terms/t/timehorizon.asp)\n> ####  An Investment Time Horizon is the period where one expects to hold an investment for a specific goal. Investments are generally broken down into two main categories: stocks (riskier) and bonds (less risky). The longer the Time Horizon, the more aggressive, or riskier portfolio, an investor can build. The shorter the Time Horizon, the more conservative, or less risky, the investor may want to adopt. \n\n[source: investopedia.com](https://www.investopedia.com/terms/t/timehorizon.asp)"},{"metadata":{},"cell_type":"markdown","source":"<a id='date'></a>\n## **Date**"},{"metadata":{},"cell_type":"markdown","source":"### Checking the unique values of date"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.date.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### The Date seem to contain 2 years of trading data since the trading days of the year are approximately 252 : 253 days\nhttps://en.wikipedia.org/wiki/Trading_day"},{"metadata":{},"cell_type":"markdown","source":"### Now we check the Resp Amount and Number of operations for each day"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.area(data_frame= df.groupby('date')[['resp']].count(),title='Number of operation per day')\nfig.update_traces( showlegend = False)\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Number of operations'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.area(data_frame= df.groupby('date')[['resp']].sum(),title='Resp sum of operation per day')\nfig.update_traces( showlegend = False)\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Resp sum'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It can be noticed that Resp has many fluctuation"},{"metadata":{},"cell_type":"markdown","source":"### We will create a moving standard deviation of 20 days (which is a month of trading) for the average of resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_df = df.groupby('date')[['resp']].mean()\nstd20 = []\nfor i in range(len(date_df)):\n    if i <20:\n        std20.append(np.nan)\n    else:\n        moving_std = date_df['resp'][i-20:i].std()\n        std20.append(moving_std)\ndate_df['moving_std'] = std20\ndate_df.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(data_frame=date_df,y=['resp','moving_std'],title='Average Resp & 20 day moving standard deviation')\nfig.layout.xaxis.title = 'Day' \nfig.layout.yaxis.title = 'Avg Resp'\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we check the standard deviation of each resp for each day"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(22,14))\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4']].std().plot(ax=ax1,color=['steelblue','darkorange','red','green'],alpha=.8)\nax1.axvspan(0,92,linestyle=':',linewidth=2,label='first 92 days',color='yellow',alpha=.1)\ndf.groupby('date')[['resp_1', 'resp_2', 'resp_3', 'resp_4']].std().plot.kde(ax=ax2)\nfig.suptitle('Resp\\'s Std',fontsize=18,y=.96)\nax2.set_xlabel('')\nax1.set_xlabel('')\nax2.set_title('kde of each resp std', fontsize=14)\nax1.set_title('std of Resp\\'s for each trading day',fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### As it was mentioned before the standard deviation seems to increase with resp mostly related to longer time horizon investments\n> ### It can also be noticed that the deviation was kind of higher in the first 100 days as it was mentioned by many kagglers that there may was some kind of trading model adjustment done after the 80th day."},{"metadata":{},"cell_type":"markdown","source":"<a id='weight'></a>\n## **Weight**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,7))\ngrid =  gridspec.GridSpec(2,3,figure=fig,hspace=.3,wspace=.2)\nax1 = fig.add_subplot(grid[0, 0])\nax2 = fig.add_subplot(grid[0, 1])\nax3 = fig.add_subplot(grid[1, 0])\nax4 = fig.add_subplot(grid[1, 1])\nax5 = fig.add_subplot(grid[:, 2])\nsns.boxplot(x = df.weight,width=.5,ax=ax1)\nax2.hist(df.weight, color='#404788ff',alpha=.6, bins= list([-.05] + list(10**np.arange(-2,2.24,.05))))\nax2.set_xscale('symlog')\nax2.set_xlim(-.05,227)\nsns.boxplot(x = df.weight[df.weight != 0],width=.5,ax=ax3)\nax1.set_title('Weights including zero weights',fontsize=14)\nax3.set_title('Weights not including zero weights',fontsize=14)\nax2.set_title('Weights including zero weights (log)',fontsize=14)\nax4.set_title('Weights not including zero weights (log)',fontsize=14)\nprops = dict(boxstyle='round', facecolor='white', alpha=0.4)\nax1.text(.2,.9,'μ = {}    std = {}\\nmin = {}    max = {}'.format(round(df.weight.mean(),3),round(df.weight.std(),3),round(df.weight.min(),3),round(df.weight.max(),3)),\n         transform=ax1.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax3.text(.2,.9,'μ = {}        std = {}\\nmin = {}    max = {}'.format(round(df.weight[df.weight != 0].mean(),3),round(df.weight[df.weight != 0].std(),3),\n                                                              round(df.weight[df.weight != 0].min(),3),round(df.weight[df.weight != 0].max(),3)),\n         transform=ax3.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax4.hist(df.weight[df.weight !=0],color='#404788ff',alpha=.6,bins=10**np.arange(-2.16,2.24,.05))\nax4.set_xscale('log')\nax4.set_xticks((.01,.03,.1,.3,1,3,10,30,100))\nax4.set_xticklabels((.01,.03,.1,.3,1,3,10,30,100))\nax5.pie(((df.weight==0).mean(),(1-(df.weight==0).mean())),startangle=300,wedgeprops=dict(width=0.5),\n        labels=('Zeros\\n{}%'.format(round((df.weight==0).mean()*100,2)),'Nonzeros\\n{}%'.format(round((1-(df.weight==0).mean())*100,2))),\n        textprops={'fontsize': 12},colors=['#404788ff','#55c667ff'])\nax5.set_title('Zeros vs non-zero weights',fontsize=14)\nax1.set_xlabel('')\nax2.set_xlabel('')\nax3.set_xlabel('')\nax2.set_ylabel('')\nax5.set_ylabel('')\nax4.set_xlabel('');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nfig.suptitle('Nonzero weights histogram in different scales',fontsize=18)\nax1 = plt.subplot(3,1,1)\nax1.hist(df.weight[df.weight !=0],color='darkblue',alpha=.7, bins=10**np.arange(-2.16,2.23,.05))\nplt.xscale('log')\nplt.xticks((.01,.03,.1,.3,1,3,10,30,100),(.01,.03,.1,.3,1,3,10,30,100))\nax2 = plt.subplot(3,1,2)\nsns.distplot(df.weight[df.weight != 0], color='darkblue', bins=400, ax=ax2) \nax3 = plt.subplot(3,1,3)\nax3.hist(df.weight[(df.weight !=0) & (df.weight < 3.197 )],color='darkblue',alpha=.7, bins=200)\nax3.set_xlim(0,3.3)\nax2.set_xlabel('') \nax1.set_title('All values (log-scale)',fontsize=14)\nax2.set_title('kde of the distribution',fontsize=14)\nax3.set_title('75% of the Values',fontsize=14)\nplt.subplots_adjust(hspace=.4);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A deeper look at outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(16,8))\nfig.suptitle('Weight outliers',fontsize=18)\nsns.boxplot(df.weight,width=.5, ax=ax1)\nax1.axvline(np.percentile(df.weight,95), color= 'green',label='95.0%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99), color= 'darkblue',label='99.0%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99.9), color= 'darkorange',label='99.9%',linestyle=':',linewidth=3)\nax1.axvline(np.percentile(df.weight,99.99), color= 'magenta',label='99.99%',linestyle=':',linewidth=3)\nax1.legend(fontsize=13)\nsns.boxplot(df.weight[df.weight !=0],width=.5, ax=ax2)\nax2.axvline(np.percentile(df.weight[df.weight !=0],95), color= 'green',label='95.0%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99), color= 'darkblue',label='99.0%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99.9), color= 'darkorange',label='99.9%',linestyle=':',linewidth=3)\nax2.axvline(np.percentile(df.weight[df.weight !=0],99.99), color= 'magenta',label='99.99%',linestyle=':',linewidth=3)\nax2.legend(fontsize=13)\nax1.set_title('All weights', fontsize= 14)\nax2.set_title('Non-zero weights', fontsize= 14)\nax1.set_xlabel('')\nax2.set_xlabel('');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=df, x='resp',y='weight', color= 'blue', alpha=.3)\nplt.title('Resp vs Weight\\ncorrelation={}'.format(round(df.weight.corr(df.resp),4)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can see that weight is not linearly correlated with Resp but it's clear that higher weight are only associated with low Resp values"},{"metadata":{},"cell_type":"markdown","source":"<a id='features'></a>\n# Features data analysis"},{"metadata":{},"cell_type":"markdown","source":"### Loading the features csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_f =  pd.read_csv('../input/jane-street-market-prediction/features.csv')\ndf_f.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df_f.set_index('feature').T.sum(), title='Number of tags for each feature')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces( showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='nulls'></a>\n## **Exploring the Null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(x = df.isnull().sum().index,y= df.isnull().sum().values,title= 'Number of Null values')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.yaxis. dtick = 100000\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.layout.xaxis.showgrid = True\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding out the features with missing values more than 10 %"},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls = df.isnull().sum()\nnulls_list = list(nulls[nulls >(0.1 * len(df))].index)\nnulls_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **Looking into the relationship between these features since there is kind of pattern in the number of null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['resp','resp_1','resp_2','resp_3','resp_4','weight']+nulls_list].corr().style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Since the number of null values in these columns are huge (larger than one quarter of a million!) we will be dropping features with more than 10% null values since there is no correlation with any of these features and Resp's and weight."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=nulls_list,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### now concerning the remaining nulls we will look firstly to the coefficient of variation"},{"metadata":{"trusted":true},"cell_type":"code","source":"(df.iloc[:,7:-2].std() / df.iloc[:,7:-2].mean()).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### the coefficient of variation seems unreliable due to the value of the mean being near to zero"},{"metadata":{},"cell_type":"markdown","source":"### Now we can take a bird's-eye view  of features distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[:,7:-2].hist(bins=100,figsize=(20,74),layout=(29,4));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **The following code will make a grid of horizontal box plot with the mean ploted too to get a comprehensive solid understanding of the features distributions**\n> #### please note that I used customized 0.1%:99.9% whisker to show extreme outliers since the data is strongly centered."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,80))\nfig.suptitle('Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\nfeatstr = [i for i in df.columns[7:-2]]\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= df[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.axvline(df[featstr[counter]].mean(),color= 'darkorange', label='Mean', linestyle=':',linewidth=3)\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        counter += 1\n        gc.collect()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that there is a lot of outliers affecting the distribution of each feature.\n### Also since the majority of values are heavily centerd around the mean, we will fill null values using the mean."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(df.mean(axis=0),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='growth'></a>\n## Features growth "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('date')[featstr].mean().cumsum().plot(layout=(29,4),subplots=True,figsize=(20,82),xlabel='')\nfig = plt.gcf()\nfig.text(0.5, 0.19, 'Date',ha='center', fontsize = 24)\nfig.suptitle('Cumulative features means per day',fontsize=24,y=.886);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Many features cumulative mean seem to be linearly growing but some features like 81, 82, and 83 are actually decreasing, and there are some features that fluctuate like feature 3"},{"metadata":{},"cell_type":"markdown","source":"<a id='multicollinearity'></a>\n## Correlation between features"},{"metadata":{},"cell_type":"markdown","source":"### First we make a correlation dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.iloc[:,7:-2].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting a heatmap for features correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,12))\nax = plt.subplot(1,1,1)\nsns.heatmap(corr,ax= ax, cmap='coolwarm');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">### **It looks like there is a lot of multicollinearity between features and also it looks like there is a pattern of couples in the features space and this pattern is kind of broke at some features  like feature_41**"},{"metadata":{"trusted":true},"cell_type":"code","source":"featstr2 = [ i for i in featstr if i not in ['feature_41','feature_64']]\nlen(featstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(22,44))\ngrid =  gridspec.GridSpec(12,5,figure=fig,hspace=.5,wspace=.2)\ncounter = 1\nfor i in range(12):\n    for j in range(5):\n        if counter == 113:\n            break\n        subf = fig.add_subplot(grid[i, j]);\n        sns.scatterplot(x= df[featstr2[counter]], y = df[featstr2[counter+1]], ax= subf);\n        cor = round(df[featstr2[counter]].corr(df[featstr2[counter+1]]) * 100,2)\n        subf.set_xlabel('')\n        subf.set_ylabel('')\n        subf.set_title('{} & {}\\nCorrelation = {}%'.format(featstr2[counter],featstr2[counter+1],cor),fontsize=14)\n        counter += 2\n        gc.collect();  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### As expected since these are finance related features a lot of features are highly correlate with each other"},{"metadata":{},"cell_type":"markdown","source":"### Now we explore highly correlated groups of features"},{"metadata":{},"cell_type":"markdown","source":"#### We start off with featrues: **[feature_19, feature_20, feature_21, feature_22, feature_23, feature_24, feature_25, feature_26, feature_29]** since there is kind of multicollinearity cluster"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6)) \nsns.heatmap(df[featstr2[15:23]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[featstr2[15:23]],corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Despite the fact that Pearson coefficients of correlation are really high between these features the relationships are not completely linear, also it can be noticed that the outliers affect the shape of scatter plots."},{"metadata":{},"cell_type":"markdown","source":"### Now we check the correlation again but between other group "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6)) \nsns.heatmap(df[featstr2[23:31]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[featstr2[23:31]],corner=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### things looks almost the same with the other cluster, it  also worth mentioning that both of these features clusters are negatively correlated with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,6)) \nsns.heatmap(df[featstr2[15:31]].corr(),center=0,cmap='coolwarm',annot=True,cbar=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> <a id='outlier'></a>\n## Outliers"},{"metadata":{},"cell_type":"markdown","source":"### First we take a look at the mean of features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].mean(), title='Features mean values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].max(), title='Features Max Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].min(), title='Features Min Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1,ax2,ax3)= plt.subplots(3,1,figsize=(10,12))\nplt.subplots_adjust(hspace=.3)\nsns.distplot(df[featstr].max(),ax= ax1 )\nsns.distplot(df[featstr].min(),ax= ax2)\nsns.distplot(df[featstr].mean(),ax= ax3)\nfig.suptitle('distribution of mean max and min for features',fontsize=16)\nax1.set_title('distribution  of features max values',fontsize=14)\nax1.text(.82,.56,'std = {}'.format(round(df[featstr].max().std(),2)),transform=ax1.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax2.set_title('distribution  of features min values',fontsize=14)\nax2.text(.82,.56,'std = {}'.format(round(df[featstr].min().std(),2)),transform=ax2.transAxes, verticalalignment='top',bbox=props,fontsize=12)\nax3.set_title('distribution  of features mean values',fontsize=14)\nax3.text(.82,.56,'std = {}'.format(round(df[featstr].mean().std(),2)),transform=ax3.transAxes, verticalalignment='top',bbox=props,fontsize=12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can see that despite the fact the mean values are not that different from each other the min and max values are very deviated with highly skewed distribution"},{"metadata":{},"cell_type":"markdown","source":"### A more statistically oriented exploring to outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in featstr[1:]:\n    print('{}\\n0.1%:99.9% are between: {}\\nmax: {}\\nmin: {}\\n75% are under: {}'.format(i,np.percentile(df[i],(.1,99.9)), df[i].max(),df[i].min(),np.percentile(df[i],75)),\n         '\\n===============================')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[(df.feature_56== df.feature_56.max())|(df.feature_57== df.feature_57.max())|(df.feature_58== df.feature_58.max()) | (df.feature_59== df.feature_59.max())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It can be inferred that the dataset has extreme outliers it also worth mentioning that some of outliers are accompanied with large values from neighbor columns which is a result of high multicollinearity between features"},{"metadata":{},"cell_type":"markdown","source":"### Now we will make a bold move by removing these extreme outliers that are above 99.9% of feature data\n#### To avoid removing more data while looping through the data set we will make a list of 99.9% mark for each and every single feature, We will also create a list for negative outliers values \"using .1 % mark\" to be explored later "},{"metadata":{"trusted":true},"cell_type":"code","source":"n999 = [ np.percentile(df[i],99.9) for i in featstr[1:]]\nn001 = [ np.percentile(df[i],.1) for i in featstr[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in enumerate(featstr[1:]):\n    df = df[df[j] < n999[i]]\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding the ratio of the data lost in removing the outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"str(round(((org_len - len(df))/org_len)*100,2))+'%'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(df[featstr].max(), title='Features Max Values')\nfig.layout.xaxis.tickangle = 300\nfig.update_traces(showlegend = False)\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = ''\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we make a boxplot grid again with customized .1% : 99.9% whiskers."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,80))\nfig.suptitle('Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= df[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        counter += 1\n        gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### The dataset still has many obvious outliers especially negative values since we only removed positive outliers so features which used to have symmetrical outliers now have some kind of left skewed outliers.\n>### comparing both of the boxplots (before and after removing positive outliers) we can notice features from feature_3 to feature_40 which used to have symmetrical outliers now changed to have extreme negative outliers after trimming\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Manual outlier trimming to these features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in zip(featstr[1:][2:34],n001[2:34]):\n    df = df[df[i] > j]\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The following code shows the ratio of observations lost because of cleaning so far"},{"metadata":{"trusted":true},"cell_type":"code","source":"str(round(((org_len - len(df))/org_len)*100,2))+'%'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The kde's and histograms of features after dropping outliers and taking in consideration Resp value "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,80))\nfig.suptitle('KDE plot of Features',fontsize=24,transform =fig.transFigure, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.01)\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.distplot(df[df.action==0][featstr[counter]],bins= 100,label='Negative',\n                     color='darkorange', kde_kws={'linewidth':4},ax=subf)\n        sns.distplot(df[df.action!=0][featstr[counter]],bins= 100,label='Positive',\n                     color='blue', kde_kws={'alpha':.9,'linewidth':2},hist_kws={'alpha':.3},ax=subf)\n        subf.axvline(np.percentile(df[featstr[counter]],99.5),color= 'darkblue', label='99.5%', linestyle=':',linewidth=2)\n        subf.axvline(np.percentile(df[featstr[counter]],.5),color= 'red', label='0.5%', linestyle=':',linewidth=2)\n        subf.legend().set_visible(False)\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        kurt=round(df[featstr[counter]].kurt(),2)\n        skew=round(df[featstr[counter]].skew(),2)\n        subf.text(.6,.92,'Kurt = {:.2f}\\nSkew = {:.2f}'.format(kurt ,skew),\n         transform=subf.transAxes, verticalalignment='top',bbox=props,fontsize=10)\n        counter += 1\n        gc.collect();\nhandles, labels = subf.get_legend_handles_labels()\nfig.legend(handles, labels,ncol=4, bbox_to_anchor=(0.86, 0.893),fontsize=10,\n           title= 'Resp',title_fontsize=14,bbox_transform =fig.transFigure);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### By Adding Resp value to the features distributions can now see the following:\n- The histograms of features now have way less outliers and more formal distribution.\n- We can also see that some features like 1, 2, 85, 87, 88 and 91 have many negative outliers values.\n- Some features like 49, 50, 51, 55, 56, 57, 58, and 59 still have many positive outliers.\n- Features distributions are not affected by the resp value."},{"metadata":{},"cell_type":"markdown","source":"### Features and resp Correlation"},{"metadata":{},"cell_type":"markdown","source":"### First we make a correlation pandas series for the relationship between resp and each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"respcorr =  pd.Series([ df.resp.corr(df[i]) for i in featstr],index=featstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(respcorr,color = respcorr, color_continuous_scale=['red','blue'], title= 'Features Correlation with Resp')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### We can see that features are not really correlated to Resp"},{"metadata":{},"cell_type":"markdown","source":"### Features and Weight"},{"metadata":{},"cell_type":"markdown","source":"### Again we make a pandas series of weight correlation with feature but only taking in consideration weights larger than 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"wecorr = pd.Series([df[df.weight != 0].weight.corr(df[df.weight != 0][i]) for i in featstr],index=featstr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wecorr.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(wecorr,title= 'Features Correlation with Weight (not including zero weights)')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.update_layout(showlegend=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We explore largest and lowest correlation coefficients with a scatter plots"},{"metadata":{},"cell_type":"markdown","source":"### first we start with highest correlation which belongs to features 51 "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(df[df.weight != 0].weight,df[df.weight != 0].feature_51, color = 'darkblue', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_51',fontsize=14)\nplt.title('Feature_51 vs Weight\\nCorrelation = {}%'.format(round(df[df.weight != 0].weight.corr(df[df.weight != 0].feature_51),4)*100),fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### It seems that weight is highly correlated to feature 51"},{"metadata":{},"cell_type":"markdown","source":"### Now we check the lowest correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(df[df.weight != 0].weight,df[df.weight != 0].feature_126, color = 'darkblue', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_126',fontsize=14)\nplt.title('Feature_126 vs Weight\\nCorrelation{}%'.format(round(df[df.weight != 0].weight.corr(df[df.weight != 0].feature_126),4)*100),fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### While there is some kind negative correlation between weight and feature 126 the relation seem to be weak"},{"metadata":{},"cell_type":"markdown","source":"<a id='f0'></a>\n## **Feature 0**"},{"metadata":{},"cell_type":"markdown","source":"### Finding the unique values of Feature 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5)) \ndf.feature_0.value_counts().plot.bar(color='darkblue',alpha=.6,width=.5)\nplt.title('Feature_0',fontsize=18) \nplt.xticks(rotation=0,fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Feature_0 seem to be some kind of binary feature"},{"metadata":{},"cell_type":"markdown","source":"### Taking Resp value in consideration"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6)) \nsns.countplot(data=df, x='feature_0', hue='action',palette='viridis')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=13)\nplt.xlabel('Feature 0',fontsize=12)\nplt.title('Feature 0 and Resp', fontsize=18)\nplt.ylabel('')\nplt.xlim(-1,2)\nh, l = plt.gca().get_legend_handles_labels()\nplt.legend(h,['Negative','Positive'],ncol=1, fontsize=12, loc=3,title= 'Resp',title_fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### seems like there is not an obvious relation between feature_0 and resp being negative or positive"},{"metadata":{},"cell_type":"markdown","source":"<a id='pcacls'></a>\n# PCA & Clustering"},{"metadata":{},"cell_type":"markdown","source":"<a id='pca'></a>\n## PCA Starter"},{"metadata":{},"cell_type":"markdown","source":"### First we scale the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = scale()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(df[featstr[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = pd.DataFrame(scaler.transform(df[featstr[1:]]))\ndf_pca.columns = featstr[1:]\ngc.collect()\ndf_pca.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reduce the dimensionality of the data to 8 principal components"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca =  PCA(n_components= 8).fit(df_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = pd.DataFrame(pca.transform(df_pca))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pcs = ['pc'+str(i+1) for i in range(8)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding weight, Resp and action to the new dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca.columns = pcs\ndf_pca['action'] = df.action.values\ndf_pca['weight'] = df.weight.values\ndf_pca['resp'] = df.resp.values\ndf_pca.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring correlation in the PCA dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca.corr().style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### PCA did't really help as principal components still don't have a clear relation with resp"},{"metadata":{},"cell_type":"markdown","source":"<a id='cluster'></a>\n## Clustering"},{"metadata":{},"cell_type":"markdown","source":"### Exploring the possible clusters in the PCA dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = k_means(n_clusters= 8, max_iter= 400, random_state= 0,X=df_pca[pcs])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding the clusters to the PCA dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca['cluster'] = kmeans[1]\ndf_pca['cluster'] = df_pca['cluster'].astype('category')\ndf_pca.head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the relation between clusters and Resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nax = plt.subplot(1,1,1)\nsns.countplot(data=df_pca,x='cluster',hue='action',ax=ax,palette='viridis')\nh, l = plt.gca().get_legend_handles_labels()\nplt.legend(h,['Negative','Positive'],ncol= 1, fontsize= 12, loc= 1,title= 'Resp',title_fontsize=14)\nplt.xlabel('Clusters',fontsize=18)\nplt.ylabel('')\nplt.xticks(fontsize=14) \nplt.title('PCA Clusters and Resp', fontsize=22);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Clusters seem to be also scattered with no clear relation between a cluster and resp"},{"metadata":{},"cell_type":"markdown","source":"# <font color='green'>Work in progress  █████████▒</font> "},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Upvote this notebook and Michelangelo will get his favorite pizza🍕</center></h3>\n\n![](http://i.imgflip.com/1ydu71.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}