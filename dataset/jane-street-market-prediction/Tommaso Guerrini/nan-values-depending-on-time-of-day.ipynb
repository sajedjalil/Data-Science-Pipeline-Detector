{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n# Nan Values Analysis Jane Street\n\n<img src=\"https://www.europol.europa.eu/sites/default/files/images/finance_budget.jpg\">\n\n\nThe notebook is divided in the following sections:\n\n- [*0.0 Imports and Data Loading*](#imports)<br>\n- [*1.0 Top 40 Features for number of NaNs*](#top40)<br>\n- [*1.1 NaN values based on daily_ts_id, for the first 30000 trades*](#30k)<br>\n- [*1.2 Distribution of Time Of Day where Nan Occur for each feature*](#timeofday)<br>\n- [**Take on NaNs vs Time of Day**](#nan_vs_timeofday)<br>\n- [*2.0 Market basket analysis on Sets of Nan Features*](#market_basket)<br>\n- [*2.1 Distribution of feature itemsets*](#feature_itemsets)<br>\n- [**Take on Features having the same NaN behaviour**](#take_market_basket)<br>\n\n\n\nPlease let me know your opinion in the comments! Glad to improve!\n\n## TLDR: \n\n- For most of the features having NaN values, NaN values occur in the first trades of each day\n\n- Group of features have NaNs for the same trades (Market Basket Analysis)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"imports\"></a>\n# Imports and Data Loading\n\nHere we just install necessary packages (datatable), import them, define functions for later usage and load train.csv data. "},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!pip install datatable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tqdm\nimport itertools\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\npd.options.display.max_columns = 200\nimport os\nimport gc\nimport re\nimport datatable as dt\n\ndef chunks(l, n):\n    \"\"\" Yield n successive chunks from l.\n    \"\"\"\n    newn = int(len(l) / n)\n    for i in range(0, n-1):\n        yield l[i*newn:i*newn+newn]\n    yield l[n*newn-newn:]\n    \ninput_path = '/kaggle/input/'\nroot_path = os.path.join(input_path, 'jane-street-market-prediction')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n\ntrain = (dt.fread(os.path.join(root_path, \"train.csv\")).to_pandas()\n        .query('weight > 0').pipe(reduce_mem_usage)\n        .reset_index(drop = True))\n\ntrain['action'] = (train.resp > 0).astype(int)\n\nresp_cols = [i for i in train.columns if 'resp' in i]\n\nfeatures_names = list(set(train.columns) - set(resp_cols) - set(['weight', 'ts_id', 'date', 'action']))\nfeatures_index = list(map(lambda x: int(re.sub(\"feature_\", \"\", x)), features_names))\nfeatures = sorted(list(zip(features_names, features_index)), key = lambda x: x[1])\nfeatures = [i[0] for i in features] + resp_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#count\nnan_values_train = (train\n .apply(lambda x: x.isna().sum(axis = 0)/len(train))\n .to_frame()\n .rename(columns = {0: 'percentage_nan_values'})\n.sort_values('percentage_nan_values', ascending = False)\n)\n\ndisplay((train\n .apply(lambda x: x.isna().sum(axis = 0))\n .to_frame()\n .rename(columns = {0: 'count_nan_values'})\n.sort_values('count_nan_values', ascending = False)\n.transpose()), nan_values_train.transpose(),\n       print(\"Number of features with at least one NaN value: {}/{}\".format(len(nan_values_train.query('percentage_nan_values>0')),\n                                                                           len(train.columns))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top40\"></a>\n\n# Top 40 Features for number of NaNs"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20, 12))\n\nsns.set_palette(\"RdBu\", 10)\n#RdBu, YlGn\nax = sns.barplot(x='percentage_nan_values', \n            y='feature', \n            palette = 'GnBu_r',\n            data=nan_values_train.reset_index().rename(columns = {'index': 'feature'}).head(40))\n\nfor p in ax.patches:\n    width = p.get_width() \n    if width < 0.01:# get bar length\n        ax.text(width,       # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center')  # vertical alignment\n    else:\n        if width < 0.04:\n            color_text = 'black'\n        else:\n            color_text = 'white'\n        ax.text(width /2, \n                # set the text at 1 unit right of the bar\n            p.get_y() + p.get_height() / 2, # get Y coordinate + X coordinate / 2\n            '{:1.4f}'.format(width), # set variable to display, 2 decimals\n            ha = 'left',   # horizontal alignment\n            va = 'center',\n            color = color_text,\n            fontsize = 10)  # vertical alignment\n\nax.set_title('Top 40 Features for percentage of NaN Values')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nSome features have almost the same number of nan values. Let's plot their distribution over time.\nFirst of all I define a new column which represent the daily_ts_id (or the trade number - 1 of the day). Then I'll be looking for relationships between Nan Values and time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['daily_ts_id'] = (train.groupby('date').cumcount())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"30k\"></a>\n# NaN values based on daily_ts_id, for the first 30000 trades\n\nLet's see some of them and then I'll leave you the option to check all of them."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"top_nan_features = nan_values_train.head(40).index.tolist()\nmini_df = pd.concat([(train[top_nan_features].isna().astype(int)),train[['ts_id']]], 1).iloc[:30000, :]\nnew_day = (train.iloc[:30000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\nfor feature_name in top_nan_features[:4]:\n    fig, ax = plt.subplots(1, 1, figsize = (15, 8))\n    \n    mini_df[[feature_name, \"ts_id\"]].plot(y = feature_name , kind = 'line',\n                                         xlabel = 'ts_id', \n                                         ylabel = feature_name+ \"_nans\", linewidth=0.3,\n                                         legend = False,\n                                         ax = ax)\n    for m in range(len(new_day)):\n        ax.axvline(new_day[m], alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        if m == 2:\n            ax.text(new_day[m]-1700, 1.1, \"day {}\".format(m), size = 16, alpha = 0.8)\n        else:\n            ax.text(new_day[m]+70, 1.1, \"day {}\".format(m), size = 16, alpha = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unhide the following to see all of the NaN plots"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"top_nan_features = nan_values_train.head(40).index.tolist() #take the first 40 with most nans\nmini_df = pd.concat([(train[top_nan_features].isna().astype(int)),train[['ts_id']]], 1).iloc[:30000, :]\nnew_day = (train.iloc[:30000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\n\nfor feature_name in top_nan_features:\n    fig, ax = plt.subplots(1, 1, figsize = (15, 8))\n    \n    mini_df[[feature_name, \"ts_id\"]].plot(y = feature_name , kind = 'line',\n                                         xlabel = 'ts_id', \n                                         ylabel = feature_name+ \"_nans\", linewidth=0.3,\n                                         legend = False,\n                                         ax = ax)\n    for m in range(len(new_day)):\n        ax.axvline(new_day[m], alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n        if m == 2:\n            ax.text(new_day[m]-1700, 1.1, \"day {}\".format(m), size = 16, alpha = 0.8)\n        else:\n            ax.text(new_day[m]+70, 1.1, \"day {}\".format(m), size = 16, alpha = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nI think we can clearly see some patterns here: \n\n- some features (17, 27, 28, etc.) have most NaNs some time/trades after the first one;\n\n- some features (21, 22, 31, 32, etc.) have most NaNs right at the start of each day;\n\n- almost all other features belong to one of the 2 groups above, but their NaNs are more sporadic.\n\nLet's try to verify these hypotheses.\n\nI will add a new column 'pseudo_time_of_day', which is the trade number scaled to [0,1] interval.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = (train.set_index('date').join(train.groupby('date').size().reset_index().rename(columns = {0: 'daily_number_of_trades'}).set_index('date'))\n        .reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train['pseudo_time_of_day'] = (train['daily_ts_id']/train['daily_number_of_trades'])\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"nan_df = pd.concat([(train[top_nan_features].isna().astype(int)), train[['pseudo_time_of_day']]], 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"timeofday\"></a>\n# Distribution of Time Of Day where Nan Occur for each feature \n\nLet's see some of them and then I'll leave you the option to check all of them. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features_chunks = chunks(top_nan_features, 10)\nchunk_len = 4\n\nfor enum, k_chunk in enumerate(features_chunks):\n    if enum > 0:\n        break\n    fig, axes = plt.subplots(2, 2, figsize = (40, 30))\n    ax = axes.ravel()\n    \n    for i in range(len(k_chunk)):\n\n        feature_name = k_chunk[i]\n        feature = nan_df.loc[nan_df[feature_name] == 1]['pseudo_time_of_day']\n        \n        sns.distplot(feature, hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'},\n                     kde_kws={'linewidth': 2, 'color': 'blue'}, ax = ax[i%10])\n        ax[i%10].grid(True)\n        ax[i%10].set(xlabel = 'pseudo_time_of_day')\n        ax[i%10].set_title(feature_name, fontsize=30)\n    fig.suptitle('Time of Day where NaN occur', fontsize = 45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unhide the following to see all distributions"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"features_chunks = chunks(top_nan_features, 10)\nchunk_len = 4\n\nfor enum, k_chunk in enumerate(features_chunks):\n    \n    fig, axes = plt.subplots(2, 2, figsize = (40, 30))\n    ax = axes.ravel()\n    \n    for i in range(len(k_chunk)):\n        try:\n            feature_name = k_chunk[i]\n            \n            feature = nan_df.loc[nan_df[feature_name] == 1]['pseudo_time_of_day']\n        except:\n            print(k_chunk)\n        \n        sns.distplot(feature, hist=True, kde=True, color = 'red', hist_kws={'edgecolor':'black'},\n                     kde_kws={'linewidth': 2, 'color': 'blue'}, ax = ax[i%10])\n        ax[i%10].grid(True)\n        ax[i%10].set(xlabel = 'pseudo_time_of_day')\n        ax[i%10].set_title(feature_name, fontsize=30)\n    fig.suptitle('Time of Day where NaN occur', fontsize = 45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"nan_vs_timeofday\"></a>\n# Take on NaNs vs Time of Day"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nI guess we could state the following:\n\n- almost all features have most nans occurring at the beginning of each day \n\n- all features have nans occurring just after 'midday' (where midday corresponds to half of the daily trades)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"market_basket\"></a>\n# Market basket analysis on Sets of Nan Features\n\n\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/AffinityAnalysis.png\">\n"},{"metadata":{},"cell_type":"markdown","source":"Here I consider each trade as a transaction where the features bought are those that are Nan: \n    \nthe idea is to look for frequent itemsets, i.e. group of features being often NaN together!"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"gc.collect()\nALREADY_CALCULATED = True\nif not ALREADY_CALCULATED:\n    nan_df['nan_features'] = nan_df.iloc[:, :-1].apply(lambda x: list(np.where(np.array(x) == 1)[0]), axis = 1)\n    nan_df = nan_df.loc[nan_df.nan_features.apply(lambda x: len(x) > 0)]\n    nan_df.reset_index(drop = True).to_pickle('nan_df_itemsets.pickle')\n    te = TransactionEncoder()\n    te_ary = te.fit(nan_df.nan_features).transform(nan_df.nan_features)\n\n    df = pd.DataFrame(te_ary, columns=te.columns_)\n\n    frequent_itemsets = (apriori(df, min_support=0.35, use_colnames=True))\n    frequent_itemsets = (frequent_itemsets.loc[frequent_itemsets.itemsets.apply(lambda x: len(x)>1)].reset_index(drop = True)\n                        .sort_values('support', ascending = False))\n    frequent_itemsets['n_of_items'] = frequent_itemsets.itemsets.apply(lambda x: len(x))\n    frequent_itemsets = (frequent_itemsets.set_index('support').join(frequent_itemsets.groupby('support').n_of_items.max().rename('max_n_of_items'))\n    .query('n_of_items == max_n_of_items'))\n    pd.options.display.max_colwidth = 300\nelse:\n    nan_df = pd.read_pickle(\"/kaggle/input/nandfforitemsets/nan_df_itemsets.pickle\")\n    frequent_itemsets = pd.read_pickle('/kaggle/input/freq-itemsets/frequent_itemsets_035_already_calculated.pickle')\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(frequent_itemsets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"group_of_features = nan_df.columns.tolist()[:14]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features 0 to 13 have a crazy similar behaviour! "},{"metadata":{},"cell_type":"markdown","source":"<a id = \"feature_itemsets\"></a>\n## Distribution of feature itemsets"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features_chunks = list(chunks(group_of_features, 3))\nfeatures_chunks = [sorted(i) for i in features_chunks]\nmini_df = train[group_of_features + ['ts_id']].iloc[:50000, :]\nnew_day = (train.iloc[:50000, :].query(\"daily_ts_id == 0\").ts_id.tolist())\n\nfor k_chunk in features_chunks:\n    chunk_len = len(k_chunk)\n\n    fig, axes = plt.subplots(chunk_len, 1, figsize = (40, 30))\n    \n    ax = axes.ravel()\n    \n    for i in range(len(k_chunk)):\n\n        feature_name = k_chunk[i]\n        feature = train[feature_name]\n        \n        mini_df[[feature_name , \"ts_id\"]].plot(kind = 'line',\n                                              xlabel = 'ts_id', \n                                              linewidth=0.3, legend = False,\n                                              ax = ax[i%chunk_len], sharex=True)\n        \n        ax[i%chunk_len].grid(True)\n        ax[i%chunk_len].set(xlabel = 'ts_id')\n        ax[i%chunk_len].set_title(feature_name)\n        ax[i%chunk_len].set_ylim(ymin=mini_df[feature_name].min(), ymax=mini_df[feature_name].max())\n        for m in range(len(new_day)):\n            ax[i%chunk_len].axvline(new_day[m], alpha = 0.5, ymin = 0, ymax = 1, linestyle = \":\", color = 'blue')\n            if m == 2:\n                ax[i%chunk_len].text(new_day[m]-1700, 1.1, \"day {}\".format(m), size = 16, alpha = 0.8)\n            else:\n                ax[i%chunk_len].text(new_day[m]+70, 1.1, \"day {}\".format(m), size = 16, alpha = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"take_market_basket\"></a>\n# Take on Features having the same NaN behaviour"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://images.emojiterra.com/google/android-10/512px/1f914.png\" width=\"50\" height=\"50\" style=\"top:03%; left:80%\"> \n\nI guess we could state the following:\n\n- looking at the first days (which of course is a limitation) of our dataset it seems some features are very similar in their behaviour (17-18, 27-28, 7-8, 108-84, 102-114, 72-78, 90-96). Of course a correlation/crosscorrelation analysis maybe very helpful and provide further insight. I did not include that here since I wanted to focus just on NaN analysis. There's another notebook of mine on that, but I saw there are plenty of others so I won't brag about mine! "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}