{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport tensorflow_addons as tfa\nfrom random import choices\nfrom numba import njit\nimport matplotlib.pyplot as plt\nimport os,gc\nimport random\nfrom random import choices\n\nSEED = 1111\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\ntrain = pd.read_csv('../input/jane-street-market-prediction/train.csv')\ntrain = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\ntrain.fillna(train.mean(),inplace=True)\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\nfeatures = [c for c in train.columns if \"feature\" in c]\nf_mean = np.mean(train[features[1:]].values,axis=0)\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\nX_train = train.loc[:, train.columns.str.contains('feature')]\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n\n###### Here, we will now create a pipeline #####\n#from sklearn.compose import ColumnTransformer\n#from sklearn.impute import SimpleImputer\n#from sklearn.preprocessing import OneHotEncoder\n#from sklearn.pipeline import Pipeline\n\n#numerical_transformer = SimpleImputer(strategy = 'constant')\n#categorical_transformer = Pipeline(steps = [('imputer',SimpleImputer(strategy = 'most_frequent')), \n#                                            ('one_hot',OneHotEncoder(handle_unknown = 'ignore'))])\n#preprocessor = ColumnTransformer (transformers = [('num', numerical_transformer, resp#need to modify this#),\n#                                                  ('cat', categorial_transformer, resp_3#need to modify this#)])\n##### Pipeline basic opening over #####\n\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 300\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.20, 0.20, 0.20, 0.20]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(\n    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\nclf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)\nclf.save(f'model.h5')\n\n##### Here we will replace the clf with our pipeline for cleaner processing #####\n#from sklearn.metrices import mean_absolute_error\n#res_pipeline = Pipeline (steps = [('preprocessor',preprocessor),\n#                                  ('clf',clf)                                 \n#                                  ])\n#res_pipeline.fit(X_train, y_train, epochs=200, batch_size=5000)\n##### And the work ends ! #####\n\n## Cross-validation ##\n#from sklearn.model_selection import cross_val_score\n#cv_scores = cross_val_score(clf, X_train, y_train, \n#                            cv=5,\n#                            scoring='neg_mean_absolute_error')\n#print(\"MAE scores:\\n\", scores)\n## Process ends !##\n\nmodels = []\nmodels.append(clf)\nth = 0.502\nf = np.median\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = np.mean([model(x_tt, training = False).numpy() for model in models],axis=0)\n        pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All utility scores - Do consider these before making a submission ! ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"#def utility_score_LDMTWO(df, labels='action,.r0,.weight,.date'.split(',')):\n\n#    action,resp,weight,date = labels\n#    df = df.set_index(date)\n#    p = df[weight]  * df[resp] * df[action]\n#    p_i = p.groupby(date).sum()\n#    t = (p_i.sum() / np.sqrt((p_i**2).sum())) * (np.sqrt(250 / p_i.index.size))\n#    return np.clip(t,0,6) * p_i.sum()\n\n#def utility_score_Jorijn(df):\n\n#    df['p'] = df['weight']  * df['resp'] * df['action']\n#    p_i = df.set_index('date')['p'].groupby('date').sum()\n#    t = (p_i.sum() / np.sqrt((p_i**2).sum())) * (np.sqrt(250 / p_i.index.size))\n#    return min(max(t, 0), 6) * p_i.sum()\n\n#print('LDMTWO\\'s:')\n#%timeit utility_score_LDMTWO(train, labels = 'action,resp,weight,date'.split(','))\n#print('-' * 70)\n#print('Jorijn\\'s:')\n#%timeit utility_score_Jorijn(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding correlation\nThis link can be used to make a correlational neural network model :\nhttps://towardsdatascience.com/a-comprehensive-guide-to-correlational-neural-network-with-keras-3f7886028e4a"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}