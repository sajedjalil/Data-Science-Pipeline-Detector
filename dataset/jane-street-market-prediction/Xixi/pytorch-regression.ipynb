{"cells":[{"metadata":{},"cell_type":"markdown","source":"## TODO\n1. add features\n2. change targets\n3. model\n4. loss_fn\n5. tune the parameters"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## packages and settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\nimport os\nimport copy\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm import tqdm\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nseed = 123\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\nif not os.path.exists(\"results\"):\n    os.mkdir(\"results\")\n\nTRAINING = True\nread_path = '/kaggle/input/jane-street-market-prediction/train.csv'\n#model_path = \"./results/123/best_model\"\n#save_path = os.path.join(\"results\", 'v5')\nsave_path = 'results'\n\ndevice = torch.device(\"cuda:0\")\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\n    \n# train = pd.read_csv(read_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## preprocess the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(read_path)\ntrain = train.query('date > 85').reset_index(drop = True) \nfeatures = [c for c in train.columns if 'feature' in c]\n\n# preprocess the features\nf_mean = train[features].mean()\ntrain = train.loc[train.weight > 0].reset_index(drop = True)\ntrain[features] = train[features].fillna(f_mean)\n\ntrain = train.astype(\"float32\")\ntrain['action'] = (train['resp'] > 0).astype('int')\ntrain['action1'] = (train['resp_1'] > 0).astype('int')\ntrain['action2'] = (train['resp_2'] > 0).astype('int')\ntrain['action3'] = (train['resp_3'] > 0).astype('int')\ntrain['action4'] = (train['resp_4'] > 0).astype('int')\n\ntargets = ['resp0']\ntargets = ['resp0','resp_10','resp_20','resp_30','resp_40']\n# train[targets] = (train[targets]>0).astype('int')\n\n\ndef add_features(df, features):\n    new_features = copy.deepcopy(features)\n    \n    # todo\n    df[\"cross_1_2\"] = df[\"feature_1\"] / (df[\"feature_2\"] + 1e-5)\n    df[\"cross_41_42_43\"] = df[\"feature_41\"] + df[\"feature_42\"] + df[\"feature_43\"]\n    new_features.extend([\"cross_1_2\", \"cross_41_42_43\"])\n\n    return df, new_features\n\ntrain, train_features = add_features(train, features)\n\n\n# to do: update the mean online\n# f_mean = f_mean.values\n# np.save(os.path.join(save_path, 'f_mean.npy'), f_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['resp0'] = train.apply(lambda x: min(max(x['resp'],-0.05),0.05),axis=1)\ntrain['resp_10'] = train.apply(lambda x: min(max(x['resp_1'],-0.05),0.05),axis=1)\ntrain['resp_20'] = train.apply(lambda x: min(max(x['resp_2'],-0.05),0.05),axis=1)\ntrain['resp_30'] = train.apply(lambda x: min(max(x['resp_3'],-0.05),0.05),axis=1)\ntrain['resp_40'] = train.apply(lambda x: min(max(x['resp_4'],-0.05),0.05),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"date: 1~499\n\nvalid: 440~469\n\ntest: 470~499"},{"metadata":{"trusted":true},"cell_type":"code","source":"n1 = np.sum(train['date']<450)\nn2 = np.sum(train['date']<450)\ntr = list(range(n1))\n\n# test using the last 60 days data\nte = list(range(n1, train.shape[0]))\ntest = train.iloc[te, :]\n\n# another test set with gap (30 days)\nte_with_gap = list(range(n2, train.shape[0]))\ntest_with_gap = train.iloc[te, :]\n\n\n# train = train.iloc[tr, :]\n# print(te[-1], train.shape[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## the dataset and model(resnet)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataset:\n    def __init__(self, df, features, targets):\n        self.features = df[features].values\n        \n        # preprocess the labels\n        # self.labels = (df[targets] > 0).astype('int').values\n        self.labels = df[targets].values\n        self.weights = df['weight'].values\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        feat_ = torch.tensor(self.features[idx], dtype=torch.float)\n        label_ = torch.tensor(self.labels[idx], dtype=torch.float)\n        weight_ = torch.tensor(self.weights[idx], dtype=torch.float)\n        \n        return feat_, label_, weight_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model0(nn.Module):\n    def __init__(self, features, targets):\n        super(Model0, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.10143786981358652)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 384)\n        self.batch_norm1 = nn.BatchNorm1d(384)\n        self.dropout1 = nn.Dropout(0.19720339053599725)\n\n        self.dense2 = nn.Linear(384, 896)\n        self.batch_norm2 = nn.BatchNorm1d(896)\n        self.dropout2 = nn.Dropout(0.2703017847244654)\n\n        self.dense3 = nn.Linear(896, 896)\n        self.batch_norm3 = nn.BatchNorm1d(896)\n        self.dropout3 = nn.Dropout(0.23148340929571917)\n\n        self.dense4 = nn.Linear(896, 384)\n        self.batch_norm4 = nn.BatchNorm1d(384)\n        self.dropout4 = nn.Dropout(0.2357768967777311)\n\n        self.dense5 = nn.Linear(384, len(targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout4(x)\n\n        x = self.dense5(x)\n\n        return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model3(nn.Module):\n    def __init__(self, features, targets):\n        super(Model3, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.1)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 512)\n        self.batch_norm1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.2)\n\n        self.dense2 = nn.Linear(512, 896)\n        self.batch_norm2 = nn.BatchNorm1d(896)\n        self.dropout2 = nn.Dropout(0.2)\n\n        self.dense3 = nn.Linear(896, 1024)\n        self.batch_norm3 = nn.BatchNorm1d(1024)\n        self.dropout3 = nn.Dropout(0.2)\n\n        self.dense4 = nn.Linear(1024, 896)\n        self.batch_norm4 = nn.BatchNorm1d(896)\n        self.dropout4 = nn.Dropout(0.2)\n        \n        self.dense5 = nn.Linear(896, 512)\n        self.batch_norm5 = nn.BatchNorm1d(512)\n        self.dropout5 = nn.Dropout(0.2)\n        \n        self.dense6 = nn.Linear(512, len(targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout4(x)\n        \n        x = self.dense5(x)\n        x = self.batch_norm5(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout5(x)\n               \n        x = self.dense6(x)\n\n        return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model2(nn.Module):\n    def __init__(self, features, targets):\n        super(Model2, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.1)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 512)\n        self.batch_norm1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.2)\n\n        self.dense2 = nn.Linear(512, 896)\n        self.batch_norm2 = nn.BatchNorm1d(896)\n        self.dropout2 = nn.Dropout(0.2)\n\n        self.dense3 = nn.Linear(896, 1024)\n        self.batch_norm3 = nn.BatchNorm1d(1024)\n        self.dropout3 = nn.Dropout(0.2)\n\n        self.dense4 = nn.Linear(1024, 1024)\n        self.batch_norm4 = nn.BatchNorm1d(1024)\n        self.dropout4 = nn.Dropout(0.2)\n\n        self.dense5 = nn.Linear(1024, 896)\n        self.batch_norm5 = nn.BatchNorm1d(896)\n        self.dropout5 = nn.Dropout(0.2)\n        \n        self.dense6 = nn.Linear(896, 512)\n        self.batch_norm6 = nn.BatchNorm1d(512)\n        self.dropout6 = nn.Dropout(0.2)\n        \n        self.dense7 = nn.Linear(512, len(targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout4(x)\n        \n        x = self.dense5(x)\n        x = self.batch_norm5(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout5(x)\n        \n        x = self.dense6(x)\n        x = self.batch_norm6(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout6(x)\n        \n        x = self.dense7(x)\n\n        return x\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model1(nn.Module):\n    def __init__(self, features, targets):\n        super(Model1, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.1)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 512)\n        self.batch_norm1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(0.4)\n\n        self.dense2 = nn.Linear(512, 1024)\n        self.batch_norm2 = nn.BatchNorm1d(1024)\n        self.dropout2 = nn.Dropout(0.4)\n\n        self.dense3 = nn.Linear(1024, 1024)\n        self.batch_norm3 = nn.BatchNorm1d(1024)\n        self.dropout3 = nn.Dropout(0.4)\n\n        self.dense4 = nn.Linear(1024, 512)\n        self.batch_norm4 = nn.BatchNorm1d(512)\n        self.dropout4 = nn.Dropout(0.4)\n\n        self.dense5 = nn.Linear(512, len(targets))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * torch.sigmoid(x)\n        x = self.dropout4(x)\n\n        x = self.dense5(x)\n\n        return x\n    \n    \n    \nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets, weights=None):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets, weights)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss\n\n    \nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}, best_score: {}'.format(self.counter,self.patience,-self.best_score))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved. Saving model!')\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{},"cell_type":"markdown","source":"### utility"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for feature, label, weight in dataloader:\n        feature = feature.to(device)\n        label = label.to(device)\n        weight = weight.to(device)\n        optimizer.zero_grad()\n        outputs = model(feature)\n        \n        # 这里output/label几维？处理后什么维数\n        loss = loss_fn(outputs.reshape(-1, 1), label.reshape(-1, 1)) #  weight.reshape(-1,1)\n        \n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss /= len(dataloader)\n    return final_loss\n        \n    \ndef inference_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    preds = []\n    labels = []\n    final_loss = 0\n    \n    for feature, label, weight in dataloader:\n        feature = feature.to(device)\n        label = label.to(device)\n        with torch.no_grad():\n            outputs = model(feature)\n            preds.append(outputs.cpu().numpy())\n            labels.append(label.cpu().numpy())\n            \n            loss = loss_fn(outputs.reshape(-1, 1), label.reshape(-1, 1))\n        final_loss += loss.item()\n    \n    preds = np.concatenate(preds, axis=0)\n    labels = np.concatenate(labels, axis=0)\n    final_loss /= len(dataloader)\n    \n    return preds, labels, final_loss\n    \n    \ndef utility_score(date, weight, resp, action):    \n    values = weight * resp * action\n    to_bincount = {}\n\n    for d, v in zip(date, values):\n        to_bincount.setdefault(d, []).append(v)\n\n    Pi = []\n    for val in to_bincount.values():\n        Pi.append(np.sum(val))\n    Pi = np.array(Pi)\n    count_i = len(np.unique(date))\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    \n    return u\n\n    \ndef loss_mse(preds, targets):\n    \n    return ((preds-targets)**2).mean()\n\n    \ndef loss_mse_threshold(preds, targets):\n    \n    tmp = torch.max((preds-targets)**2-torch.ones(len(preds)).to(device)*0.0001,torch.zeros(len(preds)).to(device)).mean()\n    \n    return tmp\n\ndef loss_mse_threshold_2(preds, targets):\n    \n    ones = torch.ones(len(preds)).to(device)\n    zeros = torch.zeros(len(preds)).to(device)\n    tmp = (10*torch.max((preds-targets)**2-ones*0.000001,zeros)+(preds-targets)**2).mean()\n    \n    return tmp\n\n\ndef loss_ce(preds, targets, weight=None):\n    \n    return F.binary_cross_entropy_with_logits(preds, targets, weight)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query('date > 85').reset_index(drop = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAINING:\n    batch_size = 4096\n    label_smoothing = 1e-2\n    \n    np.save(f'{save_path}/f_mean_online.npy', f_mean)\n    \n    f = open(\"record.txt\",\"w\")\n    f.write(\"fold,epoch,train_loss,valid_loss,u_score_tr,u_score,learning_rate\\n\")\n    f.close()\n    \n    import time\n    start_time = time.time()\n    oof = np.zeros(len(train['action']))\n    gkf = GroupKFold(n_splits = 5)\n    for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n        train_set = MyDataset(train.loc[tr], train_features, targets)\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n        valid_set = MyDataset(train.loc[te], train_features, targets)\n        valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)\n\n        torch.cuda.empty_cache()\n        device = torch.device(\"cuda:0\")\n        model = Model1(train_features, targets)\n        model.to(device)\n        \n        learning_rate = 1e-3\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer,5,gamma=0.5)\n        #loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n        #loss_fn = loss_mse\n        loss_fn = loss_mse_threshold_2 # 损失函数！！！\n\n        ckp_path = f\"{save_path}/JSModel_{fold}.pth\"\n\n        es = EarlyStopping(patience=40, mode=\"max\") #以score做目标时用max，以loss做目标时用min\n\n        for epoch in range(60):\n            if epoch%10==0 and epoch>0:\n                lr = optimizer.state_dict()['param_groups'][0]['lr']*0.5\n                for param_group in optimizer.param_groups:\n                    param_group[\"lr\"] = lr\n            train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n            valid_pred, true_valid_labels, valid_loss = inference_fn(model, loss_fn, valid_loader, device)\n            train_pred, true_train_labels, l = inference_fn(model, loss_fn, train_loader, device)\n            # auc_score = roc_auc_score((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n            # logloss_score = log_loss((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n            valid_pred = np.median(valid_pred, axis=1)\n            valid_pred = np.where(valid_pred >= 0, 1, 0).astype(int)\n            train_pred = np.median(train_pred, axis=1)\n            train_pred = np.where(train_pred >= 0, 1, 0).astype(int)\n          \n            u_score = utility_score(date=train.loc[te].date.values.reshape(-1),\n                                    weight=train.loc[te].weight.values.reshape(-1),\n                                    resp=train.loc[te].resp.values.reshape(-1),\n                                    action=valid_pred.reshape(-1))\n            u_score_max = utility_score(date=train.loc[te].date.values.reshape(-1),\n                                        weight=train.loc[te].weight.values.reshape(-1),\n                                        resp=train.loc[te].resp.values.reshape(-1),\n                                        action=train.loc[te].action.values.reshape(-1))\n            \n            u_score_tr = utility_score(date=train.loc[tr].date.values.reshape(-1),\n                                    weight=train.loc[tr].weight.values.reshape(-1),\n                                    resp=train.loc[tr].resp.values.reshape(-1),\n                                    action=train_pred.reshape(-1))\n            u_score_max_tr = utility_score(date=train.loc[tr].date.values.reshape(-1),\n                                        weight=train.loc[tr].weight.values.reshape(-1),\n                                        resp=train.loc[tr].resp.values.reshape(-1),\n                                        action=train.loc[tr].action.values.reshape(-1))\n            \n            # print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f},max_u_score:{u_score_max:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n                  # f\"time: {(time.time() - start_time) / 60:.2f}min\")\n            lr = optimizer.state_dict()['param_groups'][0]['lr']\n            print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, valid_loss:{valid_loss:.5f}, learning_rate:{lr}, \"\n                  f\"time: {(time.time() - start_time) / 60:.2f}min\")\n                  \n            print(f\"u_score_tr:{u_score_tr:.5f}, max_u_score_tr:{u_score_max_tr:.2f}, u_score:{u_score:.5f}, max_u_score:{u_score_max:.2f}, \"\n                  f\"F{fold} E{epoch:3}\")\n            f = open(\"record.txt\",\"a\")\n            f.write(f\"{fold},{epoch:3},{train_loss:.5f},{valid_loss:.5f},{u_score_tr:.5f},{u_score:.5f},{lr}\\n\")\n            f.close()\n            \n            es(u_score, model, model_path=ckp_path) \n            if es.early_stop:\n                print(\"Early stopping\")\n                break\n        # break # only train 1 model for fast, you can remove it to train 5 folds\n\n        # if using test data with gap; using test_with_gap dataset instead\n        test_set = MyDataset(test, train_features, targets)\n        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n        \n        model_best = Model1(train_features, targets)\n        model_best.to(device)\n        model_best.eval()\n        model_best.load_state_dict(torch.load(ckp_path))\n        test_preds, true_test_labels, test_loss = inference_fn(model_best, loss_fn, test_loader, device)\n        test_preds = np.median(test_preds, axis=1)\n        test_preds = np.where(test_preds >= 0, 1, 0).astype(int)\n\n        auc_score = roc_auc_score(test['action'].values.reshape(-1, 1), test_preds)\n        print(\"auc on test set:\", auc_score)\n        u_score = utility_score(date=test.date.values.reshape(-1),\n                                weight=test.weight.values.reshape(-1),\n                                resp=test.resp.values.reshape(-1),\n                                action=test_preds.reshape(-1))\n        print(\"utility score on test set:\", u_score)\n        u_score2 = utility_score(date=test.date.values.reshape(-1),\n                                weight=test.weight.values.reshape(-1),\n                                resp=test.resp.values.reshape(-1),\n                                action=test.action.values.reshape(-1))\n        print(\"max utility score on test set(if using true action):\", u_score2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### save train result"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp_set = MyDataset(train, train_features, targets)\n# tmp_loader = DataLoader(tmp_set, batch_size=batch_size, shuffle=False, num_workers=4)\n# preds, labels, loss = inference_fn(model, loss_fn, tmp_loader, device)\n\n# train_tmp = train[['date','weight','resp','ts_id']].copy()\n# train_tmp['preds'] = preds\n# train_tmp.to_csv('train_result.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_tmp.to_csv('train_result.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_preds, true_test_labels = inference_fn(model, test_loader, device)\n# test_preds = np.where(test_preds >= 0.5, 1, 0).astype(int)\n# u_score = utility_score(date=test.date.values.reshape(-1),\n#                         weight=test.weight.values.reshape(-1),\n#                         resp=test.resp.values.reshape(-1),\n#                         action=action1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## evaluate our model using test data (we define)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# models = []\n# for i in [4]: # for fast inference, you can change 1-->5 to get higher score\n#     torch.cuda.empty_cache()\n#     device = torch.device(\"cuda:0\")\n#     model = Model(train_features, targets)\n#     model.to(device)\n#     model.eval()\n#     ckp_path = f'/kaggle/input/skeleton-with-pytorch/JSModel_{i}.pth'\n#     model.load_state_dict(torch.load(ckp_path))\n#     models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import janestreet\n# env = janestreet.make_env()\n# env_iter = env.iter_test()\n\n# th = 0.5\n\n# for (test_df, pred_df) in tqdm(env_iter):\n#     if test_df['weight'].item() > 0:\n#         x_tt = test_df.loc[:, features].values\n        \n#         if np.isnan(x_tt.sum()):\n#             x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean.values.reshape(1, -1)\n\n#         feature_inp = pd.DataFrame(x_tt)\n#         feature_inp.columns = features\n#         feature_inp, _ = add_features(feature_inp,features)\n#         feature_inp = torch.tensor(feature_inp.values, dtype=torch.float).to(device)\n        \n#         pred = np.zeros((1, len(targets)))\n#         for model in models: \n#             pred += model(feature_inp).sigmoid().detach().cpu().numpy()\n#         pred /= len(models)\n        \n#         pred = pred.mean(axis=1).item()\n#         pred_df.action = int(pred >= th)\n        \n#     else:\n#         pred_df.action = 0\n        \n#     env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}