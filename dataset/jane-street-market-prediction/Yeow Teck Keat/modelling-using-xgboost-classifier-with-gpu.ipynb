{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of content\n\n1. Data processing and manipulation\n2. Model training with initial setting\n3. RandomizedSearchCv to search for best hyper parameter\n4. Model training with tuned setting\n5. Model error comparison between initial setting and tuned hyper-parameter model\n6. Tuned-model feature importance visualization"},{"metadata":{},"cell_type":"markdown","source":"### Part 1:Data processing and manipulation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport janestreet\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport time\nfrom xgboost import XGBRegressor# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"chunksize = 10 ** 6\nfilename = r'/kaggle/input/jane-street-market-prediction/train.csv'\ndata_chunk = []\nstart_time = time.time()\ndata_chunk = pd.read_csv(filename)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\nStart with processing the label , since evaluation is only fixed to either buy or pass\n\nDefine a function to return either 0 = Pass , 1 = Buy \n\n**Buy = weight * resp > 0**\n\n**Pass = weight * resp <= 0** \n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"def buy_or_pass(df):\n    if df['action'] > 0:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process on feature and action for model learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_chunk = data_chunk[data_chunk['weight'] > 0]\ndata_aggregated_weight = data_chunk.groupby(['date']).agg({'weight':['mean',np.std]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_aggregated_weight.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data mentioned the return will be based on weight and resp columns . create another column call return \ndef feature_action_split(dataframe_market):\n    '''\n    Input : Sample dataframe from Jane market prediction data\n    Output : feature = not response , weight , date or ts_id\n             action = 0 for pass and 1 for buy\n    '''\n#     dataframe_market = dataframe_market[dataframe_market['weight'] > 0]\n    dataframe_market['action'] = dataframe_market['weight']*dataframe_market['resp']\n    dataframe_market['action'] = dataframe_market.apply(buy_or_pass,axis=1) \n    feature = dataframe_market.drop(['date','weight','resp_1','resp_2','resp_3','resp_4','resp','ts_id','action'],axis=1)\n    print(\"Features columns : \",feature.columns)\n    action = dataframe_market[['action']]\n    print(\"Action counts : \\n\",action.value_counts())\n    return feature,action\n\nmissing_val = pd.DataFrame(data_chunk.isna().sum().sort_values(ascending=False)*100/data_chunk.shape[0],columns=['missing %'])\nmissing_val.style.background_gradient(cmap='Oranges_r')\n\nimport seaborn as sns\nsns.lineplot(x='ts_id',y='weight',data=data_chunk)\ndata_chunk = data_chunk[data_chunk['weight'] > 0]\nfeature,action = feature_action_split(data_chunk)\nfeature = feature.loc[:,['feature_44','feature_41','feature_45','feature_43','feature_62','feature_42','feature_5','feature_60','feature_6','feature_55']]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 2 : Model training with initial setting\n\nModel training . Imputer is not necessary needed for xgboost.\n\nWith basic model , we fixed the params to run.\n\nIf you are running xgboost on gpu , enable tree_method = 'gpu_hist' else run on normal CPU. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport time\n\n\ntrain_X, test_X, train_y, test_y = train_test_split(feature, action.values.flatten(), test_size=0.25) # By default shuffle is true\n\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)\nstart_time = time.time()\n\ninitial_model = xgb.XGBClassifier(n_estimators=1000, \n                        max_depth=5, \n                        learning_rate=0.1, \n                        subsample=0.7,\n                        colsample_bytree=0.8, \n                        colsample_bylevel=0.8, \n                        base_score=train_y.mean(),\n                        tree_method= 'gpu_hist',\n                        random_state=42, seed=42)\n\ninit_mod = initial_model.fit(train_X, train_y, \n                    early_stopping_rounds=10, \n                    eval_set=[(test_X, test_y)], eval_metric='error', \n                    verbose=100)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['pass','buy']\n\ndisp = plot_confusion_matrix(init_mod, test_X, test_y,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues)\nplt.title('Initial Model without tuning using Xgboost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initial result is decent where test data of 250k points. the losses will incurred when it's predicted __buy__ and it's actual __pass__.\n\nIdeal case for the market is to minimize losses where prediction false positive where true label is pass but predicted as buy and secondly maximizing profit where true positive is predicted buy and true label as buy.\n"},{"metadata":{},"cell_type":"markdown","source":"\n### Part 3: Model tuning using RandomizedSearchCv to search for best hyper parameter\n\n\n\nThis run will take time , it took 155 minutes to run over 1500 fits with GPU on. \n* Please make sure you on GPU setting when you are running this.\n\n__This part will be commented out as it will take approximately 150 minutes to run__"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV\n# params = {\n#         'learning_rate': [0.03, 0.01, 0.003, 0.001],\n#         'min_child_weight': [1,3, 5,7, 10],\n#         'gamma': [0, 0.5, 1, 1.5, 2, 2.5, 5],\n#         'subsample': [0.6, 0.8, 1.0, 1.2, 1.4],\n#         'colsample_bytree': [0.6, 0.8, 1.0, 1.2, 1.4],\n#         'max_depth': [3, 4, 5, 6, 7, 8, 9 ,10, 12, 14],\n#         'reg_lambda':np.array([0.4, 0.6, 0.8, 1, 1.2, 1.4])}\n\n# # specific parameters. I set early stopping to avoid overfitting and specify the validation dataset \n# fit_params = { \n#         'early_stopping_rounds':10,\n#         'eval_set':[(test_X, test_y)]}\n\n# # let's run the optimization\n# random_search = RandomizedSearchCV(init_mod, param_distributions=params, n_iter=500,\n#                                    scoring=\"precision\", n_jobs=-1,  verbose=3, random_state=42, cv=3 )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_search.fit(train_X,train_y, **fit_params)\n# print(\" Results from Random Search \" )\n# print(\"\\n The best estimator across ALL searched params:\\n\", random_search.best_estimator_)\n# print(\"\\n The best score across ALL searched params:\\n\", random_search.best_score_)\n# print(\"\\n The best parameters across ALL searched params:\\n\", random_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 4:  Model training with best hyper-parameter result"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport time\n\n\n\ntrain_X, test_X, train_y, test_y = train_test_split(feature, action.values.flatten(), test_size=0.25) # By default shuffle is true\n\n# my_imputer = SimpleImputer()\n# train_X = my_imputer.fit_transform(train_X)\n# test_X = my_imputer.transform(test_X)\nstart_time = time.time()\n\ntuned_model = xgb.XGBClassifier(n_estimators=1000, \n                        max_depth=16, \n                        learning_rate=0.03, \n                        subsample=0.6,\n                        colsample_bytree=0.6, \n                        colsample_bylevel=0.8, \n                        gamma=0.5,reg_lambda = 1.4,\n#                         objective = 'binary:logistic',\n                        base_score=train_y.mean(),\n                        tree_method= 'gpu_hist',\n                        random_state=42, seed=42)\n# 'subsample': 0.6, 'reg_lambda': 1.4, 'min_child_weight': 3, 'max_depth': 16, 'learning_rate': 0.03, 'gamma': 0.5, 'colsample_bytree': 0.6\n\n\ntuned_mod = tuned_model.fit(train_X, train_y, \n                    early_stopping_rounds=20, \n                    eval_set=[(test_X, test_y)], eval_metric='error', \n                    verbose=100)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['pass','buy']\n\ndisp = plot_confusion_matrix(tuned_mod, test_X, test_y,\n                             display_labels=class_names,\n                             cmap=plt.cm.Blues)\nplt.title('Tuned Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 5: Model error comparison between initial Xgboost classifer model and tuned hyperparameter xgboost classifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninit_model_res = init_mod.evals_result()\ntuned_model_res = tuned_mod.evals_result()\nepochs_init = len(init_model_res['validation_0']['error'])\nepochs_tuned = len(tuned_model_res['validation_0']['error'])\nx_axis_init = range(0, epochs_init)\nx_axis_tuned = range(0,epochs_tuned)\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis_init, init_model_res['validation_0']['error'], label='Initial classification error')\nax.plot(x_axis_tuned, tuned_model_res['validation_0']['error'], label='Tuned classification error')\n\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification error on test data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 6: Tuned-model feature importance visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nprint(tuned_mod.get_booster().get_score(importance_type='weight'))\n\nfig,ax = plt.subplots(figsize=(15,15))\nplot_importance(tuned_mod,ax=ax,max_num_features=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df = test_df.loc[:,['feature_44','feature_41','feature_45','feature_43','feature_62','feature_42','feature_5','feature_60','feature_6','feature_55']]\n\n    sample_prediction_df.action = tuned_mod.predict(test_df)\n    env.predict(sample_prediction_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}