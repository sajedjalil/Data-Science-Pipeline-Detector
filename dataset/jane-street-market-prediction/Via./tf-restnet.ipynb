{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport pathlib\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa \nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GPUs = tf.config.experimental.list_physical_devices(device_type='GPU')                                           \nfor gpu in GPUs:\n    tf.config.experimental.set_memory_growth(gpu, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('tensorflow_version_is',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED=42\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)  \n    tf.random.set_seed(seed)\nseed_everything(seed=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Loading dataset\nprint('Loading data...')\ntrain = pd.read_feather('../input/janestreet-save-as-feather/train.feather')\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend as K\n\nclass Mish(tf.keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return inputs * K.tanh(K.softplus(inputs))\n\n    def get_config(self):\n        base_config = super(Mish, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\ndef mish(x):\n    return tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n\ntf.keras.utils.get_custom_objects().update({'mish': tf.keras.layers.Activation(mish)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATA OF NN-MLP&RESTNET\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query('date > 85').reset_index(drop = True) \ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain.fillna(train.mean(),inplace=True)  # Fillna with its cols' mean value\n\nbase_features = [c for c in train.columns if \"feature\" in c]\nf_mean = np.mean(train[base_features[1:]].values,axis=0)  # Cause feature_0 shouldn't be fill with a float\n\ntrain['action'] = (train['resp'] > 0).astype('int')\ntrain['action_1'] = (train['resp_1'] > 0).astype('int')\ntrain['action_2'] = (train['resp_2'] > 0).astype('int')\ntrain['action_3'] = (train['resp_3'] > 0).astype('int')\ntrain['action_4'] = (train['resp_4'] > 0).astype('int')\n\ndef add_features(train_df):\n    train_df['feature_cross_41_42_43'] = train_df['feature_41']+train_df['feature_42']+train_df['feature_43']\n    train_df['feature_cross_1_2'] = train_df['feature_1']/(train_df['feature_2']+2e-5)\n    return train_df\n\ntrain = add_features(train)\nfeatures = [c for c in train.columns if \"feature\" in c]\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\nX = train[features].values\ny = np.stack([(train[c] > 0).astype('int') for c in target_cols]).T\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RESTNET_MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RestNet_(num_columns, \n             num_labels, \n             hidden_size, \n             dropout_rate, \n             label_smoothing, \n             learning_rate):\n    \n    inp = layers.Input(shape=(num_columns,))\n    x = layers.BatchNormalization()(inp)\n    x = layers.Dense(132)(x)\n    x = layers.LeakyReLU()(x)\n    x = layers.Dropout(0.315)(x)\n    \n    \n    x1 = layers.Dense(hidden_size*1.2)(x)\n    x1 = layers.BatchNormalization()(x1)\n    x1 = layers.Activation('mish')(x1)\n    x1 = layers.Dropout(dropout_rate)(x1)\n\n    x = layers.concatenate([x, x1], axis=1)\n\n    x2 = layers.Dense(hidden_size*1.1)(x)\n    x2 = layers.BatchNormalization(axis=1)(x2)\n    x2 = layers.Activation('mish')(x2)\n    x2 = layers.Dropout(dropout_rate)(x2)\n    \n    x = layers.concatenate([x1, x2], axis=1)\n\n    \n    x3 = layers.Dense(hidden_size*1.0)(x)\n    x3 = layers.BatchNormalization(axis=1)(x3)\n    x3 = layers.Activation('mish')(x3)\n    x3 = layers.Dropout(dropout_rate)(x3)\n    \n    x = layers.concatenate([x2, x3], axis=1)\n\n    x4 = layers.Dense(hidden_size*0.9)(x)\n    x4 = layers.BatchNormalization(axis=1)(x4)\n    x4 = layers.Activation('mish')(x4)\n    x4 = layers.Dropout(dropout_rate)(x4)\n    \n    x = layers.concatenate([x3, x4], axis=1)\n    \n    x5 = layers.Dense(hidden_size*0.8)(x)\n    x5 = layers.BatchNormalization(axis=1)(x5)\n    x5 = layers.LeakyReLU()(x5)\n    x5 = layers.Dropout(dropout_rate)(x5)\n    \n    x = layers.concatenate([x1, x3, x5], axis=1)\n    x = layers.Dense(num_labels)(x)    \n    \n    out = layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate,weight_decay=1e-5),\n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n                  metrics=tf.keras.metrics.AUC(name=\"AUC\")\n                 )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set defaults\nhidden_units = 256\ndropout_rates = 0.3\nlabel_smoothing = 5e-3\nlearning_rate = 1e-3\n\nmodel = RestNet_(X.shape[1],\n                 y.shape[1], \n                 hidden_units,\n                 dropout_rates,\n                 label_smoothing,\n                 learning_rate)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 7\nEPOCHS = 500\nBATCH_SIZE = 6500\nTRAINING = False\nCV = True\n\nif TRAINING:\n    if CV:\n        gkf = PurgedGroupTimeSeriesSplit(n_splits = NUM_FOLDS, group_gap=15)\n        splits = list(gkf.split(y, groups=train['date'].values))    \n        for fold, (train_indices, test_indices) in enumerate(splits):\n            keras.backend.clear_session()\n\n            reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                          verbose=1,\n                                                          factor=0.2,\n                                                          patience=12, mode='min')\n            early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=25)\n            checkpoint_callback = keras.callbacks.ModelCheckpoint(\n                filepath='logs/BestVal_model_{}.h5'.format(fold),\n                save_weights_only=True,\n                monitor='val_AUC',\n                mode='max',\n                verbose=1,\n                save_best_only=True)\n\n            model = RestNet_(X.shape[1],\n                         y.shape[1], \n                         hidden_units,\n                         dropout_rates,\n                         label_smoothing,\n                         learning_rate)\n\n            X_train, X_test = X[train_indices], X[test_indices]\n            y_train, y_test = y[train_indices], y[test_indices]\n\n            model.fit(X_train,\n                      y_train,\n                      validation_data=(X_test,y_test),\n                      epochs=EPOCHS,\n                      batch_size=BATCH_SIZE,\n                      callbacks=[reduce_lr,\n                                 early_stop,\n                                 checkpoint_callback])\n\n            model.save_weights(f'logs/model_{SEED}_{fold}.h5')\n\n            del model\n            gc.collect()\n    else:\n        keras.backend.clear_session()\n        model = RestNet_(len(features1),\n                     len(features2),\n                     y.shape[1], \n                     hidden_units,\n                     dropout_rates,\n                     label_smoothing,\n                     learning_rate)\n        \n        model.fit(X,\n                  y,\n                  epochs=EPOCHS,\n                  batch_size=BATCH_SIZE)\n\n        model.save_weights(f'.logs/model_{SEED}_NONE.h5')\n        \nelse:\n    SEED1=42\n#    Val_models = []\n    Full_models_42 = []\n    for fold in range(NUM_FOLDS):\n        tf.keras.backend.clear_session()\n        model1 = RestNet_(X.shape[1],\n                          y.shape[1], \n                          hidden_units,\n                          dropout_rates,\n                          label_smoothing,\n                          learning_rate)\n        \n        model1.load_weights(pathlib.Path(f'../input/drop0315/model_{SEED1}_{fold}.h5'))\n        Full_models_42.append(model1)\n    \n    SEED2=73\n#    Val_models = []\n    Full_models_73 = []\n    for fold in range(NUM_FOLDS):\n        tf.keras.backend.clear_session()\n        model2 = RestNet_(X.shape[1],\n                          y.shape[1], \n                          hidden_units,\n                          dropout_rates,\n                          label_smoothing,\n                          learning_rate)\n        \n        model2.load_weights(pathlib.Path(f'../input/73cv7/model_{SEED2}_{fold}.h5'))\n        Full_models_73.append(model2)\n        \n    SEED3=2021\n#    Val_models = []\n    Full_models_2021 = []\n    for fold in range(NUM_FOLDS):\n        tf.keras.backend.clear_session()\n        model3 = RestNet_(X.shape[1],\n                          y.shape[1], \n                          hidden_units,\n                          dropout_rates,\n                          label_smoothing,\n                          learning_rate)\n        \n        model3.load_weights(pathlib.Path(f'../input/2021cv7/model_{SEED3}_{fold}.h5'))\n        Full_models_2021.append(model3)\n        \n#    for fold in range(NUM_FOLDS):\n#         tf.keras.backend.clear_session()\n#         model2 = RestNet_(X.shape[1],\n#                          y.shape[1], \n#                          hidden_units,\n#                          dropout_rates,\n#                          label_smoothing,\n#                          learning_rate)   \n#         model2.load_weights(pathlib.Path(f'../input/drop0315/BestVal_model_{fold}.h5'))\n#         Val_models.append(model2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Val_models = Val_models[1:]\nFull_models_42 = Full_models_42[1:]\nFull_models_73 = Full_models_73[1:]\nFull_models_2021 = Full_models_2021[1:]\nprint(len(Full_models_42) + len(Full_models_73) + len(Full_models_2021))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclass LiteModel:\n    \n    @classmethod\n    def from_file(cls, model_path):\n        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n    \n    @classmethod\n    def from_keras_model(cls, kmodel):\n        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n        tflite_model = converter.convert()\n        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n    \n    def __init__(self, interpreter):\n        self.interpreter = interpreter\n        self.interpreter.allocate_tensors()\n        input_det = self.interpreter.get_input_details()[0]\n        output_det = self.interpreter.get_output_details()[0]\n        self.input_index = input_det[\"index\"]\n        self.output_index = output_det[\"index\"]\n        self.input_shape = input_det[\"shape\"]\n        self.output_shape = output_det[\"shape\"]\n        self.input_dtype = input_det[\"dtype\"]\n        self.output_dtype = output_det[\"dtype\"]\n        \n    def predict(self, inp):\n        inp = inp.astype(self.input_dtype)\n        count = inp.shape[0]\n        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n        for i in range(count):\n            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n            self.interpreter.invoke()\n            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n        return out\n    \n    def predict_single(self, inp):\n        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n        inp = np.array([inp], dtype=self.input_dtype)\n        self.interpreter.set_tensor(self.input_index, inp)\n        self.interpreter.invoke()\n        out = self.interpreter.get_tensor(self.output_index)\n        return out[0]\n\n# tflite_models_val=[]\n\n# for i in range(len(Val_models)):\n#     tflite_model_ = LiteModel.from_keras_model(Val_models[i])\n#     tflite_models_val.append(tflite_model_)\n    \n    \ntflite_models_42=[]\nfor i in range(len(Full_models_42)):\n    tflite_model_ = LiteModel.from_keras_model(Full_models_42[i])\n    tflite_models_42.append(tflite_model_)\n    \ntflite_models_73=[]\nfor i in range(len(Full_models_73)):\n    tflite_model_ = LiteModel.from_keras_model(Full_models_73[i])\n    tflite_models_73.append(tflite_model_)\n    \ntflite_models_2021=[]\nfor i in range(len(Full_models_2021)):\n    tflite_model_ = LiteModel.from_keras_model(Full_models_2021[i])\n    tflite_models_2021.append(tflite_model_)\n\nprint(len(tflite_models_42) + len(tflite_models_73) + len(tflite_models_2021))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = np.median\nth = 0.502\nweight_model = [1,1,1,2,2,2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        test_df_ = test_df.loc[:, base_features].values\n        if np.isnan(test_df_[:, 1:].sum()):\n            test_df_[:, 1:] = np.nan_to_num(test_df_[:, 1:]) + np.isnan(test_df_[:, 1:]) * f_mean\n            \n        cross_41_42_43 = test_df_[:, 41] + test_df_[:, 42] + test_df_[:, 43]\n        cross_1_2 = test_df_[:, 1] / (test_df_[:, 2] + 2e-5)\n        x_tt = np.concatenate((\n            test_df_,\n            np.array(cross_41_42_43).reshape(test_df_.shape[0], 1),\n            np.array(cross_1_2).reshape(test_df_.shape[0], 1),\n        ), axis=1)\n        \n        pred_42 = np.average([clf.predict(x_tt) for clf in tflite_models_42], axis=0, weights=np.array(weight_model))\n        pred_42 = f(pred_42)\n        pred_73 = np.average([clf.predict(x_tt) for clf in tflite_models_73], axis=0, weights=np.array(weight_model))\n        pred_42 = f(pred_73)\n        pred_2021 = np.average([clf.predict(x_tt) for clf in tflite_models_2021], axis=0, weights=np.array(weight_model))\n        pred_2021 = f(pred_2021)\n        pred = pred_42*0.3 + pred_73*0.3 + pred_73*0.4\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}