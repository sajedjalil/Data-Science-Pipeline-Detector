{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick planning\n\n- UX design: Design thinking approach\n- Modeling language: UML\n- Programming type: Object-Oriented Programming\n- Environments: pandas, scikit-learn, pmdarima\n- Computer-science field: Machine Learning\n- Knowledge field: trading - International Finance Management"},{"metadata":{},"cell_type":"markdown","source":"# Quick Idea\n\n- Get dataset\n- Get trading models\n- Get a trading model tuner\n- Get a trading model optimizer\n- Get a returns optimizer\n- Get a submission file"},{"metadata":{},"cell_type":"markdown","source":"# Request of missing packages\nFurther information on the request guide is available at [Missing Packages](http://https://github.com/Kaggle/docker-python/wiki/Missing-Packages)\n\nMissing packages\n- [pmdarima](http://https://pypi.org/project/pmdarima/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# System Mode"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"\"\"\"\nTurn to code when testing\n\"\"\"\ntry:\n    print(\"Set the system mode (1) Test mode (2) Commissioning mode \")\n    systemModeOption = int(input())\n\nexcept StdinNotImplementedError or NameError:\n    print(\"System runs without support of input requests due to Kaggle competition rules.\")\n    systemModeOption = 2\n\nif(systemModeOption == 1): systemMode = 'test'\nif(systemModeOption == 2): systemMode = 'commissioning'"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"System runs without support of input requests due to Kaggle competition rules.\")\nsystemModeOption = 2\nif(systemModeOption == 1): systemMode = 'test'\nif(systemModeOption == 2): systemMode = 'commissioning'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase 0 | Get raw dataset and customization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getCustomization():\n    \n    print(\"=========================================\")\n    print(\"Project : Market prediction\")\n    print(\"=========================================\")\n    print(\"Customization\")\n    print(\"=========================================\")\n    \n    # Input testing type\n    print(\"\\nRegression method for a linear model\")\n    preference0a = input(\"(1) Ordinary Least Squares (Cross-validation 10 folds) (2) Bayesian Ridge (Cross-validation 10 folds) (3) Bayesian Ridge: \" )\n    preference0a = int(preference0a)\n    \n    if(preference0a == 1): # Ordinary Least Squares\n        preference0b = 'NA'\n        # Input sample size | train\n        preference1 = input(\"Train dataset | Input instances size (0 - 1,048,575 ): \")\n        preference1 = int(preference1)\n        preference2 = 'NA'\n\n    \n    if(preference0a == 2): # Bayesian Ridge (Cross validaation | 10 folds)\n        preference0b = 'NA'\n        # Input sample size | train\n        preference1 = input(\"Train dataset | Input instances size (0 - 1,048,575 ): \")\n        preference1 = int(preference1)\n        preference2 = 'NA'\n    \n    if(preference0a == 3): # Bayesian Ridge\n        preference0b = 'NA'\n        # Input sample size | train\n        preference1 = input(\"Train dataset | Input instances size (0 - 1,048,575 ): \")\n        preference1 = int(preference1)\n        preference2 = 'NA'\n        \n\n    preference0 = [preference0a] + [preference0b]\n    \n    # Input prefered features | train and test\n    \n    preference3Previous = input(\"\\nMarket Features | Input (1) 3 features (2) 129 features: \")\n    preference3Previous = int(preference3Previous)\n    \n    if(preference3Previous == 1):\n        print(\"\\nMarket Features | Input 3 numbers from 0 to 129 \")\n        preference3a = input(\"   Number 1: \")\n        preference3a = 'feature_' + str(preference3a)\n        \n        preference3b = input(\"   Number 2: \")\n        preference3b = 'feature_' + str(preference3b)\n        \n        preference3c = input(\"   Number 3: \")\n        preference3c = 'feature_' + str(preference3c)\n        \n        preference3 = [preference3a] + [preference3b] + [preference3c]\n    \n    else:\n        featuresToInclude = []\n        \n        for i in range(0,130):\n            itemToInclude = 'feature_' + str(i)\n            featuresToInclude = featuresToInclude + [itemToInclude]\n        \n        preference3 = featuresToInclude\n    \n    # Input return value policy to value actions\n    ## (1) Model prediction correspondance: 1-point action value when an actual return corresponds to a predicted return and it is within the confidence interval.\n    ## (2) Maximax correspondance: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return.\n    ## (3) Maximax correspondance biased by tag-confidence perception: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return. Then, the action value has to be multiplied by the confidence probability. Finally, the result must be rounded to the nearest decision-making boundary, i.e., 0 or 1.\n    \n    print(\"\\n_________________________________________\")\n    print(\"\\nReturn value policy \")\n    print(\"\\n(1) Prediction Model correspondance: Assign 1-point action value when an actual return corresponds to a predicted return and it is within the confidence interval.\")\n    print(\"\\n(2) Maximax correspondance: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return.\")\n    print(\"\\n(3) Maximax correspondance biased by tag-confidence perception: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return. Then, the action value has to be multiplied by the confidence probability. Finally, the result must be rounded to the nearest decision-making boundary, i.e., 0 or 1.\")\n    preference4 = input(\"\\n   Option: \")\n    preference4 = int(preference4)\n    \n    # Input risk exposure to loss\n    print(\"\\n_________________________________________\")\n    print(\"\\nRisk exposure to loss\")\n    print(\"\\nScenarios | (1) 25% (2) 50% (3) 75% \")\n    preference5 = input(\"\\n   Option: \")\n    preference5 = int(preference5)\n    \n    # Input investing proficiency\n    # Investing proficiency: (No proficiency) 0.25 - 0.5 - 0.75 - 0.99 (proficiency)\n    print(\"\\n_________________________________________\")\n    print(\"\\nInvesting proficiency\")\n    print(\"\\nScenarios | (1) 25%-beginner  (2) 50% (3) 75% (4) 99%-expert\")\n    preference6 = input(\"\\n   Option: \")\n    preference6 = int(preference6)\n    \n    # Build customization vector\n    customizationVector = [preference0, preference1, preference2, preference3, preference4, preference5, preference6]\n    \n    return customizationVector\n\n\ndef getRawDataset(customizationVector):\n    \"\"\" Function getRawDataset get Kaggle dataset: train, test, submission \"\"\"\n    import pandas as pd\n    import numpy as np\n    \n    # Get datasets\n\n    if(customizationVector[0][0] in [1,2,3]): # Bayesian Ridge\n\n        # Train | Set columns and rows to be used\n        usecolsTrain = ['date','resp','resp_1','resp_2','resp_3','resp_4'] + customizationVector[3] + ['weight','ts_id']\n        nrowsTrain = customizationVector[1]\n\n        # Train | Raw dataset\n        filename = 'train.csv'\n        path = '/kaggle/input/jane-street-market-prediction/' #path = 'E:/Kaggle_MarketData/1-Data/'\n        rawDataset = pd.read_csv(path+filename, nrows=nrowsTrain, usecols=usecolsTrain)\n        labels =  list(rawDataset.index)\n        \n        # Train\n        rawDatasetTrain = rawDataset.drop(labels[nrowsTrain:])\n        \n        # Test | Set columns and rows to be used\n        usecolsTest = ['date'] + customizationVector[3] + ['weight','ts_id']\n        nrowsTest = customizationVector[1]\n\n        # Test | Raw dataset\n        filename = 'example_test.csv'\n        path = '/kaggle/input/jane-street-market-prediction/' #path = 'E:/Kaggle_MarketData/1-Data/'\n        rawDataset_test = pd.read_csv(path+filename, nrows=nrowsTest, usecols=usecolsTest)\n        labels =  list(rawDataset_test.index)\n        \n        # Test\n        rawDatasetTest = rawDataset_test.drop(labels[nrowsTest:])\n        \n\n        \n    # Submission file\n    filename = 'example_sample_submission.csv'\n    path = '/kaggle/input/jane-street-market-prediction/' #path = 'E:/Kaggle_MarketData/1-Data/'\n    rawDatasetSubmission = pd.read_csv(path+filename)\n    \n    # Feature file\n    filename = 'features.csv'\n    path = '/kaggle/input/jane-street-market-prediction/' #path = 'E:/Kaggle_MarketData/1-Data/'\n    rawFeatureDataset = pd.read_csv(path+filename)\n    \n    return rawDatasetTrain, rawDatasetTest, rawDatasetSubmission, rawFeatureDataset\n\n\ndef f(x, noise_amount):\n    \"\"\" Support function to predict \"\"\"\n    import numpy as np\n    \n    y = np.sqrt(x) * np.sin(x)\n    noise = np.random.normal(0, 1, len(x))\n    \n    return y + noise_amount * noise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase 1 | Object Building - Object 1: Linear Model Project"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearModelProject(object):\n    \"\"\"\n    LinearModelProject is an Linear Model solution that returns a Kaggle submission. \n    \n    \"\"\"\n    \n    def __init__(self):\n        \"\"\" Create empty DataFrame \"\"\"\n        import numpy as np\n        import pandas as pd\n        import matplotlib.pyplot as plt\n        \n        # Empty objects\n        dictionaryEmpty = {}\n        DataFrameEmpty = pd.DataFrame(data=dictionaryEmpty)\n        \n        # Default DataFrames\n        \"\"\" getDatasetDefault loads Arima dataset in the view of Smith et al. (2019)\"\"\"\n        #self.df = load_msft()\n        self.dfc = DataFrameEmpty\n        \n        # DataFrames | datasets (raw)\n        self.datasetTrain = DataFrameEmpty\n        self.datasetTest = DataFrameEmpty\n        \n        # DataFrames | datasets (conditioned)\n        self.train = DataFrameEmpty\n        self.test = DataFrameEmpty\n\n    \n    def insert(self,rawDatasetTrain,rawDatasetTest,train,test,customizationVector):\n        \"\"\" Assumes rawDtasetTrain is the Kaggle-train dataset \"\"\"\n        \"\"\" Assumes rawDtasetTest is the Kaggle-test dataset \"\"\"\n        \n        # Dataset (raw)\n        self.datasetTrain = rawDatasetTrain\n        self.datasetTest = rawDatasetTest\n        \n        # Dataset (conditioned)\n        self.train = train\n        self.test = test\n        \n        # Customization vector\n        self.customizationVector = customizationVector\n    \n    def getDatasets(self):\n        \"\"\" Get inserted datasets \"\"\"\n        \n        return self.datasetTrain, self.datasetTest, self.train, self.test\n\n    \n    def datasetConditioning(self, dataset):\n        \"\"\" Conditioning 1: Set date typification in ISO 8601\"\"\"\n        \n        # Libraries\n        from datetime import date\n        import pandas as pd\n        \n        self.dfc = dataset\n\n        \n        # Typing 'date'\n        self.dfc.astype({'date': 'str'})\n        \n        # Conditioning\n        datasetRange = len(self.dfc)\n        isoDateList = []\n        #startNumber = self.dfc['date'][0]\n        startNumber = self.dfc.index[0]\n        \n        for i in range(startNumber,datasetRange):\n            \n            # State | Reference date -> year = 2000          \n            dayNumber = int(self.dfc.at[i,'date'])\n            timesToSubstract_month = dayNumber//30\n            day = dayNumber - (timesToSubstract_month * 30)\n            if (day == 0): day = 1\n            \n            monthNumber = timesToSubstract_month\n            timesToSubstract_year = monthNumber//12\n            month = monthNumber - (timesToSubstract_year * 12)\n            if (month == 0): month = 1\n            \n            year = timesToSubstract_year + 2000\n\n            # Date list maker\n            dateToInclude = date(year, month, day).isoformat()\n            isoDateList = isoDateList + [dateToInclude]\n\n        # Insert 'date' with ISO 8601 values    \n        \n        self.dfc = self.dfc.drop(columns='date')\n        isoDateList = pd.Series(isoDateList)\n        self.dfc.insert(column='date',value=isoDateList,loc=0)\n        \n        \n        return self.dfc\n        \n    \n    def getLinearModel(self,train,test,customizationVector):\n        \"\"\"\n        getLinearModel_BayesianRidgeRegression return a linear model solution intended for regression using Bayesian Ridge Regression\n        Code adapted from e scikit-learn example available at https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py\n        \"\"\"\n        import numpy as np\n        import matplotlib.pyplot as plt\n        from scipy import stats\n        import pandas as pd\n        \n        from sklearn.linear_model import BayesianRidge, LinearRegression\n        from sklearn.model_selection import cross_val_predict\n        \n        # Set input DataFrames | y -> resp\n        X = train.drop(columns = ['date','resp'])       \n        X = X.fillna(0)\n        #X = X.to_numpy()\n        \n        yDictionary = {}\n        yDictionary['resp'] = train.resp\n        y = pd.DataFrame(data=yDictionary)\n        y = y.fillna(0)\n        #y = y.to_numpy()\n        \n        if (customizationVector[0][0] == 1): # Ordinary Least Squares\n            # Fit the model\n            clf = LinearRegression()\n            clf.fit(X, y)\n            \n            # Fit the model\n            clf = BayesianRidge(compute_score=True)\n            clf.fit(X, y)\n       \n            # Get predictions\n            # cross_val_predict returns an array of the same size as `y` where each entry\n            # is a prediction obtained by cross validation:\n            y_pred = cross_val_predict(clf, X, y, cv=10)\n            y_pred_std = []\n            for i in range(0,len(y_pred)):y_pred_std = y_pred_std + [np.nan]\n\n\n        if (customizationVector[0][0] == 2): # Bayesian Ridge Regression\n            # Fit the model\n            clf = BayesianRidge(compute_score=True)\n            clf.fit(X, y)\n        \n            # Get predictions\n            y_pred, y_pred_std = clf.predict(X, return_std=True)        \n        \n        if (customizationVector[0][0] == 3): # Bayesian Ridge Regression | Cross-validation (10 folds)\n            # Fit the model\n            clf = BayesianRidge(compute_score=True)\n            clf.fit(X, y)\n       \n            # Get predictions\n            # cross_val_predict returns an array of the same size as `y` where each entry\n            # is a prediction obtained by cross validation:\n            y_pred = cross_val_predict(clf, X, y, cv=10)\n            y_pred_std = []\n            for i in range(0,len(y_pred)):y_pred_std = y_pred_std + [np.nan]\n            \n        # Plotting Predictions\n        fig, ax = plt.subplots()\n        ax.scatter(y, y_pred, edgecolors=(0, 0, 0))\n        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n        ax.set_xlabel('Measured')\n        ax.set_ylabel('Predicted')\n        plt.show()\n        \n        # Create a DataFrame with forecast instances\n        forecastsDictionary = {}\n        forecastsDictionary['respPredict'] = y_pred\n        forecastsDataFrame = pd.DataFrame(data=forecastsDictionary, index=train.index)\n\n        # Create a Dataframe with confidence intervals\n        confidence_intervalsDictionary = {}\n        confidence_intervalsDictionary['respStd'] = y_pred_std\n        confidence_intervalsDataFrame = pd.DataFrame(data=confidence_intervalsDictionary,index=train.index)\n\n        # Include forecast instances into training dataset\n        trainPredict = pd.concat([train, forecastsDataFrame], axis=1, join=\"inner\")\n        trainPredict = pd.concat([trainPredict, confidence_intervalsDataFrame], axis=1, join=\"inner\")\n        \n        # https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py\n        # https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression\n        # https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py\n        \n        return trainPredict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase 1 | Object Building - Object 2: Feature Analytics Project"},{"metadata":{"trusted":true},"cell_type":"code","source":"class featureAnalytics(object):\n    \"\"\"\n    featureAnalytics returns a component to biased models.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\" Create empty DataFrame \"\"\"\n        import numpy as np\n        import pandas as pd\n        \n        # Empty objects\n        dictionaryEmpty = {}\n        DataFrameEmpty = pd.DataFrame(data=dictionaryEmpty)\n        \n        # DataFrames | datasets (raw)\n        self.featureDataset = DataFrameEmpty\n        \n        # DataFrames | datasets (conditioned)\n        self.confidenceProbabilityDataset = DataFrameEmpty\n\n    \n    def insert(self,customizationVector,rawFeatureDataset):\n        \"\"\" Assumes rawFeatureDataset is the Kaggle-feature dataset \"\"\"\n        \n        # Dataset (raw)\n        self.featureDataset = rawFeatureDataset\n        \n        # Customization vector\n        self.customizationVector = customizationVector\n    \n    \n    def getDatasets(self):\n        \"\"\" Get inserted datasets \"\"\"\n        \n        return self.featureDataset\n    \n    \n    def confidenceProbabilityProject(self,featureDataset,customizationVector):\n        \"\"\" Get confidence probabilities to support computations under a return value policy 3 \"\"\"\n        \n        from math import exp\n        \n        # Get feature mixture\n        featureMixture = customizationVector[3]\n        \n        # Get proficiency constant\n        if(customizationVector[6] == 1):proficiencyConstant = 0.25\n        if(customizationVector[6] == 2):proficiencyConstant = 0.50\n        if(customizationVector[6] == 3):proficiencyConstant = 0.75\n        if(customizationVector[6] == 4):proficiencyConstant = 0.99\n        \n        # Get confidence probability list\n        \n        trueList = []\n        \n        \n        for i in range(0,len(featureDataset.feature)):\n                \n            if featureDataset.feature[i] in featureMixture:\n                trueSet = sum(featureDataset.loc[i][1:] == True) #series\n                \n                if(trueSet != 0):\n                    trueList = trueList + [trueSet]\n        \n        # Get confidence probability value\n        featureManaged = len(featureMixture)\n        probability = sum(trueList)/(featureManaged*29)\n        \n        # Adjust probability using a success factor due to expertise\n        ## Investing proficiency: (No proficiency) 0 - 0.25 - 0.5 - 0.75 - 0.99 (proficiency)\n        ## featureManagedAdjustment: Tune constant to adjust the factor so that confidenceProbability is the highest one if proficiency is the best one when managing the maximum possible available features\n        ## It is assumed that featureManagedAdjustment implies that feature management goes more suitable in the measure in which more features are managed.\n        \n        featureManagedAdjustment = exp(2.62) \n        factor = proficiencyConstant * featureManaged / featureManagedAdjustment\n        confidenceProbability = probability * factor \n        \n        return confidenceProbability #confidenceProbability","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase 1 | Object Building - Object 3: Returns optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class returnsOptimizer(object):\n    \"\"\"\n    returnsOptimizer builds action instances maximizing returns on the trade (i.e. resp) given a volatility policy.\n    returnsOptimizer returns the utility score in the basis of the optimization\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\" Create empty DataFrame \"\"\"\n        import numpy as np\n        import pandas as pd\n        \n        # Empty objects\n        dictionaryEmpty = {}\n        DataFrameEmpty = pd.DataFrame(data=dictionaryEmpty)\n        \n        # Default DataFrames\n        self.dfc = DataFrameEmpty\n        \n        # DataFrames | training dataset with predictions\n        self.train = DataFrameEmpty\n        self.trainAction = DataFrameEmpty\n        \n        # Lists | customizationVector\n        self.customizationVector = []\n\n    \n    def insert(self,trainPredict,customizationVector,confidenceProbability):\n        \"\"\" Assumes rawDtasetTrain is the Kaggle-train dataset \"\"\"\n        \"\"\" Assumes rawDtasetTest is the Kaggle-test dataset \"\"\"\n        \n        # Variables\n        self.probability = confidenceProbability\n        \n        # Dataset (conditioned)\n        self.train = trainPredict\n        \n        # Customization vector\n        self.customizationVector = customizationVector\n\n\n    def actionValuer(self,train,customizationVector,probability):\n        \"\"\"\n        Policy 1 - Model prediction correspondance\n        Action valuer policy due to Kaggle competition context (Optimizing criterion)\n        << Assign 1 to action value if the test return value keeps similarity with\n        its corresponding prediction instance. >>\n        i.e. test return value keeps within the confidence interval computed for predicting\n        Underpinned statement (theory): Outliers, test instances outside the confidence\n        interval of the prediction model, disrupt the model so those instances are deemed under loss exposure.\n        \n        Policy 2 - Maximax correspondance\n        << Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return.\" >>\n        Underpinned statement (theory): Selecting just gain instances (i.e. + actual returns and + predicted returns) will reach return maximization.\n        \n        Policy 3 - Maximax correspondance biased by tag-confidence perception\n        <<Assign points as policy 2 but times a confidence probability.>>\n        \n        Good Practice theory on Jane Street Investment manners: \"Trade-decision making using ML models to predict is biased by a tailored milestone detection.\"\n        \n        Assumptions:\n        (1) In Kaggle competition, the tag assignment is provided as \"features.csv\".\n        (2) The tag assignment refers to a view of the return behaviour in terms of tracing milestones given a set of trading transactions.\n        (3) A feature mixture requires a confidence probability to biased model prediction values due to a given tag assignment.\n        (4) A tag is deemed an attribute that specifies a feature. \n        (5) A tag-assignment refers to flag a confidence attribute that configures a feature customization for predicting.\n        (6) The confidence probability implies a trade decision if it is more than a minimum probability of occurence to trade. In other words, the confidence probability is bounded below to a minimum expected probability of occurrence\n        (7) The probability of occurrence to trade implies the minimum affordable probability to avoid a loss risk. Hence, if the confidence probability is smaller than the probabilit of occurrence, it is recommende to avoid trading.\n        \"\"\"\n        # Set risk exposure to loss\n        if(customizationVector[5] == 1): riskExposure = 0.75\n        if(customizationVector[5] == 2): riskExposure = 0.50\n        if(customizationVector[5] == 3): riskExposure = 0.25\n        \n        \n        if (customizationVector[4] == 1): # Return Value policy does not apply in this case; it is for ARIMA.\n            option = customizationVector[0][0]\n            if(option in [1,2,3]):\n            \n                import pandas as pd\n                \n                # Value assignment\n                actionList = []\n                trainRange = len(train)\n                for i in range(0,trainRange):\n                    \n                    itemToValue = train.resp[i]\n                    lowerBoundary = train.confInterval[i][0]\n                    upperBoundary = train.confInterval[i][1]\n                    \n                    \n                    if(itemToValue >= lowerBoundary and itemToValue <= upperBoundary and probability >= riskExposure):\n                        actionList = actionList + [1]\n                    else:\n                        actionList = actionList + [0]\n                \n                # Concatenate DataFrames| trainPredict and action\n                \n                ## Create a DataFrame with forecast instances\n                actionDictionary = {}\n                actionDictionary['action'] = actionList\n                actionDataFrame = pd.DataFrame(data=actionDictionary, index=train.index)\n        \n                ## Include action instances into training dataset\n                trainAction = pd.concat([train, actionDataFrame], axis=1, join=\"inner\")\n                self.trainAction = trainAction\n        \n        if (customizationVector[4] == 2):\n            option = customizationVector[0][0]\n            if(option in [1,2,3]):\n            \n                import pandas as pd\n                \n                # Value assignment\n                actionList = []\n                trainRange = len(train)\n                for i in range(0,trainRange):\n                    \n                    itemToValue = train.resp[i]\n                    \n                    if(itemToValue > 0 and probability >= riskExposure):\n                        actionList = actionList + [1]\n                    else:\n                        actionList = actionList + [0]\n                \n                # Concatenate DataFrames| trainPredict and action\n                \n                ## Create a DataFrame with forecast instances\n                actionDictionary = {}\n                actionDictionary['action'] = actionList\n                actionDataFrame = pd.DataFrame(data=actionDictionary, index=train.index)\n        \n                ## Include action instances into training dataset\n                trainAction = pd.concat([train, actionDataFrame], axis=1, join=\"inner\")\n                self.trainAction = trainAction\n                \n        if (customizationVector[4] == 3):\n            option = customizationVector[0][0]\n            if(option in [1,2,3]):\n            \n                import pandas as pd\n                \n                actionList = []\n                trainRange = len(train)\n                for i in range(0,trainRange):\n                    \n                    itemToValue = train.resp[i]\n                    \n                    if(itemToValue > 0 and probability >= riskExposure):\n                        actionList = actionList + [1]\n                    else:\n                        actionList = actionList + [0]\n\n                \n                # Concatenate DataFrames| trainPredict and action\n                \n                ## Create a DataFrame with forecast instances\n                actionDictionary = {}\n                actionDictionary['action'] = actionList\n                actionDataFrame = pd.DataFrame(data=actionDictionary, index=train.index)\n        \n                ## Include action instances into training dataset\n                trainAction = pd.concat([train, actionDataFrame], axis=1, join=\"inner\")\n                self.trainAction = trainAction\n            \n        \n        return self.trainAction\n    \n    \n    def utilityScore(self,train):\n        \"\"\"\n        Returns utility score proposed by Kaggle\n        \"\"\"\n        import pandas as pd\n        \n        # Create pUnitary DataFrame\n        pUnitaryDictionary = {}\n        pUnitary2Dictionary = {}\n        pUnitaryList = []\n        pUnitary2List = []\n        \n        for i in train.index: # Issue -0.0\n\n            weight = train.weight[i]\n            resp = train.resp[i]\n            action = train.action[i]\n            \n            pUnitary = weight * resp * action\n            pUnitaryList = pUnitaryList + [pUnitary]\n            \n            pUnitary2 = pUnitary**2 \n            pUnitary2List = pUnitary2List + [pUnitary2]\n        \n        pUnitaryDictionary['pUnitary'] = pUnitaryList\n        pUnitaryDataFrame = pd.DataFrame(data=pUnitaryDictionary, index=train.index)\n        \n        pUnitary2Dictionary['pUnitary2'] = pUnitary2List\n        pUnitary2DataFrame = pd.DataFrame(data=pUnitary2Dictionary, index=train.index)\n        \n        # Concatenate DataFrames | train and pUnitaryDataFrame\n        trainPUnitary_transition = pd.concat([train, pUnitaryDataFrame], axis=1, join=\"inner\")\n        \n        # Concatenate DataFrames | train and pUnitaryDataFrame\n        trainPUnitary = pd.concat([trainPUnitary_transition, pUnitary2DataFrame], axis=1, join=\"inner\")\n        \n        # Grouped by date | trainPUnitary\n        grouped = trainPUnitary.groupby(by=['date','pUnitary','pUnitary2']).count()\n        \n        # Summation lists\n        i = 0\n        \n        dateList = []\n        pUnitaryGroupedList = []\n        pUnitary2GroupedList = []\n        dataFrequencyList = []\n        \n        for i in range(0,len(grouped.index)):\n            \n            pUnitary = grouped.index[i][1]\n            pUnitary2 = grouped.index[i][2]\n            \n            if(i == 0):\n            \n                date = grouped.index[i][0]\n                dateList = dateList + [date]\n                \n                pUnitary = grouped.index[i][1]\n                pUnitaryGroupedList = pUnitaryGroupedList + [pUnitary]\n                \n                pUnitary2 = grouped.index[i][2]\n                pUnitary2GroupedList = pUnitary2GroupedList + [pUnitary2]\n                \n                datum = 1\n                dataFrequencyList = dataFrequencyList + [datum]\n            \n            else:\n                \n                if(grouped.index[i-1][0] == grouped.index[i][0]):\n                \n                    instance = len(pUnitaryGroupedList)-1\n                    pUnitaryGroupedList[instance] = pUnitaryGroupedList[instance] + pUnitary\n                    pUnitary2GroupedList[instance] = pUnitary2GroupedList[instance] + pUnitary2\n                    dataFrequencyList[instance] = dataFrequencyList[instance] + 1\n                    \n                else:\n                    \n                    date = grouped.index[i][0]\n                    dateList = dateList + [date]\n                \n                    pUnitary = grouped.index[i][1]\n                    pUnitaryGroupedList = pUnitaryGroupedList + [pUnitary]\n                    \n                    pUnitary2 = grouped.index[i][1]\n                    pUnitary2GroupedList = pUnitary2GroupedList + [pUnitary2]\n                    \n                    datum = 1\n                    dataFrequencyList = dataFrequencyList + [datum]\n        \n        # Create DataFrame | utilityScoreDataFrame\n        utilityScoreDictionary = {}\n        utilityScoreDictionary['date'] = dateList\n        utilityScoreDictionary['p'] = pUnitaryGroupedList\n        utilityScoreDictionary['p2'] = pUnitary2GroupedList\n        utilityScoreDictionary['frequency'] = dataFrequencyList\n        \n        utilityScoreDataFrame = pd.DataFrame(data=utilityScoreDictionary)\n        \n        # Compute utilityScore\n        ## sum p and sum P2\n        sum_p = sum(utilityScoreDataFrame.p)\n        sum_p2 = sum(utilityScoreDataFrame.p2)\n        ## Number of unique dates\n        uniqueDates = len(utilityScoreDataFrame.frequency)\n        ## t\n        t = (sum_p / sum_p2**0.5) * (250/uniqueDates)**0.5\n        ## utilityScore\n        utilityScore = min(max(t,0),6) * sum_p\n        \n        print(\"=========================================\")\n        print(\"Utility Score | Data\")\n        print(\"=========================================\")\n        print(\"Summation p: \",sum_p)\n        print(\"t: \",t)\n        print(\"u: \",utilityScore)\n        print(\"=========================================\")        \n        \n        return trainPUnitary,utilityScoreDataFrame,utilityScore\n\n\n    def submission(self,train_raw,trainAction,customizationVector):\n        \"\"\"\n        submission delivery\n        \"\"\"\n        import pandas as pd\n        train = trainAction.copy()\n        train['date'] = train_raw['date'].astype('int32')\n        \n        from tqdm import tqdm     \n        import janestreet\n        janestreet.make_env.__called__ = False\n        env = janestreet.make_env() # initialize the environment\n        iter_test = env.iter_test() # an iterator which loops over the test set\n        \n        for (test_df, sample_prediction_df) in tqdm(iter_test):\n            #sample_prediction_df.action = train.action.astype('int32')\n            sample_prediction_df[\"action\"] = train[\"action\"].astype('int32')\n            env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase 2 | Test mode"},{"metadata":{"trusted":true},"cell_type":"code","source":"if systemMode == 'test':\n    \n    # Get customization\n    customizationVector = getCustomization()\n    \n    # Get raw dataset\n    rawDatasetTrain, rawDatasetTest, rawDatasetSubmission, rawFeatureDataset = getRawDataset(customizationVector)\n    \n    \n    \"\"\" Build object 1 | class Linear Model Project \"\"\"\n    \n    # Buil object\n    LinearModelProject1 = LinearModelProject()\n    \n    # Call method insert\n    train = [] # Conditioning in progress\n    test = [] # Conditioning in progress\n    LinearModelProject1.insert(rawDatasetTrain,rawDatasetTest,train,test,customizationVector)\n    \n    # Call method getDatasets\n    train_raw, test_raw, train, test = LinearModelProject1.getDatasets()\n    \n    # Call method datasetConditioning\n    train = LinearModelProject1.datasetConditioning(train_raw)\n    test = LinearModelProject1.datasetConditioning(test_raw)\n    \n    # Call method insert\n    LinearModelProject1.insert(rawDatasetTrain,rawDatasetTest,train,test,customizationVector)\n        \n    # Call method getLinearModel(self,dataset,datasetType,customizationVector)\n    ## train dataset\n    trainPredict = LinearModelProject1.getLinearModel(train,test,customizationVector)\n    \n    \n    \"\"\" Build object 2 | class Feature Analytics Project \"\"\"\n    # Build object\n    FeatureAnalyticsProject1 = featureAnalytics()\n    \n    # Call method insert\n    FeatureAnalyticsProject1.insert(customizationVector,rawFeatureDataset)\n    \n    # Call method getDatasets\n    featureDataset = FeatureAnalyticsProject1.getDatasets()\n    \n    # Call method \n    confidenceProbability = FeatureAnalyticsProject1.confidenceProbabilityProject(featureDataset,customizationVector)\n    \n    \n    \"\"\" Build object 3 | returnsOptimizer \"\"\"\n    # Build object\n    LinearModelProject1_Submission = returnsOptimizer()\n    \n    # Call method insert\n    LinearModelProject1_Submission.insert(trainPredict,customizationVector,confidenceProbability)\n    \n    # Call method actionValuer\n    trainAction = LinearModelProject1_Submission.actionValuer(trainPredict,customizationVector,confidenceProbability)\n    \n    # Call method utilityScore\n    trainPUnitary,utilityScoreDataFrame,utilityScore = LinearModelProject1_Submission.utilityScore(trainAction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Phase 3 | Commissioning mode"},{"metadata":{},"cell_type":"markdown","source":"* System mode | Options: (1) Test (2) Commissioning \n\n      --> Commissioning mode\n    \n\n* Regression method for a linear model | Options: (1) Ordinary Least Squares (Cross-validation 10 folds) (2) Bayesian Ridge (Cross-validation 10 folds) (3) Bayesian Ridge\n\n      --> Bayesian Ridge\n\n* Train dataset | Input instances size (0 - 1,048,575 )\n\n      --> 15219\n\n* Return value policy | (1) Prediction Model correspondance: Assign 1-point action value when an actual return corresponds to a predicted return and it is within the confidence interval (2) Maximax correspondance: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return (3) Maximax correspondance biased by tag-confidence perception: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return. Then, the action value has to be multiplied by the confidence probability. Finally, the result must be rounded to the nearest decision-making boundary, i.e., 0 or 1..\n\n      --> (3)\n\n* Risk exposure to loss | Scenarios | (1) 25% (2) 50% (3) 75% \n\n      --> (1)\n\n* Investing proficiency | Scenarios | (1) 25%-beginner  (2) 50% (3) 75% (4) 99%-expert\n\n      --> (4)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif systemMode == 'commissioning':\n    \n    # Get customization\n    \n    customizationVector = [[3, 'NA'],\n     15219,\n     'NA',\n     ['feature_0',\n      'feature_1',\n      'feature_2',\n      'feature_3',\n      'feature_4',\n      'feature_5',\n      'feature_6',\n      'feature_7',\n      'feature_8',\n      'feature_9',\n      'feature_10',\n      'feature_11',\n      'feature_12',\n      'feature_13',\n      'feature_14',\n      'feature_15',\n      'feature_16',\n      'feature_17',\n      'feature_18',\n      'feature_19',\n      'feature_20',\n      'feature_21',\n      'feature_22',\n      'feature_23',\n      'feature_24',\n      'feature_25',\n      'feature_26',\n      'feature_27',\n      'feature_28',\n      'feature_29',\n      'feature_30',\n      'feature_31',\n      'feature_32',\n      'feature_33',\n      'feature_34',\n      'feature_35',\n      'feature_36',\n      'feature_37',\n      'feature_38',\n      'feature_39',\n      'feature_40',\n      'feature_41',\n      'feature_42',\n      'feature_43',\n      'feature_44',\n      'feature_45',\n      'feature_46',\n      'feature_47',\n      'feature_48',\n      'feature_49',\n      'feature_50',\n      'feature_51',\n      'feature_52',\n      'feature_53',\n      'feature_54',\n      'feature_55',\n      'feature_56',\n      'feature_57',\n      'feature_58',\n      'feature_59',\n      'feature_60',\n      'feature_61',\n      'feature_62',\n      'feature_63',\n      'feature_64',\n      'feature_65',\n      'feature_66',\n      'feature_67',\n      'feature_68',\n      'feature_69',\n      'feature_70',\n      'feature_71',\n      'feature_72',\n      'feature_73',\n      'feature_74',\n      'feature_75',\n      'feature_76',\n      'feature_77',\n      'feature_78',\n      'feature_79',\n      'feature_80',\n      'feature_81',\n      'feature_82',\n      'feature_83',\n      'feature_84',\n      'feature_85',\n      'feature_86',\n      'feature_87',\n      'feature_88',\n      'feature_89',\n      'feature_90',\n      'feature_91',\n      'feature_92',\n      'feature_93',\n      'feature_94',\n      'feature_95',\n      'feature_96',\n      'feature_97',\n      'feature_98',\n      'feature_99',\n      'feature_100',\n      'feature_101',\n      'feature_102',\n      'feature_103',\n      'feature_104',\n      'feature_105',\n      'feature_106',\n      'feature_107',\n      'feature_108',\n      'feature_109',\n      'feature_110',\n      'feature_111',\n      'feature_112',\n      'feature_113',\n      'feature_114',\n      'feature_115',\n      'feature_116',\n      'feature_117',\n      'feature_118',\n      'feature_119',\n      'feature_120',\n      'feature_121',\n      'feature_122',\n      'feature_123',\n      'feature_124',\n      'feature_125',\n      'feature_126',\n      'feature_127',\n      'feature_128',\n      'feature_129'],\n     3,1,4]\n    \n    # Get raw datasets\n    rawDatasetTrain, rawDatasetTest, rawDatasetSubmission, rawFeatureDataset = getRawDataset(customizationVector)\n    \n    \n    \"\"\" Build object 1 | class Linear Model Project \"\"\"\n    \n    # Buil object\n    LinearModelProject1 = LinearModelProject()\n    \n    # Call method insert\n    train = [] # Conditioning in progress\n    test = [] # Conditioning in progress\n    LinearModelProject1.insert(rawDatasetTrain,rawDatasetTest,train,test,customizationVector)\n    \n    # Call method getDatasets\n    train_raw, test_raw, train, test = LinearModelProject1.getDatasets()\n    \n    # Call method datasetConditioning\n    train = LinearModelProject1.datasetConditioning(train_raw)\n    test = LinearModelProject1.datasetConditioning(test_raw)\n    \n    # Call method insert\n    LinearModelProject1.insert(rawDatasetTrain,rawDatasetTest,train,test,customizationVector)\n        \n    # Call method getLinearModel(self,dataset,datasetType,customizationVector)\n    ## train dataset\n    trainPredict = LinearModelProject1.getLinearModel(train,test,customizationVector)\n    \n    \n    \"\"\" Build object 2 | class Feature Analytics Project \"\"\"\n    # Build object\n    FeatureAnalyticsProject1 = featureAnalytics()\n    \n    # Call method insert\n    FeatureAnalyticsProject1.insert(customizationVector,rawFeatureDataset)\n    \n    # Call method getDatasets\n    featureDataset = FeatureAnalyticsProject1.getDatasets()\n    \n    # Call method \n    confidenceProbability = FeatureAnalyticsProject1.confidenceProbabilityProject(featureDataset,customizationVector)\n    \n    \n    \"\"\" Build object 3 | returnsOptimizer \"\"\"\n    # Build object\n    LinearModelProject1_Submission = returnsOptimizer()\n    \n    # Call method insert\n    LinearModelProject1_Submission.insert(trainPredict,customizationVector,confidenceProbability)\n    \n    # Call method actionValuer\n    trainAction = LinearModelProject1_Submission.actionValuer(trainPredict,customizationVector,confidenceProbability)\n    \n    # Call method utilityScore\n    trainPUnitary,utilityScoreDataFrame,utilityScore = LinearModelProject1_Submission.utilityScore(trainAction)    \n    \n    # Call method submission\n    LinearModelProject1_Submission.submission(train_raw,trainAction,customizationVector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bibliography\n\n[0] Design\n\nLarman, G. (2002). Applying UML and Patterns: An introduction to\n    Objected-Oriented Analysis and Design and the Unified Process (2nd ed.).\n    New Jersey, The United States of America: Prentice Hall.\n\nSoegaard, M. (2018). The Basics of User Experience Design: A UX Design Book by\n    the Interaction Design Foundation (1st ed.). Denmark: Interaction Design\n    Foundation.\n\n\n[1] Programming\nGuttag, J. (2013). Introduction to Computation and Programming Using Python.\n    Massachusetts, The United States of America: The MIT Press.\n\nLott, S. (2019). Mastering Object-Oriented Python | Build powerful applications\n    with reusable code using OOP design patterns and Python 3.7 (2nd ed.).\n    Birmingham, United Kingdom: Packt Publishing Ltd.\n\nSmith, Taylor G., et al. pmdarima: ARIMA estimators for Python, 2017-,\n    http://www.alkaline-ml.com/pmdarima [Online; accessed 2020-12-18].\n\n\n[2] Computer Science\n\nRaschka, S., & Mirjalili, V. (2019). Python Machine Learning: Machine Learning\n    and Deep Learning with Python, scikit-learn, and TensorFlow 2 (3rd ed.).\n    Birmingham, United Kingdom: Packt Publishing Ltd.\n\n\n[3] Knowledge field\n\nBaekert, G., & Hodrick, R. (2012). International Financial Management (2nd ed.).\n    New York, The United States of America: Pearson Education, Inc.\n\nYan, Y. (2017). Python for Finance | Apply powerful finance models and\n    quantitative analysis with Python (2nd ed.). Birmingham, United Kingdom:\n        Packt Publishing Ltd.\n\nLagplot. (2016). Available at https://www.statisticshowto.com/lag-plot/\n        \n\n[4] Technical Notes\n\nIssue: NumPy 1.19.4 to NumPy 1.19.3 because of RuntimeError\nNote: https://tinyurl.com/y3dm3h86 (see (1) hessler.evan\n03 Nov a 03: 28; (2) Kevin Sheppard 03 Nov a 08: 04)\n\nIssue: What everyone is looking for: API Workarounds \nNote: https://www.kaggle.com/c/jane-street-market-prediction/discussion/200024"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}