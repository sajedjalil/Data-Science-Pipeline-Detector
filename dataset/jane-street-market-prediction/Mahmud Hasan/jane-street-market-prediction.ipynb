{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" * Light GBM version: https://www.kaggle.com/binhlc/jane-street-light-gbm\n* Scored: 5155  \n* Time: 45 minutes on GPU","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# System Files","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport gc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\ntrain = reduce_mem_usage(train)\nfeatures = [c for c in train.columns if 'feature' in c]\n\nNAN_VALUE = -999","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train = train.fillna(NAN_VALUE)\n# Fix mean error due to float16\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float16').columns}) \ntrain = train.fillna(train.mean())\nf_mean = np.mean(train[features[1:]].values,axis=0)\ntrain = train.query('date > 85').reset_index(drop = True)\ntrain = train[train.weight != 0]\nn_folds = 5\nseed = 2020\nskf = StratifiedKFold(n_splits=n_folds, shuffle=False)\n\n#X = train.loc[:, features].values\n#if np.isnan(X[:, 1:].sum()):\n#    X[:, 1:] = np.nan_to_num(X[:, 1:]) + np.isnan(X[:, 1:]) * NAN_VALUE\n    \n#y = (train['resp'].values > 0).astype(int)\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n#y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T \n\n#train_index, test_index = next(skf.split(X, y[:,0]))\n#train_index, test_index = next(skf.split(X, y))\n#X_train, X_test = X[train_index], X[test_index]\n#y_train, y_test = y[train_index], y[test_index]\n\nX_train = train[train['date'] < 400][features]\nX_test = train[train['date'] >= 400][features]\n\ny_train = np.stack([(train[train['date'] < 400][c] > 0).astype('int') for c in resp_cols]).T\ny_test = np.stack([(train[train['date'] >= 400][c] > 0).astype('int') for c in resp_cols]).T\n\n\nX_train = train[features]\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.argwhere(np.isinf(X_train))\n#np.sum(X_train)\n#np.sum(X_train[512:515,:])\n#X_train[512:515,:]\n#len(y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TUNNING = False\n\ndef create_model(hp,input_dim,output_dim):\n    inputs = tf.keras.layers.Input(input_dim)\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.GaussianNoise(hp.Choice('noise',[0.0,0.03,0.05]))(x)\n    x = tf.keras.layers.Dropout(hp.Choice('init_dropout',[0.0,0.3,0.5]))(x)    \n    x = tf.keras.layers.Dense(hp.Int('num_units_1', 128, 2048, 64), activation=hp.Choice('activation_1', ['tanh','relu','swish']))(x)\n    x = tf.keras.layers.Dropout(hp.Choice(f'dropout_1',[0.0,0.3,0.5]))(x)\n    x = tf.keras.layers.Dense(hp.Int('num_units_2', 128, 1024, 32), activation=hp.Choice('activation_2', ['tanh','relu','swish']))(x)\n    x = tf.keras.layers.Dropout(hp.Choice(f'dropout_2',[0.0,0.3,0.5]))(x)\n    x = tf.keras.layers.Dense(output_dim, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=inputs,outputs=x)\n    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('lr',[1e-2, 1e-3, 1e-5])),loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=hp.Choice('label_smoothing',[0.0, 0.01, 0.1])),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n    return model\n\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape = len(features)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.GaussianNoise(0.05),\n    tf.keras.layers.Dropout(0.3),        \n    tf.keras.layers.Dense(256, activation='tanh'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(64, activation='tanh'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(256, activation='tanh'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),   \n    tf.keras.layers.Dense(5, activation = 'sigmoid')\n  ])\n\nEPOCHS = 500\nBATCH_SIZE = 4096\n\nif TUNNING:\n    import kerastuner as kt\n    EPOCHS = 50\n    MAX_TRIAL = 20\n    model_fn = lambda hp: create_model(hp, X_train.shape[-1], y_train.shape[-1])\n    tuner = kt.tuners.BayesianOptimization(model_fn, kt.Objective('val_auc', direction='max'), MAX_TRIAL, seed = 2020)\n    tuner.search(X_train, y_train, epochs=EPOCHS, validation_data=(X_test, y_test),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10, restore_best_weights=True)])\n    model = tuner.get_best_models()[0]\nelse:\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n    #optimizer = tf.keras.optimizers.RMSprop()\n    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2)\n    model.compile(loss = loss, optimizer=optimizer, metrics=[tf.keras.metrics.AUC()])\n    #history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callback], validation_data=(X_test, y_test)) \n    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)\n#0.63\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TUNNING:\n    tuner.results_summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import janestreet\nfrom tqdm.notebook import tqdm\n#janestreet.competition.make_env.__called__ = False\nenv = janestreet.make_env()\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n            #x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * NAN_VALUE\n        action = np.mean(model(x_tt, training = False).numpy()[0])\n        #action = np.median(model(x_tt, training = False).numpy()[0])\n        if (action > 0.5):\n            sample_prediction_df.action = 1\n        else:\n            sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = 0 \n    env.predict(sample_prediction_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}