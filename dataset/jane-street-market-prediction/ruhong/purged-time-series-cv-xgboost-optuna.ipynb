{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nimport datatable as dt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=pd.core.common.SettingWithCopyWarning)\n    \nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = [14, 8]  # width, height","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Time Series Cross Validation\n\n> \"There are many different ways one can do cross-validation, and **it is the most critical step when building a good machine learning model** which is generalizable when it comes to unseen data.\"\n-- **Approaching (Almost) Any Machine Learning Problem**, by Abhishek Thakur\n\nCV is the **first** step, but very few notebooks are talking about this. Here we look at \"purged rolling time series CV\" and actually apply it in hyperparameter tuning for a basic estimator. This notebook owes a debt of gratitude to the notebook [\"Found the Holy Grail GroupTimeSeriesSplit\"](https://www.kaggle.com/jorijnsmit/found-the-holy-grail-grouptimeseriessplit). That notebook is excellent and this solution is an extention of the quoted pending sklearn estimator. I modify that estimator to make it more suitable for the task at hand in this competition. The changes are\n\n- you can specify a **gap** between each train and validation split. This is important because even though the **group** aspect keeps whole days together, we suspect that the anonymized features have some kind of lag or window calculations in them (which would be standard for financial features). By introducing a gap, we mitigate the risk that we leak information from train into validation\n- we can specify the size of the train and validation splits in terms of **number of days**. The ability to specify a validation set size is new and the the ability to specify days, as opposed to samples, is new.\n\nThe code for `PurgedTimeSeriesSplit` is below. I've hiden it becaused it is really meant to act as an imported class. If you want to see the code and copy for your work, click on the \"Code\" box."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# TODO: make GitHub GIST\n# TODO: add as dataset\n# TODO: add logging with verbose\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups // n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To show the general idea, we generate some simple grouped data. Imagine we have a dataset of 2,000 samples which below to 20 groups."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 2000\nn_groups = 20\nassert n_samples % n_groups == 0\n\nidx = np.linspace(0, n_samples-1, num=n_samples)\nX_train = np.random.random(size=(n_samples, 5))\ny_train = np.random.choice([0, 1], n_samples)\ngroups = np.repeat(np.linspace(0, n_groups-1, num=n_groups), n_samples/n_groups)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nimport numpy as np\nimport matplotlib.pyplot as plt\n    \n# this is code slightly modified from the sklearn docs here:\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    cmap_cv = plt.cm.coolwarm\n\n    jet = plt.cm.get_cmap('jet', 256)\n    seq = np.linspace(0, 1, 256)\n    _ = np.random.shuffle(seq)   # inplace\n    cmap_data = ListedColormap(jet(seq))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n    # Plot the data classes and groups at the end\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n               c=y, marker='_', lw=lw, cmap=plt.cm.Set3)\n\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n               c=group, marker='_', lw=lw, cmap=cmap_data)\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + ['target', 'day']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[n_splits+2.2, -.2], xlim=[0, len(y)])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's again imagine we want to do\n- a rolling time series split\n- where we have a gap of 2 days between train and validation sets\n- and we make the maximum size of each train set to be 7 days\n\nHere we specify the number of splits, the maximum number of groups in each train set, and the maximum number of groups in each valdiation set (sklearn has this convention where they call it the \"test\" set; I preserve that in the variable names, but prefer to call it the validation set)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\ncv = PurgedGroupTimeSeriesSplit(\n    n_splits=5,\n    max_train_group_size=7,\n    group_gap=2,\n    max_test_group_size=3\n)\n\nplot_cv_indices(cv, X_train, y_train, groups, ax, 5, lw=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With the Real Competition Data\n\nIn the real competition data, the number of datapoints per day (that is per \"group\") is not constant as it was in the spoofed data. We need to confirm that the time series split respects that there are different counts of samples in the the days.\n\nWe load the data and reduce memory footprint."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = (\n    dt.fread('../input/jane-street-market-prediction/train.csv')\n      .to_pandas()\n      .query('weight > 0')\n      .pipe(reduce_mem_usage)\n)\n\nfeature_names = train_data.columns[train_data.columns.str.contains('feature')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's imagine that you want to fit on 15 days of data, leave a gap of 5 days between test and validation, and then validate on 5 days of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\ncv = PurgedGroupTimeSeriesSplit(\n    n_splits=5,\n    max_train_group_size=15,\n    group_gap=5,\n    max_test_group_size=5\n)\n\nplot_cv_indices(\n    cv,\n    train_data.query('date < 50')[\n        train_data.columns[train_data.columns.str.contains('feature')]\n    ].values,\n    (train_data.query('date < 50')['resp'] > 0).astype(int).values,\n    train_data.query('date < 50')['date'].values,\n    ax,\n    5,\n    lw=20\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Alt Text](https://media.giphy.com/media/5bGYUuT3VEVLa/giphy.gif)\n\nBoom... there you go. Notice that the sizes of the train, gap, and valid sets respect the different number of samples per day. Now you can split the data on **number of days** in the validation set, **number of days** gap between, and **number of days** in the train set. You can see in the \"group\" bars that the days are different lengths."},{"metadata":{},"cell_type":"markdown","source":"# A Worked Example of Hyperparameter Tuning with CV"},{"metadata":{},"cell_type":"markdown","source":"Now that we can split the data in a very sensible way, we can proceed to build a model. This section builds a very simple (maybe the most simple) pipeline:\n- we first mean impute missing values. This dataset has many NaN\n- we then scale the data as we are going to use a linear model which requires this\n- and lastly, we fit a LogisticRegression\n\nThe Logistic Regression has only one hyperparameter: the \"C\" value which specifies the inverse of the regularization strength. We use the sklearn `Pipeline` and `GridSearchCV` with our `PurgedGroupTimeSeriesSplit` class. Note one benefit of the pipeline approach is that when we impute the means, we are **not** looking ahead into the validation set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression, Lasso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import set_config\nset_config(display='diagram') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set up the Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: feature_0 should not be scaled\n\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nscaler = StandardScaler()\n\nlogistic = LogisticRegression(\n    max_iter=1000,\n    tol=0.1,\n    verbose=10,\n    penalty='l1',\n    solver='liblinear',\n    random_state=42\n)\n\npipe = Pipeline(steps=[\n    ('imputer', imp_mean),\n    ('scaler', scaler),\n    ('logistic', logistic)\n])\n\npipe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This â˜ is a nice HTML display of the pipeline. You can click on the objects for more info. In this case, it's a simple pipeline, but this is a really nice feature to explore complex pipelines."},{"metadata":{"trusted":true},"cell_type":"code","source":"# pipe.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run Cross Validation\n\nWe will sequentially try a range of regularization hyperparameters and look at the CV for each. The training data is 500 days; I make 4 splits looking at 125 days in train, skipping 40 days (two months), and validating on the next 40 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n 'logistic__C': np.logspace(-3, 1.5, 7),  # lower C is more regularization\n}\n\nscoring = {'AUC': 'roc_auc'}\n\ncv = PurgedGroupTimeSeriesSplit(\n    n_splits=3,\n    max_train_group_size=150,\n    group_gap=20,\n    max_test_group_size=60\n)\n\nsearch = GridSearchCV(\n    pipe,\n    param_grid,\n    n_jobs=3,\n    cv=cv,\n    verbose=10,\n    scoring=scoring,\n    refit=False, # 'AUC',   # <-- do we want to refit on the entire dataset?\n    return_train_score=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nFIT = True\n\nif FIT:\n    search.fit(\n        train_data[\n            train_data.columns[train_data.columns.str.contains('feature')]\n        ].values,\n        (train_data['resp'] > 0).astype(int).values,\n        groups=train_data['date'].values,\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize the CV Results"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results = search.cv_results_\n\nparam = 'param_' + list(param_grid.keys())[0]\n\nplt.figure(figsize=(12, 6))\nplt.title(\"GridSearchCV\", fontsize=16)\nplt.xlabel(list(param_grid.keys())[0])\nplt.ylabel(\"Score\")\n\nax = plt.gca()\nax.set_xlim(0.00075, 40)\nax.set_ylim(0.45, 0.55)\nax.set_xscale('log')\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results[param].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), ['g', 'k']):\n    for sample, style in (('train', '--'), ('test', '-')):\n        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n        sample_score_std = results['std_%s_%s' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == 'test' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == 'test' else 0.7,\n                label=\"%s (%s)\" % (scorer, sample))\n\n    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n    best_score = results['mean_test_%s' % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(\"%0.4f\" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=\"best\")\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the model is not that sensitive to the regularization parameter. I will err on the side of caution and choose a lower one."},{"metadata":{"trusted":true},"cell_type":"code","source":"results_idx = np.argmax(results['mean_test_AUC'])\nbest_param = results[param][results_idx]\n\nprint(f'The best setting for C is {best_param}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit the Best Estimator on the Entire Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = LogisticRegression(\n    max_iter=1000,\n    C=0.1,\n    tol=0.1,\n    verbose=10,\n    penalty='l1',   # <-- L1 norm to enforce sparsity of coefficients\n    solver='liblinear'\n)\n\npipe_lr = Pipeline(steps=[\n    ('imputer', imp_mean),\n    ('scaler', scaler),\n    ('logistic', logistic)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npipe_lr.fit(\n    train_data.query('date > 100')[\n        train_data.columns[train_data.columns.str.contains('feature')]\n    ].values,\n    (train_data.query('date > 100')['resp'] > 0).astype(int).values\n)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optuna Hyperparam Search for XGBoost\n\nUnlike Logistic Regression, XGBoost has many parameters. We will use the same CV scheme (`PurgedGroupTimeSeriesSplit`) so that the results are comparable. To search for the best parameters, however, we will use a Baysian optimzer, [**Optuna**](https://github.com/optuna/optuna). "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport optuna\n\ny_labels = (train_data['resp'] > 0).astype(int).values\nX_train = train_data[\n    train_data.columns[train_data.columns.str.contains('feature')]\n].values\ngroups = train_data['date'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting up the Optuna objective function is very straightforward. The new code below is `trial.suggest_...`. This is Optuna code to generate a parameter setting."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = PurgedGroupTimeSeriesSplit(\n    n_splits=3,\n    max_train_group_size=150,\n    group_gap=20,\n    max_test_group_size=60\n)\n\ndef objective(trial, cv=cv, cv_fold_func=np.average):\n\n    # Optuna suggest params\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 350, 1000),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.10),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 0.90),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 0.90),\n        'gamma': trial.suggest_int('gamma', 0, 20),\n        'missing': -999,\n        'tree_method': 'gpu_hist'  \n    }\n    \n    # setup the pieline\n    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n    scaler = StandardScaler()\n    clf = xgb.XGBClassifier(**params)\n\n    pipe = Pipeline(steps=[\n        ('imputer', imp_mean),\n        ('scaler', scaler),\n        ('xgb', clf)\n    ])\n\n\n    # fit for all folds and return composite AUC score\n    aucs = []\n    for i, (train_idx, valid_idx) in enumerate(cv.split(\n        X_train,\n        y_labels,\n        groups=groups)):\n        \n        train_data = X_train[train_idx, :], y_labels[train_idx]\n        valid_data = X_train[valid_idx, :], y_labels[valid_idx]\n        \n        _ = pipe.fit(X_train[train_idx, :], y_labels[train_idx])\n        preds = pipe.predict(X_train[valid_idx, :])\n        auc = roc_auc_score(y_labels[valid_idx], preds)\n        aucs.append(auc)\n    \n    print(f'Trial done: AUC values on folds: {aucs}')\n    return cv_fold_func(aucs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.seterr(over='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nFIT_XGB = True\n\nn_trials = 60\n\nif FIT_XGB:\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=n_trials)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = trial.params\n\nbest_params['missing'] = -999\nbest_params['tree_method'] = 'gpu_hist' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit the XGBoost Classifier with Optimal Hyperparams"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(**best_params)\n\npipe_xgb = Pipeline(steps=[\n    ('imputer', imp_mean),\n    ('scaler', scaler),\n    ('xgb', clf)\n])\n\npipe_xgb.fit(\n    train_data.query('date > 100')[\n        train_data.columns[train_data.columns.str.contains('feature')]\n    ].values,\n    (train_data.query('date > 100')['resp'] > 0).astype(int).values\n)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict on the Test Set\n\nPrediction is a simple loop calling the `predict` method on `pipe`. Note that the mean imputation and scaling is done automatically."},{"metadata":{"trusted":true},"cell_type":"code","source":"# which pipeline are we using?\n\npipe_prod = pipe_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env()\nenv_iter = env.iter_test()\n\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        pred_df.action = pipe_prod.predict(test_df.loc[:, feature_names].values)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}