{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Jane Street Market Prediction: A simple EDA and prediction**\n"},{"metadata":{},"cell_type":"markdown","source":"**\"Machine learning (ML) at Jane Street begins, unsurprisingly, with data. We collect and store around 2.3TB of market data every day. Hidden in those petabytes of data are the relationships and statistical regularities which inform the models inside our strategies. But it’s not just awesome models. ML work in a production environment like Jane Street’s involves many interconnected pieces.\" -- Jane Street Tech Blog \"Real world machine learning\".**"},{"metadata":{},"cell_type":"markdown","source":"# **1.Import**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# numpy\nimport numpy as np\n\n# pandas stuff\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# plotting stuff\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\n#plt.rcParams.update({'font.size': 12})\n\n\n# install dabl\n!pip install dabl > /dev/null\nimport dabl\n# install datatable\n!pip install datatable > /dev/null\nimport datatable as dt\n\n# misc\nimport missingno as msno\n\n# system\nimport warnings\nwarnings.filterwarnings('ignore')\n# for the image import\nimport os\nfrom IPython.display import Image\n# garbage collector to keep RAM in check\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2.The train.csv file is big**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wc -l ../input/jane-street-market-prediction/train.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see that it has a total of 2,390,492 rows. I recommend reading this magnificent Tutorial on reading large datasets by Vopani.\n\nI have used pandas to load in the train.csv and it took almost 2 minutes. To speed things up here I shall use datatable:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_datatable = dt.fread('../input/jane-street-market-prediction/train.csv')\ntrain_data = train_data_datatable.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3.Deal with feather---resp**"},{"metadata":{},"cell_type":"markdown","source":"**There are a total of 500 days of data in train.csv (i.e. two years of trading data). Let us take a look at the cumulative values of resp over time**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"Cumulative resp\", fontsize=18);\nbalance.plot(lw=3);\ndel balance\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**as well as four time horizons**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nresp_1= pd.Series(train_data['resp_1']).cumsum()\nresp_2= pd.Series(train_data['resp_2']).cumsum()\nresp_3= pd.Series(train_data['resp_3']).cumsum()\nresp_4= pd.Series(train_data['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us now plot a histogram of all of the resp values (here only shown for values between -0.05 and 0.05)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['resp'], \n             bins=3000, \n             kde_kws={\"clip\":(-0.05,0.05)}, \n             hist_kws={\"range\":(-0.05,0.05)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.show();\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This distribution has very long tails**"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_resp = train_data['resp'].min()\nprint('The minimum value for resp is: %.5f' % min_resp)\nmax_resp = train_data['resp'].max()\nprint('The maximum value for resp is:  %.5f' % max_resp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us also calculate the skew and kurtosis of this distribution:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skew of resp is:      %.2f\" %train_data['resp'].skew() )\nprint(\"Kurtosis of resp is: %.2f\"  %train_data['resp'].kurtosis() )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finally, let us fit a Cauchy distribution to this data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import curve_fit\n# the values\nx = list(range(len(values)))\nx = [((i)-1500)/30000 for i in x]\ny = values\n\ndef Lorentzian(x, x0, gamma, A):\n    return A * gamma**2/(gamma**2+( x - x0 )**2)\n\n# seed guess\ninitial_guess=(0, 0.001, 3000)\n\n# the fit\nparameters,covariance=curve_fit(Lorentzian,x,y,initial_guess)\nsigma=np.sqrt(np.diag(covariance))\n\n# and plot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data['resp'], \n             bins=3000, \n             kde_kws={\"clip\":(-0.05,0.05)}, \n             hist_kws={\"range\":(-0.05,0.05)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\n#norm = plt.Normalize(values.min(), values.max())\n#colors = plt.cm.jet(norm(values))\n#for rec, col in zip(ax.patches, colors):\n#    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.plot(x,Lorentzian(x,*parameters),'--',color='black',lw=3)\nplt.show();\ndel values\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **4.Weight**"},{"metadata":{},"cell_type":"markdown","source":"Each trade has an associated weight and resp, which together represents a return on the trade. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation."},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_zeros = (100/train_data.shape[0])*((train_data.weight.values == 0).sum())\nprint('Percentage of zero weights is: %i' % percent_zeros +\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**An now to find the maximum weight used**"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_weight = train_data['weight'].max()\nprint('The maximum weight was: %.2f' % max_weight)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**which occured on day 446**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['weight']==train_data['weight'].max()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us take a look at a histogram of the non-zero weights**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['weight'], \n             bins=1400, \n             kde_kws={\"clip\":(0.001,1.4)}, \n             hist_kws={\"range\":(0.001,1.4)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of non-zero weights\", size=14)\nplt.show();\ndel values\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **5.Cumulative return**"},{"metadata":{},"cell_type":"markdown","source":"Let us take a look at the cumulative daily return over time, which is given by weight multiplied by the value of resp"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['weight_resp']   = train_data['weight']*train_data['resp']\ntrain_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\ntrain_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\ntrain_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\ntrain_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return for resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nplt.legend(loc=\"lower left\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the shortest time horizons, resp_1, resp_2 and resp_3, representing a more conservative strategy, result in the lowest return.\n\nWe shall now plot a histogram of the weight multiplied by the value of resp (after removing the 0 weights)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_no_0 = train_data.query('weight > 0').reset_index(drop = True)\ntrain_data_no_0['wAbsResp'] = train_data_no_0['weight'] * (train_data_no_0['resp'])\n#plot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data_no_0['wAbsResp'], \n             bins=1500, \n             kde_kws={\"clip\":(-0.02,0.02)}, \n             hist_kws={\"range\":(-0.02,0.02)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the weights * resp\", size=14)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **6.Time**"},{"metadata":{},"cell_type":"markdown","source":"Let us plot the number of ts_id per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder did Jane Street modify their trading model around day 85? Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or vice versa)."},{"metadata":{"trusted":true},"cell_type":"code","source":"trades_per_day = train_data.groupby(['date'])['ts_id'].count()\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(trades_per_day)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of ts_id for each day\", fontsize=18)\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we assume a trading day is 6½ hours long (i.e. 23400 seconds) then"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(23400/trades_per_day)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_ylabel (\"Av. time between trades (s)\", fontsize=18)\nax.set_title (\"Average time between trades for each day\", fontsize=18)\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nax.set_ylim(ymin=0)\nax.set_ylim(ymax=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is a histogram of the number of trades per day (it has been suggested that the number of trades per day is an indication of the volatility that day)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,4))\n# the minimum has been set to 1000 so as not to draw the partial days like day 2 and day 294\n# the maximum number of trades per day is 18884\n# I have used 125 bins for the 500 days\nax = sns.distplot(trades_per_day, \n             bins=125, \n             kde_kws={\"clip\":(1000,20000)}, \n             hist_kws={\"range\":(1000,20000)},\n             color='darkcyan', \n             kde=True);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Number of trades per day\", size=14)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **7.The features**"},{"metadata":{},"cell_type":"markdown","source":"\"This dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['feature_0'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, feature_0 is the only feature in the features.csv file that has no True tags."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 4))\nfeature_0 = pd.Series(train_data['feature_0']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_0 (cumulative)\", fontsize=18);\nfeature_0.plot(lw=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also very interesting to plot the cumulative resp and return (resp*weight) for feature_0 = +1 and feature_0 = -1 individually (Credit: \"An observation about feature_0\" by therocket290)"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_0_is_plus_one  = train_data.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = train_data.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']).cumsum()), lw=3, label='resp')\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']).cumsum()), lw=3, label='resp')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return')\nax1.set_title (\"feature 0 = 1\", fontsize=18)\nax2.set_title (\"feature 0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"upper left\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**feature_{1...129}**"},{"metadata":{},"cell_type":"markdown","source":"\nThere seem to be four general 'types' of features, here is a plot of an example of one of each:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(20,10))\n\nax1.plot((pd.Series(train_data['feature_1']).cumsum()), lw=3, color='red')\nax1.set_title (\"Linear\", fontsize=22);\nax1.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax1.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax1.set_xlim(xmin=0)\nax1.set_ylabel (\"feature_1\", fontsize=18);\n\nax2.plot((pd.Series(train_data['feature_3']).cumsum()), lw=3, color='green')\nax2.set_title (\"Noisy\", fontsize=22);\nax2.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\nax2.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax2.set_xlim(xmin=0)\nax2.set_ylabel (\"feature_3\", fontsize=18);\n\nax3.plot((pd.Series(train_data['feature_55']).cumsum()), lw=3, color='darkorange')\nax3.set_title (\"Hybryd (Tag 21)\", fontsize=22);\nax3.set_xlabel (\"Trade\", fontsize=18)\nax3.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax3.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax3.set_xlim(xmin=0)\nax3.set_ylabel (\"feature_55\", fontsize=18);\n\nax4.plot((pd.Series(train_data['feature_73']).cumsum()), lw=3, color='blue')\nax4.set_title (\"Negative\", fontsize=22)\nax4.set_xlabel (\"Trade\", fontsize=18)\nax4.set_ylabel (\"feature_73\", fontsize=18);\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Tag 14 set are interesting as they appear to be \"stratified\"; only adopting discrete values throughout the day (could these be a value of a security?). Here are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the missing data section below):"},{"metadata":{"trusted":true},"cell_type":"code","source":"day_0 = train_data.loc[train_data['date'] == 0]\nday_1 = train_data.loc[train_data['date'] == 1]\nday_3 = train_data.loc[train_data['date'] == 3]\nthree_days = pd.concat([day_0, day_1, day_3])\nthree_days.plot.scatter(x='ts_id', y='feature_41', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_42', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_43', s=0.5, figsize=(15,3));\ndel day_1\ndel day_3\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These three features also have very interesting lag plots, where we plot the value of the feature at ts_id  (n)  with respect to the next value of the feature, i.e. at ts_id  (n+1) , (here for day 0). Red markers have been placed at (0,0) as a visual aid."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(17, 4))\nlag_plot(day_0['feature_41'], lag=1, s=0.5, ax=ax[0])\nlag_plot(day_0['feature_42'], lag=1, s=0.5, ax=ax[1])\nlag_plot(day_0['feature_43'], lag=1, s=0.5, ax=ax[2])\nax[0].title.set_text('feature_41')\nax[0].set_xlabel(\"ts_id (n)\")\nax[0].set_ylabel(\"ts_id (n+1)\")\nax[1].title.set_text('feature_42')\nax[1].set_xlabel(\"ts_id (n)\")\nax[1].set_ylabel(\"ts_id (n+1)\")\nax[2].title.set_text('feature_43')\nax[2].set_xlabel(\"ts_id (n)\")\nax[2].set_ylabel(\"ts_id (n+1)\")\n\nax[0].plot(0, 0, 'r.', markersize=15.0)\nax[1].plot(0, 0, 'r.', markersize=15.0)\nax[2].plot(0, 0, 'r.', markersize=15.0);\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **8.Data prediction and processing results**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Pytorch Resnet part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import namedtuple\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nfrom random import choices\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings (\"ignore\")\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nDATA_PATH = '../input/jane-street-market-prediction/'\n\nNFOLDS = 5\n\nTRAIN = False\nCACHE_PATH = '../input/mlp012003weights'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\nf_mean = np.load(f'{CACHE_PATH}/f_mean_online.npy')\n\n##### Making features\nall_feat_cols = [col for col in feat_cols]\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n##### Model&Data fnc\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.2)\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x\n\nif True:\n    device = torch.device(\"cpu\")\n    \n    model_list = []\n    tmp = np.zeros(len(feat_cols))\n    for _fold in range(NFOLDS):\n        torch.cuda.empty_cache()\n        model = Model()\n        model.to(device)\n        model_weights = f\"{CACHE_PATH}/online_model{_fold}.pth\"\n        model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n        model.eval()\n        model_list.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tensorflow part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/jane-street-with-keras-nn-overfit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nSEED = 1111\n\nnp.random.seed(SEED)\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates)(x)\n    for i in range(3):\n        x = tf.keras.layers.Dense(hidden_units)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates)(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 200\nbatch_size = 4096\nhidden_units = 160\ndropout_rates = 0.2\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(\n    len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\nclf.load_weights('../input/jane-street-with-keras-nn-overfit/model.h5')\n\ntf_models = [clf]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **EMbeddings NN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FEAT_TAGS = 29\nDEVICE = device\nN_FEATURES = 130\nTHREE_HIDDEN_LAYERS = [400, 400, 400]\n\nclass FFN (nn.Module):\n    \n    def __init__(self, inputCount=130, outputCount=5, hiddenLayerCounts=[150, 150, 150], \n                 drop_prob=0.2, nonlin=nn.SiLU (), isOpAct=False):\n        \n        super(FFN, self).__init__()\n        \n        self.nonlin     = nonlin\n        self.dropout    = nn.Dropout (drop_prob)\n        self.batchnorm0 = nn.BatchNorm1d (inputCount)\n        self.dense1     = nn.Linear (inputCount, hiddenLayerCounts[0])\n        self.batchnorm1 = nn.BatchNorm1d (hiddenLayerCounts[0])\n        self.dense2     = nn.Linear(hiddenLayerCounts[0], hiddenLayerCounts[1])\n        self.batchnorm2 = nn.BatchNorm1d (hiddenLayerCounts[1])\n        self.dense3     = nn.Linear(hiddenLayerCounts[1], hiddenLayerCounts[2])\n        self.batchnorm3 = nn.BatchNorm1d (hiddenLayerCounts[2])        \n        self.outDense   = None\n        if outputCount > 0:\n            self.outDense   = nn.Linear (hiddenLayerCounts[-1], outputCount)\n        self.outActivtn = None\n        if isOpAct:\n            if outputCount == 1 or outputCount == 2:\n                self.outActivtn = nn.Sigmoid ()\n            elif outputCount > 0:\n                self.outActivtn = nn.Softmax (dim=-1)\n        return\n\n    def forward (self, X):\n        \n        # X = self.dropout (self.batchnorm0 (X))\n        X = self.batchnorm0 (X)\n        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm3 (self.dense3 (X))))\n        if self.outDense:\n            X = self.outDense (X)\n        if self.outActivtn:\n            X = self.outActivtn (X)\n        return X\n    \n    \nclass Emb_NN_Model (nn.Module):\n    \n    def __init__(self, three_hidden_layers=THREE_HIDDEN_LAYERS, embed_dim=(N_FEAT_TAGS),\n                 csv_file='../input/jane-street-market-prediction/features.csv'):\n        \n        super (Emb_NN_Model, self).__init__()\n        global N_FEAT_TAGS\n        N_FEAT_TAGS = 29\n        dtype = {'tag_0' : 'int8'}\n        for i in range (1, 29):\n            k = 'tag_' + str (i)\n            dtype[k] = 'int8'\n        t_df = pd.read_csv (csv_file, usecols=range (1,N_FEAT_TAGS+1), dtype=dtype)\n        t_df['tag_29'] = np.array ([1] + ([0] * (t_df.shape[0]-1)) ).astype ('int8')\n        self.features_tag_matrix = torch.tensor (t_df.to_numpy ())\n        N_FEAT_TAGS += 1\n        \n        self.embed_dim     = embed_dim\n        self.tag_embedding = nn.Embedding (N_FEAT_TAGS+1, embed_dim)\n        self.tag_weights   = nn.Linear (N_FEAT_TAGS, 1)\n        \n        drop_prob          = 0.5\n        self.ffn           = FFN (inputCount=(130+embed_dim), outputCount=0,\n                                  hiddenLayerCounts=[(three_hidden_layers[0]+embed_dim),\n                                                     (three_hidden_layers[1]+embed_dim),\n                                                     (three_hidden_layers[2]+embed_dim)],\n                                  drop_prob=drop_prob)\n        self.outDense      = nn.Linear (three_hidden_layers[2]+embed_dim, 5)\n        return\n    \n    def features2emb (self):\n        \n        all_tag_idxs = torch.LongTensor (np.arange (N_FEAT_TAGS))\n        tag_bools    = self.features_tag_matrix\n        f_emb        = self.tag_embedding (all_tag_idxs).repeat (130, 1, 1)\n        f_emb        = f_emb * tag_bools[:, :, None]\n        s            = torch.sum (tag_bools, dim=1)       \n        f_emb        = torch.sum (f_emb, dim=-2) / s[:, None]\n        return f_emb\n    \n    def forward (self, cat_featrs, features):\n        \n        cat_featrs = None\n        features   = features.view (-1, N_FEATURES)\n        f_emb      = self.features2emb ()\n        features_2 = torch.matmul (features, f_emb)\n        features   = torch.hstack ((features, features_2))\n        x          = self.ffn (features)\n        out_logits = self.outDense (x)\n        return out_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embNN_model = Emb_NN_Model ()\n\ntry:\n    embNN_model.load_state_dict (torch.load (\"../input/jane-embnn5-auc-400-400-400/Jane_EmbNN5_auc_400_400_400.pth\"))\nexcept:\n    embNN_model.load_state_dict (torch.load (\"../input/jane-embnn5-auc-400-400-400/Jane_EmbNN5_auc_400_400_400.pth\", map_location='cpu'))\n    \nembNN_model = embNN_model.eval ()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Inference**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\nenv_iter = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if True:\n\n    for (test_df, pred_df) in tqdm(env_iter):\n        if test_df['weight'].item() > 0:\n            x_tt = test_df.loc[:, feat_cols].values\n            if np.isnan(x_tt.sum()):\n                x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean\n\n            cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n            cross_1_2 = x_tt[:, 1] / (x_tt[:, 2] + 1e-5)\n            feature_inp = np.concatenate((\n                x_tt,\n                np.array(cross_41_42_43).reshape(x_tt.shape[0], 1),\n                np.array(cross_1_2).reshape(x_tt.shape[0], 1),\n            ), axis=1)\n\n            # torch_pred\n            torch_pred = np.zeros((1, len(target_cols)))\n            for model in model_list:\n                torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() / NFOLDS\n            torch_pred = np.median(torch_pred)\n            \n            # tf_pred\n            tf_pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in tf_models],axis=0))\n            \n            # torch embedding_NN pred\n            x_tt    = torch.tensor (x_tt).float ().view (-1, 130)\n            embnn_p = np.median (torch.sigmoid (embNN_model (None, x_tt)).detach ().cpu ().numpy ().reshape ((-1, 5)), axis=1)\n            \n            # avg\n            pred_pr = torch_pred*0.4 + tf_pred*0.4 + embnn_p*0.2\n            \n            pred_df.action = np.where (pred_pr >= 0.5, 1, 0).astype (int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}