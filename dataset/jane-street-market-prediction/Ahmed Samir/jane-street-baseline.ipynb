{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Notes about the data: \n> 1. The training data has over 2 million rows, which the largest I have ever dealt with so far.\n> 2. <strike>The difference between train and test data is resp columns, which I don't understand what they signify so far.</strike> \n> 3. resp represent market returns over different time horizons\n> 4. weight and resp represent the return on the trade, but resp isn't included in the test data, so one thought is to predict resp in the test data based on features and then predict the action.\n> 5. <strike>action isn't included in the training set, why?</strike>\n> 6. Point no. 4 was on point, but I didn't comprehend that I would use the return to determine the action directly. \n> 7. What should be done here is to simply set the action to 1 if the return is positive, or 0 if the return is negative. \n> 8. Since the scoring function is a utility function based on the presence of real test <strike>resps</strike> resp during evaluation but absence in prediction, the action we determine would increase the positive score of the sum of the utility function or decrease it by adding a trade that has an overall negative resp.\n> 9. Also the weight of the trade would increase the effect which a single trade have on the scoring function.\n> 10. Should the sign of <strike>resps sum</strike> resp be enough for setting an action to 1 or 0, or should there be a threshold? If there shall be a threshold, how would that be determined?\n> 11. resp isn't the sum of resp_{1, 2, 3, 4}\n> 12. Weights are heavily right skewed, which could mean that there are two strategies here, focusing on the high weighted trades, and focusing on the low wighted trades. Maybe for each range there should be a different model.\n> 13. Different weight ranges should be treated differently by a model. Or maybe different models would be suited for different weight ranges, as resp distribution differs according to weight. Also maybe models outputs should be clipped at their specific weight range of resps?\n> 14. resp_4 is most similar to resp in distribution, and also highly correlated with resp.\n> 15. Trade counts evidently differ between days. Would this have any implication during training?\n> 16. Mean resp_{1, 2, 3, 4} differ over days and could be using for feature engineering by mean \nencodings and more.\n> 17. Several features blocks show high correlation with other blocks or with each other.\n> 18. Features ranges differ greatly and need standardization before training.\n> 19. Most popular cv strategy in kernels is GroupTimeSeriesSplit.\n> 20. Should we use rows with weight 0? For example, we could predict resps instead of action, then calculate the action based on (weight * resp) > 0. I guess this would boost our model's ability by giving it more data?\n> 21. What if we predicted resp_{1, 2, 3, 4} and then decided a majority vote on action based on all?\n> 22. If we chose action to be resp > 0, should we evaluate model performance based on precision and recall, or ROC AUC? That might depend on the distribution of action."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport cudf\nimport janestreet\nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PLOT = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\ndir_path = '/kaggle/input/jane-street-market-prediction/'\n\ntrain = cudf.read_csv(dir_path+'train.csv')\n# feats = pd.read_csv(dir_path+'features.csv')\n# test = pd.read_csv(dir_path+'example_test.csv')\n# submission = pd.read_csv(dir_path+'example_sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"## What are the dimensions of the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimensions\\n\")\nprint(\"TRAIN:\", train.shape)\n# print(\"FEATS:\", feats.shape)\n# print(\"TEST:\", test.shape)\n# print(\"SUB:\", submission.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which features aren't present in test data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cols difference between train and test\nif PLOT:\n    set(train.columns.values.tolist()).difference(test.columns.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How does the training data look like?"},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT:\n    train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How many unique days do we have in the training data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many days do we have?\nif PLOT:\n    train.date.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have 500 unique days of data in the training set.**"},{"metadata":{},"cell_type":"markdown","source":"## Is resp that sum of resp_{1, 2, 3, 4}?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is resp the sum resp_{1, 2, 3, 4}?\nif PLOT:\n    train[[f'resp_{i}' for i in range(1, 5)]].sum(axis=1) == train['resp']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**resp isn't the sum of resp_{1, 2, 3, 4}**"},{"metadata":{},"cell_type":"markdown","source":"## How are weights distributed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is the distribution of weights\nif PLOT:\n    n = 2\n    bins = np.arange(0, 167+n, n)\n    train.weight.hist(bins=bins);\n    plt.xlim(0, 50);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Weights are heavily right skewed, which could mean that there are two strategies here, focusing on the high weighted trades, and focusing on the low wighted trades. Maybe for each range there should be a different model.**"},{"metadata":{},"cell_type":"markdown","source":"## Are weights and resps in any way related?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are weights and resps in any way related?\nif PLOT:\n    train.plot(kind='scatter', x='weight', y='resp', figsize=(8, 8));\n    plt.xticks(np.arange(0, 170, 10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since the concentration of trades decreases with increasing weights, we can also see a decrease in the spread of the resp with increasing weight, which maybe indicates that different weight ranges should be treated differently by a model. Or maybe that different models would be suited for different ranges. Also that models outputs should be clipped at their specific weight range of resps?**"},{"metadata":{},"cell_type":"markdown","source":"## How are resp and resp_{1, 2, 3, 4} distributed and are they related?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we could answer this question using a scatter matrix\n\nif PLOT:\n    resp_cols = train.columns[train.columns.str.contains('resp')]\n\n    sns.pairplot(train[resp_cols], plot_kws={'alpha': 0.1}, \n                                   diag_kws={'bins': 50});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distributions\n1. resp_4 is most similar to resp in distribution.\n\n\n### Correlations\n1. resp_1 is most correlated with resp_2, and it's correlations with resp_3, resp_4 and resp decrease gradually.\n2. reps_2 is most correlated with resp_1 and resp_3.\n3. resp_3 shows moderate correlations with resp_1, resp_4 and resp.\n4. resp_4 is highly correlated with resp."},{"metadata":{},"cell_type":"markdown","source":"## How many trades do we have per day?"},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT:\n    train.date.value_counts().sort_index().plot(figsize=(12, 6), title='Trades over days',\n                                                xlabel='Day',\n                                                ylabel='count');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trade counts evidently differ between days. Would this have any implication during training?**"},{"metadata":{},"cell_type":"markdown","source":"## How do mean returns differ each day?"},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT:\n    fig, axes = plt.subplots(3, 2, figsize=(14, 10))\n    group_date = train.groupby('date')\n\n    plt.subplots_adjust(hspace=0.5)\n    group_date.resp.mean().plot(title='Mean resp over Days',\n                                           xlabel='Day',\n                                           ylabel='Mean resp', ax=axes[0][0]);\n\n    group_date.resp_1.mean().plot(title='Mean resp_1 over Days',\n                                           xlabel='Day',\n                                           ylabel='Mean resp_1', ax=axes[0][1]);\n\n    group_date.resp_2.mean().plot(title='Mean resp_2 over Days',\n                                           xlabel='Day',\n                                           ylabel='Mean resp_2', ax=axes[1][0]);\n\n    group_date.resp_3.mean().plot(title='Mean resp_3 over Days',\n                                           xlabel='Day',\n                                           ylabel='Mean resp_3', ax=axes[1][1]);\n\n    group_date.resp_4.mean().plot(title='Mean resp_4 over Days',\n                                           xlabel='Day',\n                                           ylabel='Mean resp_4', ax=axes[2][0]);\n\n    group_date['resp_1', 'resp_2', 'resp_3', 'resp_4'].mean().sum(axis=1).plot(title='Mean resp_{1, 2, 3, 4} sum over Days',\n                                           xlabel='Day',\n                                           ylabel='Mean resp_{1, 2, 3, 4} sum', ax=axes[2][1]);\n    \n    del group_date\n    \n    rubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I guess due to their difference, we could predict resp_{1, 2, 3, 4} and use them for mean encodings, or predict their means over days without predicting them. I guess there are alot of uses for predicting them and using them as features, and engineering more features using them.**"},{"metadata":{},"cell_type":"markdown","source":"## How are features related?"},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT:\n    heatmap_cols = train.columns[1: -1]\n    heatmap_corr = train[heatmap_cols].corr()\n\n    plt.figure(figsize=(14, 14))\n    sns.heatmap(heatmap_corr, cmap='coolwarm', alpha=0.75)\n    plt.title('Correlation between features', fontsize=15, weight='bold')\n    \n    del heatmap_corr\n    rubbish = gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some feature blocks show high correlation, which could indicate that their source might be the same? Unfortunately I have no idea what these anonymized features could be, but it wouldn't hurt to look into their distributions.**"},{"metadata":{},"cell_type":"markdown","source":"## How are features distributed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT:\n    plt.figure(figsize=(16, 16))\n\n    sns.set_style('whitegrid')\n\n    rand_feats = np.random.choice(130, 16)\n    for i, col in enumerate(rand_feats):\n        plt.subplot(4, 4, i+1)\n        plt.hist(train.loc[:, f'feature_{col}'], bins=50)\n        plt.title(f'feature_{col}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The ranges of the features are extremely different, which indicates that some type of standardization should be carried out before training any model.**"},{"metadata":{},"cell_type":"markdown","source":"## Understanding the utility function"},{"metadata":{},"cell_type":"markdown","source":"$p_i = \\sum_j(weight_{ij} * resp_{ij} * action_{ij})$\n\n> **First for each date, we get the sum of product of the weight, return and action of each trade.**\n\n$t = \\frac{\\sum p_i }{\\sqrt{\\sum p_i^2}} * \\sqrt{\\frac{250}{|i|}}$\n\n> **Then we sum each date ps and divide it by square root sum of squares of date ps, then multiply it by square root 250 divided by the number of dates.**\n\n$u = min(max(t,0), 6)  \\sum p_i$\n\n> **Then the minimum of either t or 0 is chosen, and the maximum of either the result or 6 is multiplied by the sum of date ps.**\n\n**What all of this means is that we need t to be positive to have a score above 0, and more than 6 in order to maximize the score. This in turn is decided upon the action threshold. let's try different thresholds for action in the training set an calculate the different ts and us.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def determine_action(df, thresh):\n    \"\"\"Determines action based on defined threshold.\"\"\"\n    action = (df.weight * df.resp).astype(int)\n    return action\n\ndef date_weighted_resp(df):\n    \"\"\"Calculates the sum of weight, resp, action product.\"\"\"\n    cols = ['weight', 'resp', 'action']\n    weighted_resp = np.prod(df[cols], axis=1)\n    return weighted_resp.sum()\n\ndef calculate_t(dates_p):\n    \"\"\"Calculate t based on dates sum of weighted returns\"\"\"\n    e_1 =  dates_p.sum() / np.sqrt((dates_p**2).sum())\n    e_2 = np.sqrt(250/np.abs(len(dates_p)))\n    return e_1 * e_2\n\ndef calculate_u(df, thresh):\n    \"\"\"Calculates utility score, and return t and u.\"\"\"\n    df = df.copy()\n    \n    # determines action based on threshold\n    df['action'] = determine_action(df, thresh)\n    \n    # calculates sum of dates weighted returns\n    dates_p = df.groupby('date').apply(date_weighted_resp)\n        \n    # calculate t\n    t = calculate_t(dates_p)\n    \n\n    return t, min(max(t, 0), 6) * dates_p.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Testing function time\n# thresh = 0\n\n# %time u = calculate_u(train, thresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if PLOT:\n    threshs = np.linspace(-0.5, 0.5, 100)\n    ts = []\n    us = []\n\n    for thresh in threshs:\n        t, u = calculate_u(train, thresh)\n        ts.append(t)\n        us.append(u)\n        \n    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n    axes[0].plot(threshs, ts)\n    axes[0].set_title('Different t scores by threshold')\n    axes[0].set_xlabel('Threshold')\n\n    axes[1].plot(threshs, us)\n    axes[1].set_title('Different u scores by threshold')\n    axes[1].set_xlabel('Threshold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Filling missing values\n\nSince my baseline model will be XGBoost, I'll set NaNs to -999.\n\n\n## Rows with 0 weight\n\nI'll also begin by training the model on all data, even data with weights set to zero.\n\n\n## Features\n\nI'll use only features_{0, 129}"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NaN filling\ntrain = train.fillna(-999)\n\n# Action determination\ntrain['action'] = (train['weight'] * train['resp'] > 0).astype(int)\n\n# Features\nfeatures = train.columns[train.columns.str.contains('feature')]\n# train[features] = train[features].astype(np.float64)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{},"cell_type":"markdown","source":"## Model Parameters\n\n<strike>I'll stick with parameters provided by this Yirun's kernel: https://www.kaggle.com/gogo827jz/jane-street-xgboost-grouptimesplitkfold</strike>\n\nAt first I was going to tackle this as a classification problem, but then I decided that I'd like to begin by tackling it as a regression one. So I'll start with just regular parameters to check.\n\n## Problem definition\n\nI have a hunch that says treat the problem as regression, then determine the action post model using the equation (weight * resp) > thresh, where thresh right now is 0, but I think it could be changed."},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1995\n\n# params = {'booster': 'gblinear',\n#           'objective': 'reg:squarederror',\n#           'ntree_limit' : 0,\n#           'colsample_bytree': 0.5,\n#           'learning_rate': 0.05,\n#           'max_depth': 5,\n#           'alpha': 10,\n#           'n_estimators': 10, \n#           'tree_method': 'gpu_hist', \n#           'random_state': seed,\n#          }\n\n# target = 'resp'\n\n\nparams = {'learning_rate': 0.050055662268729532,\n          'max_depth': 6, \n          'gamma': 0.07902741481945934, \n          'min_child_weight': 9.9404564544994, \n          'subsample': 0.7001330243186357, \n          'colsample_bytree': 0.7064645381596891, \n          'objective': 'binary:logistic',\n          'eval_metric': 'auc', \n          'tree_method': 'gpu_hist', \n          'random_state': seed,\n         }\n\ntarget = 'action'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CV Strategy\n\nI don't know the most optimal strategy here, so maybe I should just start simple with a normal kfold, then I could try more complex stuff once this one passes the test.\n\n<strike>A couple of ideas the I have are:\n1. Splitting Stratified Kfold according to reset ts_id based on day so each fold has equal number of ordered trades.\n2. Splitting Stratified Kfold based on date.\n3. Splitting Stratified Kfold based on action.\n4. Splitting Multiple Stratfied Kfold based on any combination of the 3 previous splits.</strike>\n\nI took a look at the kernels, and the consensus seems that using TimeSeries splits is the popular strategy here. As far as I understand, GroupTimeSeriesSplit splits the data with respect to it's temporal significance group, which is date, in an ascending manner. Where for example if KFolds was the number of dates, the first training set is just date 1 and validation is date 2, and so on, until the training data is dates 1-499 and validation is 500. That's of course is just a leave one out per group, but it simplifies the idea behind it.\n\n\n## GroupTimeSplitKFold\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\nclass GroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Provides train/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_size : int, default=None\n        Maximum size for a single training set.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n                           'b', 'b', 'b', 'b', 'b',\\\n                           'c', 'c', 'c', 'c',\\\n                           'd', 'd', 'd'])\n    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n                  \"TEST GROUP:\", groups[test_idx])\n    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n    TEST GROUP: ['c' 'c' 'c' 'c']\n    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n    TEST: [15, 16, 17]\n    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n    TEST GROUP: ['d' 'd' 'd']\n    \"\"\"\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_size=None\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_size = max_train_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n        group_test_size = n_groups // n_folds\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n            for train_group_idx in unique_groups[:group_test_start]:\n                train_array_tmp = group_dict[train_group_idx]\n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n            train_end = train_array.size\n            if self.max_train_size and self.max_train_size < train_end:\n                train_array = train_array[train_end -\n                                          self.max_train_size:train_end]\n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n            yield [int(i) for i in train_array], [int(i) for i in test_array]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just for testing the pipeline's readiness\n\n# train = train.iloc[:10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, precision_recall_curve\nimport xgboost as xgb\n\nn_splits = 5\nthresh = 0\n\n# oof validation prediction array\noof = np.zeros(len(train['action']))\n\n# validation indices in case of time series split\nval_idx_all = []\n\n# a list to store k-folds models\nmodels = []\n\n# cv strategy\ngkf = GroupTimeSeriesSplit(n_splits=n_splits)\n# kfold = KFold(n_splits=n_splits)\n\n\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train.action.values.get(), groups=train.date.values.get())):\n    \n    X_train, X_val = train.loc[train_idx, features].values.get(), train.loc[val_idx, features].values.get()\n    y_train, y_val = train.loc[train_idx, target].values.get(), train.loc[val_idx, target].values.get()\n    \n    # init dmatrix for optimized learning\n    D_train = xgb.DMatrix(X_train, label=y_train)\n    D_val = xgb.DMatrix(X_val, label=y_val)\n    \n    # training and evaluation score\n    xg_reg = xgb.train(params, D_train, 10000, [(D_val, 'eval')], early_stopping_rounds=100, verbose_eval=0)\n\n#     xg_reg = xgb.XGBRegressor(**params)\n#     xg_reg.fit(X_train, y_train)\n    \n    # evaluation of validation predictin using rmse\n    oof[val_idx] += xg_reg.predict(D_val, ntree_limit=0)\n    score = roc_auc_score(y_val, oof[val_idx])\n    print(f'FOLD {fold} ROC AUC:\\t {score}')\n    \n    # appending model to list of models for further inferences\n    models.append(xg_reg)\n    \n    # appending val_idx in case of group time series split\n    val_idx_all.append(val_idx)\n    \n    # deleting excess data to avoid running out of memory\n    del X_train, X_val, y_train, y_val, D_train, D_val\n    gc.collect()\n    \n\n# concatenation of all val_idx for further acessing\nval_idx = np.concatenate(val_idx_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating predicted weighted resp\noof_weighted_resp = train.loc[val_idx, 'weight'].values.get() * oof[val_idx]\n\n# calculating action based on predicted resp same way as train (weight * resp > 0)\noof_action = oof_weighted_resp.astype(int)\n\n# holding a resp only array with trades having zero weight set to 0 action\noof_zero = np.where(train.loc[val_idx, 'weight'].values.get() == 0, 0, oof[val_idx])\n\n# settings targets\ntargets_val = train.loc[val_idx, 'action'].values.get()\n\nauc_oof = roc_auc_score(targets_val, oof_action)\nprint(auc_oof)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of different (weight x resp) thresholds using ROC AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(targets_val, oof_weighted_resp)\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--', label='Random')  # dashed diagonal\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right')\n    plt.grid()\n    \nplot_roc_curve(fpr, tpr, 'XGB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of different resp thresholds using ROC AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(targets_val, oof_zero)\nplot_roc_curve(fpr, tpr, 'XGB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of different (weight x resp) thresholds using Precision/Recall curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_recall_curve(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(loc='lower left')\n    plt.grid()\n    \nprecisions, recalls, thresholds = precision_recall_curve(targets_val, oof_weighted_resp)\nplot_precision_recall_curve(precisions, recalls, thresholds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of different resp thresholds using Precision/Recall curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"precisions, recalls, thresholds = precision_recall_curve(targets_val, oof_zero)\nplot_precision_recall_curve(precisions, recalls, thresholds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating different (weight * resp) thresholds based on utility score"},{"metadata":{"trusted":true},"cell_type":"code","source":"threshs = np.linspace(-0.5, 0.5, 100)\nts = []\nus = []\n\n\nfor thresh in threshs:\n    train.loc[val_idx, 'action'] = (oof_weighted_resp > thresh).astype(int)\n    print(train['action'].value_counts())\n    t, u = calculate_u(train, thresh)\n    ts.append(t)\n    us.append(u)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\naxes[0].plot(threshs, ts)\naxes[0].set_title('Different t scores by threshold')\naxes[0].set_xlabel('Threshold')\n\naxes[1].plot(threshs, us)\naxes[1].set_title('Different u scores by threshold')\naxes[1].set_xlabel('Threshold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating different resp thresholds based on utility score"},{"metadata":{"trusted":true},"cell_type":"code","source":"threshs = np.linspace(-0.5, 0.5, 100)\nts = []\nus = []\n\n\nfor thresh in threshs:\n    train.loc[val_idx, 'action'] = (oof_zero > thresh).astype(int)\n    print(train['action'].value_counts())\n    t, u = calculate_u(train, thresh)\n    ts.append(t)\n    us.append(u)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\naxes[0].plot(threshs, ts)\naxes[0].set_title('Different t scores by threshold')\naxes[0].set_xlabel('Threshold')\n\naxes[1].plot(threshs, us)\naxes[1].set_title('Different u scores by threshold')\naxes[1].set_xlabel('Threshold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\n\nenv = janestreet.make_env()\nenv_iter = env.iter_test()\n    \nfor test_df, pred_df in tqdm.tqdm(env_iter):\n    test_df = test_df.fillna(-999)\n    D_test = xgb.DMatrix(test_df.loc[:, features])\n    for i, reg in enumerate(models):\n        if i == 0:\n            pred = reg.predict(D_test, ntree_limit=0) / len(models)\n        else:\n            pred += reg.predict(D_test, ntree_limit=0) / len(models)\n        \n    # set according to different action strategty\n    pred_action = (test_df['weight'] * pred).astype(int)\n\n    pred_df.action = pred_action\n    \n    env.predict(pred_df)\n    \n    del test_df, pred_df, D_test\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}