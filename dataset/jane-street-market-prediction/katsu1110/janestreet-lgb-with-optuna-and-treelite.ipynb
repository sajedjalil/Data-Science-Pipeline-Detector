{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h2>Jane Street Market Prediction | LGB Hyperparameter Optimization | katsu1110 </h2></center><hr>\n\n![](https://optuna.org/assets/img/optuna-logo@2x.png)\n\nHere I demonstrate how to use [Optuna](https://optuna.org/) to get a better set of hyperparameters by the Bayesian Optimization. I need a good LGB model for my ensemble:D\n\nAs a bonus, I save the tuned model in the [Treelite](https://treelite.readthedocs.io/en/latest/) format to accelerate the inference speed.\n\nThis notebook loads feathered-data from [my another notebook](https://www.kaggle.com/code1110/janestreet-save-as-feather?scriptVersionId=47635784) such that we don't have to spend our time on waiting long for loading csv files.\n\nIn this notebook we treat the task as a binary classification."},{"metadata":{},"cell_type":"markdown","source":"# Install Treelite"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip --quiet install ../input/treelite/treelite-0.93-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip --quiet install ../input/treelite/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os, sys\nimport gc\nimport math\nimport random\nimport pathlib\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport operator\nimport xgboost as xgb\nimport lightgbm as lgb\nimport optuna\nfrom tqdm import tqdm_notebook as tqdm\n\n# treelite\nimport treelite\nimport treelite_runtime \n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config\nSome configuration setups."},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 20201225 # Merry Christmas!\n# INPUT_DIR = '../input/jane-street-market-prediction/'\nINPUT_DIR = '../input/janestreet-save-as-feather/'\nTRADING_THRESHOLD = 0.50 # 0 ~ 1: The smaller, the more aggressive\nDATE_BEGIN = 0 # 0 ~ 499: set 0 for model training using the complete data ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data\nI have already saved the training data in the feather-format in [my another notebook](https://www.kaggle.com/code1110/janestreet-save-as-feather?scriptVersionId=47635784). Loading csv takes time but loading feather is really light:)"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(INPUT_DIR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%time\n\n# load data blitz fast!\ndef load_data(input_dir=INPUT_DIR):\n    train = pd.read_feather(pathlib.Path(input_dir + 'train.feather'))\n    features = pd.read_feather(pathlib.Path(input_dir + 'features.feather'))\n    example_test = pd.read_feather(pathlib.Path(input_dir + 'example_test.feather'))\n    ss = pd.read_feather(pathlib.Path(input_dir + 'example_sample_submission.feather'))\n    return train, features, example_test, ss\n\ntrain, features, example_test, ss = load_data(INPUT_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete irrelevant files to save memory\ndel features, example_test, ss\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove weight = 0 for saving memory \noriginal_size = train.shape[0]\ntrain = train.query('weight > 0').reset_index(drop=True)\n\n# use data later than DATE_BEGIN\ntrain = train.query(f'date >= {DATE_BEGIN}')\n\nprint('Train size reduced from {:,} to {:,}.'.format(original_size, train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target\ntrain['action'] = train['resp'] * train['weight']\ntrain['action'] = 1 * (train['action'] > 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features to use\nfeats = [f for f in train.columns.values.tolist() if f.startswith('feature')]\nprint('There are {:,} features.'.format(len(feats)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter optimization\nI use the last dates as a validation data (Time-series split) to Bayesian-Optimize hyperparameters of my LGB."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['date'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# time series split like\npivot = 460\nx_train = train.query(f'date < {pivot}')[feats]\ny_train = train.query(f'date < {pivot}')['action']\nx_val = train.query(f'date >= {pivot}')[feats]\ny_val = train.query(f'date >= {pivot}')['action']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/gogo827jz/jane-street-super-fast-utility-score-function/notebook\nfrom numba import njit\n\n@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Theoretical best score for this validation period\ndate = train.query(f'date >= {pivot}')['date'].values\nweight = train.query(f'date >= {pivot}')['weight'].values\nresp = train.query(f'date >= {pivot}')['resp'].values\naction = 1 * (train.query(f'date >= {pivot}')['action'].values > TRADING_THRESHOLD)\nscore = utility_score_numba(date, weight, resp, action)\nprint(f\"Utility Score = {score}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(x_train, y_train)\nlgb_eval = lgb.Dataset(x_val, y_val)\n\ndef objective(trial):    \n    params = {\n            'num_leaves': trial.suggest_int('num_leaves', 32, 1024),\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'metric': 'binary_logloss',\n            'max_depth': trial.suggest_int('max_depth', 4, 16),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 12),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n            'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-6, 1.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-6, 1.0),\n            }\n\n    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], \n                      early_stopping_rounds=10, verbose_eval=1000)\n    val_pred = model.predict(x_val)\n    \n    # score\n    date = train.query(f'date >= {pivot}')['date'].values\n    weight = train.query(f'date >= {pivot}')['weight'].values\n    resp = train.query(f'date >= {pivot}')['resp'].values\n    action = 1 * (val_pred > TRADING_THRESHOLD)\n    score = utility_score_numba(date, weight, resp, action)\n    print(f\"Utility Score = {score}\")\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Bayesian optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Best sets of hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of finished trials: {}'.format(len(study.trials)))\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\n\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\nfrom optuna.visualization import plot_optimization_history\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Re-training LGB with the best params"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint('Starting training...')\nlgb_train = lgb.Dataset(train[feats], train['action'])\nmodel = lgb.train(trial.params,\n                lgb_train,\n                num_boost_round=480,\n                valid_sets=lgb_train,  # eval training data\n                feature_name=feats,\n                categorical_feature=[])\n\nprint('Saving model...')\n# save model to file\nmodel.save_model('my_model.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance\nLet's see feature importance given by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(model, importance_type=\"gain\", figsize=(7, 40))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treelite\nI believe Treelite is must in this competition, to avoid the sumission error due to the long inference time."},{"metadata":{"trusted":true},"cell_type":"code","source":"# load LGB with Treelite\nmodel = treelite.Model.load('my_model.txt', model_format='lightgbm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate shared library\ntoolchain = 'gcc'\nmodel.export_lib(toolchain=toolchain, libpath='./mymodel.so',\n                 params={'parallel_comp': 32}, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictor from treelite\npredictor = treelite_runtime.Predictor('./mymodel.so', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit\nLet's use Treelite for faster inference."},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n    \nfor (test_df, pred_df) in tqdm(iter_test):\n    if test_df['weight'].item() > 0:\n        # inference with treelite\n        batch = treelite_runtime.Batch.from_npy2d(test_df[feats].values)\n        pred_df.action = (predictor.predict(batch) > TRADING_THRESHOLD).astype('int')\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All done!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}