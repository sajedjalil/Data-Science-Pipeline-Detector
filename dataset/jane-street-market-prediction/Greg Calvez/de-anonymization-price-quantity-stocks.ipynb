{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# De-anonymization: Price, Quantity and Stocks\n\n### Introduction\n\nIn a previous notebook  [Time aggregation tags](https://www.kaggle.com/gregorycalvez/de-anonymization-time-aggregation-tags), I showed the meaning of `tag_{0, 1, 2, 3, 4, 5}`. These are the \"time-aggregation tag\", see table below. \n\n\n| Tag      | Aggregation Type | Characteristic Time (% of day) | Characteristic Time in min\n| ----------- | ----------- | ----------- | ----------- |\n| `tag_0`      | Rolling Sum/Average       | 0.03 | ~ 20 sec\n| `tag_1`      | Rolling Sum/Average       | 0.27 | ~ 3 min\n| `tag_2`      | Rolling Sum/Average       | 0.59 | ~ 5 min\n| `tag_3`      | Rolling Sum/Average       | 1.33 | ~ 10 min\n| `tag_4`      | Rolling Sum/Average       | 4.55 | ~ 30 min\n| `tag_5`      | Cumulative throughout the day        | None | None\n\n\nWe continue the study by showing that it is possible to identify trades on the same financial instrument in the same day. \nIn this notebook I will use the word \"stock\" instead of \"financial instrument\". Yet, I am not sure that we are actually dealing with stocks (it could be options, ETFs, or something else entirely). \n\nHere we try to show that: \n- Trades for a stock / day combination can be isolated \n- `tag_14`, `tag_18` might be embedding of the stocks \n- `tag_6` means a price metric\n- `tag_23` means a volume-like metric\n- `tag_20` might be a spread\n\n# Stock Identification\n\n### How `tag_5` gives the stocks\n\n<span style=\"color:blue\">**Remark:** this sections is useless as there is another (easier) way of identifying stocks. But this section explains how I found the easier way. Feel free to jump to the next section for an \"easier\" stock identification</span>\n\nIn the previous notebook, `tag_5` appeared to be a time-aggregation tag. `tag_5` is always associated with `tag_23`. \n\nFurthermore, `tag_23` seems to be \"modulated\" by other tags. A feature that has `tag_23` will always be of the form: \n```feature_x = tag_{0, 1, 2, 3, 4, 5} x tag_{15, 17} x tag_23 x tag_{24, 25, 26, 27}```\n\nFor a given day (date=12), let's plot the features with `tag_5`, `tag_23`, `tag_26`, i.e. `feature_101`, `feature_107`. Those are two very similar features that represent some cumulative variables, hence - and it's the key part - smooth function of time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly_express as px\n\nimport utility\nu = utility.Utility('/kaggle/input/jane-street-market-prediction/')\ntrain = pd.read_csv(u.filepath_train(), nrows=int(1e5))\ntrain = u.add_intraday_ts(train)\ntrain_date = train[train['date'] == 12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(u.select_tags('tag_5').astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter_3d(\n    train_date, \n    'intraday_ts', \n    'feature_101', \n    'feature_107', \n    size=np.ones(train_date.shape[0]) * 0.01,\n    color='intraday_ts'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### What do we see? \nOn this 3D graph, we can easily see that the data (`feature_101`, `feature_107`) consists of points being sampled on a set of continuous functions of time. \nThis is the case for all the features sharing `tag_5` and `tag_23`. \n\n##### What does it mean and how can we use it? \nI understood `tag_23` as being an \"additive\" metric. For instance the number of trades or the volume traded. Therefore, each one of the \"continuous functions\" seen on the graph must be the same instrument.\n\nThen, we need to cluster those points. \n\n##### Cluster the points\nTo categorize the points into set of points forming \"smooth\" functions, I couldn't find a nice \"ready-to-use\" library. An EM algorithm might have worked. But instead, I designed a greedy algorithm based on the a smooth-time-series approach. \n\nThe algorithm is fully implemented in the `utility.py` file and I will only call it here for demo purposes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### Preparing the data\ntrain_date['intraday_ts'] = 1 - train_date['intraday_ts']  # Flip time axis\ntrain_date.sort_values('intraday_ts', inplace=True)\nfeatures_to_use = ['feature_101', 'feature_107']\ntimes = train_date['intraday_ts'].values\nx_points = train_date[features_to_use].values\npoints = [(t, x) for t, x in zip(times, x_points)]\n\n### Running the algo (details in the utility file)\nalpha_prior = np.array([-3.22728931, -6.3402686 ])   # Found with regression on the whole dataset\nepsilon = 20                                         # Manual Tweaking\ndistance_threshold = 0.08                            # Manual Tweaking\nt_threshold = 0.3                                    # Do not start a new curve in the middle of the day\nclusterer = utility.Cluster1DSmoothFunctions(\n    distance_threshold, \n    t_threshold, \n    epsilon, \n    verbose=True\n)\nstock_ids = clusterer.run(points, alpha_prior)\ntrain_date['stock_id'] = stock_ids\ntrain_date['stock_id'] = train_date['stock_id'].astype(str)\ntrain_date['intraday_ts'] = 1 - train_date['intraday_ts']  # Re-flip time axis\n# Plotting the results\nfig = px.scatter_3d(\n    train_date,\n    'intraday_ts', \n    'feature_101', \n    'feature_107', \n    size=np.ones(train_date.shape[0]) * 0.01,\n    color='stock_id', \n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ploting some tag_5 features one 1 stock\nplt.figure(figsize=(10, 5))\none_stock = train_date[train_date['stock_id'] == '41']\nplt.plot(one_stock['intraday_ts'], one_stock['feature_101'], label='feature_101')\nplt.plot(one_stock['intraday_ts'], one_stock['feature_107'], label='feature_101')\nplt.plot(one_stock['intraday_ts'], one_stock['feature_77'], label='feature_101')\nplt.xlabel('Time')\nplt.ylabel('Features')\nplt.title('Some features on one Stock')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### What now ? \nThis \"clustering\" is not perfect. \nBut at least, some clusters seem to make sense. So we retrieved at least a series of trades from one stock. \n\nBased on that series of trades, I figured out many things. But the most important one: all this process (`tag_5+tag_23` and custom clustering) is useless: \n\n`tag_14` and related features `feature_{41, 42, 43}` give the stock. "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(one_stock[['feature_41', 'feature_42', 'feature_43']].drop_duplicates())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embedding of stocks: `tag_14` and `tag_18`\n\n`tag_14` seems a good indicator of a stock. The values of the `feature_{41, 42, 43, 44, 45}` (note the continuity in the naming of features) seem to be constant for a stock / day combination. That allows us to \"cluster\" more efficiently the graph seen previously. "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(u.select_tags('tag_14 | tag_18').astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_date = train[train['date'] == 3]\ntrain_date = u.add_stock_id(train_date)\nmost_present_stocks = train_date.groupby('stock_id', as_index=False).agg({'ts_id': 'count'}).rename(columns={'ts_id': 'num_trades'}).sort_values('num_trades', ascending=False).head(100)['stock_id'].values.tolist()\nmost_present_stocks = [str(i) for i in most_present_stocks]\n\nto_plot = train_date[train_date['stock_id'].isin(most_present_stocks)]\nfig = px.scatter_3d(\n    to_plot, \n    'intraday_ts', \n    'feature_101', \n    'feature_107', \n    size=np.ones(to_plot.shape[0]) * 0.01,\n    color='stock_id'\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prices `tag_6` and `feature_0`\n\nWith all that new information, I started to look for a price chart. And it seems that `tag_6` is the answer. \n\nA quick view of the `features.csv` file show how important this tag is. As for `tag_23`, `tag_6` seems to be associated with other tags \"modulating\" the price. A normal feature with `tag_6` is something like: \n```feature_x = tag_{0, 1, 2, 3, 4, 5} x tag_6 x tag_9 (y/n) x tag_{11, 12, 13}```\nor\n```feature_x = tag_6 x tag_9 (y/n) x tag_{7, 8, 10}```\n\nI can't fully explain this pattern today. But if `tag_{0, 1, 2, 3, 4, 5}` are all 0 with `tag_6` being one, I guessed that the features were representing a non aggregated price, \"instantaneous price\"\n\nFor every feature with `tag_9 == 1` and `tag_6 == 1`, there is a feature with `tag_9 == 0 & tag_6 == 1`. And they are always very very correlated (maybe a bid/ask or buy/sell tag). So let's select some features, and look at our stock !"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(u.select_tags('tag_6 & ~tag_9 & ~tag_0 & ~tag_1 & ~tag_2 & ~tag_3 & ~tag_4').astype(int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_look_at = u.get_features('tag_6 & ~tag_9 & ~tag_0 & ~tag_1 & ~tag_2 & ~tag_3 & ~tag_4')\nstock_id = '533'\nstock = to_plot[to_plot['stock_id'] == stock_id]\nsns.pairplot(stock[features_to_look_at])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `feature_0` helping us\n\n\n`feature_0` was the first feature to be de-anonymized. It seems to be the side of the trade. How can it help us then ?\n\nFirst thing, those features are not distributed randomly. But the distribution of `feature_{3, 5, 37, 39}` was clustered in 2 parts. I clustered that data and studied it more closely. It turned out that they clustered perfectly on the `feature_0` values. \n\nSo I tried multiplying those features with `feature_0`. \nIn financial terms, multiplying a price with the side of the trade gives you the \"cost\" of the trade. Example: You BUY 1 share @ 100USD, your cash becomes -100USD (i.e. you used 100USD to buy something). If you sold those share, you cash would become +100USD. \n\nHere what those new features give us: "},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_look_at = ['feature_3', 'feature_5', 'feature_37', 'feature_39']\nnew_features_to_look_at = []\nfor f in features_to_look_at: \n    new_f = f'{f}_0'\n    new_features_to_look_at.append(new_f)\n    stock[new_f] = stock['feature_0'] * stock[f]\nsns.pairplot(stock[new_features_to_look_at])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Back to prices\n\nLet's see what it looks like once we plot those new features with time.\nI also added some colors to show `feature_0` being `1` or `-1`. \n\nThose graphs are similar to \"classical\" intraday price chart. With the fact that `tag_6` features with rolling windows are impacted by NaNs at open, mid-day and randomly throughout the day makes me think of `tag_6` as a price tag. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nn_fig = len(new_features_to_look_at)\nfor i, f in enumerate(new_features_to_look_at): \n    plt.subplot(n_fig, 1, i+1)\n    plt.scatter(stock['intraday_ts'], stock[f], c=stock['feature_0'], cmap='plasma')\n    plt.ylabel(f)\nplt.xlabel('Time')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other guesses\n\n### On embedding values\n\n##### `tag_14`, a PCA?\nJust a guess, would `feature_{41, 42, 43}` be the 3 first components of a PCA embedding the stocks ?\n\n##### `tag_18`, average of daily trading volume ?\n`tag_18` is associated with `tag_{15, 17}`, which are assocatied to `tag_23`. I already said that `tag_23` was an additive variable (maybe the volume or number of trades). So `tag_18` would be the [ADTV](https://www.investopedia.com/terms/a/averagedailytradingvolume.asp#:~:text=Average%20daily%20trading%20volume%20(ADTV)%20is%20the%20average%20number%20of,find%20the%20average%20daily%20volume.) ?\n\n### `tag_{15, 17}` buy / sell ?\nAnother guess. No justification\n\n### `tag_{7, 8, 10}` related instruments price?\nMaybe currencies, options, related to the main financial instrument ?\n\n### `tag_9`, bid / ask indicator?\n\n### `tag_20` a spread?\nSee below the pairplot of the features related to `tag_20`. All these features are \"aligned\" they are on a grid. \n\nIn financial markets, markets are like an auction. People ready to buy \"bid\" for a price, people ready to sell \"ask\" for a price. Those prices are are not equal (otherwise a trade would be possible). There difference between those two values is called a spread. \n\nPrices are \"regulated\" and must be a multiple of a \"tick size\", eg. 0.01USD. Therefore, the spread must be a multiple of that tick size as well. It migt be what we see in that pairplot below. "},{"metadata":{"trusted":true},"cell_type":"code","source":"display(u.select_tags('tag_20').astype(int))\nfeatures_spread = u.get_features('tag_20')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(stock[features_spread])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nI hope that this notebook, with the previous notebook [Time aggregation tags](https://www.kaggle.com/gregorycalvez/de-anonymization-time-aggregation-tags) will make other people look into further de-anonymization of the data. \n\n### Next Steps\n\n\n##### Further feature relations\nWe managed to plot \"price charts\" of some stocks with the data. But there remains many things to be done. With the knowledge on how to isolate trades relating to a unique instrument, many new relations between features can be uncovered. \n\n##### `date` de-anonymization\nFurthermore, if one manages to fully de-anonymize the date, one could imagine being able to match the price charts computed form the dataset with the tick prices of \"famous\" securities. If that is done, we could dream of being able to explain most of the features. \n\nLet me know your thoughts ! "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}