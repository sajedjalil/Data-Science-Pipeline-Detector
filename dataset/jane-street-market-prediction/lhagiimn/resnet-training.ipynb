{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pickle\nimport os\nfrom sklearn.model_selection import GroupKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nfrom sklearn.metrics import precision_score, roc_auc_score\nimport pickle\nimport pandas as pd\nimport datatable as dt\nimport matplotlib.pylab as plt\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Competition score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score_numba(date, weight, resp, action):\n\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Ensemble Neural Net Architecture "},{"metadata":{"trusted":true},"cell_type":"code","source":"################################# ENSEMBLE MODELS STRUCTURE GENERATION #####################################\n\ndef ensemble_structure(number_models, num_features, MODEL_ROOT):\n\n    if number_models == 1:\n\n        hidden_dims = [150]\n        number_of_dims = [num_features]\n        input_dims = [np.arange(num_features)]\n\n    elif os.path.isfile(MODEL_ROOT + \"/\" + 'hidden_dims.pkl'):\n        with open(MODEL_ROOT + \"/\" + 'hidden_dims.pkl', 'rb') as handle:\n            hidden_dims = pickle.load(handle)\n\n        with open(MODEL_ROOT + \"/\" + 'number_of_dims.pkl', 'rb') as handle:\n            number_of_dims = pickle.load(handle)\n\n        with open(MODEL_ROOT + \"/\" + 'input_dims.pkl', 'rb') as handle:\n            input_dims = pickle.load(handle)\n\n    else:\n\n        hidden_dims = np.random.randint(96, 160, number_models)\n        number_of_dims = np.random.randint(int(num_features*0.7), num_features, number_models)\n        input_dims = []\n        for i in range(number_models):\n            input_dims.append(np.random.randint(0, num_features, number_of_dims[i]))\n\n        with open(MODEL_ROOT + \"/\" + 'hidden_dims.pkl', 'wb') as handle:\n            pickle.dump(hidden_dims, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open(MODEL_ROOT + \"/\" + 'number_of_dims.pkl', 'wb') as handle:\n            pickle.dump(number_of_dims, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open(MODEL_ROOT + \"/\" + 'input_dims.pkl', 'wb') as handle:\n            pickle.dump(input_dims, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return hidden_dims, number_of_dims, input_dims\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fold Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################# FOLD SELECTION #####################################\ndef fold_selection(folds, kf=0, random = False,\n                   date=450, Debug=False):\n\n    folds['fold']=int(0)\n\n    if Debug==True:\n\n        val = folds.sample(n=int(folds.shape[0]/10))\n        folds.loc[val.index, 'fold'] = int(1)\n\n    else:\n        if random==True:\n            val = folds.sample(n=int(folds.shape[0] / 10))\n            folds.loc[val.index, 'fold'] = int(1)\n\n        elif kf>0:\n\n            group_kfold = GroupKFold(n_splits=kf)\n            group = folds['date']\n\n            f = 1\n            for train_index, test_index in group_kfold.split(folds, folds['resp'], group):\n                folds.loc[test_index, 'fold'] = int(f)\n                f = f+1\n\n        else:\n            folds.loc[folds['date']>date, 'fold'] = int(1)\n\n    folds['fold'] = folds['fold'].astype(int)\n\n    return folds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"############## Models ######################################################################\nclass simple_linear_layer(nn.Module):\n    def __init__(self, input_dim, out_dim):\n        super(simple_linear_layer, self).__init__()\n        \n        self.dense = nn.Linear(input_dim, out_dim)\n        self.batch_norm = nn.BatchNorm1d(out_dim)\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        \n        x = self.dense(x)\n        x = self.batch_norm(x)\n        x = self.dropout(x)\n        \n        return x\n\nclass MLP_Model(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, out_dim=4):\n        super(MLP_Model, self).__init__()\n\n        self.block1 = simple_linear_layer(input_dim, hidden_dim)\n        self.block2 = simple_linear_layer(hidden_dim, hidden_dim)\n        self.block3 = nn.Linear(int(hidden_dim), out_dim)\n\n    def forward(self, x):\n\n        v = F.relu(self.block1(x))\n        v = F.relu(self.block2(v))\n        out = self.block3(v)\n\n        return out\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, downsample=None):\n        super(ResidualBlock, self).__init__()\n        \n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.2)\n       \n        \n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.bn2 = nn.BatchNorm1d(output_dim)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.2)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n        \n        out = self.fc1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n        out = self.dropout1(out)\n        \n        \n        out1 = self.fc2(out)\n        out1 = self.bn2(out1)\n        out1 = self.relu2(out1)\n        out1 = self.dropout2(out1)\n        \n        if self.downsample:\n            residual = self.downsample(residual)\n            \n        out1 = torch.cat([out1, residual], dim=1)\n        \n        return out1\n\nclass ResNet(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, block, num_classes=4):\n        super(ResNet, self).__init__()\n\n        self.layer1 = self.make_layer(block, input_dim, hidden_dim, hidden_dim)\n        self.layer2 = self.make_layer(block, 2*hidden_dim, hidden_dim, hidden_dim)\n        self.out = nn.Linear(2*hidden_dim, num_classes)\n\n    def make_layer(self, block, input_dim, hidden_dim, output_dim):\n        downsample = None\n        if (input_dim != output_dim):\n            downsample = nn.Sequential(\n                nn.Linear(input_dim, output_dim))\n\n        layer=block(input_dim, hidden_dim, output_dim, downsample)\n\n        return layer\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.out(out)\n        return out\n\n\n####################### Final Ensemble Model ######################################\nclass Ensemble_MoaModel(nn.Module):\n\n    def __init__(self, input_dims, number_of_dims, hidden_dims,\n                 model_name, out_dim=4):\n        super(Ensemble_MoaModel, self).__init__()\n\n        self.models = torch.nn.ModuleList()\n        self.input_dims = input_dims\n        self.model_name = model_name\n\n        for i in range(len(hidden_dims)):\n            if self.model_name=='Simple_MLP':\n                self.models.append(MLP_Model(input_dim=number_of_dims[i],\n                                                    hidden_dim=hidden_dims[i], out_dim=out_dim))\n\n            elif self.model_name==\"ResNet\":\n                self.models.append(ResNet(input_dim=number_of_dims[i],\n                                          hidden_dim=hidden_dims[i],\n                                          block=ResidualBlock,\n                                          num_classes=out_dim))\n            else:\n                print(\"Please check model name. There is no this model!!!\")\n\n    def forward(self, x):\n\n        out = []\n\n        for i in range(len(self.input_dims)):\n            temp = self.models[i](x[:, self.input_dims[i]])\n            out.append(temp.unsqueeze(0))\n\n        out = torch.cat(out, dim=0)\n        out = out.permute(1, 0, 2)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pytorch Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class JaneDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y': torch.tensor(self.targets[idx], dtype=torch.float),\n        }\n        return dct\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        dct = {\n            'x': torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"################################# training inference #####################################\ndef train_fn(model, optimizer, scheduler,\n             loss_fn, dataloader, device):\n\n    model.train()\n    final_loss = 0\n    train_preds = []\n    train_y = []\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        train_y.append(targets.detach().cpu().numpy())\n        outputs = model(inputs)\n        loss = 0\n\n        for i in range(outputs.shape[1]):\n            loss = loss + loss_fn(outputs[:, i],\n                                  targets.to(device))\n\n        loss.backward()\n        optimizer.step()\n\n        final_loss += loss.item()\n        train_preds.append(np.mean(outputs.sigmoid().detach().cpu().numpy(), axis=1))\n\n    final_loss /= len(dataloader)\n    train_preds = np.concatenate(train_preds)\n    train_y = np.concatenate(train_y)\n\n    auc = roc_auc_score(train_y, train_preds)\n\n    return auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################# validation inference #####################################\ndef valid_fn(model, df, dataloader, device,\n             scheduler=None, loss_fn=None):\n\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    val_y = []\n\n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        val_y.append(targets.detach().cpu().numpy())\n\n        outputs = model(inputs)\n\n        if loss_fn==None:\n            valid_preds.append(np.mean(outputs.sigmoid().detach().cpu().numpy(), axis=1))\n            final_loss = 0\n\n        else:\n            loss = loss_fn(torch.mean(outputs, dim=1), targets.to(device))\n            final_loss += loss.item()\n            valid_preds.append(np.mean(outputs.sigmoid().detach().cpu().numpy(), axis=1))\n\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    val_y = np.concatenate(val_y)\n\n\n    actual = np.where(valid_preds>=0.5, 1, 0)\n    score = utility_score_numba(df['date'].values, df['weight'].values,\n                                df['resp'].values, actual[:, 0])\n\n    auc = roc_auc_score(val_y, valid_preds)\n    if scheduler!=None:\n        scheduler.step(auc)\n\n    return auc, score, valid_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################################# Model Training Phase #####################################\ndef run_training(folds, target, feature_cols, target_cols,\n                 input_dims, number_of_dims, hidden_dims,\n                 model_name, BATCH_SIZE, LEARNING_RATE, WEIGHT_DECAY,\n                 EPOCHS, EARLY_STOP, EARLY_STOPPING_STEPS, fold, seed, MODEL_ROOT):\n\n    seed_everything(seed)\n\n    train_df = folds.reset_index(drop=True) #folds[folds['fold'] != fold].reset_index(drop=True)\n    valid_df = folds[folds['fold'] == fold].reset_index(drop=True)\n\n    x_train, y_train = train_df[feature_cols].values, train_df[target].values\n    x_valid, y_valid = valid_df[feature_cols].values, valid_df[target].values\n\n    train_dataset = JaneDataset(x_train, y_train)\n    valid_dataset = JaneDataset(x_valid, y_valid)\n    trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    if os.path.isfile(MODEL_ROOT + \"/\"+ f\"FOLD_{fold}_{seed}.pth\"):\n\n        model_new = torch.load(MODEL_ROOT + \"/\"+ f\"FOLD_{fold}_{seed}.pth\")\n        model_new.to(DEVICE)\n        model_new.eval()\n        valid_tpr, valid_score, valid_preds = valid_fn(model_new, valid_df, validloader, DEVICE, scheduler=None, loss_fn=None)\n        oof = valid_preds\n\n    else:\n        model_new = Ensemble_MoaModel(input_dims, number_of_dims, hidden_dims, model_name, out_dim=len(target_cols))\n        print(model_new)\n        model_new.to(DEVICE)\n\n        optimizer = torch.optim.Adam(model_new.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n        # scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n        #                                           max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n\n        loss_fn = nn.BCEWithLogitsLoss()\n\n        early_stopping_steps = EARLY_STOPPING_STEPS\n        early_step = 0\n\n        best_tpr = -np.inf\n\n        for epoch in range(EPOCHS):\n\n            train_loss = train_fn(model_new, optimizer, scheduler\n                                  , loss_fn, trainloader, DEVICE)\n\n            valid_tpr, valid_score, valid_preds = valid_fn(model_new, valid_df, validloader, DEVICE, scheduler=scheduler, loss_fn=loss_fn)\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_auc: {train_loss}, valid_auc: {valid_tpr},  valid_score: {valid_score}\")\n\n            if valid_tpr > best_tpr:\n\n                best_tpr = valid_tpr\n                oof = valid_preds\n                early_step = 0\n                torch.save(model_new, MODEL_ROOT + \"/\"+ f\"FOLD_{fold}_{seed}.pth\")\n\n            elif (EARLY_STOP == True):\n\n                early_step += 1\n                if (early_step >= early_stopping_steps):\n                    break\n\n            del train_loss, valid_tpr, valid_score\n            gc.collect()\n\n        model_new = torch.load(MODEL_ROOT + \"/\"+ f\"FOLD_{fold}_{seed}.pth\")\n        model_new.to(DEVICE)\n\n    return oof, valid_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running Phase"},{"metadata":{"trusted":true},"cell_type":"code","source":"######################## MAIN ####################################\n# HyperParameters\nEPOCHS = 50\nBATCH_SIZE = 5000\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\nDate = 475\nDebug = False\nRandom = True\nkf=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nuseless_cols = ['date', 'weight', 'ts_id']\ntarget = ['action', 'action_1', 'action_2', 'action_3', 'action_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inference = True\nif inference==True:\n    import pickle\n    from tqdm import tqdm\n    \n    model = torch.load('../input/prerained-model-with-50-epoch/ResNet_5/FOLD_1_10.pth')#, map_location=torch.device('cpu'))\n    f_mean = np.load('../input/prerained-model-with-50-epoch/fmean.npy')\n    with open('../input/prerained-model-with-50-epoch/feature_cols.pkl', 'rb') as handle:\n        feature_cols = pickle.load(handle)\n        \n    import janestreet\n    env = janestreet.make_env()\n\n    for (test_df, pred_df) in tqdm(env.iter_test()):\n\n        if test_df['weight'].item() > 0:\n\n            x_tst = test_df.loc[:, feature_cols].values\n\n            if np.isnan(x_tst[:, :].sum()):\n                x_tst[:, 1:] = np.nan_to_num(x_tst[:, 1:]) + np.isnan(x_tst[:, 1:]) * f_mean\n\n            pred = np.zeros((1, len(target_cols)))\n            pred = np.mean(model(torch.tensor(x_tst, dtype=torch.float).to(DEVICE)).sigmoid().detach().cpu().numpy(), axis=1)\n            pred = np.mean(pred, axis=1)\n            pred_df.action = np.where(pred >= 0.5, 1, 0).astype(int)\n\n        else:\n\n            pred_df.action = 0\n            \n        env.predict(pred_df)\n    \nelse:\n    train = dt.fread('../input/jane-street-market-prediction/train.csv').to_pandas()\n\n    feature_cols = list(train.drop(useless_cols + target_cols, axis=1))\n\n    for action, col in zip(target, target_cols):\n        train[action] = np.where(train[col]>0, 1, 0)\n\n    for col in feature_cols:\n        if train[col].isnull().sum()/train.shape[0]>0.05:\n            feature_cols.remove(col)\n\n    with open('feature_cols.pkl', 'wb') as handle:\n            pickle.dump(feature_cols, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    train = train[train['weight'] != 0]\n    train = train.loc[train['date']>85]\n    train = train.reset_index(drop=True)\n\n    train.fillna(train.mean(),inplace=True)\n    f_mean = np.mean(train[feature_cols[1:]].values,axis=0)\n    np.save('fmean.npy', f_mean)\n\n    list_number_of_models=[5]\n    model_names = [\"ResNet\"] #ResNet\n    SEED = 10\n\n    num_features=len(feature_cols)\n    num_targets=len(target_cols)\n\n    weights_mod = [0.5, 0.5]\n\n    oof_ensemble = np.zeros((int(train.shape[0]/5), len(target_cols)))\n    for m, model_name in enumerate(model_names):\n        for w, number_of_models in enumerate(list_number_of_models):\n            MODEL_ROOT = \"%s_%s\" % (model_name, number_of_models)\n            fold_name = 'fold.pkl'\n\n            if not os.path.exists(MODEL_ROOT):\n                os.makedirs(MODEL_ROOT)\n\n            train = fold_selection(train, kf=kf, random=Random, date=Date, Debug=Debug)\n            hidden_dims, number_of_dims, input_dims = ensemble_structure(number_of_models, num_features, MODEL_ROOT)\n\n            if kf==0:\n                oof, val_df = run_training(folds=train, target=target, feature_cols=feature_cols,\n                                                target_cols=target_cols, input_dims=input_dims,\n                                                number_of_dims=number_of_dims, hidden_dims=hidden_dims,\n                                                model_name=model_name, BATCH_SIZE=BATCH_SIZE, LEARNING_RATE=LEARNING_RATE,\n                                                WEIGHT_DECAY=WEIGHT_DECAY, EPOCHS=EPOCHS, EARLY_STOP=EARLY_STOP,\n                                                EARLY_STOPPING_STEPS=EARLY_STOPPING_STEPS, fold=1, seed=SEED,\n                                                MODEL_ROOT=MODEL_ROOT)\n\n                for th in range(490, 520):\n                    action = np.where(oof>th/1000, 1, 0)\n                    print(model_name, th, 'score:', utility_score_numba(date=val_df['date'].values, weight=val_df['weight'].values,\n                                              resp=val_df['resp'].values, action=action[:, 0]))\n\n            else:\n                for fold in range(kf):\n                    oof, val_df = run_training(folds=train, target=target, feature_cols=feature_cols,\n                                                    target_cols=target_cols, input_dims=input_dims,\n                                                    number_of_dims=number_of_dims, hidden_dims=hidden_dims,\n                                                    model_name=model_name, BATCH_SIZE=BATCH_SIZE, LEARNING_RATE=LEARNING_RATE,\n                                                    WEIGHT_DECAY=WEIGHT_DECAY, EPOCHS=EPOCHS, EARLY_STOP=EARLY_STOP,\n                                                    EARLY_STOPPING_STEPS=EARLY_STOPPING_STEPS, fold=fold+1, seed=SEED,\n                                                    MODEL_ROOT=MODEL_ROOT)\n\n                for th in range(490, 520):\n                    action = np.where(oof>th/1000, 1, 0)\n                    print(model_name, th, 'score:', utility_score_numba(date=val_df['date'].values, weight=val_df['weight'].values,\n                                              resp=val_df['resp'].values, action=action[:, 0]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}