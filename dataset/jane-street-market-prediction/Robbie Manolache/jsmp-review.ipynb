{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jane Street Market Prediction \n## *My final submission ... and how I got there*\n***\n\n**A quick heads up:** *My final submission is just a simple, basic ensemble of relatively shallow neural nets and probably nothing no one's not seen before. It ain't gonna make you [Jim Simons](https://www.investopedia.com/articles/investing/030516/jim-simons-success-story-net-worth-education-top-quotes.asp). But it's made me a lucky ducky (thus far). So enjoy and please don't steal too much of my luck! :-)*\n\n***\n\nFirst of all, this notebook is a clone of my final submission, which I kept super-lightweight. All of the underlying work is spread across quite a few notebooks and my GitHub repo for the competition - I've inserted relevant links throughout. Anyway, here's a recap of my journey in this competition:\n\n## 1. Prototyping\n\nMost of the ground work is in my GitHub repo [here](https://github.com/robert-manolache/kaggle-jsmp). I like working with scalable, reusable functions, so I built a `jsmp` package to make my life easier. Note that it includes some preliminary functions from a Python package I'm trying to develop as a wrapper for the Kaggle API, aptly called [lazykaggler](https://github.com/robert-manolache/lazy-kaggler) - hopefully I'll get the time to finish it someday and share it with the Kaggle community!\n\n### 1.1 LightGBM experiment\n\nI've outlined this experiment in this [notebook](https://github.com/robert-manolache/kaggle-jsmp/blob/main/lightGBM_experiment.ipynb) in my GitHub repo. In summary:\n* I tried to predict classes for various `resp` ranges using a LightGBM multi-classifier\n* Though it looked reasonably promising, it was somewhat tricky to come up with a good way to translate the various class probabilities into a 0/1 action (I even briefly considered a reinforcement learning approach, but it got messy and complicated very quickly!)\n* That's when I started thinking more about using a neural network to directly model the 0/1 action from the baseline features\n\n### 1.2 Neural Network experiment\n\nI wanted to avoid building a binary return classifier, but still wanted the model to somehow predict a 0/1 action decision. The solution was to add a custom objective function that `tensorflow` could handle: \n\n$$ L = - \\sum_{i} a_{i}r_{i}w_{i}^{h} $$\n\n* $a_{i}$ is the output of a single final sigmoid activation node in the neural network\n* $r_{i}$ is the observed return `resp` for the trade $i$\n* $w_{i}^{h}$ is the `weight` of trade $i$, adjusted using a factor of $h \\in (0,1)$ to avoid over-emphasising the large-weight trades in training\n\nThis is effectively a profit maximisation function and, since it takes into account the magnitude of `resp`, it actually leads more to a regression-type optimization rather than classification (that's my intuition, anyway, I might be wrong). Anyway, I did a bit of prototyping to get a ballpark figure for all hyperparameters before moving on.\n\nPS: *Initially, I wanted to build this on top of the LightGBM return class predictions, but it turned out better to let the neural network work its magic on the original features. Plus, having more overhead in the final submission for a LightGBM model seemed quite unecessary.* \n\n## 2. Cross-Validation\n\nI was quite happy with the neural network approach described above, so it was time to scale it up and get some CV notebooks working around the clock! I ran these on Kaggle, because I really needed the extra computational power - thank you, Kaggle! :-)\n\nHere's a [sample](https://www.kaggle.com/slashie/jsmp-08-cv-final) of one of my CV notebooks. It's not very well commented (apologies!), so I'll give quick high-level summary of the approach:\n\n* I have a `__create_NN_model__()` function that takes a `NN_params` input dictionary to configure models with different hyperparameters, architecture etc.\n* For models with different `NN_params` and other settings, I ran CV on the same folds each time and recorded performance on each fold, before training a full model\n* The notebook outputs 3 files each time it runs:\n  * A CSV with the model's K-fold performance for each fold using different action thresholds\n  * A JSON file with the model weights\n  * A JSON file with the model's `NN_params` and other configuration settings\n\n## 3. Ensemble Selection\n\nAfter multiple CV runs, I collected a lot of K-fold performance data to help guide my selection of models for the ensemble. Here's the [notebook](https://github.com/robert-manolache/kaggle-jsmp/blob/main/ensemble_selection.ipynb) on GitHub where I came up with my final ensemble - it's not particularly scientific, but I was sprinting for the finish line and didn't have time to be precise. In the end I chose:\n* The top 6 models by average utility score across all folds\n* The top 7 models by ratio of average utility to utility standard deviation\n* The top model in each of the 5 folds\n* I also gave my 'favourite' model a couple of extra ensemble votes because ... why not? :-)\n\n## 4. Final Submission\n\nAnd so here we are ... this notebook! After a nightmare run of failed submissions during the public LB stage, I went to the extreme to make this notebook error-proof and as lightweight as possible: \n* It only imports `json`, `numpy` and `pandas` in addition to the necessary `janestreet` package\n* There's no training - I realised it was pointless to re-train since you'll never get any new data to update and re-optimize (as far as I understand, at least...)\n* It uses the model weights that were saved for each model during the CV stage\n  * These are sourced by the notebook from private datasets\n* I just use `numpy` to manually make the neural network matrix operations to derive predictions \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nimport json\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load models and ensemble weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create feature names and load ensemble config\nfeatures = [\"feature_\" + str(f) for f in range(130)]\ne_df = pd.read_csv(\"../input/jsmpensemble004/ensemble_weights.csv\")\n\n\n# iterate through each model in the ensemble\nwb = {} # for weights/biases\nnL = [] # number of layers\nfor m in e_df[\"model\"]:\n    \n    # load model weights file\n    with open(\"../input/jsmpensemble004/%s_model_weights.json\"%m) as rf:\n        params = json.load(rf)\n    \n    # convert weights/biases to numpy arrays\n    for k,v in params.items():\n        params[k] = np.array(v)\n    \n    # append model params and number of layers\n    wb[m] = params\n    nL.append(int(len(list(params.keys()))/2))\n\n# include number of layers in ensemble config df for easy iteration\ne_df.loc[:, \"n_layers\"] = nL\ne_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    # only consider trades that count towards score\n    if (test_df['weight'].values[0] > 0):\n        \n        # wrap in try/except to avoid failures\n        try:  \n            \n            # subset features and fill NAs with 0\n            X = test_df[features].fillna(0).values\n            \n            # get weight and assign decision threshold based on formula\n            w = test_df['weight'].values[0]\n            t = 0.5 + ((w/300) ** 1.5)\n            if t > 0.75:\n                t = 0.75\n            y = 0\n            \n            # iterate through ensemble models\n            for m, eW, nL in e_df.itertuples(index=False):\n            # m:  model\n            # eW: ensemble weight\n            # nL: number of NN layers\n                \n                # get the parameter set\n                params = wb[m]\n                \n                # iterate through each layer\n                for i in range(nL):\n                    \n                    # initialize first layer input\n                    if i == 0:\n                        a = X.copy()\n                    else:\n                        pass\n                    \n                    # compute linear output \n                    a = a @ params[\"w\"+str(i)] + params[\"b\"+str(i)]\n                    \n                    # default to relu unless it's the last layer - use sigmoid there\n                    if i < (nL-1):\n                        a = np.where(a < 0, 0, a)\n                    else:\n                        a = 1/(1+np.exp(-a))\n                \n                # increment action probability using model's assigned weight\n                y += (a[0][0] * eW)\n            \n            # have some fail-safes before predicting 0/1 action\n            if np.isnan(y):\n                action = 0\n            elif y > t:\n                action = 1\n            else:\n                action = 0\n                \n        except:           \n            action = 0\n            \n    else:\n        action = 0\n    \n    #to test it works\n    #print(\"Choose action %d with dtype %s\"%(action, type(action)))\n    \n    # submit action\n    sample_prediction_df[\"action\"] = action #make your 0/1 prediction here\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}