Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project version=\"4\">\n  <component name=\"ChangeListManager\">\n    <list default=\"true\" id=\"6f8ef791-9a28-495f-8d55-6451c12439c2\" name=\"Changes\" comment=\"Added pipeline detector for all steps\">\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/.~lock.API-dictionary.xlsx#\" beforeDir=\"false\" />\n      <change beforePath=\"$PROJECT_DIR$/requirements.txt\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/requirements.txt\" afterDir=\"false\" />\n    </list>\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\n  </component>\n  <component name=\"FileTemplateManagerImpl\">\n    <option name=\"RECENT_TEMPLATES\">\n      <list>\n        <option value=\"Python Unit Test\" />\n        <option value=\"Python Script\" />\n      </list>\n    </option>\n  </component>\n  <component name=\"Git.Settings\">\n    <option name=\"PUSH_TAGS\">\n      <GitPushTagMode>\n        <option name=\"argument\" value=\"--tags\" />\n        <option name=\"title\" value=\"All\" />\n      </GitPushTagMode>\n    </option>\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\n    <option name=\"SIGN_OFF_COMMIT\" value=\"true\" />\n  </component>\n  <component name=\"MarkdownSettingsMigration\">\n    <option name=\"stateVersion\" value=\"1\" />\n  </component>\n  <component name=\"ProjectId\" id=\"2Brad0s2Pn5V62tUPijrg2UMxWe\" />\n  <component name=\"ProjectLevelVcsManager\">\n    <ConfirmationsSetting value=\"2\" id=\"Add\" />\n  </component>\n  <component name=\"ProjectViewState\">\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\n    <option name=\"showLibraryContents\" value=\"true\" />\n  </component>\n  <component name=\"PropertiesComponent\">{\n  &quot;keyToString&quot;: {\n    &quot;ASKED_ADD_EXTERNAL_FILES&quot;: &quot;true&quot;,\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\n    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,\n    &quot;last_opened_file_path&quot;: &quot;/home/sajed/GitHub/Data-Science-Pipeline-Detector/dataset/house-prices-advanced-regression-techniques/Pedro Marcelino, PhD/comprehensive-data-exploration-with-python.ipynb&quot;,\n    &quot;settings.editor.selected.configurable&quot;: &quot;shared-indexes&quot;\n  }\n}</component>\n  <component name=\"RecentsManager\">\n    <key name=\"CopyFile.RECENT_KEYS\">\n      <recent name=\"$PROJECT_DIR$/test\" />\n    </key>\n  </component>\n  <component name=\"RunManager\" selected=\"Python.__init__\">\n    <list>\n      <item itemvalue=\"Python.__init__\" />\n      <item itemvalue=\"Python tests.Unittests in src\" />\n    </list>\n  </component>\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\n  <component name=\"TaskManager\">\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\n      <changelist id=\"6f8ef791-9a28-495f-8d55-6451c12439c2\" name=\"Changes\" comment=\"\" />\n      <created>1657666741696</created>\n      <option name=\"number\" value=\"Default\" />\n      <option name=\"presentableId\" value=\"Default\" />\n      <updated>1657666741696</updated>\n      <workItem from=\"1657666742883\" duration=\"34000\" />\n      <workItem from=\"1657666786772\" duration=\"15108000\" />\n      <workItem from=\"1657819385562\" duration=\"14857000\" />\n      <workItem from=\"1658091535902\" duration=\"8778000\" />\n    </task>\n    <task id=\"LOCAL-00001\" summary=\"Added Travis CI\">\n      <created>1657684623095</created>\n      <option name=\"number\" value=\"00001\" />\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1657684623095</updated>\n    </task>\n    <task id=\"LOCAL-00002\" summary=\"Changed structure\">\n      <created>1657688518775</created>\n      <option name=\"number\" value=\"00002\" />\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1657688518775</updated>\n    </task>\n    <task id=\"LOCAL-00003\" summary=\"XLSX reader added\">\n      <created>1657820464121</created>\n      <option name=\"number\" value=\"00003\" />\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1657820464121</updated>\n    </task>\n    <task id=\"LOCAL-00004\" summary=\"Added pipeline detector for all steps\">\n      <created>1658100606019</created>\n      <option name=\"number\" value=\"00004\" />\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\n      <option name=\"project\" value=\"LOCAL\" />\n      <updated>1658100606019</updated>\n    </task>\n    <option name=\"localTasksCounter\" value=\"5\" />\n    <servers />\n  </component>\n  <component name=\"TypeScriptGeneratedFilesManager\">\n    <option name=\"version\" value=\"3\" />\n  </component>\n  <component name=\"Vcs.Log.Tabs.Properties\">\n    <option name=\"TAB_STATES\">\n      <map>\n        <entry key=\"MAIN\">\n          <value>\n            <State />\n          </value>\n        </entry>\n      </map>\n    </option>\n  </component>\n  <component name=\"VcsManagerConfiguration\">\n    <option name=\"CHECK_CODE_CLEANUP_BEFORE_PROJECT_COMMIT\" value=\"true\" />\n    <option name=\"ADD_EXTERNAL_FILES_SILENTLY\" value=\"true\" />\n    <MESSAGE value=\"Added Travis CI\" />\n    <MESSAGE value=\"Changed structure\" />\n    <MESSAGE value=\"XLSX reader added\" />\n    <MESSAGE value=\"Added pipeline detector for all steps\" />\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"Added pipeline detector for all steps\" />\n    <option name=\"REFORMAT_BEFORE_PROJECT_COMMIT\" value=\"true\" />\n    <option name=\"REARRANGE_BEFORE_PROJECT_COMMIT\" value=\"true\" />\n  </component>\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$__init____1_.coverage\" NAME=\"__init__ (1) Coverage Results\" MODIFIED=\"1657667412960\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/test\" />\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$Unittests_in_test.coverage\" NAME=\"Unittests in tests Coverage Results\" MODIFIED=\"1657670101671\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/src\" />\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$Unittests_in_src.coverage\" NAME=\"Unittests in src Coverage Results\" MODIFIED=\"1657810834240\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"\" />\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$Nosetests_in_bin.coverage\" NAME=\"Nosetests in bin Coverage Results\" MODIFIED=\"1657670304492\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/tests\" />\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$pytest_in_bin.coverage\" NAME=\"pytest in bin Coverage Results\" MODIFIED=\"1657670349541\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$__init__.coverage\" NAME=\"__init__ Coverage Results\" MODIFIED=\"1658100333091\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/src\" />\n    <SUITE FILE_PATH=\"coverage/Data_Science_Pipeline_Detector$.coverage\" NAME=\" Coverage Results\" MODIFIED=\"1657679886778\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/src\" />\n  </component>\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/.idea/workspace.xml	(date 1658512100301)
@@ -1,10 +1,25 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="ChangeListManager">
-    <list default="true" id="6f8ef791-9a28-495f-8d55-6451c12439c2" name="Changes" comment="Added pipeline detector for all steps">
+    <list default="true" id="6f8ef791-9a28-495f-8d55-6451c12439c2" name="Changes" comment="Changed path in unittest">
+      <change afterPath="$PROJECT_DIR$/dataset/titanic/Masum Rumi/test.ipynb" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/res/.~lock.API-dictionary.xlsx#" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/.~lock.API-dictionary.xlsx#" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/requirements.txt" beforeDir="false" afterPath="$PROJECT_DIR$/requirements.txt" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/src/ipynb_pipeline_detector.py" beforeDir="false" afterPath="$PROJECT_DIR$/src/ipynb_pipeline_detector.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/src/temp.py" beforeDir="false" />
+      <change beforePath="$PROJECT_DIR$/src/test_ipynb_pipeline_detector.py" beforeDir="false" afterPath="$PROJECT_DIR$/src/test_ipynb_pipeline_detector.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -46,7 +61,7 @@
     &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,
     &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
     &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,
-    &quot;last_opened_file_path&quot;: &quot;/home/sajed/GitHub/Data-Science-Pipeline-Detector/dataset/house-prices-advanced-regression-techniques/Pedro Marcelino, PhD/comprehensive-data-exploration-with-python.ipynb&quot;,
+    &quot;last_opened_file_path&quot;: &quot;/home/sajed/GitHub/reference-model.ipynb&quot;,
     &quot;settings.editor.selected.configurable&quot;: &quot;shared-indexes&quot;
   }
 }</component>
@@ -55,11 +70,32 @@
       <recent name="$PROJECT_DIR$/test" />
     </key>
   </component>
-  <component name="RunManager" selected="Python.__init__">
+  <component name="RunManager" selected="Python tests.Python tests for test_ipynb_pipeline_detector.TestIpynbPipelineDetector.test_get_ast_notebook_file_3">
+    <configuration name="Python tests for test_ipynb_pipeline_detector.TestIpynbPipelineDetector.test_get_ast_notebook_file_3" type="tests" factoryName="Autodetect" temporary="true" nameIsGenerated="true">
+      <module name="Data-Science-Pipeline-Detector" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/src" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
+      <option name="_new_additionalArguments" value="&quot;&quot;" />
+      <option name="_new_target" value="&quot;test_ipynb_pipeline_detector.TestIpynbPipelineDetector.test_get_ast_notebook_file_3&quot;" />
+      <option name="_new_targetType" value="&quot;PYTHON&quot;" />
+      <method v="2" />
+    </configuration>
     <list>
       <item itemvalue="Python.__init__" />
       <item itemvalue="Python tests.Unittests in src" />
+      <item itemvalue="Python tests.Python tests for test_ipynb_pipeline_detector.TestIpynbPipelineDetector.test_get_ast_notebook_file_3" />
     </list>
+    <recent_temporary>
+      <list>
+        <item itemvalue="Python tests.Python tests for test_ipynb_pipeline_detector.TestIpynbPipelineDetector.test_get_ast_notebook_file_3" />
+      </list>
+    </recent_temporary>
   </component>
   <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="application-level" UseSingleDictionary="true" transferred="true" />
   <component name="TaskManager">
@@ -136,7 +172,7 @@
     <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$Unittests_in_src.coverage" NAME="Unittests in src Coverage Results" MODIFIED="1657810834240" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="" />
     <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$Nosetests_in_bin.coverage" NAME="Nosetests in bin Coverage Results" MODIFIED="1657670304492" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/tests" />
     <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$pytest_in_bin.coverage" NAME="pytest in bin Coverage Results" MODIFIED="1657670349541" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$__init__.coverage" NAME="__init__ Coverage Results" MODIFIED="1658100333091" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/src" />
-    <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$.coverage" NAME=" Coverage Results" MODIFIED="1657679886778" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/src" />
+    <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$__init__.coverage" NAME="__init__ Coverage Results" MODIFIED="1658446466033" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/src" />
+    <SUITE FILE_PATH="coverage/Data_Science_Pipeline_Detector$.coverage" NAME=" Coverage Results" MODIFIED="1658509809260" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/src" />
   </component>
 </project>
\ No newline at end of file
Index: dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat_minor\":4,\"nbformat\":4,\"cells\":[{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Motivation</span>\\n\\nThis notebook aims to provide animations for time-space congestion visualizations. The idea is to animate the congestion change during time for all the 12 locations and 65 roadways. For a detail EDA, please visit the [notebook](https://www.kaggle.com/sytuannguyen/tps-mar-2022-eda-model)\",\"metadata\":{\"_uuid\":\"0ad34bbd-e9af-4d34-b40c-fee10238b827\",\"_cell_guid\":\"1e04659d-b137-4cfb-a8ae-2e6cdcaa1a56\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nplt.rcParams['axes.facecolor'] = 'gray'\\n\\nimport seaborn as sns\\nfrom matplotlib import animation\\nfrom IPython.display import HTML\\n\\nimport warnings\\nwarnings.simplefilter('ignore')\",\"metadata\":{\"_uuid\":\"b3a0feec-b03d-46a9-9716-87cbcab3e9c0\",\"_cell_guid\":\"ab5b2a97-00fa-4610-947b-f15aff0f0b8b\",\"collapsed\":false,\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2022-03-09T19:59:01.77303Z\",\"iopub.execute_input\":\"2022-03-09T19:59:01.773373Z\",\"iopub.status.idle\":\"2022-03-09T19:59:01.803049Z\",\"shell.execute_reply.started\":\"2022-03-09T19:59:01.773277Z\",\"shell.execute_reply\":\"2022-03-09T19:59:01.802224Z\"},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"train = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv', index_col='row_id')\\ntrain.time = pd.to_datetime(train.time)\\ntrain['dailytime_id'] = ( ( train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\\ntrain['time_id'] = ( ( (train.time.dt.dayofyear-91)*24*60 + train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\",\"metadata\":{\"_uuid\":\"f4843922-3963-4234-b4e0-76a1fe5d2b90\",\"_cell_guid\":\"4f11ed48-a3a4-4506-aefc-668aaa99ab9d\",\"collapsed\":false,\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2022-03-09T19:59:01.804787Z\",\"iopub.execute_input\":\"2022-03-09T19:59:01.805295Z\",\"iopub.status.idle\":\"2022-03-09T19:59:03.20001Z\",\"shell.execute_reply.started\":\"2022-03-09T19:59:01.805251Z\",\"shell.execute_reply\":\"2022-03-09T19:59:03.199037Z\"},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"# Map directions to vectors\\ntrain['dir_xy'] = train.direction.map({'EB':'[1,0]', 'NB':'[0,1]', 'SB':'[0,-1]', 'WB':'[-1,0]', 'NE':'[1,1]', 'SW':'[-1,-1]', 'NW':'[-1,1]', 'SE':'[1,-1]'})\\n\\nloc_dir = train.groupby(['x','y']).dir_xy.unique().reset_index()\\nloc_dir['num_dir'] = loc_dir.dir_xy.apply(lambda x: len(x))\",\"metadata\":{\"_uuid\":\"68329b8f-f4d1-4a6b-9bbf-f3534a4eccfa\",\"_cell_guid\":\"64bd633d-6c75-4232-8015-e3aa9bff7182\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2022-03-09T19:59:03.201127Z\",\"iopub.execute_input\":\"2022-03-09T19:59:03.201817Z\",\"iopub.status.idle\":\"2022-03-09T19:59:03.503898Z\",\"shell.execute_reply.started\":\"2022-03-09T19:59:03.201786Z\",\"shell.execute_reply\":\"2022-03-09T19:59:03.503153Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Data repartition for each roadway</span>\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"# create a categorical feature for distinguishing the 65 roadways\\ntrain['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\\n\\n# create a color column for the scatter plot: white for data close to the mean of each instant, \\n# black for data outside the range (mean-std) to (mean+std)\\ntrain['color'] = 'white'\\ncolor=[]\\nfor roadway in train.roadway.unique():\\n    df = train[train.roadway==roadway]\\n    df['color'] = 'white'\\n    for dailytime_id in df.dailytime_id.unique():\\n        congestion = df.congestion[df.dailytime_id==dailytime_id]\\n        mean = np.mean(congestion)\\n        std = np.std(congestion)\\n        cond = abs(congestion-mean)<std\\n        df.color[df.dailytime_id==dailytime_id]=cond.map({False:'black', True:'white'}).tolist()\\n    train.color[train.roadway==roadway] = df.color\\n\\n# highlight the last day 30 Sep 1991\\ntrain.color[train.time.dt.dayofyear==273] = 'red'\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"The scatter plot below clearly shows the position of the last day morning congestion comparing to data of all the previous days for the first roadway (0_0_EB). The red points are for the last day morning (30 Sep), the white zone is for the range between (mean-std) to (mean+std) where mean and std are computed for each instant.  \",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"fig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(0,100)\\nplt.xlim(-1,73)\\nplt.xlabel('Time (x20 minutes)', fontsize=16)\\nplt.ylabel('Congestion', fontsize=16)\\n\\ndf = train[train.roadway=='0_0_EB']\\nsct = plt.scatter(df.dailytime_id, df.congestion)\\nsct.set_color(df.color)\\nax.set_title(f'Roadway 0_0_EB', fontsize=16)\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(0,100)\\nplt.xlim(-1,73)\\nplt.xlabel('Time (x20 minutes)', fontsize=16)\\nplt.ylabel('Congestion', fontsize=16)\\n\\nsct = plt.scatter(range(72),[0]*72, color=['white'])\\n\\ndef update(idx, sct, roadways):\\n    roadway = roadways[idx]\\n    df = train[train.roadway==roadway]\\n\\n    sct.set_offsets(np.array([df.dailytime_id, df.congestion]).T)\\n    sct.set_color(df.color)\\n    ax.set_title(f'Roadway {roadway}', fontsize=16)\\n\\n    return sct\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(sct, train.roadway.unique()),\\n                               interval=300, frames=train.roadway.nunique(), blit=False)\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"The white zone is in the range *(mean-std)* to *(mean+std)* where *mean* and *std* are computed for each roadway at each instant.\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Data flow over time</span>\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(0,100)\\nplt.xlabel('Time', fontsize=16)\\nplt.ylabel('Mean congestion', fontsize=16)\\nplt.xticks([])\\n\\ndailytime_ids = range(72)\\nbars = plt.bar(dailytime_ids, [0]*len(dailytime_ids), color='white')\\n\\n\\ndef update(idx):\\n    df = train[(train.time_id>=idx) & (train.time_id<(idx+72))]\\n    for idx, dailytime_id in enumerate(df.dailytime_id.unique()):\\n        congestion = df.congestion[df.dailytime_id==dailytime_id].mean()\\n        bars[idx].set_height(congestion)\\n        bars[idx].set_color('white')\\n        \\n    ax.set_title(f'Congestion flow over time ({df.time.dt.day.iloc[0]} {df.time.dt.month_name().iloc[0]})', fontsize=16)\\n\\ndaily_ani = animation.FuncAnimation(fig, update,\\n                               interval=100, frames=1000, blit=False)\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"Note that you can slow down or speed up the animation by using the buttons - or +\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Daily congestion animation for all the days of year</span>\",\"metadata\":{\"_uuid\":\"a0d5b2ff-9cec-45f6-a615-4fd270d58ea7\",\"_cell_guid\":\"f33ac29f-c694-4a6d-9086-3e43d645c323\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(30,60)\\nplt.xlabel('Day of year', fontsize=16)\\nplt.ylabel('Daily congestion', fontsize=16)\\n\\ndayofyear = train.time.dt.dayofyear.unique()\\nbars = plt.bar(dayofyear, [0]*len(dayofyear), color='white')\\n\\ndef update(dailytime_id, bars, dummy):\\n    df = train[train.dailytime_id==dailytime_id]\\n    \\n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean()\\n        bars[idx].set_height(congestion)\\n\\n    ax.set_title(f'Average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n\\n    return bars\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)\",\"metadata\":{\"_uuid\":\"b6aaeff0-1971-4d4d-b3c5-52d84ab6bacc\",\"_cell_guid\":\"464d71d1-0d41-4e37-bd66-5f3de641cdf7\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"2870ba36-31cb-4140-9a54-f50b16b51eee\",\"_cell_guid\":\"105fc1a6-bb1a-459c-b325-ef4117e0d266\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(-10,10)\\nplt.xlabel('Day of year', fontsize=16)\\nplt.ylabel('Daily congestion', fontsize=16)\\n\\ndayofyear = train.time.dt.dayofyear.unique()\\nbars = plt.bar(dayofyear, [0]*len(dayofyear), color='white')\\n\\ndef update(dailytime_id, bars, dummy):\\n    df = train[train.dailytime_id==dailytime_id]\\n    \\n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean() - df.congestion.median()\\n        bars[idx].set_height(congestion)\\n        if congestion<0:\\n            bars[idx].set_color('black')\\n        else:\\n            bars[idx].set_color('white')\\n        \\n    ax.set_title(f'Deviation from daily average (median of all days) at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n\\n    return bars\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\\n                               interval=200, frames=train.dailytime_id.nunique(), blit=False)\",\"metadata\":{\"_uuid\":\"5f6ed49e-4949-4a32-a3a9-41df35a9b677\",\"_cell_guid\":\"d0d348e7-43d6-4f3e-afcb-171f19d97e4a\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"45eff1f6-1920-4fae-bbb5-617f230e3ed2\",\"_cell_guid\":\"45d9d2fb-9955-487d-a6a6-633c09f69aa4\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(-10,10)\\nplt.xlabel('Daily time index', fontsize=16)\\nplt.ylabel('Congestion deviation', fontsize=16)\\n\\ndailytime_ids = train.dailytime_id.unique()\\nbars = plt.bar(dailytime_ids, [0]*len(dailytime_ids), color='white')\\n\\n\\ndef update(idx, bars, dayofyears):\\n    dayofyear = dayofyears[idx]\\n    \\n    median = train.groupby(train.dailytime_id).congestion.median().round().astype(int).tolist()\\n    \\n    df = train[train.time.dt.dayofyear==dayofyear]\\n    for idx, dailytime_id in enumerate(df.dailytime_id.unique()):\\n        congestion = df.congestion[df.dailytime_id==dailytime_id].mean() - median[dailytime_id]\\n        bars[idx].set_height(congestion)\\n        if congestion<0:\\n            bars[idx].set_color('black')\\n        else:\\n            bars[idx].set_color('white')\\n        \\n    ax.set_title(f'Deviation from daily average (median of all days) of the day {dayofyear} of year', fontsize=16)\\n\\n    return bars\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.time.dt.dayofyear.unique()),\\n                               interval=500, frames=train.time.dt.dayofyear.nunique(), blit=False)\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Animate histograms of the roadways</span>\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"train['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-03-09T19:59:12.512305Z\",\"iopub.execute_input\":\"2022-03-09T19:59:12.512622Z\",\"iopub.status.idle\":\"2022-03-09T19:59:14.839623Z\",\"shell.execute_reply.started\":\"2022-03-09T19:59:12.512592Z\",\"shell.execute_reply\":\"2022-03-09T19:59:14.838855Z\"},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(10,7))\\n\\nplt.xlabel('Congestion', fontsize=16)\\nplt.ylabel('Count', fontsize=16)\\n_,_,hist = ax.hist(train.congestion, 100, color='white')\\n\\ndef update(idx, hist, roadways):\\n    df = train.congestion[train.roadway==roadways[idx]]\\n    n,_ = np.histogram(df, 100)\\n    for count, rect in zip(n, hist):\\n        rect.set_height(count)\\n\\n    ax.set_ylim(0,np.max(n))\\n    ax.set_title(f'Roadway: {roadways[idx]}', fontsize=16)\\n    return hist\\n\\nani = animation.FuncAnimation(fig, update, fargs=(hist, train.roadway.unique()),\\n                               interval=500, frames=train.roadway.nunique(), blit=False)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-03-09T19:59:40.0917Z\",\"iopub.execute_input\":\"2022-03-09T19:59:40.091999Z\",\"iopub.status.idle\":\"2022-03-09T19:59:40.797935Z\",\"shell.execute_reply.started\":\"2022-03-09T19:59:40.091968Z\",\"shell.execute_reply\":\"2022-03-09T19:59:40.796993Z\"},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(ani.to_jshtml())\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-03-09T19:59:41.820318Z\",\"iopub.execute_input\":\"2022-03-09T19:59:41.820804Z\",\"iopub.status.idle\":\"2022-03-09T19:59:57.693209Z\",\"shell.execute_reply.started\":\"2022-03-09T19:59:41.820766Z\",\"shell.execute_reply\":\"2022-03-09T19:59:57.692243Z\"},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Daily correlation between the 12 locations</span>\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"%%capture\\ndef animate(hour):\\n    dfs = []\\n    columns = []\\n    for x in range(3):\\n        for y in range(4):\\n            df = train[(train.x == x) & (train.y==y) & (train.time.dt.hour==hour) & (train.time.dt.minute==0)]\\n            dfs.append(df.groupby('time').congestion.mean().tolist())\\n            columns.append(f'x{x}y{y}')      \\n    location_congestions = pd.DataFrame(np.array(dfs).T, columns=columns)\\n\\n    ax.cla()\\n    sns.heatmap(ax = ax, data = location_congestions.corr(), annot=True, cbar_ax = cbar_ax)\\n    ax.set_title(f'Correlation between the locations at {hour}h00', fontsize=16)\\n\\ngrid_kws = {'width_ratios': (0.9, 0.05), 'wspace': 0.2}\\nfig, (ax, cbar_ax) = plt.subplots(1, 2, gridspec_kw = grid_kws, figsize = (10, 8))\\nani = animation.FuncAnimation(fig = fig, func = animate, frames = train.time.dt.hour.nunique(), interval = 500)\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(ani.to_jshtml())\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Daily congestion animation for the Monday of year</span>\",\"metadata\":{\"_uuid\":\"b298c1bd-21c3-431c-aa3f-537cb9740d4a\",\"_cell_guid\":\"fca95dac-fc17-42cb-945a-8713b553d4c6\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(30,60)\\nplt.xlabel('Day of year', fontsize=16)\\nplt.ylabel('Daily congestion', fontsize=16)\\n\\nmondayofyear = train[train.time.dt.weekday==0].time.dt.dayofyear.unique()\\nbars = plt.bar(mondayofyear, [0]*len(mondayofyear), color='white')\\n\\ndef update(dailytime_id, bars, dummy):\\n    df = train[(train.time.dt.weekday==0) & (train.dailytime_id==dailytime_id)]\\n    \\n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean()\\n        bars[idx].set_height(congestion)\\n\\n    ax.set_title(f'Average Monday congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n\\n    return bars\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)\",\"metadata\":{\"_uuid\":\"451570fa-a30d-4e4f-9214-4c8718342ad5\",\"_cell_guid\":\"35d8e36d-9730-4ff3-a7bf-4cc5b2dc2f32\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"bf2563b7-5aba-42e0-8570-48cfa3619f82\",\"_cell_guid\":\"97580e56-b7b3-4a84-aa35-32ed7d4966f8\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Average daily congestion for the weekdays</span>\",\"metadata\":{\"_uuid\":\"71bde277-1ed6-49cf-8607-f22674a68e52\",\"_cell_guid\":\"8d3d34c4-1c7f-468b-aca8-01e21d4fff3e\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig, ax = plt.subplots(figsize=(15, 5))\\nplt.ylim(30,60)\\nax.set_xticklabels(['', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], fontsize=16)\\nplt.ylabel('Daily congestion', fontsize=16)\\n\\nweekday = train.time.dt.weekday.unique()\\nbars = plt.bar(weekday, [0]*len(weekday), color='white')\\n\\ndef update(dailytime_id, bars, dummy):\\n    df = train[train.dailytime_id==dailytime_id]\\n    \\n    for idx, weekday in enumerate(df.time.dt.weekday.unique()):\\n        congestion = df.congestion[df.time.dt.weekday==weekday].mean()\\n        bars[idx].set_height(congestion)\\n\\n    ax.set_title(f'Average week days congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n\\n    return bars\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)\",\"metadata\":{\"_uuid\":\"078dc525-b690-4b04-9026-a6f8caa85153\",\"_cell_guid\":\"d3e47c78-ee35-493f-87fd-b5b606f4b43f\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"58e7ea29-f7e9-4c6b-afde-780dc11b8e84\",\"_cell_guid\":\"aaa9a5b5-5274-46bd-b151-8aad60d36a46\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Average daily congestion for all the 12 locations and 65 roadways</span>\",\"metadata\":{\"_uuid\":\"2567aa09-8844-4e75-abf9-bf1816162973\",\"_cell_guid\":\"aaab56af-f2d1-41b4-9c57-0c88c2b56c7b\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig = plt.figure(figsize=(16,12))\\n\\n# Plot the 12 locations\\nax1 = plt.subplot(1,2,1)\\nplt.xlim(-0.5,2.5)\\nplt.ylim(-0.5,3.5)\\nplt.xticks([])\\nplt.yticks([])\\n\\ntheta = np.arange(0,2.01*np.pi,0.01*np.pi)\\nr=0.1\\n\\nlines = []\\nfor ox in range(3):\\n    for oy in range(4):\\n        x = ox + r*np.sin(theta)\\n        y = oy + r*np.cos(theta)\\n        line, = ax1.plot(x,y, 'white', linewidth=10)\\n        lines.append(line)\\n\\n# Plot the 65 roadways\\norigins=[]\\nfor idx, row in loc_dir.iterrows():\\n    origin = np.repeat(np.array([[row.x],[row.y]]),row.num_dir, axis=-1)\\n    origins.append(origin)\\n    \\norigin = np.concatenate(origins, axis=1)\\n\\nax2 = plt.subplot(1,2,2)\\nplt.xlim(-0.5,2.5)\\nplt.ylim(-0.5,3.5)\\nplt.xticks([])\\nplt.yticks([])\\nQ = ax2.quiver(*origin, [0]*origin.shape[1], [0]*origin.shape[1], scale=1, color='white')\\n\\ndef update(dailytime_id, lines, Q, dummy):\\n    # update locations\\n    theta = np.arange(0,2.01*np.pi,0.01*np.pi)\\n    colors = ['silver', 'gainsboro', 'white']\\n    idx=0\\n    for ox in range(3):\\n        for oy in range(4):\\n            congestion = train.congestion[(train.x==ox) & (train.y==oy) & (train.dailytime_id==dailytime_id)].mean()\\n            r = congestion * 0.5/100\\n            x = ox + r*np.sin(theta)\\n            y = oy + r*np.cos(theta)\\n            \\n            \\n            lines[idx].set_data(x,y)\\n            lines[idx].set_color(colors[int(congestion//33)])\\n            idx+=1\\n            \\n    ax1.set_title(f'Location average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n    \\n    # update roadways\\n    directions = []\\n\\n    for idx, row in loc_dir.iterrows():\\n        df = train[(train.x==row.x) & (train.y==row.y) & (train.dailytime_id==dailytime_id)]\\n        direction=[]\\n        for d in row.dir_xy:\\n            \\n            congestion_d = df.congestion[df.dir_xy==d].mean()\\n            direction.append(np.array(eval(d)) * congestion_d/500)\\n        \\n        directions.append(direction)\\n        \\n    direction = np.concatenate(directions, axis=0)\\n\\n    Q.set_UVC(direction[:,0], direction[:,1])\\n    ax2.set_title(f'Roadway average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n\\n    return lines, Q\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(lines, Q, train.dailytime_id.unique()),\\n                               interval=300, frames=train.dailytime_id.nunique(), blit=False)\",\"metadata\":{\"_uuid\":\"b6148664-4033-454f-851e-a906680f9074\",\"_cell_guid\":\"0c6ff1ba-060d-410b-aa5f-6acc44f58f84\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"d759a992-9c8d-4e13-85c4-e022ce8379fc\",\"_cell_guid\":\"e3983667-8491-46bc-b4f9-1074197b54e5\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Average daily congestion for the 8 directions</span>\",\"metadata\":{\"_uuid\":\"13e66b47-3a3b-4485-9036-deed04754c36\",\"_cell_guid\":\"c526cf6a-64cb-41b5-9e50-71721e583fbc\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nfig = plt.figure(figsize=(10,10))\\nax = plt.subplot(projection='polar')\\nplt.ylim(0,60)\\nax.set_xticklabels(['EB', 'NE', 'NB', 'NW', 'WB', 'SW', 'SB', 'SE'], fontsize=16)\\n\\nangles = np.linspace(0, 2 * np.pi, 9)\\nbars = plt.bar(angles[:8], [1]*8, width=np.pi / 8, color='white')\\n\\ndef update(dailytime_id, bars, dummy):\\n    df = train[train.dailytime_id==dailytime_id]\\n    \\n    for idx, direction in enumerate(df.direction.unique()):\\n        congestion = df.congestion[df.direction==direction].mean()\\n        bars[idx].set_height(congestion)\\n\\n    ax.set_title(f'Average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\\n\\n    return bars\\n\\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)\",\"metadata\":{\"_uuid\":\"c3fbc110-be82-477a-be55-d1b0055738d2\",\"_cell_guid\":\"4d600f6c-d17c-4b5f-a22d-1a8f7d088d03\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(daily_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"9d73e61b-31a9-46ca-b10a-f8a44bd32d77\",\"_cell_guid\":\"7fb8da45-345e-4b77-a4b7-1e462e3c19ca\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"# This function returns an animation for a weekday\\ndef weekday_ani(weekday):\\n    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\\n    fig = plt.figure(figsize=(16,12))\\n\\n    # Plot the 12 locations\\n    ax1 = plt.subplot(1,2,1)\\n    plt.xlim(-0.5,2.5)\\n    plt.ylim(-0.5,3.5)\\n    plt.xticks([])\\n    plt.yticks([])\\n\\n    theta = np.arange(0,2.01*np.pi,0.01*np.pi)\\n    r=0.1\\n\\n    lines = []\\n    for ox in range(3):\\n        for oy in range(4):\\n            x = ox + r*np.sin(theta)\\n            y = oy + r*np.cos(theta)\\n            line, = ax1.plot(x,y, 'white', linewidth=10)\\n            lines.append(line)\\n\\n    # Plot the 65 roadways\\n    origins=[]\\n    for idx, row in loc_dir.iterrows():\\n        origin = np.repeat(np.array([[row.x],[row.y]]),row.num_dir, axis=-1)\\n        origins.append(origin)\\n\\n    origin = np.concatenate(origins, axis=1)\\n\\n    ax2 = plt.subplot(1,2,2)\\n    plt.xlim(-0.5,2.5)\\n    plt.ylim(-0.5,3.5)\\n    plt.xticks([])\\n    plt.yticks([])\\n    Q = ax2.quiver(*origin, [0]*origin.shape[1], [0]*origin.shape[1], scale=1, color='white')\\n\\n    def update(dailytime_id, lines, Q, dummy):\\n        # update locations\\n        theta = np.arange(0,2.01*np.pi,0.01*np.pi)\\n        colors = ['silver', 'gainsboro', 'white']\\n        idx=0\\n        for ox in range(3):\\n            for oy in range(4):\\n                congestion = train.congestion[(train.time.dt.weekday==weekday) & (train.x==ox) & (train.y==oy) & (train.dailytime_id==dailytime_id)].mean()\\n                r = congestion * 0.5/100\\n                x = ox + r*np.sin(theta)\\n                y = oy + r*np.cos(theta)\\n\\n\\n                lines[idx].set_data(x,y)\\n                lines[idx].set_color(colors[int(congestion//33)])\\n                idx+=1\\n\\n        ax1.set_title(f'Average {weekdays[weekday]} congestion at {dailytime_id//3}h{dailytime_id%3*20} for all locations', fontsize=16)\\n\\n        # update roadways\\n        directions = []\\n\\n        for idx, row in loc_dir.iterrows():\\n            df = train[(train.time.dt.weekday==weekday) & (train.x==row.x) & (train.y==row.y) & (train.dailytime_id==dailytime_id)]\\n            direction=[]\\n            for d in row.dir_xy:\\n\\n                congestion_d = df.congestion[df.dir_xy==d].mean()\\n                direction.append(np.array(eval(d)) * congestion_d/500)\\n\\n            directions.append(direction)\\n\\n        direction = np.concatenate(directions, axis=0)\\n\\n        Q.set_UVC(direction[:,0], direction[:,1])\\n        ax2.set_title(f'Average {weekdays[weekday]} congestion at {dailytime_id//3}h{dailytime_id%3*20} for all roadways', fontsize=16)\\n\\n        return lines, Q\\n\\n    daily_ani = animation.FuncAnimation(fig, update, fargs=(lines, Q, train.dailytime_id.unique()),\\n                                   interval=100, frames=train.dailytime_id.nunique(), blit=False)\\n    \\n    return daily_ani\",\"metadata\":{\"_uuid\":\"8178d664-4d0d-487d-bdd0-299e0f1a8b62\",\"_cell_guid\":\"2e433630-22f9-49f6-95bf-5636ec588540\",\"collapsed\":false,\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2022-03-07T21:14:04.775973Z\",\"iopub.execute_input\":\"2022-03-07T21:14:04.776391Z\",\"iopub.status.idle\":\"2022-03-07T21:14:04.792815Z\",\"shell.execute_reply.started\":\"2022-03-07T21:14:04.776351Z\",\"shell.execute_reply\":\"2022-03-07T21:14:04.792014Z\"},\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Monday congestion</span>\",\"metadata\":{\"_uuid\":\"65f7420a-e2e6-4d02-91da-09076041f404\",\"_cell_guid\":\"33bba666-c7fc-4bbf-b8e3-ee5eec864215\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"%%capture\\nMonday_ani = weekday_ani(0)\",\"metadata\":{\"_uuid\":\"854fe2a1-cb51-4929-8b17-1de1ceaf9200\",\"_cell_guid\":\"7e650180-7c8a-48e8-963f-3309508c0613\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(Monday_ani.to_jshtml())\",\"metadata\":{\"_uuid\":\"26ce9d19-f8de-4495-a75f-fab70a960216\",\"_cell_guid\":\"9a4a361f-104f-47b0-9251-704628a3a401\",\"collapsed\":false,\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2022-03-07T21:16:55.672922Z\",\"iopub.execute_input\":\"2022-03-07T21:16:55.673213Z\",\"iopub.status.idle\":\"2022-03-07T21:18:26.233831Z\",\"shell.execute_reply.started\":\"2022-03-07T21:16:55.673183Z\",\"shell.execute_reply\":\"2022-03-07T21:18:26.232997Z\"},\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Daily congestion of each roadway</span>\",\"metadata\":{\"_uuid\":\"a57359b4-477f-4a7b-9500-43c95658d9c9\",\"_cell_guid\":\"994e73ab-c561-4d7e-a6e0-9709582ac49a\",\"trusted\":true}},{\"cell_type\":\"code\",\"source\":\"def roadway_daily_congestion(x, y, direction):\\n    fig, ax = plt.subplots(figsize=(15, 5))\\n    plt.ylim(0,100)\\n    plt.xlim(-1,73)\\n    plt.xlabel('Time (x20 minutes)', fontsize=16)\\n    plt.ylabel('Congestion', fontsize=16)\\n\\n    sct = plt.scatter(range(72),[0]*72, color=['black']*36 + ['white']*36)\\n\\n    def update(idx, sct, dayofyears):\\n        dayofyear = dayofyears[idx]\\n        df = train[(train.time.dt.dayofyear==dayofyear) & (train.x==x) & (train.y==y) & (train.direction==direction)]\\n\\n        sct.set_offsets(np.array([df.dailytime_id, df.congestion]).T)\\n\\n        ax.set_title(f'Roadway {x}_{y}_{direction} on {df.time.dt.day.unique()[0]} {df.time.dt.month_name().unique()[0]}', fontsize=16)\\n\\n        return sct\\n\\n    daily_ani = animation.FuncAnimation(fig, update, fargs=(sct, train.time.dt.dayofyear.unique()),\\n                                   interval=300, frames=train.time.dt.dayofyear.nunique(), blit=False)\\n    return daily_ani\",\"metadata\":{\"_uuid\":\"a5908722-c0a8-4597-bb72-fc1bb0c30d93\",\"_cell_guid\":\"7e025a6d-5b7c-4477-a379-cd94c23ab66a\",\"collapsed\":false,\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"%%capture\\nanis =[]\\nfor x in range(3):\\n    for y in range(4):\\n        for direction in train.direction.unique():\\n            if train[(train.x==x) & (train.y==y) & (train.direction==direction)].shape[0]>0:\\n                ani = roadway_daily_congestion(x, y, direction)\\n                anis.append(ani)\",\"metadata\":{\"_uuid\":\"6a969143-635f-46b7-8492-d7db73671be3\",\"_cell_guid\":\"dacb6b82-dcf3-46e2-812a-cd96cfc3063e\",\"collapsed\":false,\"jupyter\":{\"outputs_hidden\":false},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"HTML(anis[0].to_jshtml())\",\"metadata\":{\"_uuid\":\"fe8224a1-e608-43ad-8f4a-88283a8ea5cf\",\"_cell_guid\":\"23ae6b4f-7c0a-4fbe-8bcf-d8134f7deaf1\",\"collapsed\":false,\"jupyter\":{\"outputs_hidden\":false},\"_kg_hide-input\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# <span style='color:#A80808'>Bonnus: simple baseline without ML that outperforms top ML models</span>\\n\\nIt is said that: \\\"Don't jump too soon into the water!!!\\\"\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"train = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv')\\ntrain.time = pd.to_datetime(train.time)\\ntrain['daytime_id'] = ( ( train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\\ntrain = train.set_index('row_id', drop=True)\\ntrain['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\\n\\ntest = pd.read_csv('../input/tabular-playground-series-mar-2022/test.csv', index_col='row_id')\\ntest.time = pd.to_datetime(test.time)\\ntest['roadway'] = test.x.astype('str') +'_'+ test.y.astype('str') +'_'+ test.direction.astype('str') \\nsubmission = pd.read_csv('../input/tabular-playground-series-mar-2022/sample_submission.csv')\",\"metadata\":{\"_kg_hide-input\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"test['median_'] = -1\\n\\nfor roadway in train.roadway.unique():\\n    # extract data for each roadway\\n    df = train[train.roadway==roadway]\\n\\n    if df.shape[0]>0:                \\n        test.median_[test.roadway==roadway] = df.groupby(df.daytime_id).congestion.median().tolist()[-36:]\\n\\ntest.median_[(test.roadway=='2_2_SE') & (test.time.dt.hour<15)] = 20\\ntest['median_'] = test['median_'].round().astype(int).tolist()\",\"metadata\":{\"_kg_hide-input\":false},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"submission.congestion = test.median_.tolist()\\nsubmission.to_csv('submission.csv', index=False)\\nsubmission\",\"metadata\":{},\"execution_count\":null,\"outputs\":[]}]}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb b/dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb
--- a/dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/tabular-playground-series-mar-2022/Sy-Tuan Nguyen/tps-mar-2022-animation-new-baseline.ipynb	(date 1658512097817)
@@ -1,1 +1,674 @@
-{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <span style='color:#A80808'>Motivation</span>\n\nThis notebook aims to provide animations for time-space congestion visualizations. The idea is to animate the congestion change during time for all the 12 locations and 65 roadways. For a detail EDA, please visit the [notebook](https://www.kaggle.com/sytuannguyen/tps-mar-2022-eda-model)","metadata":{"_uuid":"0ad34bbd-e9af-4d34-b40c-fee10238b827","_cell_guid":"1e04659d-b137-4cfb-a8ae-2e6cdcaa1a56","trusted":true}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.facecolor'] = 'gray'\n\nimport seaborn as sns\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"b3a0feec-b03d-46a9-9716-87cbcab3e9c0","_cell_guid":"ab5b2a97-00fa-4610-947b-f15aff0f0b8b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-09T19:59:01.77303Z","iopub.execute_input":"2022-03-09T19:59:01.773373Z","iopub.status.idle":"2022-03-09T19:59:01.803049Z","shell.execute_reply.started":"2022-03-09T19:59:01.773277Z","shell.execute_reply":"2022-03-09T19:59:01.802224Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv', index_col='row_id')\ntrain.time = pd.to_datetime(train.time)\ntrain['dailytime_id'] = ( ( train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\ntrain['time_id'] = ( ( (train.time.dt.dayofyear-91)*24*60 + train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)","metadata":{"_uuid":"f4843922-3963-4234-b4e0-76a1fe5d2b90","_cell_guid":"4f11ed48-a3a4-4506-aefc-668aaa99ab9d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-09T19:59:01.804787Z","iopub.execute_input":"2022-03-09T19:59:01.805295Z","iopub.status.idle":"2022-03-09T19:59:03.20001Z","shell.execute_reply.started":"2022-03-09T19:59:01.805251Z","shell.execute_reply":"2022-03-09T19:59:03.199037Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map directions to vectors\ntrain['dir_xy'] = train.direction.map({'EB':'[1,0]', 'NB':'[0,1]', 'SB':'[0,-1]', 'WB':'[-1,0]', 'NE':'[1,1]', 'SW':'[-1,-1]', 'NW':'[-1,1]', 'SE':'[1,-1]'})\n\nloc_dir = train.groupby(['x','y']).dir_xy.unique().reset_index()\nloc_dir['num_dir'] = loc_dir.dir_xy.apply(lambda x: len(x))","metadata":{"_uuid":"68329b8f-f4d1-4a6b-9bbf-f3534a4eccfa","_cell_guid":"64bd633d-6c75-4232-8015-e3aa9bff7182","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-09T19:59:03.201127Z","iopub.execute_input":"2022-03-09T19:59:03.201817Z","iopub.status.idle":"2022-03-09T19:59:03.503898Z","shell.execute_reply.started":"2022-03-09T19:59:03.201786Z","shell.execute_reply":"2022-03-09T19:59:03.503153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Data repartition for each roadway</span>","metadata":{}},{"cell_type":"code","source":"# create a categorical feature for distinguishing the 65 roadways\ntrain['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\n\n# create a color column for the scatter plot: white for data close to the mean of each instant, \n# black for data outside the range (mean-std) to (mean+std)\ntrain['color'] = 'white'\ncolor=[]\nfor roadway in train.roadway.unique():\n    df = train[train.roadway==roadway]\n    df['color'] = 'white'\n    for dailytime_id in df.dailytime_id.unique():\n        congestion = df.congestion[df.dailytime_id==dailytime_id]\n        mean = np.mean(congestion)\n        std = np.std(congestion)\n        cond = abs(congestion-mean)<std\n        df.color[df.dailytime_id==dailytime_id]=cond.map({False:'black', True:'white'}).tolist()\n    train.color[train.roadway==roadway] = df.color\n\n# highlight the last day 30 Sep 1991\ntrain.color[train.time.dt.dayofyear==273] = 'red'","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scatter plot below clearly shows the position of the last day morning congestion comparing to data of all the previous days for the first roadway (0_0_EB). The red points are for the last day morning (30 Sep), the white zone is for the range between (mean-std) to (mean+std) where mean and std are computed for each instant.  ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(0,100)\nplt.xlim(-1,73)\nplt.xlabel('Time (x20 minutes)', fontsize=16)\nplt.ylabel('Congestion', fontsize=16)\n\ndf = train[train.roadway=='0_0_EB']\nsct = plt.scatter(df.dailytime_id, df.congestion)\nsct.set_color(df.color)\nax.set_title(f'Roadway 0_0_EB', fontsize=16)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(0,100)\nplt.xlim(-1,73)\nplt.xlabel('Time (x20 minutes)', fontsize=16)\nplt.ylabel('Congestion', fontsize=16)\n\nsct = plt.scatter(range(72),[0]*72, color=['white'])\n\ndef update(idx, sct, roadways):\n    roadway = roadways[idx]\n    df = train[train.roadway==roadway]\n\n    sct.set_offsets(np.array([df.dailytime_id, df.congestion]).T)\n    sct.set_color(df.color)\n    ax.set_title(f'Roadway {roadway}', fontsize=16)\n\n    return sct\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(sct, train.roadway.unique()),\n                               interval=300, frames=train.roadway.nunique(), blit=False)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The white zone is in the range *(mean-std)* to *(mean+std)* where *mean* and *std* are computed for each roadway at each instant.","metadata":{}},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Data flow over time</span>","metadata":{}},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(0,100)\nplt.xlabel('Time', fontsize=16)\nplt.ylabel('Mean congestion', fontsize=16)\nplt.xticks([])\n\ndailytime_ids = range(72)\nbars = plt.bar(dailytime_ids, [0]*len(dailytime_ids), color='white')\n\n\ndef update(idx):\n    df = train[(train.time_id>=idx) & (train.time_id<(idx+72))]\n    for idx, dailytime_id in enumerate(df.dailytime_id.unique()):\n        congestion = df.congestion[df.dailytime_id==dailytime_id].mean()\n        bars[idx].set_height(congestion)\n        bars[idx].set_color('white')\n        \n    ax.set_title(f'Congestion flow over time ({df.time.dt.day.iloc[0]} {df.time.dt.month_name().iloc[0]})', fontsize=16)\n\ndaily_ani = animation.FuncAnimation(fig, update,\n                               interval=100, frames=1000, blit=False)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that you can slow down or speed up the animation by using the buttons - or +","metadata":{}},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Daily congestion animation for all the days of year</span>","metadata":{"_uuid":"a0d5b2ff-9cec-45f6-a615-4fd270d58ea7","_cell_guid":"f33ac29f-c694-4a6d-9086-3e43d645c323","trusted":true}},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(30,60)\nplt.xlabel('Day of year', fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\ndayofyear = train.time.dt.dayofyear.unique()\nbars = plt.bar(dayofyear, [0]*len(dayofyear), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)","metadata":{"_uuid":"b6aaeff0-1971-4d4d-b3c5-52d84ab6bacc","_cell_guid":"464d71d1-0d41-4e37-bd66-5f3de641cdf7","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_uuid":"2870ba36-31cb-4140-9a54-f50b16b51eee","_cell_guid":"105fc1a6-bb1a-459c-b325-ef4117e0d266","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(-10,10)\nplt.xlabel('Day of year', fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\ndayofyear = train.time.dt.dayofyear.unique()\nbars = plt.bar(dayofyear, [0]*len(dayofyear), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean() - df.congestion.median()\n        bars[idx].set_height(congestion)\n        if congestion<0:\n            bars[idx].set_color('black')\n        else:\n            bars[idx].set_color('white')\n        \n    ax.set_title(f'Deviation from daily average (median of all days) at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=200, frames=train.dailytime_id.nunique(), blit=False)","metadata":{"_uuid":"5f6ed49e-4949-4a32-a3a9-41df35a9b677","_cell_guid":"d0d348e7-43d6-4f3e-afcb-171f19d97e4a","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_uuid":"45eff1f6-1920-4fae-bbb5-617f230e3ed2","_cell_guid":"45d9d2fb-9955-487d-a6a6-633c09f69aa4","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(-10,10)\nplt.xlabel('Daily time index', fontsize=16)\nplt.ylabel('Congestion deviation', fontsize=16)\n\ndailytime_ids = train.dailytime_id.unique()\nbars = plt.bar(dailytime_ids, [0]*len(dailytime_ids), color='white')\n\n\ndef update(idx, bars, dayofyears):\n    dayofyear = dayofyears[idx]\n    \n    median = train.groupby(train.dailytime_id).congestion.median().round().astype(int).tolist()\n    \n    df = train[train.time.dt.dayofyear==dayofyear]\n    for idx, dailytime_id in enumerate(df.dailytime_id.unique()):\n        congestion = df.congestion[df.dailytime_id==dailytime_id].mean() - median[dailytime_id]\n        bars[idx].set_height(congestion)\n        if congestion<0:\n            bars[idx].set_color('black')\n        else:\n            bars[idx].set_color('white')\n        \n    ax.set_title(f'Deviation from daily average (median of all days) of the day {dayofyear} of year', fontsize=16)\n\n    return bars\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.time.dt.dayofyear.unique()),\n                               interval=500, frames=train.time.dt.dayofyear.nunique(), blit=False)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Animate histograms of the roadways</span>","metadata":{}},{"cell_type":"code","source":"train['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')","metadata":{"execution":{"iopub.status.busy":"2022-03-09T19:59:12.512305Z","iopub.execute_input":"2022-03-09T19:59:12.512622Z","iopub.status.idle":"2022-03-09T19:59:14.839623Z","shell.execute_reply.started":"2022-03-09T19:59:12.512592Z","shell.execute_reply":"2022-03-09T19:59:14.838855Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(10,7))\n\nplt.xlabel('Congestion', fontsize=16)\nplt.ylabel('Count', fontsize=16)\n_,_,hist = ax.hist(train.congestion, 100, color='white')\n\ndef update(idx, hist, roadways):\n    df = train.congestion[train.roadway==roadways[idx]]\n    n,_ = np.histogram(df, 100)\n    for count, rect in zip(n, hist):\n        rect.set_height(count)\n\n    ax.set_ylim(0,np.max(n))\n    ax.set_title(f'Roadway: {roadways[idx]}', fontsize=16)\n    return hist\n\nani = animation.FuncAnimation(fig, update, fargs=(hist, train.roadway.unique()),\n                               interval=500, frames=train.roadway.nunique(), blit=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T19:59:40.0917Z","iopub.execute_input":"2022-03-09T19:59:40.091999Z","iopub.status.idle":"2022-03-09T19:59:40.797935Z","shell.execute_reply.started":"2022-03-09T19:59:40.091968Z","shell.execute_reply":"2022-03-09T19:59:40.796993Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(ani.to_jshtml())","metadata":{"execution":{"iopub.status.busy":"2022-03-09T19:59:41.820318Z","iopub.execute_input":"2022-03-09T19:59:41.820804Z","iopub.status.idle":"2022-03-09T19:59:57.693209Z","shell.execute_reply.started":"2022-03-09T19:59:41.820766Z","shell.execute_reply":"2022-03-09T19:59:57.692243Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Daily correlation between the 12 locations</span>","metadata":{}},{"cell_type":"code","source":"%%capture\ndef animate(hour):\n    dfs = []\n    columns = []\n    for x in range(3):\n        for y in range(4):\n            df = train[(train.x == x) & (train.y==y) & (train.time.dt.hour==hour) & (train.time.dt.minute==0)]\n            dfs.append(df.groupby('time').congestion.mean().tolist())\n            columns.append(f'x{x}y{y}')      \n    location_congestions = pd.DataFrame(np.array(dfs).T, columns=columns)\n\n    ax.cla()\n    sns.heatmap(ax = ax, data = location_congestions.corr(), annot=True, cbar_ax = cbar_ax)\n    ax.set_title(f'Correlation between the locations at {hour}h00', fontsize=16)\n\ngrid_kws = {'width_ratios': (0.9, 0.05), 'wspace': 0.2}\nfig, (ax, cbar_ax) = plt.subplots(1, 2, gridspec_kw = grid_kws, figsize = (10, 8))\nani = animation.FuncAnimation(fig = fig, func = animate, frames = train.time.dt.hour.nunique(), interval = 500)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(ani.to_jshtml())","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Daily congestion animation for the Monday of year</span>","metadata":{"_uuid":"b298c1bd-21c3-431c-aa3f-537cb9740d4a","_cell_guid":"fca95dac-fc17-42cb-945a-8713b553d4c6","trusted":true}},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(30,60)\nplt.xlabel('Day of year', fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\nmondayofyear = train[train.time.dt.weekday==0].time.dt.dayofyear.unique()\nbars = plt.bar(mondayofyear, [0]*len(mondayofyear), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[(train.time.dt.weekday==0) & (train.dailytime_id==dailytime_id)]\n    \n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average Monday congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)","metadata":{"_uuid":"451570fa-a30d-4e4f-9214-4c8718342ad5","_cell_guid":"35d8e36d-9730-4ff3-a7bf-4cc5b2dc2f32","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_uuid":"bf2563b7-5aba-42e0-8570-48cfa3619f82","_cell_guid":"97580e56-b7b3-4a84-aa35-32ed7d4966f8","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Average daily congestion for the weekdays</span>","metadata":{"_uuid":"71bde277-1ed6-49cf-8607-f22674a68e52","_cell_guid":"8d3d34c4-1c7f-468b-aca8-01e21d4fff3e","trusted":true}},{"cell_type":"code","source":"%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(30,60)\nax.set_xticklabels(['', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\nweekday = train.time.dt.weekday.unique()\nbars = plt.bar(weekday, [0]*len(weekday), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, weekday in enumerate(df.time.dt.weekday.unique()):\n        congestion = df.congestion[df.time.dt.weekday==weekday].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average week days congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)","metadata":{"_uuid":"078dc525-b690-4b04-9026-a6f8caa85153","_cell_guid":"d3e47c78-ee35-493f-87fd-b5b606f4b43f","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_uuid":"58e7ea29-f7e9-4c6b-afde-780dc11b8e84","_cell_guid":"aaa9a5b5-5274-46bd-b151-8aad60d36a46","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Average daily congestion for all the 12 locations and 65 roadways</span>","metadata":{"_uuid":"2567aa09-8844-4e75-abf9-bf1816162973","_cell_guid":"aaab56af-f2d1-41b4-9c57-0c88c2b56c7b","trusted":true}},{"cell_type":"code","source":"%%capture\nfig = plt.figure(figsize=(16,12))\n\n# Plot the 12 locations\nax1 = plt.subplot(1,2,1)\nplt.xlim(-0.5,2.5)\nplt.ylim(-0.5,3.5)\nplt.xticks([])\nplt.yticks([])\n\ntheta = np.arange(0,2.01*np.pi,0.01*np.pi)\nr=0.1\n\nlines = []\nfor ox in range(3):\n    for oy in range(4):\n        x = ox + r*np.sin(theta)\n        y = oy + r*np.cos(theta)\n        line, = ax1.plot(x,y, 'white', linewidth=10)\n        lines.append(line)\n\n# Plot the 65 roadways\norigins=[]\nfor idx, row in loc_dir.iterrows():\n    origin = np.repeat(np.array([[row.x],[row.y]]),row.num_dir, axis=-1)\n    origins.append(origin)\n    \norigin = np.concatenate(origins, axis=1)\n\nax2 = plt.subplot(1,2,2)\nplt.xlim(-0.5,2.5)\nplt.ylim(-0.5,3.5)\nplt.xticks([])\nplt.yticks([])\nQ = ax2.quiver(*origin, [0]*origin.shape[1], [0]*origin.shape[1], scale=1, color='white')\n\ndef update(dailytime_id, lines, Q, dummy):\n    # update locations\n    theta = np.arange(0,2.01*np.pi,0.01*np.pi)\n    colors = ['silver', 'gainsboro', 'white']\n    idx=0\n    for ox in range(3):\n        for oy in range(4):\n            congestion = train.congestion[(train.x==ox) & (train.y==oy) & (train.dailytime_id==dailytime_id)].mean()\n            r = congestion * 0.5/100\n            x = ox + r*np.sin(theta)\n            y = oy + r*np.cos(theta)\n            \n            \n            lines[idx].set_data(x,y)\n            lines[idx].set_color(colors[int(congestion//33)])\n            idx+=1\n            \n    ax1.set_title(f'Location average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n    \n    # update roadways\n    directions = []\n\n    for idx, row in loc_dir.iterrows():\n        df = train[(train.x==row.x) & (train.y==row.y) & (train.dailytime_id==dailytime_id)]\n        direction=[]\n        for d in row.dir_xy:\n            \n            congestion_d = df.congestion[df.dir_xy==d].mean()\n            direction.append(np.array(eval(d)) * congestion_d/500)\n        \n        directions.append(direction)\n        \n    direction = np.concatenate(directions, axis=0)\n\n    Q.set_UVC(direction[:,0], direction[:,1])\n    ax2.set_title(f'Roadway average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return lines, Q\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(lines, Q, train.dailytime_id.unique()),\n                               interval=300, frames=train.dailytime_id.nunique(), blit=False)","metadata":{"_uuid":"b6148664-4033-454f-851e-a906680f9074","_cell_guid":"0c6ff1ba-060d-410b-aa5f-6acc44f58f84","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_uuid":"d759a992-9c8d-4e13-85c4-e022ce8379fc","_cell_guid":"e3983667-8491-46bc-b4f9-1074197b54e5","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Average daily congestion for the 8 directions</span>","metadata":{"_uuid":"13e66b47-3a3b-4485-9036-deed04754c36","_cell_guid":"c526cf6a-64cb-41b5-9e50-71721e583fbc","trusted":true}},{"cell_type":"code","source":"%%capture\nfig = plt.figure(figsize=(10,10))\nax = plt.subplot(projection='polar')\nplt.ylim(0,60)\nax.set_xticklabels(['EB', 'NE', 'NB', 'NW', 'WB', 'SW', 'SB', 'SE'], fontsize=16)\n\nangles = np.linspace(0, 2 * np.pi, 9)\nbars = plt.bar(angles[:8], [1]*8, width=np.pi / 8, color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, direction in enumerate(df.direction.unique()):\n        congestion = df.congestion[df.direction==direction].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)","metadata":{"_uuid":"c3fbc110-be82-477a-be55-d1b0055738d2","_cell_guid":"4d600f6c-d17c-4b5f-a22d-1a8f7d088d03","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(daily_ani.to_jshtml())","metadata":{"_uuid":"9d73e61b-31a9-46ca-b10a-f8a44bd32d77","_cell_guid":"7fb8da45-345e-4b77-a4b7-1e462e3c19ca","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function returns an animation for a weekday\ndef weekday_ani(weekday):\n    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    fig = plt.figure(figsize=(16,12))\n\n    # Plot the 12 locations\n    ax1 = plt.subplot(1,2,1)\n    plt.xlim(-0.5,2.5)\n    plt.ylim(-0.5,3.5)\n    plt.xticks([])\n    plt.yticks([])\n\n    theta = np.arange(0,2.01*np.pi,0.01*np.pi)\n    r=0.1\n\n    lines = []\n    for ox in range(3):\n        for oy in range(4):\n            x = ox + r*np.sin(theta)\n            y = oy + r*np.cos(theta)\n            line, = ax1.plot(x,y, 'white', linewidth=10)\n            lines.append(line)\n\n    # Plot the 65 roadways\n    origins=[]\n    for idx, row in loc_dir.iterrows():\n        origin = np.repeat(np.array([[row.x],[row.y]]),row.num_dir, axis=-1)\n        origins.append(origin)\n\n    origin = np.concatenate(origins, axis=1)\n\n    ax2 = plt.subplot(1,2,2)\n    plt.xlim(-0.5,2.5)\n    plt.ylim(-0.5,3.5)\n    plt.xticks([])\n    plt.yticks([])\n    Q = ax2.quiver(*origin, [0]*origin.shape[1], [0]*origin.shape[1], scale=1, color='white')\n\n    def update(dailytime_id, lines, Q, dummy):\n        # update locations\n        theta = np.arange(0,2.01*np.pi,0.01*np.pi)\n        colors = ['silver', 'gainsboro', 'white']\n        idx=0\n        for ox in range(3):\n            for oy in range(4):\n                congestion = train.congestion[(train.time.dt.weekday==weekday) & (train.x==ox) & (train.y==oy) & (train.dailytime_id==dailytime_id)].mean()\n                r = congestion * 0.5/100\n                x = ox + r*np.sin(theta)\n                y = oy + r*np.cos(theta)\n\n\n                lines[idx].set_data(x,y)\n                lines[idx].set_color(colors[int(congestion//33)])\n                idx+=1\n\n        ax1.set_title(f'Average {weekdays[weekday]} congestion at {dailytime_id//3}h{dailytime_id%3*20} for all locations', fontsize=16)\n\n        # update roadways\n        directions = []\n\n        for idx, row in loc_dir.iterrows():\n            df = train[(train.time.dt.weekday==weekday) & (train.x==row.x) & (train.y==row.y) & (train.dailytime_id==dailytime_id)]\n            direction=[]\n            for d in row.dir_xy:\n\n                congestion_d = df.congestion[df.dir_xy==d].mean()\n                direction.append(np.array(eval(d)) * congestion_d/500)\n\n            directions.append(direction)\n\n        direction = np.concatenate(directions, axis=0)\n\n        Q.set_UVC(direction[:,0], direction[:,1])\n        ax2.set_title(f'Average {weekdays[weekday]} congestion at {dailytime_id//3}h{dailytime_id%3*20} for all roadways', fontsize=16)\n\n        return lines, Q\n\n    daily_ani = animation.FuncAnimation(fig, update, fargs=(lines, Q, train.dailytime_id.unique()),\n                                   interval=100, frames=train.dailytime_id.nunique(), blit=False)\n    \n    return daily_ani","metadata":{"_uuid":"8178d664-4d0d-487d-bdd0-299e0f1a8b62","_cell_guid":"2e433630-22f9-49f6-95bf-5636ec588540","collapsed":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-07T21:14:04.775973Z","iopub.execute_input":"2022-03-07T21:14:04.776391Z","iopub.status.idle":"2022-03-07T21:14:04.792815Z","shell.execute_reply.started":"2022-03-07T21:14:04.776351Z","shell.execute_reply":"2022-03-07T21:14:04.792014Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Monday congestion</span>","metadata":{"_uuid":"65f7420a-e2e6-4d02-91da-09076041f404","_cell_guid":"33bba666-c7fc-4bbf-b8e3-ee5eec864215","trusted":true}},{"cell_type":"code","source":"%%capture\nMonday_ani = weekday_ani(0)","metadata":{"_uuid":"854fe2a1-cb51-4929-8b17-1de1ceaf9200","_cell_guid":"7e650180-7c8a-48e8-963f-3309508c0613","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(Monday_ani.to_jshtml())","metadata":{"_uuid":"26ce9d19-f8de-4495-a75f-fab70a960216","_cell_guid":"9a4a361f-104f-47b0-9251-704628a3a401","collapsed":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-07T21:16:55.672922Z","iopub.execute_input":"2022-03-07T21:16:55.673213Z","iopub.status.idle":"2022-03-07T21:18:26.233831Z","shell.execute_reply.started":"2022-03-07T21:16:55.673183Z","shell.execute_reply":"2022-03-07T21:18:26.232997Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Daily congestion of each roadway</span>","metadata":{"_uuid":"a57359b4-477f-4a7b-9500-43c95658d9c9","_cell_guid":"994e73ab-c561-4d7e-a6e0-9709582ac49a","trusted":true}},{"cell_type":"code","source":"def roadway_daily_congestion(x, y, direction):\n    fig, ax = plt.subplots(figsize=(15, 5))\n    plt.ylim(0,100)\n    plt.xlim(-1,73)\n    plt.xlabel('Time (x20 minutes)', fontsize=16)\n    plt.ylabel('Congestion', fontsize=16)\n\n    sct = plt.scatter(range(72),[0]*72, color=['black']*36 + ['white']*36)\n\n    def update(idx, sct, dayofyears):\n        dayofyear = dayofyears[idx]\n        df = train[(train.time.dt.dayofyear==dayofyear) & (train.x==x) & (train.y==y) & (train.direction==direction)]\n\n        sct.set_offsets(np.array([df.dailytime_id, df.congestion]).T)\n\n        ax.set_title(f'Roadway {x}_{y}_{direction} on {df.time.dt.day.unique()[0]} {df.time.dt.month_name().unique()[0]}', fontsize=16)\n\n        return sct\n\n    daily_ani = animation.FuncAnimation(fig, update, fargs=(sct, train.time.dt.dayofyear.unique()),\n                                   interval=300, frames=train.time.dt.dayofyear.nunique(), blit=False)\n    return daily_ani","metadata":{"_uuid":"a5908722-c0a8-4597-bb72-fc1bb0c30d93","_cell_guid":"7e025a6d-5b7c-4477-a379-cd94c23ab66a","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nanis =[]\nfor x in range(3):\n    for y in range(4):\n        for direction in train.direction.unique():\n            if train[(train.x==x) & (train.y==y) & (train.direction==direction)].shape[0]>0:\n                ani = roadway_daily_congestion(x, y, direction)\n                anis.append(ani)","metadata":{"_uuid":"6a969143-635f-46b7-8492-d7db73671be3","_cell_guid":"dacb6b82-dcf3-46e2-812a-cd96cfc3063e","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HTML(anis[0].to_jshtml())","metadata":{"_uuid":"fe8224a1-e608-43ad-8f4a-88283a8ea5cf","_cell_guid":"23ae6b4f-7c0a-4fbe-8bcf-d8134f7deaf1","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style='color:#A80808'>Bonnus: simple baseline without ML that outperforms top ML models</span>\n\nIt is said that: \"Don't jump too soon into the water!!!\"","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv')\ntrain.time = pd.to_datetime(train.time)\ntrain['daytime_id'] = ( ( train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\ntrain = train.set_index('row_id', drop=True)\ntrain['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\n\ntest = pd.read_csv('../input/tabular-playground-series-mar-2022/test.csv', index_col='row_id')\ntest.time = pd.to_datetime(test.time)\ntest['roadway'] = test.x.astype('str') +'_'+ test.y.astype('str') +'_'+ test.direction.astype('str') \nsubmission = pd.read_csv('../input/tabular-playground-series-mar-2022/sample_submission.csv')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['median_'] = -1\n\nfor roadway in train.roadway.unique():\n    # extract data for each roadway\n    df = train[train.roadway==roadway]\n\n    if df.shape[0]>0:                \n        test.median_[test.roadway==roadway] = df.groupby(df.daytime_id).congestion.median().tolist()[-36:]\n\ntest.median_[(test.roadway=='2_2_SE') & (test.time.dt.hour<15)] = 20\ntest['median_'] = test['median_'].round().astype(int).tolist()","metadata":{"_kg_hide-input":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.congestion = test.median_.tolist()\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{},"execution_count":null,"outputs":[]}]}
\ No newline at end of file
+{
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat_minor": 4,
+ "nbformat": 4,
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Motivation</span>\n\nThis notebook aims to provide animations for time-space congestion visualizations. The idea is to animate the congestion change during time for all the 12 locations and 65 roadways. For a detail EDA, please visit the [notebook](https://www.kaggle.com/sytuannguyen/tps-mar-2022-eda-model)",
+   "metadata": {
+    "_uuid": "0ad34bbd-e9af-4d34-b40c-fee10238b827",
+    "_cell_guid": "1e04659d-b137-4cfb-a8ae-2e6cdcaa1a56",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.facecolor'] = 'gray'\n\nimport seaborn as sns\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\nimport warnings\nwarnings.simplefilter('ignore')",
+   "metadata": {
+    "_uuid": "b3a0feec-b03d-46a9-9716-87cbcab3e9c0",
+    "_cell_guid": "ab5b2a97-00fa-4610-947b-f15aff0f0b8b",
+    "collapsed": false,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "execution": {
+     "iopub.status.busy": "2022-03-09T19:59:01.77303Z",
+     "iopub.execute_input": "2022-03-09T19:59:01.773373Z",
+     "iopub.status.idle": "2022-03-09T19:59:01.803049Z",
+     "shell.execute_reply.started": "2022-03-09T19:59:01.773277Z",
+     "shell.execute_reply": "2022-03-09T19:59:01.802224Z"
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "train = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv', index_col='row_id')\ntrain.time = pd.to_datetime(train.time)\ntrain['dailytime_id'] = ( ( train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\ntrain['time_id'] = ( ( (train.time.dt.dayofyear-91)*24*60 + train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)",
+   "metadata": {
+    "_uuid": "f4843922-3963-4234-b4e0-76a1fe5d2b90",
+    "_cell_guid": "4f11ed48-a3a4-4506-aefc-668aaa99ab9d",
+    "collapsed": false,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "execution": {
+     "iopub.status.busy": "2022-03-09T19:59:01.804787Z",
+     "iopub.execute_input": "2022-03-09T19:59:01.805295Z",
+     "iopub.status.idle": "2022-03-09T19:59:03.20001Z",
+     "shell.execute_reply.started": "2022-03-09T19:59:01.805251Z",
+     "shell.execute_reply": "2022-03-09T19:59:03.199037Z"
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "# Map directions to vectors\ntrain['dir_xy'] = train.direction.map({'EB':'[1,0]', 'NB':'[0,1]', 'SB':'[0,-1]', 'WB':'[-1,0]', 'NE':'[1,1]', 'SW':'[-1,-1]', 'NW':'[-1,1]', 'SE':'[1,-1]'})\n\nloc_dir = train.groupby(['x','y']).dir_xy.unique().reset_index()\nloc_dir['num_dir'] = loc_dir.dir_xy.apply(lambda x: len(x))",
+   "metadata": {
+    "_uuid": "68329b8f-f4d1-4a6b-9bbf-f3534a4eccfa",
+    "_cell_guid": "64bd633d-6c75-4232-8015-e3aa9bff7182",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "execution": {
+     "iopub.status.busy": "2022-03-09T19:59:03.201127Z",
+     "iopub.execute_input": "2022-03-09T19:59:03.201817Z",
+     "iopub.status.idle": "2022-03-09T19:59:03.503898Z",
+     "shell.execute_reply.started": "2022-03-09T19:59:03.201786Z",
+     "shell.execute_reply": "2022-03-09T19:59:03.503153Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Data repartition for each roadway</span>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "# create a categorical feature for distinguishing the 65 roadways\ntrain['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\n\n# create a color column for the scatter plot: white for data close to the mean of each instant, \n# black for data outside the range (mean-std) to (mean+std)\ntrain['color'] = 'white'\ncolor=[]\nfor roadway in train.roadway.unique():\n    df = train[train.roadway==roadway]\n    df['color'] = 'white'\n    for dailytime_id in df.dailytime_id.unique():\n        congestion = df.congestion[df.dailytime_id==dailytime_id]\n        mean = np.mean(congestion)\n        std = np.std(congestion)\n        cond = abs(congestion-mean)<std\n        df.color[df.dailytime_id==dailytime_id]=cond.map({False:'black', True:'white'}).tolist()\n    train.color[train.roadway==roadway] = df.color\n\n# highlight the last day 30 Sep 1991\ntrain.color[train.time.dt.dayofyear==273] = 'red'",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "The scatter plot below clearly shows the position of the last day morning congestion comparing to data of all the previous days for the first roadway (0_0_EB). The red points are for the last day morning (30 Sep), the white zone is for the range between (mean-std) to (mean+std) where mean and std are computed for each instant.  ",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "fig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(0,100)\nplt.xlim(-1,73)\nplt.xlabel('Time (x20 minutes)', fontsize=16)\nplt.ylabel('Congestion', fontsize=16)\n\ndf = train[train.roadway=='0_0_EB']\nsct = plt.scatter(df.dailytime_id, df.congestion)\nsct.set_color(df.color)\nax.set_title(f'Roadway 0_0_EB', fontsize=16)",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(0,100)\nplt.xlim(-1,73)\nplt.xlabel('Time (x20 minutes)', fontsize=16)\nplt.ylabel('Congestion', fontsize=16)\n\nsct = plt.scatter(range(72),[0]*72, color=['white'])\n\ndef update(idx, sct, roadways):\n    roadway = roadways[idx]\n    df = train[train.roadway==roadway]\n\n    sct.set_offsets(np.array([df.dailytime_id, df.congestion]).T)\n    sct.set_color(df.color)\n    ax.set_title(f'Roadway {roadway}', fontsize=16)\n\n    return sct\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(sct, train.roadway.unique()),\n                               interval=300, frames=train.roadway.nunique(), blit=False)",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "The white zone is in the range *(mean-std)* to *(mean+std)* where *mean* and *std* are computed for each roadway at each instant.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Data flow over time</span>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(0,100)\nplt.xlabel('Time', fontsize=16)\nplt.ylabel('Mean congestion', fontsize=16)\nplt.xticks([])\n\ndailytime_ids = range(72)\nbars = plt.bar(dailytime_ids, [0]*len(dailytime_ids), color='white')\n\n\ndef update(idx):\n    df = train[(train.time_id>=idx) & (train.time_id<(idx+72))]\n    for idx, dailytime_id in enumerate(df.dailytime_id.unique()):\n        congestion = df.congestion[df.dailytime_id==dailytime_id].mean()\n        bars[idx].set_height(congestion)\n        bars[idx].set_color('white')\n        \n    ax.set_title(f'Congestion flow over time ({df.time.dt.day.iloc[0]} {df.time.dt.month_name().iloc[0]})', fontsize=16)\n\ndaily_ani = animation.FuncAnimation(fig, update,\n                               interval=100, frames=1000, blit=False)",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Note that you can slow down or speed up the animation by using the buttons - or +",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Daily congestion animation for all the days of year</span>",
+   "metadata": {
+    "_uuid": "a0d5b2ff-9cec-45f6-a615-4fd270d58ea7",
+    "_cell_guid": "f33ac29f-c694-4a6d-9086-3e43d645c323",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(30,60)\nplt.xlabel('Day of year', fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\ndayofyear = train.time.dt.dayofyear.unique()\nbars = plt.bar(dayofyear, [0]*len(dayofyear), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)",
+   "metadata": {
+    "_uuid": "b6aaeff0-1971-4d4d-b3c5-52d84ab6bacc",
+    "_cell_guid": "464d71d1-0d41-4e37-bd66-5f3de641cdf7",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "2870ba36-31cb-4140-9a54-f50b16b51eee",
+    "_cell_guid": "105fc1a6-bb1a-459c-b325-ef4117e0d266",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(-10,10)\nplt.xlabel('Day of year', fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\ndayofyear = train.time.dt.dayofyear.unique()\nbars = plt.bar(dayofyear, [0]*len(dayofyear), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean() - df.congestion.median()\n        bars[idx].set_height(congestion)\n        if congestion<0:\n            bars[idx].set_color('black')\n        else:\n            bars[idx].set_color('white')\n        \n    ax.set_title(f'Deviation from daily average (median of all days) at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=200, frames=train.dailytime_id.nunique(), blit=False)",
+   "metadata": {
+    "_uuid": "5f6ed49e-4949-4a32-a3a9-41df35a9b677",
+    "_cell_guid": "d0d348e7-43d6-4f3e-afcb-171f19d97e4a",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "45eff1f6-1920-4fae-bbb5-617f230e3ed2",
+    "_cell_guid": "45d9d2fb-9955-487d-a6a6-633c09f69aa4",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(-10,10)\nplt.xlabel('Daily time index', fontsize=16)\nplt.ylabel('Congestion deviation', fontsize=16)\n\ndailytime_ids = train.dailytime_id.unique()\nbars = plt.bar(dailytime_ids, [0]*len(dailytime_ids), color='white')\n\n\ndef update(idx, bars, dayofyears):\n    dayofyear = dayofyears[idx]\n    \n    median = train.groupby(train.dailytime_id).congestion.median().round().astype(int).tolist()\n    \n    df = train[train.time.dt.dayofyear==dayofyear]\n    for idx, dailytime_id in enumerate(df.dailytime_id.unique()):\n        congestion = df.congestion[df.dailytime_id==dailytime_id].mean() - median[dailytime_id]\n        bars[idx].set_height(congestion)\n        if congestion<0:\n            bars[idx].set_color('black')\n        else:\n            bars[idx].set_color('white')\n        \n    ax.set_title(f'Deviation from daily average (median of all days) of the day {dayofyear} of year', fontsize=16)\n\n    return bars\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.time.dt.dayofyear.unique()),\n                               interval=500, frames=train.time.dt.dayofyear.nunique(), blit=False)",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Animate histograms of the roadways</span>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "train['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-03-09T19:59:12.512305Z",
+     "iopub.execute_input": "2022-03-09T19:59:12.512622Z",
+     "iopub.status.idle": "2022-03-09T19:59:14.839623Z",
+     "shell.execute_reply.started": "2022-03-09T19:59:12.512592Z",
+     "shell.execute_reply": "2022-03-09T19:59:14.838855Z"
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(10,7))\n\nplt.xlabel('Congestion', fontsize=16)\nplt.ylabel('Count', fontsize=16)\n_,_,hist = ax.hist(train.congestion, 100, color='white')\n\ndef update(idx, hist, roadways):\n    df = train.congestion[train.roadway==roadways[idx]]\n    n,_ = np.histogram(df, 100)\n    for count, rect in zip(n, hist):\n        rect.set_height(count)\n\n    ax.set_ylim(0,np.max(n))\n    ax.set_title(f'Roadway: {roadways[idx]}', fontsize=16)\n    return hist\n\nani = animation.FuncAnimation(fig, update, fargs=(hist, train.roadway.unique()),\n                               interval=500, frames=train.roadway.nunique(), blit=False)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-03-09T19:59:40.0917Z",
+     "iopub.execute_input": "2022-03-09T19:59:40.091999Z",
+     "iopub.status.idle": "2022-03-09T19:59:40.797935Z",
+     "shell.execute_reply.started": "2022-03-09T19:59:40.091968Z",
+     "shell.execute_reply": "2022-03-09T19:59:40.796993Z"
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(ani.to_jshtml())",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-03-09T19:59:41.820318Z",
+     "iopub.execute_input": "2022-03-09T19:59:41.820804Z",
+     "iopub.status.idle": "2022-03-09T19:59:57.693209Z",
+     "shell.execute_reply.started": "2022-03-09T19:59:41.820766Z",
+     "shell.execute_reply": "2022-03-09T19:59:57.692243Z"
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Daily correlation between the 12 locations</span>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\ndef animate(hour):\n    dfs = []\n    columns = []\n    for x in range(3):\n        for y in range(4):\n            df = train[(train.x == x) & (train.y==y) & (train.time.dt.hour==hour) & (train.time.dt.minute==0)]\n            dfs.append(df.groupby('time').congestion.mean().tolist())\n            columns.append(f'x{x}y{y}')      \n    location_congestions = pd.DataFrame(np.array(dfs).T, columns=columns)\n\n    ax.cla()\n    sns.heatmap(ax = ax, data = location_congestions.corr(), annot=True, cbar_ax = cbar_ax)\n    ax.set_title(f'Correlation between the locations at {hour}h00', fontsize=16)\n\ngrid_kws = {'width_ratios': (0.9, 0.05), 'wspace': 0.2}\nfig, (ax, cbar_ax) = plt.subplots(1, 2, gridspec_kw = grid_kws, figsize = (10, 8))\nani = animation.FuncAnimation(fig = fig, func = animate, frames = train.time.dt.hour.nunique(), interval = 500)",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(ani.to_jshtml())",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Daily congestion animation for the Monday of year</span>",
+   "metadata": {
+    "_uuid": "b298c1bd-21c3-431c-aa3f-537cb9740d4a",
+    "_cell_guid": "fca95dac-fc17-42cb-945a-8713b553d4c6",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(30,60)\nplt.xlabel('Day of year', fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\nmondayofyear = train[train.time.dt.weekday==0].time.dt.dayofyear.unique()\nbars = plt.bar(mondayofyear, [0]*len(mondayofyear), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[(train.time.dt.weekday==0) & (train.dailytime_id==dailytime_id)]\n    \n    for idx, dayofyear in enumerate(df.time.dt.dayofyear.unique()):\n        congestion = df.congestion[df.time.dt.dayofyear==dayofyear].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average Monday congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)",
+   "metadata": {
+    "_uuid": "451570fa-a30d-4e4f-9214-4c8718342ad5",
+    "_cell_guid": "35d8e36d-9730-4ff3-a7bf-4cc5b2dc2f32",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "bf2563b7-5aba-42e0-8570-48cfa3619f82",
+    "_cell_guid": "97580e56-b7b3-4a84-aa35-32ed7d4966f8",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Average daily congestion for the weekdays</span>",
+   "metadata": {
+    "_uuid": "71bde277-1ed6-49cf-8607-f22674a68e52",
+    "_cell_guid": "8d3d34c4-1c7f-468b-aca8-01e21d4fff3e",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.ylim(30,60)\nax.set_xticklabels(['', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], fontsize=16)\nplt.ylabel('Daily congestion', fontsize=16)\n\nweekday = train.time.dt.weekday.unique()\nbars = plt.bar(weekday, [0]*len(weekday), color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, weekday in enumerate(df.time.dt.weekday.unique()):\n        congestion = df.congestion[df.time.dt.weekday==weekday].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average week days congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)",
+   "metadata": {
+    "_uuid": "078dc525-b690-4b04-9026-a6f8caa85153",
+    "_cell_guid": "d3e47c78-ee35-493f-87fd-b5b606f4b43f",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "58e7ea29-f7e9-4c6b-afde-780dc11b8e84",
+    "_cell_guid": "aaa9a5b5-5274-46bd-b151-8aad60d36a46",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Average daily congestion for all the 12 locations and 65 roadways</span>",
+   "metadata": {
+    "_uuid": "2567aa09-8844-4e75-abf9-bf1816162973",
+    "_cell_guid": "aaab56af-f2d1-41b4-9c57-0c88c2b56c7b",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig = plt.figure(figsize=(16,12))\n\n# Plot the 12 locations\nax1 = plt.subplot(1,2,1)\nplt.xlim(-0.5,2.5)\nplt.ylim(-0.5,3.5)\nplt.xticks([])\nplt.yticks([])\n\ntheta = np.arange(0,2.01*np.pi,0.01*np.pi)\nr=0.1\n\nlines = []\nfor ox in range(3):\n    for oy in range(4):\n        x = ox + r*np.sin(theta)\n        y = oy + r*np.cos(theta)\n        line, = ax1.plot(x,y, 'white', linewidth=10)\n        lines.append(line)\n\n# Plot the 65 roadways\norigins=[]\nfor idx, row in loc_dir.iterrows():\n    origin = np.repeat(np.array([[row.x],[row.y]]),row.num_dir, axis=-1)\n    origins.append(origin)\n    \norigin = np.concatenate(origins, axis=1)\n\nax2 = plt.subplot(1,2,2)\nplt.xlim(-0.5,2.5)\nplt.ylim(-0.5,3.5)\nplt.xticks([])\nplt.yticks([])\nQ = ax2.quiver(*origin, [0]*origin.shape[1], [0]*origin.shape[1], scale=1, color='white')\n\ndef update(dailytime_id, lines, Q, dummy):\n    # update locations\n    theta = np.arange(0,2.01*np.pi,0.01*np.pi)\n    colors = ['silver', 'gainsboro', 'white']\n    idx=0\n    for ox in range(3):\n        for oy in range(4):\n            congestion = train.congestion[(train.x==ox) & (train.y==oy) & (train.dailytime_id==dailytime_id)].mean()\n            r = congestion * 0.5/100\n            x = ox + r*np.sin(theta)\n            y = oy + r*np.cos(theta)\n            \n            \n            lines[idx].set_data(x,y)\n            lines[idx].set_color(colors[int(congestion//33)])\n            idx+=1\n            \n    ax1.set_title(f'Location average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n    \n    # update roadways\n    directions = []\n\n    for idx, row in loc_dir.iterrows():\n        df = train[(train.x==row.x) & (train.y==row.y) & (train.dailytime_id==dailytime_id)]\n        direction=[]\n        for d in row.dir_xy:\n            \n            congestion_d = df.congestion[df.dir_xy==d].mean()\n            direction.append(np.array(eval(d)) * congestion_d/500)\n        \n        directions.append(direction)\n        \n    direction = np.concatenate(directions, axis=0)\n\n    Q.set_UVC(direction[:,0], direction[:,1])\n    ax2.set_title(f'Roadway average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return lines, Q\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(lines, Q, train.dailytime_id.unique()),\n                               interval=300, frames=train.dailytime_id.nunique(), blit=False)",
+   "metadata": {
+    "_uuid": "b6148664-4033-454f-851e-a906680f9074",
+    "_cell_guid": "0c6ff1ba-060d-410b-aa5f-6acc44f58f84",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "d759a992-9c8d-4e13-85c4-e022ce8379fc",
+    "_cell_guid": "e3983667-8491-46bc-b4f9-1074197b54e5",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Average daily congestion for the 8 directions</span>",
+   "metadata": {
+    "_uuid": "13e66b47-3a3b-4485-9036-deed04754c36",
+    "_cell_guid": "c526cf6a-64cb-41b5-9e50-71721e583fbc",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nfig = plt.figure(figsize=(10,10))\nax = plt.subplot(projection='polar')\nplt.ylim(0,60)\nax.set_xticklabels(['EB', 'NE', 'NB', 'NW', 'WB', 'SW', 'SB', 'SE'], fontsize=16)\n\nangles = np.linspace(0, 2 * np.pi, 9)\nbars = plt.bar(angles[:8], [1]*8, width=np.pi / 8, color='white')\n\ndef update(dailytime_id, bars, dummy):\n    df = train[train.dailytime_id==dailytime_id]\n    \n    for idx, direction in enumerate(df.direction.unique()):\n        congestion = df.congestion[df.direction==direction].mean()\n        bars[idx].set_height(congestion)\n\n    ax.set_title(f'Average daily congestion at {dailytime_id//3}h{dailytime_id%3*20}', fontsize=16)\n\n    return bars\n\ndaily_ani = animation.FuncAnimation(fig, update, fargs=(bars, train.dailytime_id.unique()),\n                               interval=100, frames=train.dailytime_id.nunique(), blit=False)",
+   "metadata": {
+    "_uuid": "c3fbc110-be82-477a-be55-d1b0055738d2",
+    "_cell_guid": "4d600f6c-d17c-4b5f-a22d-1a8f7d088d03",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(daily_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "9d73e61b-31a9-46ca-b10a-f8a44bd32d77",
+    "_cell_guid": "7fb8da45-345e-4b77-a4b7-1e462e3c19ca",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "# This function returns an animation for a weekday\ndef weekday_ani(weekday):\n    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    fig = plt.figure(figsize=(16,12))\n\n    # Plot the 12 locations\n    ax1 = plt.subplot(1,2,1)\n    plt.xlim(-0.5,2.5)\n    plt.ylim(-0.5,3.5)\n    plt.xticks([])\n    plt.yticks([])\n\n    theta = np.arange(0,2.01*np.pi,0.01*np.pi)\n    r=0.1\n\n    lines = []\n    for ox in range(3):\n        for oy in range(4):\n            x = ox + r*np.sin(theta)\n            y = oy + r*np.cos(theta)\n            line, = ax1.plot(x,y, 'white', linewidth=10)\n            lines.append(line)\n\n    # Plot the 65 roadways\n    origins=[]\n    for idx, row in loc_dir.iterrows():\n        origin = np.repeat(np.array([[row.x],[row.y]]),row.num_dir, axis=-1)\n        origins.append(origin)\n\n    origin = np.concatenate(origins, axis=1)\n\n    ax2 = plt.subplot(1,2,2)\n    plt.xlim(-0.5,2.5)\n    plt.ylim(-0.5,3.5)\n    plt.xticks([])\n    plt.yticks([])\n    Q = ax2.quiver(*origin, [0]*origin.shape[1], [0]*origin.shape[1], scale=1, color='white')\n\n    def update(dailytime_id, lines, Q, dummy):\n        # update locations\n        theta = np.arange(0,2.01*np.pi,0.01*np.pi)\n        colors = ['silver', 'gainsboro', 'white']\n        idx=0\n        for ox in range(3):\n            for oy in range(4):\n                congestion = train.congestion[(train.time.dt.weekday==weekday) & (train.x==ox) & (train.y==oy) & (train.dailytime_id==dailytime_id)].mean()\n                r = congestion * 0.5/100\n                x = ox + r*np.sin(theta)\n                y = oy + r*np.cos(theta)\n\n\n                lines[idx].set_data(x,y)\n                lines[idx].set_color(colors[int(congestion//33)])\n                idx+=1\n\n        ax1.set_title(f'Average {weekdays[weekday]} congestion at {dailytime_id//3}h{dailytime_id%3*20} for all locations', fontsize=16)\n\n        # update roadways\n        directions = []\n\n        for idx, row in loc_dir.iterrows():\n            df = train[(train.time.dt.weekday==weekday) & (train.x==row.x) & (train.y==row.y) & (train.dailytime_id==dailytime_id)]\n            direction=[]\n            for d in row.dir_xy:\n\n                congestion_d = df.congestion[df.dir_xy==d].mean()\n                direction.append(np.array(eval(d)) * congestion_d/500)\n\n            directions.append(direction)\n\n        direction = np.concatenate(directions, axis=0)\n\n        Q.set_UVC(direction[:,0], direction[:,1])\n        ax2.set_title(f'Average {weekdays[weekday]} congestion at {dailytime_id//3}h{dailytime_id%3*20} for all roadways', fontsize=16)\n\n        return lines, Q\n\n    daily_ani = animation.FuncAnimation(fig, update, fargs=(lines, Q, train.dailytime_id.unique()),\n                                   interval=100, frames=train.dailytime_id.nunique(), blit=False)\n    \n    return daily_ani",
+   "metadata": {
+    "_uuid": "8178d664-4d0d-487d-bdd0-299e0f1a8b62",
+    "_cell_guid": "2e433630-22f9-49f6-95bf-5636ec588540",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.status.busy": "2022-03-07T21:14:04.775973Z",
+     "iopub.execute_input": "2022-03-07T21:14:04.776391Z",
+     "iopub.status.idle": "2022-03-07T21:14:04.792815Z",
+     "shell.execute_reply.started": "2022-03-07T21:14:04.776351Z",
+     "shell.execute_reply": "2022-03-07T21:14:04.792014Z"
+    },
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Monday congestion</span>",
+   "metadata": {
+    "_uuid": "65f7420a-e2e6-4d02-91da-09076041f404",
+    "_cell_guid": "33bba666-c7fc-4bbf-b8e3-ee5eec864215",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nMonday_ani = weekday_ani(0)",
+   "metadata": {
+    "_uuid": "854fe2a1-cb51-4929-8b17-1de1ceaf9200",
+    "_cell_guid": "7e650180-7c8a-48e8-963f-3309508c0613",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(Monday_ani.to_jshtml())",
+   "metadata": {
+    "_uuid": "26ce9d19-f8de-4495-a75f-fab70a960216",
+    "_cell_guid": "9a4a361f-104f-47b0-9251-704628a3a401",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.status.busy": "2022-03-07T21:16:55.672922Z",
+     "iopub.execute_input": "2022-03-07T21:16:55.673213Z",
+     "iopub.status.idle": "2022-03-07T21:18:26.233831Z",
+     "shell.execute_reply.started": "2022-03-07T21:16:55.673183Z",
+     "shell.execute_reply": "2022-03-07T21:18:26.232997Z"
+    },
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Daily congestion of each roadway</span>",
+   "metadata": {
+    "_uuid": "a57359b4-477f-4a7b-9500-43c95658d9c9",
+    "_cell_guid": "994e73ab-c561-4d7e-a6e0-9709582ac49a",
+    "trusted": true
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "def roadway_daily_congestion(x, y, direction):\n    fig, ax = plt.subplots(figsize=(15, 5))\n    plt.ylim(0,100)\n    plt.xlim(-1,73)\n    plt.xlabel('Time (x20 minutes)', fontsize=16)\n    plt.ylabel('Congestion', fontsize=16)\n\n    sct = plt.scatter(range(72),[0]*72, color=['black']*36 + ['white']*36)\n\n    def update(idx, sct, dayofyears):\n        dayofyear = dayofyears[idx]\n        df = train[(train.time.dt.dayofyear==dayofyear) & (train.x==x) & (train.y==y) & (train.direction==direction)]\n\n        sct.set_offsets(np.array([df.dailytime_id, df.congestion]).T)\n\n        ax.set_title(f'Roadway {x}_{y}_{direction} on {df.time.dt.day.unique()[0]} {df.time.dt.month_name().unique()[0]}', fontsize=16)\n\n        return sct\n\n    daily_ani = animation.FuncAnimation(fig, update, fargs=(sct, train.time.dt.dayofyear.unique()),\n                                   interval=300, frames=train.time.dt.dayofyear.nunique(), blit=False)\n    return daily_ani",
+   "metadata": {
+    "_uuid": "a5908722-c0a8-4597-bb72-fc1bb0c30d93",
+    "_cell_guid": "7e025a6d-5b7c-4477-a379-cd94c23ab66a",
+    "collapsed": false,
+    "_kg_hide-input": true,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "%%capture\nanis =[]\nfor x in range(3):\n    for y in range(4):\n        for direction in train.direction.unique():\n            if train[(train.x==x) & (train.y==y) & (train.direction==direction)].shape[0]>0:\n                ani = roadway_daily_congestion(x, y, direction)\n                anis.append(ani)",
+   "metadata": {
+    "_uuid": "6a969143-635f-46b7-8492-d7db73671be3",
+    "_cell_guid": "dacb6b82-dcf3-46e2-812a-cd96cfc3063e",
+    "collapsed": false,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "HTML(anis[0].to_jshtml())",
+   "metadata": {
+    "_uuid": "fe8224a1-e608-43ad-8f4a-88283a8ea5cf",
+    "_cell_guid": "23ae6b4f-7c0a-4fbe-8bcf-d8134f7deaf1",
+    "collapsed": false,
+    "jupyter": {
+     "outputs_hidden": false
+    },
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <span style='color:#A80808'>Bonnus: simple baseline without ML that outperforms top ML models</span>\n\nIt is said that: \"Don't jump too soon into the water!!!\"",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "train = pd.read_csv('../input/tabular-playground-series-mar-2022/train.csv')\ntrain.time = pd.to_datetime(train.time)\ntrain['daytime_id'] = ( ( train.time.dt.hour*60 + train.time.dt.minute ) /20 ).astype(int)\ntrain = train.set_index('row_id', drop=True)\ntrain['roadway'] = train.x.astype('str') +'_'+ train.y.astype('str') +'_'+ train.direction.astype('str')\n\ntest = pd.read_csv('../input/tabular-playground-series-mar-2022/test.csv', index_col='row_id')\ntest.time = pd.to_datetime(test.time)\ntest['roadway'] = test.x.astype('str') +'_'+ test.y.astype('str') +'_'+ test.direction.astype('str') \nsubmission = pd.read_csv('../input/tabular-playground-series-mar-2022/sample_submission.csv')",
+   "metadata": {
+    "_kg_hide-input": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "test['median_'] = -1\n\nfor roadway in train.roadway.unique():\n    # extract data for each roadway\n    df = train[train.roadway==roadway]\n\n    if df.shape[0]>0:                \n        test.median_[test.roadway==roadway] = df.groupby(df.daytime_id).congestion.median().tolist()[-36:]\n\ntest.median_[(test.roadway=='2_2_SE') & (test.time.dt.hour<15)] = 20\ntest['median_'] = test['median_'].round().astype(int).tolist()",
+   "metadata": {
+    "_kg_hide-input": false
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "submission.congestion = test.median_.tolist()\nsubmission.to_csv('submission.csv', index=False)\nsubmission",
+   "metadata": {},
+   "execution_count": null,
+   "outputs": []
+  }
+ ]
+}
\ No newline at end of file
Index: dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat_minor\":4,\"nbformat\":4,\"cells\":[{\"cell_type\":\"markdown\",\"source\":\"# 2nd Place Solution - CV 741, Public 727, Private 740\\nWe (Chun Ming Lee @leecming , Udbhav Bamba @ubamba98, and Chris Deotte @cdeotte ) are excited to present our 2nd place solution to Kaggle's \\\"Feedback Prize - Evaluating Student Writing\\\" Competition. Thank you Georgia State University, The Learning Agency Lab, and Kaggle for an awesome competition.\\n\\nOur full solution write up is [here][1]. The main ingredients to our solution are \\n* powerful post process per model\\n* huge variety of NLP models trained on NER task\\n* ensemble with weighted box fusion (from ZFTurbo's GitHub [here][3]). \\n\\n[1]: https://www.kaggle.com/c/feedback-prize-2021/discussion/313389\\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"# Inference Script with Post Process\\nThe following Python script accepts a filename of a saved model, infers the test texts, applies post process, and writes a `submission.csv` file with an extra column of confidence scores per span (i.e. the probability that this span is correct). **Click \\\"show hidden cell\\\" to see the code.**\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"%%writefile generate_preds.py\\n\\nimport os\\nimport argparse\\n\\nap = argparse.ArgumentParser()\\nap.add_argument('--model_paths', nargs='+', required=True)\\nap.add_argument(\\\"--save_name\\\", type=str, required=True)\\nap.add_argument(\\\"--max_len\\\", type=int, required=True)\\nargs = ap.parse_args()\\n\\nif args.save_name == \\\"yoso\\\":\\n    os.system(\\\"cp -r ../input/hf-transformers/transformers-4.16.0 .\\\")\\n    os.system(\\\"pip install -U --no-build-isolation --no-deps /kaggle/working/transformers-4.16.0\\\")\\n    \\nif (\\\"v3\\\" in args.save_name)|(\\\"v2\\\" in args.save_name):\\n    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\\n    # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\\n    # This must be done before importing transformers\\n    import shutil\\n    from pathlib import Path\\n\\n    transformers_path = Path(\\\"/opt/conda/lib/python3.7/site-packages/transformers\\\")\\n\\n    input_dir = Path(\\\"../input/deberta-v2-3-fast-tokenizer\\\")\\n\\n    convert_file = input_dir / \\\"convert_slow_tokenizer.py\\\"\\n    conversion_path = transformers_path/convert_file.name\\n\\n    if conversion_path.exists():\\n        conversion_path.unlink()\\n\\n    shutil.copy(convert_file, transformers_path)\\n    deberta_v2_path = transformers_path / \\\"models\\\" / \\\"deberta_v2\\\"\\n\\n    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\\n        filepath = deberta_v2_path/filename\\n        if filepath.exists():\\n            filepath.unlink()\\n\\n        shutil.copy(input_dir/filename, filepath)\\n\\nif args.save_name == \\\"longformerwithlstm\\\":\\n    os.system(\\\"cp -r ../input/longformerwithbilstmhead/model.py .\\\")\\n    from model import LongformerForTokenClassificationwithbiLSTM\\n    \\nif args.save_name == \\\"debertawithlstm\\\":\\n    os.system(\\\"cp -r ../input/deberta-lstm/model.py .\\\")\\n    from model import DebertaForTokenClassificationwithbiLSTM\\n        \\nimport gc\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nimport transformers\\nimport multiprocessing as mp\\nfrom scipy.special import softmax\\nfrom torch.utils.data import Dataset\\nfrom transformers import (AutoModelForTokenClassification, \\n                          AutoTokenizer, \\n                          TrainingArguments, \\n                          Trainer)\\n\\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\\n\\nNUM_CORES = 16\\nBATCH_SIZE = 4\\nMAX_SEQ_LENGTH = args.max_len\\nPRETRAINED_MODEL_PATHS = args.model_paths\\nif \\\"debertal_chris\\\" in args.save_name:\\n    print('==> using -1 in offset mapping...')\\nif (\\\"v3\\\" in args.save_name)|(\\\"v2\\\" in args.save_name):\\n    print('==> using -1 in offset mapping...')\\n    \\nAGG_FUNC = np.mean\\nprint('==> using span token mean...')\\n\\nTEST_DIR = '../input/feedback-prize-2021/test/'\\n\\nMIN_TOKENS = {\\n    \\\"Lead\\\": 32,\\n    \\\"Position\\\": 5,\\n    \\\"Evidence\\\": 35,\\n    \\\"Claim\\\": 7,\\n    \\\"Concluding Statement\\\": 6,\\n    \\\"Counterclaim\\\": 6,\\n    \\\"Rebuttal\\\": 6\\n}\\n\\nif \\\"chris\\\" not in args.save_name:\\n    ner_labels = {'O': 0,\\n                  'B-Lead': 1,\\n                  'I-Lead': 2,\\n                  'B-Position': 3,\\n                  'I-Position': 4,\\n                  'B-Evidence': 5,\\n                  'I-Evidence': 6,\\n                  'B-Claim': 7,\\n                  'I-Claim': 8,\\n                  'B-Concluding Statement': 9,\\n                  'I-Concluding Statement': 10,\\n                  'B-Counterclaim': 11,\\n                  'I-Counterclaim': 12,\\n                  'B-Rebuttal': 13,\\n                  'I-Rebuttal': 14}\\nelse:\\n    print(\\\"==> Using Chris BIO\\\")\\n    ner_labels = {'O': 14,\\n                  'B-Lead': 0,\\n                  'I-Lead': 1,\\n                  'B-Position': 2,\\n                  'I-Position': 3,\\n                  'B-Evidence': 4,\\n                  'I-Evidence': 5,\\n                  'B-Claim': 6,\\n                  'I-Claim': 7,\\n                  'B-Concluding Statement': 8,\\n                  'I-Concluding Statement': 9,\\n                  'B-Counterclaim': 10,\\n                  'I-Counterclaim': 11,\\n                  'B-Rebuttal': 12,\\n                  'I-Rebuttal': 13}\\n\\n\\ninverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\\ninverted_ner_labels[-100] = 'Special Token'\\n\\ntest_files = os.listdir(TEST_DIR)\\n\\n# accepts file path, returns tuple of (file_ID, txt split, NER labels)\\ndef generate_text_for_file(input_filename):\\n    curr_id = input_filename.split('.')[0]\\n    with open(os.path.join(TEST_DIR, input_filename)) as f:\\n        curr_txt = f.read()\\n\\n    return curr_id, curr_txt\\n\\nwith mp.Pool(NUM_CORES) as p:\\n    ner_test_rows = p.map(generate_text_for_file, test_files)\\n    \\nif (\\\"v3\\\" in args.save_name)|(\\\"v2\\\" in args.save_name):\\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(PRETRAINED_MODEL_PATHS[0])\\nelse:\\n    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\\n# Check is rust-based fast tokenizer\\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\\n\\nner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\\n\\n# tokenize and store word ids\\ndef tokenize_with_word_ids(ner_raw_data):\\n    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\\n    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data], \\n                                 max_length=MAX_SEQ_LENGTH,\\n                                 return_offsets_mapping=True,\\n                                 truncation=True)\\n    \\n    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\\n    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\\n    \\n    return tokenized_inputs\\n\\ntokenized_all = tokenize_with_word_ids(ner_test_rows)\\n\\nclass NERDataset(Dataset):\\n    def __init__(self, input_dict):\\n        self.input_dict = input_dict\\n        \\n    def __getitem__(self, index):\\n        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\\n    \\n    def get_filename(self, index):\\n        return self.input_dict['id'][index]\\n    \\n    def get_offset(self, index):\\n        return self.input_dict['offset_mapping'][index]\\n    \\n    def __len__(self):\\n        return len(self.input_dict['input_ids'])\\n\\ntest_dataset = NERDataset(tokenized_all)\\n\\nsoft_predictions = None\\nhfargs = TrainingArguments(output_dir='None',\\n                         log_level='warning',\\n                         per_device_eval_batch_size=BATCH_SIZE)\\n\\nfor idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\\n\\n    if args.save_name == \\\"longformerwithlstm\\\":\\n        model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\\n    elif args.save_name == \\\"debertawithlstm\\\":\\n        model = DebertaForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\\n    else:\\n        model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\\n    trainer = Trainer(model,\\n                      hfargs,\\n                      tokenizer=tokenizer)\\n    \\n    curr_preds, _, _ = trainer.predict(test_dataset)\\n    curr_preds = curr_preds.astype(np.float16)\\n    curr_preds = softmax(curr_preds, -1)\\n\\n    if soft_predictions is not None:\\n        soft_predictions = soft_predictions + curr_preds\\n    else:\\n        soft_predictions = curr_preds\\n        \\n    del model, trainer, curr_preds\\n    gc.collect()\\n\\nsoft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\\n\\nsoft_claim_predictions = soft_predictions[:, :, 8]\\n\\npredictions = np.argmax(soft_predictions, axis=2)\\nsoft_predictions = np.max(soft_predictions, axis=2)\\n\\ndef generate_token_to_word_mapping(txt, offset):\\n    # GET WORD POSITIONS IN CHARS\\n    w = []\\n    blank = True\\n    for i in range(len(txt)):\\n        if not txt[i].isspace() and blank==True:\\n            w.append(i)\\n            blank=False\\n        elif txt[i].isspace():\\n            blank=True\\n    w.append(1e6)\\n\\n    # MAPPING FROM TOKENS TO WORDS\\n    word_map = -1 * np.ones(len(offset),dtype='int32')\\n    w_i = 0\\n    for i in range(len(offset)):\\n        if offset[i][1]==0: continue\\n        while offset[i][0]>=(w[w_i+1]-(\\\"debertal_chris\\\" in args.save_name)-(\\\"v3\\\" in args.save_name)\\\\\\n                             -(\\\"v2\\\" in args.save_name) ): w_i += 1\\n        word_map[i] = int(w_i)\\n\\n    return word_map\\n\\nall_preds = []\\n\\n# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\\nfor curr_sample_id in range(len(test_dataset)):\\n    curr_preds = []\\n    sample_preds = predictions[curr_sample_id]\\n    sample_offset = test_dataset.get_offset(curr_sample_id)\\n    sample_txt = ner_test_rows[curr_sample_id][1]\\n    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\\n\\n    word_preds = [''] * (max(sample_word_map) + 1)\\n    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\\n    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\\n\\n    for i, curr_word_id in enumerate(sample_word_map):\\n        if curr_word_id != -1:\\n            if word_preds[curr_word_id] == '': # only use 1st subword\\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\\n            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\\n\\n    # Dict to hold Lead, Position, Concluding Statement\\n    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\\n\\n    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\\n    for j in range(1, len(word_preds) - 1):\\n        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\\n        splitted_trio = [x.split('-')[0] for x in pred_trio]\\n        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\\n            word_preds[j] = word_preds[j-1]\\n\\n    # B-X, ? (not B), I-X -> change ? to I-X\\n    for j in range(1, len(word_preds) - 1):\\n        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\\\"I-{word_preds[j-1].split('-')[-1]}\\\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\\n            word_preds[j] = word_preds[j+1]\\n\\n     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\\n    for j in range(1, len(word_preds) - 1):\\n        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\\n            word_preds[j] = word_preds[j-1]\\n\\n    j = 0 # start of candidate discourse\\n    while j < len(word_preds): \\n        cls = word_preds[j] \\n        cls_splitted = cls.split('-')[-1]\\n        end = j + 1 # try to extend discourse as far as possible\\n\\n        if word_probs[j] > 0.54: \\n            # Must match suffix i.e., I- to I- only; no B- to I-\\n            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\\n                end += 1\\n            # if we're here, end is not the same pred as start\\n            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\\n                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\\n                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\\n                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\\n                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\\n                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\\n                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\\n                            \\n                            # Delete other preds that lie inside the joined LC discourse\\n                            for l in range(len(curr_preds) - 1, 0, -1):\\n                                check_span = curr_preds[l][2]\\n                                check_start, check_end = int(check_span[0]), int(check_span[-1])\\n                                if check_start > prev_start and check_end < end:\\n                                    del curr_preds[l]\\n                            \\n                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\\n                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\\n                    else: # Add to it\\n                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\\n                else:\\n                    # Lookback and add preceding I- tokens\\n                    while j - 1 > 0 and word_preds[j-1] == cls:\\n                        j = j - 1\\n                    # Try to add the matching B- tag if immediately precedes the current I- sequence\\n                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\\n                        j = j - 1\\n\\n\\n                    #############################################################\\n                    # Run a bunch of adjustments to discourse predictions based on CV \\n                    adj_start, adj_end = j, end + 1\\n\\n                    # Run some heuristics against previous discourse\\n                    if len(curr_preds) > 0:\\n                        prev_span = list(map(int, curr_preds[-1][2].split()))\\n                        prev_start, prev_end = prev_span[0], prev_span[-1]\\n\\n                        # Join adjacent rebuttals\\n                        if cls_splitted in 'Rebuttal':                        \\n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\\n                                del curr_preds[-1]\\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \\n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \\n                                                   cls_splitted, \\n                                                   ' '.join(map(str, combined_list)),\\n                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\\n                                j = end\\n                                continue\\n                                \\n                        elif cls_splitted in 'Counterclaim':                        \\n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\\n                                del curr_preds[-1]\\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \\n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \\n                                                   cls_splitted, \\n                                                   ' '.join(map(str, combined_list)),\\n                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\\n                                j = end\\n                                continue\\n\\n                        elif cls_splitted in 'Evidence':                        \\n                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\\n                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\\n                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\\n                                    if len(claim_tokens) > 2:\\n                                        curr_preds.append((test_dataset.get_filename(curr_sample_id), \\n                                                           'Claim', \\n                                                           ' '.join(claim_tokens),\\n                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\\n                        # If gap with discourse of same type, extend to it \\n                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\\n                            adj_start -= 1\\n\\n                    # Adjust discourse lengths if too long or short\\n                    if cls_splitted == 'Evidence':\\n                        if adj_end - adj_start < 45:\\n                            adj_start -= 9\\n                        else:\\n                            adj_end -= 1\\n                    elif cls_splitted == 'Claim':\\n                        if adj_end - adj_start > 24:\\n                            adj_end -= 1\\n                    elif cls_splitted == 'Counterclaim':\\n                        if adj_end - adj_start > 24:\\n                            adj_end -= 1\\n                        else:\\n                            adj_start -= 1\\n                            adj_end += 1\\n                    elif cls_splitted == 'Rebuttal':\\n                        if adj_end - adj_start > 32:\\n                            adj_end -= 1\\n                        else:\\n                            adj_start -= 1\\n                            adj_end += 1\\n                    adj_start = max(0, adj_start)\\n                    adj_end = min(len(word_preds) - 1, adj_end)\\n                    curr_preds.append((test_dataset.get_filename(curr_sample_id), \\n                                       cls_splitted, \\n                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\\n                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\\n\\n        j = end \\n\\n    # Add the Lead, Position, Concluding Statement\\n    for k, v in let_one_dict.items():\\n        pred_start = v[1]\\n        pred_end = v[2]\\n\\n        # Lookback and add preceding I- tokens\\n        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\\n            pred_start = pred_start - 1\\n        # Try to add the matching B- tag if immediately precedes the current I- sequence\\n        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\\n            pred_start = pred_start - 1\\n\\n        # Extend short Leads and Concluding Statements\\n        if k == 'Lead':\\n            if pred_end - pred_start < 33:\\n                pred_end = min(len(word_preds), pred_end + 5)\\n            else:\\n                pred_end -= 5\\n        elif k == 'Concluding Statement':\\n            if pred_end - pred_start < 23:\\n                pred_start = max(0, pred_start - 1)\\n                pred_end = min(len(word_preds), pred_end + 10)\\n        elif k == 'Position':\\n            if pred_end - pred_start < 18:\\n                pred_end = min(len(word_preds), pred_end + 3)\\n\\n        pred_start = max(0, pred_start)\\n        if pred_end - pred_start > 6:\\n            curr_preds.append((test_dataset.get_filename(curr_sample_id), \\n                               k, \\n                               ' '.join(map(str, list(range(pred_start, pred_end)))),\\n                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\\n\\n    all_preds.extend(curr_preds)\\n\\noutput_df = pd.DataFrame(all_preds)\\noutput_df.columns = ['id', 'class', 'predictionstring', 'scores']\\noutput_df.to_csv(f'{args.save_name}.csv', index=False)\",\"metadata\":{\"_kg_hide-input\":true,\"_kg_hide-output\":true,\"execution\":{\"iopub.status.busy\":\"2022-03-13T07:39:00.274578Z\",\"iopub.execute_input\":\"2022-03-13T07:39:00.274891Z\",\"iopub.status.idle\":\"2022-03-13T07:39:00.313298Z\",\"shell.execute_reply.started\":\"2022-03-13T07:39:00.274808Z\",\"shell.execute_reply\":\"2022-03-13T07:39:00.312379Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# Weighted Box Fusion\\nThe following code comes from ZFTurbo's GitHub [here][1]. First, the text `predictionstring` are converted to 1 dimensional boxes. Next they are ensembled with ZFTurbo's WBF code. **Click \\\"show hidden cell\\\" to see the code.**\\n\\n[1]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"'''\\nCode taken and modified for 1D sequences from:\\nhttps://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_wbf.py\\n'''\\nimport warnings\\nimport numpy as np\\n\\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\\n    # Create dict with boxes stored by its label\\n    new_boxes = dict()\\n\\n    for t in range(len(boxes)):\\n\\n        if len(boxes[t]) != len(scores[t]):\\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\\n            exit()\\n\\n        for j in range(len(boxes[t])):\\n            score = scores[t][j]\\n            if score < thr:\\n                continue\\n            label = labels[t][j]\\n            box_part = boxes[t][j]\\n\\n            x = float(box_part[0])\\n            y = float(box_part[1])\\n\\n            # Box data checks\\n            if y < x:\\n                warnings.warn('Y < X value in box. Swap them.')\\n                x, y = y, x\\n\\n            # [label, score, weight, model index, x, y]\\n            b = [label, float(score) * weights[t], weights[t], t, x, y]\\n            if label not in new_boxes:\\n                new_boxes[label] = []\\n            new_boxes[label].append(b)\\n\\n    # Sort each list in dict by score and transform it to numpy array\\n    for k in new_boxes:\\n        current_boxes = np.array(new_boxes[k])\\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\\n\\n    return new_boxes\\n\\n\\ndef get_weighted_box(boxes, conf_type='avg'):\\n    \\\"\\\"\\\"\\n    Create weighted box for set of boxes\\n    :param boxes: set of boxes to fuse\\n    :param conf_type: type of confidence one of 'avg' or 'max'\\n    :return: weighted box (label, score, weight, model index, x, y)\\n    \\\"\\\"\\\"\\n\\n    box = np.zeros(6, dtype=np.float32)\\n    conf = 0\\n    conf_list = []\\n    w = 0\\n    for b in boxes:\\n        box[4:] += (b[1] * b[4:])\\n        conf += b[1]\\n        conf_list.append(b[1])\\n        w += b[2]\\n    box[0] = boxes[0][0]\\n    if conf_type == 'avg':\\n        box[1] = conf / len(boxes)\\n    elif conf_type == 'max':\\n        box[1] = np.array(conf_list).max()\\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\\n        box[1] = conf / len(boxes)\\n    box[2] = w\\n    box[3] = -1 # model index field is retained for consistensy but is not used.\\n    box[4:] /= conf\\n    return box\\n\\n\\ndef find_matching_box_quickly(boxes_list, new_box, match_iou):\\n    \\\"\\\"\\\" \\n        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\\n        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\\n\\n        boxes_list: shape: (N, label, score, weight, model index, x, y)\\n        new_box: shape: (label, score, weight, model index, x, y)\\n    \\\"\\\"\\\"\\n    def bb_iou_array(boxes, new_box):\\n        '''\\n        boxes: shape: (N, x, y)\\n        new_box: shape: (x, y)\\n        '''\\n        # bb interesection over union\\n        x_min = np.minimum(boxes[:, 0], new_box[0])\\n        x_max = np.maximum(boxes[:, 0], new_box[0])\\n        y_min = np.minimum(boxes[:, 1], new_box[1])+1\\n        y_max = np.maximum(boxes[:, 1], new_box[1])+1\\n\\n        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\\n\\n        return iou\\n\\n    if boxes_list.shape[0] == 0:\\n        return -1, match_iou\\n\\n    # boxes = np.array(boxes_list)\\n    boxes = boxes_list\\n\\n    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\\n\\n    ious[boxes[:, 0] != new_box[0]] = -1\\n\\n    best_idx = np.argmax(ious)\\n    best_iou = ious[best_idx]\\n\\n    if best_iou <= match_iou:\\n        best_iou = match_iou\\n        best_idx = -1\\n\\n    return best_idx, best_iou\\n\\n\\ndef weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\\n    '''\\n    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\\n     It has 3 dimensions (models_number, model_preds, 2)\\n     Order of boxes: x, y.\\n    :param scores_list: list of scores for each model\\n    :param labels_list: list of labels for each model\\n    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\\n    :param iou_thr: IoU value for boxes to be a match\\n    :param skip_box_thr: exclude boxes with score lower than this variable\\n    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\\n    :param allows_overflow: false if we want confidence score not exceed 1.0\\n\\n    :return: boxes: boxes coordinates (Order of boxes: x, y).\\n    :return: scores: confidence scores\\n    :return: labels: boxes labels\\n    '''\\n\\n    if weights is None:\\n        weights = np.ones(len(boxes_list))\\n    if len(weights) != len(boxes_list):\\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\\n        weights = np.ones(len(boxes_list))\\n    weights = np.array(weights)\\n\\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\\n        print('Unknown conf_type: {}. Must be \\\"avg\\\", \\\"max\\\" or \\\"box_and_model_avg\\\", or \\\"absent_model_aware_avg\\\"'.format(conf_type))\\n        exit()\\n\\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\\n    if len(filtered_boxes) == 0:\\n        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\\n\\n    overall_boxes = []\\n    for label in filtered_boxes:\\n        boxes = filtered_boxes[label]\\n        new_boxes = []\\n        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\\n        # Clusterize boxes\\n        for j in range(0, len(boxes)):\\n            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\\n\\n            if index != -1:\\n                new_boxes[index].append(boxes[j])\\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\\n            else:\\n                new_boxes.append([boxes[j].copy()])\\n                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\\n\\n        # Rescale confidence based on number of models and boxes\\n        for i in range(len(new_boxes)):\\n            clustered_boxes = np.array(new_boxes[i])\\n            if conf_type == 'box_and_model_avg':\\n                # weighted average for boxes\\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\\n                # identify unique model index by model index column\\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\\n                # rescale by unique model weights\\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\\n            elif conf_type == 'absent_model_aware_avg':\\n                # get unique model index in the cluster\\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\\n                # create a mask to get unused model weights\\n                mask = np.ones(len(weights), dtype=bool)\\n                mask[models] = False\\n                # absent model aware weighted average\\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\\n            elif conf_type == 'max':\\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\\n            elif not allows_overflow:\\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\\n            else:\\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\\n        \\n        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\\n        #for i in range(len(new_boxes)):\\n        #    clustered_boxes = np.array(new_boxes[i])\\n        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\\n        #        overall_boxes.append(weighted_boxes[i])\\n                \\n        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \\\"REQUIRE TWO MODELS\\\" ABOVE\\n    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \\\"REQUIRE TWO MODELS\\\" ABOVE\\n    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \\\"REQUIRE TWO MODELS\\\" ABOVE\\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\\n    boxes = overall_boxes[:, 4:]\\n    scores = overall_boxes[:, 1]\\n    labels = overall_boxes[:, 0]\\n    return boxes, scores, labels\\n\",\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.status.busy\":\"2022-03-13T07:39:00.315048Z\",\"iopub.execute_input\":\"2022-03-13T07:39:00.315488Z\",\"iopub.status.idle\":\"2022-03-13T07:39:00.434581Z\",\"shell.execute_reply.started\":\"2022-03-13T07:39:00.315448Z\",\"shell.execute_reply\":\"2022-03-13T07:39:00.433364Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# Our NLP models trained on NER task\\nHere is a summary of our models and their individual performance:\\n\\n| Hugging Face Model | CV | Public LB | Private LB | special |\\n| --- | --- | | | |\\n| microsoft/deberta-large | 706 | 710 | 721 | trained with 100% train data|\\n| microsoft/deberta-large | 699 | 706 | 719 | add lstm, add jaccard loss |\\n| microsoft/deberta-v3-large | 705 | |  | [convert slow tokenizer to fast][9] |\\n| microsoft/deberta-xlarge | 708 | 704 | 713 | |\\n| microsoft/deberta-v2-xlarge | 702 |  |  | [convert slow tokenizer to fast][9] |\\n| allenai/longformer-large-4096 | 702 | 705 | 716 | add lstm head|\\n| [LSG converted roberta][1] | 703 | 702 | 714 | convert 512 roberta to 1536 |\\n| funnel-transformer/large | 688 | 689 | 708 |\\n| google/bigbird-roberta-base | 675 | 676 | 692 | train 1024 infer 1024 |\\n| uw-madison/yoso-4096 | 652 | 655 | 668 | lsh_backward=False |\\n\\n[1]: https://github.com/ccdv-ai/convert_checkpoint_to_lsg\\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\\n[9]: https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"!python generate_preds.py --model_paths ../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 \\\\\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f2/checkpoint-7500 \\\\\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f5/checkpoint-6000 \\\\\\n                            --save_name longformerwithlstm --max_len 1536\\n\\n!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\\\\n                                        ../input/deberta-large-100/fold1 \\\\\\n                                        ../input/deberta-large-100/fold2 \\\\\\n                            --save_name debertal_chris --max_len 1536\\n\\n!python generate_preds.py --model_paths ../input/deberta-large-v2/deberta-large-v2100-f0/checkpoint-10500 \\\\\\n                                        ../input/deberta-large-v2/deberta-large-v2101-f1/checkpoint-11500 \\\\\\n                                        ../input/deberta-large-v2/deberta-large-v2102-f2/checkpoint-8500 \\\\\\n                            --save_name debertal --max_len 1536\\n\\n!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\\\\n                                        ../input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\\\\n                            --save_name debertaxl --max_len 1536\\n\\n!python generate_preds.py --model_paths ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6000-f0/checkpoint-7500 \\\\\\n                                        ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6003-f3/checkpoint-9000 \\\\\\n                            --save_name deberta_v2 --max_len 1536\\n\\n!python generate_preds.py --model_paths ../input/deberta-lstm-jaccard/jcl-deberta-large-f1/checkpoint-4500 \\\\\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f2/checkpoint-5000 \\\\\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f3/checkpoint-3500 \\\\\\n                            --save_name debertawithlstm --max_len 1536\\n\\n!python generate_preds.py --model_paths  ../input/funnel-large-6folds/large-v628-f1/checkpoint-11500 \\\\\\n                                         ../input/funnel-large-6folds/large-v627-f3/checkpoint-11000 \\\\\\n                                         ../input/funnel-large-6folds/large-v623-f4/checkpoint-10500 \\\\\\n                            --save_name funnel --max_len 1536\\n\\n!python generate_preds.py --model_paths  ../input/auglsgrobertalarge/lsg-roberta-large-0/checkpoint-6750 \\\\\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-2/checkpoint-7000 \\\\\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-5/checkpoint-6500 \\\\\\n                             --save_name lsg --max_len 1536\\n\\n!python generate_preds.py --model_paths  ../input/bird-base/fold1 \\\\\\n                                         ../input/bird-base/fold3 \\\\\\n                                         ../input/bird-base/fold5 \\\\\\n                            --save_name bigbird_base_chris --max_len 1024\\n\\n!python generate_preds.py --model_paths  ../input/feedbackyoso/yoso-4096-0/checkpoint-12500 \\\\\\n                                         ../input/feedbackyoso/yoso-4096-2/checkpoint-11000 \\\\\\n                                         ../input/feedbackyoso/yoso-4096-4/checkpoint-12500 \\\\\\n                            --save_name yoso --max_len 1536\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-03-13T07:39:00.721366Z\",\"iopub.execute_input\":\"2022-03-13T07:39:00.721767Z\"},\"_kg_hide-output\":true,\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"import pandas as pd\\nimport numpy as np, os\\n\\nlongformer_csv = pd.read_csv(\\\"longformerwithlstm.csv\\\").dropna()\\ndeberta_v3_csv = pd.read_csv(\\\"debertawithlstm.csv\\\").dropna()\\ndeberta_v2_csv = pd.read_csv(\\\"deberta_v2.csv\\\").dropna()\\ndebertaxl_csv = pd.read_csv(\\\"debertaxl.csv\\\").dropna()\\ndebertal_chris_csv = pd.read_csv(\\\"debertal_chris.csv\\\").dropna()\\ndebertal_csv = pd.read_csv(\\\"debertal.csv\\\").dropna()\\nyoso_csv = pd.read_csv(\\\"yoso.csv\\\").dropna()\\nfunnel_csv = pd.read_csv(\\\"funnel.csv\\\").dropna()\\nbird_base_chris_csv = pd.read_csv(\\\"bigbird_base_chris.csv\\\").dropna()\\nlsg_csv = pd.read_csv(\\\"lsg.csv\\\").dropna()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-02-23T10:00:32.486594Z\",\"iopub.execute_input\":\"2022-02-23T10:00:32.487659Z\",\"iopub.status.idle\":\"2022-02-23T10:00:32.591336Z\",\"shell.execute_reply.started\":\"2022-02-23T10:00:32.487604Z\",\"shell.execute_reply\":\"2022-02-23T10:00:32.589768Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# Ensemble Models with WBF\\nWe will now read in the 10 submission files generated above and apply WBF to ensemble them. After applying WBF, it is important to remove predictions with confidence score below threshold. This is explained [here][1]. \\n\\nIf only 1 model out of 10 models makes a certain span prediction, that prediction will still be present in WBF's outcome. However that prediction will have a very low confidence score because that model's confidence score will be averaged with 9 zero confidence scores. We found optimal confidence scores per class by analyzing our CV OOF score. For each class, we vary the threshold and compute the corresponding class metric score.\\n\\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2022/conf_scores.png)\\n\\n[1]: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307609\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"TEST_DIR = '../input/feedback-prize-2021/test/'\\ntest_files = os.listdir(TEST_DIR)\\nv_ids = [f.replace('.txt','') for f in test_files]\",\"metadata\":{\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"import math\\n\\nclass_to_label = {\\n    'Claim': 0, \\n    'Evidence': 1, \\n    'Lead':2, \\n    'Position':3, \\n    'Concluding Statement':4,\\n    'Counterclaim':5, \\n    'Rebuttal':6\\n}\\n\\n# Threshold found from CV\\nlabel_to_threshold = {\\n    0 : 0.275, #Claim\\n    1 : 0.375, #Evidence\\n    2 : 0.325, #Lead\\n    3 : 0.325, #Position\\n    4 : 0.4, #Concluding Statement\\n    5 : 0.275, #Counterclaim\\n    6 : 0.275 #Rebuttal\\n}\\n\\nlabel_to_class = {v:k for k, v in class_to_label.items()}\\n\\ndef preprocess_for_wbf(df_list):\\n    boxes_list=[]\\n    scores_list=[]\\n    labels_list=[]\\n    \\n    for df in df_list:\\n        scores_list.append(df['scores'].values.tolist())\\n        labels_list.append(df['class'].map(class_to_label).values.tolist())\\n        predictionstring = df.predictionstring.str.split().values\\n        df_box_list = []\\n        for bb in predictionstring:\\n            df_box_list.append([int(bb[0]), int(bb[-1])])\\n        boxes_list.append(df_box_list)\\n    return boxes_list, scores_list, labels_list\\n\\ndef postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\\n    preds = []\\n    for box, score, label in zip(boxes_list, scores_list, labels_list):\\n        if score > label_to_threshold[label]: \\n            start = math.ceil(box[0])\\n            end = int(box[1])\\n            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)])))\\n    return preds\\n\\ndef generate_wbf_for_id(i):\\n    df1 = debertal_csv[debertal_csv['id']==i]\\n    df2 = debertal_chris_csv[debertal_chris_csv['id']==i]\\n    df3 = funnel_csv[funnel_csv['id']==i]\\n    df4 = debertaxl_csv[debertaxl_csv['id']==i]\\n    df5 = longformer_csv[longformer_csv['id']==i]\\n    df6 = deberta_v3_csv[deberta_v3_csv['id']==i]\\n    df7 = yoso_csv[yoso_csv['id']==i]\\n    df8 = bird_base_chris_csv[bird_base_chris_csv['id']==i]\\n    df9 = lsg_csv[lsg_csv['id']==i]\\n    df10 = deberta_v2_csv[deberta_v2_csv['id']==i]\\n    \\n    boxes_list, scores_list, labels_list = preprocess_for_wbf([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\\n    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\\n\\n    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)\",\"metadata\":{\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"import multiprocessing as mp\\n\\nwith mp.Pool(2) as p:\\n    list_of_list = p.map(generate_wbf_for_id, v_ids)\\n\\npreds = [x for sub_list in list_of_list for x in sub_list]\",\"metadata\":{},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"sub = pd.DataFrame(preds)\\nsub.columns = [\\\"id\\\", \\\"class\\\", \\\"predictionstring\\\"]\\nsub.to_csv('submission.csv', index=False)\",\"metadata\":{\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# Visualize Test Predictions\\nBelow we visualize the test predictions\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from pathlib import Path\\nfrom spacy import displacy\\n\\ntest_path = Path('../input/feedback-prize-2021/test')\\n\\ncolors = {\\n            'Lead': '#8000ff',\\n            'Position': '#2b7ff6',\\n            'Evidence': '#2adddd',\\n            'Claim': '#80ffb4',\\n            'Concluding Statement': 'd4dd80',\\n            'Counterclaim': '#ff8042',\\n            'Rebuttal': '#ff0000',\\n            'Other': '#007f00',\\n         }\\n\\ndef get_test_text(ids):\\n    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\\n    return data\\n\\ndef visualize(df):\\n    ids = df[\\\"id\\\"].unique()\\n    for i in range(len(ids)):\\n        ents = []\\n        example = ids[i]\\n        curr_df = df[df[\\\"id\\\"]==example]\\n        text = \\\" \\\".join(get_test_text(example).split())\\n        splitted_text = text.split()\\n        for i, row in curr_df.iterrows():\\n            predictionstring = row['predictionstring']\\n            predictionstring = predictionstring.split()\\n            wstart = int(predictionstring[0])\\n            wend = int(predictionstring[-1])\\n            ents.append({\\n                             'start': len(\\\" \\\".join(splitted_text[:wstart])), \\n                             'end': len(\\\" \\\".join(splitted_text[:wend+1])), \\n                             'label': row['class']\\n                        })\\n        ents = sorted(ents, key = lambda i: i['start'])\\n\\n        doc2 = {\\n            \\\"text\\\": text,\\n            \\\"ents\\\": ents,\\n            \\\"title\\\": example\\n        }\\n\\n        options = {\\\"ents\\\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \\\"colors\\\": colors}\\n        displacy.render(doc2, style=\\\"ent\\\", options=options, manual=True, jupyter=True)\",\"metadata\":{\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"if len(sub[\\\"id\\\"].unique())==5:\\n    visualize(sub)\",\"metadata\":{\"trusted\":true},\"execution_count\":null,\"outputs\":[]}]}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb b/dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb
--- a/dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/feedback-prize-2021/Chris Deotte/2nd-place-solution-cv741-public727-private740.ipynb	(date 1658512097829)
@@ -1,1 +1,172 @@
-{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 2nd Place Solution - CV 741, Public 727, Private 740\nWe (Chun Ming Lee @leecming , Udbhav Bamba @ubamba98, and Chris Deotte @cdeotte ) are excited to present our 2nd place solution to Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Thank you Georgia State University, The Learning Agency Lab, and Kaggle for an awesome competition.\n\nOur full solution write up is [here][1]. The main ingredients to our solution are \n* powerful post process per model\n* huge variety of NLP models trained on NER task\n* ensemble with weighted box fusion (from ZFTurbo's GitHub [here][3]). \n\n[1]: https://www.kaggle.com/c/feedback-prize-2021/discussion/313389\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion","metadata":{}},{"cell_type":"markdown","source":"# Inference Script with Post Process\nThe following Python script accepts a filename of a saved model, infers the test texts, applies post process, and writes a `submission.csv` file with an extra column of confidence scores per span (i.e. the probability that this span is correct). **Click \"show hidden cell\" to see the code.**","metadata":{}},{"cell_type":"code","source":"%%writefile generate_preds.py\n\nimport os\nimport argparse\n\nap = argparse.ArgumentParser()\nap.add_argument('--model_paths', nargs='+', required=True)\nap.add_argument(\"--save_name\", type=str, required=True)\nap.add_argument(\"--max_len\", type=int, required=True)\nargs = ap.parse_args()\n\nif args.save_name == \"yoso\":\n    os.system(\"cp -r ../input/hf-transformers/transformers-4.16.0 .\")\n    os.system(\"pip install -U --no-build-isolation --no-deps /kaggle/working/transformers-4.16.0\")\n    \nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\n    # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n    # This must be done before importing transformers\n    import shutil\n    from pathlib import Path\n\n    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\n    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\n    convert_file = input_dir / \"convert_slow_tokenizer.py\"\n    conversion_path = transformers_path/convert_file.name\n\n    if conversion_path.exists():\n        conversion_path.unlink()\n\n    shutil.copy(convert_file, transformers_path)\n    deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\n    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n        filepath = deberta_v2_path/filename\n        if filepath.exists():\n            filepath.unlink()\n\n        shutil.copy(input_dir/filename, filepath)\n\nif args.save_name == \"longformerwithlstm\":\n    os.system(\"cp -r ../input/longformerwithbilstmhead/model.py .\")\n    from model import LongformerForTokenClassificationwithbiLSTM\n    \nif args.save_name == \"debertawithlstm\":\n    os.system(\"cp -r ../input/deberta-lstm/model.py .\")\n    from model import DebertaForTokenClassificationwithbiLSTM\n        \nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport multiprocessing as mp\nfrom scipy.special import softmax\nfrom torch.utils.data import Dataset\nfrom transformers import (AutoModelForTokenClassification, \n                          AutoTokenizer, \n                          TrainingArguments, \n                          Trainer)\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nNUM_CORES = 16\nBATCH_SIZE = 4\nMAX_SEQ_LENGTH = args.max_len\nPRETRAINED_MODEL_PATHS = args.model_paths\nif \"debertal_chris\" in args.save_name:\n    print('==> using -1 in offset mapping...')\nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    print('==> using -1 in offset mapping...')\n    \nAGG_FUNC = np.mean\nprint('==> using span token mean...')\n\nTEST_DIR = '../input/feedback-prize-2021/test/'\n\nMIN_TOKENS = {\n    \"Lead\": 32,\n    \"Position\": 5,\n    \"Evidence\": 35,\n    \"Claim\": 7,\n    \"Concluding Statement\": 6,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 6\n}\n\nif \"chris\" not in args.save_name:\n    ner_labels = {'O': 0,\n                  'B-Lead': 1,\n                  'I-Lead': 2,\n                  'B-Position': 3,\n                  'I-Position': 4,\n                  'B-Evidence': 5,\n                  'I-Evidence': 6,\n                  'B-Claim': 7,\n                  'I-Claim': 8,\n                  'B-Concluding Statement': 9,\n                  'I-Concluding Statement': 10,\n                  'B-Counterclaim': 11,\n                  'I-Counterclaim': 12,\n                  'B-Rebuttal': 13,\n                  'I-Rebuttal': 14}\nelse:\n    print(\"==> Using Chris BIO\")\n    ner_labels = {'O': 14,\n                  'B-Lead': 0,\n                  'I-Lead': 1,\n                  'B-Position': 2,\n                  'I-Position': 3,\n                  'B-Evidence': 4,\n                  'I-Evidence': 5,\n                  'B-Claim': 6,\n                  'I-Claim': 7,\n                  'B-Concluding Statement': 8,\n                  'I-Concluding Statement': 9,\n                  'B-Counterclaim': 10,\n                  'I-Counterclaim': 11,\n                  'B-Rebuttal': 12,\n                  'I-Rebuttal': 13}\n\n\ninverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\ninverted_ner_labels[-100] = 'Special Token'\n\ntest_files = os.listdir(TEST_DIR)\n\n# accepts file path, returns tuple of (file_ID, txt split, NER labels)\ndef generate_text_for_file(input_filename):\n    curr_id = input_filename.split('.')[0]\n    with open(os.path.join(TEST_DIR, input_filename)) as f:\n        curr_txt = f.read()\n\n    return curr_id, curr_txt\n\nwith mp.Pool(NUM_CORES) as p:\n    ner_test_rows = p.map(generate_text_for_file, test_files)\n    \nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(PRETRAINED_MODEL_PATHS[0])\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n# Check is rust-based fast tokenizer\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n\nner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\n\n# tokenize and store word ids\ndef tokenize_with_word_ids(ner_raw_data):\n    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\n    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data], \n                                 max_length=MAX_SEQ_LENGTH,\n                                 return_offsets_mapping=True,\n                                 truncation=True)\n    \n    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\n    \n    return tokenized_inputs\n\ntokenized_all = tokenize_with_word_ids(ner_test_rows)\n\nclass NERDataset(Dataset):\n    def __init__(self, input_dict):\n        self.input_dict = input_dict\n        \n    def __getitem__(self, index):\n        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\n    \n    def get_filename(self, index):\n        return self.input_dict['id'][index]\n    \n    def get_offset(self, index):\n        return self.input_dict['offset_mapping'][index]\n    \n    def __len__(self):\n        return len(self.input_dict['input_ids'])\n\ntest_dataset = NERDataset(tokenized_all)\n\nsoft_predictions = None\nhfargs = TrainingArguments(output_dir='None',\n                         log_level='warning',\n                         per_device_eval_batch_size=BATCH_SIZE)\n\nfor idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\n\n    if args.save_name == \"longformerwithlstm\":\n        model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n    elif args.save_name == \"debertawithlstm\":\n        model = DebertaForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n    else:\n        model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\n    trainer = Trainer(model,\n                      hfargs,\n                      tokenizer=tokenizer)\n    \n    curr_preds, _, _ = trainer.predict(test_dataset)\n    curr_preds = curr_preds.astype(np.float16)\n    curr_preds = softmax(curr_preds, -1)\n\n    if soft_predictions is not None:\n        soft_predictions = soft_predictions + curr_preds\n    else:\n        soft_predictions = curr_preds\n        \n    del model, trainer, curr_preds\n    gc.collect()\n\nsoft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\n\nsoft_claim_predictions = soft_predictions[:, :, 8]\n\npredictions = np.argmax(soft_predictions, axis=2)\nsoft_predictions = np.max(soft_predictions, axis=2)\n\ndef generate_token_to_word_mapping(txt, offset):\n    # GET WORD POSITIONS IN CHARS\n    w = []\n    blank = True\n    for i in range(len(txt)):\n        if not txt[i].isspace() and blank==True:\n            w.append(i)\n            blank=False\n        elif txt[i].isspace():\n            blank=True\n    w.append(1e6)\n\n    # MAPPING FROM TOKENS TO WORDS\n    word_map = -1 * np.ones(len(offset),dtype='int32')\n    w_i = 0\n    for i in range(len(offset)):\n        if offset[i][1]==0: continue\n        while offset[i][0]>=(w[w_i+1]-(\"debertal_chris\" in args.save_name)-(\"v3\" in args.save_name)\\\n                             -(\"v2\" in args.save_name) ): w_i += 1\n        word_map[i] = int(w_i)\n\n    return word_map\n\nall_preds = []\n\n# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\nfor curr_sample_id in range(len(test_dataset)):\n    curr_preds = []\n    sample_preds = predictions[curr_sample_id]\n    sample_offset = test_dataset.get_offset(curr_sample_id)\n    sample_txt = ner_test_rows[curr_sample_id][1]\n    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n\n    word_preds = [''] * (max(sample_word_map) + 1)\n    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n\n    for i, curr_word_id in enumerate(sample_word_map):\n        if curr_word_id != -1:\n            if word_preds[curr_word_id] == '': # only use 1st subword\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n\n    # Dict to hold Lead, Position, Concluding Statement\n    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n\n    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n    for j in range(1, len(word_preds) - 1):\n        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n        splitted_trio = [x.split('-')[0] for x in pred_trio]\n        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n            word_preds[j] = word_preds[j-1]\n\n    # B-X, ? (not B), I-X -> change ? to I-X\n    for j in range(1, len(word_preds) - 1):\n        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\"I-{word_preds[j-1].split('-')[-1]}\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\n            word_preds[j] = word_preds[j+1]\n\n     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n    for j in range(1, len(word_preds) - 1):\n        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\n            word_preds[j] = word_preds[j-1]\n\n    j = 0 # start of candidate discourse\n    while j < len(word_preds): \n        cls = word_preds[j] \n        cls_splitted = cls.split('-')[-1]\n        end = j + 1 # try to extend discourse as far as possible\n\n        if word_probs[j] > 0.54: \n            # Must match suffix i.e., I- to I- only; no B- to I-\n            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n                end += 1\n            # if we're here, end is not the same pred as start\n            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\n                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\n                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\n                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n                            \n                            # Delete other preds that lie inside the joined LC discourse\n                            for l in range(len(curr_preds) - 1, 0, -1):\n                                check_span = curr_preds[l][2]\n                                check_start, check_end = int(check_span[0]), int(check_span[-1])\n                                if check_start > prev_start and check_end < end:\n                                    del curr_preds[l]\n                            \n                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n                    else: # Add to it\n                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n                else:\n                    # Lookback and add preceding I- tokens\n                    while j - 1 > 0 and word_preds[j-1] == cls:\n                        j = j - 1\n                    # Try to add the matching B- tag if immediately precedes the current I- sequence\n                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n                        j = j - 1\n\n\n                    #############################################################\n                    # Run a bunch of adjustments to discourse predictions based on CV \n                    adj_start, adj_end = j, end + 1\n\n                    # Run some heuristics against previous discourse\n                    if len(curr_preds) > 0:\n                        prev_span = list(map(int, curr_preds[-1][2].split()))\n                        prev_start, prev_end = prev_span[0], prev_span[-1]\n\n                        # Join adjacent rebuttals\n                        if cls_splitted in 'Rebuttal':                        \n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\n                                del curr_preds[-1]\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                   cls_splitted, \n                                                   ' '.join(map(str, combined_list)),\n                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n                                j = end\n                                continue\n                                \n                        elif cls_splitted in 'Counterclaim':                        \n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\n                                del curr_preds[-1]\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                   cls_splitted, \n                                                   ' '.join(map(str, combined_list)),\n                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n                                j = end\n                                continue\n\n                        elif cls_splitted in 'Evidence':                        \n                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\n                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\n                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\n                                    if len(claim_tokens) > 2:\n                                        curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                           'Claim', \n                                                           ' '.join(claim_tokens),\n                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\n                        # If gap with discourse of same type, extend to it \n                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\n                            adj_start -= 1\n\n                    # Adjust discourse lengths if too long or short\n                    if cls_splitted == 'Evidence':\n                        if adj_end - adj_start < 45:\n                            adj_start -= 9\n                        else:\n                            adj_end -= 1\n                    elif cls_splitted == 'Claim':\n                        if adj_end - adj_start > 24:\n                            adj_end -= 1\n                    elif cls_splitted == 'Counterclaim':\n                        if adj_end - adj_start > 24:\n                            adj_end -= 1\n                        else:\n                            adj_start -= 1\n                            adj_end += 1\n                    elif cls_splitted == 'Rebuttal':\n                        if adj_end - adj_start > 32:\n                            adj_end -= 1\n                        else:\n                            adj_start -= 1\n                            adj_end += 1\n                    adj_start = max(0, adj_start)\n                    adj_end = min(len(word_preds) - 1, adj_end)\n                    curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                       cls_splitted, \n                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\n                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\n\n        j = end \n\n    # Add the Lead, Position, Concluding Statement\n    for k, v in let_one_dict.items():\n        pred_start = v[1]\n        pred_end = v[2]\n\n        # Lookback and add preceding I- tokens\n        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n            pred_start = pred_start - 1\n        # Try to add the matching B- tag if immediately precedes the current I- sequence\n        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n            pred_start = pred_start - 1\n\n        # Extend short Leads and Concluding Statements\n        if k == 'Lead':\n            if pred_end - pred_start < 33:\n                pred_end = min(len(word_preds), pred_end + 5)\n            else:\n                pred_end -= 5\n        elif k == 'Concluding Statement':\n            if pred_end - pred_start < 23:\n                pred_start = max(0, pred_start - 1)\n                pred_end = min(len(word_preds), pred_end + 10)\n        elif k == 'Position':\n            if pred_end - pred_start < 18:\n                pred_end = min(len(word_preds), pred_end + 3)\n\n        pred_start = max(0, pred_start)\n        if pred_end - pred_start > 6:\n            curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                               k, \n                               ' '.join(map(str, list(range(pred_start, pred_end)))),\n                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\n\n    all_preds.extend(curr_preds)\n\noutput_df = pd.DataFrame(all_preds)\noutput_df.columns = ['id', 'class', 'predictionstring', 'scores']\noutput_df.to_csv(f'{args.save_name}.csv', index=False)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-13T07:39:00.274578Z","iopub.execute_input":"2022-03-13T07:39:00.274891Z","iopub.status.idle":"2022-03-13T07:39:00.313298Z","shell.execute_reply.started":"2022-03-13T07:39:00.274808Z","shell.execute_reply":"2022-03-13T07:39:00.312379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Box Fusion\nThe following code comes from ZFTurbo's GitHub [here][1]. First, the text `predictionstring` are converted to 1 dimensional boxes. Next they are ensembled with ZFTurbo's WBF code. **Click \"show hidden cell\" to see the code.**\n\n[1]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion","metadata":{}},{"cell_type":"code","source":"'''\nCode taken and modified for 1D sequences from:\nhttps://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_wbf.py\n'''\nimport warnings\nimport numpy as np\n\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\n    # Create dict with boxes stored by its label\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = labels[t][j]\n            box_part = boxes[t][j]\n\n            x = float(box_part[0])\n            y = float(box_part[1])\n\n            # Box data checks\n            if y < x:\n                warnings.warn('Y < X value in box. Swap them.')\n                x, y = y, x\n\n            # [label, score, weight, model index, x, y]\n            b = [label, float(score) * weights[t], weights[t], t, x, y]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes\n\n\ndef get_weighted_box(boxes, conf_type='avg'):\n    \"\"\"\n    Create weighted box for set of boxes\n    :param boxes: set of boxes to fuse\n    :param conf_type: type of confidence one of 'avg' or 'max'\n    :return: weighted box (label, score, weight, model index, x, y)\n    \"\"\"\n\n    box = np.zeros(6, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf / len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf / len(boxes)\n    box[2] = w\n    box[3] = -1 # model index field is retained for consistensy but is not used.\n    box[4:] /= conf\n    return box\n\n\ndef find_matching_box_quickly(boxes_list, new_box, match_iou):\n    \"\"\" \n        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n\n        boxes_list: shape: (N, label, score, weight, model index, x, y)\n        new_box: shape: (label, score, weight, model index, x, y)\n    \"\"\"\n    def bb_iou_array(boxes, new_box):\n        '''\n        boxes: shape: (N, x, y)\n        new_box: shape: (x, y)\n        '''\n        # bb interesection over union\n        x_min = np.minimum(boxes[:, 0], new_box[0])\n        x_max = np.maximum(boxes[:, 0], new_box[0])\n        y_min = np.minimum(boxes[:, 1], new_box[1])+1\n        y_max = np.maximum(boxes[:, 1], new_box[1])+1\n\n        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\n\n        return iou\n\n    if boxes_list.shape[0] == 0:\n        return -1, match_iou\n\n    # boxes = np.array(boxes_list)\n    boxes = boxes_list\n\n    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n\n    ious[boxes[:, 0] != new_box[0]] = -1\n\n    best_idx = np.argmax(ious)\n    best_iou = ious[best_idx]\n\n    if best_iou <= match_iou:\n        best_iou = match_iou\n        best_idx = -1\n\n    return best_idx, best_iou\n\n\ndef weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n    '''\n    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\n     It has 3 dimensions (models_number, model_preds, 2)\n     Order of boxes: x, y.\n    :param scores_list: list of scores for each model\n    :param labels_list: list of labels for each model\n    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n    :param iou_thr: IoU value for boxes to be a match\n    :param skip_box_thr: exclude boxes with score lower than this variable\n    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n    :param allows_overflow: false if we want confidence score not exceed 1.0\n\n    :return: boxes: boxes coordinates (Order of boxes: x, y).\n    :return: scores: confidence scores\n    :return: labels: boxes labels\n    '''\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\n\n    overall_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n\n        # Rescale confidence based on number of models and boxes\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n            elif conf_type == 'max':\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n            elif not allows_overflow:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n            else:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n        \n        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\n        #for i in range(len(new_boxes)):\n        #    clustered_boxes = np.array(new_boxes[i])\n        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\n        #        overall_boxes.append(weighted_boxes[i])\n                \n        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    return boxes, scores, labels\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-13T07:39:00.315048Z","iopub.execute_input":"2022-03-13T07:39:00.315488Z","iopub.status.idle":"2022-03-13T07:39:00.434581Z","shell.execute_reply.started":"2022-03-13T07:39:00.315448Z","shell.execute_reply":"2022-03-13T07:39:00.433364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Our NLP models trained on NER task\nHere is a summary of our models and their individual performance:\n\n| Hugging Face Model | CV | Public LB | Private LB | special |\n| --- | --- | | | |\n| microsoft/deberta-large | 706 | 710 | 721 | trained with 100% train data|\n| microsoft/deberta-large | 699 | 706 | 719 | add lstm, add jaccard loss |\n| microsoft/deberta-v3-large | 705 | |  | [convert slow tokenizer to fast][9] |\n| microsoft/deberta-xlarge | 708 | 704 | 713 | |\n| microsoft/deberta-v2-xlarge | 702 |  |  | [convert slow tokenizer to fast][9] |\n| allenai/longformer-large-4096 | 702 | 705 | 716 | add lstm head|\n| [LSG converted roberta][1] | 703 | 702 | 714 | convert 512 roberta to 1536 |\n| funnel-transformer/large | 688 | 689 | 708 |\n| google/bigbird-roberta-base | 675 | 676 | 692 | train 1024 infer 1024 |\n| uw-madison/yoso-4096 | 652 | 655 | 668 | lsh_backward=False |\n\n[1]: https://github.com/ccdv-ai/convert_checkpoint_to_lsg\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\n[9]: https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer","metadata":{}},{"cell_type":"code","source":"!python generate_preds.py --model_paths ../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 \\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f2/checkpoint-7500 \\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f5/checkpoint-6000 \\\n                            --save_name longformerwithlstm --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\n                                        ../input/deberta-large-100/fold1 \\\n                                        ../input/deberta-large-100/fold2 \\\n                            --save_name debertal_chris --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-large-v2/deberta-large-v2100-f0/checkpoint-10500 \\\n                                        ../input/deberta-large-v2/deberta-large-v2101-f1/checkpoint-11500 \\\n                                        ../input/deberta-large-v2/deberta-large-v2102-f2/checkpoint-8500 \\\n                            --save_name debertal --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n                                        ../input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\n                            --save_name debertaxl --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6000-f0/checkpoint-7500 \\\n                                        ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6003-f3/checkpoint-9000 \\\n                            --save_name deberta_v2 --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-lstm-jaccard/jcl-deberta-large-f1/checkpoint-4500 \\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f2/checkpoint-5000 \\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f3/checkpoint-3500 \\\n                            --save_name debertawithlstm --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/funnel-large-6folds/large-v628-f1/checkpoint-11500 \\\n                                         ../input/funnel-large-6folds/large-v627-f3/checkpoint-11000 \\\n                                         ../input/funnel-large-6folds/large-v623-f4/checkpoint-10500 \\\n                            --save_name funnel --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/auglsgrobertalarge/lsg-roberta-large-0/checkpoint-6750 \\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-2/checkpoint-7000 \\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-5/checkpoint-6500 \\\n                             --save_name lsg --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/bird-base/fold1 \\\n                                         ../input/bird-base/fold3 \\\n                                         ../input/bird-base/fold5 \\\n                            --save_name bigbird_base_chris --max_len 1024\n\n!python generate_preds.py --model_paths  ../input/feedbackyoso/yoso-4096-0/checkpoint-12500 \\\n                                         ../input/feedbackyoso/yoso-4096-2/checkpoint-11000 \\\n                                         ../input/feedbackyoso/yoso-4096-4/checkpoint-12500 \\\n                            --save_name yoso --max_len 1536","metadata":{"execution":{"iopub.status.busy":"2022-03-13T07:39:00.721366Z","iopub.execute_input":"2022-03-13T07:39:00.721767Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np, os\n\nlongformer_csv = pd.read_csv(\"longformerwithlstm.csv\").dropna()\ndeberta_v3_csv = pd.read_csv(\"debertawithlstm.csv\").dropna()\ndeberta_v2_csv = pd.read_csv(\"deberta_v2.csv\").dropna()\ndebertaxl_csv = pd.read_csv(\"debertaxl.csv\").dropna()\ndebertal_chris_csv = pd.read_csv(\"debertal_chris.csv\").dropna()\ndebertal_csv = pd.read_csv(\"debertal.csv\").dropna()\nyoso_csv = pd.read_csv(\"yoso.csv\").dropna()\nfunnel_csv = pd.read_csv(\"funnel.csv\").dropna()\nbird_base_chris_csv = pd.read_csv(\"bigbird_base_chris.csv\").dropna()\nlsg_csv = pd.read_csv(\"lsg.csv\").dropna()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T10:00:32.486594Z","iopub.execute_input":"2022-02-23T10:00:32.487659Z","iopub.status.idle":"2022-02-23T10:00:32.591336Z","shell.execute_reply.started":"2022-02-23T10:00:32.487604Z","shell.execute_reply":"2022-02-23T10:00:32.589768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Models with WBF\nWe will now read in the 10 submission files generated above and apply WBF to ensemble them. After applying WBF, it is important to remove predictions with confidence score below threshold. This is explained [here][1]. \n\nIf only 1 model out of 10 models makes a certain span prediction, that prediction will still be present in WBF's outcome. However that prediction will have a very low confidence score because that model's confidence score will be averaged with 9 zero confidence scores. We found optimal confidence scores per class by analyzing our CV OOF score. For each class, we vary the threshold and compute the corresponding class metric score.\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2022/conf_scores.png)\n\n[1]: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307609","metadata":{}},{"cell_type":"code","source":"TEST_DIR = '../input/feedback-prize-2021/test/'\ntest_files = os.listdir(TEST_DIR)\nv_ids = [f.replace('.txt','') for f in test_files]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nclass_to_label = {\n    'Claim': 0, \n    'Evidence': 1, \n    'Lead':2, \n    'Position':3, \n    'Concluding Statement':4,\n    'Counterclaim':5, \n    'Rebuttal':6\n}\n\n# Threshold found from CV\nlabel_to_threshold = {\n    0 : 0.275, #Claim\n    1 : 0.375, #Evidence\n    2 : 0.325, #Lead\n    3 : 0.325, #Position\n    4 : 0.4, #Concluding Statement\n    5 : 0.275, #Counterclaim\n    6 : 0.275 #Rebuttal\n}\n\nlabel_to_class = {v:k for k, v in class_to_label.items()}\n\ndef preprocess_for_wbf(df_list):\n    boxes_list=[]\n    scores_list=[]\n    labels_list=[]\n    \n    for df in df_list:\n        scores_list.append(df['scores'].values.tolist())\n        labels_list.append(df['class'].map(class_to_label).values.tolist())\n        predictionstring = df.predictionstring.str.split().values\n        df_box_list = []\n        for bb in predictionstring:\n            df_box_list.append([int(bb[0]), int(bb[-1])])\n        boxes_list.append(df_box_list)\n    return boxes_list, scores_list, labels_list\n\ndef postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\n    preds = []\n    for box, score, label in zip(boxes_list, scores_list, labels_list):\n        if score > label_to_threshold[label]: \n            start = math.ceil(box[0])\n            end = int(box[1])\n            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)])))\n    return preds\n\ndef generate_wbf_for_id(i):\n    df1 = debertal_csv[debertal_csv['id']==i]\n    df2 = debertal_chris_csv[debertal_chris_csv['id']==i]\n    df3 = funnel_csv[funnel_csv['id']==i]\n    df4 = debertaxl_csv[debertaxl_csv['id']==i]\n    df5 = longformer_csv[longformer_csv['id']==i]\n    df6 = deberta_v3_csv[deberta_v3_csv['id']==i]\n    df7 = yoso_csv[yoso_csv['id']==i]\n    df8 = bird_base_chris_csv[bird_base_chris_csv['id']==i]\n    df9 = lsg_csv[lsg_csv['id']==i]\n    df10 = deberta_v2_csv[deberta_v2_csv['id']==i]\n    \n    boxes_list, scores_list, labels_list = preprocess_for_wbf([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\n\n    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing as mp\n\nwith mp.Pool(2) as p:\n    list_of_list = p.map(generate_wbf_for_id, v_ids)\n\npreds = [x for sub_list in list_of_list for x in sub_list]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(preds)\nsub.columns = [\"id\", \"class\", \"predictionstring\"]\nsub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Test Predictions\nBelow we visualize the test predictions","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom spacy import displacy\n\ntest_path = Path('../input/feedback-prize-2021/test')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000',\n            'Other': '#007f00',\n         }\n\ndef get_test_text(ids):\n    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\n    return data\n\ndef visualize(df):\n    ids = df[\"id\"].unique()\n    for i in range(len(ids)):\n        ents = []\n        example = ids[i]\n        curr_df = df[df[\"id\"]==example]\n        text = \" \".join(get_test_text(example).split())\n        splitted_text = text.split()\n        for i, row in curr_df.iterrows():\n            predictionstring = row['predictionstring']\n            predictionstring = predictionstring.split()\n            wstart = int(predictionstring[0])\n            wend = int(predictionstring[-1])\n            ents.append({\n                             'start': len(\" \".join(splitted_text[:wstart])), \n                             'end': len(\" \".join(splitted_text[:wend+1])), \n                             'label': row['class']\n                        })\n        ents = sorted(ents, key = lambda i: i['start'])\n\n        doc2 = {\n            \"text\": text,\n            \"ents\": ents,\n            \"title\": example\n        }\n\n        options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \"colors\": colors}\n        displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(sub[\"id\"].unique())==5:\n    visualize(sub)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
\ No newline at end of file
+{
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat_minor": 4,
+ "nbformat": 4,
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "source": "# 2nd Place Solution - CV 741, Public 727, Private 740\nWe (Chun Ming Lee @leecming , Udbhav Bamba @ubamba98, and Chris Deotte @cdeotte ) are excited to present our 2nd place solution to Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Thank you Georgia State University, The Learning Agency Lab, and Kaggle for an awesome competition.\n\nOur full solution write up is [here][1]. The main ingredients to our solution are \n* powerful post process per model\n* huge variety of NLP models trained on NER task\n* ensemble with weighted box fusion (from ZFTurbo's GitHub [here][3]). \n\n[1]: https://www.kaggle.com/c/feedback-prize-2021/discussion/313389\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# Inference Script with Post Process\nThe following Python script accepts a filename of a saved model, infers the test texts, applies post process, and writes a `submission.csv` file with an extra column of confidence scores per span (i.e. the probability that this span is correct). **Click \"show hidden cell\" to see the code.**",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "%%writefile generate_preds.py\n\nimport os\nimport argparse\n\nap = argparse.ArgumentParser()\nap.add_argument('--model_paths', nargs='+', required=True)\nap.add_argument(\"--save_name\", type=str, required=True)\nap.add_argument(\"--max_len\", type=int, required=True)\nargs = ap.parse_args()\n\nif args.save_name == \"yoso\":\n    os.system(\"cp -r ../input/hf-transformers/transformers-4.16.0 .\")\n    os.system(\"pip install -U --no-build-isolation --no-deps /kaggle/working/transformers-4.16.0\")\n    \nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\n    # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n    # This must be done before importing transformers\n    import shutil\n    from pathlib import Path\n\n    transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\n    input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\n    convert_file = input_dir / \"convert_slow_tokenizer.py\"\n    conversion_path = transformers_path/convert_file.name\n\n    if conversion_path.exists():\n        conversion_path.unlink()\n\n    shutil.copy(convert_file, transformers_path)\n    deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\n    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n        filepath = deberta_v2_path/filename\n        if filepath.exists():\n            filepath.unlink()\n\n        shutil.copy(input_dir/filename, filepath)\n\nif args.save_name == \"longformerwithlstm\":\n    os.system(\"cp -r ../input/longformerwithbilstmhead/model.py .\")\n    from model import LongformerForTokenClassificationwithbiLSTM\n    \nif args.save_name == \"debertawithlstm\":\n    os.system(\"cp -r ../input/deberta-lstm/model.py .\")\n    from model import DebertaForTokenClassificationwithbiLSTM\n        \nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport multiprocessing as mp\nfrom scipy.special import softmax\nfrom torch.utils.data import Dataset\nfrom transformers import (AutoModelForTokenClassification, \n                          AutoTokenizer, \n                          TrainingArguments, \n                          Trainer)\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\nNUM_CORES = 16\nBATCH_SIZE = 4\nMAX_SEQ_LENGTH = args.max_len\nPRETRAINED_MODEL_PATHS = args.model_paths\nif \"debertal_chris\" in args.save_name:\n    print('==> using -1 in offset mapping...')\nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    print('==> using -1 in offset mapping...')\n    \nAGG_FUNC = np.mean\nprint('==> using span token mean...')\n\nTEST_DIR = '../input/feedback-prize-2021/test/'\n\nMIN_TOKENS = {\n    \"Lead\": 32,\n    \"Position\": 5,\n    \"Evidence\": 35,\n    \"Claim\": 7,\n    \"Concluding Statement\": 6,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 6\n}\n\nif \"chris\" not in args.save_name:\n    ner_labels = {'O': 0,\n                  'B-Lead': 1,\n                  'I-Lead': 2,\n                  'B-Position': 3,\n                  'I-Position': 4,\n                  'B-Evidence': 5,\n                  'I-Evidence': 6,\n                  'B-Claim': 7,\n                  'I-Claim': 8,\n                  'B-Concluding Statement': 9,\n                  'I-Concluding Statement': 10,\n                  'B-Counterclaim': 11,\n                  'I-Counterclaim': 12,\n                  'B-Rebuttal': 13,\n                  'I-Rebuttal': 14}\nelse:\n    print(\"==> Using Chris BIO\")\n    ner_labels = {'O': 14,\n                  'B-Lead': 0,\n                  'I-Lead': 1,\n                  'B-Position': 2,\n                  'I-Position': 3,\n                  'B-Evidence': 4,\n                  'I-Evidence': 5,\n                  'B-Claim': 6,\n                  'I-Claim': 7,\n                  'B-Concluding Statement': 8,\n                  'I-Concluding Statement': 9,\n                  'B-Counterclaim': 10,\n                  'I-Counterclaim': 11,\n                  'B-Rebuttal': 12,\n                  'I-Rebuttal': 13}\n\n\ninverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\ninverted_ner_labels[-100] = 'Special Token'\n\ntest_files = os.listdir(TEST_DIR)\n\n# accepts file path, returns tuple of (file_ID, txt split, NER labels)\ndef generate_text_for_file(input_filename):\n    curr_id = input_filename.split('.')[0]\n    with open(os.path.join(TEST_DIR, input_filename)) as f:\n        curr_txt = f.read()\n\n    return curr_id, curr_txt\n\nwith mp.Pool(NUM_CORES) as p:\n    ner_test_rows = p.map(generate_text_for_file, test_files)\n    \nif (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    tokenizer = DebertaV2TokenizerFast.from_pretrained(PRETRAINED_MODEL_PATHS[0])\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n# Check is rust-based fast tokenizer\nassert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n\nner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\n\n# tokenize and store word ids\ndef tokenize_with_word_ids(ner_raw_data):\n    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\n    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data], \n                                 max_length=MAX_SEQ_LENGTH,\n                                 return_offsets_mapping=True,\n                                 truncation=True)\n    \n    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\n    \n    return tokenized_inputs\n\ntokenized_all = tokenize_with_word_ids(ner_test_rows)\n\nclass NERDataset(Dataset):\n    def __init__(self, input_dict):\n        self.input_dict = input_dict\n        \n    def __getitem__(self, index):\n        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\n    \n    def get_filename(self, index):\n        return self.input_dict['id'][index]\n    \n    def get_offset(self, index):\n        return self.input_dict['offset_mapping'][index]\n    \n    def __len__(self):\n        return len(self.input_dict['input_ids'])\n\ntest_dataset = NERDataset(tokenized_all)\n\nsoft_predictions = None\nhfargs = TrainingArguments(output_dir='None',\n                         log_level='warning',\n                         per_device_eval_batch_size=BATCH_SIZE)\n\nfor idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\n\n    if args.save_name == \"longformerwithlstm\":\n        model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n    elif args.save_name == \"debertawithlstm\":\n        model = DebertaForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n    else:\n        model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\n    trainer = Trainer(model,\n                      hfargs,\n                      tokenizer=tokenizer)\n    \n    curr_preds, _, _ = trainer.predict(test_dataset)\n    curr_preds = curr_preds.astype(np.float16)\n    curr_preds = softmax(curr_preds, -1)\n\n    if soft_predictions is not None:\n        soft_predictions = soft_predictions + curr_preds\n    else:\n        soft_predictions = curr_preds\n        \n    del model, trainer, curr_preds\n    gc.collect()\n\nsoft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\n\nsoft_claim_predictions = soft_predictions[:, :, 8]\n\npredictions = np.argmax(soft_predictions, axis=2)\nsoft_predictions = np.max(soft_predictions, axis=2)\n\ndef generate_token_to_word_mapping(txt, offset):\n    # GET WORD POSITIONS IN CHARS\n    w = []\n    blank = True\n    for i in range(len(txt)):\n        if not txt[i].isspace() and blank==True:\n            w.append(i)\n            blank=False\n        elif txt[i].isspace():\n            blank=True\n    w.append(1e6)\n\n    # MAPPING FROM TOKENS TO WORDS\n    word_map = -1 * np.ones(len(offset),dtype='int32')\n    w_i = 0\n    for i in range(len(offset)):\n        if offset[i][1]==0: continue\n        while offset[i][0]>=(w[w_i+1]-(\"debertal_chris\" in args.save_name)-(\"v3\" in args.save_name)\\\n                             -(\"v2\" in args.save_name) ): w_i += 1\n        word_map[i] = int(w_i)\n\n    return word_map\n\nall_preds = []\n\n# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\nfor curr_sample_id in range(len(test_dataset)):\n    curr_preds = []\n    sample_preds = predictions[curr_sample_id]\n    sample_offset = test_dataset.get_offset(curr_sample_id)\n    sample_txt = ner_test_rows[curr_sample_id][1]\n    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n\n    word_preds = [''] * (max(sample_word_map) + 1)\n    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n\n    for i, curr_word_id in enumerate(sample_word_map):\n        if curr_word_id != -1:\n            if word_preds[curr_word_id] == '': # only use 1st subword\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\n                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n\n    # Dict to hold Lead, Position, Concluding Statement\n    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n\n    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n    for j in range(1, len(word_preds) - 1):\n        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n        splitted_trio = [x.split('-')[0] for x in pred_trio]\n        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n            word_preds[j] = word_preds[j-1]\n\n    # B-X, ? (not B), I-X -> change ? to I-X\n    for j in range(1, len(word_preds) - 1):\n        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\"I-{word_preds[j-1].split('-')[-1]}\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\n            word_preds[j] = word_preds[j+1]\n\n     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n    for j in range(1, len(word_preds) - 1):\n        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\n            word_preds[j] = word_preds[j-1]\n\n    j = 0 # start of candidate discourse\n    while j < len(word_preds): \n        cls = word_preds[j] \n        cls_splitted = cls.split('-')[-1]\n        end = j + 1 # try to extend discourse as far as possible\n\n        if word_probs[j] > 0.54: \n            # Must match suffix i.e., I- to I- only; no B- to I-\n            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n                end += 1\n            # if we're here, end is not the same pred as start\n            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\n                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\n                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\n                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n                            \n                            # Delete other preds that lie inside the joined LC discourse\n                            for l in range(len(curr_preds) - 1, 0, -1):\n                                check_span = curr_preds[l][2]\n                                check_start, check_end = int(check_span[0]), int(check_span[-1])\n                                if check_start > prev_start and check_end < end:\n                                    del curr_preds[l]\n                            \n                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n                    else: # Add to it\n                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n                else:\n                    # Lookback and add preceding I- tokens\n                    while j - 1 > 0 and word_preds[j-1] == cls:\n                        j = j - 1\n                    # Try to add the matching B- tag if immediately precedes the current I- sequence\n                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n                        j = j - 1\n\n\n                    #############################################################\n                    # Run a bunch of adjustments to discourse predictions based on CV \n                    adj_start, adj_end = j, end + 1\n\n                    # Run some heuristics against previous discourse\n                    if len(curr_preds) > 0:\n                        prev_span = list(map(int, curr_preds[-1][2].split()))\n                        prev_start, prev_end = prev_span[0], prev_span[-1]\n\n                        # Join adjacent rebuttals\n                        if cls_splitted in 'Rebuttal':                        \n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\n                                del curr_preds[-1]\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                   cls_splitted, \n                                                   ' '.join(map(str, combined_list)),\n                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n                                j = end\n                                continue\n                                \n                        elif cls_splitted in 'Counterclaim':                        \n                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\n                                del curr_preds[-1]\n                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                   cls_splitted, \n                                                   ' '.join(map(str, combined_list)),\n                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n                                j = end\n                                continue\n\n                        elif cls_splitted in 'Evidence':                        \n                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\n                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\n                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\n                                    if len(claim_tokens) > 2:\n                                        curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                                           'Claim', \n                                                           ' '.join(claim_tokens),\n                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\n                        # If gap with discourse of same type, extend to it \n                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\n                            adj_start -= 1\n\n                    # Adjust discourse lengths if too long or short\n                    if cls_splitted == 'Evidence':\n                        if adj_end - adj_start < 45:\n                            adj_start -= 9\n                        else:\n                            adj_end -= 1\n                    elif cls_splitted == 'Claim':\n                        if adj_end - adj_start > 24:\n                            adj_end -= 1\n                    elif cls_splitted == 'Counterclaim':\n                        if adj_end - adj_start > 24:\n                            adj_end -= 1\n                        else:\n                            adj_start -= 1\n                            adj_end += 1\n                    elif cls_splitted == 'Rebuttal':\n                        if adj_end - adj_start > 32:\n                            adj_end -= 1\n                        else:\n                            adj_start -= 1\n                            adj_end += 1\n                    adj_start = max(0, adj_start)\n                    adj_end = min(len(word_preds) - 1, adj_end)\n                    curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                                       cls_splitted, \n                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\n                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\n\n        j = end \n\n    # Add the Lead, Position, Concluding Statement\n    for k, v in let_one_dict.items():\n        pred_start = v[1]\n        pred_end = v[2]\n\n        # Lookback and add preceding I- tokens\n        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n            pred_start = pred_start - 1\n        # Try to add the matching B- tag if immediately precedes the current I- sequence\n        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n            pred_start = pred_start - 1\n\n        # Extend short Leads and Concluding Statements\n        if k == 'Lead':\n            if pred_end - pred_start < 33:\n                pred_end = min(len(word_preds), pred_end + 5)\n            else:\n                pred_end -= 5\n        elif k == 'Concluding Statement':\n            if pred_end - pred_start < 23:\n                pred_start = max(0, pred_start - 1)\n                pred_end = min(len(word_preds), pred_end + 10)\n        elif k == 'Position':\n            if pred_end - pred_start < 18:\n                pred_end = min(len(word_preds), pred_end + 3)\n\n        pred_start = max(0, pred_start)\n        if pred_end - pred_start > 6:\n            curr_preds.append((test_dataset.get_filename(curr_sample_id), \n                               k, \n                               ' '.join(map(str, list(range(pred_start, pred_end)))),\n                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\n\n    all_preds.extend(curr_preds)\n\noutput_df = pd.DataFrame(all_preds)\noutput_df.columns = ['id', 'class', 'predictionstring', 'scores']\noutput_df.to_csv(f'{args.save_name}.csv', index=False)",
+   "metadata": {
+    "_kg_hide-input": true,
+    "_kg_hide-output": true,
+    "execution": {
+     "iopub.status.busy": "2022-03-13T07:39:00.274578Z",
+     "iopub.execute_input": "2022-03-13T07:39:00.274891Z",
+     "iopub.status.idle": "2022-03-13T07:39:00.313298Z",
+     "shell.execute_reply.started": "2022-03-13T07:39:00.274808Z",
+     "shell.execute_reply": "2022-03-13T07:39:00.312379Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# Weighted Box Fusion\nThe following code comes from ZFTurbo's GitHub [here][1]. First, the text `predictionstring` are converted to 1 dimensional boxes. Next they are ensembled with ZFTurbo's WBF code. **Click \"show hidden cell\" to see the code.**\n\n[1]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "'''\nCode taken and modified for 1D sequences from:\nhttps://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/ensemble_boxes/ensemble_boxes_wbf.py\n'''\nimport warnings\nimport numpy as np\n\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\n    # Create dict with boxes stored by its label\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = labels[t][j]\n            box_part = boxes[t][j]\n\n            x = float(box_part[0])\n            y = float(box_part[1])\n\n            # Box data checks\n            if y < x:\n                warnings.warn('Y < X value in box. Swap them.')\n                x, y = y, x\n\n            # [label, score, weight, model index, x, y]\n            b = [label, float(score) * weights[t], weights[t], t, x, y]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes\n\n\ndef get_weighted_box(boxes, conf_type='avg'):\n    \"\"\"\n    Create weighted box for set of boxes\n    :param boxes: set of boxes to fuse\n    :param conf_type: type of confidence one of 'avg' or 'max'\n    :return: weighted box (label, score, weight, model index, x, y)\n    \"\"\"\n\n    box = np.zeros(6, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf / len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf / len(boxes)\n    box[2] = w\n    box[3] = -1 # model index field is retained for consistensy but is not used.\n    box[4:] /= conf\n    return box\n\n\ndef find_matching_box_quickly(boxes_list, new_box, match_iou):\n    \"\"\" \n        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n\n        boxes_list: shape: (N, label, score, weight, model index, x, y)\n        new_box: shape: (label, score, weight, model index, x, y)\n    \"\"\"\n    def bb_iou_array(boxes, new_box):\n        '''\n        boxes: shape: (N, x, y)\n        new_box: shape: (x, y)\n        '''\n        # bb interesection over union\n        x_min = np.minimum(boxes[:, 0], new_box[0])\n        x_max = np.maximum(boxes[:, 0], new_box[0])\n        y_min = np.minimum(boxes[:, 1], new_box[1])+1\n        y_max = np.maximum(boxes[:, 1], new_box[1])+1\n\n        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\n\n        return iou\n\n    if boxes_list.shape[0] == 0:\n        return -1, match_iou\n\n    # boxes = np.array(boxes_list)\n    boxes = boxes_list\n\n    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n\n    ious[boxes[:, 0] != new_box[0]] = -1\n\n    best_idx = np.argmax(ious)\n    best_iou = ious[best_idx]\n\n    if best_iou <= match_iou:\n        best_iou = match_iou\n        best_idx = -1\n\n    return best_idx, best_iou\n\n\ndef weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n    '''\n    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\n     It has 3 dimensions (models_number, model_preds, 2)\n     Order of boxes: x, y.\n    :param scores_list: list of scores for each model\n    :param labels_list: list of labels for each model\n    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n    :param iou_thr: IoU value for boxes to be a match\n    :param skip_box_thr: exclude boxes with score lower than this variable\n    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n    :param allows_overflow: false if we want confidence score not exceed 1.0\n\n    :return: boxes: boxes coordinates (Order of boxes: x, y).\n    :return: scores: confidence scores\n    :return: labels: boxes labels\n    '''\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\n\n    overall_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n\n        # Rescale confidence based on number of models and boxes\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n            elif conf_type == 'max':\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n            elif not allows_overflow:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n            else:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n        \n        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\n        #for i in range(len(new_boxes)):\n        #    clustered_boxes = np.array(new_boxes[i])\n        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\n        #        overall_boxes.append(weighted_boxes[i])\n                \n        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    return boxes, scores, labels\n",
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.status.busy": "2022-03-13T07:39:00.315048Z",
+     "iopub.execute_input": "2022-03-13T07:39:00.315488Z",
+     "iopub.status.idle": "2022-03-13T07:39:00.434581Z",
+     "shell.execute_reply.started": "2022-03-13T07:39:00.315448Z",
+     "shell.execute_reply": "2022-03-13T07:39:00.433364Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# Our NLP models trained on NER task\nHere is a summary of our models and their individual performance:\n\n| Hugging Face Model | CV | Public LB | Private LB | special |\n| --- | --- | | | |\n| microsoft/deberta-large | 706 | 710 | 721 | trained with 100% train data|\n| microsoft/deberta-large | 699 | 706 | 719 | add lstm, add jaccard loss |\n| microsoft/deberta-v3-large | 705 | |  | [convert slow tokenizer to fast][9] |\n| microsoft/deberta-xlarge | 708 | 704 | 713 | |\n| microsoft/deberta-v2-xlarge | 702 |  |  | [convert slow tokenizer to fast][9] |\n| allenai/longformer-large-4096 | 702 | 705 | 716 | add lstm head|\n| [LSG converted roberta][1] | 703 | 702 | 714 | convert 512 roberta to 1536 |\n| funnel-transformer/large | 688 | 689 | 708 |\n| google/bigbird-roberta-base | 675 | 676 | 692 | train 1024 infer 1024 |\n| uw-madison/yoso-4096 | 652 | 655 | 668 | lsh_backward=False |\n\n[1]: https://github.com/ccdv-ai/convert_checkpoint_to_lsg\n[3]: https://github.com/ZFTurbo/Weighted-Boxes-Fusion\n[9]: https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "!python generate_preds.py --model_paths ../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 \\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f2/checkpoint-7500 \\\n                                        ../input/longformerwithbilstmhead/aug-longformer-large-4096-f5/checkpoint-6000 \\\n                            --save_name longformerwithlstm --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\n                                        ../input/deberta-large-100/fold1 \\\n                                        ../input/deberta-large-100/fold2 \\\n                            --save_name debertal_chris --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-large-v2/deberta-large-v2100-f0/checkpoint-10500 \\\n                                        ../input/deberta-large-v2/deberta-large-v2101-f1/checkpoint-11500 \\\n                                        ../input/deberta-large-v2/deberta-large-v2102-f2/checkpoint-8500 \\\n                            --save_name debertal --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n                                        ../input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\n                            --save_name debertaxl --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6000-f0/checkpoint-7500 \\\n                                        ../input/deberta-v2-xlarge/deberta-v2-xlarge-v6003-f3/checkpoint-9000 \\\n                            --save_name deberta_v2 --max_len 1536\n\n!python generate_preds.py --model_paths ../input/deberta-lstm-jaccard/jcl-deberta-large-f1/checkpoint-4500 \\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f2/checkpoint-5000 \\\n                                        ../input/deberta-lstm-jaccard/jcl-deberta-large-f3/checkpoint-3500 \\\n                            --save_name debertawithlstm --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/funnel-large-6folds/large-v628-f1/checkpoint-11500 \\\n                                         ../input/funnel-large-6folds/large-v627-f3/checkpoint-11000 \\\n                                         ../input/funnel-large-6folds/large-v623-f4/checkpoint-10500 \\\n                            --save_name funnel --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/auglsgrobertalarge/lsg-roberta-large-0/checkpoint-6750 \\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-2/checkpoint-7000 \\\n                                         ../input/auglsgrobertalarge/lsg-roberta-large-5/checkpoint-6500 \\\n                             --save_name lsg --max_len 1536\n\n!python generate_preds.py --model_paths  ../input/bird-base/fold1 \\\n                                         ../input/bird-base/fold3 \\\n                                         ../input/bird-base/fold5 \\\n                            --save_name bigbird_base_chris --max_len 1024\n\n!python generate_preds.py --model_paths  ../input/feedbackyoso/yoso-4096-0/checkpoint-12500 \\\n                                         ../input/feedbackyoso/yoso-4096-2/checkpoint-11000 \\\n                                         ../input/feedbackyoso/yoso-4096-4/checkpoint-12500 \\\n                            --save_name yoso --max_len 1536",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-03-13T07:39:00.721366Z",
+     "iopub.execute_input": "2022-03-13T07:39:00.721767Z"
+    },
+    "_kg_hide-output": true,
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "import pandas as pd\nimport numpy as np, os\n\nlongformer_csv = pd.read_csv(\"longformerwithlstm.csv\").dropna()\ndeberta_v3_csv = pd.read_csv(\"debertawithlstm.csv\").dropna()\ndeberta_v2_csv = pd.read_csv(\"deberta_v2.csv\").dropna()\ndebertaxl_csv = pd.read_csv(\"debertaxl.csv\").dropna()\ndebertal_chris_csv = pd.read_csv(\"debertal_chris.csv\").dropna()\ndebertal_csv = pd.read_csv(\"debertal.csv\").dropna()\nyoso_csv = pd.read_csv(\"yoso.csv\").dropna()\nfunnel_csv = pd.read_csv(\"funnel.csv\").dropna()\nbird_base_chris_csv = pd.read_csv(\"bigbird_base_chris.csv\").dropna()\nlsg_csv = pd.read_csv(\"lsg.csv\").dropna()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-02-23T10:00:32.486594Z",
+     "iopub.execute_input": "2022-02-23T10:00:32.487659Z",
+     "iopub.status.idle": "2022-02-23T10:00:32.591336Z",
+     "shell.execute_reply.started": "2022-02-23T10:00:32.487604Z",
+     "shell.execute_reply": "2022-02-23T10:00:32.589768Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# Ensemble Models with WBF\nWe will now read in the 10 submission files generated above and apply WBF to ensemble them. After applying WBF, it is important to remove predictions with confidence score below threshold. This is explained [here][1]. \n\nIf only 1 model out of 10 models makes a certain span prediction, that prediction will still be present in WBF's outcome. However that prediction will have a very low confidence score because that model's confidence score will be averaged with 9 zero confidence scores. We found optimal confidence scores per class by analyzing our CV OOF score. For each class, we vary the threshold and compute the corresponding class metric score.\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2022/conf_scores.png)\n\n[1]: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307609",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "TEST_DIR = '../input/feedback-prize-2021/test/'\ntest_files = os.listdir(TEST_DIR)\nv_ids = [f.replace('.txt','') for f in test_files]",
+   "metadata": {
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "import math\n\nclass_to_label = {\n    'Claim': 0, \n    'Evidence': 1, \n    'Lead':2, \n    'Position':3, \n    'Concluding Statement':4,\n    'Counterclaim':5, \n    'Rebuttal':6\n}\n\n# Threshold found from CV\nlabel_to_threshold = {\n    0 : 0.275, #Claim\n    1 : 0.375, #Evidence\n    2 : 0.325, #Lead\n    3 : 0.325, #Position\n    4 : 0.4, #Concluding Statement\n    5 : 0.275, #Counterclaim\n    6 : 0.275 #Rebuttal\n}\n\nlabel_to_class = {v:k for k, v in class_to_label.items()}\n\ndef preprocess_for_wbf(df_list):\n    boxes_list=[]\n    scores_list=[]\n    labels_list=[]\n    \n    for df in df_list:\n        scores_list.append(df['scores'].values.tolist())\n        labels_list.append(df['class'].map(class_to_label).values.tolist())\n        predictionstring = df.predictionstring.str.split().values\n        df_box_list = []\n        for bb in predictionstring:\n            df_box_list.append([int(bb[0]), int(bb[-1])])\n        boxes_list.append(df_box_list)\n    return boxes_list, scores_list, labels_list\n\ndef postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\n    preds = []\n    for box, score, label in zip(boxes_list, scores_list, labels_list):\n        if score > label_to_threshold[label]: \n            start = math.ceil(box[0])\n            end = int(box[1])\n            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)])))\n    return preds\n\ndef generate_wbf_for_id(i):\n    df1 = debertal_csv[debertal_csv['id']==i]\n    df2 = debertal_chris_csv[debertal_chris_csv['id']==i]\n    df3 = funnel_csv[funnel_csv['id']==i]\n    df4 = debertaxl_csv[debertaxl_csv['id']==i]\n    df5 = longformer_csv[longformer_csv['id']==i]\n    df6 = deberta_v3_csv[deberta_v3_csv['id']==i]\n    df7 = yoso_csv[yoso_csv['id']==i]\n    df8 = bird_base_chris_csv[bird_base_chris_csv['id']==i]\n    df9 = lsg_csv[lsg_csv['id']==i]\n    df10 = deberta_v2_csv[deberta_v2_csv['id']==i]\n    \n    boxes_list, scores_list, labels_list = preprocess_for_wbf([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10])\n    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\n\n    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)",
+   "metadata": {
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "import multiprocessing as mp\n\nwith mp.Pool(2) as p:\n    list_of_list = p.map(generate_wbf_for_id, v_ids)\n\npreds = [x for sub_list in list_of_list for x in sub_list]",
+   "metadata": {},
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "sub = pd.DataFrame(preds)\nsub.columns = [\"id\", \"class\", \"predictionstring\"]\nsub.to_csv('submission.csv', index=False)",
+   "metadata": {
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# Visualize Test Predictions\nBelow we visualize the test predictions",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from pathlib import Path\nfrom spacy import displacy\n\ntest_path = Path('../input/feedback-prize-2021/test')\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000',\n            'Other': '#007f00',\n         }\n\ndef get_test_text(ids):\n    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\n    return data\n\ndef visualize(df):\n    ids = df[\"id\"].unique()\n    for i in range(len(ids)):\n        ents = []\n        example = ids[i]\n        curr_df = df[df[\"id\"]==example]\n        text = \" \".join(get_test_text(example).split())\n        splitted_text = text.split()\n        for i, row in curr_df.iterrows():\n            predictionstring = row['predictionstring']\n            predictionstring = predictionstring.split()\n            wstart = int(predictionstring[0])\n            wend = int(predictionstring[-1])\n            ents.append({\n                             'start': len(\" \".join(splitted_text[:wstart])), \n                             'end': len(\" \".join(splitted_text[:wend+1])), \n                             'label': row['class']\n                        })\n        ents = sorted(ents, key = lambda i: i['start'])\n\n        doc2 = {\n            \"text\": text,\n            \"ents\": ents,\n            \"title\": example\n        }\n\n        options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \"colors\": colors}\n        displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)",
+   "metadata": {
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "if len(sub[\"id\"].unique())==5:\n    visualize(sub)",
+   "metadata": {
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  }
+ ]
+}
\ No newline at end of file
Index: dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"metadata\":{\"_uuid\":\"caa44bc10a9d34d96916eba5a4f90dba73e84735\"},\"cell_type\":\"markdown\",\"source\":\"Due to leaks found in the past week, I wondered how it would modify the simple XGB scoring method demonstrated in this notebook.\\n\\nFor this purpose I use the results found in : https://www.kaggle.com/johnfarrell/breaking-lb-fresh-start-with-lag-selection/output\\n\"},{\"metadata\":{\"_uuid\":\"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\"_cell_guid\":\"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\"trusted\":true,\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"import numpy as np\\nimport pandas as pd\\nfrom xgboost import XGBRegressor\\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\\nfrom sklearn.model_selection import KFold\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"8e9fddf207ae05e8c1d0e76ce43f12df6252acee\"},\"cell_type\":\"code\",\"source\":\"%%time\\ndata = pd.read_csv('../input/santander-value-prediction-challenge/train.csv')\\ntarget = np.log1p(data['target'])\\ndata.drop(['ID', 'target'], axis=1, inplace=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"632e6a2563936127c06287cfc3a1cf309385934e\"},\"cell_type\":\"markdown\",\"source\":\"### Add train leak\"},{\"metadata\":{\"_cell_guid\":\"79c7e3d0-c299-4dcb-8224-4455121ee9b0\",\"_uuid\":\"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"%%time\\nleak = pd.read_csv('../input/breaking-lb-fresh-start-with-lag-selection/train_leak.csv')\\ndata['leak'] = leak['compiled_leak'].values\\ndata['log_leak'] = np.log1p(leak['compiled_leak'].values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0c8610c5187001d7ec17a7ffaf203859c6f9aa46\"},\"cell_type\":\"markdown\",\"source\":\"### Feature Scoring using XGBoost with the leak feature\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"68e309e111e4a772a6ec4d8cf00c11e134c23f05\",\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"%%time\\ndef rmse(y_true, y_pred):\\n    return mean_squared_error(y_true, y_pred) ** .5\\n\\nreg = XGBRegressor(n_estimators=1000)\\nfolds = KFold(4, True, 134259)\\nfold_idx = [(trn_, val_) for trn_, val_ in folds.split(data)]\\nscores = []\\n\\nnb_values = data.nunique(dropna=False)\\nnb_zeros = (data == 0).astype(np.uint8).sum(axis=0)\\n\\nfeatures = [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID']]\\nfor _f in features:\\n    score = 0\\n    for trn_, val_ in fold_idx:\\n        reg.fit(\\n            data[['log_leak', _f]].iloc[trn_], target.iloc[trn_],\\n            eval_set=[(data[['log_leak', _f]].iloc[val_], target.iloc[val_])],\\n            eval_metric='rmse',\\n            early_stopping_rounds=50,\\n            verbose=False\\n        )\\n        score += rmse(target.iloc[val_], reg.predict(data[['log_leak', _f]].iloc[val_], ntree_limit=reg.best_ntree_limit)) / folds.n_splits\\n    scores.append((_f, score))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3694822b9bc81bb6896f417cee646e1990577917\"},\"cell_type\":\"markdown\",\"source\":\"### Create dataframe\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"7863f32d4b51d6dc957453365d8e79682848f4e4\",\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"report = pd.DataFrame(scores, columns=['feature', 'rmse']).set_index('feature')\\nreport['nb_zeros'] = nb_zeros\\nreport['nunique'] = nb_values\\nreport.sort_values(by='rmse', ascending=True, inplace=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"60fdbaefae3d78b8f27cb109d2f5caf38fd9646a\"},\"cell_type\":\"markdown\",\"source\":\"### Plot a few diagrams\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"660318ef180911d56dc1fadca3ea196917a92c22\",\"_kg_hide-input\":true,\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"plt.figure(figsize=(10, 7))\\nplt.xlabel('Number of zeros in the feature', fontsize=14)\\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\\nplt.title('Feature score vs number of zeros', fontsize=16, fontweight='bold', color='#ae3453')\\nplt.scatter(report['nb_zeros'], report['rmse'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"deb4e77513ce86867a6d653e9da6c74e2aed768d\",\"_kg_hide-input\":true,\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"fig, ax = plt.subplots(figsize=(10, 7))\\nplt.xlabel('Number of unique values in the feature', fontsize=14)\\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\\nax.set_title('Feature score vs number of unique values', fontsize=16, fontweight='bold', color='#ae3453')\\nscatter = ax.scatter(report['nunique'], report['rmse'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_kg_hide-input\":true,\"trusted\":true,\"_uuid\":\"b8231eee7ab03863592853a2c8aa59e7a39d2e19\",\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"from bokeh.plotting import figure, show, output_file, output_notebook, ColumnDataSource\\n\\nreport.sort_values('rmse', ascending=False, inplace=True)\\n\\nradii = 1000 * (report['rmse'].max() - report['rmse']).values\\n\\nsource = ColumnDataSource(data=dict(\\n    x=report['nunique'].tolist(),\\n    y=report['nb_zeros'].tolist(),\\n    desc=report.index.tolist(),\\n    radius=radii,\\n    fill_color=[\\n       \\\"#%02x%02x%02x\\\" % (int(r), 100, 150) for r in 255 * ((report['rmse'].max() - report['rmse']) / (report['rmse'].max() - report['rmse'].min())).values\\n    ],\\n    rmse=report['rmse'].tolist()\\n))\\n\\nTOOLTIPS = [\\n    (\\\"rmse\\\", \\\"@rmse\\\"),\\n    (\\\"(nunique, nb_zeros)\\\", \\\"(@x, @y)\\\"),\\n    (\\\"feature\\\", \\\"@desc\\\"),\\n]\\nTOOLS = \\\"hover, crosshair, pan, wheel_zoom, zoom_in, zoom_out, box_zoom, undo, redo, reset, tap, save, box_select, poly_select, lasso_select\\\"\\n\\np = figure(plot_width=600, plot_height=600, tooltips=TOOLTIPS, tools=TOOLS,\\n           title=\\\"Number of unique values vs Number of zeros\\\")\\np.xaxis.axis_label = 'Number of unique values in feature'\\np.yaxis.axis_label = 'Number of zeros in feature'\\np.xaxis.axis_label_text_font_style ='bold'\\np.yaxis.axis_label_text_font_style ='bold'\\np.title.text_color = '#ae3453'\\np.title.text_font_size = '16pt'\\np.scatter(\\n    'x', 'y', source=source,\\n    radius='radius',\\n    fill_color='fill_color',\\n    line_color=None,\\n    fill_alpha=0.8\\n)\\n\\noutput_notebook()\\n\\nshow(p)  # open a browser\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"4db7b58fc5b7dc4acb58b0b23a3e05f46dc612e2\",\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"report.to_csv('feature_report.csv', index=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0036802c721bc8082d74e17fd9454aed9939a386\"},\"cell_type\":\"markdown\",\"source\":\"### Select some features (threshold is not optimized)\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"7a8a6ce954124e2a42dddc19c5f55174f86c8984\",\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"good_features = report.loc[report['rmse'] <= 0.7925].index\\nrmses = report.loc[report['rmse'] <= 0.7925, 'rmse'].values\\ngood_features\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_uuid\":\"75dfb7f6467ba6cddc0648d4452fa4968c63c484\"},\"cell_type\":\"code\",\"source\":\"test = pd.read_csv('../input/santander-value-prediction-challenge/test.csv')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c4a7885aae0243a8e5e97ad7b564f2f0d1804ba\"},\"cell_type\":\"markdown\",\"source\":\"### Display distributions of test and train for selected features\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"9172be793b24b90bc0d729ee5c2583cd1c74c60b\",\"collapsed\":true},\"cell_type\":\"code\",\"source\":\"for i, f in enumerate(good_features):\\n    plt.subplots(figsize=(10, 3))\\n    plt.title('Feature %s RMSE %.3f train/test distributions' % (f, rmses[i]), fontsize=16, fontweight='bold', color='#ae3453')\\n    hists = plt.hist(np.log1p(data[f].replace(0, np.nan).dropna().values), alpha=.7, label='train', \\n             bins=50, density=True,  histtype='bar')\\n    plt.hist(np.log1p(test[f].replace(0, np.nan).dropna().values), alpha=.5, label='test', \\n             bins=hists[1], density=True, histtype='bar')\\n    plt.legend()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"collapsed\":true,\"_uuid\":\"9144fdbff21261ea77f7a900f15ef079474f4b16\"},\"cell_type\":\"markdown\",\"source\":\"### Add leak to test\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"4ed1665c3928cd9063296de5791a7f9b869a7619\"},\"cell_type\":\"code\",\"source\":\"%%time\\ntst_leak = pd.read_csv('../input/breaking-lb-fresh-start-with-lag-selection/test_leak.csv')\\ntest['leak'] = tst_leak['compiled_leak']\\ntest['log_leak'] = np.log1p(tst_leak['compiled_leak'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"d681176d6c0dd3b5fdcdd6ea610fd229368ea57f\"},\"cell_type\":\"markdown\",\"source\":\"### Train lightgbm\"},{\"metadata\":{\"trusted\":true,\"_uuid\":\"c8d31ec188185853b05a562a97ffa6982b52e1ce\"},\"cell_type\":\"code\",\"source\":\"from sklearn.metrics import mean_squared_error\\nfrom sklearn.model_selection import KFold\\nimport lightgbm as lgb\\n\\nfolds = KFold(n_splits=5, shuffle=True, random_state=1)\\n\\n# Use all features for stats\\nfeatures = [f for f in data if f not in ['ID', 'leak', 'log_leak', 'target']]\\ndata.replace(0, np.nan, inplace=True)\\ndata['log_of_mean'] = np.log1p(data[features].replace(0, np.nan).mean(axis=1))\\ndata['mean_of_log'] = np.log1p(data[features]).replace(0, np.nan).mean(axis=1)\\ndata['log_of_median'] = np.log1p(data[features].replace(0, np.nan).median(axis=1))\\ndata['nb_nans'] = data[features].isnull().sum(axis=1)\\ndata['the_sum'] = np.log1p(data[features].sum(axis=1))\\ndata['the_std'] = data[features].std(axis=1)\\ndata['the_kur'] = data[features].kurtosis(axis=1)\\n\\ntest.replace(0, np.nan, inplace=True)\\ntest['log_of_mean'] = np.log1p(test[features].replace(0, np.nan).mean(axis=1))\\ntest['mean_of_log'] = np.log1p(test[features]).replace(0, np.nan).mean(axis=1)\\ntest['log_of_median'] = np.log1p(test[features].replace(0, np.nan).median(axis=1))\\ntest['nb_nans'] = test[features].isnull().sum(axis=1)\\ntest['the_sum'] = np.log1p(test[features].sum(axis=1))\\ntest['the_std'] = test[features].std(axis=1)\\ntest['the_kur'] = test[features].kurtosis(axis=1)\\n\\n# Only use good features, log leak and stats for training\\nfeatures = good_features.tolist()\\nfeatures = features + ['log_leak', 'log_of_mean', 'mean_of_log', 'log_of_median', 'nb_nans', 'the_sum', 'the_std', 'the_kur']\\ndtrain = lgb.Dataset(data=data[features], \\n                     label=target, free_raw_data=False)\\ntest['target'] = 0\\n\\ndtrain.construct()\\noof_preds = np.zeros(data.shape[0])\\n\\nfor trn_idx, val_idx in folds.split(data):\\n    lgb_params = {\\n        'objective': 'regression',\\n        'num_leaves': 58,\\n        'subsample': 0.6143,\\n        'colsample_bytree': 0.6453,\\n        'min_split_gain': np.power(10, -2.5988),\\n        'reg_alpha': np.power(10, -2.2887),\\n        'reg_lambda': np.power(10, 1.7570),\\n        'min_child_weight': np.power(10, -0.1477),\\n        'verbose': -1,\\n        'seed': 3,\\n        'boosting_type': 'gbdt',\\n        'max_depth': -1,\\n        'learning_rate': 0.05,\\n        'metric': 'l2',\\n    }\\n\\n    clf = lgb.train(\\n        params=lgb_params,\\n        train_set=dtrain.subset(trn_idx),\\n        valid_sets=dtrain.subset(val_idx),\\n        num_boost_round=10000, \\n        early_stopping_rounds=100,\\n        verbose_eval=0\\n    )\\n\\n    oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\\n    test['target'] += clf.predict(test[features]) / folds.n_splits\\n    print(mean_squared_error(target.iloc[val_idx], \\n                             oof_preds[val_idx]) ** .5)\\n\\ndata['predictions'] = oof_preds\\ndata.loc[data['leak'].notnull(), 'predictions'] = np.log1p(data.loc[data['leak'].notnull(), 'leak'])\\nprint('OOF SCORE : %9.6f' \\n      % (mean_squared_error(target, oof_preds) ** .5))\\nprint('OOF SCORE with LEAK : %9.6f' \\n      % (mean_squared_error(target, data['predictions']) ** .5))\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"949a9ce471a9041d842f94a3a2ad139587b5edd1\"},\"cell_type\":\"markdown\",\"source\":\"### Save submission\"},{\"metadata\":{\"trusted\":true,\"collapsed\":true,\"_uuid\":\"513e25d309100480da7dcacf2049598b69d7fbe6\"},\"cell_type\":\"code\",\"source\":\"test['target'] = np.expm1(test['target'])\\ntest.loc[test['leak'].notnull(), 'target'] = test.loc[test['leak'].notnull(), 'leak']\\ntest[['ID', 'target']].to_csv('leaky_submission.csv', index=False, float_format='%.2f')\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"}},\"nbformat\":4,\"nbformat_minor\":1}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb b/dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb
--- a/dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/santander-value-prediction-challenge/olivier/feature-scoring-vs-zeros.ipynb	(date 1658512097853)
@@ -1,1 +1,262 @@
-{"cells":[{"metadata":{"_uuid":"caa44bc10a9d34d96916eba5a4f90dba73e84735"},"cell_type":"markdown","source":"Due to leaks found in the past week, I wondered how it would modify the simple XGB scoring method demonstrated in this notebook.\n\nFor this purpose I use the results found in : https://www.kaggle.com/johnfarrell/breaking-lb-fresh-start-with-lag-selection/output\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e9fddf207ae05e8c1d0e76ce43f12df6252acee"},"cell_type":"code","source":"%%time\ndata = pd.read_csv('../input/santander-value-prediction-challenge/train.csv')\ntarget = np.log1p(data['target'])\ndata.drop(['ID', 'target'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"632e6a2563936127c06287cfc3a1cf309385934e"},"cell_type":"markdown","source":"### Add train leak"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\nleak = pd.read_csv('../input/breaking-lb-fresh-start-with-lag-selection/train_leak.csv')\ndata['leak'] = leak['compiled_leak'].values\ndata['log_leak'] = np.log1p(leak['compiled_leak'].values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c8610c5187001d7ec17a7ffaf203859c6f9aa46"},"cell_type":"markdown","source":"### Feature Scoring using XGBoost with the leak feature"},{"metadata":{"trusted":true,"_uuid":"68e309e111e4a772a6ec4d8cf00c11e134c23f05","collapsed":true},"cell_type":"code","source":"%%time\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred) ** .5\n\nreg = XGBRegressor(n_estimators=1000)\nfolds = KFold(4, True, 134259)\nfold_idx = [(trn_, val_) for trn_, val_ in folds.split(data)]\nscores = []\n\nnb_values = data.nunique(dropna=False)\nnb_zeros = (data == 0).astype(np.uint8).sum(axis=0)\n\nfeatures = [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID']]\nfor _f in features:\n    score = 0\n    for trn_, val_ in fold_idx:\n        reg.fit(\n            data[['log_leak', _f]].iloc[trn_], target.iloc[trn_],\n            eval_set=[(data[['log_leak', _f]].iloc[val_], target.iloc[val_])],\n            eval_metric='rmse',\n            early_stopping_rounds=50,\n            verbose=False\n        )\n        score += rmse(target.iloc[val_], reg.predict(data[['log_leak', _f]].iloc[val_], ntree_limit=reg.best_ntree_limit)) / folds.n_splits\n    scores.append((_f, score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3694822b9bc81bb6896f417cee646e1990577917"},"cell_type":"markdown","source":"### Create dataframe"},{"metadata":{"trusted":true,"_uuid":"7863f32d4b51d6dc957453365d8e79682848f4e4","collapsed":true},"cell_type":"code","source":"report = pd.DataFrame(scores, columns=['feature', 'rmse']).set_index('feature')\nreport['nb_zeros'] = nb_zeros\nreport['nunique'] = nb_values\nreport.sort_values(by='rmse', ascending=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60fdbaefae3d78b8f27cb109d2f5caf38fd9646a"},"cell_type":"markdown","source":"### Plot a few diagrams"},{"metadata":{"trusted":true,"_uuid":"660318ef180911d56dc1fadca3ea196917a92c22","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nplt.xlabel('Number of zeros in the feature', fontsize=14)\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\nplt.title('Feature score vs number of zeros', fontsize=16, fontweight='bold', color='#ae3453')\nplt.scatter(report['nb_zeros'], report['rmse'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deb4e77513ce86867a6d653e9da6c74e2aed768d","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 7))\nplt.xlabel('Number of unique values in the feature', fontsize=14)\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\nax.set_title('Feature score vs number of unique values', fontsize=16, fontweight='bold', color='#ae3453')\nscatter = ax.scatter(report['nunique'], report['rmse'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b8231eee7ab03863592853a2c8aa59e7a39d2e19","collapsed":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, output_file, output_notebook, ColumnDataSource\n\nreport.sort_values('rmse', ascending=False, inplace=True)\n\nradii = 1000 * (report['rmse'].max() - report['rmse']).values\n\nsource = ColumnDataSource(data=dict(\n    x=report['nunique'].tolist(),\n    y=report['nb_zeros'].tolist(),\n    desc=report.index.tolist(),\n    radius=radii,\n    fill_color=[\n       \"#%02x%02x%02x\" % (int(r), 100, 150) for r in 255 * ((report['rmse'].max() - report['rmse']) / (report['rmse'].max() - report['rmse'].min())).values\n    ],\n    rmse=report['rmse'].tolist()\n))\n\nTOOLTIPS = [\n    (\"rmse\", \"@rmse\"),\n    (\"(nunique, nb_zeros)\", \"(@x, @y)\"),\n    (\"feature\", \"@desc\"),\n]\nTOOLS = \"hover, crosshair, pan, wheel_zoom, zoom_in, zoom_out, box_zoom, undo, redo, reset, tap, save, box_select, poly_select, lasso_select\"\n\np = figure(plot_width=600, plot_height=600, tooltips=TOOLTIPS, tools=TOOLS,\n           title=\"Number of unique values vs Number of zeros\")\np.xaxis.axis_label = 'Number of unique values in feature'\np.yaxis.axis_label = 'Number of zeros in feature'\np.xaxis.axis_label_text_font_style ='bold'\np.yaxis.axis_label_text_font_style ='bold'\np.title.text_color = '#ae3453'\np.title.text_font_size = '16pt'\np.scatter(\n    'x', 'y', source=source,\n    radius='radius',\n    fill_color='fill_color',\n    line_color=None,\n    fill_alpha=0.8\n)\n\noutput_notebook()\n\nshow(p)  # open a browser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db7b58fc5b7dc4acb58b0b23a3e05f46dc612e2","collapsed":true},"cell_type":"code","source":"report.to_csv('feature_report.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0036802c721bc8082d74e17fd9454aed9939a386"},"cell_type":"markdown","source":"### Select some features (threshold is not optimized)"},{"metadata":{"trusted":true,"_uuid":"7a8a6ce954124e2a42dddc19c5f55174f86c8984","collapsed":true},"cell_type":"code","source":"good_features = report.loc[report['rmse'] <= 0.7925].index\nrmses = report.loc[report['rmse'] <= 0.7925, 'rmse'].values\ngood_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75dfb7f6467ba6cddc0648d4452fa4968c63c484"},"cell_type":"code","source":"test = pd.read_csv('../input/santander-value-prediction-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c4a7885aae0243a8e5e97ad7b564f2f0d1804ba"},"cell_type":"markdown","source":"### Display distributions of test and train for selected features"},{"metadata":{"trusted":true,"_uuid":"9172be793b24b90bc0d729ee5c2583cd1c74c60b","collapsed":true},"cell_type":"code","source":"for i, f in enumerate(good_features):\n    plt.subplots(figsize=(10, 3))\n    plt.title('Feature %s RMSE %.3f train/test distributions' % (f, rmses[i]), fontsize=16, fontweight='bold', color='#ae3453')\n    hists = plt.hist(np.log1p(data[f].replace(0, np.nan).dropna().values), alpha=.7, label='train', \n             bins=50, density=True,  histtype='bar')\n    plt.hist(np.log1p(test[f].replace(0, np.nan).dropna().values), alpha=.5, label='test', \n             bins=hists[1], density=True, histtype='bar')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9144fdbff21261ea77f7a900f15ef079474f4b16"},"cell_type":"markdown","source":"### Add leak to test"},{"metadata":{"trusted":true,"_uuid":"4ed1665c3928cd9063296de5791a7f9b869a7619"},"cell_type":"code","source":"%%time\ntst_leak = pd.read_csv('../input/breaking-lb-fresh-start-with-lag-selection/test_leak.csv')\ntest['leak'] = tst_leak['compiled_leak']\ntest['log_leak'] = np.log1p(tst_leak['compiled_leak'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d681176d6c0dd3b5fdcdd6ea610fd229368ea57f"},"cell_type":"markdown","source":"### Train lightgbm"},{"metadata":{"trusted":true,"_uuid":"c8d31ec188185853b05a562a97ffa6982b52e1ce"},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# Use all features for stats\nfeatures = [f for f in data if f not in ['ID', 'leak', 'log_leak', 'target']]\ndata.replace(0, np.nan, inplace=True)\ndata['log_of_mean'] = np.log1p(data[features].replace(0, np.nan).mean(axis=1))\ndata['mean_of_log'] = np.log1p(data[features]).replace(0, np.nan).mean(axis=1)\ndata['log_of_median'] = np.log1p(data[features].replace(0, np.nan).median(axis=1))\ndata['nb_nans'] = data[features].isnull().sum(axis=1)\ndata['the_sum'] = np.log1p(data[features].sum(axis=1))\ndata['the_std'] = data[features].std(axis=1)\ndata['the_kur'] = data[features].kurtosis(axis=1)\n\ntest.replace(0, np.nan, inplace=True)\ntest['log_of_mean'] = np.log1p(test[features].replace(0, np.nan).mean(axis=1))\ntest['mean_of_log'] = np.log1p(test[features]).replace(0, np.nan).mean(axis=1)\ntest['log_of_median'] = np.log1p(test[features].replace(0, np.nan).median(axis=1))\ntest['nb_nans'] = test[features].isnull().sum(axis=1)\ntest['the_sum'] = np.log1p(test[features].sum(axis=1))\ntest['the_std'] = test[features].std(axis=1)\ntest['the_kur'] = test[features].kurtosis(axis=1)\n\n# Only use good features, log leak and stats for training\nfeatures = good_features.tolist()\nfeatures = features + ['log_leak', 'log_of_mean', 'mean_of_log', 'log_of_median', 'nb_nans', 'the_sum', 'the_std', 'the_kur']\ndtrain = lgb.Dataset(data=data[features], \n                     label=target, free_raw_data=False)\ntest['target'] = 0\n\ndtrain.construct()\noof_preds = np.zeros(data.shape[0])\n\nfor trn_idx, val_idx in folds.split(data):\n    lgb_params = {\n        'objective': 'regression',\n        'num_leaves': 58,\n        'subsample': 0.6143,\n        'colsample_bytree': 0.6453,\n        'min_split_gain': np.power(10, -2.5988),\n        'reg_alpha': np.power(10, -2.2887),\n        'reg_lambda': np.power(10, 1.7570),\n        'min_child_weight': np.power(10, -0.1477),\n        'verbose': -1,\n        'seed': 3,\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.05,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=lgb_params,\n        train_set=dtrain.subset(trn_idx),\n        valid_sets=dtrain.subset(val_idx),\n        num_boost_round=10000, \n        early_stopping_rounds=100,\n        verbose_eval=0\n    )\n\n    oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n    test['target'] += clf.predict(test[features]) / folds.n_splits\n    print(mean_squared_error(target.iloc[val_idx], \n                             oof_preds[val_idx]) ** .5)\n\ndata['predictions'] = oof_preds\ndata.loc[data['leak'].notnull(), 'predictions'] = np.log1p(data.loc[data['leak'].notnull(), 'leak'])\nprint('OOF SCORE : %9.6f' \n      % (mean_squared_error(target, oof_preds) ** .5))\nprint('OOF SCORE with LEAK : %9.6f' \n      % (mean_squared_error(target, data['predictions']) ** .5))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"949a9ce471a9041d842f94a3a2ad139587b5edd1"},"cell_type":"markdown","source":"### Save submission"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"513e25d309100480da7dcacf2049598b69d7fbe6"},"cell_type":"code","source":"test['target'] = np.expm1(test['target'])\ntest.loc[test['leak'].notnull(), 'target'] = test.loc[test['leak'].notnull(), 'leak']\ntest[['ID', 'target']].to_csv('leaky_submission.csv', index=False, float_format='%.2f')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "metadata": {
+    "_uuid": "caa44bc10a9d34d96916eba5a4f90dba73e84735"
+   },
+   "cell_type": "markdown",
+   "source": "Due to leaks found in the past week, I wondered how it would modify the simple XGB scoring method demonstrated in this notebook.\n\nFor this purpose I use the results found in : https://www.kaggle.com/johnfarrell/breaking-lb-fresh-start-with-lag-selection/output\n"
+  },
+  {
+   "metadata": {
+    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
+    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
+    "trusted": true,
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "import numpy as np\nimport pandas as pd\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "8e9fddf207ae05e8c1d0e76ce43f12df6252acee"
+   },
+   "cell_type": "code",
+   "source": "%%time\ndata = pd.read_csv('../input/santander-value-prediction-challenge/train.csv')\ntarget = np.log1p(data['target'])\ndata.drop(['ID', 'target'], axis=1, inplace=True)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "632e6a2563936127c06287cfc3a1cf309385934e"
+   },
+   "cell_type": "markdown",
+   "source": "### Add train leak"
+  },
+  {
+   "metadata": {
+    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
+    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "%%time\nleak = pd.read_csv('../input/breaking-lb-fresh-start-with-lag-selection/train_leak.csv')\ndata['leak'] = leak['compiled_leak'].values\ndata['log_leak'] = np.log1p(leak['compiled_leak'].values)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "0c8610c5187001d7ec17a7ffaf203859c6f9aa46"
+   },
+   "cell_type": "markdown",
+   "source": "### Feature Scoring using XGBoost with the leak feature"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "68e309e111e4a772a6ec4d8cf00c11e134c23f05",
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "%%time\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred) ** .5\n\nreg = XGBRegressor(n_estimators=1000)\nfolds = KFold(4, True, 134259)\nfold_idx = [(trn_, val_) for trn_, val_ in folds.split(data)]\nscores = []\n\nnb_values = data.nunique(dropna=False)\nnb_zeros = (data == 0).astype(np.uint8).sum(axis=0)\n\nfeatures = [f for f in data.columns if f not in ['log_leak', 'leak', 'target', 'ID']]\nfor _f in features:\n    score = 0\n    for trn_, val_ in fold_idx:\n        reg.fit(\n            data[['log_leak', _f]].iloc[trn_], target.iloc[trn_],\n            eval_set=[(data[['log_leak', _f]].iloc[val_], target.iloc[val_])],\n            eval_metric='rmse',\n            early_stopping_rounds=50,\n            verbose=False\n        )\n        score += rmse(target.iloc[val_], reg.predict(data[['log_leak', _f]].iloc[val_], ntree_limit=reg.best_ntree_limit)) / folds.n_splits\n    scores.append((_f, score))",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "3694822b9bc81bb6896f417cee646e1990577917"
+   },
+   "cell_type": "markdown",
+   "source": "### Create dataframe"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "7863f32d4b51d6dc957453365d8e79682848f4e4",
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "report = pd.DataFrame(scores, columns=['feature', 'rmse']).set_index('feature')\nreport['nb_zeros'] = nb_zeros\nreport['nunique'] = nb_values\nreport.sort_values(by='rmse', ascending=True, inplace=True)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "60fdbaefae3d78b8f27cb109d2f5caf38fd9646a"
+   },
+   "cell_type": "markdown",
+   "source": "### Plot a few diagrams"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "660318ef180911d56dc1fadca3ea196917a92c22",
+    "_kg_hide-input": true,
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "plt.figure(figsize=(10, 7))\nplt.xlabel('Number of zeros in the feature', fontsize=14)\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\nplt.title('Feature score vs number of zeros', fontsize=16, fontweight='bold', color='#ae3453')\nplt.scatter(report['nb_zeros'], report['rmse'])",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "deb4e77513ce86867a6d653e9da6c74e2aed768d",
+    "_kg_hide-input": true,
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "fig, ax = plt.subplots(figsize=(10, 7))\nplt.xlabel('Number of unique values in the feature', fontsize=14)\nplt.ylabel('Feature RMSE (on np.log1p)', fontsize=14)\nax.set_title('Feature score vs number of unique values', fontsize=16, fontweight='bold', color='#ae3453')\nscatter = ax.scatter(report['nunique'], report['rmse'])",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_kg_hide-input": true,
+    "trusted": true,
+    "_uuid": "b8231eee7ab03863592853a2c8aa59e7a39d2e19",
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "from bokeh.plotting import figure, show, output_file, output_notebook, ColumnDataSource\n\nreport.sort_values('rmse', ascending=False, inplace=True)\n\nradii = 1000 * (report['rmse'].max() - report['rmse']).values\n\nsource = ColumnDataSource(data=dict(\n    x=report['nunique'].tolist(),\n    y=report['nb_zeros'].tolist(),\n    desc=report.index.tolist(),\n    radius=radii,\n    fill_color=[\n       \"#%02x%02x%02x\" % (int(r), 100, 150) for r in 255 * ((report['rmse'].max() - report['rmse']) / (report['rmse'].max() - report['rmse'].min())).values\n    ],\n    rmse=report['rmse'].tolist()\n))\n\nTOOLTIPS = [\n    (\"rmse\", \"@rmse\"),\n    (\"(nunique, nb_zeros)\", \"(@x, @y)\"),\n    (\"feature\", \"@desc\"),\n]\nTOOLS = \"hover, crosshair, pan, wheel_zoom, zoom_in, zoom_out, box_zoom, undo, redo, reset, tap, save, box_select, poly_select, lasso_select\"\n\np = figure(plot_width=600, plot_height=600, tooltips=TOOLTIPS, tools=TOOLS,\n           title=\"Number of unique values vs Number of zeros\")\np.xaxis.axis_label = 'Number of unique values in feature'\np.yaxis.axis_label = 'Number of zeros in feature'\np.xaxis.axis_label_text_font_style ='bold'\np.yaxis.axis_label_text_font_style ='bold'\np.title.text_color = '#ae3453'\np.title.text_font_size = '16pt'\np.scatter(\n    'x', 'y', source=source,\n    radius='radius',\n    fill_color='fill_color',\n    line_color=None,\n    fill_alpha=0.8\n)\n\noutput_notebook()\n\nshow(p)  # open a browser",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "4db7b58fc5b7dc4acb58b0b23a3e05f46dc612e2",
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "report.to_csv('feature_report.csv', index=True)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "0036802c721bc8082d74e17fd9454aed9939a386"
+   },
+   "cell_type": "markdown",
+   "source": "### Select some features (threshold is not optimized)"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "7a8a6ce954124e2a42dddc19c5f55174f86c8984",
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "good_features = report.loc[report['rmse'] <= 0.7925].index\nrmses = report.loc[report['rmse'] <= 0.7925, 'rmse'].values\ngood_features",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "75dfb7f6467ba6cddc0648d4452fa4968c63c484"
+   },
+   "cell_type": "code",
+   "source": "test = pd.read_csv('../input/santander-value-prediction-challenge/test.csv')",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "2c4a7885aae0243a8e5e97ad7b564f2f0d1804ba"
+   },
+   "cell_type": "markdown",
+   "source": "### Display distributions of test and train for selected features"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "9172be793b24b90bc0d729ee5c2583cd1c74c60b",
+    "collapsed": true
+   },
+   "cell_type": "code",
+   "source": "for i, f in enumerate(good_features):\n    plt.subplots(figsize=(10, 3))\n    plt.title('Feature %s RMSE %.3f train/test distributions' % (f, rmses[i]), fontsize=16, fontweight='bold', color='#ae3453')\n    hists = plt.hist(np.log1p(data[f].replace(0, np.nan).dropna().values), alpha=.7, label='train', \n             bins=50, density=True,  histtype='bar')\n    plt.hist(np.log1p(test[f].replace(0, np.nan).dropna().values), alpha=.5, label='test', \n             bins=hists[1], density=True, histtype='bar')\n    plt.legend()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "collapsed": true,
+    "_uuid": "9144fdbff21261ea77f7a900f15ef079474f4b16"
+   },
+   "cell_type": "markdown",
+   "source": "### Add leak to test"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "4ed1665c3928cd9063296de5791a7f9b869a7619"
+   },
+   "cell_type": "code",
+   "source": "%%time\ntst_leak = pd.read_csv('../input/breaking-lb-fresh-start-with-lag-selection/test_leak.csv')\ntest['leak'] = tst_leak['compiled_leak']\ntest['log_leak'] = np.log1p(tst_leak['compiled_leak'])",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "d681176d6c0dd3b5fdcdd6ea610fd229368ea57f"
+   },
+   "cell_type": "markdown",
+   "source": "### Train lightgbm"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_uuid": "c8d31ec188185853b05a562a97ffa6982b52e1ce"
+   },
+   "cell_type": "code",
+   "source": "from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=1)\n\n# Use all features for stats\nfeatures = [f for f in data if f not in ['ID', 'leak', 'log_leak', 'target']]\ndata.replace(0, np.nan, inplace=True)\ndata['log_of_mean'] = np.log1p(data[features].replace(0, np.nan).mean(axis=1))\ndata['mean_of_log'] = np.log1p(data[features]).replace(0, np.nan).mean(axis=1)\ndata['log_of_median'] = np.log1p(data[features].replace(0, np.nan).median(axis=1))\ndata['nb_nans'] = data[features].isnull().sum(axis=1)\ndata['the_sum'] = np.log1p(data[features].sum(axis=1))\ndata['the_std'] = data[features].std(axis=1)\ndata['the_kur'] = data[features].kurtosis(axis=1)\n\ntest.replace(0, np.nan, inplace=True)\ntest['log_of_mean'] = np.log1p(test[features].replace(0, np.nan).mean(axis=1))\ntest['mean_of_log'] = np.log1p(test[features]).replace(0, np.nan).mean(axis=1)\ntest['log_of_median'] = np.log1p(test[features].replace(0, np.nan).median(axis=1))\ntest['nb_nans'] = test[features].isnull().sum(axis=1)\ntest['the_sum'] = np.log1p(test[features].sum(axis=1))\ntest['the_std'] = test[features].std(axis=1)\ntest['the_kur'] = test[features].kurtosis(axis=1)\n\n# Only use good features, log leak and stats for training\nfeatures = good_features.tolist()\nfeatures = features + ['log_leak', 'log_of_mean', 'mean_of_log', 'log_of_median', 'nb_nans', 'the_sum', 'the_std', 'the_kur']\ndtrain = lgb.Dataset(data=data[features], \n                     label=target, free_raw_data=False)\ntest['target'] = 0\n\ndtrain.construct()\noof_preds = np.zeros(data.shape[0])\n\nfor trn_idx, val_idx in folds.split(data):\n    lgb_params = {\n        'objective': 'regression',\n        'num_leaves': 58,\n        'subsample': 0.6143,\n        'colsample_bytree': 0.6453,\n        'min_split_gain': np.power(10, -2.5988),\n        'reg_alpha': np.power(10, -2.2887),\n        'reg_lambda': np.power(10, 1.7570),\n        'min_child_weight': np.power(10, -0.1477),\n        'verbose': -1,\n        'seed': 3,\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.05,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=lgb_params,\n        train_set=dtrain.subset(trn_idx),\n        valid_sets=dtrain.subset(val_idx),\n        num_boost_round=10000, \n        early_stopping_rounds=100,\n        verbose_eval=0\n    )\n\n    oof_preds[val_idx] = clf.predict(dtrain.data.iloc[val_idx])\n    test['target'] += clf.predict(test[features]) / folds.n_splits\n    print(mean_squared_error(target.iloc[val_idx], \n                             oof_preds[val_idx]) ** .5)\n\ndata['predictions'] = oof_preds\ndata.loc[data['leak'].notnull(), 'predictions'] = np.log1p(data.loc[data['leak'].notnull(), 'leak'])\nprint('OOF SCORE : %9.6f' \n      % (mean_squared_error(target, oof_preds) ** .5))\nprint('OOF SCORE with LEAK : %9.6f' \n      % (mean_squared_error(target, data['predictions']) ** .5))\n",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "949a9ce471a9041d842f94a3a2ad139587b5edd1"
+   },
+   "cell_type": "markdown",
+   "source": "### Save submission"
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "collapsed": true,
+    "_uuid": "513e25d309100480da7dcacf2049598b69d7fbe6"
+   },
+   "cell_type": "code",
+   "source": "test['target'] = np.expm1(test['target'])\ntest.loc[test['leak'].notnull(), 'target'] = test.loc[test['leak'].notnull(), 'leak']\ntest[['ID', 'target']].to_csv('leaky_submission.csv', index=False, float_format='%.2f')",
+   "execution_count": null,
+   "outputs": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "name": "python",
+   "version": "3.6.6",
+   "mimetype": "text/x-python",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "file_extension": ".py"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 1
+}
\ No newline at end of file
Index: dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"metadata\":{\"_uuid\":\"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\"_cell_guid\":\"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import os\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#libs\\nfrom copy import deepcopy\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nimport os\\nfrom tqdm.notebook import tqdm\\nimport types\\nimport ast\\nimport gc\\ndef imports():\\n    for name, val in globals().items():\\n        # module imports\\n        if isinstance(val, types.ModuleType):\\n            yield name, val\\n        # functions / callables\\n        if hasattr(val, '__call__'):\\n            yield name, val\\nnp.seterr(divide='ignore', invalid='ignore')\\nnoglobal = lambda fn: types.FunctionType(fn.__code__, dict(imports()))\\nimport collections\\nfrom scipy.sparse import lil_matrix\\nimport scipy.sparse\\n%load_ext Cython\\nfrom itertools import chain\\nfrom IPython.display import display, HTML\\nimport lightgbm as lgb\\nfrom pprint import pprint\\nfrom sklearn.metrics import roc_auc_score\\nimport warnings\\nwarnings.filterwarnings('ignore', 'Mean of empty slice')\\n\\n#for GPU\\nimport torch\\nfrom torch import nn\\nimport torch.utils.data as torchdata\\nimport torch.nn.functional as F\\nimport math\\nfrom collections import OrderedDict\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#utils\\nclass RiiidEnv:\\n    def __init__(self, sn, iterate_wo_predict = False):\\n        self.sn = sn\\n        self.sub = sn.loc[sn['content_type_id'] == 0][['row_id']].copy()\\n        self.sub['answered_correctly'] = 0.5\\n        self.can_yield = True\\n        self.iterate_wo_predict = iterate_wo_predict\\n        self.num_groups = self.sn.index.max() + 1\\n        \\n    def iter_test(self):\\n        for i in range(self.num_groups):\\n            self.i = i\\n            assert(self.can_yield)\\n            if not self.iterate_wo_predict:\\n                self.can_yield = False\\n\\n            if i in self.sub.index:\\n                yield self.sn.loc[[i]], self.sub.loc[[i]]\\n            elif i not in self.sub.index:\\n                yield self.sn.loc[[i]], None\\n                    \\n    def predict(self, my_sub):\\n        assert(my_sub['row_id'].dtype == 'int64')\\n        assert(my_sub['answered_correctly'].dtype == 'float64')\\n        assert(my_sub.index.name == 'group_num')\\n        assert(my_sub.index.dtype == 'int64')\\n        \\n        if self.i in self.sub.index:\\n            assert(np.all(my_sub['answered_correctly'] >= 0))\\n            assert(np.all(my_sub['answered_correctly'] <= 1))\\n            assert(np.all(my_sub.index == self.i))\\n            assert(np.all(my_sub['row_id'] == self.sub.loc[[self.i]]['row_id']))\\n            self.sub.loc[[self.i]] = my_sub\\n            self.can_yield = True\\n            \\n        elif self.i not in self.sub.index:\\n            assert(my_sub.shape[0] == 0)\\n            self.can_yield = True\\n\\n@noglobal\\ndef save_pickle(obj, path):\\n    with open(path, mode='wb') as f:\\n        pickle.dump(obj, f)\\n\\n@noglobal\\ndef load_pickle(path):\\n    with open(path, mode='rb') as f:\\n        obj = pickle.load(f)\\n    return obj\\n\\n@noglobal\\ndef encode(train, column_name):\\n    encoded = pd.merge(train[[column_name]], pd.DataFrame(train[column_name].unique(), columns=[column_name])\\\\\\n                       .reset_index().dropna(), how='left', on=column_name)[['index']].rename(\\n        columns={'index': column_name})\\n    return encoded\\n\\n@noglobal\\ndef update_user_map(tes, user_map_ref, n_users_ref):\\n    #new_users = tes[tes['timestamp'] == 0]['user_id'].unique()\\n    users = tes['user_id'].unique()\\n    keys = user_map_ref.keys()\\n    new_users = users[np.array([user not in keys for user in users])]\\n    n_new_users = new_users.shape[0]\\n    if n_new_users > 0:\\n        user_map_ref[new_users] = np.arange(n_users_ref, n_users_ref + n_new_users)\\n    return user_map_ref, n_users_ref + n_new_users\\n    \\n@noglobal\\ndef write_to_ref_map(path, ref_name, f_names):\\n    ref_map = load_pickle(path)\\n    ref_map[ref_name] = f_names\\n    save_pickle(ref_map, path)\\n    \\nclass VectorizedDict():\\n    def __init__(self):\\n        self.tr_dict = dict()\\n        self.set_value = np.vectorize(self.tr_dict.__setitem__)\\n        self.get_value = np.vectorize(self.tr_dict.__getitem__)\\n        \\n    def keys(self):\\n        return self.tr_dict.keys()\\n        \\n    def __setitem__(self, indices, values):\\n        self.set_value(indices, values)\\n    \\n    def __getitem__(self, indices):\\n        if indices.shape[0] == 0:\\n            return np.array([], dtype=np.int32)\\n        return self.get_value(indices)    \",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"%%cython \\nimport numpy as np\\ncimport numpy as np\\n\\ncpdef np.ndarray[int] cget_memory_indices(np.ndarray task):\\n    \\n    cdef Py_ssize_t n = task.shape[1]\\n    cdef np.ndarray[int, ndim = 2] res = np.zeros_like(task, dtype = np.int32)\\n    cdef np.ndarray[int] tmp_counter = np.full(task.shape[0], -1, dtype = np.int32)\\n    cdef np.ndarray[int] u_counter = np.full(task.shape[0], task.shape[1] - 1, dtype = np.int32)\\n    \\n    for i in range(n):\\n        res[:, i] = u_counter\\n        tmp_counter += 1\\n        if i != n - 1:\\n            mask = (task[:, i] != task[:, i + 1])\\n            u_counter[mask] = tmp_counter[mask]\\n    return res\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#func_gpu\\n@noglobal\\ndef nn_online_get_content_id_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = spc_tes['content_id'].values\\n    return pd.DataFrame(res, columns = ['nn_content_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_content_id_history(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    content_ids = spc_prev_tes_cp['content_id'].values\\n    for user, content_id in zip(enc_users, content_ids):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = content_id\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_normed_log_timestamp_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    spc_tes['log_timestamp'] = np.log1p(spc_tes['timestamp'].values.astype(np.float32))\\n    std = 3.3530\\n    mean = 20.863\\n    spc_tes['normed_log_timestamp'] = (spc_tes['log_timestamp'].values - mean)/std\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = spc_tes['normed_log_timestamp'].values\\n    return pd.DataFrame(res, columns = ['nn_normed_log_timestamp_history_' + str(i) + '_length_' + str(n_sample) \\\\\\n                                        for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_normed_log_timestamp_history(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    timestamp = spc_prev_tes_cp['timestamp'].values.astype(np.float32)\\n    std = 3.3530\\n    mean = 20.863\\n    normed_log_timestamps = (np.log1p(timestamp) - mean)/std\\n    for user, normed_log_timestamp in zip(enc_users, normed_log_timestamps):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = normed_log_timestamp\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_correctness_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = 2\\n    return pd.DataFrame(res, columns = ['nn_correctness_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_correctness_history(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    targets = spc_prev_tes_cp['answered_correctly'].values\\n    for user, target in zip(enc_users, targets):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = target\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_question_had_explanation_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = 2\\n    return pd.DataFrame(res, columns = ['nn_question_had_explanation_history_' + str(i) + '_length_' + str(n_sample) \\\\\\n                                        for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_early_update_reference_get_question_had_explanation_history(r_ref, tes, user_map_ref):\\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\\\\n                    & (~tes['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    explanations = spc_tes_cp['prior_question_had_explanation'].values.astype('int')\\n    for explanation, idx in zip(explanations, row_idx):\\n        r_ref[idx, r_ref[idx] == 2] = explanation\\n    return r_ref\\n\\n@noglobal\\ndef nn_update_reference_get_question_had_explanation_history(r_ref, prev_tes, tes, user_map_ref):\\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\\n    for user in row_idx:\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = 2\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_normed_elapsed_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = -999\\n    return pd.DataFrame(res, columns = ['nn_normed_elapsed_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_early_update_reference_get_normed_elapsed_history(r_ref, tes, user_map_ref):\\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\\\\n                    & (~tes['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    elapsed = spc_tes_cp['prior_question_elapsed_time'].values\\n    clipped = np.clip(elapsed, 0, 1000 * 300).astype(np.float32)\\n    mean = 25953\\n    std = 20418\\n    normed_elapsed = (clipped - mean)/std\\n    for el, idx in zip(normed_elapsed, row_idx):\\n        r_ref[idx, r_ref[idx] == -999] = el\\n    return r_ref\\n\\n@noglobal\\ndef nn_update_reference_get_normed_elapsed_history(r_ref, prev_tes, tes, user_map_ref):\\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\\n    for user in row_idx:\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = -999\\n    return r_ref\\n\\n@noglobal\\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\\n    spc_tes['count'] = np.bincount(enc)[enc]\\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_normed_modified_timedelta_history(r_ref, tes, f_tes_delta, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n\\n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\\n    mean = 126568\\n    std = 218000\\n    clipped = (clipped - mean)/std\\n    clipped[np.isnan(clipped)] = 0\\n    spc_tes['normed_modified_timedelta'] = clipped.astype(np.float32)\\n\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = spc_tes['normed_modified_timedelta'].values\\n    return pd.DataFrame(res, columns = ['nn_normed_modified_timedelta_history_' + str(i) + '_length_' + str(n_sample) \\\\\\n                                        for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_normed_modified_timedelta_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    \\n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\\n    mean = 126568\\n    std = 218000\\n    clipped = (clipped - mean)/std\\n    clipped[np.isnan(clipped)] = 0\\n    spc_prev_tes_cp['normed_modified_timedelta'] = clipped.astype(np.float32)\\n\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    normed_modified_timedeltas = spc_prev_tes_cp['normed_modified_timedelta'].values\\n    for user, normed_modified_timedelta in zip(enc_users, normed_modified_timedeltas):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = normed_modified_timedelta\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_user_answer_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = 4\\n    return pd.DataFrame(res, columns = ['nn_user_answer_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_user_answer_history(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    targets = spc_prev_tes_cp['user_answer'].values\\n    for user, target in zip(enc_users, targets):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = target\\n    return r_ref\\n\\n@noglobal\\ndef online_get_task_container_id_diff(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = spc_tes['task_container_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    return pd.DataFrame(res, columns = ['task_container_id_diff']).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_task_container_id_diff(r_ref, prev_tes, tes, user_map_ref):\\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['task_container_id'].values\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_task_container_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    value = f_tes_delta['task_container_id_diff'].values\\n    value[np.isnan(value)] = 0\\n    spc_tes['task_container_id_diff'] = value\\n    \\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = spc_tes['task_container_id_diff'].values\\n    return pd.DataFrame(res, columns = ['nn_task_container_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\\\\n                                        for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_task_container_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    \\n    value = f_tes_delta['task_container_id_diff'].values\\n    value[np.isnan(value)] = 0\\n    spc_prev_tes_cp['task_container_id_diff'] = value\\n\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    task_container_id_diffs = spc_prev_tes_cp['task_container_id_diff'].values\\n    for user, task_container_id_diff in zip(enc_users, task_container_id_diffs):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = task_container_id_diff\\n    return r_ref\\n\\n@noglobal\\ndef online_get_content_type_id_diff(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = spc_tes['content_type_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    return pd.DataFrame(res, columns = ['content_type_id_diff']).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_content_type_id_diff(r_ref, prev_tes, tes, user_map_ref):\\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['content_type_id'].values\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_content_type_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    value = f_tes_delta['content_type_id_diff'].values\\n    value[np.isnan(value)] = 0\\n    spc_tes['content_type_id_diff'] = value\\n    \\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = spc_tes['content_type_id_diff'].values\\n    return pd.DataFrame(res, columns = ['nn_content_type_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\\\\n                                        for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_content_type_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    \\n    value = f_tes_delta['content_type_id_diff'].values\\n    value[np.isnan(value)] = 0\\n    spc_prev_tes_cp['content_type_id_diff'] = value\\n\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    content_type_id_diffs = spc_prev_tes_cp['content_type_id_diff'].values\\n    for user, content_type_id_diff in zip(enc_users, content_type_id_diffs):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = content_type_id_diff\\n    return r_ref\\n\\n@noglobal\\ndef nn_online_get_task_container_id_history(r_ref, tes, user_map_ref):\\n    n_sample = r_ref.shape[1]\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\\n    res[:, n_sample] = spc_tes['task_container_id'].values\\n    return pd.DataFrame(res, columns = ['nn_task_container_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\\n\\n@noglobal\\ndef nn_update_reference_get_task_container_id_history(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    task_container_ids = spc_prev_tes_cp['task_container_id'].values\\n    for user, task_container_id in zip(enc_users, task_container_ids):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = task_container_id\\n    return r_ref\\n\\nclass Embedder(nn.Module):\\n    def __init__(self, n_proj, n_dims):\\n        super(Embedder, self).__init__()\\n        self.n_proj = n_proj\\n        self.n_dims = n_dims\\n        self.embed = nn.Embedding(n_proj, n_dims)\\n\\n    def forward(self, indices):\\n        z = self.embed(indices)\\n        return z\\n\\nclass PositionalEncoding(nn.Module):\\n\\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\\n        super(PositionalEncoding, self).__init__()\\n        self.dropout = nn.Dropout(p=dropout)\\n\\n        pe = torch.zeros(max_len, d_model)\\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\\n        pe[:, 0::2] = torch.sin(position * div_term)\\n        pe[:, 1::2] = torch.cos(position * div_term)\\n        pe = pe.unsqueeze(0).transpose(0, 1)\\n        self.register_buffer('pe', pe)\\n\\n    def forward(self, x):\\n        x = x + self.pe[:x.size(0), :]\\n        return self.dropout(x)\\n\\nclass MyEncoderLayer(nn.Module):\\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\\\"relu\\\"):\\n        super(MyEncoderLayer, self).__init__()\\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\\n        # Implementation of Feedforward model\\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\\n        self.dropout = nn.Dropout(dropout)\\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\\n\\n        self.norm1 = nn.LayerNorm(d_model)\\n        self.norm2 = nn.LayerNorm(d_model)\\n        self.dropout1 = nn.Dropout(dropout)\\n        self.dropout2 = nn.Dropout(dropout)\\n        if activation == 'relu':\\n            self.activation = F.relu\\n        elif activation == 'gelu':\\n            self.activation = F.gelu\\n\\n    def forward(self, q, k, v, src_mask, src_key_padding_mask):\\n        src2 = self.self_attn(q, k, v, attn_mask=src_mask,\\n                              key_padding_mask=src_key_padding_mask)[0]\\n        src = q + self.dropout1(src2)\\n        src = self.norm1(src)\\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\\n        src = src + self.dropout2(src2)\\n        src = self.norm2(src)\\n        return src\\n\\nclass MyEncoderExp162(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp162, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n        \\n        #conv\\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\\n\\n    def forward(self, batch, args):\\n        #input\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n            \\n        # relative positional encoding\\n        position = batch['position']\\n            \\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #key and value\\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n        \\n        #query process\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n        \\n        #key and value process\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n        \\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        \\n        #conv process\\n        conv_inputs_int = batch['conv_inputs_int']\\n        conv_inputs_float = batch['conv_inputs_float']\\n        conv_inputs_const = batch['conv_inputs_const']\\n\\n        conv_explanation = conv_inputs_int[:, :, :, 0]\\n        conv_correctness = conv_inputs_int[:, :, :, 1]\\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\\n    \\n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \\n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\\n        \\n        #concat \\n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n            \\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n            \\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n            \\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query[:, :n_sample].clone()\\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \\n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n            \\n            #conv process\\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\\n            \\n            \\n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\\n\\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \\n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\\n\\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp166(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp166, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n        \\n        #conv\\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\\n\\n    def forward(self, batch, args):\\n        #input\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n            \\n        # relative positional encoding\\n        position = batch['position']\\n            \\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #key and value\\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n        \\n        #query process\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n        \\n        #key and value process\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n        \\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        \\n        #conv process\\n        conv_inputs_int = batch['conv_inputs_int']\\n        conv_inputs_float = batch['conv_inputs_float']\\n        conv_inputs_const = batch['conv_inputs_const']\\n\\n        conv_explanation = conv_inputs_int[:, :, :, 0]\\n        conv_correctness = conv_inputs_int[:, :, :, 1]\\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\\n    \\n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \\n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\\n        \\n        #concat \\n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n            \\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n            \\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n            \\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query[:, :n_sample].clone()\\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \\n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n            \\n            #conv process\\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\\n            \\n            \\n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\\n\\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \\n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\\n\\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp184(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp184, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 16 * 8, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n        \\n        #conv\\n#         self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\\n#         self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\\n#         self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\\n        \\n        #rnn\\n        self.rnn = nn.GRU(13, 16, 8, batch_first = True)\\n\\n    def forward(self, batch, args):\\n        #input\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n            \\n        # relative positional encoding\\n        position = batch['position']\\n            \\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #key and value\\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n        \\n        #query process\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n        \\n        #key and value process\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n        \\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\\n        \\n        #conv process\\n        conv_inputs_int = batch['conv_inputs_int']\\n        conv_inputs_float = batch['conv_inputs_float']\\n        conv_inputs_const = batch['conv_inputs_const']\\n\\n        conv_explanation = conv_inputs_int[:, :, :, 0]\\n        conv_correctness = conv_inputs_int[:, :, :, 1]\\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\\n    \\n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \\n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\\n        \\n        #rnn\\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\\n        _, out_rnn = self.rnn(for_rnn)\\n        out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\\n\\n        #cat\\n        out = torch.cat([out, out_rnn, inputs_add], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 16 * 8)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n            \\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n            \\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n            \\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query[:, :n_sample].clone()\\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \\n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n            \\n            #conv process\\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\\n            \\n            \\n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\\n\\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \\n            const_normed_timedelta.unsqueeze(2)], dim = 2)\\n            \\n            #rnn\\n            for_rnn = for_conv\\n            _, out_rnn = self.rnn(for_rnn)\\n            out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, out_rnn, features], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp218(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp218, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #for rnn\\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.norm3 = nn.LayerNorm(args.emb_dim)\\n\\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm4 = nn.LayerNorm(args.emb_dim)\\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True, dropout = args.dropout)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n\\n    def forward(self, batch, args):\\n        #inputs\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n\\n        # relative positional encoding\\n        position = batch['position']\\n\\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #initial query and memory \\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #for rnn process\\n        memory[padding_mask] = memory[padding_mask] * 0\\n\\n        tmp, _ = self.rnn(memory)\\n        out_rnn = tmp.clone()\\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\\n\\n        memory_idx = batch['memory_idx']\\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\\n        memory_rnn = out_rnn[memory_idx_]\\n\\n        #new query and memory\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n\\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\\n\\n        #cat\\n        out = torch.cat([out, inputs_add], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n\\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n\\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n\\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n            memory[:, :-1][padding_mask] = 0\\n            memory[:, -1] = 0\\n            out_rnn, _ = self.rnn(memory)\\n            out_rnn[:, -1] = 0\\n\\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\\n            memory_idx = cget_memory_indices(task_container_id)\\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\\n\\n            #query process 2 \\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n            #key and value process 2\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n            memory = memory[:, :-1]\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, features], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp224(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp224, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #for rnn\\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.norm3 = nn.LayerNorm(args.emb_dim)\\n\\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm4 = nn.LayerNorm(args.emb_dim)\\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256 + 16 * 8, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n        \\n        self.fixlen_rnn = nn.GRU(13, 16, 8, batch_first = True)\\n\\n    def forward(self, batch, args):\\n        #inputs\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n\\n        # relative positional encoding\\n        position = batch['position']\\n\\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #initial query and memory \\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #for rnn process\\n        memory[padding_mask] = memory[padding_mask] * 0\\n\\n        tmp, _ = self.rnn(memory)\\n        out_rnn = tmp.clone()\\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\\n\\n        memory_idx = batch['memory_idx']\\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\\n        memory_rnn = out_rnn[memory_idx_]\\n\\n        #new query and memory\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n\\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\\n        \\n        #conv process\\n        conv_inputs_int = batch['conv_inputs_int']\\n        conv_inputs_float = batch['conv_inputs_float']\\n        conv_inputs_const = batch['conv_inputs_const']\\n\\n        conv_explanation = conv_inputs_int[:, :, :, 0]\\n        conv_correctness = conv_inputs_int[:, :, :, 1]\\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\\n    \\n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \\n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\\n        \\n        #rnn\\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\\n        _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\\n        out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\\n\\n        #cat\\n        out = torch.cat([out, out_fixlen_rnn, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256 + 16 * 8)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n\\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n\\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n\\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n            memory[:, :-1][padding_mask] = 0\\n            memory[:, -1] = 0\\n            out_rnn, _ = self.rnn(memory)\\n            out_rnn[:, -1] = 0\\n\\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\\n            memory_idx = cget_memory_indices(task_container_id)\\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\\n\\n            #query process 2 \\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n            #key and value process 2\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n            memory = memory[:, :-1]\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n            \\n            #conv process\\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\\n            \\n            \\n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\\n\\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\\n\\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \\n            const_normed_timedelta.unsqueeze(2)], dim = 2)\\n            \\n            #rnn\\n            for_rnn = for_conv\\n            _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\\n            out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, out_fixlen_rnn, features, memory_rnn[:, -1]], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp219(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp219, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #for rnn\\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.norm3 = nn.LayerNorm(args.emb_dim)\\n\\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm4 = nn.LayerNorm(args.emb_dim)\\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n\\n    def forward(self, batch, args):\\n        #inputs\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n\\n        # relative positional encoding\\n        position = batch['position']\\n\\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #initial query and memory \\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #for rnn process\\n        memory[padding_mask] = memory[padding_mask] * 0\\n\\n        tmp, _ = self.rnn(memory)\\n        out_rnn = tmp.clone()\\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\\n\\n        memory_idx = batch['memory_idx']\\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\\n        memory_rnn = out_rnn[memory_idx_]\\n\\n        #new query and memory\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n\\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\\n\\n        #cat\\n        out = torch.cat([out, inputs_add], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n\\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n\\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n\\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n            memory[:, :-1][padding_mask] = 0\\n            memory[:, -1] = 0\\n            out_rnn, _ = self.rnn(memory)\\n            out_rnn[:, -1] = 0\\n\\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\\n            memory_idx = cget_memory_indices(task_container_id)\\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\\n\\n            #query process 2 \\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n            #key and value process 2\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n            memory = memory[:, :-1]\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, features], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp221(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp221, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #for rnn\\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.norm3 = nn.LayerNorm(args.emb_dim)\\n\\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm4 = nn.LayerNorm(args.emb_dim)\\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n\\n    def forward(self, batch, args):\\n        #inputs\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n\\n        # relative positional encoding\\n        position = batch['position']\\n\\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #initial query and memory \\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #for rnn process\\n        memory[padding_mask] = memory[padding_mask] * 0\\n\\n        tmp, _ = self.rnn(memory)\\n        out_rnn = tmp.clone()\\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\\n\\n        memory_idx = batch['memory_idx']\\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\\n        memory_rnn = out_rnn[memory_idx_]\\n\\n        #new query and memory\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n\\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\\n\\n        #cat\\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n\\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n\\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n\\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n            memory[:, :-1][padding_mask] = 0\\n            memory[:, -1] = 0\\n            out_rnn, _ = self.rnn(memory)\\n            out_rnn[:, -1] = 0\\n\\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\\n            memory_idx = cget_memory_indices(task_container_id)\\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\\n\\n            #query process 2 \\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n            #key and value process 2\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n            memory = memory[:, :-1]\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n    \\nclass MyEncoderExp222(nn.Module):\\n    def __init__(self, args):\\n        super(MyEncoderExp222, self).__init__()\\n        #query, key and value\\n        self.embedder_content_id = Embedder(13523, 512)\\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.dropout = nn.Dropout(args.dropout)\\n        self.norm1 = nn.LayerNorm(args.emb_dim)\\n\\n        #key and value\\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm2 = nn.LayerNorm(args.emb_dim)\\n\\n        #for rnn\\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\\n        self.norm3 = nn.LayerNorm(args.emb_dim)\\n\\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\\n        self.norm4 = nn.LayerNorm(args.emb_dim)\\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\\n\\n        #MyEncoder\\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \\n                                                dropout = args.dropout)\\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\\n\\n    def forward(self, batch, args):\\n        #inputs\\n        inputs_int = batch['inputs_int']\\n        inputs_float = batch['inputs_float']\\n        inputs_add = batch['inputs_add'].transpose(0, 1)\\n        inputs_add_2 = batch['inputs_add_2']\\n        N = inputs_int.shape[0]\\n        n_length = int(inputs_int.shape[1]/2)\\n\\n        #for query, key and value\\n        content_id = inputs_int[:, :, 0]\\n        part = (inputs_int[:, :, 1] - 1)\\n        tags = inputs_int[:, :, 2:8]\\n        tag_mask = batch['tag_mask']\\n        #digit_timedelta = inputs_int[:, :, 8]\\n        normed_timedelta = inputs_float[:, :, 2]\\n        normed_log_timestamp = inputs_float[:, :, 1]\\n        correct_answer = inputs_int[:, :, 13]\\n        task_container_id_diff = inputs_add_2[:, :, 0]\\n        content_type_id_diff = inputs_add_2[:, :, 1]\\n\\n        #for key and value\\n        explanation = inputs_int[:, :, 9]\\n        correctness = inputs_int[:, :, 10]\\n        normed_elapsed = inputs_float[:, :, 0]\\n        user_answer = inputs_int[:, :, 11]\\n        end_pos_idx = batch['end_pos_idx']\\n\\n        #mask\\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\\n        padding_mask = batch['padding_mask']\\n\\n        #target, loss_mask\\n        target = batch['target']\\n        if 'loss_mask' in batch.keys():\\n            loss_mask = batch['loss_mask']\\n        else:\\n            loss_mask = batch['cut_mask']\\n\\n        # relative positional encoding\\n        position = batch['position']\\n\\n        #query, key and value\\n        emb_content_id = self.embedder_content_id(content_id)\\n        ohe_part = F.one_hot(part, num_classes = 7)\\n        ohe_tags = F.one_hot(tags, num_classes = 189)\\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n        #initial query and memory \\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n\\n        #for rnn process\\n        memory[padding_mask] = memory[padding_mask] * 0\\n\\n        tmp, _ = self.rnn(memory)\\n        out_rnn = tmp.clone()\\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\\n\\n        memory_idx = batch['memory_idx']\\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\\n        memory_rnn = out_rnn[memory_idx_]\\n\\n        #new query and memory\\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n        query_clone = query.clone()\\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n\\n        #transpose\\n        query = query.transpose(0, 1)\\n        memory = memory.transpose(0, 1)\\n\\n        #MyEncoder\\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\\n\\n        #cat\\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\\\\n        .reshape(-1, N).transpose(0, 1)\\n        score = torch.sigmoid(out)\\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\\n        return err, score\\n    \\n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\\n        torch.set_grad_enabled(False)\\n        N = inputs['content_id'].shape[0]\\n        scores = torch.zeros(N).to(args.device)\\n        for i in range(0, N, pred_bs):\\n            content_id = inputs['content_id'][i: i + pred_bs]\\n            n_batch = content_id.shape[0]\\n            n_sample = content_id.shape[1] - 1\\n\\n            padding_mask = (content_id == -1)[:, :n_sample]\\n            token_idx = padding_mask[:, -1].copy()\\n            padding_mask[:, -1] = False\\n\\n            part = que['part'].values[content_id] - 1\\n            tags = que_proc_int[content_id]\\n            tag_mask = (tags != 188)\\n            correct_answer = que['correct_answer'].values[content_id]\\n\\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\\n            explanation = inputs['explanation'][i: i + pred_bs]\\n            correctness = inputs['correctness'][i: i + pred_bs]\\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\\n            user_answer = inputs['user_answer'][i: i + pred_bs]\\n\\n            #diff features\\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\\n\\n            #preprop\\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\\n            content_id[content_id == -1] = 1\\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\\n            token_idx = torch.from_numpy(token_idx).to(args.device)\\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\\n\\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\\n            explanation[explanation == -2] = 1\\n            explanation[explanation == 2] = 1\\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\\n            correctness[correctness == -1] = 1\\n            correctness[correctness == 2] = 1\\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\\n            normed_elapsed[normed_elapsed == -999] = 0\\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\\n            user_answer[user_answer == -1] = 1\\n            user_answer[user_answer == 4] = 1\\n\\n            #diff features\\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\\n\\n            #generate token\\n            explanation[token_idx, n_sample - 1] = 2\\n            correctness[token_idx, n_sample - 1] = 2\\n            normed_elapsed[token_idx, n_sample - 1] = 0\\n            user_answer[token_idx, n_sample - 1] = 4\\n\\n            #for conv process\\n            explanation[:, :-1][padding_mask] = 2\\n            correctness[:, :-1][padding_mask] = 2\\n            normed_elapsed[:, :-1][padding_mask] = 0\\n            normed_timedelta[:, :-1][padding_mask] = 0\\n            normed_log_timestamp[:, :-1][padding_mask] = 0\\n\\n            #query, key and value\\n            emb_content_id = self.embedder_content_id(content_id)\\n            ohe_part = F.one_hot(part, num_classes = 7)\\n            ohe_tags = F.one_hot(tags, num_classes = 189)\\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\\n\\n            #key and value\\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\\n\\n            # relative positional encoding\\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\\n\\n            #query process\\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\\n\\n            #key and value process\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\\n            memory[:, :-1][padding_mask] = 0\\n            memory[:, -1] = 0\\n            out_rnn, _ = self.rnn(memory)\\n            out_rnn[:, -1] = 0\\n\\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\\n            memory_idx = cget_memory_indices(task_container_id)\\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\\n\\n            #query process 2 \\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \\n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \\n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \\n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\\n\\n            #key and value process 2\\n            query_clone = query.clone()\\n            query_clone[token_idx, -2] = 0\\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \\n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\\n            memory = memory[:, :-1]\\n\\n            #transpose\\n            query = query.transpose(0, 1)\\n            memory = memory.transpose(0, 1)\\n            spc_query = query[-1:]\\n\\n            #MyEncoder\\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\\n\\n            #features\\n            features = inputs['features'][i: i + pred_bs].copy()\\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\\n            features[np.isnan(features)] = 0\\n            features = torch.from_numpy(features).to(args.device)\\n\\n            #concat and last fc\\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\\n            score = torch.sigmoid(out_cat[:, 0])\\n            scores[i: i + pred_bs] = score\\n        return scores.cpu().numpy()\\n\\n#func_cpu\\n@noglobal\\ndef online_get_user_id_content_id_task_container_id(tes):\\n    f_tes = tes[tes['content_type_id'] == 0][['user_id', \\n                                           'content_id', 'task_container_id']].astype('float32').reset_index(drop = True)\\n    return f_tes\\n\\n@noglobal\\ndef online_get_timestamp_and_prior_question_elapsed_time_and_prior_question_had_explanation(tes):\\n    f_tes = tes[tes['content_type_id'] == 0][['timestamp', \\n                                              'prior_question_elapsed_time', 'prior_question_had_explanation']].astype('float32').reset_index(drop = True)\\n    return f_tes\\n\\n@noglobal\\ndef online_get_part(tes, que):\\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\\n    f_tes = pd.DataFrame(que['part'].values[spc_tes_cp['content_id'].values], columns = ['part']).astype(np.float32)\\n    return f_tes\\n\\n@noglobal\\ndef online_get_correct_answer(tes, que):\\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\\n    return f_tes\\n\\n@noglobal\\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\\n    spc_tes['count'] = np.bincount(enc)[enc]\\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\\n\\n@noglobal\\ndef online_get_tags(tes, que_proc_2):\\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\\n    return pd.DataFrame(que_proc_2.values[spc_tes_cp['content_id'].values], columns = ['tags_' + str(i) for i in range(6)])\\n\\n@noglobal\\ndef online_get_hell_rolling_mean_for_tags(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[:, :-1][user_map_ref[spc_tes['user_id'].values]]\\n    f_tes = pd.DataFrame(sum_count[:, :, 0]/sum_count[:, :, 1], columns = ['hell_rolling_mean_for_tag_' + str(i) for i in range(188)])\\n    return f_tes\\n\\n@noglobal\\ndef online_get_rolling_mean_sum_count(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    mean = sum_count[:, 0]/sum_count[:, 1]\\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\\n    res[:, 0] = mean\\n    res[:, 1:] = sum_count\\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\\n    res[:, 0] = mean\\n    res[:, 1:] = sum_count\\n    return pd.DataFrame(res, columns = ['target_full_mean', 'target_full_sum', 'target_count'])\\n\\n@noglobal\\ndef update_reference_get_rolling_mean_sum_count(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    cts = spc_prev_tes_cp['user_id'].value_counts()\\n    pos_cts = spc_prev_tes_cp['user_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\\n    r_ref[user_map_ref[pos_cts.index], 0] += pos_cts.values\\n    r_ref[user_map_ref[cts.index], 1] += cts.values\\n    return r_ref\\n\\n@noglobal\\ndef online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, tes, que_proc, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    col_idx = que_proc.values[spc_tes['content_id'].values]\\n    users = user_map_ref[spc_tes['user_id'].values]\\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\\n\\n    res = np.zeros((spc_tes.shape[0], 7, 3), dtype = np.float32)\\n    sliced = r_ref[row_idx, col_idx]\\n    res[:, :6, 1:] = sliced\\n    res[:, 6, 1:] = np.nansum(sliced, axis = 1)\\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\\n    res = res.reshape(-1, 3 * 7)\\n    f_names = sum([['tags_order_mean_' + str(i), 'tags_order_sum_' + str(i), 'tags_order_count_' + str(i)] for i in range(6)], [])\\n    f_names += ['whole_tags_order_mean', 'whole_tags_order_sum', 'whole_tags_order_count']\\n    return pd.DataFrame(res, columns = f_names)\\n\\n@noglobal\\ndef update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, prev_tes, tes, que_onehot, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n\\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(188)]).T\\n    r_ref[user_map_ref[uniq], :-1, 0] += sum_cts\\n\\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id']]\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(188)]).T\\n    r_ref[user_map_ref[uniq], :-1, 1] += cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_rolling_mean_sum_count_for_part(r_ref, tes, que, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    spc_tes['part'] = que['part'].values[spc_tes['content_id'].values]\\n    res = np.zeros((spc_tes.shape[0], 8, 3), dtype = np.float32)\\n    res[:, :7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res[:, 7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values], spc_tes['part'] - 1]\\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\\n    res = res.reshape(-1, 3 * 8)\\n    f_names = sum([['part_' + str(i) + '_mean', 'part_' + str(i) + '_sum', 'part_' + str(i) + '_count'] for i in range(1, 8)], []) + \\\\\\n    ['part_cut_mean', 'part_cut_sum', 'part_cut_count']\\n    return pd.DataFrame(res, columns = f_names)\\n\\n@noglobal\\ndef update_reference_get_rolling_mean_sum_count_for_part(r_ref, prev_tes, tes, que_part_onehot, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    \\n    for_sum = que_part_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(7)]).T\\n    r_ref[user_map_ref[uniq], :, 0] += sum_cts\\n    \\n    for_count = que_part_onehot.values[spc_prev_tes_cp['content_id']]\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(7)]).T\\n    r_ref[user_map_ref[uniq], :, 1] += cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_lec_rolling_count(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    return pd.DataFrame(r_ref[user_map_ref[spc_tes['user_id'].values]], columns = ['lec_rolling_count'])\\n\\n@noglobal\\ndef update_reference_get_lec_rolling_count(r_ref, prev_tes, tes, user_map_ref):\\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\\n    r_ref[user_map_ref[spc_prev_tes['user_id'].values]] += 1\\n    return r_ref\\n\\n@noglobal\\ndef online_get_lec_part_rolling_count(r_ref, tes, user_map_ref, que):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = np.zeros((spc_tes.shape[0], 8), dtype = np.float32)\\n    part_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res[:, :7] = part_count\\n    res[:, 7] = part_count[np.arange(spc_tes.shape[0]), que['part'].values[spc_tes['content_id'].values] - 1]\\n    return pd.DataFrame(res, columns = ['lec_part_' + str(i + 1) for i in range(7)] + ['lec_part_cut'])\\n\\n@noglobal\\ndef update_reference_get_lec_part_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\\n    col_idx = lec['part'].values[lec_map[spc_prev_tes['content_id'].values]] - 1\\n    r_ref[row_idx, col_idx] += 1\\n    return r_ref\\n\\n@noglobal\\ndef online_get_lec_type_of_rolling_count(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    return pd.DataFrame(res, columns = ['lec_type_of_' + str(i) for i in range(4)]).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_lec_type_of_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec_proc, lec_map):\\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\\n    col_idx = lec_proc['type_of'].values[lec_map[spc_prev_tes['content_id'].values]]\\n    r_ref[row_idx, col_idx] += 1\\n    return r_ref\\n\\n@noglobal\\ndef online_get_lec_tags_rolling_count(r_ref, tes, user_map_ref, que_proc):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\\n    users = user_map_ref[spc_tes['user_id'].values]\\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\\n    col_idx = que_proc.values[spc_tes['content_id'].values]\\n    \\n    sliced = r_ref[row_idx, col_idx]\\n    res[:, :6] = sliced\\n    res[:, 6] = np.nansum(sliced, axis = 1)\\n    return pd.DataFrame(res, columns = sum([['lec_tags_order_count_' + str(i)] for i in range(6)], []) + ['lec_whole_tags_order_count'])\\n\\n@noglobal\\ndef update_reference_get_lec_tags_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\\n    prev_tes_cp = prev_tes.copy()\\n    #prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 1].copy().reset_index(drop = True)\\n    col_idx = lec['tag'].values[lec_map[spc_prev_tes_cp['content_id'].values]]\\n    row_idx = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    r_ref[row_idx, col_idx] += 1\\n    return r_ref\\n\\n@noglobal \\ndef online_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    row_idx = user_map_ref[spc_tes['user_id'].values]\\n    col_idx = spc_tes['content_id'].values\\n    f_sum = r_ref_sum[row_idx, col_idx].toarray()[0]\\n    f_count = r_ref_count[row_idx, col_idx].toarray()[0]\\n    f_mean = f_sum/f_count\\n    f_tr = pd.DataFrame()\\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\\n    return f_tr\\n\\n@noglobal\\ndef update_reference_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    contents = spc_prev_tes_cp['content_id'].values\\n    targets = spc_prev_tes_cp['answered_correctly'].values\\n    for user, content, target in zip(users, contents, targets):\\n        r_ref_sum[user, content] = r_ref_sum[user, content] + target\\n        r_ref_count[user, content] = r_ref_count[user, content] + 1\\n    return r_ref_sum, r_ref_count\\n\\n@noglobal\\ndef online_get_timestamp_diff(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    return pd.DataFrame(res, columns = ['timestamp_diff']).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\\n    return r_ref\\n\\n@noglobal\\ndef online_get_whole_oof_target_encoding_content_id(r_ref, tes):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[spc_tes['content_id'].values]\\n    res = (sum_count[:, 0]/sum_count[:, 1]).astype(np.float32)\\n    return pd.DataFrame(res, columns = ['whole_oof_target_encoding_content_id'])\\n\\n@noglobal\\ndef update_reference_get_whole_oof_target_encoding_content_id(r_ref, prev_tes, tes):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    cts = spc_prev_tes_cp['content_id'].value_counts()\\n    pos_cts = spc_prev_tes_cp['content_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\\n    r_ref[pos_cts.index.values, 0] += pos_cts.values\\n    r_ref[cts.index.values, 1] += cts.values\\n    return r_ref\\n\\n@noglobal\\ndef online_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, tes, que_proc):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[que_proc.values[spc_tes['content_id'].values]]\\n    mean = sum_count[:, :, 0]/sum_count[:, :, 1]\\n    nansum = np.nansum(sum_count, axis = 1)\\n    whole_mean = nansum[:, 0]/nansum[:, 1]\\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\\n    res[:, :6] = mean\\n    res[:, 6] = whole_mean\\n    f_names = ['whole_oof_target_encoding_tags_order_' + str(i) for i in range(6)] + ['whole_oof_target_encoding_whole_tags']\\n    return pd.DataFrame(res, columns = f_names)\\n\\n@noglobal\\ndef update_reference_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, prev_tes, tes, que_onehot):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\\n    sum_cts = for_sum.sum(axis = 0)\\n    r_ref[:-1, 0] += sum_cts\\n\\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id'].values]\\n    cts = for_count.sum(axis = 0)\\n    r_ref[:-1, 1] += cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_correct_answer(tes, que):\\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\\n    return f_tes\\n\\n@noglobal\\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\\n    spc_tes['count'] = np.bincount(enc)[enc]\\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n\\n    for_count = np.zeros((spc_prev_tes_cp.shape[0], 4))\\n    for_count[np.arange(for_count.shape[0]), spc_prev_tes_cp['user_answer'].values] = 1\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(4)]).T\\n    r_ref[user_map_ref[uniq]] += cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_norm_rolling_count_and_cut_for_user_answer(r_ref, tes, que, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = np.zeros((spc_tes.shape[0], 5), dtype = np.float32)\\n    cts = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    norm_cts = cts/cts.sum(axis = 1)[:, None]\\n    res[:, :4] = norm_cts\\n    correct_answer = que['correct_answer'].values[spc_tes['content_id'].values]\\n    res[:, 4] = norm_cts[np.arange(norm_cts.shape[0]), correct_answer]\\n    f_names = ['norm_rolling_count_user_answer_' + str(i) for i in range(4)] + ['cut_norm_rolling_count_user_answer']\\n    return pd.DataFrame(res, columns = f_names)\\n\\n@noglobal\\ndef online_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res = sum_count[:, 0]/sum_count[:, 1]\\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\\n\\n@noglobal\\ndef early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\\n    tes_cp = tes.copy()\\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\\\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\\n    return r_ref\\n\\n@noglobal\\ndef online_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res = sum_count[:, 0]/sum_count[:, 1]\\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\\n\\n@noglobal\\ndef early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\\n    tes_cp = tes.copy()\\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\\\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values.astype('int') * r_ref[row_idx, 2]\\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\\n    return r_ref\\n\\n@noglobal\\ndef online_get_rolling_sum_for_prior_question_isnull(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    return pd.DataFrame(res, columns = ['rolling_sum_for_prior_question_isnull']).astype(np.float32)\\n\\n@noglobal\\ndef update_reference_get_rolling_sum_for_prior_question_isnull(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes[(prev_tes['content_type_id'] == 0) & (prev_tes['prior_question_elapsed_time'].isnull())].copy()\\n    r_ref[user_map_ref[prev_tes_cp['user_id'].values]] += prev_tes_cp['prior_question_elapsed_time'].isnull().astype('int').values\\n    return r_ref\\n\\n@noglobal\\ndef online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res = sum_count[:, 0]/sum_count[:, 1]\\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\\n\\n@noglobal\\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\\n    tes_cp = tes.copy()\\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\\\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\\n    return r_ref\\n\\n@noglobal\\ndef update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\\n    r_ref[user_map_ref[uniq], 2] = sum_cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res = sum_count[:, 0]/sum_count[:, 1]\\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\\n\\n@noglobal\\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\\n    tes_cp = tes.copy()\\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\\\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\\n    return r_ref\\n\\n@noglobal\\ndef update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\\n    r_ref[user_map_ref[uniq], 2] = sum_cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res = sum_count[:, 0]/sum_count[:, 1]\\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\\n\\n@noglobal\\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\\n    tes_cp = tes.copy()\\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\\\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\\n    return r_ref\\n\\n@noglobal\\ndef update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\\n    r_ref[user_map_ref[uniq], 2] = sum_cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\\n    res = sum_count[:, 0]/sum_count[:, 1]\\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\\n\\n@noglobal\\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\\n    tes_cp = tes.copy()\\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\\\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\\n    return r_ref\\n\\n@noglobal\\ndef update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\\n    r_ref[user_map_ref[uniq], 2] = sum_cts\\n    return r_ref\\n\\n@noglobal\\ndef online_get_diff_modified_timedelta_and_prior_elapsed_time_rolling_mean(f_tes_base, f_tes_pn, f_tes_p, f_tes_n):\\n    f_names = ['rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \\n                                   'positive_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \\n                                   'negative_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta']\\n    f_tes = pd.DataFrame()\\n    f_tes[f_names[0]] = f_tes_base.values[:, 0] - f_tes_pn.values[:, 0]\\n    f_tes[f_names[1]] = f_tes_base.values[:, 0] - f_tes_p.values[:, 0]\\n    f_tes[f_names[2]] = f_tes_base.values[:, 0] - f_tes_n.values[:, 0]\\n    return f_tes\\n\\n@noglobal\\ndef online_get_n_samples_rolling_mean(r_ref, tes, user_map_ref):\\n    n_samples = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 200])\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    users = spc_tes['user_id'].values\\n    history = r_ref[user_map_ref[users]].astype(np.float32)\\n    history[history == -1] = np.nan\\n    res = np.zeros((spc_tes.shape[0], len(n_samples)), dtype = np.float32)\\n    for i, n_sample in enumerate(n_samples):\\n        res[:, i] = np.nanmean(history[:, -n_sample:], axis = 1)\\n    return pd.DataFrame(res, columns = [str(i) + '_samples_rolling_mean' for i in n_samples])\\n\\n@noglobal\\ndef update_reference_get_n_samples_rolling_mean(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    targets = spc_prev_tes_cp['answered_correctly'].values\\n    for user, target in zip(enc_users, targets):\\n        r_ref[user, :-1] = r_ref[user, 1:]\\n        r_ref[user, -1] = target\\n    return r_ref\\n\\n@noglobal \\ndef online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, tes, user_map_ref):\\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    row_idx = user_map_ref[spc_tes['user_id'].values]\\n    col_idx = spc_tes['content_id'].values\\n    f_base = r_ref[row_idx, col_idx].toarray()[0]\\n    f_sum = f_base % 128\\n    f_count = f_base // 128\\n    f_mean = f_sum/f_count\\n    f_tr = pd.DataFrame()\\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\\n    return f_tr\\n\\n@noglobal\\ndef update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, prev_tes, tes, user_map_ref):\\n    prev_tes_cp = prev_tes.copy()\\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\\n    contents = spc_prev_tes_cp['content_id'].values\\n    targets = spc_prev_tes_cp['answered_correctly'].values\\n    for user, content, target in zip(users, contents, targets):\\n        r_ref[user, content] = r_ref[user, content] + target + 128\\n    return r_ref\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#initialization\\n#comment out\\n#prefix = '..'\\nprefix = '/kaggle/input/mamastan-gpu-v27'\\n#sn = pd.read_pickle('../others/ex_tes.pkl'); env = RiiidEnv(sn, iterate_wo_predict = False)\\nimport riiideducation; env = riiideducation.make_env()\\niter_test = env.iter_test()\\ngc.collect()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#params162\\nargs = OrderedDict()\\nargs.n_length = 200\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.n_conv = 30\\nargs.pred_bs = 256\\nargs.device = torch.device('cuda')\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp162 = deepcopy(args)\\ndel args;\\n\\n#params166\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.n_conv = 30\\nargs.pred_bs = 256\\nargs.device = torch.device('cuda')\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp166 = deepcopy(args)\\ndel args;\\n\\n#params184\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.n_conv = 30\\nargs.device = torch.device('cuda')\\nargs.pred_bs = 256\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\n\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\n\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\n\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\n\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp184 = deepcopy(args)\\ndel args;\\n\\n#params218\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.n_conv = 30\\nargs.device = torch.device('cuda')\\nargs.pred_bs = 256\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\n\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\n\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\n\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\n\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp218 = deepcopy(args)\\ndel args;\\n\\n#params224\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.n_conv = 30\\nargs.device = torch.device('cuda')\\nargs.pred_bs = 256\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\n\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp224 = deepcopy(args)\\ndel args;\\n\\n#params219\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.lr = 0.2 * 1e-3\\nargs.num_features = 90\\nargs.device = torch.device('cuda')\\nargs.pred_bs = 256\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\n\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp219 = deepcopy(args)\\ndel args;\\n\\n#params221\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.device = torch.device('cuda')\\nargs.pred_bs = 256\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\n\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp221 = deepcopy(args)\\ndel args;\\n\\n#params222\\nargs = OrderedDict()\\nargs.n_length = 400\\nargs.emb_dim = 512\\nargs.dim_feedforward = 1028\\nargs.nhead = 4\\nargs.dropout = 0.2\\nargs.dropout_pe = 0\\nargs.num_features = 90\\nargs.device = torch.device('cuda')\\nargs.pred_bs = 256\\n\\nf_names = ['target_full_mean',\\n 'target_count',\\n 'tags_order_mean_0',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_mean_1',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_mean_2',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_mean_3',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_mean_4',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_mean_5',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_mean',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_mean',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_mean',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_mean',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_mean',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_mean',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_mean',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_mean',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_mean',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_mean',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'norm_rolling_count_user_answer_0',\\n 'norm_rolling_count_user_answer_1',\\n 'norm_rolling_count_user_answer_2',\\n 'norm_rolling_count_user_answer_3',\\n 'cut_norm_rolling_count_user_answer',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'rolling_mean_for_prior_question_had_explanation',\\n 'rolling_sum_for_prior_question_isnull',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_had_explanation',\\n 'negative_rolling_mean_for_prior_question_had_explanation',\\n '1_samples_rolling_mean',\\n '2_samples_rolling_mean',\\n '3_samples_rolling_mean',\\n '4_samples_rolling_mean',\\n '5_samples_rolling_mean',\\n '10_samples_rolling_mean',\\n '20_samples_rolling_mean',\\n '30_samples_rolling_mean',\\n '40_samples_rolling_mean',\\n '50_samples_rolling_mean',\\n '100_samples_rolling_mean',\\n '200_samples_rolling_mean']\\nlog_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'lec_part_1',\\n 'lec_part_2',\\n 'lec_part_3',\\n 'lec_part_4',\\n 'lec_part_5',\\n 'lec_part_6',\\n 'lec_part_7',\\n 'lec_part_cut',\\n 'lec_type_of_0',\\n 'lec_type_of_1',\\n 'lec_type_of_2',\\n 'lec_tags_order_count_0',\\n 'lec_tags_order_count_1',\\n 'lec_tags_order_count_2',\\n 'lec_whole_tags_order_count',\\n 'content_id_cut_sum',\\n 'content_id_cut_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_f_names = [\\n 'target_count',\\n 'tags_order_sum_0',\\n 'tags_order_count_0',\\n 'tags_order_sum_1',\\n 'tags_order_count_1',\\n 'tags_order_sum_2',\\n 'tags_order_count_2',\\n 'tags_order_sum_3',\\n 'tags_order_count_3',\\n 'tags_order_sum_4',\\n 'tags_order_count_4',\\n 'tags_order_sum_5',\\n 'tags_order_count_5',\\n 'whole_tags_order_sum',\\n 'whole_tags_order_count',\\n 'part_1_sum',\\n 'part_1_count',\\n 'part_2_sum',\\n 'part_2_count',\\n 'part_3_sum',\\n 'part_3_count',\\n 'part_4_sum',\\n 'part_4_count',\\n 'part_5_sum',\\n 'part_5_count',\\n 'part_6_sum',\\n 'part_6_count',\\n 'part_7_sum',\\n 'part_7_count',\\n 'part_cut_sum',\\n 'part_cut_count',\\n 'lec_rolling_count',\\n 'rolling_mean_for_prior_question_elapsed_time',\\n 'positive_rolling_mean_for_prior_question_elapsed_time',\\n 'negative_rolling_mean_for_prior_question_elapsed_time']\\nstdize_params = {'target_count': (0, 5.929629),\\n 'tags_order_sum_0': (0, 2.034838),\\n 'tags_order_count_0': (0, 2.360298),\\n 'tags_order_sum_1': (0, 2.352074),\\n 'tags_order_count_1': (0, 2.646724),\\n 'tags_order_sum_2': (0, 3.368914),\\n 'tags_order_count_2': (0, 3.698544),\\n 'tags_order_sum_3': (0, 3.515896),\\n 'tags_order_count_3': (0, 3.854377),\\n 'tags_order_sum_4': (0, 3.355033),\\n 'tags_order_count_4': (0, 3.719298),\\n 'tags_order_sum_5': (0, 3.871832),\\n 'tags_order_count_5': (0, 4.218161),\\n 'whole_tags_order_sum': (0, 3.005077),\\n 'whole_tags_order_count': (0, 3.356816),\\n 'part_1_sum': (0, 2.838132),\\n 'part_1_count': (0, 3.105394),\\n 'part_2_sum': (0, 3.631354),\\n 'part_2_count': (0, 3.947554),\\n 'part_3_sum': (0, 2.574938),\\n 'part_3_count': (0, 2.8732),\\n 'part_4_sum': (0, 2.353698),\\n 'part_4_count': (0, 2.740394),\\n 'part_5_sum': (0, 4.350539),\\n 'part_5_count': (0, 4.86018),\\n 'part_6_sum': (0, 2.64874),\\n 'part_6_count': (0, 2.971092),\\n 'part_7_sum': (0, 1.864806),\\n 'part_7_count': (0, 2.166899),\\n 'part_cut_sum': (0, 4.154958),\\n 'part_cut_count': (0, 4.582286),\\n 'lec_rolling_count': (0, 2.10689),\\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\\n\\nargs.f_names = f_names\\nargs.log_f_names = log_f_names\\nargs.stdize_f_names = stdize_f_names\\nargs.stdize_params = stdize_params\\n\\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\\nargs_exp222 = deepcopy(args)\\ndel args;\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"#load data\\nmy_encoder_exp162 = MyEncoderExp162(args_exp162)\\nmy_encoder_exp162.load_state_dict(torch.load(prefix + '/models/my_encoder_exp162_best.pth'))\\nmy_encoder_exp162.to(args_exp162.device)\\nmy_encoder_exp162.eval();\\n\\nmy_encoder_exp166 = MyEncoderExp166(args_exp166)\\nmy_encoder_exp166.load_state_dict(torch.load(prefix + '/models/my_encoder_exp166_best.pth'))\\nmy_encoder_exp166.to(args_exp166.device)\\nmy_encoder_exp166.eval();\\n\\nmy_encoder_exp184 = MyEncoderExp184(args_exp184)\\nmy_encoder_exp184.load_state_dict(torch.load(prefix + '/models/my_encoder_exp184_best.pth'))\\nmy_encoder_exp184.to(args_exp184.device)\\nmy_encoder_exp184.eval();\\n\\nmy_encoder_exp218 = MyEncoderExp218(args_exp218)\\nmy_encoder_exp218.load_state_dict(torch.load(prefix + '/models/my_encoder_exp248_best.pth'))\\nmy_encoder_exp218.to(args_exp218.device)\\nmy_encoder_exp218.eval();\\n\\nmy_encoder_exp224 = MyEncoderExp224(args_exp224)\\nmy_encoder_exp224.load_state_dict(torch.load(prefix + '/models/my_encoder_exp233_best.pth'))\\nmy_encoder_exp224.to(args_exp224.device)\\nmy_encoder_exp224.eval();\\n\\nmy_encoder_exp222 = MyEncoderExp222(args_exp222)\\nmy_encoder_exp222.load_state_dict(torch.load(prefix + '/models/my_encoder_exp249_best.pth'))\\nmy_encoder_exp222.to(args_exp222.device)\\nmy_encoder_exp222.eval();\\n\\nque =  pd.read_csv(prefix + '/data/questions.csv').astype({'question_id': 'int16', 'bundle_id': 'int16', 'correct_answer': 'int8', 'part': 'int8'})\\nque_proc = pd.read_pickle(prefix + '/others/que_proc.pkl')\\nque_proc_2 = pd.read_pickle(prefix + '/others/que_proc_2.pkl')\\nque_onehot = pd.read_pickle(prefix + '/others/que_onehot.pkl')\\nque_part_onehot = pd.read_pickle(prefix + '/others/que_part_onehot.pkl')\\nlec = pd.read_csv(prefix + '/data/lectures.csv').astype({'lecture_id': 'int16', 'tag': 'int16', 'part': 'int8'})\\nlec_map = np.load(prefix + '/others/lec_map.npy')\\nlec_proc = pd.read_pickle(prefix + '/others/lec_proc.pkl')\\nque_proc_int = np.load(prefix + '/others/que_proc_int.npy')\\n\\nn_sample = 400\\nuser_map_ref = load_pickle(prefix + '/others/user_map_train.npy')\\nn_users_ref = np.load(prefix + '/others/n_users_train.npy')\\n#nn\\nr_ref_g1 = np.load(prefix + '/references/nn_content_id_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g2 = np.load(prefix + '/references/nn_normed_log_timestamp_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g3 = np.load(prefix + '/references/nn_correctness_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g4 = np.load(prefix + '/references/nn_question_had_explanation_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g5 = np.load(prefix + '/references/nn_normed_elapsed_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g6 = np.load(prefix + '/references/nn_normed_modified_timedelta_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g7 = np.load(prefix + '/references/nn_user_answer_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g8 = np.load(prefix + '/references/nn_task_container_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g9 = np.load(prefix + '/references/nn_content_type_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_g10 = np.load(prefix + '/references/nn_task_container_id_history' + '_length_' + str(n_sample) + '_train.npy')\\nr_ref_delta = np.load(prefix + '/references/timestamp_diff_train.npy')\\nr_ref_delta_2 = np.load(prefix + '/references/task_container_id_diff_train.npy')\\nr_ref_delta_3 = np.load(prefix + '/references/content_type_id_diff_train.npy')\\n#cpu\\nr_ref_1 = np.load(prefix + '/references/rolling_mean_sum_count_train.npy')\\nr_ref_2 = np.load(prefix + '/references/rolling_mean_sum_count_for_6_tags_and_whole_tag_train.npy')\\nr_ref_3 = np.load(prefix + '/references/rolling_mean_sum_count_for_part_train.npy')\\nr_ref_4 = np.load(prefix + '/references/lec_rolling_count_train.npy')\\nr_ref_5 = np.load(prefix + '/references/lec_part_rolling_count_train.npy')\\nr_ref_6 = np.load(prefix + '/references/lec_type_of_rolling_count_train.npy')\\nr_ref_7 = np.load(prefix + '/references/lec_tags_rolling_count_train.npy')\\nr_ref_8 = load_pickle(prefix + '/references/rolling_mean_sum_count_for_content_id_darkness_train.npy')\\nr_ref_12 = np.load(prefix + '/references/norm_rolling_count_and_cut_for_user_answer_train.npy')\\nr_ref_13 = np.load(prefix + '/references/rolling_mean_for_prior_question_elapsed_time_train.npy')\\nr_ref_14 = np.load(prefix + '/references/rolling_mean_for_prior_question_had_explanation_train.npy')\\nr_ref_15 = np.load(prefix + '/references/rolling_sum_for_prior_question_isnull_train.npy')\\nr_ref_16 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_elapsed_time_train.npy')\\nr_ref_17 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_elapsed_time_train.npy')\\nr_ref_18 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_had_explanation_train.npy')\\nr_ref_19 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_had_explanation_train.npy')\\nr_ref_20 = np.load(prefix + '/references/n_samples_rolling_mean_train.npy')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# run\\ni = 0\\nfor (tes, _) in tqdm(iter_test):\\n    user_map_ref, n_users_ref = update_user_map(tes, user_map_ref, n_users_ref)\\n    if i != 0:\\n        # update\\n        r_ref_1 = update_reference_get_rolling_mean_sum_count(r_ref_1, prev_tes, tes, user_map_ref)\\n        r_ref_2 = update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, prev_tes, tes, que_onehot, user_map_ref)\\n        r_ref_3 = update_reference_get_rolling_mean_sum_count_for_part(r_ref_3, prev_tes, tes, que_part_onehot, user_map_ref)\\n        r_ref_4 = update_reference_get_lec_rolling_count(r_ref_4, prev_tes, tes, user_map_ref)\\n        r_ref_5 = update_reference_get_lec_part_rolling_count(r_ref_5, prev_tes, tes, user_map_ref, lec, lec_map)\\n        r_ref_6 = update_reference_get_lec_type_of_rolling_count(r_ref_6, prev_tes, tes, user_map_ref, lec_proc, lec_map)\\n        r_ref_7 = update_reference_get_lec_tags_rolling_count(r_ref_7, prev_tes, tes, user_map_ref, lec, lec_map)\\n        r_ref_8 = update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, prev_tes, tes, user_map_ref)\\n        \\n        r_ref_12 = update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, prev_tes, tes, user_map_ref)\\n        r_ref_15 = update_reference_get_rolling_sum_for_prior_question_isnull(r_ref_15, prev_tes, tes, user_map_ref)\\n        r_ref_16 = update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, prev_tes, tes, user_map_ref)\\n        r_ref_17 = update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, prev_tes, tes, user_map_ref)\\n        r_ref_18 = update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, prev_tes, tes, user_map_ref)\\n        r_ref_19 = update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, prev_tes, tes, user_map_ref)\\n        r_ref_20 = update_reference_get_n_samples_rolling_mean(r_ref_20, prev_tes, tes, user_map_ref)\\n\\n        r_ref_delta = update_reference_get_timestamp_diff(r_ref_delta, prev_tes, tes, user_map_ref)\\n        r_ref_delta_2 = update_reference_get_task_container_id_diff(r_ref_delta_2, prev_tes, tes, user_map_ref)\\n        r_ref_delta_3 = update_reference_get_content_type_id_diff(r_ref_delta_3, prev_tes, tes, user_map_ref)\\n        r_ref_g1 = nn_update_reference_get_content_id_history(r_ref_g1, prev_tes, tes, user_map_ref)\\n        r_ref_g2 = nn_update_reference_get_normed_log_timestamp_history(r_ref_g2, prev_tes, tes, user_map_ref)\\n        r_ref_g3 = nn_update_reference_get_correctness_history(r_ref_g3, prev_tes, tes, user_map_ref)\\n        r_ref_g4 = nn_update_reference_get_question_had_explanation_history(r_ref_g4, prev_tes, tes, user_map_ref)\\n        r_ref_g5 = nn_update_reference_get_normed_elapsed_history(r_ref_g5, prev_tes, tes, user_map_ref)\\n        r_ref_g6 = nn_update_reference_get_normed_modified_timedelta_history(r_ref_g6, prev_tes, tes, f_tes_delta, user_map_ref)\\n        r_ref_g7 = nn_update_reference_get_user_answer_history(r_ref_g7, prev_tes, tes, user_map_ref)\\n        r_ref_g8 = nn_update_reference_get_task_container_id_diff_history(r_ref_g8, prev_tes, tes, f_tes_delta_2, user_map_ref)\\n        r_ref_g9 = nn_update_reference_get_content_type_id_diff_history(r_ref_g9, prev_tes, tes, f_tes_delta_3, user_map_ref)\\n        r_ref_g10 = nn_update_reference_get_task_container_id_history(r_ref_g10, prev_tes, tes, user_map_ref)\\n        \\n    # early update\\n    r_ref_g4 = nn_early_update_reference_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\\n    r_ref_g5 = nn_early_update_reference_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\\n    r_ref_13 = early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\\n    r_ref_14 = early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\\n    r_ref_16 = early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\\n    r_ref_17 = early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\\n    r_ref_18 = early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\\n    r_ref_19 = early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\\n    \\n    # online function\\n    f_tes_1 = online_get_rolling_mean_sum_count(r_ref_1, tes, user_map_ref)\\n    f_tes_2 = online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, tes, que_proc, user_map_ref)\\n    f_tes_3 = online_get_rolling_mean_sum_count_for_part(r_ref_3, tes, que, user_map_ref)\\n    f_tes_4 = online_get_lec_rolling_count(r_ref_4, tes, user_map_ref)\\n    f_tes_5 = online_get_lec_part_rolling_count(r_ref_5, tes, user_map_ref, que)\\n    f_tes_6 = online_get_lec_type_of_rolling_count(r_ref_6, tes, user_map_ref)\\n    f_tes_7 = online_get_lec_tags_rolling_count(r_ref_7, tes, user_map_ref, que_proc)\\n    f_tes_8 = online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, tes, user_map_ref)\\n    \\n    f_tes_12 = online_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, tes, que, user_map_ref)\\n    f_tes_13 = online_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\\n    f_tes_14 = online_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\\n    f_tes_15 = online_get_rolling_sum_for_prior_question_isnull(r_ref_15, tes, user_map_ref)\\n    f_tes_16 = online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\\n    f_tes_17 = online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\\n    f_tes_18 = online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\\n    f_tes_19 = online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\\n    f_tes_20 = online_get_n_samples_rolling_mean(r_ref_20, tes, user_map_ref)\\n    \\n    f_tes_delta = online_get_modified_timedelta(r_ref_delta, tes, user_map_ref)\\n    f_tes_delta_2 = online_get_task_container_id_diff(r_ref_delta_2, tes, user_map_ref)\\n    f_tes_delta_3 = online_get_content_type_id_diff(r_ref_delta_3, tes, user_map_ref)\\n    \\n    f_tes_g1 = nn_online_get_content_id_history(r_ref_g1, tes, user_map_ref)\\n    f_tes_g2 = nn_online_get_normed_log_timestamp_history(r_ref_g2, tes, user_map_ref)\\n    f_tes_g3 = nn_online_get_correctness_history(r_ref_g3, tes, user_map_ref)\\n    f_tes_g4 = nn_online_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\\n    f_tes_g5 = nn_online_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\\n    f_tes_g6 = nn_online_get_normed_modified_timedelta_history(r_ref_g6, tes, f_tes_delta, user_map_ref)\\n    f_tes_g7 = nn_online_get_user_answer_history(r_ref_g7, tes, user_map_ref)\\n    f_tes_g8 = nn_online_get_task_container_id_diff_history(r_ref_g8, tes, f_tes_delta_2, user_map_ref)\\n    f_tes_g9 = nn_online_get_content_type_id_diff_history(r_ref_g9, tes, f_tes_delta_3, user_map_ref)\\n    f_tes_g10 = nn_online_get_task_container_id_history(r_ref_g10, tes, user_map_ref)\\n    \\n\\n    # make a prediction \\n    concated = pd.concat([f_tes_1, f_tes_2, f_tes_3, f_tes_4, f_tes_5, f_tes_6, f_tes_7, \\n                         f_tes_8, f_tes_12, f_tes_13, f_tes_14, f_tes_15, f_tes_16, f_tes_17, f_tes_18, \\n                         f_tes_19, f_tes_20], axis = 1)\\n    X_tes = concated[args_exp166.f_names].values.astype(np.float32)\\n    \\n    inputs = {'content_id' : f_tes_g1.values, 'normed_timedelta' : f_tes_g6.values, 'normed_log_timestamp' : f_tes_g2.values,\\n         'explanation': f_tes_g4.values, 'correctness': f_tes_g3.values, 'normed_elapsed': f_tes_g5.values, 'user_answer' : f_tes_g7.values,\\n             'task_container_id_diff': f_tes_g8.values, 'content_type_id_diff': f_tes_g9.values, 'task_container_id': f_tes_g10.values, \\n              'features': X_tes}\\n    inputs_200 = {key: inputs[key][:, -201:] for key in inputs.keys()}\\n    \\n    \\n    score_exp162 = my_encoder_exp162.predict_on_batch(inputs_200, args_exp162, args_exp162.pred_bs, que, que_proc_int) \\\\\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\\n    score_exp166 = my_encoder_exp166.predict_on_batch(inputs, args_exp166, args_exp166.pred_bs, que, que_proc_int) \\\\\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\\n    score_exp184 = my_encoder_exp184.predict_on_batch(inputs, args_exp184, args_exp184.pred_bs, que, que_proc_int) \\\\\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\\n    score_exp218 = my_encoder_exp218.predict_on_batch(inputs, args_exp218, args_exp218.pred_bs, que, que_proc_int) \\\\\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\\n    score_exp224 = my_encoder_exp224.predict_on_batch(inputs, args_exp224, args_exp224.pred_bs, que, que_proc_int) \\\\\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\\n    score_exp222 = my_encoder_exp222.predict_on_batch(inputs, args_exp222, args_exp222.pred_bs, que, que_proc_int) \\\\\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\\n    \\n    preds = [score_exp162, score_exp166, score_exp184, score_exp218, score_exp222, score_exp224]\\n    weights = np.array([0.42, 0.25, 0.49, 0.85, 0.84, 0.82])\\n    weights[[3, 4, 5]] += 0.1\\n    weights = weights/weights.sum()\\n    score = 0\\n    for pred, weight in zip(preds, weights):\\n        score += pred * weight\\n    \\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\\n    spc_tes['answered_correctly'] = score.astype(np.float64)\\n    env.predict(spc_tes[['row_id', 'answered_correctly']])\\n    \\n    # save previous test\\n    prev_tes = tes.copy()\\n    i += 1\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat\":4,\"nbformat_minor\":4}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb b/dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb
--- a/dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/riiid-test-answer-prediction/mamas/public-private-2nd-place-solution.ipynb	(date 1658512097837)
@@ -1,1 +1,162 @@
-{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#libs\nfrom copy import deepcopy\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm\nimport types\nimport ast\nimport gc\ndef imports():\n    for name, val in globals().items():\n        # module imports\n        if isinstance(val, types.ModuleType):\n            yield name, val\n        # functions / callables\n        if hasattr(val, '__call__'):\n            yield name, val\nnp.seterr(divide='ignore', invalid='ignore')\nnoglobal = lambda fn: types.FunctionType(fn.__code__, dict(imports()))\nimport collections\nfrom scipy.sparse import lil_matrix\nimport scipy.sparse\n%load_ext Cython\nfrom itertools import chain\nfrom IPython.display import display, HTML\nimport lightgbm as lgb\nfrom pprint import pprint\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore', 'Mean of empty slice')\n\n#for GPU\nimport torch\nfrom torch import nn\nimport torch.utils.data as torchdata\nimport torch.nn.functional as F\nimport math\nfrom collections import OrderedDict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#utils\nclass RiiidEnv:\n    def __init__(self, sn, iterate_wo_predict = False):\n        self.sn = sn\n        self.sub = sn.loc[sn['content_type_id'] == 0][['row_id']].copy()\n        self.sub['answered_correctly'] = 0.5\n        self.can_yield = True\n        self.iterate_wo_predict = iterate_wo_predict\n        self.num_groups = self.sn.index.max() + 1\n        \n    def iter_test(self):\n        for i in range(self.num_groups):\n            self.i = i\n            assert(self.can_yield)\n            if not self.iterate_wo_predict:\n                self.can_yield = False\n\n            if i in self.sub.index:\n                yield self.sn.loc[[i]], self.sub.loc[[i]]\n            elif i not in self.sub.index:\n                yield self.sn.loc[[i]], None\n                    \n    def predict(self, my_sub):\n        assert(my_sub['row_id'].dtype == 'int64')\n        assert(my_sub['answered_correctly'].dtype == 'float64')\n        assert(my_sub.index.name == 'group_num')\n        assert(my_sub.index.dtype == 'int64')\n        \n        if self.i in self.sub.index:\n            assert(np.all(my_sub['answered_correctly'] >= 0))\n            assert(np.all(my_sub['answered_correctly'] <= 1))\n            assert(np.all(my_sub.index == self.i))\n            assert(np.all(my_sub['row_id'] == self.sub.loc[[self.i]]['row_id']))\n            self.sub.loc[[self.i]] = my_sub\n            self.can_yield = True\n            \n        elif self.i not in self.sub.index:\n            assert(my_sub.shape[0] == 0)\n            self.can_yield = True\n\n@noglobal\ndef save_pickle(obj, path):\n    with open(path, mode='wb') as f:\n        pickle.dump(obj, f)\n\n@noglobal\ndef load_pickle(path):\n    with open(path, mode='rb') as f:\n        obj = pickle.load(f)\n    return obj\n\n@noglobal\ndef encode(train, column_name):\n    encoded = pd.merge(train[[column_name]], pd.DataFrame(train[column_name].unique(), columns=[column_name])\\\n                       .reset_index().dropna(), how='left', on=column_name)[['index']].rename(\n        columns={'index': column_name})\n    return encoded\n\n@noglobal\ndef update_user_map(tes, user_map_ref, n_users_ref):\n    #new_users = tes[tes['timestamp'] == 0]['user_id'].unique()\n    users = tes['user_id'].unique()\n    keys = user_map_ref.keys()\n    new_users = users[np.array([user not in keys for user in users])]\n    n_new_users = new_users.shape[0]\n    if n_new_users > 0:\n        user_map_ref[new_users] = np.arange(n_users_ref, n_users_ref + n_new_users)\n    return user_map_ref, n_users_ref + n_new_users\n    \n@noglobal\ndef write_to_ref_map(path, ref_name, f_names):\n    ref_map = load_pickle(path)\n    ref_map[ref_name] = f_names\n    save_pickle(ref_map, path)\n    \nclass VectorizedDict():\n    def __init__(self):\n        self.tr_dict = dict()\n        self.set_value = np.vectorize(self.tr_dict.__setitem__)\n        self.get_value = np.vectorize(self.tr_dict.__getitem__)\n        \n    def keys(self):\n        return self.tr_dict.keys()\n        \n    def __setitem__(self, indices, values):\n        self.set_value(indices, values)\n    \n    def __getitem__(self, indices):\n        if indices.shape[0] == 0:\n            return np.array([], dtype=np.int32)\n        return self.get_value(indices)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%cython \nimport numpy as np\ncimport numpy as np\n\ncpdef np.ndarray[int] cget_memory_indices(np.ndarray task):\n    \n    cdef Py_ssize_t n = task.shape[1]\n    cdef np.ndarray[int, ndim = 2] res = np.zeros_like(task, dtype = np.int32)\n    cdef np.ndarray[int] tmp_counter = np.full(task.shape[0], -1, dtype = np.int32)\n    cdef np.ndarray[int] u_counter = np.full(task.shape[0], task.shape[1] - 1, dtype = np.int32)\n    \n    for i in range(n):\n        res[:, i] = u_counter\n        tmp_counter += 1\n        if i != n - 1:\n            mask = (task[:, i] != task[:, i + 1])\n            u_counter[mask] = tmp_counter[mask]\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#func_gpu\n@noglobal\ndef nn_online_get_content_id_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['content_id'].values\n    return pd.DataFrame(res, columns = ['nn_content_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_content_id_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    content_ids = spc_prev_tes_cp['content_id'].values\n    for user, content_id in zip(enc_users, content_ids):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = content_id\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_log_timestamp_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['log_timestamp'] = np.log1p(spc_tes['timestamp'].values.astype(np.float32))\n    std = 3.3530\n    mean = 20.863\n    spc_tes['normed_log_timestamp'] = (spc_tes['log_timestamp'].values - mean)/std\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['normed_log_timestamp'].values\n    return pd.DataFrame(res, columns = ['nn_normed_log_timestamp_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_normed_log_timestamp_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    timestamp = spc_prev_tes_cp['timestamp'].values.astype(np.float32)\n    std = 3.3530\n    mean = 20.863\n    normed_log_timestamps = (np.log1p(timestamp) - mean)/std\n    for user, normed_log_timestamp in zip(enc_users, normed_log_timestamps):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = normed_log_timestamp\n    return r_ref\n\n@noglobal\ndef nn_online_get_correctness_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 2\n    return pd.DataFrame(res, columns = ['nn_correctness_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_correctness_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal\ndef nn_online_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 2\n    return pd.DataFrame(res, columns = ['nn_question_had_explanation_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_early_update_reference_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n                    & (~tes['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    explanations = spc_tes_cp['prior_question_had_explanation'].values.astype('int')\n    for explanation, idx in zip(explanations, row_idx):\n        r_ref[idx, r_ref[idx] == 2] = explanation\n    return r_ref\n\n@noglobal\ndef nn_update_reference_get_question_had_explanation_history(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n    for user in row_idx:\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = 2\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = -999\n    return pd.DataFrame(res, columns = ['nn_normed_elapsed_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_early_update_reference_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n                    & (~tes['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    elapsed = spc_tes_cp['prior_question_elapsed_time'].values\n    clipped = np.clip(elapsed, 0, 1000 * 300).astype(np.float32)\n    mean = 25953\n    std = 20418\n    normed_elapsed = (clipped - mean)/std\n    for el, idx in zip(normed_elapsed, row_idx):\n        r_ref[idx, r_ref[idx] == -999] = el\n    return r_ref\n\n@noglobal\ndef nn_update_reference_get_normed_elapsed_history(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n    for user in row_idx:\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = -999\n    return r_ref\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_modified_timedelta_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n\n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n    mean = 126568\n    std = 218000\n    clipped = (clipped - mean)/std\n    clipped[np.isnan(clipped)] = 0\n    spc_tes['normed_modified_timedelta'] = clipped.astype(np.float32)\n\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['normed_modified_timedelta'].values\n    return pd.DataFrame(res, columns = ['nn_normed_modified_timedelta_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_normed_modified_timedelta_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n    mean = 126568\n    std = 218000\n    clipped = (clipped - mean)/std\n    clipped[np.isnan(clipped)] = 0\n    spc_prev_tes_cp['normed_modified_timedelta'] = clipped.astype(np.float32)\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    normed_modified_timedeltas = spc_prev_tes_cp['normed_modified_timedelta'].values\n    for user, normed_modified_timedelta in zip(enc_users, normed_modified_timedeltas):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = normed_modified_timedelta\n    return r_ref\n\n@noglobal\ndef nn_online_get_user_answer_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 4\n    return pd.DataFrame(res, columns = ['nn_user_answer_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_user_answer_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['user_answer'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal\ndef online_get_task_container_id_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['task_container_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['task_container_id_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_task_container_id_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['task_container_id'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_task_container_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    value = f_tes_delta['task_container_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_tes['task_container_id_diff'] = value\n    \n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['task_container_id_diff'].values\n    return pd.DataFrame(res, columns = ['nn_task_container_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_task_container_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    value = f_tes_delta['task_container_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_prev_tes_cp['task_container_id_diff'] = value\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    task_container_id_diffs = spc_prev_tes_cp['task_container_id_diff'].values\n    for user, task_container_id_diff in zip(enc_users, task_container_id_diffs):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = task_container_id_diff\n    return r_ref\n\n@noglobal\ndef online_get_content_type_id_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['content_type_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['content_type_id_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_content_type_id_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['content_type_id'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_content_type_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    value = f_tes_delta['content_type_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_tes['content_type_id_diff'] = value\n    \n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['content_type_id_diff'].values\n    return pd.DataFrame(res, columns = ['nn_content_type_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_content_type_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    value = f_tes_delta['content_type_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_prev_tes_cp['content_type_id_diff'] = value\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    content_type_id_diffs = spc_prev_tes_cp['content_type_id_diff'].values\n    for user, content_type_id_diff in zip(enc_users, content_type_id_diffs):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = content_type_id_diff\n    return r_ref\n\n@noglobal\ndef nn_online_get_task_container_id_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['task_container_id'].values\n    return pd.DataFrame(res, columns = ['nn_task_container_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_task_container_id_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    task_container_ids = spc_prev_tes_cp['task_container_id'].values\n    for user, task_container_id in zip(enc_users, task_container_ids):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = task_container_id\n    return r_ref\n\nclass Embedder(nn.Module):\n    def __init__(self, n_proj, n_dims):\n        super(Embedder, self).__init__()\n        self.n_proj = n_proj\n        self.n_dims = n_dims\n        self.embed = nn.Embedding(n_proj, n_dims)\n\n    def forward(self, indices):\n        z = self.embed(indices)\n        return z\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass MyEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(MyEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        if activation == 'relu':\n            self.activation = F.relu\n        elif activation == 'gelu':\n            self.activation = F.gelu\n\n    def forward(self, q, k, v, src_mask, src_key_padding_mask):\n        src2 = self.self_attn(q, k, v, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[0]\n        src = q + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n\nclass MyEncoderExp162(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp162, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n        \n        #concat \n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp166(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp166, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n        \n        #concat \n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp184(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp184, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 16 * 8, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n#         self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n#         self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n#         self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n        \n        #rnn\n        self.rnn = nn.GRU(13, 16, 8, batch_first = True)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        \n        #rnn\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n        _, out_rnn = self.rnn(for_rnn)\n        out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n\n        #cat\n        out = torch.cat([out, out_rnn, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 16 * 8)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2)\n            \n            #rnn\n            for_rnn = for_conv\n            _, out_rnn = self.rnn(for_rnn)\n            out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_rnn, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp218(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp218, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp224(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp224, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256 + 16 * 8, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        self.fixlen_rnn = nn.GRU(13, 16, 8, batch_first = True)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        \n        #rnn\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n        _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n        out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n\n        #cat\n        out = torch.cat([out, out_fixlen_rnn, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256 + 16 * 8)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2)\n            \n            #rnn\n            for_rnn = for_conv\n            _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n            out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_fixlen_rnn, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp219(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp219, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp221(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp221, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp222(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp222, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n\n#func_cpu\n@noglobal\ndef online_get_user_id_content_id_task_container_id(tes):\n    f_tes = tes[tes['content_type_id'] == 0][['user_id', \n                                           'content_id', 'task_container_id']].astype('float32').reset_index(drop = True)\n    return f_tes\n\n@noglobal\ndef online_get_timestamp_and_prior_question_elapsed_time_and_prior_question_had_explanation(tes):\n    f_tes = tes[tes['content_type_id'] == 0][['timestamp', \n                                              'prior_question_elapsed_time', 'prior_question_had_explanation']].astype('float32').reset_index(drop = True)\n    return f_tes\n\n@noglobal\ndef online_get_part(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['part'].values[spc_tes_cp['content_id'].values], columns = ['part']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_correct_answer(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef online_get_tags(tes, que_proc_2):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    return pd.DataFrame(que_proc_2.values[spc_tes_cp['content_id'].values], columns = ['tags_' + str(i) for i in range(6)])\n\n@noglobal\ndef online_get_hell_rolling_mean_for_tags(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[:, :-1][user_map_ref[spc_tes['user_id'].values]]\n    f_tes = pd.DataFrame(sum_count[:, :, 0]/sum_count[:, :, 1], columns = ['hell_rolling_mean_for_tag_' + str(i) for i in range(188)])\n    return f_tes\n\n@noglobal\ndef online_get_rolling_mean_sum_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    mean = sum_count[:, 0]/sum_count[:, 1]\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n    res[:, 0] = mean\n    res[:, 1:] = sum_count\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n    res[:, 0] = mean\n    res[:, 1:] = sum_count\n    return pd.DataFrame(res, columns = ['target_full_mean', 'target_full_sum', 'target_count'])\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    cts = spc_prev_tes_cp['user_id'].value_counts()\n    pos_cts = spc_prev_tes_cp['user_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n    r_ref[user_map_ref[pos_cts.index], 0] += pos_cts.values\n    r_ref[user_map_ref[cts.index], 1] += cts.values\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, tes, que_proc, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    col_idx = que_proc.values[spc_tes['content_id'].values]\n    users = user_map_ref[spc_tes['user_id'].values]\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\n\n    res = np.zeros((spc_tes.shape[0], 7, 3), dtype = np.float32)\n    sliced = r_ref[row_idx, col_idx]\n    res[:, :6, 1:] = sliced\n    res[:, 6, 1:] = np.nansum(sliced, axis = 1)\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n    res = res.reshape(-1, 3 * 7)\n    f_names = sum([['tags_order_mean_' + str(i), 'tags_order_sum_' + str(i), 'tags_order_count_' + str(i)] for i in range(6)], [])\n    f_names += ['whole_tags_order_mean', 'whole_tags_order_sum', 'whole_tags_order_count']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, prev_tes, tes, que_onehot, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(188)]).T\n    r_ref[user_map_ref[uniq], :-1, 0] += sum_cts\n\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id']]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(188)]).T\n    r_ref[user_map_ref[uniq], :-1, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_sum_count_for_part(r_ref, tes, que, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['part'] = que['part'].values[spc_tes['content_id'].values]\n    res = np.zeros((spc_tes.shape[0], 8, 3), dtype = np.float32)\n    res[:, :7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res[:, 7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values], spc_tes['part'] - 1]\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n    res = res.reshape(-1, 3 * 8)\n    f_names = sum([['part_' + str(i) + '_mean', 'part_' + str(i) + '_sum', 'part_' + str(i) + '_count'] for i in range(1, 8)], []) + \\\n    ['part_cut_mean', 'part_cut_sum', 'part_cut_count']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_part(r_ref, prev_tes, tes, que_part_onehot, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    for_sum = que_part_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(7)]).T\n    r_ref[user_map_ref[uniq], :, 0] += sum_cts\n    \n    for_count = que_part_onehot.values[spc_prev_tes_cp['content_id']]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(7)]).T\n    r_ref[user_map_ref[uniq], :, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_lec_rolling_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    return pd.DataFrame(r_ref[user_map_ref[spc_tes['user_id'].values]], columns = ['lec_rolling_count'])\n\n@noglobal\ndef update_reference_get_lec_rolling_count(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    r_ref[user_map_ref[spc_prev_tes['user_id'].values]] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_part_rolling_count(r_ref, tes, user_map_ref, que):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 8), dtype = np.float32)\n    part_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res[:, :7] = part_count\n    res[:, 7] = part_count[np.arange(spc_tes.shape[0]), que['part'].values[spc_tes['content_id'].values] - 1]\n    return pd.DataFrame(res, columns = ['lec_part_' + str(i + 1) for i in range(7)] + ['lec_part_cut'])\n\n@noglobal\ndef update_reference_get_lec_part_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\n    col_idx = lec['part'].values[lec_map[spc_prev_tes['content_id'].values]] - 1\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_type_of_rolling_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['lec_type_of_' + str(i) for i in range(4)]).astype(np.float32)\n\n@noglobal\ndef update_reference_get_lec_type_of_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec_proc, lec_map):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\n    col_idx = lec_proc['type_of'].values[lec_map[spc_prev_tes['content_id'].values]]\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_tags_rolling_count(r_ref, tes, user_map_ref, que_proc):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n    users = user_map_ref[spc_tes['user_id'].values]\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\n    col_idx = que_proc.values[spc_tes['content_id'].values]\n    \n    sliced = r_ref[row_idx, col_idx]\n    res[:, :6] = sliced\n    res[:, 6] = np.nansum(sliced, axis = 1)\n    return pd.DataFrame(res, columns = sum([['lec_tags_order_count_' + str(i)] for i in range(6)], []) + ['lec_whole_tags_order_count'])\n\n@noglobal\ndef update_reference_get_lec_tags_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n    prev_tes_cp = prev_tes.copy()\n    #prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 1].copy().reset_index(drop = True)\n    col_idx = lec['tag'].values[lec_map[spc_prev_tes_cp['content_id'].values]]\n    row_idx = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal \ndef online_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_tes['user_id'].values]\n    col_idx = spc_tes['content_id'].values\n    f_sum = r_ref_sum[row_idx, col_idx].toarray()[0]\n    f_count = r_ref_count[row_idx, col_idx].toarray()[0]\n    f_mean = f_sum/f_count\n    f_tr = pd.DataFrame()\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n    return f_tr\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    contents = spc_prev_tes_cp['content_id'].values\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, content, target in zip(users, contents, targets):\n        r_ref_sum[user, content] = r_ref_sum[user, content] + target\n        r_ref_count[user, content] = r_ref_count[user, content] + 1\n    return r_ref_sum, r_ref_count\n\n@noglobal\ndef online_get_timestamp_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['timestamp_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n    return r_ref\n\n@noglobal\ndef online_get_whole_oof_target_encoding_content_id(r_ref, tes):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[spc_tes['content_id'].values]\n    res = (sum_count[:, 0]/sum_count[:, 1]).astype(np.float32)\n    return pd.DataFrame(res, columns = ['whole_oof_target_encoding_content_id'])\n\n@noglobal\ndef update_reference_get_whole_oof_target_encoding_content_id(r_ref, prev_tes, tes):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    cts = spc_prev_tes_cp['content_id'].value_counts()\n    pos_cts = spc_prev_tes_cp['content_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n    r_ref[pos_cts.index.values, 0] += pos_cts.values\n    r_ref[cts.index.values, 1] += cts.values\n    return r_ref\n\n@noglobal\ndef online_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, tes, que_proc):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[que_proc.values[spc_tes['content_id'].values]]\n    mean = sum_count[:, :, 0]/sum_count[:, :, 1]\n    nansum = np.nansum(sum_count, axis = 1)\n    whole_mean = nansum[:, 0]/nansum[:, 1]\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n    res[:, :6] = mean\n    res[:, 6] = whole_mean\n    f_names = ['whole_oof_target_encoding_tags_order_' + str(i) for i in range(6)] + ['whole_oof_target_encoding_whole_tags']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, prev_tes, tes, que_onehot):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    sum_cts = for_sum.sum(axis = 0)\n    r_ref[:-1, 0] += sum_cts\n\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id'].values]\n    cts = for_count.sum(axis = 0)\n    r_ref[:-1, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_correct_answer(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n\n    for_count = np.zeros((spc_prev_tes_cp.shape[0], 4))\n    for_count[np.arange(for_count.shape[0]), spc_prev_tes_cp['user_answer'].values] = 1\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(4)]).T\n    r_ref[user_map_ref[uniq]] += cts\n    return r_ref\n\n@noglobal\ndef online_get_norm_rolling_count_and_cut_for_user_answer(r_ref, tes, que, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 5), dtype = np.float32)\n    cts = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    norm_cts = cts/cts.sum(axis = 1)[:, None]\n    res[:, :4] = norm_cts\n    correct_answer = que['correct_answer'].values[spc_tes['content_id'].values]\n    res[:, 4] = norm_cts[np.arange(norm_cts.shape[0]), correct_answer]\n    f_names = ['norm_rolling_count_user_answer_' + str(i) for i in range(4)] + ['cut_norm_rolling_count_user_answer']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef online_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values.astype('int') * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n    return r_ref\n\n@noglobal\ndef online_get_rolling_sum_for_prior_question_isnull(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['rolling_sum_for_prior_question_isnull']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_rolling_sum_for_prior_question_isnull(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes[(prev_tes['content_type_id'] == 0) & (prev_tes['prior_question_elapsed_time'].isnull())].copy()\n    r_ref[user_map_ref[prev_tes_cp['user_id'].values]] += prev_tes_cp['prior_question_elapsed_time'].isnull().astype('int').values\n    return r_ref\n\n@noglobal\ndef online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_diff_modified_timedelta_and_prior_elapsed_time_rolling_mean(f_tes_base, f_tes_pn, f_tes_p, f_tes_n):\n    f_names = ['rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n                                   'positive_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n                                   'negative_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta']\n    f_tes = pd.DataFrame()\n    f_tes[f_names[0]] = f_tes_base.values[:, 0] - f_tes_pn.values[:, 0]\n    f_tes[f_names[1]] = f_tes_base.values[:, 0] - f_tes_p.values[:, 0]\n    f_tes[f_names[2]] = f_tes_base.values[:, 0] - f_tes_n.values[:, 0]\n    return f_tes\n\n@noglobal\ndef online_get_n_samples_rolling_mean(r_ref, tes, user_map_ref):\n    n_samples = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 200])\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    history = r_ref[user_map_ref[users]].astype(np.float32)\n    history[history == -1] = np.nan\n    res = np.zeros((spc_tes.shape[0], len(n_samples)), dtype = np.float32)\n    for i, n_sample in enumerate(n_samples):\n        res[:, i] = np.nanmean(history[:, -n_sample:], axis = 1)\n    return pd.DataFrame(res, columns = [str(i) + '_samples_rolling_mean' for i in n_samples])\n\n@noglobal\ndef update_reference_get_n_samples_rolling_mean(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal \ndef online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_tes['user_id'].values]\n    col_idx = spc_tes['content_id'].values\n    f_base = r_ref[row_idx, col_idx].toarray()[0]\n    f_sum = f_base % 128\n    f_count = f_base // 128\n    f_mean = f_sum/f_count\n    f_tr = pd.DataFrame()\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n    return f_tr\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    contents = spc_prev_tes_cp['content_id'].values\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, content, target in zip(users, contents, targets):\n        r_ref[user, content] = r_ref[user, content] + target + 128\n    return r_ref","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialization\n#comment out\n#prefix = '..'\nprefix = '/kaggle/input/mamastan-gpu-v27'\n#sn = pd.read_pickle('../others/ex_tes.pkl'); env = RiiidEnv(sn, iterate_wo_predict = False)\nimport riiideducation; env = riiideducation.make_env()\niter_test = env.iter_test()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params162\nargs = OrderedDict()\nargs.n_length = 200\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.pred_bs = 256\nargs.device = torch.device('cuda')\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp162 = deepcopy(args)\ndel args;\n\n#params166\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.pred_bs = 256\nargs.device = torch.device('cuda')\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp166 = deepcopy(args)\ndel args;\n\n#params184\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\n\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp184 = deepcopy(args)\ndel args;\n\n#params218\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\n\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp218 = deepcopy(args)\ndel args;\n\n#params224\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp224 = deepcopy(args)\ndel args;\n\n#params219\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.lr = 0.2 * 1e-3\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp219 = deepcopy(args)\ndel args;\n\n#params221\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp221 = deepcopy(args)\ndel args;\n\n#params222\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp222 = deepcopy(args)\ndel args;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data\nmy_encoder_exp162 = MyEncoderExp162(args_exp162)\nmy_encoder_exp162.load_state_dict(torch.load(prefix + '/models/my_encoder_exp162_best.pth'))\nmy_encoder_exp162.to(args_exp162.device)\nmy_encoder_exp162.eval();\n\nmy_encoder_exp166 = MyEncoderExp166(args_exp166)\nmy_encoder_exp166.load_state_dict(torch.load(prefix + '/models/my_encoder_exp166_best.pth'))\nmy_encoder_exp166.to(args_exp166.device)\nmy_encoder_exp166.eval();\n\nmy_encoder_exp184 = MyEncoderExp184(args_exp184)\nmy_encoder_exp184.load_state_dict(torch.load(prefix + '/models/my_encoder_exp184_best.pth'))\nmy_encoder_exp184.to(args_exp184.device)\nmy_encoder_exp184.eval();\n\nmy_encoder_exp218 = MyEncoderExp218(args_exp218)\nmy_encoder_exp218.load_state_dict(torch.load(prefix + '/models/my_encoder_exp248_best.pth'))\nmy_encoder_exp218.to(args_exp218.device)\nmy_encoder_exp218.eval();\n\nmy_encoder_exp224 = MyEncoderExp224(args_exp224)\nmy_encoder_exp224.load_state_dict(torch.load(prefix + '/models/my_encoder_exp233_best.pth'))\nmy_encoder_exp224.to(args_exp224.device)\nmy_encoder_exp224.eval();\n\nmy_encoder_exp222 = MyEncoderExp222(args_exp222)\nmy_encoder_exp222.load_state_dict(torch.load(prefix + '/models/my_encoder_exp249_best.pth'))\nmy_encoder_exp222.to(args_exp222.device)\nmy_encoder_exp222.eval();\n\nque =  pd.read_csv(prefix + '/data/questions.csv').astype({'question_id': 'int16', 'bundle_id': 'int16', 'correct_answer': 'int8', 'part': 'int8'})\nque_proc = pd.read_pickle(prefix + '/others/que_proc.pkl')\nque_proc_2 = pd.read_pickle(prefix + '/others/que_proc_2.pkl')\nque_onehot = pd.read_pickle(prefix + '/others/que_onehot.pkl')\nque_part_onehot = pd.read_pickle(prefix + '/others/que_part_onehot.pkl')\nlec = pd.read_csv(prefix + '/data/lectures.csv').astype({'lecture_id': 'int16', 'tag': 'int16', 'part': 'int8'})\nlec_map = np.load(prefix + '/others/lec_map.npy')\nlec_proc = pd.read_pickle(prefix + '/others/lec_proc.pkl')\nque_proc_int = np.load(prefix + '/others/que_proc_int.npy')\n\nn_sample = 400\nuser_map_ref = load_pickle(prefix + '/others/user_map_train.npy')\nn_users_ref = np.load(prefix + '/others/n_users_train.npy')\n#nn\nr_ref_g1 = np.load(prefix + '/references/nn_content_id_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g2 = np.load(prefix + '/references/nn_normed_log_timestamp_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g3 = np.load(prefix + '/references/nn_correctness_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g4 = np.load(prefix + '/references/nn_question_had_explanation_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g5 = np.load(prefix + '/references/nn_normed_elapsed_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g6 = np.load(prefix + '/references/nn_normed_modified_timedelta_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g7 = np.load(prefix + '/references/nn_user_answer_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g8 = np.load(prefix + '/references/nn_task_container_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g9 = np.load(prefix + '/references/nn_content_type_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g10 = np.load(prefix + '/references/nn_task_container_id_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_delta = np.load(prefix + '/references/timestamp_diff_train.npy')\nr_ref_delta_2 = np.load(prefix + '/references/task_container_id_diff_train.npy')\nr_ref_delta_3 = np.load(prefix + '/references/content_type_id_diff_train.npy')\n#cpu\nr_ref_1 = np.load(prefix + '/references/rolling_mean_sum_count_train.npy')\nr_ref_2 = np.load(prefix + '/references/rolling_mean_sum_count_for_6_tags_and_whole_tag_train.npy')\nr_ref_3 = np.load(prefix + '/references/rolling_mean_sum_count_for_part_train.npy')\nr_ref_4 = np.load(prefix + '/references/lec_rolling_count_train.npy')\nr_ref_5 = np.load(prefix + '/references/lec_part_rolling_count_train.npy')\nr_ref_6 = np.load(prefix + '/references/lec_type_of_rolling_count_train.npy')\nr_ref_7 = np.load(prefix + '/references/lec_tags_rolling_count_train.npy')\nr_ref_8 = load_pickle(prefix + '/references/rolling_mean_sum_count_for_content_id_darkness_train.npy')\nr_ref_12 = np.load(prefix + '/references/norm_rolling_count_and_cut_for_user_answer_train.npy')\nr_ref_13 = np.load(prefix + '/references/rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_14 = np.load(prefix + '/references/rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_15 = np.load(prefix + '/references/rolling_sum_for_prior_question_isnull_train.npy')\nr_ref_16 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_17 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_18 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_19 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_20 = np.load(prefix + '/references/n_samples_rolling_mean_train.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run\ni = 0\nfor (tes, _) in tqdm(iter_test):\n    user_map_ref, n_users_ref = update_user_map(tes, user_map_ref, n_users_ref)\n    if i != 0:\n        # update\n        r_ref_1 = update_reference_get_rolling_mean_sum_count(r_ref_1, prev_tes, tes, user_map_ref)\n        r_ref_2 = update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, prev_tes, tes, que_onehot, user_map_ref)\n        r_ref_3 = update_reference_get_rolling_mean_sum_count_for_part(r_ref_3, prev_tes, tes, que_part_onehot, user_map_ref)\n        r_ref_4 = update_reference_get_lec_rolling_count(r_ref_4, prev_tes, tes, user_map_ref)\n        r_ref_5 = update_reference_get_lec_part_rolling_count(r_ref_5, prev_tes, tes, user_map_ref, lec, lec_map)\n        r_ref_6 = update_reference_get_lec_type_of_rolling_count(r_ref_6, prev_tes, tes, user_map_ref, lec_proc, lec_map)\n        r_ref_7 = update_reference_get_lec_tags_rolling_count(r_ref_7, prev_tes, tes, user_map_ref, lec, lec_map)\n        r_ref_8 = update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, prev_tes, tes, user_map_ref)\n        \n        r_ref_12 = update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, prev_tes, tes, user_map_ref)\n        r_ref_15 = update_reference_get_rolling_sum_for_prior_question_isnull(r_ref_15, prev_tes, tes, user_map_ref)\n        r_ref_16 = update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, prev_tes, tes, user_map_ref)\n        r_ref_17 = update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, prev_tes, tes, user_map_ref)\n        r_ref_18 = update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, prev_tes, tes, user_map_ref)\n        r_ref_19 = update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, prev_tes, tes, user_map_ref)\n        r_ref_20 = update_reference_get_n_samples_rolling_mean(r_ref_20, prev_tes, tes, user_map_ref)\n\n        r_ref_delta = update_reference_get_timestamp_diff(r_ref_delta, prev_tes, tes, user_map_ref)\n        r_ref_delta_2 = update_reference_get_task_container_id_diff(r_ref_delta_2, prev_tes, tes, user_map_ref)\n        r_ref_delta_3 = update_reference_get_content_type_id_diff(r_ref_delta_3, prev_tes, tes, user_map_ref)\n        r_ref_g1 = nn_update_reference_get_content_id_history(r_ref_g1, prev_tes, tes, user_map_ref)\n        r_ref_g2 = nn_update_reference_get_normed_log_timestamp_history(r_ref_g2, prev_tes, tes, user_map_ref)\n        r_ref_g3 = nn_update_reference_get_correctness_history(r_ref_g3, prev_tes, tes, user_map_ref)\n        r_ref_g4 = nn_update_reference_get_question_had_explanation_history(r_ref_g4, prev_tes, tes, user_map_ref)\n        r_ref_g5 = nn_update_reference_get_normed_elapsed_history(r_ref_g5, prev_tes, tes, user_map_ref)\n        r_ref_g6 = nn_update_reference_get_normed_modified_timedelta_history(r_ref_g6, prev_tes, tes, f_tes_delta, user_map_ref)\n        r_ref_g7 = nn_update_reference_get_user_answer_history(r_ref_g7, prev_tes, tes, user_map_ref)\n        r_ref_g8 = nn_update_reference_get_task_container_id_diff_history(r_ref_g8, prev_tes, tes, f_tes_delta_2, user_map_ref)\n        r_ref_g9 = nn_update_reference_get_content_type_id_diff_history(r_ref_g9, prev_tes, tes, f_tes_delta_3, user_map_ref)\n        r_ref_g10 = nn_update_reference_get_task_container_id_history(r_ref_g10, prev_tes, tes, user_map_ref)\n        \n    # early update\n    r_ref_g4 = nn_early_update_reference_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n    r_ref_g5 = nn_early_update_reference_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n    r_ref_13 = early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n    r_ref_14 = early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n    r_ref_16 = early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n    r_ref_17 = early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n    r_ref_18 = early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n    r_ref_19 = early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n    \n    # online function\n    f_tes_1 = online_get_rolling_mean_sum_count(r_ref_1, tes, user_map_ref)\n    f_tes_2 = online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, tes, que_proc, user_map_ref)\n    f_tes_3 = online_get_rolling_mean_sum_count_for_part(r_ref_3, tes, que, user_map_ref)\n    f_tes_4 = online_get_lec_rolling_count(r_ref_4, tes, user_map_ref)\n    f_tes_5 = online_get_lec_part_rolling_count(r_ref_5, tes, user_map_ref, que)\n    f_tes_6 = online_get_lec_type_of_rolling_count(r_ref_6, tes, user_map_ref)\n    f_tes_7 = online_get_lec_tags_rolling_count(r_ref_7, tes, user_map_ref, que_proc)\n    f_tes_8 = online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, tes, user_map_ref)\n    \n    f_tes_12 = online_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, tes, que, user_map_ref)\n    f_tes_13 = online_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n    f_tes_14 = online_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n    f_tes_15 = online_get_rolling_sum_for_prior_question_isnull(r_ref_15, tes, user_map_ref)\n    f_tes_16 = online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n    f_tes_17 = online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n    f_tes_18 = online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n    f_tes_19 = online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n    f_tes_20 = online_get_n_samples_rolling_mean(r_ref_20, tes, user_map_ref)\n    \n    f_tes_delta = online_get_modified_timedelta(r_ref_delta, tes, user_map_ref)\n    f_tes_delta_2 = online_get_task_container_id_diff(r_ref_delta_2, tes, user_map_ref)\n    f_tes_delta_3 = online_get_content_type_id_diff(r_ref_delta_3, tes, user_map_ref)\n    \n    f_tes_g1 = nn_online_get_content_id_history(r_ref_g1, tes, user_map_ref)\n    f_tes_g2 = nn_online_get_normed_log_timestamp_history(r_ref_g2, tes, user_map_ref)\n    f_tes_g3 = nn_online_get_correctness_history(r_ref_g3, tes, user_map_ref)\n    f_tes_g4 = nn_online_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n    f_tes_g5 = nn_online_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n    f_tes_g6 = nn_online_get_normed_modified_timedelta_history(r_ref_g6, tes, f_tes_delta, user_map_ref)\n    f_tes_g7 = nn_online_get_user_answer_history(r_ref_g7, tes, user_map_ref)\n    f_tes_g8 = nn_online_get_task_container_id_diff_history(r_ref_g8, tes, f_tes_delta_2, user_map_ref)\n    f_tes_g9 = nn_online_get_content_type_id_diff_history(r_ref_g9, tes, f_tes_delta_3, user_map_ref)\n    f_tes_g10 = nn_online_get_task_container_id_history(r_ref_g10, tes, user_map_ref)\n    \n\n    # make a prediction \n    concated = pd.concat([f_tes_1, f_tes_2, f_tes_3, f_tes_4, f_tes_5, f_tes_6, f_tes_7, \n                         f_tes_8, f_tes_12, f_tes_13, f_tes_14, f_tes_15, f_tes_16, f_tes_17, f_tes_18, \n                         f_tes_19, f_tes_20], axis = 1)\n    X_tes = concated[args_exp166.f_names].values.astype(np.float32)\n    \n    inputs = {'content_id' : f_tes_g1.values, 'normed_timedelta' : f_tes_g6.values, 'normed_log_timestamp' : f_tes_g2.values,\n         'explanation': f_tes_g4.values, 'correctness': f_tes_g3.values, 'normed_elapsed': f_tes_g5.values, 'user_answer' : f_tes_g7.values,\n             'task_container_id_diff': f_tes_g8.values, 'content_type_id_diff': f_tes_g9.values, 'task_container_id': f_tes_g10.values, \n              'features': X_tes}\n    inputs_200 = {key: inputs[key][:, -201:] for key in inputs.keys()}\n    \n    \n    score_exp162 = my_encoder_exp162.predict_on_batch(inputs_200, args_exp162, args_exp162.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp166 = my_encoder_exp166.predict_on_batch(inputs, args_exp166, args_exp166.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp184 = my_encoder_exp184.predict_on_batch(inputs, args_exp184, args_exp184.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp218 = my_encoder_exp218.predict_on_batch(inputs, args_exp218, args_exp218.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp224 = my_encoder_exp224.predict_on_batch(inputs, args_exp224, args_exp224.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp222 = my_encoder_exp222.predict_on_batch(inputs, args_exp222, args_exp222.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    \n    preds = [score_exp162, score_exp166, score_exp184, score_exp218, score_exp222, score_exp224]\n    weights = np.array([0.42, 0.25, 0.49, 0.85, 0.84, 0.82])\n    weights[[3, 4, 5]] += 0.1\n    weights = weights/weights.sum()\n    score = 0\n    for pred, weight in zip(preds, weights):\n        score += pred * weight\n    \n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['answered_correctly'] = score.astype(np.float64)\n    env.predict(spc_tes[['row_id', 'answered_correctly']])\n    \n    # save previous test\n    prev_tes = tes.copy()\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "metadata": {
+    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
+    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "#libs\nfrom copy import deepcopy\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm.notebook import tqdm\nimport types\nimport ast\nimport gc\ndef imports():\n    for name, val in globals().items():\n        # module imports\n        if isinstance(val, types.ModuleType):\n            yield name, val\n        # functions / callables\n        if hasattr(val, '__call__'):\n            yield name, val\nnp.seterr(divide='ignore', invalid='ignore')\nnoglobal = lambda fn: types.FunctionType(fn.__code__, dict(imports()))\nimport collections\nfrom scipy.sparse import lil_matrix\nimport scipy.sparse\n%load_ext Cython\nfrom itertools import chain\nfrom IPython.display import display, HTML\nimport lightgbm as lgb\nfrom pprint import pprint\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nwarnings.filterwarnings('ignore', 'Mean of empty slice')\n\n#for GPU\nimport torch\nfrom torch import nn\nimport torch.utils.data as torchdata\nimport torch.nn.functional as F\nimport math\nfrom collections import OrderedDict",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "#utils\nclass RiiidEnv:\n    def __init__(self, sn, iterate_wo_predict = False):\n        self.sn = sn\n        self.sub = sn.loc[sn['content_type_id'] == 0][['row_id']].copy()\n        self.sub['answered_correctly'] = 0.5\n        self.can_yield = True\n        self.iterate_wo_predict = iterate_wo_predict\n        self.num_groups = self.sn.index.max() + 1\n        \n    def iter_test(self):\n        for i in range(self.num_groups):\n            self.i = i\n            assert(self.can_yield)\n            if not self.iterate_wo_predict:\n                self.can_yield = False\n\n            if i in self.sub.index:\n                yield self.sn.loc[[i]], self.sub.loc[[i]]\n            elif i not in self.sub.index:\n                yield self.sn.loc[[i]], None\n                    \n    def predict(self, my_sub):\n        assert(my_sub['row_id'].dtype == 'int64')\n        assert(my_sub['answered_correctly'].dtype == 'float64')\n        assert(my_sub.index.name == 'group_num')\n        assert(my_sub.index.dtype == 'int64')\n        \n        if self.i in self.sub.index:\n            assert(np.all(my_sub['answered_correctly'] >= 0))\n            assert(np.all(my_sub['answered_correctly'] <= 1))\n            assert(np.all(my_sub.index == self.i))\n            assert(np.all(my_sub['row_id'] == self.sub.loc[[self.i]]['row_id']))\n            self.sub.loc[[self.i]] = my_sub\n            self.can_yield = True\n            \n        elif self.i not in self.sub.index:\n            assert(my_sub.shape[0] == 0)\n            self.can_yield = True\n\n@noglobal\ndef save_pickle(obj, path):\n    with open(path, mode='wb') as f:\n        pickle.dump(obj, f)\n\n@noglobal\ndef load_pickle(path):\n    with open(path, mode='rb') as f:\n        obj = pickle.load(f)\n    return obj\n\n@noglobal\ndef encode(train, column_name):\n    encoded = pd.merge(train[[column_name]], pd.DataFrame(train[column_name].unique(), columns=[column_name])\\\n                       .reset_index().dropna(), how='left', on=column_name)[['index']].rename(\n        columns={'index': column_name})\n    return encoded\n\n@noglobal\ndef update_user_map(tes, user_map_ref, n_users_ref):\n    #new_users = tes[tes['timestamp'] == 0]['user_id'].unique()\n    users = tes['user_id'].unique()\n    keys = user_map_ref.keys()\n    new_users = users[np.array([user not in keys for user in users])]\n    n_new_users = new_users.shape[0]\n    if n_new_users > 0:\n        user_map_ref[new_users] = np.arange(n_users_ref, n_users_ref + n_new_users)\n    return user_map_ref, n_users_ref + n_new_users\n    \n@noglobal\ndef write_to_ref_map(path, ref_name, f_names):\n    ref_map = load_pickle(path)\n    ref_map[ref_name] = f_names\n    save_pickle(ref_map, path)\n    \nclass VectorizedDict():\n    def __init__(self):\n        self.tr_dict = dict()\n        self.set_value = np.vectorize(self.tr_dict.__setitem__)\n        self.get_value = np.vectorize(self.tr_dict.__getitem__)\n        \n    def keys(self):\n        return self.tr_dict.keys()\n        \n    def __setitem__(self, indices, values):\n        self.set_value(indices, values)\n    \n    def __getitem__(self, indices):\n        if indices.shape[0] == 0:\n            return np.array([], dtype=np.int32)\n        return self.get_value(indices)    ",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "%%cython \nimport numpy as np\ncimport numpy as np\n\ncpdef np.ndarray[int] cget_memory_indices(np.ndarray task):\n    \n    cdef Py_ssize_t n = task.shape[1]\n    cdef np.ndarray[int, ndim = 2] res = np.zeros_like(task, dtype = np.int32)\n    cdef np.ndarray[int] tmp_counter = np.full(task.shape[0], -1, dtype = np.int32)\n    cdef np.ndarray[int] u_counter = np.full(task.shape[0], task.shape[1] - 1, dtype = np.int32)\n    \n    for i in range(n):\n        res[:, i] = u_counter\n        tmp_counter += 1\n        if i != n - 1:\n            mask = (task[:, i] != task[:, i + 1])\n            u_counter[mask] = tmp_counter[mask]\n    return res",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "#func_gpu\n@noglobal\ndef nn_online_get_content_id_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['content_id'].values\n    return pd.DataFrame(res, columns = ['nn_content_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_content_id_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    content_ids = spc_prev_tes_cp['content_id'].values\n    for user, content_id in zip(enc_users, content_ids):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = content_id\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_log_timestamp_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['log_timestamp'] = np.log1p(spc_tes['timestamp'].values.astype(np.float32))\n    std = 3.3530\n    mean = 20.863\n    spc_tes['normed_log_timestamp'] = (spc_tes['log_timestamp'].values - mean)/std\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['normed_log_timestamp'].values\n    return pd.DataFrame(res, columns = ['nn_normed_log_timestamp_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_normed_log_timestamp_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    timestamp = spc_prev_tes_cp['timestamp'].values.astype(np.float32)\n    std = 3.3530\n    mean = 20.863\n    normed_log_timestamps = (np.log1p(timestamp) - mean)/std\n    for user, normed_log_timestamp in zip(enc_users, normed_log_timestamps):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = normed_log_timestamp\n    return r_ref\n\n@noglobal\ndef nn_online_get_correctness_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 2\n    return pd.DataFrame(res, columns = ['nn_correctness_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_correctness_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal\ndef nn_online_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 2\n    return pd.DataFrame(res, columns = ['nn_question_had_explanation_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_early_update_reference_get_question_had_explanation_history(r_ref, tes, user_map_ref):\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n                    & (~tes['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    explanations = spc_tes_cp['prior_question_had_explanation'].values.astype('int')\n    for explanation, idx in zip(explanations, row_idx):\n        r_ref[idx, r_ref[idx] == 2] = explanation\n    return r_ref\n\n@noglobal\ndef nn_update_reference_get_question_had_explanation_history(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n    for user in row_idx:\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = 2\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = -999\n    return pd.DataFrame(res, columns = ['nn_normed_elapsed_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_early_update_reference_get_normed_elapsed_history(r_ref, tes, user_map_ref):\n    spc_tes_cp = tes[(tes['content_type_id'] == 0) \\\n                    & (~tes['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    elapsed = spc_tes_cp['prior_question_elapsed_time'].values\n    clipped = np.clip(elapsed, 0, 1000 * 300).astype(np.float32)\n    mean = 25953\n    std = 20418\n    normed_elapsed = (clipped - mean)/std\n    for el, idx in zip(normed_elapsed, row_idx):\n        r_ref[idx, r_ref[idx] == -999] = el\n    return r_ref\n\n@noglobal\ndef nn_update_reference_get_normed_elapsed_history(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_prev_tes['user_id'].values]\n    for user in row_idx:\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = -999\n    return r_ref\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_normed_modified_timedelta_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n\n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n    mean = 126568\n    std = 218000\n    clipped = (clipped - mean)/std\n    clipped[np.isnan(clipped)] = 0\n    spc_tes['normed_modified_timedelta'] = clipped.astype(np.float32)\n\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['normed_modified_timedelta'].values\n    return pd.DataFrame(res, columns = ['nn_normed_modified_timedelta_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_normed_modified_timedelta_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    clipped = np.clip(f_tes_delta['modified_timedelta'].values, 0, 1000 * 800)\n    mean = 126568\n    std = 218000\n    clipped = (clipped - mean)/std\n    clipped[np.isnan(clipped)] = 0\n    spc_prev_tes_cp['normed_modified_timedelta'] = clipped.astype(np.float32)\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    normed_modified_timedeltas = spc_prev_tes_cp['normed_modified_timedelta'].values\n    for user, normed_modified_timedelta in zip(enc_users, normed_modified_timedeltas):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = normed_modified_timedelta\n    return r_ref\n\n@noglobal\ndef nn_online_get_user_answer_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int8)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = 4\n    return pd.DataFrame(res, columns = ['nn_user_answer_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_user_answer_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['user_answer'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal\ndef online_get_task_container_id_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['task_container_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['task_container_id_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_task_container_id_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['task_container_id'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_task_container_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    value = f_tes_delta['task_container_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_tes['task_container_id_diff'] = value\n    \n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['task_container_id_diff'].values\n    return pd.DataFrame(res, columns = ['nn_task_container_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_task_container_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    value = f_tes_delta['task_container_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_prev_tes_cp['task_container_id_diff'] = value\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    task_container_id_diffs = spc_prev_tes_cp['task_container_id_diff'].values\n    for user, task_container_id_diff in zip(enc_users, task_container_id_diffs):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = task_container_id_diff\n    return r_ref\n\n@noglobal\ndef online_get_content_type_id_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['content_type_id'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['content_type_id_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_content_type_id_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['content_type_id'].values\n    return r_ref\n\n@noglobal\ndef nn_online_get_content_type_id_diff_history(r_ref, tes, f_tes_delta, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    value = f_tes_delta['content_type_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_tes['content_type_id_diff'] = value\n    \n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.float32)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['content_type_id_diff'].values\n    return pd.DataFrame(res, columns = ['nn_content_type_id_diff_history_' + str(i) + '_length_' + str(n_sample) \\\n                                        for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_content_type_id_diff_history(r_ref, prev_tes, tes, f_tes_delta, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    value = f_tes_delta['content_type_id_diff'].values\n    value[np.isnan(value)] = 0\n    spc_prev_tes_cp['content_type_id_diff'] = value\n\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    content_type_id_diffs = spc_prev_tes_cp['content_type_id_diff'].values\n    for user, content_type_id_diff in zip(enc_users, content_type_id_diffs):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = content_type_id_diff\n    return r_ref\n\n@noglobal\ndef nn_online_get_task_container_id_history(r_ref, tes, user_map_ref):\n    n_sample = r_ref.shape[1]\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    res = np.zeros((spc_tes.shape[0], n_sample + 1), dtype = np.int16)\n    res[:, :n_sample] = r_ref[user_map_ref[users]]\n    res[:, n_sample] = spc_tes['task_container_id'].values\n    return pd.DataFrame(res, columns = ['nn_task_container_id_history_' + str(i) + '_length_' + str(n_sample) for i in range(n_sample + 1)])\n\n@noglobal\ndef nn_update_reference_get_task_container_id_history(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    task_container_ids = spc_prev_tes_cp['task_container_id'].values\n    for user, task_container_id in zip(enc_users, task_container_ids):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = task_container_id\n    return r_ref\n\nclass Embedder(nn.Module):\n    def __init__(self, n_proj, n_dims):\n        super(Embedder, self).__init__()\n        self.n_proj = n_proj\n        self.n_dims = n_dims\n        self.embed = nn.Embedding(n_proj, n_dims)\n\n    def forward(self, indices):\n        z = self.embed(indices)\n        return z\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass MyEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super(MyEncoderLayer, self).__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        if activation == 'relu':\n            self.activation = F.relu\n        elif activation == 'gelu':\n            self.activation = F.gelu\n\n    def forward(self, q, k, v, src_mask, src_key_padding_mask):\n        src2 = self.self_attn(q, k, v, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[0]\n        src = q + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src\n\nclass MyEncoderExp162(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp162, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n        \n        #concat \n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp166(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp166, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 4 * args.n_conv, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n        self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n        self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n        self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        for_conv = for_conv.contiguous().transpose(2, 3).view(N * n_length * 2, -1, args.n_conv)\n        out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_length * 2, N, -1)\n        \n        #concat \n        out = torch.cat([out, out_conv, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 4 * args.n_conv)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2).transpose(1, 2)\n\n            out_conv = self.conv3(F.relu(self.conv2(F.relu(self.conv1(for_conv))))).view(n_batch, - 1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_conv, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp184(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp184, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 16 * 8, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        #conv\n#         self.conv1 = nn.Conv1d(11 + 2, 8, 7, padding = 3)\n#         self.conv2 = nn.Conv1d(8, 8, 5, padding = 2)\n#         self.conv3 = nn.Conv1d(8, 4, 3, padding = 1)\n        \n        #rnn\n        self.rnn = nn.GRU(13, 16, 8, batch_first = True)\n\n    def forward(self, batch, args):\n        #input\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n            \n        # relative positional encoding\n        position = batch['position']\n            \n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #key and value\n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n        \n        #query process\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n        \n        #key and value process\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        \n        #rnn\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n        _, out_rnn = self.rnn(for_rnn)\n        out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n\n        #cat\n        out = torch.cat([out, out_rnn, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 16 * 8)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n            \n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n            \n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n            \n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query[:, :n_sample].clone()\n            query_clone[token_idx, -1] = query_clone[token_idx, -1] * 0\n            memory_cat = torch.cat([query_clone, ohe_explanation[:, :-1], ohe_correctness[:, :-1], \n                                    normed_elapsed.unsqueeze(2)[:, :-1], ohe_user_answer[:, :-1]], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2)\n            \n            #rnn\n            for_rnn = for_conv\n            _, out_rnn = self.rnn(for_rnn)\n            out_rnn = out_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_rnn, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp218(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp218, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp224(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp224, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256 + 16 * 8, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n        \n        self.fixlen_rnn = nn.GRU(13, 16, 8, batch_first = True)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n        \n        #conv process\n        conv_inputs_int = batch['conv_inputs_int']\n        conv_inputs_float = batch['conv_inputs_float']\n        conv_inputs_const = batch['conv_inputs_const']\n\n        conv_explanation = conv_inputs_int[:, :, :, 0]\n        conv_correctness = conv_inputs_int[:, :, :, 1]\n        conv_normed_elapsed = conv_inputs_float[:, :, :, 0]\n        conv_normed_log_timestamp = conv_inputs_float[:, :, :, 1]\n        conv_normed_timedelta = conv_inputs_float[:, :, :, 2]\n        conv_task_container_id_diff = conv_inputs_float[:, :, :, 3]\n        conv_content_type_id_diff = conv_inputs_float[:, :, :, 4]\n    \n        const_normed_log_timestamp = conv_inputs_const[:, :, :, 0]\n        const_normed_timedelta = conv_inputs_const[:, :, :, 1]\n        ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n        ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n        for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n        conv_normed_elapsed.unsqueeze(3), conv_normed_log_timestamp.unsqueeze(3),\n        conv_normed_timedelta.unsqueeze(3), conv_task_container_id_diff.unsqueeze(3),\n        conv_content_type_id_diff.unsqueeze(3), const_normed_log_timestamp.unsqueeze(3), \n        const_normed_timedelta.unsqueeze(3)], dim = 3).transpose(0, 1)\n        \n        #rnn\n        for_rnn = for_conv.contiguous().view(N * n_length * 2, args.n_conv, -1)\n        _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n        out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_length *2, N, -1)\n\n        #cat\n        out = torch.cat([out, out_fixlen_rnn, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256 + 16 * 8)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n            \n            #conv process\n            conv_explanation = explanation[:, -args.n_conv - 1 : -1]\n            conv_correctness = correctness[:, -args.n_conv - 1 : -1]\n            conv_normed_elapsed = normed_elapsed[:, -args.n_conv - 1 : -1]\n            conv_normed_log_timestamp = normed_log_timestamp[:, -args.n_conv - 1 : -1]\n            conv_normed_timedelta = normed_timedelta[:, -args.n_conv - 1 : -1]\n            conv_task_container_id_diff = task_container_id_diff[:, -args.n_conv - 1 : -1]\n            conv_content_type_id_diff = content_type_id_diff[:, -args.n_conv - 1 : -1]\n            \n            \n            const_normed_log_timestamp = normed_log_timestamp[:, -1:].repeat(1, args.n_conv)\n            const_normed_timedelta = normed_timedelta[:, -1:].repeat(1, args.n_conv)\n\n            ohe_conv_explanation = F.one_hot(conv_explanation, num_classes = 3)\n            ohe_conv_correctness = F.one_hot(conv_correctness, num_classes = 3)\n\n            for_conv = torch.cat([ohe_conv_explanation, ohe_conv_correctness,\n            conv_normed_elapsed.unsqueeze(2), conv_normed_log_timestamp.unsqueeze(2),\n            conv_normed_timedelta.unsqueeze(2), conv_task_container_id_diff.unsqueeze(2),\n            conv_content_type_id_diff.unsqueeze(2), const_normed_log_timestamp.unsqueeze(2), \n            const_normed_timedelta.unsqueeze(2)], dim = 2)\n            \n            #rnn\n            for_rnn = for_conv\n            _, out_fixlen_rnn = self.fixlen_rnn(for_rnn)\n            out_fixlen_rnn = out_fixlen_rnn.transpose(0, 1).contiguous().view(n_batch, -1)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, out_fixlen_rnn, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp219(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp219, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp221(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp221, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.GRU(512, 256, 4, batch_first = True)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n    \nclass MyEncoderExp222(nn.Module):\n    def __init__(self, args):\n        super(MyEncoderExp222, self).__init__()\n        #query, key and value\n        self.embedder_content_id = Embedder(13523, 512)\n        self.linear1 = nn.Linear(714 + 1 + 2, args.emb_dim * 2)\n        self.linear2 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.dropout = nn.Dropout(args.dropout)\n        self.norm1 = nn.LayerNorm(args.emb_dim)\n\n        #key and value\n        self.linear3 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear4 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm2 = nn.LayerNorm(args.emb_dim)\n\n        #for rnn\n        self.linear5 = nn.Linear(714 + 1 + 2 + 256, args.emb_dim * 2)\n        self.linear6 = nn.Linear(args.emb_dim * 2, args.emb_dim)\n        self.norm3 = nn.LayerNorm(args.emb_dim)\n\n        self.linear7 = nn.Linear(args.emb_dim + 12, args.emb_dim)\n        self.linear8 = nn.Linear(args.emb_dim, args.emb_dim)\n        self.norm4 = nn.LayerNorm(args.emb_dim)\n        self.rnn = nn.LSTM(512, 256, 4, batch_first = True, dropout = args.dropout)\n\n        #MyEncoder\n        self.MyEncoderLayer1 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer2 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer3 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.MyEncoderLayer4 = MyEncoderLayer(args.emb_dim, args.nhead, dim_feedforward = args.dim_feedforward, \n                                                dropout = args.dropout)\n        self.last_fc1 = nn.Linear(args.emb_dim + args.num_features + 256, args.emb_dim * 4)\n        self.last_fc2 = nn.Linear(args.emb_dim * 4, 1)\n\n    def forward(self, batch, args):\n        #inputs\n        inputs_int = batch['inputs_int']\n        inputs_float = batch['inputs_float']\n        inputs_add = batch['inputs_add'].transpose(0, 1)\n        inputs_add_2 = batch['inputs_add_2']\n        N = inputs_int.shape[0]\n        n_length = int(inputs_int.shape[1]/2)\n\n        #for query, key and value\n        content_id = inputs_int[:, :, 0]\n        part = (inputs_int[:, :, 1] - 1)\n        tags = inputs_int[:, :, 2:8]\n        tag_mask = batch['tag_mask']\n        #digit_timedelta = inputs_int[:, :, 8]\n        normed_timedelta = inputs_float[:, :, 2]\n        normed_log_timestamp = inputs_float[:, :, 1]\n        correct_answer = inputs_int[:, :, 13]\n        task_container_id_diff = inputs_add_2[:, :, 0]\n        content_type_id_diff = inputs_add_2[:, :, 1]\n\n        #for key and value\n        explanation = inputs_int[:, :, 9]\n        correctness = inputs_int[:, :, 10]\n        normed_elapsed = inputs_float[:, :, 0]\n        user_answer = inputs_int[:, :, 11]\n        end_pos_idx = batch['end_pos_idx']\n\n        #mask\n        memory_mask = torch.repeat_interleave(batch['memory_mask'], args.nhead, dim = 0)\n        padding_mask = batch['padding_mask']\n\n        #target, loss_mask\n        target = batch['target']\n        if 'loss_mask' in batch.keys():\n            loss_mask = batch['loss_mask']\n        else:\n            loss_mask = batch['cut_mask']\n\n        # relative positional encoding\n        position = batch['position']\n\n        #query, key and value\n        emb_content_id = self.embedder_content_id(content_id)\n        ohe_part = F.one_hot(part, num_classes = 7)\n        ohe_tags = F.one_hot(tags, num_classes = 189)\n        ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n        ohe_tags_sum = ohe_tags.sum(dim = 2)\n        ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n        #initial query and memory \n        ohe_explanation = F.one_hot(explanation, num_classes = 3)\n        ohe_correctness = F.one_hot(correctness, num_classes = 3)\n        ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n        query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n\n        #for rnn process\n        memory[padding_mask] = memory[padding_mask] * 0\n\n        tmp, _ = self.rnn(memory)\n        out_rnn = tmp.clone()\n        out_rnn[torch.arange(N), end_pos_idx - 1] = out_rnn[torch.arange(N), end_pos_idx - 1] * 0\n\n        memory_idx = batch['memory_idx']\n        memory_idx_ = memory_idx + (torch.arange(memory_idx.shape[0]).to(args.device) * memory_idx.shape[1])[:, None]\n        out_rnn = out_rnn.contiguous().view(N * args.n_length * 2, -1)\n        memory_rnn = out_rnn[memory_idx_]\n\n        #new query and memory\n        query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                               normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                               ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                               content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n        query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n        query_clone = query.clone()\n        query_clone[torch.arange(N), end_pos_idx - 1] = query_clone[torch.arange(N), end_pos_idx - 1] * 0\n        memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n        memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n\n        #transpose\n        query = query.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n\n        #MyEncoder\n        out = self.MyEncoderLayer1(query, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer2(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer3(out, memory, memory, memory_mask, padding_mask)\n        out = self.MyEncoderLayer4(out, memory, memory, memory_mask, padding_mask)\n\n        #cat\n        out = torch.cat([out, inputs_add, memory_rnn.transpose(0, 1)], dim = 2)\n        out = self.last_fc2(self.dropout(F.relu(self.last_fc1(out.reshape(-1, args.emb_dim + args.num_features + 256)))))\\\n        .reshape(-1, N).transpose(0, 1)\n        score = torch.sigmoid(out)\n        err = F.binary_cross_entropy_with_logits(out[loss_mask], target[loss_mask])\n        return err, score\n    \n    def predict_on_batch(self, inputs, args, pred_bs, que, que_proc_int):\n        torch.set_grad_enabled(False)\n        N = inputs['content_id'].shape[0]\n        scores = torch.zeros(N).to(args.device)\n        for i in range(0, N, pred_bs):\n            content_id = inputs['content_id'][i: i + pred_bs]\n            n_batch = content_id.shape[0]\n            n_sample = content_id.shape[1] - 1\n\n            padding_mask = (content_id == -1)[:, :n_sample]\n            token_idx = padding_mask[:, -1].copy()\n            padding_mask[:, -1] = False\n\n            part = que['part'].values[content_id] - 1\n            tags = que_proc_int[content_id]\n            tag_mask = (tags != 188)\n            correct_answer = que['correct_answer'].values[content_id]\n\n            normed_timedelta = inputs['normed_timedelta'][i: i + pred_bs]\n            normed_log_timestamp = inputs['normed_log_timestamp'][i: i + pred_bs]\n            explanation = inputs['explanation'][i: i + pred_bs]\n            correctness = inputs['correctness'][i: i + pred_bs]\n            normed_elapsed = inputs['normed_elapsed'][i: i + pred_bs]\n            user_answer = inputs['user_answer'][i: i + pred_bs]\n\n            #diff features\n            task_container_id_diff = inputs['task_container_id_diff'][i: i + pred_bs]/10000\n            content_type_id_diff = inputs['content_type_id_diff'][i: i + pred_bs]\n\n            #preprop\n            content_id = torch.from_numpy(content_id.astype(np.int64)).to(args.device)\n            content_id[content_id == -1] = 1\n            padding_mask = torch.from_numpy(padding_mask).to(args.device)\n            token_idx = torch.from_numpy(token_idx).to(args.device)\n            part = torch.from_numpy(part.astype(np.int64)).to(args.device)\n            tags = torch.from_numpy(tags.astype(np.int64)).to(args.device)\n            tag_mask = torch.from_numpy(tag_mask).to(args.device)\n            correct_answer = torch.from_numpy(correct_answer.astype(np.int64)).to(args.device)\n\n            normed_timedelta = torch.from_numpy(normed_timedelta).to(args.device)\n            normed_log_timestamp = torch.from_numpy(normed_log_timestamp).to(args.device)\n            explanation = torch.from_numpy(explanation.astype(np.int64)).to(args.device)\n            explanation[explanation == -2] = 1\n            explanation[explanation == 2] = 1\n            correctness = torch.from_numpy(correctness.astype(np.int64)).to(args.device)\n            correctness[correctness == -1] = 1\n            correctness[correctness == 2] = 1\n            normed_elapsed = torch.from_numpy(normed_elapsed).to(args.device)\n            normed_elapsed[normed_elapsed == -999] = 0\n            user_answer = torch.from_numpy(user_answer.astype(np.int64)).to(args.device)\n            user_answer[user_answer == -1] = 1\n            user_answer[user_answer == 4] = 1\n\n            #diff features\n            task_container_id_diff = torch.from_numpy(task_container_id_diff.astype(np.float32)).to(args.device)\n            content_type_id_diff = torch.from_numpy(content_type_id_diff.astype(np.float32)).to(args.device)\n\n            #generate token\n            explanation[token_idx, n_sample - 1] = 2\n            correctness[token_idx, n_sample - 1] = 2\n            normed_elapsed[token_idx, n_sample - 1] = 0\n            user_answer[token_idx, n_sample - 1] = 4\n\n            #for conv process\n            explanation[:, :-1][padding_mask] = 2\n            correctness[:, :-1][padding_mask] = 2\n            normed_elapsed[:, :-1][padding_mask] = 0\n            normed_timedelta[:, :-1][padding_mask] = 0\n            normed_log_timestamp[:, :-1][padding_mask] = 0\n\n            #query, key and value\n            emb_content_id = self.embedder_content_id(content_id)\n            ohe_part = F.one_hot(part, num_classes = 7)\n            ohe_tags = F.one_hot(tags, num_classes = 189)\n            ohe_tags = ohe_tags * tag_mask.unsqueeze(3)\n            ohe_tags_sum = ohe_tags.sum(dim = 2)\n            ohe_correct_answer = F.one_hot(correct_answer, num_classes = 4)\n\n            #key and value\n            ohe_explanation = F.one_hot(explanation, num_classes = 3)\n            ohe_correctness = F.one_hot(correctness, num_classes = 3)\n            ohe_user_answer = F.one_hot(user_answer, num_classes = 5)\n\n            # relative positional encoding\n            position = torch.from_numpy(((np.arange(args.n_length + 1) - args.n_length)/(args.n_length)).astype(np.float32))\n            position = position.unsqueeze(0).repeat(n_batch, 1).to(args.device)\n\n            #query process\n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2)], dim = 2)\n            query = self.norm1(self.linear2(self.dropout(F.relu(self.linear1(query_cat)))))\n\n            #key and value process\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm2(self.linear4(self.dropout(F.relu(self.linear3(memory_cat)))))\n            memory[:, :-1][padding_mask] = 0\n            memory[:, -1] = 0\n            out_rnn, _ = self.rnn(memory)\n            out_rnn[:, -1] = 0\n\n            task_container_id = inputs['task_container_id'][i: i + pred_bs]\n            memory_idx = cget_memory_indices(task_container_id)\n            memory_idx_ = memory_idx + (np.arange(memory_idx.shape[0]) * memory_idx.shape[1])[:, None]\n            out_rnn = out_rnn.contiguous().view(n_batch * (n_sample + 1), -1)\n            memory_rnn = out_rnn[torch.from_numpy(memory_idx_).to(args.device)]\n\n            #query process 2 \n            query_cat = torch.cat([emb_content_id, ohe_part, ohe_tags_sum, \n                                   normed_timedelta.unsqueeze(2), normed_log_timestamp.unsqueeze(2), \n                                   ohe_correct_answer, task_container_id_diff.unsqueeze(2), \n                                   content_type_id_diff.unsqueeze(2), position.unsqueeze(2), memory_rnn], dim = 2)\n            query = self.norm3(self.linear6(self.dropout(F.relu(self.linear5(query_cat)))))\n\n            #key and value process 2\n            query_clone = query.clone()\n            query_clone[token_idx, -2] = 0\n            memory_cat = torch.cat([query_clone, ohe_explanation, ohe_correctness, \n                                    normed_elapsed.unsqueeze(2), ohe_user_answer], dim = 2)\n            memory = self.norm4(self.linear8(self.dropout(F.relu(self.linear7(memory_cat)))))\n            memory = memory[:, :-1]\n\n            #transpose\n            query = query.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            spc_query = query[-1:]\n\n            #MyEncoder\n            out = self.MyEncoderLayer1(spc_query, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer2(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer3(out, memory, memory, None, padding_mask)\n            out = self.MyEncoderLayer4(out, memory, memory, None, padding_mask).squeeze(0)\n\n            #features\n            features = inputs['features'][i: i + pred_bs].copy()\n            features[:, args.log_indices] = np.log1p(features[:, args.log_indices])\n            features[:, args.stdize_indices] = (features[:, args.stdize_indices] - args.stdize_mu_std[0][None, :])/args.stdize_mu_std[1][None, :]\n            features[np.isnan(features)] = 0\n            features = torch.from_numpy(features).to(args.device)\n\n            #concat and last fc\n            out_cat = torch.cat([out, features, memory_rnn[:, -1]], dim = 1)\n            out_cat = self.last_fc2(self.dropout(F.relu(self.last_fc1(out_cat))))\n            score = torch.sigmoid(out_cat[:, 0])\n            scores[i: i + pred_bs] = score\n        return scores.cpu().numpy()\n\n#func_cpu\n@noglobal\ndef online_get_user_id_content_id_task_container_id(tes):\n    f_tes = tes[tes['content_type_id'] == 0][['user_id', \n                                           'content_id', 'task_container_id']].astype('float32').reset_index(drop = True)\n    return f_tes\n\n@noglobal\ndef online_get_timestamp_and_prior_question_elapsed_time_and_prior_question_had_explanation(tes):\n    f_tes = tes[tes['content_type_id'] == 0][['timestamp', \n                                              'prior_question_elapsed_time', 'prior_question_had_explanation']].astype('float32').reset_index(drop = True)\n    return f_tes\n\n@noglobal\ndef online_get_part(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['part'].values[spc_tes_cp['content_id'].values], columns = ['part']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_correct_answer(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef online_get_tags(tes, que_proc_2):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    return pd.DataFrame(que_proc_2.values[spc_tes_cp['content_id'].values], columns = ['tags_' + str(i) for i in range(6)])\n\n@noglobal\ndef online_get_hell_rolling_mean_for_tags(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[:, :-1][user_map_ref[spc_tes['user_id'].values]]\n    f_tes = pd.DataFrame(sum_count[:, :, 0]/sum_count[:, :, 1], columns = ['hell_rolling_mean_for_tag_' + str(i) for i in range(188)])\n    return f_tes\n\n@noglobal\ndef online_get_rolling_mean_sum_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    mean = sum_count[:, 0]/sum_count[:, 1]\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n    res[:, 0] = mean\n    res[:, 1:] = sum_count\n    res = np.zeros((spc_tes.shape[0], 3), dtype = np.float32)\n    res[:, 0] = mean\n    res[:, 1:] = sum_count\n    return pd.DataFrame(res, columns = ['target_full_mean', 'target_full_sum', 'target_count'])\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    cts = spc_prev_tes_cp['user_id'].value_counts()\n    pos_cts = spc_prev_tes_cp['user_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n    r_ref[user_map_ref[pos_cts.index], 0] += pos_cts.values\n    r_ref[user_map_ref[cts.index], 1] += cts.values\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, tes, que_proc, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    col_idx = que_proc.values[spc_tes['content_id'].values]\n    users = user_map_ref[spc_tes['user_id'].values]\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\n\n    res = np.zeros((spc_tes.shape[0], 7, 3), dtype = np.float32)\n    sliced = r_ref[row_idx, col_idx]\n    res[:, :6, 1:] = sliced\n    res[:, 6, 1:] = np.nansum(sliced, axis = 1)\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n    res = res.reshape(-1, 3 * 7)\n    f_names = sum([['tags_order_mean_' + str(i), 'tags_order_sum_' + str(i), 'tags_order_count_' + str(i)] for i in range(6)], [])\n    f_names += ['whole_tags_order_mean', 'whole_tags_order_sum', 'whole_tags_order_count']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref, prev_tes, tes, que_onehot, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(188)]).T\n    r_ref[user_map_ref[uniq], :-1, 0] += sum_cts\n\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id']]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(188)]).T\n    r_ref[user_map_ref[uniq], :-1, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_sum_count_for_part(r_ref, tes, que, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['part'] = que['part'].values[spc_tes['content_id'].values]\n    res = np.zeros((spc_tes.shape[0], 8, 3), dtype = np.float32)\n    res[:, :7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res[:, 7, 1:] = r_ref[user_map_ref[spc_tes['user_id'].values], spc_tes['part'] - 1]\n    res[:, :, 0] = res[:, :, 1]/res[:, :, 2]\n    res = res.reshape(-1, 3 * 8)\n    f_names = sum([['part_' + str(i) + '_mean', 'part_' + str(i) + '_sum', 'part_' + str(i) + '_count'] for i in range(1, 8)], []) + \\\n    ['part_cut_mean', 'part_cut_sum', 'part_cut_count']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_part(r_ref, prev_tes, tes, que_part_onehot, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    \n    for_sum = que_part_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values[spc_prev_tes_cp['answered_correctly'] == 1], return_inverse = True)\n    sum_cts = np.array([np.bincount(enc, weights = for_sum[:, i]) for i in range(7)]).T\n    r_ref[user_map_ref[uniq], :, 0] += sum_cts\n    \n    for_count = que_part_onehot.values[spc_prev_tes_cp['content_id']]\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'], return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(7)]).T\n    r_ref[user_map_ref[uniq], :, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_lec_rolling_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    return pd.DataFrame(r_ref[user_map_ref[spc_tes['user_id'].values]], columns = ['lec_rolling_count'])\n\n@noglobal\ndef update_reference_get_lec_rolling_count(r_ref, prev_tes, tes, user_map_ref):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    r_ref[user_map_ref[spc_prev_tes['user_id'].values]] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_part_rolling_count(r_ref, tes, user_map_ref, que):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 8), dtype = np.float32)\n    part_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res[:, :7] = part_count\n    res[:, 7] = part_count[np.arange(spc_tes.shape[0]), que['part'].values[spc_tes['content_id'].values] - 1]\n    return pd.DataFrame(res, columns = ['lec_part_' + str(i + 1) for i in range(7)] + ['lec_part_cut'])\n\n@noglobal\ndef update_reference_get_lec_part_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\n    col_idx = lec['part'].values[lec_map[spc_prev_tes['content_id'].values]] - 1\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_type_of_rolling_count(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['lec_type_of_' + str(i) for i in range(4)]).astype(np.float32)\n\n@noglobal\ndef update_reference_get_lec_type_of_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec_proc, lec_map):\n    spc_prev_tes = prev_tes[prev_tes['content_type_id'] == 1]\n    row_idx = user_map_ref[spc_prev_tes['user_id']]\n    col_idx = lec_proc['type_of'].values[lec_map[spc_prev_tes['content_id'].values]]\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal\ndef online_get_lec_tags_rolling_count(r_ref, tes, user_map_ref, que_proc):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n    users = user_map_ref[spc_tes['user_id'].values]\n    row_idx = np.repeat(users[:, None], 6, axis = 1)\n    col_idx = que_proc.values[spc_tes['content_id'].values]\n    \n    sliced = r_ref[row_idx, col_idx]\n    res[:, :6] = sliced\n    res[:, 6] = np.nansum(sliced, axis = 1)\n    return pd.DataFrame(res, columns = sum([['lec_tags_order_count_' + str(i)] for i in range(6)], []) + ['lec_whole_tags_order_count'])\n\n@noglobal\ndef update_reference_get_lec_tags_rolling_count(r_ref, prev_tes, tes, user_map_ref, lec, lec_map):\n    prev_tes_cp = prev_tes.copy()\n    #prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 1].copy().reset_index(drop = True)\n    col_idx = lec['tag'].values[lec_map[spc_prev_tes_cp['content_id'].values]]\n    row_idx = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    r_ref[row_idx, col_idx] += 1\n    return r_ref\n\n@noglobal \ndef online_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_tes['user_id'].values]\n    col_idx = spc_tes['content_id'].values\n    f_sum = r_ref_sum[row_idx, col_idx].toarray()[0]\n    f_count = r_ref_count[row_idx, col_idx].toarray()[0]\n    f_mean = f_sum/f_count\n    f_tr = pd.DataFrame()\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n    return f_tr\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_content_id(r_ref_sum, r_ref_count, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    contents = spc_prev_tes_cp['content_id'].values\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, content, target in zip(users, contents, targets):\n        r_ref_sum[user, content] = r_ref_sum[user, content] + target\n        r_ref_count[user, content] = r_ref_count[user, content] + 1\n    return r_ref_sum, r_ref_count\n\n@noglobal\ndef online_get_timestamp_diff(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['timestamp_diff']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_timestamp_diff(r_ref, prev_tes, tes, user_map_ref):\n    r_ref[user_map_ref[prev_tes['user_id'].values]] = prev_tes['timestamp'].values\n    return r_ref\n\n@noglobal\ndef online_get_whole_oof_target_encoding_content_id(r_ref, tes):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[spc_tes['content_id'].values]\n    res = (sum_count[:, 0]/sum_count[:, 1]).astype(np.float32)\n    return pd.DataFrame(res, columns = ['whole_oof_target_encoding_content_id'])\n\n@noglobal\ndef update_reference_get_whole_oof_target_encoding_content_id(r_ref, prev_tes, tes):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    cts = spc_prev_tes_cp['content_id'].value_counts()\n    pos_cts = spc_prev_tes_cp['content_id'][spc_prev_tes_cp['answered_correctly'] == 1].value_counts()\n    r_ref[pos_cts.index.values, 0] += pos_cts.values\n    r_ref[cts.index.values, 1] += cts.values\n    return r_ref\n\n@noglobal\ndef online_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, tes, que_proc):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[que_proc.values[spc_tes['content_id'].values]]\n    mean = sum_count[:, :, 0]/sum_count[:, :, 1]\n    nansum = np.nansum(sum_count, axis = 1)\n    whole_mean = nansum[:, 0]/nansum[:, 1]\n    res = np.zeros((spc_tes.shape[0], 7), dtype = np.float32)\n    res[:, :6] = mean\n    res[:, 6] = whole_mean\n    f_names = ['whole_oof_target_encoding_tags_order_' + str(i) for i in range(6)] + ['whole_oof_target_encoding_whole_tags']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef update_reference_get_whole_oof_target_encoding_tags_order_and_whole_tags(r_ref, prev_tes, tes, que_onehot):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    for_sum = que_onehot.values[spc_prev_tes_cp['content_id'].values[spc_prev_tes_cp['answered_correctly'] == 1]]\n    sum_cts = for_sum.sum(axis = 0)\n    r_ref[:-1, 0] += sum_cts\n\n    for_count = que_onehot.values[spc_prev_tes_cp['content_id'].values]\n    cts = for_count.sum(axis = 0)\n    r_ref[:-1, 1] += cts\n    return r_ref\n\n@noglobal\ndef online_get_correct_answer(tes, que):\n    spc_tes_cp = tes[tes['content_type_id'] == 0].copy().reset_index(drop = True)\n    f_tes = pd.DataFrame(que['correct_answer'].values[spc_tes_cp['content_id'].values], columns = ['correct_answer']).astype(np.float32)\n    return f_tes\n\n@noglobal\ndef online_get_modified_timedelta(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    uniq, enc = np.unique(spc_tes['user_id'].values, return_inverse = True)\n    spc_tes['count'] = np.bincount(enc)[enc]\n    res = (spc_tes['timestamp'].values - r_ref[user_map_ref[spc_tes['user_id'].values]])/spc_tes['count'].values\n    return pd.DataFrame(res, columns = ['modified_timedelta']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n\n    for_count = np.zeros((spc_prev_tes_cp.shape[0], 4))\n    for_count[np.arange(for_count.shape[0]), spc_prev_tes_cp['user_answer'].values] = 1\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    cts = np.array([np.bincount(enc, weights = for_count[:, i]) for i in range(4)]).T\n    r_ref[user_map_ref[uniq]] += cts\n    return r_ref\n\n@noglobal\ndef online_get_norm_rolling_count_and_cut_for_user_answer(r_ref, tes, que, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = np.zeros((spc_tes.shape[0], 5), dtype = np.float32)\n    cts = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    norm_cts = cts/cts.sum(axis = 1)[:, None]\n    res[:, :4] = norm_cts\n    correct_answer = que['correct_answer'].values[spc_tes['content_id'].values]\n    res[:, 4] = norm_cts[np.arange(norm_cts.shape[0]), correct_answer]\n    f_names = ['norm_rolling_count_user_answer_' + str(i) for i in range(4)] + ['cut_norm_rolling_count_user_answer']\n    return pd.DataFrame(res, columns = f_names)\n\n@noglobal\ndef online_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n    return r_ref\n\n@noglobal\ndef online_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values.astype('int') * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    uniq, enc = np.unique(tes_cp[tes_cp['content_type_id'] == 0]['user_id'].values, return_inverse = True)\n    r_ref[user_map_ref[uniq], 2] = np.bincount(enc)\n    return r_ref\n\n@noglobal\ndef online_get_rolling_sum_for_prior_question_isnull(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    res = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    return pd.DataFrame(res, columns = ['rolling_sum_for_prior_question_isnull']).astype(np.float32)\n\n@noglobal\ndef update_reference_get_rolling_sum_for_prior_question_isnull(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes[(prev_tes['content_type_id'] == 0) & (prev_tes['prior_question_elapsed_time'].isnull())].copy()\n    r_ref[user_map_ref[prev_tes_cp['user_id'].values]] += prev_tes_cp['prior_question_elapsed_time'].isnull().astype('int').values\n    return r_ref\n\n@noglobal\ndef online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_elapsed_time']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_elapsed_time'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_elapsed_time'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['positive_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    sum_count = r_ref[user_map_ref[spc_tes['user_id'].values]]\n    res = sum_count[:, 0]/sum_count[:, 1]\n    return pd.DataFrame(res, columns = ['negative_rolling_mean_for_prior_question_had_explanation']).astype(np.float32)\n\n@noglobal\ndef early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, tes, user_map_ref):\n    tes_cp = tes.copy()\n    spc_tes_cp = tes_cp[(tes_cp['content_type_id'] == 0) \\\n                        & (~tes_cp['prior_question_had_explanation'].isnull())].copy().reset_index(drop = True)\n    row_idx = user_map_ref[spc_tes_cp['user_id'].values]\n    r_ref[row_idx, 0] += spc_tes_cp['prior_question_had_explanation'].values * r_ref[row_idx, 2]\n    r_ref[row_idx, 1] += r_ref[row_idx, 2]\n    return r_ref\n\n@noglobal\ndef update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    uniq, enc = np.unique(spc_prev_tes_cp['user_id'].values, return_inverse = True)\n    sum_cts = np.bincount(enc, weights = 1 - spc_prev_tes_cp['answered_correctly'].values)\n    r_ref[user_map_ref[uniq], 2] = sum_cts\n    return r_ref\n\n@noglobal\ndef online_get_diff_modified_timedelta_and_prior_elapsed_time_rolling_mean(f_tes_base, f_tes_pn, f_tes_p, f_tes_n):\n    f_names = ['rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n                                   'positive_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta', \n                                   'negative_rolling_mean_for_prior_question_elapsed_time_diff_modified_timedelta']\n    f_tes = pd.DataFrame()\n    f_tes[f_names[0]] = f_tes_base.values[:, 0] - f_tes_pn.values[:, 0]\n    f_tes[f_names[1]] = f_tes_base.values[:, 0] - f_tes_p.values[:, 0]\n    f_tes[f_names[2]] = f_tes_base.values[:, 0] - f_tes_n.values[:, 0]\n    return f_tes\n\n@noglobal\ndef online_get_n_samples_rolling_mean(r_ref, tes, user_map_ref):\n    n_samples = np.array([1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100, 200])\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    users = spc_tes['user_id'].values\n    history = r_ref[user_map_ref[users]].astype(np.float32)\n    history[history == -1] = np.nan\n    res = np.zeros((spc_tes.shape[0], len(n_samples)), dtype = np.float32)\n    for i, n_sample in enumerate(n_samples):\n        res[:, i] = np.nanmean(history[:, -n_sample:], axis = 1)\n    return pd.DataFrame(res, columns = [str(i) + '_samples_rolling_mean' for i in n_samples])\n\n@noglobal\ndef update_reference_get_n_samples_rolling_mean(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    #prev_tes_cp['user_answer'] = ast.literal_eval(tes['prior_group_responses'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    enc_users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, target in zip(enc_users, targets):\n        r_ref[user, :-1] = r_ref[user, 1:]\n        r_ref[user, -1] = target\n    return r_ref\n\n@noglobal \ndef online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, tes, user_map_ref):\n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    row_idx = user_map_ref[spc_tes['user_id'].values]\n    col_idx = spc_tes['content_id'].values\n    f_base = r_ref[row_idx, col_idx].toarray()[0]\n    f_sum = f_base % 128\n    f_count = f_base // 128\n    f_mean = f_sum/f_count\n    f_tr = pd.DataFrame()\n    f_tr['content_id_cut_mean'] = f_mean.astype(np.float32)\n    f_tr['content_id_cut_sum'] = f_sum.astype(np.float32)\n    f_tr['content_id_cut_count'] = f_count.astype(np.float32)\n    return f_tr\n\n@noglobal\ndef update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref, prev_tes, tes, user_map_ref):\n    prev_tes_cp = prev_tes.copy()\n    prev_tes_cp['answered_correctly'] = ast.literal_eval(tes['prior_group_answers_correct'].iloc[0])\n    spc_prev_tes_cp = prev_tes_cp[prev_tes_cp['content_type_id'] == 0].copy().reset_index(drop = True)\n    users = user_map_ref[spc_prev_tes_cp['user_id'].values]\n    contents = spc_prev_tes_cp['content_id'].values\n    targets = spc_prev_tes_cp['answered_correctly'].values\n    for user, content, target in zip(users, contents, targets):\n        r_ref[user, content] = r_ref[user, content] + target + 128\n    return r_ref",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "#initialization\n#comment out\n#prefix = '..'\nprefix = '/kaggle/input/mamastan-gpu-v27'\n#sn = pd.read_pickle('../others/ex_tes.pkl'); env = RiiidEnv(sn, iterate_wo_predict = False)\nimport riiideducation; env = riiideducation.make_env()\niter_test = env.iter_test()\ngc.collect()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "#params162\nargs = OrderedDict()\nargs.n_length = 200\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.pred_bs = 256\nargs.device = torch.device('cuda')\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp162 = deepcopy(args)\ndel args;\n\n#params166\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.pred_bs = 256\nargs.device = torch.device('cuda')\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp166 = deepcopy(args)\ndel args;\n\n#params184\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\n\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp184 = deepcopy(args)\ndel args;\n\n#params218\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\n\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\n\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp218 = deepcopy(args)\ndel args;\n\n#params224\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.n_conv = 30\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp224 = deepcopy(args)\ndel args;\n\n#params219\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.lr = 0.2 * 1e-3\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp219 = deepcopy(args)\ndel args;\n\n#params221\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp221 = deepcopy(args)\ndel args;\n\n#params222\nargs = OrderedDict()\nargs.n_length = 400\nargs.emb_dim = 512\nargs.dim_feedforward = 1028\nargs.nhead = 4\nargs.dropout = 0.2\nargs.dropout_pe = 0\nargs.num_features = 90\nargs.device = torch.device('cuda')\nargs.pred_bs = 256\n\nf_names = ['target_full_mean',\n 'target_count',\n 'tags_order_mean_0',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_mean_1',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_mean_2',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_mean_3',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_mean_4',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_mean_5',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_mean',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_mean',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_mean',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_mean',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_mean',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_mean',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_mean',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_mean',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_mean',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_mean',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'norm_rolling_count_user_answer_0',\n 'norm_rolling_count_user_answer_1',\n 'norm_rolling_count_user_answer_2',\n 'norm_rolling_count_user_answer_3',\n 'cut_norm_rolling_count_user_answer',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'rolling_mean_for_prior_question_had_explanation',\n 'rolling_sum_for_prior_question_isnull',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_had_explanation',\n 'negative_rolling_mean_for_prior_question_had_explanation',\n '1_samples_rolling_mean',\n '2_samples_rolling_mean',\n '3_samples_rolling_mean',\n '4_samples_rolling_mean',\n '5_samples_rolling_mean',\n '10_samples_rolling_mean',\n '20_samples_rolling_mean',\n '30_samples_rolling_mean',\n '40_samples_rolling_mean',\n '50_samples_rolling_mean',\n '100_samples_rolling_mean',\n '200_samples_rolling_mean']\nlog_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'lec_part_1',\n 'lec_part_2',\n 'lec_part_3',\n 'lec_part_4',\n 'lec_part_5',\n 'lec_part_6',\n 'lec_part_7',\n 'lec_part_cut',\n 'lec_type_of_0',\n 'lec_type_of_1',\n 'lec_type_of_2',\n 'lec_tags_order_count_0',\n 'lec_tags_order_count_1',\n 'lec_tags_order_count_2',\n 'lec_whole_tags_order_count',\n 'content_id_cut_sum',\n 'content_id_cut_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_f_names = [\n 'target_count',\n 'tags_order_sum_0',\n 'tags_order_count_0',\n 'tags_order_sum_1',\n 'tags_order_count_1',\n 'tags_order_sum_2',\n 'tags_order_count_2',\n 'tags_order_sum_3',\n 'tags_order_count_3',\n 'tags_order_sum_4',\n 'tags_order_count_4',\n 'tags_order_sum_5',\n 'tags_order_count_5',\n 'whole_tags_order_sum',\n 'whole_tags_order_count',\n 'part_1_sum',\n 'part_1_count',\n 'part_2_sum',\n 'part_2_count',\n 'part_3_sum',\n 'part_3_count',\n 'part_4_sum',\n 'part_4_count',\n 'part_5_sum',\n 'part_5_count',\n 'part_6_sum',\n 'part_6_count',\n 'part_7_sum',\n 'part_7_count',\n 'part_cut_sum',\n 'part_cut_count',\n 'lec_rolling_count',\n 'rolling_mean_for_prior_question_elapsed_time',\n 'positive_rolling_mean_for_prior_question_elapsed_time',\n 'negative_rolling_mean_for_prior_question_elapsed_time']\nstdize_params = {'target_count': (0, 5.929629),\n 'tags_order_sum_0': (0, 2.034838),\n 'tags_order_count_0': (0, 2.360298),\n 'tags_order_sum_1': (0, 2.352074),\n 'tags_order_count_1': (0, 2.646724),\n 'tags_order_sum_2': (0, 3.368914),\n 'tags_order_count_2': (0, 3.698544),\n 'tags_order_sum_3': (0, 3.515896),\n 'tags_order_count_3': (0, 3.854377),\n 'tags_order_sum_4': (0, 3.355033),\n 'tags_order_count_4': (0, 3.719298),\n 'tags_order_sum_5': (0, 3.871832),\n 'tags_order_count_5': (0, 4.218161),\n 'whole_tags_order_sum': (0, 3.005077),\n 'whole_tags_order_count': (0, 3.356816),\n 'part_1_sum': (0, 2.838132),\n 'part_1_count': (0, 3.105394),\n 'part_2_sum': (0, 3.631354),\n 'part_2_count': (0, 3.947554),\n 'part_3_sum': (0, 2.574938),\n 'part_3_count': (0, 2.8732),\n 'part_4_sum': (0, 2.353698),\n 'part_4_count': (0, 2.740394),\n 'part_5_sum': (0, 4.350539),\n 'part_5_count': (0, 4.86018),\n 'part_6_sum': (0, 2.64874),\n 'part_6_count': (0, 2.971092),\n 'part_7_sum': (0, 1.864806),\n 'part_7_count': (0, 2.166899),\n 'part_cut_sum': (0, 4.154958),\n 'part_cut_count': (0, 4.582286),\n 'lec_rolling_count': (0, 2.10689),\n 'rolling_mean_for_prior_question_elapsed_time': (0, 10.124079),\n 'positive_rolling_mean_for_prior_question_elapsed_time': (0, 10.075675),\n 'negative_rolling_mean_for_prior_question_elapsed_time': (0, 10.211306)}\n\nargs.f_names = f_names\nargs.log_f_names = log_f_names\nargs.stdize_f_names = stdize_f_names\nargs.stdize_params = stdize_params\n\nargs.log_indices = np.where(np.in1d(args.f_names, args.log_f_names))[0]\nargs.stdize_indices = [np.where(el == np.array(args.f_names))[0][0] for el in args.stdize_f_names]\nargs.stdize_mu_std = np.array([args.stdize_params[el] for el in args.stdize_f_names]).T\ndel f_names, log_f_names, stdize_f_names, stdize_params; gc.collect();\nargs_exp222 = deepcopy(args)\ndel args;",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "#load data\nmy_encoder_exp162 = MyEncoderExp162(args_exp162)\nmy_encoder_exp162.load_state_dict(torch.load(prefix + '/models/my_encoder_exp162_best.pth'))\nmy_encoder_exp162.to(args_exp162.device)\nmy_encoder_exp162.eval();\n\nmy_encoder_exp166 = MyEncoderExp166(args_exp166)\nmy_encoder_exp166.load_state_dict(torch.load(prefix + '/models/my_encoder_exp166_best.pth'))\nmy_encoder_exp166.to(args_exp166.device)\nmy_encoder_exp166.eval();\n\nmy_encoder_exp184 = MyEncoderExp184(args_exp184)\nmy_encoder_exp184.load_state_dict(torch.load(prefix + '/models/my_encoder_exp184_best.pth'))\nmy_encoder_exp184.to(args_exp184.device)\nmy_encoder_exp184.eval();\n\nmy_encoder_exp218 = MyEncoderExp218(args_exp218)\nmy_encoder_exp218.load_state_dict(torch.load(prefix + '/models/my_encoder_exp248_best.pth'))\nmy_encoder_exp218.to(args_exp218.device)\nmy_encoder_exp218.eval();\n\nmy_encoder_exp224 = MyEncoderExp224(args_exp224)\nmy_encoder_exp224.load_state_dict(torch.load(prefix + '/models/my_encoder_exp233_best.pth'))\nmy_encoder_exp224.to(args_exp224.device)\nmy_encoder_exp224.eval();\n\nmy_encoder_exp222 = MyEncoderExp222(args_exp222)\nmy_encoder_exp222.load_state_dict(torch.load(prefix + '/models/my_encoder_exp249_best.pth'))\nmy_encoder_exp222.to(args_exp222.device)\nmy_encoder_exp222.eval();\n\nque =  pd.read_csv(prefix + '/data/questions.csv').astype({'question_id': 'int16', 'bundle_id': 'int16', 'correct_answer': 'int8', 'part': 'int8'})\nque_proc = pd.read_pickle(prefix + '/others/que_proc.pkl')\nque_proc_2 = pd.read_pickle(prefix + '/others/que_proc_2.pkl')\nque_onehot = pd.read_pickle(prefix + '/others/que_onehot.pkl')\nque_part_onehot = pd.read_pickle(prefix + '/others/que_part_onehot.pkl')\nlec = pd.read_csv(prefix + '/data/lectures.csv').astype({'lecture_id': 'int16', 'tag': 'int16', 'part': 'int8'})\nlec_map = np.load(prefix + '/others/lec_map.npy')\nlec_proc = pd.read_pickle(prefix + '/others/lec_proc.pkl')\nque_proc_int = np.load(prefix + '/others/que_proc_int.npy')\n\nn_sample = 400\nuser_map_ref = load_pickle(prefix + '/others/user_map_train.npy')\nn_users_ref = np.load(prefix + '/others/n_users_train.npy')\n#nn\nr_ref_g1 = np.load(prefix + '/references/nn_content_id_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g2 = np.load(prefix + '/references/nn_normed_log_timestamp_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g3 = np.load(prefix + '/references/nn_correctness_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g4 = np.load(prefix + '/references/nn_question_had_explanation_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g5 = np.load(prefix + '/references/nn_normed_elapsed_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g6 = np.load(prefix + '/references/nn_normed_modified_timedelta_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g7 = np.load(prefix + '/references/nn_user_answer_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g8 = np.load(prefix + '/references/nn_task_container_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g9 = np.load(prefix + '/references/nn_content_type_id_diff_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_g10 = np.load(prefix + '/references/nn_task_container_id_history' + '_length_' + str(n_sample) + '_train.npy')\nr_ref_delta = np.load(prefix + '/references/timestamp_diff_train.npy')\nr_ref_delta_2 = np.load(prefix + '/references/task_container_id_diff_train.npy')\nr_ref_delta_3 = np.load(prefix + '/references/content_type_id_diff_train.npy')\n#cpu\nr_ref_1 = np.load(prefix + '/references/rolling_mean_sum_count_train.npy')\nr_ref_2 = np.load(prefix + '/references/rolling_mean_sum_count_for_6_tags_and_whole_tag_train.npy')\nr_ref_3 = np.load(prefix + '/references/rolling_mean_sum_count_for_part_train.npy')\nr_ref_4 = np.load(prefix + '/references/lec_rolling_count_train.npy')\nr_ref_5 = np.load(prefix + '/references/lec_part_rolling_count_train.npy')\nr_ref_6 = np.load(prefix + '/references/lec_type_of_rolling_count_train.npy')\nr_ref_7 = np.load(prefix + '/references/lec_tags_rolling_count_train.npy')\nr_ref_8 = load_pickle(prefix + '/references/rolling_mean_sum_count_for_content_id_darkness_train.npy')\nr_ref_12 = np.load(prefix + '/references/norm_rolling_count_and_cut_for_user_answer_train.npy')\nr_ref_13 = np.load(prefix + '/references/rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_14 = np.load(prefix + '/references/rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_15 = np.load(prefix + '/references/rolling_sum_for_prior_question_isnull_train.npy')\nr_ref_16 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_17 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_elapsed_time_train.npy')\nr_ref_18 = np.load(prefix + '/references/positive_rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_19 = np.load(prefix + '/references/negative_rolling_mean_for_prior_question_had_explanation_train.npy')\nr_ref_20 = np.load(prefix + '/references/n_samples_rolling_mean_train.npy')",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "# run\ni = 0\nfor (tes, _) in tqdm(iter_test):\n    user_map_ref, n_users_ref = update_user_map(tes, user_map_ref, n_users_ref)\n    if i != 0:\n        # update\n        r_ref_1 = update_reference_get_rolling_mean_sum_count(r_ref_1, prev_tes, tes, user_map_ref)\n        r_ref_2 = update_reference_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, prev_tes, tes, que_onehot, user_map_ref)\n        r_ref_3 = update_reference_get_rolling_mean_sum_count_for_part(r_ref_3, prev_tes, tes, que_part_onehot, user_map_ref)\n        r_ref_4 = update_reference_get_lec_rolling_count(r_ref_4, prev_tes, tes, user_map_ref)\n        r_ref_5 = update_reference_get_lec_part_rolling_count(r_ref_5, prev_tes, tes, user_map_ref, lec, lec_map)\n        r_ref_6 = update_reference_get_lec_type_of_rolling_count(r_ref_6, prev_tes, tes, user_map_ref, lec_proc, lec_map)\n        r_ref_7 = update_reference_get_lec_tags_rolling_count(r_ref_7, prev_tes, tes, user_map_ref, lec, lec_map)\n        r_ref_8 = update_reference_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, prev_tes, tes, user_map_ref)\n        \n        r_ref_12 = update_reference_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, prev_tes, tes, user_map_ref)\n        r_ref_15 = update_reference_get_rolling_sum_for_prior_question_isnull(r_ref_15, prev_tes, tes, user_map_ref)\n        r_ref_16 = update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, prev_tes, tes, user_map_ref)\n        r_ref_17 = update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, prev_tes, tes, user_map_ref)\n        r_ref_18 = update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, prev_tes, tes, user_map_ref)\n        r_ref_19 = update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, prev_tes, tes, user_map_ref)\n        r_ref_20 = update_reference_get_n_samples_rolling_mean(r_ref_20, prev_tes, tes, user_map_ref)\n\n        r_ref_delta = update_reference_get_timestamp_diff(r_ref_delta, prev_tes, tes, user_map_ref)\n        r_ref_delta_2 = update_reference_get_task_container_id_diff(r_ref_delta_2, prev_tes, tes, user_map_ref)\n        r_ref_delta_3 = update_reference_get_content_type_id_diff(r_ref_delta_3, prev_tes, tes, user_map_ref)\n        r_ref_g1 = nn_update_reference_get_content_id_history(r_ref_g1, prev_tes, tes, user_map_ref)\n        r_ref_g2 = nn_update_reference_get_normed_log_timestamp_history(r_ref_g2, prev_tes, tes, user_map_ref)\n        r_ref_g3 = nn_update_reference_get_correctness_history(r_ref_g3, prev_tes, tes, user_map_ref)\n        r_ref_g4 = nn_update_reference_get_question_had_explanation_history(r_ref_g4, prev_tes, tes, user_map_ref)\n        r_ref_g5 = nn_update_reference_get_normed_elapsed_history(r_ref_g5, prev_tes, tes, user_map_ref)\n        r_ref_g6 = nn_update_reference_get_normed_modified_timedelta_history(r_ref_g6, prev_tes, tes, f_tes_delta, user_map_ref)\n        r_ref_g7 = nn_update_reference_get_user_answer_history(r_ref_g7, prev_tes, tes, user_map_ref)\n        r_ref_g8 = nn_update_reference_get_task_container_id_diff_history(r_ref_g8, prev_tes, tes, f_tes_delta_2, user_map_ref)\n        r_ref_g9 = nn_update_reference_get_content_type_id_diff_history(r_ref_g9, prev_tes, tes, f_tes_delta_3, user_map_ref)\n        r_ref_g10 = nn_update_reference_get_task_container_id_history(r_ref_g10, prev_tes, tes, user_map_ref)\n        \n    # early update\n    r_ref_g4 = nn_early_update_reference_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n    r_ref_g5 = nn_early_update_reference_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n    r_ref_13 = early_update_reference_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n    r_ref_14 = early_update_reference_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n    r_ref_16 = early_update_reference_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n    r_ref_17 = early_update_reference_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n    r_ref_18 = early_update_reference_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n    r_ref_19 = early_update_reference_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n    \n    # online function\n    f_tes_1 = online_get_rolling_mean_sum_count(r_ref_1, tes, user_map_ref)\n    f_tes_2 = online_get_rolling_mean_sum_count_for_6_tags_and_whole_tag(r_ref_2, tes, que_proc, user_map_ref)\n    f_tes_3 = online_get_rolling_mean_sum_count_for_part(r_ref_3, tes, que, user_map_ref)\n    f_tes_4 = online_get_lec_rolling_count(r_ref_4, tes, user_map_ref)\n    f_tes_5 = online_get_lec_part_rolling_count(r_ref_5, tes, user_map_ref, que)\n    f_tes_6 = online_get_lec_type_of_rolling_count(r_ref_6, tes, user_map_ref)\n    f_tes_7 = online_get_lec_tags_rolling_count(r_ref_7, tes, user_map_ref, que_proc)\n    f_tes_8 = online_get_rolling_mean_sum_count_for_content_id_darkness(r_ref_8, tes, user_map_ref)\n    \n    f_tes_12 = online_get_norm_rolling_count_and_cut_for_user_answer(r_ref_12, tes, que, user_map_ref)\n    f_tes_13 = online_get_rolling_mean_for_prior_question_elapsed_time(r_ref_13, tes, user_map_ref)\n    f_tes_14 = online_get_rolling_mean_for_prior_question_had_explanation(r_ref_14, tes, user_map_ref)\n    f_tes_15 = online_get_rolling_sum_for_prior_question_isnull(r_ref_15, tes, user_map_ref)\n    f_tes_16 = online_get_positive_rolling_mean_for_prior_question_elapsed_time(r_ref_16, tes, user_map_ref)\n    f_tes_17 = online_get_negative_rolling_mean_for_prior_question_elapsed_time(r_ref_17, tes, user_map_ref)\n    f_tes_18 = online_get_positive_rolling_mean_for_prior_question_had_explanation(r_ref_18, tes, user_map_ref)\n    f_tes_19 = online_get_negative_rolling_mean_for_prior_question_had_explanation(r_ref_19, tes, user_map_ref)\n    f_tes_20 = online_get_n_samples_rolling_mean(r_ref_20, tes, user_map_ref)\n    \n    f_tes_delta = online_get_modified_timedelta(r_ref_delta, tes, user_map_ref)\n    f_tes_delta_2 = online_get_task_container_id_diff(r_ref_delta_2, tes, user_map_ref)\n    f_tes_delta_3 = online_get_content_type_id_diff(r_ref_delta_3, tes, user_map_ref)\n    \n    f_tes_g1 = nn_online_get_content_id_history(r_ref_g1, tes, user_map_ref)\n    f_tes_g2 = nn_online_get_normed_log_timestamp_history(r_ref_g2, tes, user_map_ref)\n    f_tes_g3 = nn_online_get_correctness_history(r_ref_g3, tes, user_map_ref)\n    f_tes_g4 = nn_online_get_question_had_explanation_history(r_ref_g4, tes, user_map_ref)\n    f_tes_g5 = nn_online_get_normed_elapsed_history(r_ref_g5, tes, user_map_ref)\n    f_tes_g6 = nn_online_get_normed_modified_timedelta_history(r_ref_g6, tes, f_tes_delta, user_map_ref)\n    f_tes_g7 = nn_online_get_user_answer_history(r_ref_g7, tes, user_map_ref)\n    f_tes_g8 = nn_online_get_task_container_id_diff_history(r_ref_g8, tes, f_tes_delta_2, user_map_ref)\n    f_tes_g9 = nn_online_get_content_type_id_diff_history(r_ref_g9, tes, f_tes_delta_3, user_map_ref)\n    f_tes_g10 = nn_online_get_task_container_id_history(r_ref_g10, tes, user_map_ref)\n    \n\n    # make a prediction \n    concated = pd.concat([f_tes_1, f_tes_2, f_tes_3, f_tes_4, f_tes_5, f_tes_6, f_tes_7, \n                         f_tes_8, f_tes_12, f_tes_13, f_tes_14, f_tes_15, f_tes_16, f_tes_17, f_tes_18, \n                         f_tes_19, f_tes_20], axis = 1)\n    X_tes = concated[args_exp166.f_names].values.astype(np.float32)\n    \n    inputs = {'content_id' : f_tes_g1.values, 'normed_timedelta' : f_tes_g6.values, 'normed_log_timestamp' : f_tes_g2.values,\n         'explanation': f_tes_g4.values, 'correctness': f_tes_g3.values, 'normed_elapsed': f_tes_g5.values, 'user_answer' : f_tes_g7.values,\n             'task_container_id_diff': f_tes_g8.values, 'content_type_id_diff': f_tes_g9.values, 'task_container_id': f_tes_g10.values, \n              'features': X_tes}\n    inputs_200 = {key: inputs[key][:, -201:] for key in inputs.keys()}\n    \n    \n    score_exp162 = my_encoder_exp162.predict_on_batch(inputs_200, args_exp162, args_exp162.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp166 = my_encoder_exp166.predict_on_batch(inputs, args_exp166, args_exp166.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp184 = my_encoder_exp184.predict_on_batch(inputs, args_exp184, args_exp184.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp218 = my_encoder_exp218.predict_on_batch(inputs, args_exp218, args_exp218.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp224 = my_encoder_exp224.predict_on_batch(inputs, args_exp224, args_exp224.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    score_exp222 = my_encoder_exp222.predict_on_batch(inputs, args_exp222, args_exp222.pred_bs, que, que_proc_int) \\\n    if X_tes.shape[0] > 0 else np.array([], dtype = np.float32)\n    \n    preds = [score_exp162, score_exp166, score_exp184, score_exp218, score_exp222, score_exp224]\n    weights = np.array([0.42, 0.25, 0.49, 0.85, 0.84, 0.82])\n    weights[[3, 4, 5]] += 0.1\n    weights = weights/weights.sum()\n    score = 0\n    for pred, weight in zip(preds, weights):\n        score += pred * weight\n    \n    spc_tes = tes[tes['content_type_id'] == 0].copy()\n    spc_tes['answered_correctly'] = score.astype(np.float64)\n    env.predict(spc_tes[['row_id', 'answered_correctly']])\n    \n    # save previous test\n    prev_tes = tes.copy()\n    i += 1",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "",
+   "execution_count": null,
+   "outputs": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
\ No newline at end of file
Index: dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## **Project 3**\\n\\n**Conversations Toxicity Detection**\\n\\nJigsaw Unintended Bias in Toxicity Classification\\n\\nDetect toxicity across a diverse range of conversations\\n\\nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data#\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# References\\n\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"* https://www.kaggle.com/nz0722/lstm-fast-ai-simple-tuned \\n* https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda \\n* https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing\\n* https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\\n\\nThe 3 main contributions of the kernels kernel are the following:\\n\\n- loading embedding from pickles \\n- aimed preprocessing for GloVe, Fasttext, WikiNews, GoogleNews vectors\\n- fixing some unknown words by trying their lower/ uppercase versions\\n\\nTo copy list of in-vocabulary and oov symbols and run a publlic kernel as a benchmark\\n\\n* The neural network architecture is taken from the best scoring public kernel at the time of writing: [Simple LSTM with Identity Parameters - Fast AI](https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai).\"},{\"metadata\":{\"trusted\":true,\"_kg_hide-input\":true,\"_kg_hide-output\":true},\"cell_type\":\"code\",\"source\":\"# Put these at the top of every notebook, to get automatic reloading and inline plotting\\n\\n%reload_ext autoreload\\n%autoreload 2\\n%matplotlib inline\\n\\nimport fastai\\nfrom fastai.train import Learner\\nfrom fastai.train import DataBunch\\nfrom fastai.callbacks import *\\nfrom fastai.basic_data import DatasetType\\nimport fastprogress\\nfrom fastprogress import force_console_behavior\\nimport numpy as np\\nfrom pprint import pprint\\nimport pandas as pd\\nimport os\\nimport time\\n\\nimport gc\\nimport random\\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\\nfrom keras.preprocessing import text, sequence\\nimport torch\\nfrom torch import nn\\nfrom torch.utils import data\\nfrom torch.nn import functional as F\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"tqdm.pandas()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true,\"_kg_hide-input\":true,\"_kg_hide-output\":true},\"cell_type\":\"code\",\"source\":\"# disable progress bars when submitting\\n\\ndef is_interactive():\\n   return 'SHLVL' not in os.environ\\n\\nif not is_interactive():\\n    def nop(it, *a, **k):\\n        return it\\n\\n    tqdm = nop\\n\\n    fastprogress.fastprogress.NO_BAR = True\\n    master_bar, progress_bar = force_console_behavior()\\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_kg_hide-input\":true,\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def seed_everything(seed=123):\\n    random.seed(seed)\\n    os.environ['PYTHONHASHSEED'] = str(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\nseed_everything()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"** To replace the pretrained embedding files with their pickle corresponds. Loading a pickled version (most other public kernels)**\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\\n#GOOGLE_EMBEDDING_PATH = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\\n#WIKI_EMBEDDING_PATH = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"* To adjust the load_embeddings function (Pickled dict).\"},{\"metadata\":{\"trusted\":true,\"_kg_hide-input\":false,\"_kg_hide-output\":false},\"cell_type\":\"code\",\"source\":\"NUM_MODELS = 2\\nLSTM_UNITS = 128\\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\\nMAX_LEN = 220\\n\\ndef get_coefs(word, *arr):\\n    return word, np.asarray(arr, dtype='float32')\\n\\n\\ndef load_embeddings(path):\\n    with open(path,'rb') as f:\\n        emb_arr = pickle.load(f)\\n    return emb_arr\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"* To try a \\\"lower/upper case version of a\\\" word if an embedding is not found, which sometimes gives us an embedding\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def build_matrix(word_index, path):\\n    embedding_index = load_embeddings(path)\\n    embedding_matrix = np.zeros((max_features + 1, 300))\\n    unknown_words = []\\n    \\n    for word, i in word_index.items():\\n        if i <= max_features:\\n            try:\\n                embedding_matrix[i] = embedding_index[word]\\n            except KeyError:\\n                try:\\n                    embedding_matrix[i] = embedding_index[word.lower()]\\n                except KeyError:\\n                    try:\\n                        embedding_matrix[i] = embedding_index[word.title()]\\n                    except KeyError:\\n                        unknown_words.append(word)\\n    return embedding_matrix, unknown_words\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\n\\ndef sigmoid(x):\\n    return 1 / (1 + np.exp(-x))\\n\\nclass SpatialDropout(nn.Dropout2d):\\n    def forward(self, x):\\n        x = x.unsqueeze(2)    # (N, T, 1, K)\\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\\n        x = x.squeeze(2)  # (N, T, K)\\n        return x\\n\\ndef train_model(learn,test,output_dim,lr=0.001,\\n                batch_size=512, n_epochs=5,\\n                enable_checkpoint_ensemble=True):\\n    \\n    all_test_preds = []\\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\\n    n = len(learn.data.train_dl)\\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\\n    sched = GeneralScheduler(learn, phases)\\n    learn.callbacks.append(sched)\\n    for epoch in range(n_epochs):\\n        learn.fit(1)\\n        test_preds = np.zeros((len(test), output_dim))    \\n        for i, x_batch in enumerate(test_loader):\\n            X = x_batch[0].cuda()\\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\\n\\n        all_test_preds.append(test_preds)\\n\\n\\n    if enable_checkpoint_ensemble:\\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \\n    else:\\n        test_preds = all_test_preds[-1]\\n        \\n    return test_preds\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"## Preprocessing\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"See part1 for an explanation how I came to the list of symbols and contraction function. I copied them from that kernel.\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"symbols_to_isolate = '.,?!-;*\\\":()%#$&_/@+=[]^>\\\\\\\\<~{}|`\\\\x96\\\\x92\\\\x80\\\\x91'\\nsymbols_to_delete = '\\\\n\uD83C\uDF55\\\\r\uD83D\uDC35\uD83D\uDE11\\\\xa0\\\\ue014\\\\t\\\\uf818\\\\uf04a\\\\xad\uD83D\uDE22\uD83D\uDC36\\\\uf0e0\uD83D\uDE1C\uD83D\uDE0E\uD83D\uDC4A\\\\u200b\\\\u200e\uD83D\uDE01\uD83D\uDE0D\uD83D\uDC96\uD83D\uDCB5\uD83D\uDC4E\uD83D\uDE00\uD83D\uDE02\\\\u202a\\\\u202c\uD83D\uDD25\uD83D\uDE04\uD83C\uDFFB\uD83D\uDCA5\uD83D\uDE0B\uD83D\uDC4F\uD83D\uDE31\\\\x81\\\\u2009\uD83D\uDE8C\uD83C\uDF1F\uD83D\uDE0A\uD83D\uDE33\uD83D\uDE27\uD83D\uDE40\uD83D\uDE10\uD83D\uDE15\\\\u200f\uD83D\uDC4D\uD83D\uDE2E\uD83D\uDE03\uD83D\uDE18\uD83D\uDCA9\uD83D\uDCAF\uD83D\uDE84\uD83C\uDFFC\uD83D\uDE16\uD83D\uDEB2\uD83D\uDE1F\uD83D\uDE08\uD83D\uDCAA\uD83D\uDE4F\uD83C\uDFAF\uD83C\uDF39\uD83D\uDE07\uD83D\uDC94\uD83D\uDE21\\\\x7f\uD83D\uDC4C\uD83D\uDE44\uD83D\uDE20\\\\ufeff\\\\u2028\uD83D\uDE09\uD83D\uDE24\uD83D\uDE42\\\\u3000\uD83D\uDC6E\uD83D\uDC99\uD83D\uDE0F\uD83C\uDF7E\uD83C\uDF89\uD83D\uDE1E\\\\u2008\uD83C\uDFFE\uD83D\uDE05\uD83D\uDE2D\uD83D\uDC7B\uD83D\uDE25\uD83D\uDE14\uD83D\uDE13\uD83C\uDFFD\uD83C\uDF86\uD83C\uDF7B\uD83C\uDF7D\uD83C\uDFB6\uD83C\uDF3A\uD83E\uDD14\uD83D\uDE2A\\\\x08\uD83D\uDC30\uD83D\uDC07\uD83D\uDC31\uD83D\uDE46\uD83D\uDE28\uD83D\uDE43\uD83D\uDC95\uD835\uDE0A\uD835\uDE26\uD835\uDE33\uD835\uDE22\uD835\uDE35\uD835\uDE30\uD835\uDE24\uD835\uDE3A\uD835\uDE34\uD835\uDE2A\uD835\uDE27\uD835\uDE2E\uD835\uDE23\uD83D\uDC97\uD83D\uDC9A\uD83D\uDC3E\uD83D\uDC15\uD83D\uDE06\uD83D\uDD17\uD83D\uDEBD\uD83D\uDE48\uD83D\uDE34\uD83C\uDFFF\uD83E\uDD17\uD83C\uDDFA\uD83C\uDDF8\uD83C\uDFC6\uD83C\uDF83\uD83D\uDE29\\\\u200a\uD83C\uDF20\uD83D\uDC1F\uD83D\uDCAB\uD83D\uDCB0\uD83D\uDC8E\\\\x95\uD83D\uDD90\uD83D\uDE45\uD83C\uDF70\uD83E\uDD10\uD83D\uDC46\uD83D\uDE4C\\\\u2002\uD83D\uDC9B\uD83D\uDE41\uD83D\uDC40\uD83D\uDE4A\uD83D\uDE49\\\\u2004\\\\x13\uD83D\uDEAC\uD83E\uDD13\\\\ue602\uD83D\uDE35\uD83D\uDE12\uD83C\uDD95\uD83D\uDC45\uD83D\uDC65\uD83D\uDC44\uD83D\uDD04\uD83D\uDD24\uD83D\uDC49\uD83D\uDC64\uD83D\uDC76\uD83D\uDC72\uD83D\uDD1B\uD83C\uDF93\\\\uf0b7\\\\uf04c\\\\x9f\\\\x10\uD83D\uDE23\uD83D\uDE0C\uD83E\uDD11\uD83C\uDF0F\uD83D\uDE2F\uD83D\uDE32\uD83D\uDC9E\uD83D\uDE93\uD83D\uDD14\uD83D\uDCDA\uD83C\uDFC0\uD83D\uDC50\\\\u202d\uD83D\uDCA4\uD83C\uDF47\\\\ue613\uD83C\uDFE1\\\\u202f\uD83D\uDC60\uD83C\uDDF9\uD83C\uDDFC\uD83C\uDF38\uD83C\uDF1E\uD83C\uDFB2\uD83D\uDE1B\uD83D\uDC8B\uD83D\uDC80\uD83C\uDF84\uD83D\uDC9C\uD83E\uDD22\\\\x9c\\\\x9d\uD83D\uDDD1\\\\u2005\uD83D\uDC83\uD83D\uDCE3\uD83D\uDC7F\uD83D\uDE30\uD83E\uDD23\uD83D\uDC1D\uD83C\uDF85\\\\x85\uD83C\uDF7A\uD83C\uDFB5\uD83C\uDF0E\uD83E\uDD21\uD83E\uDD25\uD83D\uDE2C\uD83E\uDD27\\\\u2003\uD83D\uDE80\uD83E\uDD34\uD83D\uDE1D\uD83D\uDD91\uD83D\uDCA8\uD83C\uDFC8\uD83D\uDE3A\uD83C\uDF0D\uD83C\uDF54\uD83D\uDC2E\uD83C\uDF41\uD83C\uDF46\uD83C\uDF51\uD83C\uDF2E\uD83C\uDF2F\uD83E\uDD26\\\\u200d\uD835\uDCD2\uD835\uDCF2\uD835\uDCFF\uD835\uDCF5\uD83C\uDF40\uD83D\uDE2B\uD83E\uDD24\uD83C\uDFBC\uD83D\uDD7A\uD83C\uDF78\uD83E\uDD42\uD83D\uDDFD\uD83C\uDF87\uD83C\uDF8A\uD83C\uDD98\uD83E\uDD20\uD83D\uDC69\uD83D\uDD92\uD83D\uDEAA\\\\u2006\uD83C\uDDEB\uD83C\uDDF7\uD83C\uDDE9\uD83C\uDDEA\uD83C\uDDEE\uD83C\uDDEC\uD83C\uDDE7\uD83D\uDE37\uD83C\uDDE8\uD83C\uDDE6\uD83C\uDF10\\\\x1f\uD835\uDDEA\uD835\uDDF5\uD835\uDDF2\uD835\uDDFB\uD835\uDE06\uD835\uDDFC\uD835\uDE02\uD835\uDDFF\uD835\uDDEE\uD835\uDDF9\uD835\uDDF6\uD835\uDE07\uD835\uDDEF\uD835\uDE01\uD835\uDDF0\uD835\uDE00\uD835\uDE05\uD835\uDDFD\uD835\uDE04\uD835\uDDF1\uD83D\uDCFA\\\\u2000\\\\u2007\\\\u2001\uD835\uDC13\uD835\uDC21\uD835\uDC1E\uD835\uDC2B\uD835\uDC2E\uD835\uDC1D\uD835\uDC1A\uD835\uDC03\uD835\uDC1C\uD835\uDC29\uD835\uDC2D\uD835\uDC22\uD835\uDC28\uD835\uDC27\uD835\uDC2C\uD835\uDC30\uD835\uDC32\uD835\uDC1B\uD835\uDC26\uD835\uDC2F\uD835\uDC11\uD835\uDC19\uD835\uDC23\uD835\uDC07\uD835\uDC02\uD835\uDC18\uD835\uDFCE\uD835\uDC33\uD835\uDC14\uD835\uDC31\uD835\uDFD4\uD835\uDFD3\uD835\uDC05\uD83D\uDC0B\uD83D\uDC98\uD83D\uDC93\uD835\uDE25\uD835\uDE2F\uD835\uDE36\uD83D\uDC90\uD83C\uDF0B\uD83C\uDF04\uD83C\uDF05\uD835\uDE6C\uD835\uDE56\uD835\uDE68\uD835\uDE64\uD835\uDE63\uD835\uDE61\uD835\uDE6E\uD835\uDE58\uD835\uDE60\uD835\uDE5A\uD835\uDE59\uD835\uDE5C\uD835\uDE67\uD835\uDE65\uD835\uDE69\uD835\uDE6A\uD835\uDE57\uD835\uDE5E\uD835\uDE5D\uD835\uDE5B\uD83D\uDC7A\uD83D\uDC37\uD835\uDC00\uD835\uDC25\uD835\uDC2A\uD83D\uDEB6\uD835\uDE62\uD83E\uDD18\uD83D\uDCB8\uD835\uDE47\uD83D\uDC42\uD83D\uDC43\uD83C\uDFAB\\\\uf0a7\uD83D\uDEA2\uD83D\uDE82\uD83C\uDFC3\uD835\uDCEC\uD835\uDCFB\uD835\uDCF4\uD835\uDCEE\uD835\uDCFD\uD835\uDCFC\\\\ue807\uD835\uDC7B\uD835\uDC86\uD835\uDC8D\uD835\uDC95\uD835\uDC89\uD835\uDC93\uD835\uDC96\uD835\uDC82\uD835\uDC8F\uD835\uDC85\uD835\uDC94\uD835\uDC8E\uD835\uDC97\uD835\uDC8A\uD83D\uDC7D\uD83D\uDE19\\\\u200c\uD83C\uDFBE\uD83D\uDC79\uD83C\uDFD2\uD83C\uDFC4\uD83D\uDC00\uD83D\uDE91\uD83E\uDD37\uD835\uDC91\uD835\uDC9A\uD835\uDC90\uD835\uDC74\uD83E\uDD19\uD83D\uDC12\uD835\uDE6B\uD83D\uDC08\uD835\uDC8C\uD835\uDE4A\uD835\uDE6D\uD835\uDE46\uD835\uDE4B\uD835\uDE4D\uD835\uDE3C\uD835\uDE45\uD83E\uDD84\uD83D\uDE97\uD83D\uDC33\uD835\uDFCF\uD835\uDC1F\uD835\uDFD6\uD835\uDFD1\uD835\uDFD5\uD835\uDC84\uD835\uDFD7\uD835\uDC20\uD835\uDE44\uD835\uDE43\uD83D\uDC47\uD835\uDDE2\uD835\uDFF3\uD835\uDFF1\uD835\uDFEC\uD835\uDE3F\uD835\uDE54\uD835\uDCA9\uD835\uDCBE\uD835\uDCC1\uD835\uDCB6\uD835\uDCC9\uD835\uDCC7\uD835\uDCCA\uD835\uDCC3\uD835\uDCC8\uD835\uDCC5\uD835\uDCBB\uD835\uDCBD\uD835\uDCC0\uD835\uDCCC\uD835\uDCB8\uD835\uDCCE\uD835\uDE4F\uD835\uDE5F\uD835\uDE03\uD835\uDDFA\uD835\uDFEE\uD835\uDFED\uD835\uDFEF\uD835\uDFF2\uD83D\uDC4B\uD83E\uDD8A\uD83D\uDC3D\uD83C\uDFBB\uD83C\uDFB9\uD83C\uDFF9\uD83C\uDF77\uD83E\uDD86\uD83C\uDFB8\uD83E\uDD15\uD83E\uDD12\uD83C\uDF81\uD83C\uDFDD\uD83E\uDD81\uD83D\uDE4B\uD83D\uDE36\uD83D\uDD2B\uD83D\uDC41\uD83D\uDCB2\uD83D\uDDEF\uD835\uDE48\uD835\uDC87\uD835\uDC88\uD835\uDC98\uD835\uDC83\uD835\uDC6C\uD835\uDC76\uD835\uDD7E\uD835\uDD99\uD835\uDD97\uD835\uDD86\uD835\uDD8E\uD835\uDD8C\uD835\uDD8D\uD835\uDD95\uD835\uDD8A\uD835\uDD94\uD835\uDD91\uD835\uDD89\uD835\uDD93\uD835\uDD90\uD835\uDD9C\uD835\uDD9E\uD835\uDD9A\uD835\uDD87\uD835\uDD7F\uD835\uDD98\uD835\uDD84\uD835\uDD9B\uD835\uDD92\uD835\uDD8B\uD835\uDD82\uD835\uDD74\uD835\uDD9F\uD835\uDD88\uD835\uDD78\uD83D\uDC51\uD83D\uDEBF\uD83D\uDCA1\\\\uf005\uD835\uDE40\uD835\uDC9B\uD835\uDC72\uD835\uDC73\uD835\uDC7E\uD835\uDC8B\uD835\uDFD2\uD83D\uDE26\uD835\uDE52\uD835\uDE3E\uD835\uDE3D\uD83C\uDFD0\uD835\uDE29\uD835\uDE28\uD835\uDC71\uD835\uDC79\uD835\uDC6B\uD835\uDC75\uD835\uDC6A\uD83C\uDDF0\uD83C\uDDF5\uD83D\uDC7E\uD83D\uDC04\uD83C\uDF88\uD83D\uDD28\uD83D\uDC0E\uD83E\uDD1E\uD83D\uDC38\uD83D\uDC9F\uD83C\uDFB0\uD83C\uDF1D\uD83D\uDEF3\uD83C\uDF6D\uD835\uDC65\uD835\uDC66\uD835\uDC67\uD83D\uDC63\\\\uf020\uD83C\uDFC9\uD83D\uDCAD\uD83C\uDFA5\uD83D\uDC34\uD83D\uDC68\uD83E\uDD33\uD83E\uDD8D\\\\x0b\uD83C\uDF69\uD835\uDC6F\uD835\uDC92\uD83D\uDE17\uD835\uDFD0\uD83C\uDFC2\uD83D\uDC73\uD83C\uDF57\uD83D\uDD49\uD83D\uDC32\uD835\uDC6E\uD835\uDDD5\uD835\uDDF4\uD83C\uDF52\uD83D\uDC11\uD83D\uDC8A\\\\uf203\\\\uf09a\\\\uf222\\\\ue608\\\\uf202\\\\uf099\\\\uf469\\\\ue607\\\\uf410\\\\ue600\uD835\uDC69\uD835\uDC70\uD835\uDC80\uD835\uDC7A\uD83C\uDF24\uD835\uDDF3\uD835\uDDDC\uD835\uDDD9\uD835\uDDE6\uD835\uDDE7\uD83C\uDF4A\uD83C\uDDF3\uD835\uDC99\uD835\uDC81\uD83D\uDD39\uD83E\uDD1A\uD83C\uDF4E\uD835\uDC77\uD83D\uDC02\uD83D\uDC85\uD835\uDE2C\uD835\uDE31\uD835\uDE38\uD835\uDE37\uD835\uDE10\uD835\uDE2D\uD835\uDE13\uD835\uDE16\uD835\uDE39\uD835\uDE32\uD835\uDE2B\uD83D\uDCA2\uD83C\uDDF1\uD835\uDF48\uD83D\uDC92\uD83D\uDEB4\uD83D\uDD95\uD83D\uDDA4\uD83E\uDD58\uD83D\uDCCD\uD83D\uDC48\uD83D\uDEAB\uD83C\uDFA8\uD83C\uDF11\uD83D\uDC3B\uD835\uDC0E\uD835\uDC0D\uD835\uDC0A\uD835\uDC6D\uD83E\uDD16\uD83C\uDF8E\uD83D\uDE3C\uD83D\uDD77\uD835\uDFF0\uD83C\uDDF4\uD83C\uDDED\uD83C\uDDFB\uD83C\uDDF2\uD835\uDDDE\uD835\uDDED\uD835\uDDD8\uD835\uDDE4\uD83D\uDC7C\uD83D\uDCC9\uD83C\uDF5F\uD83C\uDF66\uD83C\uDF08\uD83D\uDD2D\uD83D\uDC0A\uD83D\uDC0D\\\\uf10a\uD83D\uDC26\\\\U0001f92f\\\\U0001f92a\uD83D\uDC21\uD83D\uDCB3\uD83D\uDE47\uD835\uDDF8\uD835\uDDDF\uD835\uDDE0\uD835\uDDF7\uD83E\uDD5C\uD83D\uDD3C'\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"    punct = \\\"/-'?!.,#$%\\\\'()*+-/:;<=>@[\\\\\\\\]^_`{|}~`\\\" + '\\\"\\\"' + '\\\\&'\\n    small_caps_mapping = { \\n    \\\"\\\": \\\"a\\\", \\\"\\\": \\\"b\\\", \\\"\\\": \\\"c\\\", \\\"\\\": \\\"d\\\", \\\"\\\": \\\"e\\\", \\\"\\\": \\\"f\\\", \\\"\\\": \\\"g\\\", \\\"\\\": \\\"h\\\", \\\"\\\": \\\"i\\\", \\n    \\\"\\\": \\\"j\\\", \\\"\\\": \\\"k\\\", \\\"\\\": \\\"l\\\", \\\"\\\": \\\"m\\\", \\\"\\\": \\\"n\\\", \\\"\\\": \\\"o\\\", \\\"\\\": \\\"p\\\", \\\"\\\": \\\"q\\\", \\\"\\\": \\\"r\\\", \\n    \\\"s\\\": \\\"s\\\", \\\"\\\": \\\"t\\\", \\\"\\\": \\\"u\\\", \\\"\\\": \\\"v\\\", \\\"\\\": \\\"w\\\", \\\"x\\\": \\\"x\\\", \\\"\\\": \\\"y\\\", \\\"\\\": \\\"z\\\"}\\n    contraction_mapping = {\\n    \\\"ain't\\\": \\\"is not\\\", \\\"aren't\\\": \\\"are not\\\",\\\"can't\\\": \\\"cannot\\\", \\\"'cause\\\": \\\"because\\\", \\\"could've\\\": \\\"could have\\\", \\\"couldn't\\\": \\\"could not\\\", \\n    \\\"didn't\\\": \\\"did not\\\",  \\\"doesn't\\\": \\\"does not\\\", \\\"don't\\\": \\\"do not\\\", \\\"hadn't\\\": \\\"had not\\\", \\\"hasn't\\\": \\\"has not\\\", \\\"haven't\\\": \\\"have not\\\", \\n    \\\"he'd\\\": \\\"he would\\\",\\\"he'll\\\": \\\"he will\\\", \\\"he's\\\": \\\"he is\\\", \\\"how'd\\\": \\\"how did\\\", \\\"how'd'y\\\": \\\"how do you\\\", \\\"how'll\\\": \\\"how will\\\", \\\"how's\\\": \\\"how is\\\",  \\n    \\\"I'd\\\": \\\"I would\\\", \\\"I'd've\\\": \\\"I would have\\\", \\\"I'll\\\": \\\"I will\\\", \\\"I'll've\\\": \\\"I will have\\\",\\\"I'm\\\": \\\"I am\\\", \\\"I've\\\": \\\"I have\\\", \\\"i'd\\\": \\\"i would\\\", \\\"i'd've\\\": \\n    \\\"i would have\\\", \\\"i'll\\\": \\\"i will\\\",  \\\"i'll've\\\": \\\"i will have\\\",\\\"i'm\\\": \\\"i am\\\", \\\"i've\\\": \\\"i have\\\", \\\"isn't\\\": \\\"is not\\\", \\\"it'd\\\": \\\"it would\\\", \\n    \\\"it'd've\\\": \\\"it would have\\\", \\\"it'll\\\": \\\"it will\\\", \\\"it'll've\\\": \\\"it will have\\\",\\\"it's\\\": \\\"it is\\\", \\\"let's\\\": \\\"let us\\\", \\\"ma'am\\\": \\\"madam\\\", \\n    \\\"mayn't\\\": \\\"may not\\\", \\\"might've\\\": \\\"might have\\\",\\\"mightn't\\\": \\\"might not\\\",\\\"mightn't've\\\": \\\"might not have\\\", \\\"must've\\\": \\\"must have\\\", \\n    \\\"mustn't\\\": \\\"must not\\\", \\\"mustn't've\\\": \\\"must not have\\\", \\\"needn't\\\": \\\"need not\\\", \\\"needn't've\\\": \\\"need not have\\\",\\n    \\\"o'clock\\\": \\\"of the clock\\\", \\\"oughtn't\\\": \\\"ought not\\\", \\\"oughtn't've\\\": \\\"ought not have\\\", \\\"shan't\\\": \\\"shall not\\\", \\n    \\\"sha'n't\\\": \\\"shall not\\\", \\\"shan't've\\\": \\\"shall not have\\\", \\\"she'd\\\": \\\"she would\\\", \\\"she'd've\\\": \\\"she would have\\\", \\n    \\\"she'll\\\": \\\"she will\\\", \\\"she'll've\\\": \\\"she will have\\\", \\\"she's\\\": \\\"she is\\\", \\\"should've\\\": \\\"should have\\\", \\\"shouldn't\\\": \\\"should not\\\", \\n    \\\"shouldn't've\\\": \\\"should not have\\\", \\\"so've\\\": \\\"so have\\\",\\\"so's\\\": \\\"so as\\\", \\\"this's\\\":\\\"this is\\\",\\\"that'd\\\": \\\"that would\\\", \\n    \\\"that'd've\\\": \\\"that would have\\\", \\\"that's\\\": \\\"that is\\\", \\\"there'd\\\": \\\"there would\\\", \\\"there'd've\\\": \\\"there would have\\\", \\\"there's\\\": \\\"there is\\\", \\n    \\\"here's\\\": \\\"here is\\\",\\\"they'd\\\": \\\"they would\\\", \\\"they'd've\\\": \\\"they would have\\\", \\\"they'll\\\": \\\"they will\\\", \\\"they'll've\\\": \\\"they will have\\\", \\n    \\\"they're\\\": \\\"they are\\\", \\\"they've\\\": \\\"they have\\\", \\\"to've\\\": \\\"to have\\\", \\\"wasn't\\\": \\\"was not\\\", \\\"we'd\\\": \\\"we would\\\", \\\"we'd've\\\": \\\"we would have\\\", \\n    \\\"we'll\\\": \\\"we will\\\", \\\"we'll've\\\": \\\"we will have\\\", \\\"we're\\\": \\\"we are\\\", \\\"we've\\\": \\\"we have\\\", \\\"weren't\\\": \\\"were not\\\", \\\"what'll\\\": \\\"what will\\\", \\n    \\\"what'll've\\\": \\\"what will have\\\", \\\"what're\\\": \\\"what are\\\",  \\\"what's\\\": \\\"what is\\\", \\\"what've\\\": \\\"what have\\\", \\\"when's\\\": \\\"when is\\\", \\n    \\\"when've\\\": \\\"when have\\\", \\\"where'd\\\": \\\"where did\\\", \\\"where's\\\": \\\"where is\\\", \\\"where've\\\": \\\"where have\\\", \\\"who'll\\\": \\\"who will\\\", \\\"who'll've\\\": \\\"who will have\\\",\\n    \\\"who's\\\": \\\"who is\\\", \\\"who've\\\": \\\"who have\\\", \\\"why's\\\": \\\"why is\\\", \\\"why've\\\": \\\"why have\\\", \\\"will've\\\": \\\"will have\\\", \\\"won't\\\": \\\"will not\\\", \\n    \\\"won't've\\\": \\\"will not have\\\", \\\"would've\\\": \\\"would have\\\", \\\"wouldn't\\\": \\\"would not\\\", \\\"wouldn't've\\\": \\\"would not have\\\", \\n    \\\"y'all\\\": \\\"you all\\\", \\\"y'all'd\\\": \\\"you all would\\\",\\\"y'all'd've\\\": \\\"you all would have\\\",\\\"y'all're\\\": \\\"you all are\\\",\\\"y'all've\\\": \\\"you all have\\\",\\n    \\\"you'd\\\": \\\"you would\\\", \\\"you'd've\\\": \\\"you would have\\\", \\\"you'll\\\": \\\"you will\\\", \\\"you'll've\\\": \\\"you will have\\\", \\\"you're\\\": \\\"you are\\\", \\\"you've\\\": \\\"you have\\\",\\n    \\\"trump's\\\": \\\"trump is\\\", \\\"obama's\\\": \\\"obama is\\\", \\\"canada's\\\": \\\"canada is\\\", \\\"today's\\\": \\\"today is\\\"}\\n    specail_signs = { \\\"\\\": \\\"...\\\", \\\"\\\": \\\"2\\\"}\\n    specials = [\\\"\\\", \\\"\\\", \\\"\\\", \\\"`\\\"]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from nltk.tokenize.treebank import TreebankWordTokenizer\\ntokenizer = TreebankWordTokenizer()\\n\\n\\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\\n\\n\\ndef handle_punctuation(x):\\n    x = x.translate(remove_dict)\\n    x = x.translate(isolate_dict)\\n    return x\\n\\ndef handle_contractions(x):\\n    x = tokenizer.tokenize(x)\\n    return x\\n\\ndef fix_quote(x):\\n    x = [x_[1:] if x_.startswith(\\\"'\\\") else x_ for x_ in x]\\n    x = ' '.join(x)\\n    return x\\n\\ndef preprocess(x):\\n    x = handle_punctuation(x)\\n    x = handle_contractions(x)\\n    x = fix_quote(x)\\n    return x\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"* To apply that preprocess function\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train['comment_text'].head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = train['comment_text'].progress_apply(lambda x:preprocess(x))\\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\\nx_test = test['comment_text'].progress_apply(lambda x:preprocess(x))\\n\\nidentity_columns = [\\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\\n\\n# Overall\\nweights = np.ones((len(x_train),)) / 4\\n\\n# Subgroup\\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\\n\\n# Background Positive, Subgroup Negative\\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\\n\\n# Background Negative, Subgroup Positive\\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\\nloss_weight = 1.0 / weights.mean()\\n\\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\\n\\nmax_features = 410047\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"* To intitialize the keras tokenizer correctly.\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\\n\\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\\n\\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\\nprint('n unknown words (glove): ', len(unknown_words_glove))\\n\\nmax_features = max_features or len(tokenizer.word_index) + 1\\nmax_features\\n\\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\\nembedding_matrix.shape\\n\\ndel crawl_matrix\\ndel glove_matrix\\ngc.collect()\\n\\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Sequence Bucketing\\n\\n*   Use pad_sequences to convert the sequences into 2-D numpy array\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"x_train = tokenizer.texts_to_sequences(x_train)\\nx_test = tokenizer.texts_to_sequences(x_test)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\\n\\n#maxlen = lengths.max() \\nmaxlen = 300\\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\\nx_train_padded.shape\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"class SequenceBucketCollator():\\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\\n        self.choose_length = choose_length\\n        self.sequence_index = sequence_index\\n        self.length_index = length_index\\n        self.label_index = label_index\\n        \\n    def __call__(self, batch):\\n        batch = [torch.stack(x) for x in list(zip(*batch))]\\n        \\n        sequences = batch[self.sequence_index]\\n        lengths = batch[self.length_index]\\n        \\n        length = self.choose_length(lengths)\\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\\n        padded_sequences = sequences[:, mask]\\n        \\n        batch[self.sequence_index] = padded_sequences\\n        \\n        if self.label_index is not None:\\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\\n    \\n        return batch\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"*[...]The `train_model` function is exactly the same. The NN itself is also only slightly different. It also accepts an optional `lengths` parameter because lengths are part of the dataset now.[...]*\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"class NeuralNet(nn.Module):\\n    def __init__(self, embedding_matrix, num_aux_targets):\\n        super(NeuralNet, self).__init__()\\n        embed_size = embedding_matrix.shape[1]\\n        \\n        self.embedding = nn.Embedding(max_features, embed_size)\\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\\n        self.embedding.weight.requires_grad = False\\n        self.embedding_dropout = SpatialDropout(0.3)\\n        \\n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\\n    \\n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\\n        \\n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\\n        \\n    def forward(self, x, lengths=None):\\n        h_embedding = self.embedding(x.long())\\n        h_embedding = self.embedding_dropout(h_embedding)\\n        \\n        h_lstm1, _ = self.lstm1(h_embedding)\\n        h_lstm2, _ = self.lstm2(h_lstm1)\\n        \\n        # global average pooling\\n        avg_pool = torch.mean(h_lstm2, 1)\\n        # global max pooling\\n        max_pool, _ = torch.max(h_lstm2, 1)\\n        \\n        h_conc = torch.cat((max_pool, avg_pool), 1)\\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\\n        \\n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\\n        \\n        result = self.linear_out(hidden)\\n        aux_result = self.linear_aux_out(hidden)\\n        out = torch.cat([result, aux_result], 1)\\n        \\n        return out\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Training\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"* To use sequence bucketing with maximum length.\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"The validation dataset is only added so that the fast.ai DataBunch works as expected and it consists of only 2 samples.\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\\n\\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"batch_size = 512\\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\\n\\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \\n                                        sequence_index=0, \\n                                        length_index=1, \\n                                        label_index=2)\\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\\n\\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\\n\\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def custom_loss(data, targets):\\n    ''' Define custom loss function for weighted BCE on 'target' column '''\\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\\n    return (bce_loss_1 * loss_weight) + bce_loss_2\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"all_test_preds = []\\n\\nfor model_idx in range(NUM_MODELS):\\n    print('Model ', model_idx)\\n    seed_everything(1 + model_idx)\\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\\n    learn = Learner(databunch, model, loss_func=custom_loss)\\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \\n    all_test_preds.append(test_preds)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"**Prediction**\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame.from_dict({\\n    'id': test['id'],\\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\\n})\\n\\nsubmission.to_csv('submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.6.5\"}},\"nbformat\":4,\"nbformat_minor\":1}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb b/dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb
--- a/dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/jigsaw-unintended-bias-in-toxicity-classification/Cristina Sierra/pretext-lstm-tuning-v3.ipynb	(date 1658512097857)
@@ -1,1 +1,337 @@
-{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Project 3**\n\n**Conversations Toxicity Detection**\n\nJigsaw Unintended Bias in Toxicity Classification\n\nDetect toxicity across a diverse range of conversations\n\nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data#"},{"metadata":{},"cell_type":"markdown","source":"# References\n"},{"metadata":{},"cell_type":"markdown","source":"* https://www.kaggle.com/nz0722/lstm-fast-ai-simple-tuned \n* https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda \n* https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing\n* https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n\nThe 3 main contributions of the kernels kernel are the following:\n\n- loading embedding from pickles \n- aimed preprocessing for GloVe, Fasttext, WikiNews, GoogleNews vectors\n- fixing some unknown words by trying their lower/ uppercase versions\n\nTo copy list of in-vocabulary and oov symbols and run a publlic kernel as a benchmark\n\n* The neural network architecture is taken from the best scoring public kernel at the time of writing: [Simple LSTM with Identity Parameters - Fast AI](https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai)."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Put these at the top of every notebook, to get automatic reloading and inline plotting\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\n\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# disable progress bars when submitting\n\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** To replace the pretrained embedding files with their pickle corresponds. Loading a pickled version (most other public kernels)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n#GOOGLE_EMBEDDING_PATH = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n#WIKI_EMBEDDING_PATH = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* To adjust the load_embeddings function (Pickled dict)."},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"NUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* To try a \"lower/upper case version of a\" word if an embedding is not found, which sometimes gives us an embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef train_model(learn,test,output_dim,lr=0.001,\n                batch_size=512, n_epochs=5,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"See part1 for an explanation how I came to the list of symbols and contraction function. I copied them from that kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\":()%#$&_/@+=[]^>\\\\<~{}|`\\x96\\x92\\x80\\x91'\nsymbols_to_delete = '\\n\\r\\xa0\\ue014\\t\\uf818\\uf04a\\xad\\uf0e0\\u200b\\u200e\\u202a\\u202c\\x81\\u2009\\u200f\\x7f\\ufeff\\u2028\\u3000\\u2008\\x08\\u200a\\x95\\u2002\\u2004\\x13\\ue602\\uf0b7\\uf04c\\x9f\\x10\\u202d\\ue613\\u202f\\x9c\\x9d\\u2005\\x85\\u2003\\u200d\\u2006\\x1f\\u2000\\u2007\\u2001\\uf0a7\\ue807\\u200c\\uf005\\uf020\\x0b\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\\uf10a\\U0001f92f\\U0001f92a'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"' + '\\&'\n    small_caps_mapping = { \n    \"\": \"a\", \"\": \"b\", \"\": \"c\", \"\": \"d\", \"\": \"e\", \"\": \"f\", \"\": \"g\", \"\": \"h\", \"\": \"i\", \n    \"\": \"j\", \"\": \"k\", \"\": \"l\", \"\": \"m\", \"\": \"n\", \"\": \"o\", \"\": \"p\", \"\": \"q\", \"\": \"r\", \n    \"s\": \"s\", \"\": \"t\", \"\": \"u\", \"\": \"v\", \"\": \"w\", \"x\": \"x\", \"\": \"y\", \"\": \"z\"}\n    contraction_mapping = {\n    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\":\"this is\",\"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n    \"trump's\": \"trump is\", \"obama's\": \"obama is\", \"canada's\": \"canada is\", \"today's\": \"today is\"}\n    specail_signs = { \"\": \"...\", \"\": \"2\"}\n    specials = [\"\", \"\", \"\", \"`\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* To apply that preprocess function"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train['comment_text'].progress_apply(lambda x:preprocess(x))\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = test['comment_text'].progress_apply(lambda x:preprocess(x))\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Overall\nweights = np.ones((len(x_train),)) / 4\n\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n\nmax_features = 410047","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* To intitialize the keras tokenizer correctly."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()\n\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sequence Bucketing\n\n*   Use pad_sequences to convert the sequences into 2-D numpy array"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n\n#maxlen = lengths.max() \nmaxlen = 300\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\nx_train_padded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*[...]The `train_model` function is exactly the same. The NN itself is also only slightly different. It also accepts an optional `lengths` parameter because lengths are part of the dataset now.[...]*"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"* To use sequence bucketing with maximum length."},{"metadata":{},"cell_type":"markdown","source":"The validation dataset is only added so that the fast.ai DataBunch works as expected and it consists of only 2 samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1 + model_idx)\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \n    all_test_preds.append(test_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "## **Project 3**\n\n**Conversations Toxicity Detection**\n\nJigsaw Unintended Bias in Toxicity Classification\n\nDetect toxicity across a diverse range of conversations\n\nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data#"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# References\n"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "* https://www.kaggle.com/nz0722/lstm-fast-ai-simple-tuned \n* https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part1-eda \n* https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing\n* https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n\nThe 3 main contributions of the kernels kernel are the following:\n\n- loading embedding from pickles \n- aimed preprocessing for GloVe, Fasttext, WikiNews, GoogleNews vectors\n- fixing some unknown words by trying their lower/ uppercase versions\n\nTo copy list of in-vocabulary and oov symbols and run a publlic kernel as a benchmark\n\n* The neural network architecture is taken from the best scoring public kernel at the time of writing: [Simple LSTM with Identity Parameters - Fast AI](https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai)."
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_kg_hide-input": true,
+    "_kg_hide-output": true
+   },
+   "cell_type": "code",
+   "source": "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\n\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "tqdm.pandas()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_kg_hide-input": true,
+    "_kg_hide-output": true
+   },
+   "cell_type": "code",
+   "source": "# disable progress bars when submitting\n\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_kg_hide-input": true,
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "** To replace the pretrained embedding files with their pickle corresponds. Loading a pickled version (most other public kernels)**"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "CRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n#GOOGLE_EMBEDDING_PATH = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n#WIKI_EMBEDDING_PATH = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "* To adjust the load_embeddings function (Pickled dict)."
+  },
+  {
+   "metadata": {
+    "trusted": true,
+    "_kg_hide-input": false,
+    "_kg_hide-output": false
+   },
+   "cell_type": "code",
+   "source": "NUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "* To try a \"lower/upper case version of a\" word if an embedding is not found, which sometimes gives us an embedding"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef train_model(learn,test,output_dim,lr=0.001,\n                batch_size=512, n_epochs=5,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "## Preprocessing"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "See part1 for an explanation how I came to the list of symbols and contraction function. I copied them from that kernel."
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "symbols_to_isolate = '.,?!-;*\":()%#$&_/@+=[]^>\\\\<~{}|`\\x96\\x92\\x80\\x91'\nsymbols_to_delete = '\\n\\r\\xa0\\ue014\\t\\uf818\\uf04a\\xad\\uf0e0\\u200b\\u200e\\u202a\\u202c\\x81\\u2009\\u200f\\x7f\\ufeff\\u2028\\u3000\\u2008\\x08\\u200a\\x95\\u2002\\u2004\\x13\\ue602\\uf0b7\\uf04c\\x9f\\x10\\u202d\\ue613\\u202f\\x9c\\x9d\\u2005\\x85\\u2003\\u200d\\u2006\\x1f\\u2000\\u2007\\u2001\\uf0a7\\ue807\\u200c\\uf005\\uf020\\x0b\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600\\uf10a\\U0001f92f\\U0001f92a'",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"' + '\\&'\n    small_caps_mapping = { \n    \"\": \"a\", \"\": \"b\", \"\": \"c\", \"\": \"d\", \"\": \"e\", \"\": \"f\", \"\": \"g\", \"\": \"h\", \"\": \"i\", \n    \"\": \"j\", \"\": \"k\", \"\": \"l\", \"\": \"m\", \"\": \"n\", \"\": \"o\", \"\": \"p\", \"\": \"q\", \"\": \"r\", \n    \"s\": \"s\", \"\": \"t\", \"\": \"u\", \"\": \"v\", \"\": \"w\", \"x\": \"x\", \"\": \"y\", \"\": \"z\"}\n    contraction_mapping = {\n    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\":\"this is\",\"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n    \"trump's\": \"trump is\", \"obama's\": \"obama is\", \"canada's\": \"canada is\", \"today's\": \"today is\"}\n    specail_signs = { \"\": \"...\", \"\": \"2\"}\n    specials = [\"\", \"\", \"\", \"`\"]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "* To apply that preprocess function"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "train['comment_text'].head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "x_train = train['comment_text'].progress_apply(lambda x:preprocess(x))\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = test['comment_text'].progress_apply(lambda x:preprocess(x))\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Overall\nweights = np.ones((len(x_train),)) / 4\n\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n\nmax_features = 410047",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "* To intitialize the keras tokenizer correctly."
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()\n\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Sequence Bucketing\n\n*   Use pad_sequences to convert the sequences into 2-D numpy array"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n\n#maxlen = lengths.max() \nmaxlen = 300\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\nx_train_padded.shape",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "class SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "*[...]The `train_model` function is exactly the same. The NN itself is also only slightly different. It also accepts an optional `lengths` parameter because lengths are part of the dataset now.[...]*"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Training"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "* To use sequence bucketing with maximum length."
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "The validation dataset is only added so that the fast.ai DataBunch works as expected and it consists of only 2 samples."
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "batch_size = 512\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "all_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1 + model_idx)\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \n    all_test_preds.append(test_preds)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "**Prediction**"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)",
+   "execution_count": null,
+   "outputs": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.5"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 1
+}
\ No newline at end of file
Index: dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat_minor\":4,\"nbformat\":4,\"cells\":[{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCC8 Linear Regression with Python\\n\\n> Linear Regression is the simplest algorithm in machine learning, it can be trained in different ways. In this notebook we will cover the following linear algorithms:\\n\\n> 1. Linear Regression\\n> 2. Robust Regression\\n> 3. Ridge Regression\\n> 4. LASSO Regression\\n> 5. Elastic Net\\n> 6. Polynomial Regression\\n> 7. Stochastic Gradient Descent\\n> 8. Artificial Neaural Networks\\n\\n# \uD83D\uDCBE Data\\n\\n> We are going to use the `USA_Housing` dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns:\\n\\n> * '`Avg. Area Income`': Avg. Income of residents of the city house is located in.\\n> * '`Avg. Area House Age`': Avg Age of Houses in same city\\n> * '`Avg. Area Number of Rooms`': Avg Number of Rooms for Houses in same city\\n> * '`Avg. Area Number of Bedrooms`': Avg Number of Bedrooms for Houses in same city\\n> * '`Area Population`': Population of city hou  se is located in\\n> * '`Price`': Price that the house sold at\\n> * '`Address`': Address for the house\\n\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"!pip install -q hvplot\",\"metadata\":{\"_kg_hide-input\":true,\"_kg_hide-output\":true,\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:01:48.686849Z\",\"iopub.execute_input\":\"2022-01-20T10:01:48.687211Z\",\"iopub.status.idle\":\"2022-01-20T10:01:59.039054Z\",\"shell.execute_reply.started\":\"2022-01-20T10:01:48.687114Z\",\"shell.execute_reply\":\"2022-01-20T10:01:59.037915Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCE4 Import Libraries\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"import pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport hvplot.pandas\\n%matplotlib inline\\n\\n# sns.set_style(\\\"whitegrid\\\")\\n# plt.style.use(\\\"fivethirtyeight\\\")\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:01:59.040948Z\",\"iopub.execute_input\":\"2022-01-20T10:01:59.041248Z\",\"iopub.status.idle\":\"2022-01-20T10:02:02.743258Z\",\"shell.execute_reply.started\":\"2022-01-20T10:01:59.041215Z\",\"shell.execute_reply\":\"2022-01-20T10:02:02.74207Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"## \uD83D\uDCBE Check out the Data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"USAhousing = pd.read_csv('/kaggle/input/usa-housing/USA_Housing.csv')\\nUSAhousing.head()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:02.745459Z\",\"iopub.execute_input\":\"2022-01-20T10:02:02.746612Z\",\"iopub.status.idle\":\"2022-01-20T10:02:02.827068Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:02.746557Z\",\"shell.execute_reply\":\"2022-01-20T10:02:02.826095Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.info()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:02.828805Z\",\"iopub.execute_input\":\"2022-01-20T10:02:02.829617Z\",\"iopub.status.idle\":\"2022-01-20T10:02:02.862197Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:02.829569Z\",\"shell.execute_reply\":\"2022-01-20T10:02:02.861117Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.describe()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:02.863436Z\",\"iopub.execute_input\":\"2022-01-20T10:02:02.863774Z\",\"iopub.status.idle\":\"2022-01-20T10:02:02.894943Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:02.863738Z\",\"shell.execute_reply\":\"2022-01-20T10:02:02.8941Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.columns\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:02.89635Z\",\"iopub.execute_input\":\"2022-01-20T10:02:02.896596Z\",\"iopub.status.idle\":\"2022-01-20T10:02:02.90308Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:02.896566Z\",\"shell.execute_reply\":\"2022-01-20T10:02:02.902126Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCCA Exploratory Data Analysis (EDA)\\n\\nLet's create some simple plots to check out the data!\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"sns.pairplot(USAhousing)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:02.904495Z\",\"iopub.execute_input\":\"2022-01-20T10:02:02.90522Z\",\"iopub.status.idle\":\"2022-01-20T10:02:10.873068Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:02.905174Z\",\"shell.execute_reply\":\"2022-01-20T10:02:10.872373Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.hvplot.hist(by='Price', subplots=False, width=1000)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:10.874016Z\",\"iopub.execute_input\":\"2022-01-20T10:02:10.8746Z\",\"iopub.status.idle\":\"2022-01-20T10:02:11.930125Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:10.874564Z\",\"shell.execute_reply\":\"2022-01-20T10:02:11.929139Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.hvplot.hist(\\\"Price\\\")\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:11.931341Z\",\"iopub.execute_input\":\"2022-01-20T10:02:11.931771Z\",\"iopub.status.idle\":\"2022-01-20T10:02:12.06231Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:11.93173Z\",\"shell.execute_reply\":\"2022-01-20T10:02:12.061421Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.hvplot.scatter(x='Avg. Area House Age', y='Price')\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:12.065405Z\",\"iopub.execute_input\":\"2022-01-20T10:02:12.065648Z\",\"iopub.status.idle\":\"2022-01-20T10:02:12.20984Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:12.06562Z\",\"shell.execute_reply\":\"2022-01-20T10:02:12.208989Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.hvplot.scatter(x='Avg. Area Income', y='Price')\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:12.211165Z\",\"iopub.execute_input\":\"2022-01-20T10:02:12.21173Z\",\"iopub.status.idle\":\"2022-01-20T10:02:12.354449Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:12.211684Z\",\"shell.execute_reply\":\"2022-01-20T10:02:12.353403Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"USAhousing.columns\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:12.355659Z\",\"iopub.execute_input\":\"2022-01-20T10:02:12.355891Z\",\"iopub.status.idle\":\"2022-01-20T10:02:12.362827Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:12.355862Z\",\"shell.execute_reply\":\"2022-01-20T10:02:12.361963Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"sns.heatmap(USAhousing.corr(), annot=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:12.364224Z\",\"iopub.execute_input\":\"2022-01-20T10:02:12.364508Z\",\"iopub.status.idle\":\"2022-01-20T10:02:12.883733Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:12.364476Z\",\"shell.execute_reply\":\"2022-01-20T10:02:12.882791Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCC8 Training a Linear Regression Model\\n\\n> Let's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can't use.\\n\\n## X and y arrays\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\\n               'Avg. Area Number of Bedrooms', 'Area Population']]\\ny = USAhousing['Price']\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:12.885341Z\",\"iopub.execute_input\":\"2022-01-20T10:02:12.88589Z\",\"iopub.status.idle\":\"2022-01-20T10:02:12.894408Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:12.885836Z\",\"shell.execute_reply\":\"2022-01-20T10:02:12.893751Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"## \uD83E\uDDF1 Train Test Split\\n\\nNow let's split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model.\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:12.896203Z\",\"iopub.execute_input\":\"2022-01-20T10:02:12.89647Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.091764Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:12.896439Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.091067Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"from sklearn import metrics\\nfrom sklearn.model_selection import cross_val_score\\n\\ndef cross_val(model):\\n    pred = cross_val_score(model, X, y, cv=10)\\n    return pred.mean()\\n\\ndef print_evaluate(true, predicted):  \\n    mae = metrics.mean_absolute_error(true, predicted)\\n    mse = metrics.mean_squared_error(true, predicted)\\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\\n    r2_square = metrics.r2_score(true, predicted)\\n    print('MAE:', mae)\\n    print('MSE:', mse)\\n    print('RMSE:', rmse)\\n    print('R2 Square', r2_square)\\n    print('__________________________________')\\n    \\ndef evaluate(true, predicted):\\n    mae = metrics.mean_absolute_error(true, predicted)\\n    mse = metrics.mean_squared_error(true, predicted)\\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\\n    r2_square = metrics.r2_score(true, predicted)\\n    return mae, mse, rmse, r2_square\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.092924Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.093333Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.101509Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.093298Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.0998Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCE6 Preparing Data For Linear Regression\\n> Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\\n\\n> As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\\n\\n> Try different preparations of your data using these heuristics and see what works best for your problem.\\n- **Linear Assumption.** Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\\n- **Remove Noise.** Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\\n- **Remove Collinearity.** Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\\n- **Gaussian Distributions.** Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\\n- **Rescale Inputs:** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\n\\npipeline = Pipeline([\\n    ('std_scalar', StandardScaler())\\n])\\n\\nX_train = pipeline.fit_transform(X_train)\\nX_test = pipeline.transform(X_test)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.103139Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.103455Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.124129Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.103424Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.123006Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Linear Regression\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.linear_model import LinearRegression\\n\\nlin_reg = LinearRegression(normalize=True)\\nlin_reg.fit(X_train,y_train)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.126141Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.126553Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.231298Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.12651Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.230606Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"##  Model Evaluation\\n\\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"# print the intercept\\nprint(lin_reg.intercept_)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.232437Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.232802Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.237602Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.23276Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.236787Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\\ncoeff_df\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.238751Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.239517Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.256704Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.239477Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.256Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"> Interpreting the coefficients:\\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Income** is associated with an **increase of \\\\$21.52**.\\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area House Age** is associated with an **increase of \\\\$164883.28**.\\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Rooms** is associated with an **increase of \\\\$122368.67**.\\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Bedrooms** is associated with an **increase of \\\\$2233.80**.\\n>- Holding all other features fixed, a 1 unit increase in **Area Population** is associated with an **increase of \\\\$15.15**.\\n\\nDoes this make sense? Probably not because I made up this data.\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"##  Predictions from our Model\\n\\nLet's grab predictions off our test set and see how well it did!\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"pred = lin_reg.predict(X_test)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.258091Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.258585Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.266565Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.258539Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.265657Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.267909Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.268177Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.410303Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.268139Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.409488Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"**Residual Histogram**\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"pd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.411772Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.41227Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.555261Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.412221Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.554282Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"##  Regression Evaluation Metrics\\n\\n\\nHere are three common evaluation metrics for regression problems:\\n\\n> - **Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\\n$$\\\\frac 1n\\\\sum_{i=1}^n|y_i-\\\\hat{y}_i|$$\\n\\n> - **Mean Squared Error** (MSE) is the mean of the squared errors:\\n$$\\\\frac 1n\\\\sum_{i=1}^n(y_i-\\\\hat{y}_i)^2$$\\n\\n> - **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\\n$$\\\\sqrt{\\\\frac 1n\\\\sum_{i=1}^n(y_i-\\\\hat{y}_i)^2}$$\\n\\n> \uD83D\uDCCC Comparing these metrics:\\n- **MAE** is the easiest to understand, because it's the average error.\\n- **MSE** is more popular than MAE, because MSE \\\"punishes\\\" larger errors, which tends to be useful in the real world.\\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \\\"y\\\" units.\\n\\n> All of these are **loss functions**, because we want to minimize them.\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"test_pred = lin_reg.predict(X_test)\\ntrain_pred = lin_reg.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df = pd.DataFrame(data=[[\\\"Linear Regression\\\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \\n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \\\"Cross Validation\\\"])\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.556613Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.556824Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.662093Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.556797Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.661137Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Robust Regression\\n\\n> Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.\\n\\n> One instance in which robust estimation should be considered is when there is a strong suspicion of `heteroscedasticity`.\\n\\n> A common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.)\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"## Random Sample Consensus - RANSAC\\n\\n> Random sample consensus (`RANSAC`) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\\n\\n> A basic assumption is that the data consists of \\\"inliers\\\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \\\"outliers\\\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.linear_model import RANSACRegressor\\n\\nmodel = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\\nmodel.fit(X_train, y_train)\\n\\ntest_pred = model.predict(X_test)\\ntrain_pred = model.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('====================================')\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Robust Regression\\\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \\\"Cross Validation\\\"])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.663754Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.664286Z\",\"iopub.status.idle\":\"2022-01-20T10:02:13.93429Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.664236Z\",\"shell.execute_reply\":\"2022-01-20T10:02:13.933311Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Ridge Regression\\n\\n> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\\n\\n> Ridge regression addresses some of the problems of **Ordinary Least Squares** by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\\n\\n$$\\\\min_{w}\\\\big|\\\\big|Xw-y\\\\big|\\\\big|^2_2+\\\\alpha\\\\big|\\\\big|w\\\\big|\\\\big|^2_2$$\\n\\n> $\\\\alpha>=0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\\n\\n> Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function.\\n***\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.linear_model import Ridge\\n\\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\\nmodel.fit(X_train, y_train)\\npred = model.predict(X_test)\\n\\ntest_pred = model.predict(X_test)\\ntrain_pred = model.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('====================================')\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Ridge Regression\\\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \\\"Cross Validation\\\"])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:13.93581Z\",\"iopub.execute_input\":\"2022-01-20T10:02:13.936287Z\",\"iopub.status.idle\":\"2022-01-20T10:02:14.052286Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:13.936245Z\",\"shell.execute_reply\":\"2022-01-20T10:02:14.05137Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  LASSO Regression\\n\\n> A linear model that estimates sparse coefficients.\\n\\n> Mathematically, it consists of a linear model trained with $\\\\ell_1$ prior as regularizer. The objective function to minimize is:\\n\\n$$\\\\min_{w}\\\\frac{1}{2n_{samples}} \\\\big|\\\\big|Xw - y\\\\big|\\\\big|_2^2 + \\\\alpha \\\\big|\\\\big|w\\\\big|\\\\big|_1$$\\n\\n> The lasso estimate thus solves the minimization of the least-squares penalty with $\\\\alpha \\\\big|\\\\big|w\\\\big|\\\\big|_1$ added, where $\\\\alpha$ is a constant and $\\\\big|\\\\big|w\\\\big|\\\\big|_1$ is the $\\\\ell_1-norm$ of the parameter vector.\\n***\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.linear_model import Lasso\\n\\nmodel = Lasso(alpha=0.1, \\n              precompute=True, \\n#               warm_start=True, \\n              positive=True, \\n              selection='random',\\n              random_state=42)\\nmodel.fit(X_train, y_train)\\n\\ntest_pred = model.predict(X_test)\\ntrain_pred = model.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('====================================')\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Lasso Regression\\\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \\\"Cross Validation\\\"])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:14.053817Z\",\"iopub.execute_input\":\"2022-01-20T10:02:14.054316Z\",\"iopub.status.idle\":\"2022-01-20T10:02:14.168175Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:14.054269Z\",\"shell.execute_reply\":\"2022-01-20T10:02:14.167128Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Elastic Net\\n\\n> A linear regression model trained with L1 and L2 prior as regularizer. \\n> This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \\n> Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\\n> A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridges stability under rotation.\\n> The objective function to minimize is in this case\\n\\n$$\\\\min_{w}{\\\\frac{1}{2n_{samples}} \\\\big|\\\\big|X w - y\\\\big|\\\\big|_2 ^ 2 + \\\\alpha \\\\rho \\\\big|\\\\big|w\\\\big|\\\\big|_1 +\\n\\\\frac{\\\\alpha(1-\\\\rho)}{2} \\\\big|\\\\big|w\\\\big|\\\\big|_2 ^ 2}$$\\n***\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.linear_model import ElasticNet\\n\\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\\nmodel.fit(X_train, y_train)\\n\\ntest_pred = model.predict(X_test)\\ntrain_pred = model.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('====================================')\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Elastic Net Regression\\\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \\\"Cross Validation\\\"])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:14.172976Z\",\"iopub.execute_input\":\"2022-01-20T10:02:14.173543Z\",\"iopub.status.idle\":\"2022-01-20T10:02:14.291318Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:14.173498Z\",\"shell.execute_reply\":\"2022-01-20T10:02:14.290298Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Polynomial Regression\\n> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\\n\\n***\\n\\n> One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\\n\\n> For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\\n\\n$$\\\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\\n\\n> If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\\n\\n$$\\\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\\n\\n> The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\\n\\n$$z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\\n\\n> With this re-labeling of the data, our problem can be written\\n\\n$$\\\\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\\n\\n> We see that the resulting polynomial regression is in the same class of linear models wed considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\\n***\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.preprocessing import PolynomialFeatures\\n\\npoly_reg = PolynomialFeatures(degree=2)\\n\\nX_train_2_d = poly_reg.fit_transform(X_train)\\nX_test_2_d = poly_reg.transform(X_test)\\n\\nlin_reg = LinearRegression(normalize=True)\\nlin_reg.fit(X_train_2_d,y_train)\\n\\ntest_pred = lin_reg.predict(X_test_2_d)\\ntrain_pred = lin_reg.predict(X_train_2_d)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('====================================')\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Polynomail Regression\\\", *evaluate(y_test, test_pred), 0]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:14.292812Z\",\"iopub.execute_input\":\"2022-01-20T10:02:14.293292Z\",\"iopub.status.idle\":\"2022-01-20T10:02:14.325563Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:14.293249Z\",\"shell.execute_reply\":\"2022-01-20T10:02:14.324694Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Stochastic Gradient Descent\\n\\n> Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Sescent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum.\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.linear_model import SGDRegressor\\n\\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\\nsgd_reg.fit(X_train, y_train)\\n\\ntest_pred = sgd_reg.predict(X_test)\\ntrain_pred = sgd_reg.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\nprint('====================================')\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Stochastic Gradient Descent\\\", *evaluate(y_test, test_pred), 0]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:14.327033Z\",\"iopub.execute_input\":\"2022-01-20T10:02:14.327511Z\",\"iopub.status.idle\":\"2022-01-20T10:02:27.33603Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:14.327468Z\",\"shell.execute_reply\":\"2022-01-20T10:02:27.335175Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Artficial Neural Network\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\\nfrom tensorflow.keras.optimizers import Adam\\n\\nX_train = np.array(X_train)\\nX_test = np.array(X_test)\\ny_train = np.array(y_train)\\ny_test = np.array(y_test)\\n\\nmodel = Sequential()\\n\\nmodel.add(Dense(X_train.shape[1], activation='relu'))\\nmodel.add(Dense(32, activation='relu'))\\n# model.add(Dropout(0.2))\\n\\nmodel.add(Dense(64, activation='relu'))\\n# model.add(Dropout(0.2))\\n\\nmodel.add(Dense(128, activation='relu'))\\n# model.add(Dropout(0.2))\\n\\nmodel.add(Dense(512, activation='relu'))\\nmodel.add(Dropout(0.1))\\nmodel.add(Dense(1))\\n\\nmodel.compile(optimizer=Adam(0.00001), loss='mse')\\n\\nr = model.fit(X_train, y_train,\\n              validation_data=(X_test,y_test),\\n              batch_size=1,\\n              epochs=100)\",\"metadata\":{\"_kg_hide-output\":true,\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:02:27.337623Z\",\"iopub.execute_input\":\"2022-01-20T10:02:27.33812Z\",\"iopub.status.idle\":\"2022-01-20T10:21:14.497418Z\",\"shell.execute_reply.started\":\"2022-01-20T10:02:27.338074Z\",\"shell.execute_reply\":\"2022-01-20T10:21:14.496386Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:14.500344Z\",\"iopub.execute_input\":\"2022-01-20T10:21:14.501558Z\",\"iopub.status.idle\":\"2022-01-20T10:21:14.660432Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:14.501498Z\",\"shell.execute_reply\":\"2022-01-20T10:21:14.65934Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"pd.DataFrame(r.history)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:14.661843Z\",\"iopub.execute_input\":\"2022-01-20T10:21:14.662096Z\",\"iopub.status.idle\":\"2022-01-20T10:21:14.678344Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:14.662066Z\",\"shell.execute_reply\":\"2022-01-20T10:21:14.677355Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"pd.DataFrame(r.history).hvplot.line(y=['loss', 'val_loss'])\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:14.680226Z\",\"iopub.execute_input\":\"2022-01-20T10:21:14.680557Z\",\"iopub.status.idle\":\"2022-01-20T10:21:14.91962Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:14.680513Z\",\"shell.execute_reply\":\"2022-01-20T10:21:14.918767Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"test_pred = model.predict(X_test)\\ntrain_pred = model.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\n\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Artficial Neural Network\\\", *evaluate(y_test, test_pred), 0]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:14.920978Z\",\"iopub.execute_input\":\"2022-01-20T10:21:14.921525Z\",\"iopub.status.idle\":\"2022-01-20T10:21:15.426213Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:14.921479Z\",\"shell.execute_reply\":\"2022-01-20T10:21:15.425231Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Random Forest Regressor\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.ensemble import RandomForestRegressor\\n\\nrf_reg = RandomForestRegressor(n_estimators=1000)\\nrf_reg.fit(X_train, y_train)\\n\\ntest_pred = rf_reg.predict(X_test)\\ntrain_pred = rf_reg.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\n\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"Random Forest Regressor\\\", *evaluate(y_test, test_pred), 0]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:15.427517Z\",\"iopub.execute_input\":\"2022-01-20T10:21:15.427749Z\",\"iopub.status.idle\":\"2022-01-20T10:21:30.690845Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:15.427719Z\",\"shell.execute_reply\":\"2022-01-20T10:21:30.689855Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#  Support Vector Machine\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"from sklearn.svm import SVR\\n\\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\\nsvm_reg.fit(X_train, y_train)\\n\\ntest_pred = svm_reg.predict(X_test)\\ntrain_pred = svm_reg.predict(X_train)\\n\\nprint('Test set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_test, test_pred)\\n\\nprint('Train set evaluation:\\\\n_____________________________________')\\nprint_evaluate(y_train, train_pred)\\n\\nresults_df_2 = pd.DataFrame(data=[[\\\"SVM Regressor\\\", *evaluate(y_test, test_pred), 0]], \\n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\\nresults_df = results_df.append(results_df_2, ignore_index=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:30.692475Z\",\"iopub.execute_input\":\"2022-01-20T10:21:30.692713Z\",\"iopub.status.idle\":\"2022-01-20T10:21:34.343878Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:30.692687Z\",\"shell.execute_reply\":\"2022-01-20T10:21:34.342979Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"results_df\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:34.345289Z\",\"iopub.execute_input\":\"2022-01-20T10:21:34.345564Z\",\"iopub.status.idle\":\"2022-01-20T10:21:34.359601Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:34.345532Z\",\"shell.execute_reply\":\"2022-01-20T10:21:34.358572Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCCA Models Comparison\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"results_df.set_index('Model', inplace=True)\\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2022-01-20T10:21:34.361101Z\",\"iopub.execute_input\":\"2022-01-20T10:21:34.361407Z\",\"iopub.status.idle\":\"2022-01-20T10:21:34.655177Z\",\"shell.execute_reply.started\":\"2022-01-20T10:21:34.361368Z\",\"shell.execute_reply\":\"2022-01-20T10:21:34.65426Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# \uD83D\uDCDD Summary\\nIn this notebook you discovered the linear regression algorithm for machine learning.\\n\\nYou covered a lot of ground including:\\n> - The common linear regression models (Ridge, Lasso, ElasticNet, ...).\\n> - The representation used by the model.\\n> - Learning algorithms used to estimate the coefficients in the model.\\n> - Rules of thumb to consider when preparing data for use with linear regression.\\n> - How to evaluate a linear regression model.\\n\\n\\n# \uD83D\uDD17 References:\\n- [Scikit-learn library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\\n- [Linear Regression for Machine Learning by Jason Brownlee PhD](https://machinelearningmastery.com/linear-regression-for-machine-learning/)\",\"metadata\":{}}]}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb b/dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb
--- a/dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/house-prices-advanced-regression-techniques/Fares Sayah/practical-introduction-to-10-regression-algorithm.ipynb	(date 1658512097865)
@@ -1,1 +1,777 @@
-{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Linear Regression with Python\n\n> Linear Regression is the simplest algorithm in machine learning, it can be trained in different ways. In this notebook we will cover the following linear algorithms:\n\n> 1. Linear Regression\n> 2. Robust Regression\n> 3. Ridge Regression\n> 4. LASSO Regression\n> 5. Elastic Net\n> 6. Polynomial Regression\n> 7. Stochastic Gradient Descent\n> 8. Artificial Neaural Networks\n\n#  Data\n\n> We are going to use the `USA_Housing` dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns:\n\n> * '`Avg. Area Income`': Avg. Income of residents of the city house is located in.\n> * '`Avg. Area House Age`': Avg Age of Houses in same city\n> * '`Avg. Area Number of Rooms`': Avg Number of Rooms for Houses in same city\n> * '`Avg. Area Number of Bedrooms`': Avg Number of Bedrooms for Houses in same city\n> * '`Area Population`': Population of city hou  se is located in\n> * '`Price`': Price that the house sold at\n> * '`Address`': Address for the house\n","metadata":{}},{"cell_type":"code","source":"!pip install -q hvplot","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-20T10:01:48.686849Z","iopub.execute_input":"2022-01-20T10:01:48.687211Z","iopub.status.idle":"2022-01-20T10:01:59.039054Z","shell.execute_reply.started":"2022-01-20T10:01:48.687114Z","shell.execute_reply":"2022-01-20T10:01:59.037915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n%matplotlib inline\n\n# sns.set_style(\"whitegrid\")\n# plt.style.use(\"fivethirtyeight\")","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:01:59.040948Z","iopub.execute_input":"2022-01-20T10:01:59.041248Z","iopub.status.idle":"2022-01-20T10:02:02.743258Z","shell.execute_reply.started":"2022-01-20T10:01:59.041215Z","shell.execute_reply":"2022-01-20T10:02:02.74207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Check out the Data","metadata":{}},{"cell_type":"code","source":"USAhousing = pd.read_csv('/kaggle/input/usa-housing/USA_Housing.csv')\nUSAhousing.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:02.745459Z","iopub.execute_input":"2022-01-20T10:02:02.746612Z","iopub.status.idle":"2022-01-20T10:02:02.827068Z","shell.execute_reply.started":"2022-01-20T10:02:02.746557Z","shell.execute_reply":"2022-01-20T10:02:02.826095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:02.828805Z","iopub.execute_input":"2022-01-20T10:02:02.829617Z","iopub.status.idle":"2022-01-20T10:02:02.862197Z","shell.execute_reply.started":"2022-01-20T10:02:02.829569Z","shell.execute_reply":"2022-01-20T10:02:02.861117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:02.863436Z","iopub.execute_input":"2022-01-20T10:02:02.863774Z","iopub.status.idle":"2022-01-20T10:02:02.894943Z","shell.execute_reply.started":"2022-01-20T10:02:02.863738Z","shell.execute_reply":"2022-01-20T10:02:02.8941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:02.89635Z","iopub.execute_input":"2022-01-20T10:02:02.896596Z","iopub.status.idle":"2022-01-20T10:02:02.90308Z","shell.execute_reply.started":"2022-01-20T10:02:02.896566Z","shell.execute_reply":"2022-01-20T10:02:02.902126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Exploratory Data Analysis (EDA)\n\nLet's create some simple plots to check out the data!","metadata":{}},{"cell_type":"code","source":"sns.pairplot(USAhousing)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:02.904495Z","iopub.execute_input":"2022-01-20T10:02:02.90522Z","iopub.status.idle":"2022-01-20T10:02:10.873068Z","shell.execute_reply.started":"2022-01-20T10:02:02.905174Z","shell.execute_reply":"2022-01-20T10:02:10.872373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.hvplot.hist(by='Price', subplots=False, width=1000)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:10.874016Z","iopub.execute_input":"2022-01-20T10:02:10.8746Z","iopub.status.idle":"2022-01-20T10:02:11.930125Z","shell.execute_reply.started":"2022-01-20T10:02:10.874564Z","shell.execute_reply":"2022-01-20T10:02:11.929139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.hvplot.hist(\"Price\")","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:11.931341Z","iopub.execute_input":"2022-01-20T10:02:11.931771Z","iopub.status.idle":"2022-01-20T10:02:12.06231Z","shell.execute_reply.started":"2022-01-20T10:02:11.93173Z","shell.execute_reply":"2022-01-20T10:02:12.061421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.hvplot.scatter(x='Avg. Area House Age', y='Price')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:12.065405Z","iopub.execute_input":"2022-01-20T10:02:12.065648Z","iopub.status.idle":"2022-01-20T10:02:12.20984Z","shell.execute_reply.started":"2022-01-20T10:02:12.06562Z","shell.execute_reply":"2022-01-20T10:02:12.208989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.hvplot.scatter(x='Avg. Area Income', y='Price')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:12.211165Z","iopub.execute_input":"2022-01-20T10:02:12.21173Z","iopub.status.idle":"2022-01-20T10:02:12.354449Z","shell.execute_reply.started":"2022-01-20T10:02:12.211684Z","shell.execute_reply":"2022-01-20T10:02:12.353403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USAhousing.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:12.355659Z","iopub.execute_input":"2022-01-20T10:02:12.355891Z","iopub.status.idle":"2022-01-20T10:02:12.362827Z","shell.execute_reply.started":"2022-01-20T10:02:12.355862Z","shell.execute_reply":"2022-01-20T10:02:12.361963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(USAhousing.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:12.364224Z","iopub.execute_input":"2022-01-20T10:02:12.364508Z","iopub.status.idle":"2022-01-20T10:02:12.883733Z","shell.execute_reply.started":"2022-01-20T10:02:12.364476Z","shell.execute_reply":"2022-01-20T10:02:12.882791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Training a Linear Regression Model\n\n> Let's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can't use.\n\n## X and y arrays","metadata":{}},{"cell_type":"code","source":"X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n               'Avg. Area Number of Bedrooms', 'Area Population']]\ny = USAhousing['Price']","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:12.885341Z","iopub.execute_input":"2022-01-20T10:02:12.88589Z","iopub.status.idle":"2022-01-20T10:02:12.894408Z","shell.execute_reply.started":"2022-01-20T10:02:12.885836Z","shell.execute_reply":"2022-01-20T10:02:12.893751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Train Test Split\n\nNow let's split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:12.896203Z","iopub.execute_input":"2022-01-20T10:02:12.89647Z","iopub.status.idle":"2022-01-20T10:02:13.091764Z","shell.execute_reply.started":"2022-01-20T10:02:12.896439Z","shell.execute_reply":"2022-01-20T10:02:13.091067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.092924Z","iopub.execute_input":"2022-01-20T10:02:13.093333Z","iopub.status.idle":"2022-01-20T10:02:13.101509Z","shell.execute_reply.started":"2022-01-20T10:02:13.093298Z","shell.execute_reply":"2022-01-20T10:02:13.0998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Preparing Data For Linear Regression\n> Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\n\n> As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\n\n> Try different preparations of your data using these heuristics and see what works best for your problem.\n- **Linear Assumption.** Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n- **Remove Noise.** Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n- **Remove Collinearity.** Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n- **Gaussian Distributions.** Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n- **Rescale Inputs:** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.103139Z","iopub.execute_input":"2022-01-20T10:02:13.103455Z","iopub.status.idle":"2022-01-20T10:02:13.124129Z","shell.execute_reply.started":"2022-01-20T10:02:13.103424Z","shell.execute_reply":"2022-01-20T10:02:13.123006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.126141Z","iopub.execute_input":"2022-01-20T10:02:13.126553Z","iopub.status.idle":"2022-01-20T10:02:13.231298Z","shell.execute_reply.started":"2022-01-20T10:02:13.12651Z","shell.execute_reply":"2022-01-20T10:02:13.230606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Model Evaluation\n\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.","metadata":{}},{"cell_type":"code","source":"# print the intercept\nprint(lin_reg.intercept_)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.232437Z","iopub.execute_input":"2022-01-20T10:02:13.232802Z","iopub.status.idle":"2022-01-20T10:02:13.237602Z","shell.execute_reply.started":"2022-01-20T10:02:13.23276Z","shell.execute_reply":"2022-01-20T10:02:13.236787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.238751Z","iopub.execute_input":"2022-01-20T10:02:13.239517Z","iopub.status.idle":"2022-01-20T10:02:13.256704Z","shell.execute_reply.started":"2022-01-20T10:02:13.239477Z","shell.execute_reply":"2022-01-20T10:02:13.256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Interpreting the coefficients:\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Income** is associated with an **increase of \\$21.52**.\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area House Age** is associated with an **increase of \\$164883.28**.\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Rooms** is associated with an **increase of \\$122368.67**.\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Bedrooms** is associated with an **increase of \\$2233.80**.\n>- Holding all other features fixed, a 1 unit increase in **Area Population** is associated with an **increase of \\$15.15**.\n\nDoes this make sense? Probably not because I made up this data.","metadata":{}},{"cell_type":"markdown","source":"##  Predictions from our Model\n\nLet's grab predictions off our test set and see how well it did!","metadata":{}},{"cell_type":"code","source":"pred = lin_reg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.258091Z","iopub.execute_input":"2022-01-20T10:02:13.258585Z","iopub.status.idle":"2022-01-20T10:02:13.266565Z","shell.execute_reply.started":"2022-01-20T10:02:13.258539Z","shell.execute_reply":"2022-01-20T10:02:13.265657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.267909Z","iopub.execute_input":"2022-01-20T10:02:13.268177Z","iopub.status.idle":"2022-01-20T10:02:13.410303Z","shell.execute_reply.started":"2022-01-20T10:02:13.268139Z","shell.execute_reply":"2022-01-20T10:02:13.409488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Residual Histogram**","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.411772Z","iopub.execute_input":"2022-01-20T10:02:13.41227Z","iopub.status.idle":"2022-01-20T10:02:13.555261Z","shell.execute_reply.started":"2022-01-20T10:02:13.412221Z","shell.execute_reply":"2022-01-20T10:02:13.554282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Regression Evaluation Metrics\n\n\nHere are three common evaluation metrics for regression problems:\n\n> - **Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n> - **Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n> - **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n>  Comparing these metrics:\n- **MAE** is the easiest to understand, because it's the average error.\n- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n\n> All of these are **loss functions**, because we want to minimize them.","metadata":{}},{"cell_type":"code","source":"test_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.556613Z","iopub.execute_input":"2022-01-20T10:02:13.556824Z","iopub.status.idle":"2022-01-20T10:02:13.662093Z","shell.execute_reply.started":"2022-01-20T10:02:13.556797Z","shell.execute_reply":"2022-01-20T10:02:13.661137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Robust Regression\n\n> Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.\n\n> One instance in which robust estimation should be considered is when there is a strong suspicion of `heteroscedasticity`.\n\n> A common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.)","metadata":{}},{"cell_type":"markdown","source":"## Random Sample Consensus - RANSAC\n\n> Random sample consensus (`RANSAC`) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n\n> A basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.663754Z","iopub.execute_input":"2022-01-20T10:02:13.664286Z","iopub.status.idle":"2022-01-20T10:02:13.93429Z","shell.execute_reply.started":"2022-01-20T10:02:13.664236Z","shell.execute_reply":"2022-01-20T10:02:13.933311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Ridge Regression\n\n> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n\n> Ridge regression addresses some of the problems of **Ordinary Least Squares** by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n\n$$\\min_{w}\\big|\\big|Xw-y\\big|\\big|^2_2+\\alpha\\big|\\big|w\\big|\\big|^2_2$$\n\n> $\\alpha>=0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n\n> Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function.\n***","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:13.93581Z","iopub.execute_input":"2022-01-20T10:02:13.936287Z","iopub.status.idle":"2022-01-20T10:02:14.052286Z","shell.execute_reply.started":"2022-01-20T10:02:13.936245Z","shell.execute_reply":"2022-01-20T10:02:14.05137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  LASSO Regression\n\n> A linear model that estimates sparse coefficients.\n\n> Mathematically, it consists of a linear model trained with $\\ell_1$ prior as regularizer. The objective function to minimize is:\n\n$$\\min_{w}\\frac{1}{2n_{samples}} \\big|\\big|Xw - y\\big|\\big|_2^2 + \\alpha \\big|\\big|w\\big|\\big|_1$$\n\n> The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha \\big|\\big|w\\big|\\big|_1$ added, where $\\alpha$ is a constant and $\\big|\\big|w\\big|\\big|_1$ is the $\\ell_1-norm$ of the parameter vector.\n***","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:14.053817Z","iopub.execute_input":"2022-01-20T10:02:14.054316Z","iopub.status.idle":"2022-01-20T10:02:14.168175Z","shell.execute_reply.started":"2022-01-20T10:02:14.054269Z","shell.execute_reply":"2022-01-20T10:02:14.167128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Elastic Net\n\n> A linear regression model trained with L1 and L2 prior as regularizer. \n> This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \n> Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n> A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridges stability under rotation.\n> The objective function to minimize is in this case\n\n$$\\min_{w}{\\frac{1}{2n_{samples}} \\big|\\big|X w - y\\big|\\big|_2 ^ 2 + \\alpha \\rho \\big|\\big|w\\big|\\big|_1 +\n\\frac{\\alpha(1-\\rho)}{2} \\big|\\big|w\\big|\\big|_2 ^ 2}$$\n***","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:14.172976Z","iopub.execute_input":"2022-01-20T10:02:14.173543Z","iopub.status.idle":"2022-01-20T10:02:14.291318Z","shell.execute_reply.started":"2022-01-20T10:02:14.173498Z","shell.execute_reply":"2022-01-20T10:02:14.290298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Polynomial Regression\n> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\n\n***\n\n> One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n\n> For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n\n$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\n\n> If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n\n$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\n\n> The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\n\n$$z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\n\n> With this re-labeling of the data, our problem can be written\n\n$$\\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\n\n> We see that the resulting polynomial regression is in the same class of linear models wed considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\n***","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:14.292812Z","iopub.execute_input":"2022-01-20T10:02:14.293292Z","iopub.status.idle":"2022-01-20T10:02:14.325563Z","shell.execute_reply.started":"2022-01-20T10:02:14.293249Z","shell.execute_reply":"2022-01-20T10:02:14.324694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Stochastic Gradient Descent\n\n> Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Sescent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:02:14.327033Z","iopub.execute_input":"2022-01-20T10:02:14.327511Z","iopub.status.idle":"2022-01-20T10:02:27.33603Z","shell.execute_reply.started":"2022-01-20T10:02:14.327468Z","shell.execute_reply":"2022-01-20T10:02:27.335175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Artficial Neural Network","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(0.00001), loss='mse')\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test),\n              batch_size=1,\n              epochs=100)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-20T10:02:27.337623Z","iopub.execute_input":"2022-01-20T10:02:27.33812Z","iopub.status.idle":"2022-01-20T10:21:14.497418Z","shell.execute_reply.started":"2022-01-20T10:02:27.338074Z","shell.execute_reply":"2022-01-20T10:21:14.496386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:14.500344Z","iopub.execute_input":"2022-01-20T10:21:14.501558Z","iopub.status.idle":"2022-01-20T10:21:14.660432Z","shell.execute_reply.started":"2022-01-20T10:21:14.501498Z","shell.execute_reply":"2022-01-20T10:21:14.65934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(r.history)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:14.661843Z","iopub.execute_input":"2022-01-20T10:21:14.662096Z","iopub.status.idle":"2022-01-20T10:21:14.678344Z","shell.execute_reply.started":"2022-01-20T10:21:14.662066Z","shell.execute_reply":"2022-01-20T10:21:14.677355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(r.history).hvplot.line(y=['loss', 'val_loss'])","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:14.680226Z","iopub.execute_input":"2022-01-20T10:21:14.680557Z","iopub.status.idle":"2022-01-20T10:21:14.91962Z","shell.execute_reply.started":"2022-01-20T10:21:14.680513Z","shell.execute_reply":"2022-01-20T10:21:14.918767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:14.920978Z","iopub.execute_input":"2022-01-20T10:21:14.921525Z","iopub.status.idle":"2022-01-20T10:21:15.426213Z","shell.execute_reply.started":"2022-01-20T10:21:14.921479Z","shell.execute_reply":"2022-01-20T10:21:15.425231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:15.427517Z","iopub.execute_input":"2022-01-20T10:21:15.427749Z","iopub.status.idle":"2022-01-20T10:21:30.690845Z","shell.execute_reply.started":"2022-01-20T10:21:15.427719Z","shell.execute_reply":"2022-01-20T10:21:30.689855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"SVM Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:30.692475Z","iopub.execute_input":"2022-01-20T10:21:30.692713Z","iopub.status.idle":"2022-01-20T10:21:34.343878Z","shell.execute_reply.started":"2022-01-20T10:21:30.692687Z","shell.execute_reply":"2022-01-20T10:21:34.342979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:34.345289Z","iopub.execute_input":"2022-01-20T10:21:34.345564Z","iopub.status.idle":"2022-01-20T10:21:34.359601Z","shell.execute_reply.started":"2022-01-20T10:21:34.345532Z","shell.execute_reply":"2022-01-20T10:21:34.358572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Models Comparison","metadata":{}},{"cell_type":"code","source":"results_df.set_index('Model', inplace=True)\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))","metadata":{"execution":{"iopub.status.busy":"2022-01-20T10:21:34.361101Z","iopub.execute_input":"2022-01-20T10:21:34.361407Z","iopub.status.idle":"2022-01-20T10:21:34.655177Z","shell.execute_reply.started":"2022-01-20T10:21:34.361368Z","shell.execute_reply":"2022-01-20T10:21:34.65426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Summary\nIn this notebook you discovered the linear regression algorithm for machine learning.\n\nYou covered a lot of ground including:\n> - The common linear regression models (Ridge, Lasso, ElasticNet, ...).\n> - The representation used by the model.\n> - Learning algorithms used to estimate the coefficients in the model.\n> - Rules of thumb to consider when preparing data for use with linear regression.\n> - How to evaluate a linear regression model.\n\n\n#  References:\n- [Scikit-learn library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n- [Linear Regression for Machine Learning by Jason Brownlee PhD](https://machinelearningmastery.com/linear-regression-for-machine-learning/)","metadata":{}}]}
\ No newline at end of file
+{
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat_minor": 4,
+ "nbformat": 4,
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "source": "#  Linear Regression with Python\n\n> Linear Regression is the simplest algorithm in machine learning, it can be trained in different ways. In this notebook we will cover the following linear algorithms:\n\n> 1. Linear Regression\n> 2. Robust Regression\n> 3. Ridge Regression\n> 4. LASSO Regression\n> 5. Elastic Net\n> 6. Polynomial Regression\n> 7. Stochastic Gradient Descent\n> 8. Artificial Neaural Networks\n\n#  Data\n\n> We are going to use the `USA_Housing` dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns:\n\n> * '`Avg. Area Income`': Avg. Income of residents of the city house is located in.\n> * '`Avg. Area House Age`': Avg Age of Houses in same city\n> * '`Avg. Area Number of Rooms`': Avg Number of Rooms for Houses in same city\n> * '`Avg. Area Number of Bedrooms`': Avg Number of Bedrooms for Houses in same city\n> * '`Area Population`': Population of city hou  se is located in\n> * '`Price`': Price that the house sold at\n> * '`Address`': Address for the house\n",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "!pip install -q hvplot",
+   "metadata": {
+    "_kg_hide-input": true,
+    "_kg_hide-output": true,
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:01:48.686849Z",
+     "iopub.execute_input": "2022-01-20T10:01:48.687211Z",
+     "iopub.status.idle": "2022-01-20T10:01:59.039054Z",
+     "shell.execute_reply.started": "2022-01-20T10:01:48.687114Z",
+     "shell.execute_reply": "2022-01-20T10:01:59.037915Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Import Libraries",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n%matplotlib inline\n\n# sns.set_style(\"whitegrid\")\n# plt.style.use(\"fivethirtyeight\")",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:01:59.040948Z",
+     "iopub.execute_input": "2022-01-20T10:01:59.041248Z",
+     "iopub.status.idle": "2022-01-20T10:02:02.743258Z",
+     "shell.execute_reply.started": "2022-01-20T10:01:59.041215Z",
+     "shell.execute_reply": "2022-01-20T10:02:02.74207Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "##  Check out the Data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing = pd.read_csv('/kaggle/input/usa-housing/USA_Housing.csv')\nUSAhousing.head()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:02.745459Z",
+     "iopub.execute_input": "2022-01-20T10:02:02.746612Z",
+     "iopub.status.idle": "2022-01-20T10:02:02.827068Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:02.746557Z",
+     "shell.execute_reply": "2022-01-20T10:02:02.826095Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.info()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:02.828805Z",
+     "iopub.execute_input": "2022-01-20T10:02:02.829617Z",
+     "iopub.status.idle": "2022-01-20T10:02:02.862197Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:02.829569Z",
+     "shell.execute_reply": "2022-01-20T10:02:02.861117Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.describe()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:02.863436Z",
+     "iopub.execute_input": "2022-01-20T10:02:02.863774Z",
+     "iopub.status.idle": "2022-01-20T10:02:02.894943Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:02.863738Z",
+     "shell.execute_reply": "2022-01-20T10:02:02.8941Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.columns",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:02.89635Z",
+     "iopub.execute_input": "2022-01-20T10:02:02.896596Z",
+     "iopub.status.idle": "2022-01-20T10:02:02.90308Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:02.896566Z",
+     "shell.execute_reply": "2022-01-20T10:02:02.902126Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Exploratory Data Analysis (EDA)\n\nLet's create some simple plots to check out the data!",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "sns.pairplot(USAhousing)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:02.904495Z",
+     "iopub.execute_input": "2022-01-20T10:02:02.90522Z",
+     "iopub.status.idle": "2022-01-20T10:02:10.873068Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:02.905174Z",
+     "shell.execute_reply": "2022-01-20T10:02:10.872373Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.hvplot.hist(by='Price', subplots=False, width=1000)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:10.874016Z",
+     "iopub.execute_input": "2022-01-20T10:02:10.8746Z",
+     "iopub.status.idle": "2022-01-20T10:02:11.930125Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:10.874564Z",
+     "shell.execute_reply": "2022-01-20T10:02:11.929139Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.hvplot.hist(\"Price\")",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:11.931341Z",
+     "iopub.execute_input": "2022-01-20T10:02:11.931771Z",
+     "iopub.status.idle": "2022-01-20T10:02:12.06231Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:11.93173Z",
+     "shell.execute_reply": "2022-01-20T10:02:12.061421Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.hvplot.scatter(x='Avg. Area House Age', y='Price')",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:12.065405Z",
+     "iopub.execute_input": "2022-01-20T10:02:12.065648Z",
+     "iopub.status.idle": "2022-01-20T10:02:12.20984Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:12.06562Z",
+     "shell.execute_reply": "2022-01-20T10:02:12.208989Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.hvplot.scatter(x='Avg. Area Income', y='Price')",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:12.211165Z",
+     "iopub.execute_input": "2022-01-20T10:02:12.21173Z",
+     "iopub.status.idle": "2022-01-20T10:02:12.354449Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:12.211684Z",
+     "shell.execute_reply": "2022-01-20T10:02:12.353403Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "USAhousing.columns",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:12.355659Z",
+     "iopub.execute_input": "2022-01-20T10:02:12.355891Z",
+     "iopub.status.idle": "2022-01-20T10:02:12.362827Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:12.355862Z",
+     "shell.execute_reply": "2022-01-20T10:02:12.361963Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "sns.heatmap(USAhousing.corr(), annot=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:12.364224Z",
+     "iopub.execute_input": "2022-01-20T10:02:12.364508Z",
+     "iopub.status.idle": "2022-01-20T10:02:12.883733Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:12.364476Z",
+     "shell.execute_reply": "2022-01-20T10:02:12.882791Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Training a Linear Regression Model\n\n> Let's now begin to train out regression model! We will need to first split up our data into an X array that contains the features to train on, and a y array with the target variable, in this case the Price column. We will toss out the Address column because it only has text info that the linear regression model can't use.\n\n## X and y arrays",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n               'Avg. Area Number of Bedrooms', 'Area Population']]\ny = USAhousing['Price']",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:12.885341Z",
+     "iopub.execute_input": "2022-01-20T10:02:12.88589Z",
+     "iopub.status.idle": "2022-01-20T10:02:12.894408Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:12.885836Z",
+     "shell.execute_reply": "2022-01-20T10:02:12.893751Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "##  Train Test Split\n\nNow let's split the data into a training set and a testing set. We will train out model on the training set and then use the test set to evaluate the model.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:12.896203Z",
+     "iopub.execute_input": "2022-01-20T10:02:12.89647Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.091764Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:12.896439Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.091067Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.092924Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.093333Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.101509Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.093298Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.0998Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Preparing Data For Linear Regression\n> Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\n\n> As such, there is a lot of sophistication when talking about these requirements and expectations which can be intimidating. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\n\n> Try different preparations of your data using these heuristics and see what works best for your problem.\n- **Linear Assumption.** Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n- **Remove Noise.** Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n- **Remove Collinearity.** Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n- **Gaussian Distributions.** Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n- **Rescale Inputs:** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.103139Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.103455Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.124129Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.103424Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.123006Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Linear Regression",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.126141Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.126553Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.231298Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.12651Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.230606Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "##  Model Evaluation\n\nLet's evaluate the model by checking out it's coefficients and how we can interpret them.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "# print the intercept\nprint(lin_reg.intercept_)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.232437Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.232802Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.237602Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.23276Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.236787Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.238751Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.239517Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.256704Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.239477Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.256Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "> Interpreting the coefficients:\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Income** is associated with an **increase of \\$21.52**.\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area House Age** is associated with an **increase of \\$164883.28**.\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Rooms** is associated with an **increase of \\$122368.67**.\n>- Holding all other features fixed, a 1 unit increase in **Avg. Area Number of Bedrooms** is associated with an **increase of \\$2233.80**.\n>- Holding all other features fixed, a 1 unit increase in **Area Population** is associated with an **increase of \\$15.15**.\n\nDoes this make sense? Probably not because I made up this data.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "##  Predictions from our Model\n\nLet's grab predictions off our test set and see how well it did!",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "pred = lin_reg.predict(X_test)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.258091Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.258585Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.266565Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.258539Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.265657Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.267909Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.268177Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.410303Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.268139Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.409488Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "**Residual Histogram**",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "pd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.411772Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.41227Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.555261Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.412221Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.554282Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "##  Regression Evaluation Metrics\n\n\nHere are three common evaluation metrics for regression problems:\n\n> - **Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n> - **Mean Squared Error** (MSE) is the mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n> - **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n>  Comparing these metrics:\n- **MAE** is the easiest to understand, because it's the average error.\n- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n\n> All of these are **loss functions**, because we want to minimize them.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "test_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.556613Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.556824Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.662093Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.556797Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.661137Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Robust Regression\n\n> Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.\n\n> One instance in which robust estimation should be considered is when there is a strong suspicion of `heteroscedasticity`.\n\n> A common situation in which robust estimation is used occurs when the data contain outliers. In the presence of outliers that do not come from the same data-generating process as the rest of the data, least squares estimation is inefficient and can be biased. Because the least squares predictions are dragged towards the outliers, and because the variance of the estimates is artificially inflated, the result is that outliers can be masked. (In many situations, including some areas of geostatistics and medical statistics, it is precisely the outliers that are of interest.)",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "## Random Sample Consensus - RANSAC\n\n> Random sample consensus (`RANSAC`) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n\n> A basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Robust Regression\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.663754Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.664286Z",
+     "iopub.status.idle": "2022-01-20T10:02:13.93429Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.664236Z",
+     "shell.execute_reply": "2022-01-20T10:02:13.933311Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Ridge Regression\n\n> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n\n> Ridge regression addresses some of the problems of **Ordinary Least Squares** by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares,\n\n$$\\min_{w}\\big|\\big|Xw-y\\big|\\big|^2_2+\\alpha\\big|\\big|w\\big|\\big|^2_2$$\n\n> $\\alpha>=0$ is a complexity parameter that controls the amount of shrinkage: the larger the value of $\\alpha$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n\n> Ridge regression is an L2 penalized model. Add the squared sum of the weights to the least-squares cost function.\n***",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Ridge Regression\", *evaluate(y_test, test_pred) , cross_val(Ridge())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:13.93581Z",
+     "iopub.execute_input": "2022-01-20T10:02:13.936287Z",
+     "iopub.status.idle": "2022-01-20T10:02:14.052286Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:13.936245Z",
+     "shell.execute_reply": "2022-01-20T10:02:14.05137Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  LASSO Regression\n\n> A linear model that estimates sparse coefficients.\n\n> Mathematically, it consists of a linear model trained with $\\ell_1$ prior as regularizer. The objective function to minimize is:\n\n$$\\min_{w}\\frac{1}{2n_{samples}} \\big|\\big|Xw - y\\big|\\big|_2^2 + \\alpha \\big|\\big|w\\big|\\big|_1$$\n\n> The lasso estimate thus solves the minimization of the least-squares penalty with $\\alpha \\big|\\big|w\\big|\\big|_1$ added, where $\\alpha$ is a constant and $\\big|\\big|w\\big|\\big|_1$ is the $\\ell_1-norm$ of the parameter vector.\n***",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n#               warm_start=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Lasso Regression\", *evaluate(y_test, test_pred) , cross_val(Lasso())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:14.053817Z",
+     "iopub.execute_input": "2022-01-20T10:02:14.054316Z",
+     "iopub.status.idle": "2022-01-20T10:02:14.168175Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:14.054269Z",
+     "shell.execute_reply": "2022-01-20T10:02:14.167128Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Elastic Net\n\n> A linear regression model trained with L1 and L2 prior as regularizer. \n> This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. \n> Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n> A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridges stability under rotation.\n> The objective function to minimize is in this case\n\n$$\\min_{w}{\\frac{1}{2n_{samples}} \\big|\\big|X w - y\\big|\\big|_2 ^ 2 + \\alpha \\rho \\big|\\big|w\\big|\\big|_1 +\n\\frac{\\alpha(1-\\rho)}{2} \\big|\\big|w\\big|\\big|_2 ^ 2}$$\n***",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Elastic Net Regression\", *evaluate(y_test, test_pred) , cross_val(ElasticNet())]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:14.172976Z",
+     "iopub.execute_input": "2022-01-20T10:02:14.173543Z",
+     "iopub.status.idle": "2022-01-20T10:02:14.291318Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:14.173498Z",
+     "shell.execute_reply": "2022-01-20T10:02:14.290298Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Polynomial Regression\n> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\n\n***\n\n> One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n\n> For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n\n$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\n\n> If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n\n$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\n\n> The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\n\n$$z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\n\n> With this re-labeling of the data, our problem can be written\n\n$$\\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\n\n> We see that the resulting polynomial regression is in the same class of linear models wed considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\n***",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Polynomail Regression\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:14.292812Z",
+     "iopub.execute_input": "2022-01-20T10:02:14.293292Z",
+     "iopub.status.idle": "2022-01-20T10:02:14.325563Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:14.293249Z",
+     "shell.execute_reply": "2022-01-20T10:02:14.324694Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Stochastic Gradient Descent\n\n> Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Sescent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum.",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:14.327033Z",
+     "iopub.execute_input": "2022-01-20T10:02:14.327511Z",
+     "iopub.status.idle": "2022-01-20T10:02:27.33603Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:14.327468Z",
+     "shell.execute_reply": "2022-01-20T10:02:27.335175Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Artficial Neural Network",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(0.00001), loss='mse')\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test),\n              batch_size=1,\n              epochs=100)",
+   "metadata": {
+    "_kg_hide-output": true,
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:02:27.337623Z",
+     "iopub.execute_input": "2022-01-20T10:02:27.33812Z",
+     "iopub.status.idle": "2022-01-20T10:21:14.497418Z",
+     "shell.execute_reply.started": "2022-01-20T10:02:27.338074Z",
+     "shell.execute_reply": "2022-01-20T10:21:14.496386Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "pd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:14.500344Z",
+     "iopub.execute_input": "2022-01-20T10:21:14.501558Z",
+     "iopub.status.idle": "2022-01-20T10:21:14.660432Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:14.501498Z",
+     "shell.execute_reply": "2022-01-20T10:21:14.65934Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "pd.DataFrame(r.history)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:14.661843Z",
+     "iopub.execute_input": "2022-01-20T10:21:14.662096Z",
+     "iopub.status.idle": "2022-01-20T10:21:14.678344Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:14.662066Z",
+     "shell.execute_reply": "2022-01-20T10:21:14.677355Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "pd.DataFrame(r.history).hvplot.line(y=['loss', 'val_loss'])",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:14.680226Z",
+     "iopub.execute_input": "2022-01-20T10:21:14.680557Z",
+     "iopub.status.idle": "2022-01-20T10:21:14.91962Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:14.680513Z",
+     "shell.execute_reply": "2022-01-20T10:21:14.918767Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "test_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Artficial Neural Network\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:14.920978Z",
+     "iopub.execute_input": "2022-01-20T10:21:14.921525Z",
+     "iopub.status.idle": "2022-01-20T10:21:15.426213Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:14.921479Z",
+     "shell.execute_reply": "2022-01-20T10:21:15.425231Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Random Forest Regressor",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"Random Forest Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:15.427517Z",
+     "iopub.execute_input": "2022-01-20T10:21:15.427749Z",
+     "iopub.status.idle": "2022-01-20T10:21:30.690845Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:15.427719Z",
+     "shell.execute_reply": "2022-01-20T10:21:30.689855Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Support Vector Machine",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)\n\nresults_df_2 = pd.DataFrame(data=[[\"SVM Regressor\", *evaluate(y_test, test_pred), 0]], \n                            columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', 'Cross Validation'])\nresults_df = results_df.append(results_df_2, ignore_index=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:30.692475Z",
+     "iopub.execute_input": "2022-01-20T10:21:30.692713Z",
+     "iopub.status.idle": "2022-01-20T10:21:34.343878Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:30.692687Z",
+     "shell.execute_reply": "2022-01-20T10:21:34.342979Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "results_df",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:34.345289Z",
+     "iopub.execute_input": "2022-01-20T10:21:34.345564Z",
+     "iopub.status.idle": "2022-01-20T10:21:34.359601Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:34.345532Z",
+     "shell.execute_reply": "2022-01-20T10:21:34.358572Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Models Comparison",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "results_df.set_index('Model', inplace=True)\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8))",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2022-01-20T10:21:34.361101Z",
+     "iopub.execute_input": "2022-01-20T10:21:34.361407Z",
+     "iopub.status.idle": "2022-01-20T10:21:34.655177Z",
+     "shell.execute_reply.started": "2022-01-20T10:21:34.361368Z",
+     "shell.execute_reply": "2022-01-20T10:21:34.65426Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#  Summary\nIn this notebook you discovered the linear regression algorithm for machine learning.\n\nYou covered a lot of ground including:\n> - The common linear regression models (Ridge, Lasso, ElasticNet, ...).\n> - The representation used by the model.\n> - Learning algorithms used to estimate the coefficients in the model.\n> - Rules of thumb to consider when preparing data for use with linear regression.\n> - How to evaluate a linear regression model.\n\n\n#  References:\n- [Scikit-learn library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n- [Linear Regression for Machine Learning by Jason Brownlee PhD](https://machinelearningmastery.com/linear-regression-for-machine-learning/)",
+   "metadata": {}
+  }
+ ]
+}
\ No newline at end of file
Index: dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"nbformat\":4,\"cells\":[{\"cell_type\":\"markdown\",\"source\":\"<h2 style=\\\"margin-bottom: 18px\\\">Index</h2>\\n\\n* Imbalanced datasets\\n* The metric trap\\n* Confusion matrix\\n* Resampling\\n* Random under-sampling\\n* Random over-sampling\\n* Python imbalanced-learn module\\n* Random under-sampling and over-sampling with imbalanced-learn\\n* Under-sampling: Tomek links\\n* Under-sampling: Cluster Centroids\\n* Over-sampling: SMOTE\\n* Over-sampling followed by under-sampling\\n* Recommended reading\",\"metadata\":{\"_uuid\":\"9fa43627b6ebc212b8d2aebae60ef6ef16fcc76a\",\"_cell_guid\":\"7c102692-5b43-4f97-8394-f36ed52dbb23\"}},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t1\\\" style=\\\"margin-bottom: 18px\\\">Imbalanced datasets</h2>\\n\\nIn this kernel we will know some techniques to handle highly unbalanced datasets, with a focus on resampling. The Porto Seguro's Safe Driver Prediction competition, used in this kernel, is a classic problem of unbalanced classes, since insurance claims can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.\\n\\nLet's see how unbalanced the dataset is:\",\"metadata\":{\"_uuid\":\"1e287038acf96fc6d6825e77ba97b03f0824be0a\",\"_cell_guid\":\"e38ddf38-d027-4eae-95c8-749f8f40db43\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"63eb4f34b9a7d5106da4fe6c1c871bb0025c1ae4\",\"_cell_guid\":\"3b84706a-1eb0-435f-b1ff-3f1732cc3ae4\"},\"cell_type\":\"code\",\"source\":\"import numpy as np\\nimport pandas as pd\\n\\ndf_train = pd.read_csv('../input/train.csv')\\n\\ntarget_count = df_train.target.value_counts()\\nprint('Class 0:', target_count[0])\\nprint('Class 1:', target_count[1])\\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\\n\\ntarget_count.plot(kind='bar', title='Count (target)');\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t2\\\" style=\\\"margin-bottom: 18px\\\">The metric trap</h2>\\n\\nOne of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like <code>accuracy_score</code> can be misleading. In a dataset with highly unbalanced classes, if the classifier always \\\"predicts\\\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\\n\\nLet's do this experiment, using simple cross-validation and no feature engineering:\",\"metadata\":{\"_uuid\":\"8683656d1bfdef6b5c0c623597dcbc4160a0edc1\",\"_cell_guid\":\"c91f9c5d-05d2-478e-9dfb-1cb848bc0fe4\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"6d33cfa94ebb019b7c3065ea5f5dbe99ae52aeb1\",\"_cell_guid\":\"0efbfb8a-76ab-4a86-b093-25271dcfcc06\"},\"cell_type\":\"code\",\"source\":\"from xgboost import XGBClassifier\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score\\n\\n# Remove 'id' and 'target' columns\\nlabels = df_train.columns[2:]\\n\\nX = df_train[labels]\\ny = df_train['target']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\\n\\nmodel = XGBClassifier()\\nmodel.fit(X_train, y_train)\\ny_pred = model.predict(X_test)\\n\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(\\\"Accuracy: %.2f%%\\\" % (accuracy * 100.0))\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"Now let's run the same code, but using only one feature (which should drastically reduce the accuracy of the classifier):\",\"metadata\":{\"_uuid\":\"5b17ee8d3629cc346398e63269205e5b654cef80\",\"_cell_guid\":\"c46b98a7-d500-4911-a7b5-c8fc8fbed069\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"5c7c4e3f364c664f52d04d24f948cc416d2d7e4c\",\"_cell_guid\":\"bd97a34a-8e27-4eb2-a552-fe21f860dd15\"},\"cell_type\":\"code\",\"source\":\"model = XGBClassifier()\\nmodel.fit(X_train[['ps_calc_01']], y_train)\\ny_pred = model.predict(X_test[['ps_calc_01']])\\n\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(\\\"Accuracy: %.2f%%\\\" % (accuracy * 100.0))\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"As we can see, the high accuracy rate was just an illusion. In this way, the choice of the metric used in unbalanced datasets is extremely important. In this competition, the evaluation metric is the Normalized Gini Coefficient, a more robust metric for imbalanced datasets, that ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score.\",\"metadata\":{\"_uuid\":\"d9d8b57f458bdd107ae08b76b0008d80e0674d97\",\"_cell_guid\":\"68d4ffaf-4412-4ff3-a9cd-38bea195da3b\"}},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t3\\\" style=\\\"margin-bottom: 18px\\\">Confusion matrix</h2>\\n\\nAn interesting way to evaluate the results is by means of a confusion matrix, which shows the correct and incorrect predictions for each class. In the first row, the first column indicates how many classes 0 were predicted correctly, and the second column, how many classes 0 were predicted as 1. In the second row, we note that all class 1 entries were erroneously predicted as class 0.\\n\\nTherefore, the higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\",\"metadata\":{\"_uuid\":\"a176bd80315afd0f7c21fd26ab1841867a5eb7d0\",\"_cell_guid\":\"e53e9f0f-8888-4fd9-b0d8-3bc937448162\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"39c3256de9817f64c9ae47de5f1f78531d373015\",\"_cell_guid\":\"394daa2b-700c-45fc-99d7-db08245a7697\"},\"cell_type\":\"code\",\"source\":\"from sklearn.metrics import confusion_matrix\\nfrom matplotlib import pyplot as plt\\n\\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\\nprint('Confusion matrix:\\\\n', conf_mat)\\n\\nlabels = ['Class 0', 'Class 1']\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\\nfig.colorbar(cax)\\nax.set_xticklabels([''] + labels)\\nax.set_yticklabels([''] + labels)\\nplt.xlabel('Predicted')\\nplt.ylabel('Expected')\\nplt.show()\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t4\\\" style=\\\"margin-bottom: 18px\\\">Resampling</h2>\\n\\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).\",\"metadata\":{\"_uuid\":\"875f5ab3b5afcdaf3c7754ce957cb01fd32bf65c\",\"_cell_guid\":\"b27346eb-7bf3-4360-993a-fb91e62bb937\"}},{\"cell_type\":\"markdown\",\"source\":\"![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)\",\"metadata\":{\"_uuid\":\"2232ac0fb192a468486b400846f88913a36957e6\",\"_cell_guid\":\"03d31a16-7b66-4096-88d7-d548db734390\"}},{\"cell_type\":\"markdown\",\"source\":\"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\\n\\nLet's implement a basic example, which uses the <code>DataFrame.sample</code> method to get random samples each class:\",\"metadata\":{\"_uuid\":\"67e203e0919c818e871650ef194fe497df2d39b5\",\"_cell_guid\":\"0df1a3f3-49ff-4ada-80f1-198bbbd79525\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"f959c30be59ad1eccaed33f48a35d99be053b547\",\"_cell_guid\":\"2adbc1c9-8cdf-43d9-9a57-f24f503ba523\"},\"cell_type\":\"code\",\"source\":\"# Class count\\ncount_class_0, count_class_1 = df_train.target.value_counts()\\n\\n# Divide by class\\ndf_class_0 = df_train[df_train['target'] == 0]\\ndf_class_1 = df_train[df_train['target'] == 1]\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t5\\\">Random under-sampling</h2>\",\"metadata\":{\"_uuid\":\"b765be76a182930feb650b01dd4d1de90501bbce\",\"_cell_guid\":\"152ea73a-aa23-4fd2-a3f5-1f55c69041ae\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"2a667d73560fb6408897835809a9d67310b45323\",\"_cell_guid\":\"a0e61bc8-cef1-4828-9c5f-49c1c50c522c\"},\"cell_type\":\"code\",\"source\":\"df_class_0_under = df_class_0.sample(count_class_1)\\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\\n\\nprint('Random under-sampling:')\\nprint(df_test_under.target.value_counts())\\n\\ndf_test_under.target.value_counts().plot(kind='bar', title='Count (target)');\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t6\\\">Random over-sampling</h2>\",\"metadata\":{\"_uuid\":\"17192fe8557463941e3abd4633b58ced0037721b\",\"_cell_guid\":\"be656d47-e529-4533-b975-cf6de072d959\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"431942c08c59f0a6ae629c3c38485bf85d001892\",\"_cell_guid\":\"8ca1ac09-5d61-4cf7-99a4-e299f4955c97\"},\"cell_type\":\"code\",\"source\":\"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\\n\\nprint('Random over-sampling:')\\nprint(df_test_over.target.value_counts())\\n\\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)');\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t7\\\" style=\\\"margin-bottom: 18px\\\">Python imbalanced-learn module</h2>\\n\\nA number of more sophisticated resapling techniques have been proposed in the scientific literature.\\n\\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\\n\\nLet's apply some of these resampling techniques, using the Python library [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/). It is compatible with scikit-learn and is part of scikit-learn-contrib projects.\",\"metadata\":{\"_uuid\":\"9672899d4029b71b72897927ce464d6d7427ce77\",\"_cell_guid\":\"9fd90ddc-f2fc-487d-b177-0c540daf2eff\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"41b482ec89f13dd22e85612404300041a3e71deb\",\"_cell_guid\":\"fdf3f76d-aacb-4ccc-9649-736dce7a237c\"},\"cell_type\":\"code\",\"source\":\"import imblearn\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"For ease of visualization, let's create a small unbalanced sample dataset using the <code>make_classification</code> method:\",\"metadata\":{\"_uuid\":\"ff93d1707c416178c010c125319220b785dca984\",\"_cell_guid\":\"5542beb4-6aee-401a-8bc0-2a522fbbe90b\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"3126327b6f4b4c469701ac7a76590f5e55e17076\",\"_cell_guid\":\"e97abde1-c324-47b5-a33c-3b8b92268b5e\"},\"cell_type\":\"code\",\"source\":\"from sklearn.datasets import make_classification\\n\\nX, y = make_classification(\\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\\n    n_informative=3, n_redundant=1, flip_y=0,\\n    n_features=20, n_clusters_per_class=1,\\n    n_samples=100, random_state=10\\n)\\n\\ndf = pd.DataFrame(X)\\ndf['target'] = y\\ndf.target.value_counts().plot(kind='bar', title='Count (target)');\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"We will also create a 2-dimensional plot function, <code>plot_2d_space</code>, to see the data distribution:\",\"metadata\":{\"_uuid\":\"d348b114ec1594eeefa0a67aab8b54e5b9ee2bdf\",\"_cell_guid\":\"4de57657-3aed-4444-a609-318063391763\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"b2a02a4ff687fb099232a0468917ec8ab737d22d\",\"_cell_guid\":\"8003c84c-fee4-45c7-b490-90a185799760\"},\"cell_type\":\"code\",\"source\":\"def plot_2d_space(X, y, label='Classes'):   \\n    colors = ['#1F77B4', '#FF7F0E']\\n    markers = ['o', 's']\\n    for l, c, m in zip(np.unique(y), colors, markers):\\n        plt.scatter(\\n            X[y==l, 0],\\n            X[y==l, 1],\\n            c=c, label=l, marker=m\\n        )\\n    plt.title(label)\\n    plt.legend(loc='upper right')\\n    plt.show()\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):\",\"metadata\":{\"_uuid\":\"aabc110dd8d1b36345df6aada1c59b864e48e8e6\",\"_cell_guid\":\"2c565bbc-39ed-45a2-8b3d-bc501e2510aa\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"f64eff19d1304d4bd1e92d2c51a8a953cd17d7f9\",\"_cell_guid\":\"22b689c8-b296-4736-9aa6-46f36283f91f\"},\"cell_type\":\"code\",\"source\":\"from sklearn.decomposition import PCA\\n\\npca = PCA(n_components=2)\\nX = pca.fit_transform(X)\\n\\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"#t72\\\">Random under-sampling and over-sampling with imbalanced-learn</h2>\",\"metadata\":{\"_uuid\":\"0d7316b04837aa103003d667f63ecb05d43fc04e\",\"_cell_guid\":\"c3c9a24f-3cd0-4c8d-8a4d-4403c4f1a641\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"8a56a4b118d7cae885e4c6a45fa02b2f066ece78\",\"_cell_guid\":\"28bdbe23-5eeb-4335-8c70-7a00bbac3e03\"},\"cell_type\":\"code\",\"source\":\"from imblearn.under_sampling import RandomUnderSampler\\n\\nrus = RandomUnderSampler(return_indices=True)\\nX_rus, y_rus, id_rus = rus.fit_sample(X, y)\\n\\nprint('Removed indexes:', id_rus)\\n\\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')\",\"execution_count\":null},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"scrolled\":true,\"_uuid\":\"be5b90300f0a25bbe7d1822503e7fc5185a906b2\",\"_cell_guid\":\"c8eaed30-339e-4d23-a0f2-fbd687b217ea\"},\"cell_type\":\"code\",\"source\":\"from imblearn.over_sampling import RandomOverSampler\\n\\nros = RandomOverSampler()\\nX_ros, y_ros = ros.fit_sample(X, y)\\n\\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\\n\\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t8\\\" style=\\\"margin-bottom: 18px\\\">Under-sampling: Tomek links</h2>\\n\\nTomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\",\"metadata\":{\"_uuid\":\"b3f9ac47a157d9096a626360408859b795299c24\",\"_cell_guid\":\"3156134f-539b-48b8-b9d0-64095fe50c1c\"}},{\"cell_type\":\"markdown\",\"source\":\"![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)\",\"metadata\":{\"_uuid\":\"85c01c0e6baa1b984585c1db34c5ab0315cbf8ff\",\"_cell_guid\":\"f131f7e0-007d-406e-9e1c-db352d2d8433\"}},{\"cell_type\":\"markdown\",\"source\":\"In the code below, we'll use <code>ratio='majority'</code> to resample the majority class.\",\"metadata\":{\"_uuid\":\"733a86ccfaaabf701fb2d1f9997c732b19630df3\",\"_cell_guid\":\"45bf057b-369c-4f37-a5ac-7417ad79253d\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"0c93b1e11359f4d1eb873280b61aa0f54ee8bd36\",\"_cell_guid\":\"58f13562-c0c6-4247-bf40-c6dec764ff3a\"},\"cell_type\":\"code\",\"source\":\"from imblearn.under_sampling import TomekLinks\\n\\ntl = TomekLinks(return_indices=True, ratio='majority')\\nX_tl, y_tl, id_tl = tl.fit_sample(X, y)\\n\\nprint('Removed indexes:', id_tl)\\n\\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t9\\\" style=\\\"margin-bottom: 18px\\\">Under-sampling: Cluster Centroids</h2>\\n\\nThis technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\\n\\nIn this example we will pass the <code>{0: 10}</code> dict for the parameter <code>ratio</code>, to preserve 10 elements from the majority class (0), and all minority class (1) .\",\"metadata\":{\"_uuid\":\"b4e75fffe4c91afcd63705aa7bcb16b6fd9f6b1f\",\"_cell_guid\":\"fef831bd-ecec-429c-aef2-5d51d1188820\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"ad088a08f4f9e0b646571950928c7fff47f52f98\",\"_cell_guid\":\"91001b3e-fe5c-4dc1-8501-395e1218cba1\"},\"cell_type\":\"code\",\"source\":\"from imblearn.under_sampling import ClusterCentroids\\n\\ncc = ClusterCentroids(ratio={0: 10})\\nX_cc, y_cc = cc.fit_sample(X, y)\\n\\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t10\\\" style=\\\"margin-bottom: 18px\\\">Over-sampling: SMOTE</h2>\\n\\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\",\"metadata\":{\"_uuid\":\"b3741f5c14acdbd76e25725e2d73df5f2cb0a239\",\"_cell_guid\":\"78adb7b4-a7e1-4d10-9cc2-6bf6477c63df\"}},{\"cell_type\":\"markdown\",\"source\":\" ![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)\",\"metadata\":{\"_uuid\":\"51697e21b7cdb4064dda18aa24e6ecf039b1132b\",\"_cell_guid\":\"5162646f-da82-4877-b6d8-25a8be7f42e9\"}},{\"cell_type\":\"markdown\",\"source\":\"We'll use <code>ratio='minority'</code> to resample the minority class.\",\"metadata\":{\"_uuid\":\"9393851db694c178faf93615bf05addedf5d678b\",\"_cell_guid\":\"9c0a0d78-8427-437e-aa24-ffe2bd12edb2\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"74457c951aabf5b16be1c4282c15d9cb2034f26b\",\"_cell_guid\":\"97e9f84e-0951-4037-b882-57545f9d967a\"},\"cell_type\":\"code\",\"source\":\"from imblearn.over_sampling import SMOTE\\n\\nsmote = SMOTE(ratio='minority')\\nX_sm, y_sm = smote.fit_sample(X, y)\\n\\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t11\\\" style=\\\"margin-bottom: 18px\\\">Over-sampling followed by under-sampling</h2>\\n\\nNow, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:\",\"metadata\":{\"_uuid\":\"d7ebbeb741ad6469cb7ebbced3499acd3c49856a\",\"_cell_guid\":\"20c3cdbb-a7c1-45a5-8dd9-84cff7d9af31\"}},{\"outputs\":[],\"metadata\":{\"collapsed\":true,\"_uuid\":\"b740fbaf8677522d3b3040e2f47e3b954dc56877\",\"_cell_guid\":\"ab732fe1-45c1-4163-b70e-7a2ce2dbe42f\"},\"cell_type\":\"code\",\"source\":\"from imblearn.combine import SMOTETomek\\n\\nsmt = SMOTETomek(ratio='auto')\\nX_smt, y_smt = smt.fit_sample(X, y)\\n\\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')\",\"execution_count\":null},{\"cell_type\":\"markdown\",\"source\":\"<h2 id=\\\"t12\\\" style=\\\"margin-bottom: 18px\\\">Recommended reading</h2>\\n\\nThe imbalanced-learn documentation:<br>\\nhttp://contrib.scikit-learn.org/imbalanced-learn/stable/index.html\\n\\nThe imbalanced-learn GitHub:<br>\\nhttps://github.com/scikit-learn-contrib/imbalanced-learn\\n\\nComparison of the combination of over- and under-sampling algorithms:<br>\\nhttp://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/combine/plot_comparison_combine.html\\n\\nChawla, Nitesh V., et al. \\\"SMOTE: synthetic minority over-sampling technique.\\\" Journal of artificial intelligence research 16 (2002):<br>\\nhttps://www.jair.org/media/953/live-953-2037-jair.pdf\",\"metadata\":{\"_uuid\":\"401bcb8e508e584feca3a34ecfb9de270fde951c\",\"_cell_guid\":\"28d4431f-bc32-4cf0-988e-9df412cc22c1\"}}],\"nbformat_minor\":1,\"metadata\":{\"language_info\":{\"pygments_lexer\":\"ipython3\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"mimetype\":\"text/x-python\",\"file_extension\":\".py\",\"nbconvert_exporter\":\"python\",\"name\":\"python\",\"version\":\"3.6.3\"},\"kernelspec\":{\"name\":\"python3\",\"display_name\":\"Python 3\",\"language\":\"python\"}}}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb b/dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb
--- a/dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/porto-seguro-safe-driver-prediction/Rafael Alencar/resampling-strategies-for-imbalanced-datasets.ipynb	(date 1658512097877)
@@ -1,1 +1,413 @@
-{"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2 style=\"margin-bottom: 18px\">Index</h2>\n\n* Imbalanced datasets\n* The metric trap\n* Confusion matrix\n* Resampling\n* Random under-sampling\n* Random over-sampling\n* Python imbalanced-learn module\n* Random under-sampling and over-sampling with imbalanced-learn\n* Under-sampling: Tomek links\n* Under-sampling: Cluster Centroids\n* Over-sampling: SMOTE\n* Over-sampling followed by under-sampling\n* Recommended reading","metadata":{"_uuid":"9fa43627b6ebc212b8d2aebae60ef6ef16fcc76a","_cell_guid":"7c102692-5b43-4f97-8394-f36ed52dbb23"}},{"cell_type":"markdown","source":"<h2 id=\"t1\" style=\"margin-bottom: 18px\">Imbalanced datasets</h2>\n\nIn this kernel we will know some techniques to handle highly unbalanced datasets, with a focus on resampling. The Porto Seguro's Safe Driver Prediction competition, used in this kernel, is a classic problem of unbalanced classes, since insurance claims can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.\n\nLet's see how unbalanced the dataset is:","metadata":{"_uuid":"1e287038acf96fc6d6825e77ba97b03f0824be0a","_cell_guid":"e38ddf38-d027-4eae-95c8-749f8f40db43"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"63eb4f34b9a7d5106da4fe6c1c871bb0025c1ae4","_cell_guid":"3b84706a-1eb0-435f-b1ff-3f1732cc3ae4"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf_train = pd.read_csv('../input/train.csv')\n\ntarget_count = df_train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t2\" style=\"margin-bottom: 18px\">The metric trap</h2>\n\nOne of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like <code>accuracy_score</code> can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n\nLet's do this experiment, using simple cross-validation and no feature engineering:","metadata":{"_uuid":"8683656d1bfdef6b5c0c623597dcbc4160a0edc1","_cell_guid":"c91f9c5d-05d2-478e-9dfb-1cb848bc0fe4"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"6d33cfa94ebb019b7c3065ea5f5dbe99ae52aeb1","_cell_guid":"0efbfb8a-76ab-4a86-b093-25271dcfcc06"},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Remove 'id' and 'target' columns\nlabels = df_train.columns[2:]\n\nX = df_train[labels]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null},{"cell_type":"markdown","source":"Now let's run the same code, but using only one feature (which should drastically reduce the accuracy of the classifier):","metadata":{"_uuid":"5b17ee8d3629cc346398e63269205e5b654cef80","_cell_guid":"c46b98a7-d500-4911-a7b5-c8fc8fbed069"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"5c7c4e3f364c664f52d04d24f948cc416d2d7e4c","_cell_guid":"bd97a34a-8e27-4eb2-a552-fe21f860dd15"},"cell_type":"code","source":"model = XGBClassifier()\nmodel.fit(X_train[['ps_calc_01']], y_train)\ny_pred = model.predict(X_test[['ps_calc_01']])\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null},{"cell_type":"markdown","source":"As we can see, the high accuracy rate was just an illusion. In this way, the choice of the metric used in unbalanced datasets is extremely important. In this competition, the evaluation metric is the Normalized Gini Coefficient, a more robust metric for imbalanced datasets, that ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score.","metadata":{"_uuid":"d9d8b57f458bdd107ae08b76b0008d80e0674d97","_cell_guid":"68d4ffaf-4412-4ff3-a9cd-38bea195da3b"}},{"cell_type":"markdown","source":"<h2 id=\"t3\" style=\"margin-bottom: 18px\">Confusion matrix</h2>\n\nAn interesting way to evaluate the results is by means of a confusion matrix, which shows the correct and incorrect predictions for each class. In the first row, the first column indicates how many classes 0 were predicted correctly, and the second column, how many classes 0 were predicted as 1. In the second row, we note that all class 1 entries were erroneously predicted as class 0.\n\nTherefore, the higher the diagonal values of the confusion matrix the better, indicating many correct predictions.","metadata":{"_uuid":"a176bd80315afd0f7c21fd26ab1841867a5eb7d0","_cell_guid":"e53e9f0f-8888-4fd9-b0d8-3bc937448162"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"39c3256de9817f64c9ae47de5f1f78531d373015","_cell_guid":"394daa2b-700c-45fc-99d7-db08245a7697"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t4\" style=\"margin-bottom: 18px\">Resampling</h2>\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).","metadata":{"_uuid":"875f5ab3b5afcdaf3c7754ce957cb01fd32bf65c","_cell_guid":"b27346eb-7bf3-4360-993a-fb91e62bb937"}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)","metadata":{"_uuid":"2232ac0fb192a468486b400846f88913a36957e6","_cell_guid":"03d31a16-7b66-4096-88d7-d548db734390"}},{"cell_type":"markdown","source":"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the <code>DataFrame.sample</code> method to get random samples each class:","metadata":{"_uuid":"67e203e0919c818e871650ef194fe497df2d39b5","_cell_guid":"0df1a3f3-49ff-4ada-80f1-198bbbd79525"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"f959c30be59ad1eccaed33f48a35d99be053b547","_cell_guid":"2adbc1c9-8cdf-43d9-9a57-f24f503ba523"},"cell_type":"code","source":"# Class count\ncount_class_0, count_class_1 = df_train.target.value_counts()\n\n# Divide by class\ndf_class_0 = df_train[df_train['target'] == 0]\ndf_class_1 = df_train[df_train['target'] == 1]","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t5\">Random under-sampling</h2>","metadata":{"_uuid":"b765be76a182930feb650b01dd4d1de90501bbce","_cell_guid":"152ea73a-aa23-4fd2-a3f5-1f55c69041ae"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"2a667d73560fb6408897835809a9d67310b45323","_cell_guid":"a0e61bc8-cef1-4828-9c5f-49c1c50c522c"},"cell_type":"code","source":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.target.value_counts())\n\ndf_test_under.target.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t6\">Random over-sampling</h2>","metadata":{"_uuid":"17192fe8557463941e3abd4633b58ced0037721b","_cell_guid":"be656d47-e529-4533-b975-cf6de072d959"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"431942c08c59f0a6ae629c3c38485bf85d001892","_cell_guid":"8ca1ac09-5d61-4cf7-99a4-e299f4955c97"},"cell_type":"code","source":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())\n\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t7\" style=\"margin-bottom: 18px\">Python imbalanced-learn module</h2>\n\nA number of more sophisticated resapling techniques have been proposed in the scientific literature.\n\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n\nLet's apply some of these resampling techniques, using the Python library [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/). It is compatible with scikit-learn and is part of scikit-learn-contrib projects.","metadata":{"_uuid":"9672899d4029b71b72897927ce464d6d7427ce77","_cell_guid":"9fd90ddc-f2fc-487d-b177-0c540daf2eff"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"41b482ec89f13dd22e85612404300041a3e71deb","_cell_guid":"fdf3f76d-aacb-4ccc-9649-736dce7a237c"},"cell_type":"code","source":"import imblearn","execution_count":null},{"cell_type":"markdown","source":"For ease of visualization, let's create a small unbalanced sample dataset using the <code>make_classification</code> method:","metadata":{"_uuid":"ff93d1707c416178c010c125319220b785dca984","_cell_guid":"5542beb4-6aee-401a-8bc0-2a522fbbe90b"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"3126327b6f4b4c469701ac7a76590f5e55e17076","_cell_guid":"e97abde1-c324-47b5-a33c-3b8b92268b5e"},"cell_type":"code","source":"from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)');","execution_count":null},{"cell_type":"markdown","source":"We will also create a 2-dimensional plot function, <code>plot_2d_space</code>, to see the data distribution:","metadata":{"_uuid":"d348b114ec1594eeefa0a67aab8b54e5b9ee2bdf","_cell_guid":"4de57657-3aed-4444-a609-318063391763"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"b2a02a4ff687fb099232a0468917ec8ab737d22d","_cell_guid":"8003c84c-fee4-45c7-b490-90a185799760"},"cell_type":"code","source":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","execution_count":null},{"cell_type":"markdown","source":"Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):","metadata":{"_uuid":"aabc110dd8d1b36345df6aada1c59b864e48e8e6","_cell_guid":"2c565bbc-39ed-45a2-8b3d-bc501e2510aa"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"f64eff19d1304d4bd1e92d2c51a8a953cd17d7f9","_cell_guid":"22b689c8-b296-4736-9aa6-46f36283f91f"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"#t72\">Random under-sampling and over-sampling with imbalanced-learn</h2>","metadata":{"_uuid":"0d7316b04837aa103003d667f63ecb05d43fc04e","_cell_guid":"c3c9a24f-3cd0-4c8d-8a4d-4403c4f1a641"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"8a56a4b118d7cae885e4c6a45fa02b2f066ece78","_cell_guid":"28bdbe23-5eeb-4335-8c70-7a00bbac3e03"},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(return_indices=True)\nX_rus, y_rus, id_rus = rus.fit_sample(X, y)\n\nprint('Removed indexes:', id_rus)\n\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')","execution_count":null},{"outputs":[],"metadata":{"collapsed":true,"scrolled":true,"_uuid":"be5b90300f0a25bbe7d1822503e7fc5185a906b2","_cell_guid":"c8eaed30-339e-4d23-a0f2-fbd687b217ea"},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t8\" style=\"margin-bottom: 18px\">Under-sampling: Tomek links</h2>\n\nTomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.","metadata":{"_uuid":"b3f9ac47a157d9096a626360408859b795299c24","_cell_guid":"3156134f-539b-48b8-b9d0-64095fe50c1c"}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)","metadata":{"_uuid":"85c01c0e6baa1b984585c1db34c5ab0315cbf8ff","_cell_guid":"f131f7e0-007d-406e-9e1c-db352d2d8433"}},{"cell_type":"markdown","source":"In the code below, we'll use <code>ratio='majority'</code> to resample the majority class.","metadata":{"_uuid":"733a86ccfaaabf701fb2d1f9997c732b19630df3","_cell_guid":"45bf057b-369c-4f37-a5ac-7417ad79253d"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"0c93b1e11359f4d1eb873280b61aa0f54ee8bd36","_cell_guid":"58f13562-c0c6-4247-bf40-c6dec764ff3a"},"cell_type":"code","source":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(return_indices=True, ratio='majority')\nX_tl, y_tl, id_tl = tl.fit_sample(X, y)\n\nprint('Removed indexes:', id_tl)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t9\" style=\"margin-bottom: 18px\">Under-sampling: Cluster Centroids</h2>\n\nThis technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\n\nIn this example we will pass the <code>{0: 10}</code> dict for the parameter <code>ratio</code>, to preserve 10 elements from the majority class (0), and all minority class (1) .","metadata":{"_uuid":"b4e75fffe4c91afcd63705aa7bcb16b6fd9f6b1f","_cell_guid":"fef831bd-ecec-429c-aef2-5d51d1188820"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"ad088a08f4f9e0b646571950928c7fff47f52f98","_cell_guid":"91001b3e-fe5c-4dc1-8501-395e1218cba1"},"cell_type":"code","source":"from imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids(ratio={0: 10})\nX_cc, y_cc = cc.fit_sample(X, y)\n\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t10\" style=\"margin-bottom: 18px\">Over-sampling: SMOTE</h2>\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.","metadata":{"_uuid":"b3741f5c14acdbd76e25725e2d73df5f2cb0a239","_cell_guid":"78adb7b4-a7e1-4d10-9cc2-6bf6477c63df"}},{"cell_type":"markdown","source":" ![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)","metadata":{"_uuid":"51697e21b7cdb4064dda18aa24e6ecf039b1132b","_cell_guid":"5162646f-da82-4877-b6d8-25a8be7f42e9"}},{"cell_type":"markdown","source":"We'll use <code>ratio='minority'</code> to resample the minority class.","metadata":{"_uuid":"9393851db694c178faf93615bf05addedf5d678b","_cell_guid":"9c0a0d78-8427-437e-aa24-ffe2bd12edb2"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"74457c951aabf5b16be1c4282c15d9cb2034f26b","_cell_guid":"97e9f84e-0951-4037-b882-57545f9d967a"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t11\" style=\"margin-bottom: 18px\">Over-sampling followed by under-sampling</h2>\n\nNow, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:","metadata":{"_uuid":"d7ebbeb741ad6469cb7ebbced3499acd3c49856a","_cell_guid":"20c3cdbb-a7c1-45a5-8dd9-84cff7d9af31"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"b740fbaf8677522d3b3040e2f47e3b954dc56877","_cell_guid":"ab732fe1-45c1-4163-b70e-7a2ce2dbe42f"},"cell_type":"code","source":"from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek(ratio='auto')\nX_smt, y_smt = smt.fit_sample(X, y)\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')","execution_count":null},{"cell_type":"markdown","source":"<h2 id=\"t12\" style=\"margin-bottom: 18px\">Recommended reading</h2>\n\nThe imbalanced-learn documentation:<br>\nhttp://contrib.scikit-learn.org/imbalanced-learn/stable/index.html\n\nThe imbalanced-learn GitHub:<br>\nhttps://github.com/scikit-learn-contrib/imbalanced-learn\n\nComparison of the combination of over- and under-sampling algorithms:<br>\nhttp://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/combine/plot_comparison_combine.html\n\nChawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002):<br>\nhttps://www.jair.org/media/953/live-953-2037-jair.pdf","metadata":{"_uuid":"401bcb8e508e584feca3a34ecfb9de270fde951c","_cell_guid":"28d4431f-bc32-4cf0-988e-9df412cc22c1"}}],"nbformat_minor":1,"metadata":{"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","name":"python","version":"3.6.3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}
\ No newline at end of file
+{
+ "nbformat": 4,
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "source": "<h2 style=\"margin-bottom: 18px\">Index</h2>\n\n* Imbalanced datasets\n* The metric trap\n* Confusion matrix\n* Resampling\n* Random under-sampling\n* Random over-sampling\n* Python imbalanced-learn module\n* Random under-sampling and over-sampling with imbalanced-learn\n* Under-sampling: Tomek links\n* Under-sampling: Cluster Centroids\n* Over-sampling: SMOTE\n* Over-sampling followed by under-sampling\n* Recommended reading",
+   "metadata": {
+    "_uuid": "9fa43627b6ebc212b8d2aebae60ef6ef16fcc76a",
+    "_cell_guid": "7c102692-5b43-4f97-8394-f36ed52dbb23"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t1\" style=\"margin-bottom: 18px\">Imbalanced datasets</h2>\n\nIn this kernel we will know some techniques to handle highly unbalanced datasets, with a focus on resampling. The Porto Seguro's Safe Driver Prediction competition, used in this kernel, is a classic problem of unbalanced classes, since insurance claims can be considered unusual cases when considering all clients. Other classic examples of unbalanced classes are the detection of financial fraud and attacks on computer networks.\n\nLet's see how unbalanced the dataset is:",
+   "metadata": {
+    "_uuid": "1e287038acf96fc6d6825e77ba97b03f0824be0a",
+    "_cell_guid": "e38ddf38-d027-4eae-95c8-749f8f40db43"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "63eb4f34b9a7d5106da4fe6c1c871bb0025c1ae4",
+    "_cell_guid": "3b84706a-1eb0-435f-b1ff-3f1732cc3ae4"
+   },
+   "cell_type": "code",
+   "source": "import numpy as np\nimport pandas as pd\n\ndf_train = pd.read_csv('../input/train.csv')\n\ntarget_count = df_train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t2\" style=\"margin-bottom: 18px\">The metric trap</h2>\n\nOne of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like <code>accuracy_score</code> can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n\nLet's do this experiment, using simple cross-validation and no feature engineering:",
+   "metadata": {
+    "_uuid": "8683656d1bfdef6b5c0c623597dcbc4160a0edc1",
+    "_cell_guid": "c91f9c5d-05d2-478e-9dfb-1cb848bc0fe4"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "6d33cfa94ebb019b7c3065ea5f5dbe99ae52aeb1",
+    "_cell_guid": "0efbfb8a-76ab-4a86-b093-25271dcfcc06"
+   },
+   "cell_type": "code",
+   "source": "from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Remove 'id' and 'target' columns\nlabels = df_train.columns[2:]\n\nX = df_train[labels]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Now let's run the same code, but using only one feature (which should drastically reduce the accuracy of the classifier):",
+   "metadata": {
+    "_uuid": "5b17ee8d3629cc346398e63269205e5b654cef80",
+    "_cell_guid": "c46b98a7-d500-4911-a7b5-c8fc8fbed069"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "5c7c4e3f364c664f52d04d24f948cc416d2d7e4c",
+    "_cell_guid": "bd97a34a-8e27-4eb2-a552-fe21f860dd15"
+   },
+   "cell_type": "code",
+   "source": "model = XGBClassifier()\nmodel.fit(X_train[['ps_calc_01']], y_train)\ny_pred = model.predict(X_test[['ps_calc_01']])\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "As we can see, the high accuracy rate was just an illusion. In this way, the choice of the metric used in unbalanced datasets is extremely important. In this competition, the evaluation metric is the Normalized Gini Coefficient, a more robust metric for imbalanced datasets, that ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score.",
+   "metadata": {
+    "_uuid": "d9d8b57f458bdd107ae08b76b0008d80e0674d97",
+    "_cell_guid": "68d4ffaf-4412-4ff3-a9cd-38bea195da3b"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t3\" style=\"margin-bottom: 18px\">Confusion matrix</h2>\n\nAn interesting way to evaluate the results is by means of a confusion matrix, which shows the correct and incorrect predictions for each class. In the first row, the first column indicates how many classes 0 were predicted correctly, and the second column, how many classes 0 were predicted as 1. In the second row, we note that all class 1 entries were erroneously predicted as class 0.\n\nTherefore, the higher the diagonal values of the confusion matrix the better, indicating many correct predictions.",
+   "metadata": {
+    "_uuid": "a176bd80315afd0f7c21fd26ab1841867a5eb7d0",
+    "_cell_guid": "e53e9f0f-8888-4fd9-b0d8-3bc937448162"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "39c3256de9817f64c9ae47de5f1f78531d373015",
+    "_cell_guid": "394daa2b-700c-45fc-99d7-db08245a7697"
+   },
+   "cell_type": "code",
+   "source": "from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t4\" style=\"margin-bottom: 18px\">Resampling</h2>\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and / or adding more examples from the minority class (over-sampling).",
+   "metadata": {
+    "_uuid": "875f5ab3b5afcdaf3c7754ce957cb01fd32bf65c",
+    "_cell_guid": "b27346eb-7bf3-4360-993a-fb91e62bb937"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)",
+   "metadata": {
+    "_uuid": "2232ac0fb192a468486b400846f88913a36957e6",
+    "_cell_guid": "03d31a16-7b66-4096-88d7-d548db734390"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the <code>DataFrame.sample</code> method to get random samples each class:",
+   "metadata": {
+    "_uuid": "67e203e0919c818e871650ef194fe497df2d39b5",
+    "_cell_guid": "0df1a3f3-49ff-4ada-80f1-198bbbd79525"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "f959c30be59ad1eccaed33f48a35d99be053b547",
+    "_cell_guid": "2adbc1c9-8cdf-43d9-9a57-f24f503ba523"
+   },
+   "cell_type": "code",
+   "source": "# Class count\ncount_class_0, count_class_1 = df_train.target.value_counts()\n\n# Divide by class\ndf_class_0 = df_train[df_train['target'] == 0]\ndf_class_1 = df_train[df_train['target'] == 1]",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t5\">Random under-sampling</h2>",
+   "metadata": {
+    "_uuid": "b765be76a182930feb650b01dd4d1de90501bbce",
+    "_cell_guid": "152ea73a-aa23-4fd2-a3f5-1f55c69041ae"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "2a667d73560fb6408897835809a9d67310b45323",
+    "_cell_guid": "a0e61bc8-cef1-4828-9c5f-49c1c50c522c"
+   },
+   "cell_type": "code",
+   "source": "df_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.target.value_counts())\n\ndf_test_under.target.value_counts().plot(kind='bar', title='Count (target)');",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t6\">Random over-sampling</h2>",
+   "metadata": {
+    "_uuid": "17192fe8557463941e3abd4633b58ced0037721b",
+    "_cell_guid": "be656d47-e529-4533-b975-cf6de072d959"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "431942c08c59f0a6ae629c3c38485bf85d001892",
+    "_cell_guid": "8ca1ac09-5d61-4cf7-99a4-e299f4955c97"
+   },
+   "cell_type": "code",
+   "source": "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())\n\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)');",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t7\" style=\"margin-bottom: 18px\">Python imbalanced-learn module</h2>\n\nA number of more sophisticated resapling techniques have been proposed in the scientific literature.\n\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n\nLet's apply some of these resampling techniques, using the Python library [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/). It is compatible with scikit-learn and is part of scikit-learn-contrib projects.",
+   "metadata": {
+    "_uuid": "9672899d4029b71b72897927ce464d6d7427ce77",
+    "_cell_guid": "9fd90ddc-f2fc-487d-b177-0c540daf2eff"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "41b482ec89f13dd22e85612404300041a3e71deb",
+    "_cell_guid": "fdf3f76d-aacb-4ccc-9649-736dce7a237c"
+   },
+   "cell_type": "code",
+   "source": "import imblearn",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "For ease of visualization, let's create a small unbalanced sample dataset using the <code>make_classification</code> method:",
+   "metadata": {
+    "_uuid": "ff93d1707c416178c010c125319220b785dca984",
+    "_cell_guid": "5542beb4-6aee-401a-8bc0-2a522fbbe90b"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "3126327b6f4b4c469701ac7a76590f5e55e17076",
+    "_cell_guid": "e97abde1-c324-47b5-a33c-3b8b92268b5e"
+   },
+   "cell_type": "code",
+   "source": "from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=100, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)');",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "We will also create a 2-dimensional plot function, <code>plot_2d_space</code>, to see the data distribution:",
+   "metadata": {
+    "_uuid": "d348b114ec1594eeefa0a67aab8b54e5b9ee2bdf",
+    "_cell_guid": "4de57657-3aed-4444-a609-318063391763"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "b2a02a4ff687fb099232a0468917ec8ab737d22d",
+    "_cell_guid": "8003c84c-fee4-45c7-b490-90a185799760"
+   },
+   "cell_type": "code",
+   "source": "def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Because the dataset has many dimensions (features) and our graphs will be 2D, we will reduce the size of the dataset using Principal Component Analysis (PCA):",
+   "metadata": {
+    "_uuid": "aabc110dd8d1b36345df6aada1c59b864e48e8e6",
+    "_cell_guid": "2c565bbc-39ed-45a2-8b3d-bc501e2510aa"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "f64eff19d1304d4bd1e92d2c51a8a953cd17d7f9",
+    "_cell_guid": "22b689c8-b296-4736-9aa6-46f36283f91f"
+   },
+   "cell_type": "code",
+   "source": "from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX = pca.fit_transform(X)\n\nplot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"#t72\">Random under-sampling and over-sampling with imbalanced-learn</h2>",
+   "metadata": {
+    "_uuid": "0d7316b04837aa103003d667f63ecb05d43fc04e",
+    "_cell_guid": "c3c9a24f-3cd0-4c8d-8a4d-4403c4f1a641"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "8a56a4b118d7cae885e4c6a45fa02b2f066ece78",
+    "_cell_guid": "28bdbe23-5eeb-4335-8c70-7a00bbac3e03"
+   },
+   "cell_type": "code",
+   "source": "from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(return_indices=True)\nX_rus, y_rus, id_rus = rus.fit_sample(X, y)\n\nprint('Removed indexes:', id_rus)\n\nplot_2d_space(X_rus, y_rus, 'Random under-sampling')",
+   "execution_count": null
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "scrolled": true,
+    "_uuid": "be5b90300f0a25bbe7d1822503e7fc5185a906b2",
+    "_cell_guid": "c8eaed30-339e-4d23-a0f2-fbd687b217ea"
+   },
+   "cell_type": "code",
+   "source": "from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X.shape[0], 'new random picked points')\n\nplot_2d_space(X_ros, y_ros, 'Random over-sampling')",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t8\" style=\"margin-bottom: 18px\">Under-sampling: Tomek links</h2>\n\nTomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.",
+   "metadata": {
+    "_uuid": "b3f9ac47a157d9096a626360408859b795299c24",
+    "_cell_guid": "3156134f-539b-48b8-b9d0-64095fe50c1c"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)",
+   "metadata": {
+    "_uuid": "85c01c0e6baa1b984585c1db34c5ab0315cbf8ff",
+    "_cell_guid": "f131f7e0-007d-406e-9e1c-db352d2d8433"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "In the code below, we'll use <code>ratio='majority'</code> to resample the majority class.",
+   "metadata": {
+    "_uuid": "733a86ccfaaabf701fb2d1f9997c732b19630df3",
+    "_cell_guid": "45bf057b-369c-4f37-a5ac-7417ad79253d"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "0c93b1e11359f4d1eb873280b61aa0f54ee8bd36",
+    "_cell_guid": "58f13562-c0c6-4247-bf40-c6dec764ff3a"
+   },
+   "cell_type": "code",
+   "source": "from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(return_indices=True, ratio='majority')\nX_tl, y_tl, id_tl = tl.fit_sample(X, y)\n\nprint('Removed indexes:', id_tl)\n\nplot_2d_space(X_tl, y_tl, 'Tomek links under-sampling')",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t9\" style=\"margin-bottom: 18px\">Under-sampling: Cluster Centroids</h2>\n\nThis technique performs under-sampling by generating centroids based on clustering methods. The data will be previously grouped by similarity, in order to preserve information.\n\nIn this example we will pass the <code>{0: 10}</code> dict for the parameter <code>ratio</code>, to preserve 10 elements from the majority class (0), and all minority class (1) .",
+   "metadata": {
+    "_uuid": "b4e75fffe4c91afcd63705aa7bcb16b6fd9f6b1f",
+    "_cell_guid": "fef831bd-ecec-429c-aef2-5d51d1188820"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "ad088a08f4f9e0b646571950928c7fff47f52f98",
+    "_cell_guid": "91001b3e-fe5c-4dc1-8501-395e1218cba1"
+   },
+   "cell_type": "code",
+   "source": "from imblearn.under_sampling import ClusterCentroids\n\ncc = ClusterCentroids(ratio={0: 10})\nX_cc, y_cc = cc.fit_sample(X, y)\n\nplot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t10\" style=\"margin-bottom: 18px\">Over-sampling: SMOTE</h2>\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.",
+   "metadata": {
+    "_uuid": "b3741f5c14acdbd76e25725e2d73df5f2cb0a239",
+    "_cell_guid": "78adb7b4-a7e1-4d10-9cc2-6bf6477c63df"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": " ![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)",
+   "metadata": {
+    "_uuid": "51697e21b7cdb4064dda18aa24e6ecf039b1132b",
+    "_cell_guid": "5162646f-da82-4877-b6d8-25a8be7f42e9"
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "We'll use <code>ratio='minority'</code> to resample the minority class.",
+   "metadata": {
+    "_uuid": "9393851db694c178faf93615bf05addedf5d678b",
+    "_cell_guid": "9c0a0d78-8427-437e-aa24-ffe2bd12edb2"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "74457c951aabf5b16be1c4282c15d9cb2034f26b",
+    "_cell_guid": "97e9f84e-0951-4037-b882-57545f9d967a"
+   },
+   "cell_type": "code",
+   "source": "from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t11\" style=\"margin-bottom: 18px\">Over-sampling followed by under-sampling</h2>\n\nNow, we will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques:",
+   "metadata": {
+    "_uuid": "d7ebbeb741ad6469cb7ebbced3499acd3c49856a",
+    "_cell_guid": "20c3cdbb-a7c1-45a5-8dd9-84cff7d9af31"
+   }
+  },
+  {
+   "outputs": [],
+   "metadata": {
+    "collapsed": true,
+    "_uuid": "b740fbaf8677522d3b3040e2f47e3b954dc56877",
+    "_cell_guid": "ab732fe1-45c1-4163-b70e-7a2ce2dbe42f"
+   },
+   "cell_type": "code",
+   "source": "from imblearn.combine import SMOTETomek\n\nsmt = SMOTETomek(ratio='auto')\nX_smt, y_smt = smt.fit_sample(X, y)\n\nplot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')",
+   "execution_count": null
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h2 id=\"t12\" style=\"margin-bottom: 18px\">Recommended reading</h2>\n\nThe imbalanced-learn documentation:<br>\nhttp://contrib.scikit-learn.org/imbalanced-learn/stable/index.html\n\nThe imbalanced-learn GitHub:<br>\nhttps://github.com/scikit-learn-contrib/imbalanced-learn\n\nComparison of the combination of over- and under-sampling algorithms:<br>\nhttp://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/combine/plot_comparison_combine.html\n\nChawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002):<br>\nhttps://www.jair.org/media/953/live-953-2037-jair.pdf",
+   "metadata": {
+    "_uuid": "401bcb8e508e584feca3a34ecfb9de270fde951c",
+    "_cell_guid": "28d4431f-bc32-4cf0-988e-9df412cc22c1"
+   }
+  }
+ ],
+ "nbformat_minor": 1,
+ "metadata": {
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "mimetype": "text/x-python",
+   "file_extension": ".py",
+   "nbconvert_exporter": "python",
+   "name": "python",
+   "version": "3.6.3"
+  },
+  "kernelspec": {
+   "name": "python3",
+   "display_name": "Python 3",
+   "language": "python"
+  }
+ }
+}
\ No newline at end of file
Index: dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9c75ca41-8357-479e-8a46-ebdec5f035f3\",\"_uuid\":\"319ae25236d9fddf1745ea1c4cb365e5dbb00372\"},\"source\":\"<img src=\\\"http://data.freehdw.com/ships-titanic-vehicles-best.jpg\\\"  Width=\\\"800\\\">\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"bdce3bc433feb19f6622ab910cfe2123ccd07a1c\"},\"source\":\"<a id=\\\"introduction\\\" ></a><br>\\nThis kernel is for all aspiring data scientists to learn from and to review their knowledge. We will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation. I am super excited to share my first kernel with the Kaggle community. As I go on in this journey and learn new topics, I will incorporate them with each new updates. So, check for them and please <b>leave a comment</b> if you have any suggestions to make this kernel better!! Going back to the topics of this kernel, I will do more in-depth visualizations to explain the data, and the machine learning classifiers will be used to predict passenger survival status.\\n\\n<div style=\\\"text-align: left\\\"> \\n    <br>\\n    NOTE:\\n    <ul>\\n        <li>Follow me on <a href=\\\"https://www.youtube.com/channel/UC1mPjGyLcZmsMgZ8SJgrfdw\\\"><b>YOUTUBE</b></a> to get the video tutorial for this notebook.\\n        <li>If you want to learn more about Advanced Regression models, please check out <a href=\\\"https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing\\\">this</a> kernel.</li>\\n        <li>If you are reading this on github, I recommend you read this on <a href=\\\"https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic\\\">kaggle</a>.</li>\\n    </ul>\\n</div>\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7224a910-ec6b-481d-82f1-90ca6b5d037e\",\"_uuid\":\"9cd04af82734c5b53aaddc80992e1f499c180611\"},\"source\":\"# Kernel Goals\\n<a id=\\\"aboutthiskernel\\\"></a>\\n***\\nThere are three primary goals of this kernel.\\n- <b>Do a statistical analysis</b> of how some group of people was survived more than others. \\n- <b>Do an exploratory data analysis(EDA)</b> of titanic with visualizations and storytelling.  \\n- <b>Predict</b>: Use machine learning classification models to predict the chances of passengers survival.\\n\\nP.S. If you want to learn more about regression models, try this [kernel](https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing/edit/run/9585160). \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"b3b559a5-dad0-419e-835a-e6babd1042ff\",\"_uuid\":\"1b1a0b28ad37a349e284d1e6ce6477d11b95e7c9\"},\"source\":\"# Part 1: Importing Necessary Libraries and datasets\\n***\\n<a id=\\\"import_libraries**\\\"></a>\\n## 1a. Loading libraries\\n\\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. \"},{\"cell_type\":\"code\",\"execution_count\":1,\"metadata\":{\"_cell_guid\":\"80643cb5-64f3-4180-92a9-2f8e83263ac6\",\"_kg_hide-input\":true,\"_uuid\":\"33d54abf387474bce3017f1fc3832493355010c0\",\"tags\":[]},\"outputs\":[],\"source\":\"# Import necessary modules\\nimport pandas as pd\\nimport numpy as np\\nfrom matplotlib import pyplot as plt\\nimport seaborn as sns\\n\\n%matplotlib inline \\n# %config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \\n\\nimport warnings ## importing warnings library. \\nwarnings.filterwarnings('ignore') ## Ignore warning\\n\\n\\n\\nimport os ## imporing os\\nprint(os.listdir(\\\"../input/\\\"))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"bd41125b-6dd4-41d9-8905-31edc812d18e\",\"_uuid\":\"82ccd43cc8449346749bf8a35e1acb9a40e3b141\"},\"source\":\"## 1b. Loading Datasets\\n<a id=\\\"load_data\\\"></a>\\n***\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"30b23f046eef6d19c26e6ad967cef914cf312791\"},\"source\":\"After loading the necessary modules, we need to import the datasets. Many of the business problems usually come with a tremendous amount of messy data. We extract those data from many sources. I am hoping to write about that in a different kernel. For now, we are going to work with a less complicated and quite popular machine learning dataset.\"},{\"cell_type\":\"code\",\"execution_count\":2,\"metadata\":{\"_cell_guid\":\"28722a45-5f11-4629-8814-9ab913e9349a\",\"_kg_hide-input\":false,\"_uuid\":\"185b34e70f2efded0c665c6713f79b840ddf0c89\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:04.910168Z\",\"iopub.status.busy\":\"2021-06-26T16:35:04.909538Z\",\"iopub.status.idle\":\"2021-06-26T16:35:04.930876Z\",\"shell.execute_reply\":\"2021-06-26T16:35:04.930259Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:04.91012Z\"}},\"outputs\":[],\"source\":\"## Importing the datasets\\ntrain = pd.read_csv(\\\"../input/titanic/train.csv\\\")\\ntest = pd.read_csv(\\\"../input/titanic/test.csv\\\")\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"d55ae33391486797b979ef1117e8d8401ac1dab4\"},\"source\":\"You are probably wondering why two datasets? Also, Why have I named it \\\"train\\\" and \\\"test\\\"?  To explain that I am going to give you an overall picture of the supervised machine learning process. \\n\\n\\\"Machine Learning\\\" is simply \\\"Machine\\\" and \\\"Learning\\\". Nothing more and nothing less. In a supervised machine learning process, we are giving machine/computer/models specific inputs or data(text/number/image/audio) to learn from aka we are training the machine to learn certain aspects based on the data and the output. Now, how can we determine that machine is actually learning what we are try to teach? That is where the test set comes to play. We withhold part of the data where we know the output/result of each datapoints, and we use this data to test the trained models.  We then compare the outcomes to determine the performance of the algorithms. If you are a bit confused thats okay. I will explain more as we keep reading. Let's take a look at sample datasets.\"},{\"cell_type\":\"code\",\"execution_count\":3,\"metadata\":{},\"outputs\":[],\"source\":\"train.head()\"},{\"cell_type\":\"code\",\"execution_count\":4,\"metadata\":{},\"outputs\":[],\"source\":\"temp = train.groupby(\\\"Sex\\\")['Age'].min().to_frame().reset_index()\"},{\"cell_type\":\"code\",\"execution_count\":5,\"metadata\":{},\"outputs\":[],\"source\":\"temp\"},{\"cell_type\":\"code\",\"execution_count\":6,\"metadata\":{},\"outputs\":[],\"source\":\"temp = temp.rename(columns={\\\"Age\\\": \\\"min_age\\\"})\"},{\"cell_type\":\"code\",\"execution_count\":7,\"metadata\":{},\"outputs\":[],\"source\":\"temp\"},{\"cell_type\":\"code\",\"execution_count\":8,\"metadata\":{},\"outputs\":[],\"source\":\"train.head()\"},{\"cell_type\":\"code\",\"execution_count\":9,\"metadata\":{},\"outputs\":[],\"source\":\"train.dtypes\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"c87c72ba-c9b2-48e9-86d8-c711d0795ca0\",\"_uuid\":\"5759d720798ca115cc5d3d2f75be6961d1455832\"},\"source\":\"## 1c. A Glimpse of the Datasets. \\n<a id=\\\"glimpse\\\"></a>\\n***\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"# Train Set\"},{\"cell_type\":\"code\",\"execution_count\":10,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:04.932322Z\",\"iopub.status.busy\":\"2021-06-26T16:35:04.932015Z\",\"iopub.status.idle\":\"2021-06-26T16:35:04.958015Z\",\"shell.execute_reply\":\"2021-06-26T16:35:04.957084Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:04.932234Z\"}},\"outputs\":[],\"source\":\"%%time\\ntrain.sample(5)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ab439b32-e251-489a-89fd-cfcd61b236bf\",\"_uuid\":\"69b24241db4d4eae9e46711c384d8130f6fa8322\"},\"source\":\"# Test Set\"},{\"cell_type\":\"code\",\"execution_count\":11,\"metadata\":{\"_cell_guid\":\"0f0649fa-b003-403f-9d7c-d2d14a6cf068\",\"_kg_hide-input\":true,\"_uuid\":\"877b2fc905cd60e3f9a525b6fedad9a5c0a671e5\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:04.960536Z\",\"iopub.status.busy\":\"2021-06-26T16:35:04.960114Z\",\"iopub.status.idle\":\"2021-06-26T16:35:08.927166Z\",\"shell.execute_reply\":\"2021-06-26T16:35:08.926315Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:04.960357Z\"}},\"outputs\":[],\"source\":\"## Take a look at the overview of the dataset. \\n%timeit test.sample(5)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"5f7426639cf97db92e4ca85a13e89c8394f6aa7c\"},\"source\":\"This is a sample of train and test dataset. Lets find out a bit more about the train and test dataset. \"},{\"cell_type\":\"code\",\"execution_count\":12,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"1258a94388599a131fe08cd6e05205b15d53df66\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:08.929903Z\",\"iopub.status.busy\":\"2021-06-26T16:35:08.929444Z\",\"iopub.status.idle\":\"2021-06-26T16:35:08.945917Z\",\"shell.execute_reply\":\"2021-06-26T16:35:08.945011Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:08.92985Z\"}},\"outputs\":[],\"source\":\"print (\\\"The shape of the train data is (row, column):\\\"+ str(train.shape))\\nprint (train.info())\\nprint (\\\"The shape of the test data is (row, column):\\\"+ str(test.shape))\\nprint (test.info())\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"15c64d36-94b3-4798-af86-775f70feb2dd\",\"_uuid\":\"c72d21139ee6220aee5d8f654561864a5f6499b7\"},\"source\":\" ## 1d. About This Dataset\\n<a id=\\\"aboutthisdataset\\\"></a>\\n***\\nThe data has split into two groups:\\n\\n- training set (train.csv)\\n- test set (test.csv)\\n\\n***The training set includes our target variable(dependent variable), passenger survival status***(also known as the ground truth from the Titanic tragedy) along with other independent features like gender, class, fare, and Pclass. \\n\\nThe test set should be used to see how well our model performs on unseen data. When we say unseen data, we mean that the algorithm or machine learning models have no relation to the test data. We do not want to use any part of the test data in any way to modify our algorithms; Which are the reasons why we clean our test data and train data separately. ***The test set does not provide passengers survival status***. We are going to use our model to predict passenger survival status.\\n\\nNow let's go through the features and describe a little. There is a couple of different type of variables, They are...\\n\\n***\\n**Categorical:**\\n- **Nominal**(variables that have two or more categories, but which do not have an intrinsic order.)\\n   > - **Cabin**\\n   > - **Embarked**(Port of Embarkation)\\n            C(Cherbourg)\\n            Q(Queenstown) \\n            S(Southampton)\\n        \\n- **Dichotomous**(Nominal variable with only two categories)\\n   > - **Sex**\\n            Female\\n            Male\\n- **Ordinal**(variables that have two or more categories just like nominal variables. Only the categories can also be ordered or ranked.)\\n   > - **Pclass** (A proxy for socio-economic status (SES)) \\n            1(Upper)\\n            2(Middle) \\n            3(Lower)\\n***\\n**Numeric:**\\n- **Discrete**\\n  >  - **Passenger ID**(Unique identifing # for each passenger)\\n  >  - **SibSp**\\n  >  - **Parch**\\n  >  - **Survived** (Our outcome or dependent variable)\\n            0\\n            1\\n- **Continous**\\n>  - **Age**\\n>  - **Fare**\\n***\\n**Text Variable**\\n> - **Ticket** (Ticket number for passenger.)\\n> - **Name**(  Name of the passenger.) \\n\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7b21d695-c767-48ad-a3c8-abb9bba56e71\",\"_uuid\":\"53fdd02b149e47bd7168dba94ddff754626b1781\"},\"source\":\"## 1e. Tableau Visualization of the Data\\n<a id='tableau_visualization'></a>\\n***\\nI have incorporated a tableau visualization below of the training data. This visualization... \\n* is for us to have an overview and play around with the dataset. \\n* is done without making any changes(including Null values) to any features of the dataset.\\n***\\nLet's get a better perspective of the dataset through this visualization.\\n\"},{\"cell_type\":\"code\",\"execution_count\":13,\"metadata\":{\"_cell_guid\":\"0ca9339e-4d13-4eb6-b28b-4a9e614ca2d0\",\"_kg_hide-input\":true,\"_uuid\":\"bc9819aecc9adceb1fa3fe151388fd41f5dcece2\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:08.947896Z\",\"iopub.status.busy\":\"2021-06-26T16:35:08.947379Z\",\"iopub.status.idle\":\"2021-06-26T16:35:08.954197Z\",\"shell.execute_reply\":\"2021-06-26T16:35:08.953477Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:08.947651Z\"}},\"outputs\":[],\"source\":\"%%HTML\\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Titanic_data_mining&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"2b6ce9bc-8210-433d-ab4b-d8afe93c3810\",\"_uuid\":\"b46be01bb1ba3ff4f23c72038679542ba3f780de\"},\"source\":\"We want to see how the left vertical bar changes when we filter out unique values of certain features. We can use multiple filters to see if there are any correlations among them. For example, if we click on **upper** and **Female** tab, we would see that green color dominates the bar with a ratio of 91:3 survived and non survived female passengers; a 97% survival rate for females. We can reset the filters by clicking anywhere in the whilte space. The age distribution chart on top provides us with some more info such as, what was the age range of those three unlucky females as the red color give away the unsurvived once. If you would like to check out some of my other tableau charts, please click [here.](https://public.tableau.com/profile/masum.rumi#!/)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"24dfbb58-4708-42a1-9122-c7e0b96ad0e9\",\"_uuid\":\"e789474652ddf03c65e7bb8f17f69544b907cecb\"},\"source\":\"# Part 2: Overview and Cleaning the Data\\n<a id=\\\"cleaningthedata\\\"></a>\\n***\\n## 2a. Overview\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"359e6e3e-3a27-45aa-b6cf-ec18b8220eae\",\"_uuid\":\"f0ec8e9300f40427a2a53f9c3e3f92e120ce786b\"},\"source\":\"Datasets in the real world are often messy, However, this dataset is almost clean. Lets analyze and see what we have here.\"},{\"cell_type\":\"code\",\"execution_count\":14,\"metadata\":{\"_cell_guid\":\"bf19c831-fbe0-49b6-8bf8-d7db118f40b1\",\"_kg_hide-input\":true,\"_uuid\":\"5a0593fb4564f0284ca7fdf5c006020cb288db95\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:08.956119Z\",\"iopub.status.busy\":\"2021-06-26T16:35:08.955538Z\",\"iopub.status.idle\":\"2021-06-26T16:35:08.973222Z\",\"shell.execute_reply\":\"2021-06-26T16:35:08.972151Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:08.956072Z\"}},\"outputs\":[],\"source\":\"## saving passenger id in advance in order to submit later. \\npassengerid = test.PassengerId\\n## We will drop PassengerID and Ticket since it will be useless for our data. \\n#train.drop(['PassengerId'], axis=1, inplace=True)\\n#test.drop(['PassengerId'], axis=1, inplace=True)\\n\\nprint (train.info())\\nprint (\\\"*\\\"*40)\\nprint (test.info())\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"f9b2f56f-e95c-478d-aa49-3f6cb277830f\",\"_uuid\":\"b5accab7fc7471fea224dcae81683b9f3c0f617b\"},\"source\":\"It looks like, the features have unequal amount of data entries for every column and they have many different types of variables. This can happen for the following reasons...\\n* We may have missing values in our features.\\n* We may have categorical features. \\n* We may have alphanumerical or/and text features. \\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9912539a-12b5-4739-bc2c-e1cecf758dca\",\"_uuid\":\"6105e90cd8f0e8d49ae188edad65414678a7be23\"},\"source\":\"## 2b. Dealing with Missing values\\n<a id=\\\"dealwithnullvalues\\\"></a>\\n***\\n**Missing values in *train* dataset.**\"},{\"cell_type\":\"code\",\"execution_count\":15,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:08.975451Z\",\"iopub.status.busy\":\"2021-06-26T16:35:08.974927Z\",\"iopub.status.idle\":\"2021-06-26T16:35:08.98326Z\",\"shell.execute_reply\":\"2021-06-26T16:35:08.982644Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:08.975205Z\"}},\"outputs\":[],\"source\":\"# Let's write a functin to print the total percentage of the missing values.(this can be a good exercise for beginners to try to write simple functions like this.)\\ndef missing_percentage(df):\\n    \\\"\\\"\\\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\\\"\\\"\\\"\\n    total = df.isnull().sum().sort_values(ascending = False)\\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\"},{\"cell_type\":\"code\",\"execution_count\":16,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:08.985334Z\",\"iopub.status.busy\":\"2021-06-26T16:35:08.984844Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.090701Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.090116Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:08.985168Z\"}},\"outputs\":[],\"source\":\"%timeit -r2 -n10 missing_percentage(train) # setting the number of runs(-r) and/or loops (-n)\"},{\"cell_type\":\"code\",\"execution_count\":17,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.092256Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.09199Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.108063Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.107054Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.092212Z\"}},\"outputs\":[],\"source\":\"missing_percentage(train)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6e1b2b57-78b9-4021-bc53-a7681b63f97c\",\"_uuid\":\"197487867c9d099548c7d009c4a80418927be07c\"},\"source\":\"**Missing values in *test* set.**\"},{\"cell_type\":\"code\",\"execution_count\":18,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.110118Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.109653Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.206762Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.205452Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.10993Z\"}},\"outputs\":[],\"source\":\"%%timeit -r2 -n10 \\nmissing_percentage(test)\"},{\"cell_type\":\"code\",\"execution_count\":19,\"metadata\":{\"_cell_guid\":\"073ef91b-e401-47a1-9b0a-d08ad710abce\",\"_kg_hide-input\":true,\"_uuid\":\"1ec1de271f57c9435ce111261ba08c5d6e34dbcb\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.208229Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.207968Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.221423Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.220732Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.208186Z\"}},\"outputs\":[],\"source\":\"missing_percentage(test)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0217a17b-8017-4221-a664-dbbc42f7a5eb\",\"_uuid\":\"2051377dfc36cbeb9fda78cb02d5bd3a00ee2457\"},\"source\":\"We see that in both **train**, and **test** dataset have missing values. Let's make an effort to fill these missing values starting with \\\"Embarked\\\" feature. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"aaf73f0b-ec84-4da1-b424-0170691c50c8\",\"_uuid\":\"84d3c45c3a59e16ac2c887d6effe71434b2659ef\"},\"source\":\"### Embarked feature\\n***\"},{\"cell_type\":\"code\",\"execution_count\":20,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.223175Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.222681Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.230671Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.229793Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.223128Z\"}},\"outputs\":[],\"source\":\"def percent_value_counts(df, feature):\\n    \\\"\\\"\\\"This function takes in a dataframe and a column and finds the percentage of the value_counts\\\"\\\"\\\"\\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\\n    ## creating a df with th\\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\\n    ## concating percent and total dataframe\\n\\n    total.columns = [\\\"Total\\\"]\\n    percent.columns = ['Percent']\\n    return pd.concat([total, percent], axis = 1)\\n    \"},{\"cell_type\":\"code\",\"execution_count\":21,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.236974Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.236548Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.254321Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.253654Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.236929Z\"}},\"outputs\":[],\"source\":\"percent_value_counts(train, 'Embarked')\"},{\"cell_type\":\"code\",\"execution_count\":22,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.259474Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.259268Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.274228Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.27333Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.259433Z\"}},\"outputs\":[],\"source\":\"percent_value_counts(train, 'Embarked')\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"826ae31d-4bd0-45f6-8c05-8b5d12d41144\",\"_uuid\":\"174873ebdb2cd6c23777d464103afa26c0183ab2\"},\"source\":\"It looks like there are only two null values( ~ 0.22 %) in the Embarked feature, we can replace these with the mode value \\\"S\\\". However, let's dig a little deeper. \\n\\n**Let's see what are those two null values**\"},{\"cell_type\":\"code\",\"execution_count\":23,\"metadata\":{\"_cell_guid\":\"000ebdd7-ff57-48d9-91bf-a29ba79f1a1c\",\"_kg_hide-input\":true,\"_uuid\":\"6b9cb050e9dae424bb738ba9cdf3c84715887fa3\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.276102Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.275649Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.292037Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.291163Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.275879Z\"}},\"outputs\":[],\"source\":\"train[train.Embarked.isnull()]\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"306da283-fbd9-45fc-a79e-ac4a3fa7d396\",\"_uuid\":\"57a4016a0ff673cdf5716310d42d7f142d275132\"},\"source\":\"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values\"},{\"cell_type\":\"code\",\"execution_count\":24,\"metadata\":{\"_cell_guid\":\"bf257322-0c9c-4fc5-8790-87d8c94ad28a\",\"_kg_hide-input\":true,\"_uuid\":\"ad15052fe6cebe37161c6e01e33a5c083dc2b558\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.293919Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.293564Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.866643Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.865701Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.293817Z\"}},\"outputs\":[],\"source\":\"import seaborn as sns\\nimport matplotlib.pyplot as plt\\nsns.set_style('darkgrid')\\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\\nax1 = sns.boxplot(x=\\\"Embarked\\\", y=\\\"Fare\\\", hue=\\\"Pclass\\\", data=train, ax = ax[0]);\\nax2 = sns.boxplot(x=\\\"Embarked\\\", y=\\\"Fare\\\", hue=\\\"Pclass\\\", data=test, ax = ax[1]);\\nax1.set_title(\\\"Training Set\\\", fontsize = 18)\\nax2.set_title('Test Set',  fontsize = 18)\\n\\n\\n# ## Fixing legends\\n# leg_1 = ax1.get_legend()\\n# leg_1.set_title(\\\"PClass\\\")\\n# legs = leg_1.texts\\n# legs[0].set_text('Upper')\\n# legs[1].set_text('Middle')\\n# legs[2].set_text('Lower')\\n\\nfig.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0e353881-a7d7-4fbf-bfd3-874479c0a650\",\"_uuid\":\"c8a7f8c033f571d2fc8986009765ac4a78d3b6a7\"},\"source\":\"Here, in both training set and test set, the average fare closest to $80 are in the <b>C</b> Embarked values where pclass is 1. So, let's fill in the missing values as \\\"C\\\" \"},{\"cell_type\":\"code\",\"execution_count\":25,\"metadata\":{\"_cell_guid\":\"2f5f3c63-d22c-483c-a688-a5ec2a477330\",\"_kg_hide-input\":true,\"_uuid\":\"52e51ada5dfeb700bf775c66e9307d6d1e2233de\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.868523Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.868016Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.874135Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.873022Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.868249Z\"},\"scrolled\":true},\"outputs\":[],\"source\":\"## Replacing the null values in the Embarked column with the mode. \\ntrain.Embarked.fillna(\\\"C\\\", inplace=True)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"47c17b1e-9486-43da-84ad-f91014225e88\",\"_uuid\":\"44af808c1563671899ee498c9df12312c294277c\"},\"source\":\"### Cabin Feature\\n***\"},{\"cell_type\":\"code\",\"execution_count\":26,\"metadata\":{\"_cell_guid\":\"e76cd770-b498-4444-b47a-4ac6ae63193b\",\"_kg_hide-input\":true,\"_uuid\":\"b809a788784e2fb443457d7ef4ca17a896bf58b4\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.876171Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.875621Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.886193Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.885088Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.875859Z\"},\"scrolled\":true},\"outputs\":[],\"source\":\"print(\\\"Train Cabin missing: \\\" + str(train.Cabin.isnull().sum()/len(train.Cabin)))\\nprint(\\\"Test Cabin missing: \\\" + str(test.Cabin.isnull().sum()/len(test.Cabin)))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"47d450a8-0692-4403-8447-ab09d6dd0b8f\",\"_uuid\":\"e61d1e4613dd4f51970d504e93ae30c072ca9d98\"},\"source\":\"Approximately 77% of Cabin feature is missing in the training data and 78% missing on the test data. \\nWe have two choices, \\n* we can either get rid of the whole feature, or \\n* we can brainstorm a little and find an appropriate way to put them in use. For example, We may say passengers with cabin record had a higher socio-economic-status then others. We may also say passengers with cabin record were more likely to be taken into consideration when loading into the boat.\\n\\nLet's combine train and test data first and for now, will assign all the null values as **\\\"N\\\"**\"},{\"cell_type\":\"code\",\"execution_count\":27,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"8ff7b4f88285bc65d72063d7fdf8a09a5acb62d3\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.888377Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.88784Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.902296Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.901697Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.888114Z\"}},\"outputs\":[],\"source\":\"## Concat train and test into a variable \\\"all_data\\\"\\nsurvivers = train.Survived\\n\\ntrain.drop([\\\"Survived\\\"],axis=1, inplace=True)\\n\\nall_data = pd.concat([train,test], ignore_index=False)\\n\\n## Assign all the null values to N\\nall_data.Cabin.fillna(\\\"N\\\", inplace=True)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"dae4beab-8c5a-4192-a460-e9abc6f14d3e\",\"_uuid\":\"e2d84eff7cafdd68a471876b65e0ae866151d6d2\"},\"source\":\"All the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name. \"},{\"cell_type\":\"code\",\"execution_count\":28,\"metadata\":{\"_cell_guid\":\"87995359-8a77-4e38-b8bb-e9b4bdeb17ed\",\"_kg_hide-input\":true,\"_uuid\":\"c1e9e06eb7f2a6eeb1a6d69f000217e7de7d5f25\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.904181Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.903766Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.909654Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.908573Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.904014Z\"}},\"outputs\":[],\"source\":\"all_data.Cabin = [i[0] for i in all_data.Cabin]\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"Now let's look at the value counts of the cabin features and see how it looks. \"},{\"cell_type\":\"code\",\"execution_count\":29,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.91156Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.911098Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.928945Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.928025Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.911398Z\"}},\"outputs\":[],\"source\":\"percent_value_counts(all_data, \\\"Cabin\\\")\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"5e8cff0316f95162cdc9c2f3da905ad49fc548ca\"},\"source\":\"So, We still haven't done any effective work to replace the null values. Let's stop for a second here and think through how we can take advantage of some of the other features here.  \\n* We can use the average of the fare column We can use pythons ***groupby*** function to get the mean fare of each cabin letter. \"},{\"cell_type\":\"code\",\"execution_count\":30,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.930774Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.930283Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.942122Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.941067Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.930532Z\"}},\"outputs\":[],\"source\":\"all_data.groupby(\\\"Cabin\\\")['Fare'].mean().sort_values()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"8605664271220cb4a17fa1aca65207681503c9dd\"},\"source\":\"Now, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means. \"},{\"cell_type\":\"code\",\"execution_count\":31,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"a466da29f1989fa983147faf9e63d18783468567\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.943855Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.943364Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.952677Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.952057Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.943627Z\"}},\"outputs\":[],\"source\":\"def cabin_estimator(i):\\n    \\\"\\\"\\\"Grouping cabin feature by the first letter\\\"\\\"\\\"\\n    a = 0\\n    if i<16:\\n        a = \\\"G\\\"\\n    elif i>=16 and i<27:\\n        a = \\\"F\\\"\\n    elif i>=27 and i<38:\\n        a = \\\"T\\\"\\n    elif i>=38 and i<47:\\n        a = \\\"A\\\"\\n    elif i>= 47 and i<53:\\n        a = \\\"E\\\"\\n    elif i>= 53 and i<54:\\n        a = \\\"D\\\"\\n    elif i>=54 and i<116:\\n        a = 'C'\\n    else:\\n        a = \\\"B\\\"\\n    return a\\n    \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"6f56c9950206a5a8f30c39ca207dc47859b8d8a0\"},\"source\":\"Let's apply <b>cabin_estimator</b> function in each unknown cabins(cabin with <b>null</b> values). Once that is done we will separate our train and test to continue towards machine learning modeling. \"},{\"cell_type\":\"code\",\"execution_count\":32,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.95455Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.954083Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.96302Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.962357Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.95437Z\"}},\"outputs\":[],\"source\":\"with_N = all_data[all_data.Cabin == \\\"N\\\"]\\n\\nwithout_N = all_data[all_data.Cabin != \\\"N\\\"]\"},{\"cell_type\":\"code\",\"execution_count\":33,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"1c646b64c6e062656e5f727d5499266f847c4832\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.965179Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.96464Z\",\"iopub.status.idle\":\"2021-06-26T16:35:09.981536Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.980705Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.964885Z\"}},\"outputs\":[],\"source\":\"##applying cabin estimator function. \\nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\\n\\n## getting back train. \\nall_data = pd.concat([with_N, without_N], axis=0)\\n\\n## PassengerId helps us separate train and test. \\nall_data.sort_values(by = 'PassengerId', inplace=True)\\n\\n## Separating train and test from all_data. \\ntrain = all_data[:891]\\n\\ntest = all_data[891:]\\n\\n# adding saved target variable with train. \\ntrain['Survived'] = survivers\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"26d918c2-3c6b-48e8-8e2b-fc4531e8c59e\",\"_uuid\":\"05a777057d9803235a17d79b72eefe7085ebf2e5\"},\"source\":\"### Fare Feature\\n***\\nIf you have paid attention so far, you know that there is only one missing value in the fare column. Let's have it. \"},{\"cell_type\":\"code\",\"execution_count\":34,\"metadata\":{\"_cell_guid\":\"2c75f369-e781-43df-be06-32585b372a0a\",\"_kg_hide-input\":true,\"_uuid\":\"020cafd121f2e6cbed89265c993ef3d76566cd6b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:09.983259Z\",\"iopub.status.busy\":\"2021-06-26T16:35:09.982793Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.000785Z\",\"shell.execute_reply\":\"2021-06-26T16:35:09.999778Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:09.983086Z\"}},\"outputs\":[],\"source\":\"test[test.Fare.isnull()]\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0ffece2f-9df0-44e5-80cc-84894a8d0d45\",\"_uuid\":\"bce23c7620db2cde9bae8efa04b00c78819f0268\"},\"source\":\"Here, We can take the average of the **Fare** column to fill in the NaN value. However, for the sake of learning and practicing, we will try something else. We can take the average of the values where**Pclass** is ***3***, **Sex** is ***male*** and **Embarked** is ***S***\"},{\"cell_type\":\"code\",\"execution_count\":35,\"metadata\":{\"_cell_guid\":\"e742aa76-b6f8-4882-8bd6-aa10b96f06aa\",\"_kg_hide-input\":true,\"_uuid\":\"f1dc8c6c33ba7df075ee608467be2a83dc1764fd\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.002749Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.002232Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.012662Z\",\"shell.execute_reply\":\"2021-06-26T16:35:10.011431Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.00248Z\"}},\"outputs\":[],\"source\":\"missing_value = test[(test.Pclass == 3) & \\n                     (test.Embarked == \\\"S\\\") & \\n                     (test.Sex == \\\"male\\\")].Fare.mean()\\n## replace the test.fare null values with test.fare mean\\ntest.Fare.fillna(missing_value, inplace=True)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"3ff2fbe3-9858-4aad-9e33-e909d5128879\",\"_uuid\":\"e04222497a5dfd77ac07dbcacbdc10dc1732da21\"},\"source\":\"### Age Feature\\n***\\nWe know that the feature \\\"Age\\\" is the one with most missing values, let's see it in terms of percentage. \"},{\"cell_type\":\"code\",\"execution_count\":36,\"metadata\":{\"_cell_guid\":\"8ff25fb3-7a4a-4e06-b48f-a06b8d844917\",\"_kg_hide-input\":true,\"_uuid\":\"c356e8e85f53a27e44b5f28936773a289592c5eb\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.014347Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.014023Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.024214Z\",\"shell.execute_reply\":\"2021-06-26T16:35:10.023404Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.014284Z\"}},\"outputs\":[],\"source\":\"print (\\\"Train age missing value: \\\" + str((train.Age.isnull().sum()/len(train))*100)+str(\\\"%\\\"))\\nprint (\\\"Test age missing value: \\\" + str((test.Age.isnull().sum()/len(test))*100)+str(\\\"%\\\"))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"105d0bf8-dada-4499-8a41-499caf20fa81\",\"_uuid\":\"8678df259a8f4e7f85f92603f312e1df76a26589\"},\"source\":\"We will take a different approach since **~20% data in the Age column is missing** in both train and test dataset. The age variable seems to be promising for determining survival rate. Therefore, It would be unwise to replace the missing values with median, mean or mode. We will use machine learning model Random Forest Regressor to impute missing value instead of Null value. We will keep the age column unchanged for now and work on that in the feature engineering section.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"81537f22-2c69-45f2-90d3-a2a8790cb2fd\",\"_uuid\":\"84518982b94e7e811bf3560a3862f06a46f1b530\"},\"source\":\"# Part 3. Visualization and Feature Relations\\n<a id=\\\"visualization_and_feature_relations\\\" ></a>\\n***\\nBefore we dive into finding relations between independent variables and our dependent variable(survivor), let us create some assumptions about how the relations may turn-out among features.\\n\\n**Assumptions:**\\n- Gender: More female survived than male\\n- Pclass: Higher socio-economic status passenger survived more than others. \\n- Age: Younger passenger survived more than other passengers. \\n- Fare: Passenger with higher fare survived more that other passengers. This can be quite correlated with Pclass. \\n\\n\\nNow, let's see how the features are related to each other by creating some visualizations. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"63420775-00e1-4650-a2f3-2ae6eebab23c\",\"_uuid\":\"ca8bfb1bfe4d1079635a54c8daec3399b8355749\"},\"source\":\"## 3a. Gender and Survived\\n<a id=\\\"gender_and_survived\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":37,\"metadata\":{\"_cell_guid\":\"78322e76-ccaa-4bb9-9cc2-7a3394ddfe8c\",\"_kg_hide-input\":true,\"_uuid\":\"6008755b1522e2a849b6e1ccbb7da57270293ca4\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.026025Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.025517Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.265216Z\",\"shell.execute_reply\":\"2021-06-26T16:35:10.26434Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.025965Z\"}},\"outputs\":[],\"source\":\"import seaborn as sns\\npal = {'male':\\\"green\\\", 'female':\\\"Pink\\\"}\\nsns.set(style=\\\"darkgrid\\\")\\nplt.subplots(figsize = (15,8))\\nax = sns.barplot(x = \\\"Sex\\\", \\n                 y = \\\"Survived\\\", \\n                 data=train, \\n                 palette = pal,\\n                 linewidth=5,\\n                 order = ['female','male'],\\n                 capsize = .05,\\n\\n                )\\n\\nplt.title(\\\"Survived/Non-Survived Passenger Gender Distribution\\\", fontsize = 25,loc = 'center', pad = 40)\\nplt.ylabel(\\\"% of passenger survived\\\", fontsize = 15, )\\nplt.xlabel(\\\"Sex\\\",fontsize = 15);\\n\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"fa7cb175-3c4d-4367-8b35-d3b43fb7d07d\",\"_uuid\":\"ef171de53cb343da95d1ba82ebd961b1ff1756c3\"},\"source\":\"This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived.\"},{\"cell_type\":\"code\",\"execution_count\":38,\"metadata\":{\"_cell_guid\":\"6e303476-c1ed-49bb-8b9d-14659dd5739d\",\"_kg_hide-input\":true,\"_uuid\":\"163515a4c926323f7288f385795ea7b1ea545d7a\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.267021Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.266613Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.438911Z\",\"shell.execute_reply\":\"2021-06-26T16:35:10.437974Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.266858Z\"}},\"outputs\":[],\"source\":\"pal = {1:\\\"seagreen\\\", 0:\\\"gray\\\"}\\nsns.set(style=\\\"darkgrid\\\")\\nplt.subplots(figsize = (15,8))\\nax = sns.countplot(x = \\\"Sex\\\", \\n                   hue=\\\"Survived\\\",\\n                   data = train, \\n                   linewidth=4, \\n                   palette = pal\\n)\\n\\n## Fixing title, xlabel and ylabel\\nplt.title(\\\"Passenger Gender Distribution - Survived vs Not-survived\\\", fontsize = 25, pad=40)\\nplt.xlabel(\\\"Sex\\\", fontsize = 15);\\nplt.ylabel(\\\"# of Passenger Survived\\\", fontsize = 15)\\n\\n## Fixing xticks\\n#labels = ['Female', 'Male']\\n#plt.xticks(sorted(train.Sex.unique()), labels)\\n\\n## Fixing legends\\nleg = ax.get_legend()\\nleg.set_title(\\\"Survived\\\")\\nlegs = leg.texts\\nlegs[0].set_text(\\\"No\\\")\\nlegs[1].set_text(\\\"Yes\\\")\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0835c20e-f155-4bd7-8032-895d8c8042e6\",\"_uuid\":\"bf15a586513bdde73dfa2279b739ffca040e71e4\"},\"source\":\"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \\n\\n**Summary**\\n***\\n- As we suspected, female passengers have survived at a much better rate than male passengers. \\n- It seems about right since females and children were the priority. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"2daa3614-866c-48d7-a8cb-26ee8126a806\",\"_uuid\":\"e746a4be3c0ed3d94a7a4366a5bff565c7bc9834\"},\"source\":\"## 3b. Pclass and Survived\\n<a id=\\\"pcalss_and_survived\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":39,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.441162Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.440668Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.62703Z\",\"shell.execute_reply\":\"2021-06-26T16:35:10.62605Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.440907Z\"}},\"outputs\":[],\"source\":\"temp = train[['Pclass', 'Survived', 'PassengerId']].groupby(['Pclass', 'Survived']).count().reset_index()\\ntemp_df = pd.pivot_table(temp, values = 'PassengerId', index = 'Pclass',columns = 'Survived')\\nnames = ['No', 'Yes']\\ntemp_df.columns = names\\nr = [0,1,2]\\ntotals = [i+j for i, j in zip(temp_df['No'], temp_df['Yes'])]\\nNo_s = [i / j * 100 for i,j in zip(temp_df['No'], totals)]\\nYes_s = [i / j * 100 for i,j in zip(temp_df['Yes'], totals)]\\n## Plotting\\nplt.subplots(figsize = (15,10))\\nbarWidth = 0.60\\nnames = ('Upper', 'Middle', 'Lower')\\n# Create green Bars\\nplt.bar(r, No_s, color='Red', edgecolor='white', width=barWidth)\\n# Create orange Bars\\nplt.bar(r, Yes_s, bottom=No_s, color='Green', edgecolor='white', width=barWidth)\\n\\n \\n# Custom x axis\\nplt.xticks(r, names)\\nplt.xlabel(\\\"Pclass\\\")\\nplt.ylabel('Percentage')\\n \\n# Show graphic\\nplt.show()\\n\"},{\"cell_type\":\"code\",\"execution_count\":40,\"metadata\":{\"_cell_guid\":\"93a3a621-7be8-4f28-960d-939068944d3f\",\"_kg_hide-input\":true,\"_uuid\":\"61543e636b742647f90ea778f30a178a84e50533\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.628812Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.628387Z\",\"iopub.status.idle\":\"2021-06-26T16:35:10.938152Z\",\"shell.execute_reply\":\"2021-06-26T16:35:10.937374Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.628643Z\"}},\"outputs\":[],\"source\":\"plt.subplots(figsize = (15,10))\\nsns.barplot(x = \\\"Pclass\\\", \\n            y = \\\"Survived\\\", \\n            data=train, \\n            linewidth=6,\\n            capsize = .05,\\n            errcolor='blue',\\n            errwidth = 3\\n            \\n\\n           )\\nplt.title(\\\"Passenger Class Distribution - Survived vs Non-Survived\\\", fontsize = 25, pad=40)\\nplt.xlabel(\\\"Socio-Economic class\\\", fontsize = 15);\\nplt.ylabel(\\\"% of Passenger Survived\\\", fontsize = 15);\\nnames = ['Upper', 'Middle', 'Lower']\\n#val = sorted(train.Pclass.unique())\\nval = [0,1,2] ## this is just a temporary trick to get the label right. \\nplt.xticks(val, names);\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"e2c5ce9f-9759-43b6-b286-ec771a5a64c1\",\"_uuid\":\"6faf3d5f770c23febb20cdc81cc079ed37d59959\"},\"source\":\"- It looks like ...\\n    - ~ 63% first class passenger survived titanic tragedy, while \\n    - ~ 48% second class and \\n    - ~ only  24% third class passenger survived. \\n\\n\"},{\"cell_type\":\"code\",\"execution_count\":41,\"metadata\":{\"_cell_guid\":\"f6eba487-9c63-4cd8-908a-393e2c277e45\",\"_kg_hide-input\":true,\"_uuid\":\"10867e6cb57231ae599406d827ba5e3f13ccb088\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:10.939885Z\",\"iopub.status.busy\":\"2021-06-26T16:35:10.939421Z\",\"iopub.status.idle\":\"2021-06-26T16:35:11.16284Z\",\"shell.execute_reply\":\"2021-06-26T16:35:11.161997Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:10.939834Z\"}},\"outputs\":[],\"source\":\"# Kernel Density Plot\\nfig = plt.figure(figsize=(15,8),)\\n## I have included to different ways to code a plot below, choose the one that suites you. \\nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \\n               color='gray',\\n               shade=True,\\n               label='not survived')\\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \\n               color='g',\\n               shade=True, \\n               label='survived', \\n              )\\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25, pad = 40)\\nplt.ylabel(\\\"Frequency of Passenger Survived\\\", fontsize = 15, labelpad = 20)\\nplt.xlabel(\\\"Passenger Class\\\", fontsize = 15,labelpad =20)\\n## Converting xticks into words for better understanding\\nlabels = ['Upper', 'Middle', 'Lower']\\nplt.xticks(sorted(train.Pclass.unique()), labels);\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"43ffcf43-2d0c-4033-8112-9edcca3576f1\",\"_uuid\":\"f397633bae24a35d3fbe87d1ca54023356e065f9\"},\"source\":\"This KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \\n\\n**Summary**\\n***\\nThe first class passengers had the upper hand during the tragedy. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"1cb9d740-749b-4700-b9e9-973dbcad6aab\",\"_uuid\":\"8eeb41d08ce680d51452deeb0ad054b184d67e16\"},\"source\":\"## 3c. Fare and Survived\\n<a id=\\\"fare_and_survived\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":42,\"metadata\":{\"_cell_guid\":\"cd6eb8a9-10a6-4ab8-aaec-4820df35f4c1\",\"_kg_hide-input\":true,\"_uuid\":\"85737078f0e84fe972a5ddb81b29e114fcfb54be\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:11.164195Z\",\"iopub.status.busy\":\"2021-06-26T16:35:11.163924Z\",\"iopub.status.idle\":\"2021-06-26T16:35:11.392608Z\",\"shell.execute_reply\":\"2021-06-26T16:35:11.391811Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:11.164152Z\"}},\"outputs\":[],\"source\":\"# Kernel Density Plot\\nfig = plt.figure(figsize=(15,8),)\\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25, pad = 40)\\nplt.ylabel(\\\"Frequency of Passenger Survived\\\", fontsize = 15, labelpad = 20)\\nplt.xlabel(\\\"Fare\\\", fontsize = 15, labelpad = 20);\\n\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6073f329-df80-4ab9-b99b-72e6fcdfe0c6\",\"_uuid\":\"b5eba2b28ea428114d8ffab52feef95484bd76c0\"},\"source\":\"This plot shows something impressive..\\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \\n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check...\"},{\"cell_type\":\"code\",\"execution_count\":43,\"metadata\":{\"_cell_guid\":\"bee8b01b-a56a-4762-bde0-4404a1c5ac1a\",\"_kg_hide-input\":true,\"_uuid\":\"916ab9dc56a05105afa80127d69deb9fc0095ba2\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:11.394719Z\",\"iopub.status.busy\":\"2021-06-26T16:35:11.394204Z\",\"iopub.status.idle\":\"2021-06-26T16:35:11.41273Z\",\"shell.execute_reply\":\"2021-06-26T16:35:11.411929Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:11.394447Z\"}},\"outputs\":[],\"source\":\"train[train.Fare > 280]\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"3467e2d8-315c-4223-9166-0aca54543cdd\",\"_uuid\":\"443d93fcfbad82fc611ce88e12556a6325ccd15c\"},\"source\":\"As we assumed, it looks like an outlier with a fare of $512. We sure can delete this point. However, we will keep it for now. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"95c27d94-fa65-4bf9-a855-8e5dab17704e\",\"_uuid\":\"64ff8df884805f04692dc601da1ef99527309d54\"},\"source\":\"## 3d. Age and Survived\\n<a id=\\\"age_and_survived\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":44,\"metadata\":{\"_cell_guid\":\"9eb6733b-7577-4360-8252-e6d97c78b7db\",\"_kg_hide-input\":true,\"_uuid\":\"c6a4f46a7ce0e197f72abe293b69100c29a044ca\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:11.41461Z\",\"iopub.status.busy\":\"2021-06-26T16:35:11.414164Z\",\"iopub.status.idle\":\"2021-06-26T16:35:11.66643Z\",\"shell.execute_reply\":\"2021-06-26T16:35:11.665545Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:11.414413Z\"}},\"outputs\":[],\"source\":\"# Kernel Density Plot\\nfig = plt.figure(figsize=(15,8),)\\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 25, pad = 40)\\nplt.xlabel(\\\"Age\\\", fontsize = 15, labelpad = 20)\\nplt.ylabel('Frequency', fontsize = 15, labelpad= 20);\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a9aab64c-6170-4c8d-8446-cecdc9804b55\",\"_uuid\":\"5238df80f5454d29e3793596a21fd0c00cb64a6c\"},\"source\":\"There is nothing out of the ordinary about this plot, except the very left part of the distribution. This may hint on the posibility that children and infants were the priority. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"077605b2-e9b4-4c45-8c5a-188508165f10\",\"_uuid\":\"f8245da79c5394f7665d0b5429cb2fe4c4d0b057\"},\"source\":\"## 3e. Combined Feature Relations\\n<a id='combined_feature_relations'></a>\\n***\\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. \"},{\"cell_type\":\"code\",\"execution_count\":45,\"metadata\":{\"_cell_guid\":\"924e19c4-8d58-404c-9a84-02f096269351\",\"_kg_hide-input\":true,\"_uuid\":\"71fc1c9843f789e19a5e8b2929579914d8ecdb3f\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:11.668148Z\",\"iopub.status.busy\":\"2021-06-26T16:35:11.667828Z\",\"iopub.status.idle\":\"2021-06-26T16:35:12.368731Z\",\"shell.execute_reply\":\"2021-06-26T16:35:12.367992Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:11.668097Z\"}},\"outputs\":[],\"source\":\"pal = {1:\\\"seagreen\\\", 0:\\\"gray\\\"}\\ng = sns.FacetGrid(train,size=5, col=\\\"Sex\\\", row=\\\"Survived\\\", margin_titles=True, hue = \\\"Survived\\\",\\n                  palette=pal)\\ng = g.map(plt.hist, \\\"Age\\\", edgecolor = 'white');\\ng.fig.suptitle(\\\"Survived by Sex and Age\\\", size = 25)\\nplt.subplots_adjust(top=0.90)\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"089999b4-bc44-49c6-9f86-aaaccabaa224\",\"_uuid\":\"6aac036e1b235e5b10bc6a153ed226acfce2cfcb\"},\"source\":\"Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky\"},{\"cell_type\":\"code\",\"execution_count\":46,\"metadata\":{\"_cell_guid\":\"dcc34a91-261d-4929-a4eb-5072fcaf86ce\",\"_kg_hide-input\":true,\"_uuid\":\"b2ad776bf0254be1ccf76f46a40db7960aa3db24\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:12.370419Z\",\"iopub.status.busy\":\"2021-06-26T16:35:12.369999Z\",\"iopub.status.idle\":\"2021-06-26T16:35:14.029152Z\",\"shell.execute_reply\":\"2021-06-26T16:35:14.028323Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:12.370369Z\"}},\"outputs\":[],\"source\":\"g = sns.FacetGrid(train,size=5, col=\\\"Sex\\\", row=\\\"Embarked\\\", margin_titles=True, hue = \\\"Survived\\\",\\n                  palette = pal\\n                  )\\ng = g.map(plt.hist, \\\"Age\\\", edgecolor = 'white').add_legend();\\ng.fig.suptitle(\\\"Survived by Sex and Age\\\", size = 25)\\nplt.subplots_adjust(top=0.90)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"b9b9906c-805d-438b-b72e-a57cc60d5ae8\",\"_uuid\":\"4070616f2637a720a3cb580264cfaed9235b9020\"},\"source\":\"This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \\n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\\n* The column represents Sex(left being male, right stands for female)\\n* The row represents Embarked(from top to bottom: S, C, Q)\\n***\\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \\n* Most passengers seem to be boarded on Southampton(S).\\n* More than 60% of the passengers died boarded on Southampton. \\n* More than 60% of the passengers lived boarded on Cherbourg(C).\\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \\n* There were very few females boarded on Queenstown, however, most of them survived. \"},{\"cell_type\":\"code\",\"execution_count\":47,\"metadata\":{\"_cell_guid\":\"fd9fe9e2-f7d4-4f83-9ce4-0a22160ef4fe\",\"_kg_hide-input\":true,\"_uuid\":\"f4d77506cabc7150466fa5bda64585d15814d48c\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:14.030913Z\",\"iopub.status.busy\":\"2021-06-26T16:35:14.030621Z\",\"iopub.status.idle\":\"2021-06-26T16:35:14.493248Z\",\"shell.execute_reply\":\"2021-06-26T16:35:14.49225Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:14.030867Z\"}},\"outputs\":[],\"source\":\"g = sns.FacetGrid(train, size=5,hue=\\\"Survived\\\", col =\\\"Sex\\\", margin_titles=True,\\n                palette=pal,)\\ng.map(plt.scatter, \\\"Fare\\\", \\\"Age\\\",edgecolor=\\\"w\\\").add_legend()\\ng.fig.suptitle(\\\"Survived by Sex, Fare and Age\\\", size = 25)\\nplt.subplots_adjust(top=0.85)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"1c309d4b-3e24-406b-bd28-d5055a660f16\",\"_uuid\":\"90bbc6e6edbf6188170a4de1b38732d009f7afae\"},\"source\":\"This facet grid unveils a couple of interesting insights. Let's find out.\\n* The grid above clearly demonstrates the three outliers with Fare of over \\\\$500. At this point, I think we are quite confident that these outliers should be deleted.\\n* Most of the passengers were with in the Fare range of \\\\$100. \"},{\"cell_type\":\"code\",\"execution_count\":48,\"metadata\":{\"_cell_guid\":\"783403f6-9d3c-4a12-8505-cf321bd1a1ef\",\"_kg_hide-input\":true,\"_uuid\":\"75c41c85dc76c9749e5c417e1ed0425eed9c55e0\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:14.495102Z\",\"iopub.status.busy\":\"2021-06-26T16:35:14.494676Z\",\"iopub.status.idle\":\"2021-06-26T16:35:14.888836Z\",\"shell.execute_reply\":\"2021-06-26T16:35:14.887825Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:14.494921Z\"}},\"outputs\":[],\"source\":\"## dropping the three outliers where Fare is over $500 \\ntrain = train[train.Fare < 500]\\n## factor plot\\nsns.factorplot(x = \\\"Parch\\\", y = \\\"Survived\\\", data = train,kind = \\\"point\\\",size = 8)\\nplt.title(\\\"Factorplot of Parents/Children survived\\\", fontsize = 25)\\nplt.subplots_adjust(top=0.85)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"33916321-237d-4381-990f-0faa11723c20\",\"_uuid\":\"263113f38121c9e5f14247f05c262ee218be87f2\"},\"source\":\"**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**\"},{\"cell_type\":\"code\",\"execution_count\":49,\"metadata\":{\"_cell_guid\":\"f6ed143e-3e02-4e97-a255-73807018f0d1\",\"_kg_hide-input\":true,\"_uuid\":\"4ce5a4a6cff3966ac1811ee95f81c81fe4861a51\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:14.890716Z\",\"iopub.status.busy\":\"2021-06-26T16:35:14.890276Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.244771Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.243687Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:14.890522Z\"}},\"outputs\":[],\"source\":\"sns.factorplot(x =  \\\"SibSp\\\", y = \\\"Survived\\\", data = train,kind = \\\"point\\\",size = 8)\\nplt.title('Factorplot of Sibilings/Spouses survived', fontsize = 25)\\nplt.subplots_adjust(top=0.85)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ee5b61b4-67d3-46b4-847d-4b5b85a8c791\",\"_uuid\":\"c7a045b78e6b5f45ad891cf0515a6a4b2534d2ff\"},\"source\":\"**While, passenger who traveled in small groups with sibilings/spouses had better changes of survivint than other passengers.**\"},{\"cell_type\":\"code\",\"execution_count\":50,\"metadata\":{\"_cell_guid\":\"50a0920d-556b-4439-a67f-384ce793d094\",\"_kg_hide-input\":true,\"_uuid\":\"dfe723c71d4d29f599701d806ca97cd01a60142f\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.246815Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.246286Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.256239Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.255215Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.246539Z\"}},\"outputs\":[],\"source\":\"# Placing 0 for female and \\n# 1 for male in the \\\"Sex\\\" column. \\ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \\\"female\\\" else 1)\\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \\\"female\\\" else 1)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"003a7779-5966-45f8-a711-67e67234a654\",\"_uuid\":\"402cd49464156ead61d5dd5698ffeb00eb71d0d3\"},\"source\":\"# Part 4: Statistical Overview\\n<a id=\\\"statisticaloverview\\\"></a>\\n***\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"91eba73b-f744-478b-bd6b-13da6cff000b\",\"_uuid\":\"3e8b752c8963a76a86c8b1db80783c644090bdfa\"},\"source\":\"![title](https://cdn-images-1.medium.com/max/400/1*hFJ-LI7IXcWpxSLtaC0dfg.png)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7b7e6e77-50bf-469f-b92b-73056224bc61\",\"_uuid\":\"797aa171f2e13ea965cb9a352fcfd2001e119747\"},\"source\":\"**Train info**\"},{\"cell_type\":\"code\",\"execution_count\":51,\"metadata\":{\"_cell_guid\":\"ad856ee6-b1ec-445d-92b0-cd6a83d58301\",\"_kg_hide-input\":true,\"_uuid\":\"35fc657641cc24aff89ade7d83d8b92e472dc3e6\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.258807Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.258212Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.297787Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.297008Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.258485Z\"}},\"outputs\":[],\"source\":\"train.describe()\"},{\"cell_type\":\"code\",\"execution_count\":52,\"metadata\":{\"_cell_guid\":\"327c6775-9ba4-4d65-8c97-304cc9512e6a\",\"_kg_hide-input\":true,\"_uuid\":\"2f9f5fb7bade4d82d7b5a564a8ac91123b4921d2\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.299834Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.299339Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.331049Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.330129Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.29957Z\"}},\"outputs\":[],\"source\":\"train.describe(include =['O'])\"},{\"cell_type\":\"code\",\"execution_count\":53,\"metadata\":{\"_cell_guid\":\"5b817552-ecb8-4f6e-9950-6697d4c44d1f\",\"_kg_hide-input\":true,\"_uuid\":\"c88dcae6209f02226f2e772b42616b5650d108f4\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.33335Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.332779Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.352959Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.35204Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.333034Z\"}},\"outputs\":[],\"source\":\"# Overview(Survived vs non survied)\\nsurvived_summary = train.groupby(\\\"Survived\\\")\\nsurvived_summary.mean().reset_index()\"},{\"cell_type\":\"code\",\"execution_count\":54,\"metadata\":{\"_cell_guid\":\"502dd0d2-a51a-47da-904c-66c9840a1b74\",\"_kg_hide-input\":true,\"_uuid\":\"65f9a660b942a8f92db94fe8fc41ccfa76a354cd\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.354847Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.354359Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.371166Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.37032Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.354612Z\"}},\"outputs\":[],\"source\":\"survived_summary = train.groupby(\\\"Sex\\\")\\nsurvived_summary.mean().reset_index()\"},{\"cell_type\":\"code\",\"execution_count\":55,\"metadata\":{\"_cell_guid\":\"68cb2dac-6295-44d6-8aa0-5cddb53dd72c\",\"_kg_hide-input\":true,\"_uuid\":\"e49170e6e56329f68aba07a36389883ee1bee5ca\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.373155Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.372618Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.392055Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.391506Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.373092Z\"}},\"outputs\":[],\"source\":\"survived_summary = train.groupby(\\\"Pclass\\\")\\nsurvived_summary.mean().reset_index()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"89ba2894-b129-4709-913d-f8cb35815925\",\"_uuid\":\"e310c182f3541069329efcdd37373235fb144567\"},\"source\":\"I have gathered a small summary from the statistical overview above. Let's see what they are...\\n- This train data set has 891 raw and 9 columns. \\n- only 38% passenger survived during that tragedy.\\n- ~74% female passenger survived, while only ~19% male passenger survived. \\n- ~63% first class passengers survived, while only 24% lower class passenger survived.\\n\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5454218c-0a45-4a89-96fe-83d89b588183\",\"_uuid\":\"d00b4e471e863f766c4aad7b88e1e6d9e57d6423\"},\"source\":\"## 4a. Correlation Matrix and Heatmap\\n<a id=\\\"heatmap\\\"></a>\\n***\\n### Correlations\"},{\"cell_type\":\"code\",\"execution_count\":56,\"metadata\":{\"_cell_guid\":\"d0acfa7a-6f3e-4783-925d-6e443a9a5baa\",\"_kg_hide-input\":true,\"_uuid\":\"c4057023aa30d3ce1befae168c00f3cb8491804b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.393816Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.393405Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.406432Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.405713Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.393654Z\"}},\"outputs\":[],\"source\":\"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"92a69940-78f8-4139-a9a7-24ccf5f6afe7\",\"_uuid\":\"211c5e2e817f4b10e64a28f5f8ce1d7eec9761fc\"},\"source\":\"** Sex is the most important correlated feature with *Survived(dependent variable)* feature followed by Pclass.** \"},{\"cell_type\":\"code\",\"execution_count\":57,\"metadata\":{\"_cell_guid\":\"3e9fdd2e-f081-48ad-9c0f-afa475b15dfe\",\"_kg_hide-input\":true,\"_uuid\":\"c3212c222341c250aacee47c43b1a023b9b65857\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.408424Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.407893Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.421826Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.42092Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.408231Z\"}},\"outputs\":[],\"source\":\"## get the most important variables. \\ncorr = train.corr()**2\\ncorr.Survived.sort_values(ascending=False)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"f5f257ef-88b1-4302-ad41-d90892fbe4e9\",\"_uuid\":\"1837acd3898d4787c9011e353dfc4dc15fd1abb2\"},\"source\":\"\\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** \"},{\"cell_type\":\"code\",\"execution_count\":58,\"metadata\":{\"_cell_guid\":\"eee23849-a390-4d16-a8df-d29c6f575413\",\"_kg_hide-input\":true,\"_uuid\":\"285660c315b854497fe00847d051ceac5c9ec298\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.423924Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.423431Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.769867Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.768898Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.423681Z\"}},\"outputs\":[],\"source\":\"## heatmeap to see the correlation between features. \\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\\nimport numpy as np\\nmask = np.zeros_like(train.corr(), dtype=np.bool)\\nmask[np.triu_indices_from(mask)] = True\\nsns.set_style('whitegrid')\\nplt.subplots(figsize = (15,12))\\nsns.heatmap(train.corr(), \\n            annot=True,\\n            mask = mask,\\n            cmap = 'RdBu', ## in order to reverse the bar replace \\\"RdBu\\\" with \\\"RdBu_r\\\"\\n            linewidths=.9, \\n            linecolor='white',\\n            fmt='.2g',\\n            center = 0,\\n            square=True)\\nplt.title(\\\"Correlations Among Features\\\", y = 1.03,fontsize = 20, pad = 40);\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0e0b5ceb-fde5-40a7-b33b-b44e8f04189a\",\"_uuid\":\"41e2bc2eff5699b14a0f47d5bd2e428ee5bec3b8\"},\"source\":\"#### Positive Correlation Features:\\n- Fare and Survived: 0.26\\n\\n#### Negative Correlation Features:\\n- Fare and Pclass: -0.6\\n- Sex and Survived: -0.55\\n- Pclass and Survived: -0.33\\n\\n\\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\\n\\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"85faf680-5f78-414f-87b9-b72ef6d6ffc2\",\"_uuid\":\"18c908fdbe16ae939827ec12a4ce028094a8a587\"},\"source\":\"## 4b. Statistical Test for Correlation\\n<a id=\\\"statistical_test\\\"></a>\\n***\\n\\nStatistical tests are the scientific way to prove the validation of theories. In any case, when we look at the data, we seem to have an intuitive understanding of where data is leading us. However, when we do statistical tests, we get a scientific or mathematical perspective of how significant these results are. Let's apply some of these methods and see how we are doing with our predictions.\\n\\n###  Hypothesis Testing Outline\\n\\nA hypothesis test compares the mean of a control group and experimental group and tries to find out whether the two sample means are different from each other and if they are different, how significant that difference is.\\n \\nA **hypothesis test** usually consists of multiple parts: \\n\\n1. Formulate a well-developed research problem or question: The hypothesis test usually starts with a concrete and well-developed researched problem. We need to ask the right question that can be answered using statistical analysis. \\n2. **The null hypothesis($H_0$) and Alternating hypothesis($H_1$)**:\\n> * The **null hypothesis($H_0$)** is something that is assumed to be true. It is the status quo. In a null hypothesis, the observations are the result of pure chance. When we set out to experiment, we form the null hypothesis by saying that there is no difference between the means of the control group and the experimental group.\\n> *  An **Alternative hypothesis($H_A$)** is a claim and the opposite of the null hypothesis.  It is going against the status quo. In an alternative theory, the observations show a real effect combined with a component of chance variation.\\n    \\n3. Determine the **test statistic**: test statistic can be used to assess the truth of the null hypothesis. Depending on the standard deviation we either use t-statistics or z-statistics. In addition to that, we want to identify whether the test is a one-tailed test or two-tailed test. [This](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article explains it pretty well. [This](https://stattrek.com/hypothesis-test/hypothesis-testing.aspx) article is pretty good as well. \\n\\n4. Specify a **Significance level** and **Confidence Interval**: The significance level($\\\\alpha$) is the probability of rejecting a null hypothesis when it is true. In other words, we are ***comfortable/confident*** with rejecting the null hypothesis a significant amount of times even though it is true. This considerable amount is our Significant level. In addition to that, Significance level is one minus our Confidence interval. For example, if we say, our significance level is 5%, then our confidence interval would be (1 - 0.05) = 0.95 or 95%. \\n\\n5. Compute the **T-Statistics/Z-Statistics**: Computing the t-statistics follows a simple equation. This equation slightly differs depending on one sample test or two sample test  \\n\\n6. Compute the **P-value**: P-value is the probability that a test statistic at least as significant as the one observed would be obtained assuming that the null hypothesis is correct. The p-value is known to be unintuitive, and even many professors are known to explain it wrong. I think this [video](https://www.youtube.com/watch?v=E4KCfcVwzyw) explains the p-value well. **The smaller the P-value, the stronger the evidence against the null hypothesis.**\\n\\n7. **Describe the result and compare the p-value with the significance value($\\\\alpha$)**: If p<=$\\\\alpha$, then the observed effect is statistically significant, the null hypothesis is ruled out, and the alternative hypothesis is valid. However if the p> $\\\\alpha$, we say that, we fail to reject the null hypothesis. Even though this sentence is grammatically wrong, it is logically right. We never accept the null hypothesis just because we are doing the statistical test with sample data points.\\n\\nWe will follow each of these steps above to do your hypothesis testing below.\\n\\nP.S. Khan Academy has a set of videos that I think are intuative and helped me understand conceptually. \\n\\n***\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"f3b49278bd1b8eff8fe1b14c1506d73cf53bd859\"},\"source\":\"### Hypothesis testing for Titanic\\n#### Formulating a well developed researched question: \\nRegarding this dataset, we can formulate the null hypothesis and alternative hypothesis by asking the following questions. \\n> * **Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive?**. \\n> * **Is there a substantial difference in the survival rate between the male and female passengers?**\\n\\n\\n#### The Null Hypothesis and The Alternative Hypothesis:\\nWe can formulate our hypothesis by asking questions differently. However, it is essential to understand what our end goal is. Here our dependent variable or target variable is **Survived**. Therefore, we say\\n\\n> ** Null Hypothesis($H_0$):** There is no difference in the survival rate between the male and female passengers. or the mean difference between male and female passenger in the survival rate is zero.  \\n>  ** Alternative Hypothesis($H_A$):** There is a difference in the survival rate between the male and female passengers. or the mean difference in the survival rate between male and female is not zero.\\n\\n\\nOnc thing we can do is try to set up the Null and Alternative Hypothesis in such way that, when we do our t-test, we can choose to do one tailed test. According to [this](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article, one-tailed tests are more powerful than two-tailed test. In addition to that, [this](https://www.youtube.com/watch?v=5NcMFlrnYp8&list=PLIeGtxpvyG-LrjxQ60pxZaimkaKKs0zGF) video is also quite helpful understanding these topics. with this in mind we can update/modify our null and alternative hypothesis. Let's see how we can rewrite this..\\n\\n> **Null Hypothesis(H0):** male mean is greater or equal to female mean.\\n\\n> **Alternative Hypothesis(H1):** male mean is less than female mean. \\n\\n#### Determine the test statistics:\\n> This will be a two-tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. \\n> Since we do not know the standard deviation($\\\\sigma$) and n is small, we will use the t-distribution. \\n\\n#### Specify the significance level:\\n> Specifying a significance level is an important step of the hypothesis test. It is an ultimate balance between type 1 error and type 2 error. We will discuss more in-depth about those in another lesson. For now, we have decided to make our significance level($\\\\alpha$) = 0.05. So, our confidence interval or non-rejection region would be (1 - $\\\\alpha$)=(1-0.05) = 95%. \\n\\n#### Computing T-statistics and P-value:\\nLet's take a random sample and see the difference.\"},{\"cell_type\":\"code\",\"execution_count\":59,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"abd034cffc591bf1ef2b4a8ed3e5a65eb133d61e\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.771771Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.771345Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.783362Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.782301Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.771603Z\"}},\"outputs\":[],\"source\":\"male_mean = train[train['Sex'] == 1].Survived.mean()\\n\\nfemale_mean = train[train['Sex'] == 0].Survived.mean()\\nprint (\\\"Male survival mean: \\\" + str(male_mean))\\nprint (\\\"female survival mean: \\\" + str(female_mean))\\n\\nprint (\\\"The mean difference between male and female survival rate: \\\" + str(female_mean - male_mean))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"0c1c27af262ba094ff1fd02867b1a41d5369720f\"},\"source\":\"Now, we have to understand that those two means are not  **the population mean ($\\\\bar{\\\\mu}$)**.  *The population mean is a statistical term statistician uses to indicate the actual average of the entire group. The group can be any gathering of multiple numbers such as animal, human, plants, money, stocks.* For example, To find the age population mean of Bulgaria; we will have to account for every single person's age and take their age. Which is almost impossible and if we were to go that route; there is no point of doing statistics in the first place. Therefore we approach this problem using sample sets. The idea of using sample set is that; if we take multiple samples of the same population and take the mean of them and put them in a distribution; eventually the distribution start to look more like a **normal distribution**. The more samples we take and the more sample means will be added and, the closer the normal distribution will reach towards population mean. This is where **Central limit theory** comes from. We will go more in depth of this topic later on. \\n\\nGoing back to our dataset, like we are saying these means above are part of the whole story. We were given part of the data to train our machine learning models, and the other part of the data was held back for testing. Therefore, It is impossible for us at this point to know the population means of survival for male and females. Situation like this calls for a statistical approach. We will use the sampling distribution approach to do the test. let's take 50 random sample of male and female from our train data.\"},{\"cell_type\":\"code\",\"execution_count\":60,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.785359Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.784861Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.815921Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.815302Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.785103Z\"}},\"outputs\":[],\"source\":\"# separating male and female dataframe. \\nimport random\\nmale = train[train['Sex'] == 1]\\nfemale = train[train['Sex'] == 0]\\n\\n## empty list for storing mean sample\\nm_mean_samples = []\\nf_mean_samples = []\\n\\nfor i in range(50):\\n    m_mean_samples.append(np.mean(random.sample(list(male['Survived']),50,)))\\n    f_mean_samples.append(np.mean(random.sample(list(female['Survived']),50,)))\\n    \\n\\n# Print them out\\nprint (f\\\"Male mean sample mean: {round(np.mean(m_mean_samples),2)}\\\")\\nprint (f\\\"Male mean sample mean: {round(np.mean(f_mean_samples),2)}\\\")\\nprint (f\\\"Difference between male and female mean sample mean: {round(np.mean(f_mean_samples) - np.mean(m_mean_samples),2)}\\\")\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"H0: male mean is greater or equal to female mean<br>\\nH1: male mean is less than female mean. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"706d89356793f306d807c3fb277963e07181915c\"},\"source\":\"According to the samples our male samples ($\\\\bar{x}_m$) and female samples($\\\\bar{x}_f$) mean measured difference is ~ 0.55(statistically this is called the point estimate of the male population mean and female population mean). keeping in mind that...\\n* We randomly select 50 people to be in the male group and 50 people to be in the female group. \\n* We know our sample is selected from a broader population(trainning set). \\n* We know we could have totally ended up with a different random sample of males and females.\\n***\\nWith all three points above in mind, how confident are we that, the measured difference is real or statistically significant? we can perform a **t-test** to evaluate that. When we perform a **t-test** we are usually trying to find out **an evidence of significant difference between population mean with hypothesized mean(1 sample t-test) or in our case difference between two population means(2 sample t-test).** \\n\\n\\n\\nThe **t-statistics** is the measure of a degree to which our groups differ standardized by the variance of our measurements. In order words, it is basically the measure of signal over noise. Let us describe the previous sentence a bit more for clarification. I am going to use [this post](http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen) as reference to describe the t-statistics here. \\n\\n\\n#### Calculating the t-statistics\\n# $$t = \\\\frac{\\\\bar{x}-\\\\mu}{\\\\frac{S} {\\\\sqrt{n}} }$$\\n\\nHere..\\n* $\\\\bar{x}$ is the sample mean. \\n* $\\\\mu$ is the hypothesized mean. \\n* S is the standard deviation. \\n* n is the sample size. \\n\\n\\n1. Now, the denominator of this fraction $(\\\\bar{x}-\\\\mu)$ is basically the strength of the signal. where we calculate the difference between hypothesized mean and sample mean. If the mean difference is higher, then the signal is stronger. \\n\\nthe numerator of this fraction ** ${S}/ {\\\\sqrt{n}}$ ** calculates the amount of variation or noise of the data set. Here S is standard deviation, which tells us how much variation is there in the data. n is the sample size. \\n\\nSo, according to the explanation above, the t-value or t-statistics is basically measures the strength of the signal(the difference) to the amount of noise(the variation) in the data and that is how we calculate the t-value in one sample t-test. However, in order to calculate between two sample population mean or in our case we will use the follow equation. \\n\\n# $$t = \\\\frac{\\\\bar{x}_M - \\\\bar{x}_F}{\\\\sqrt {s^2 (\\\\frac{1}{n_M} + \\\\frac{1}{n_F})}}$$\\n\\nThis equation may seem too complex, however, the idea behind these two are similar. Both of them have the concept of signal/noise. The only difference is that we replace our hypothesis mean with another sample mean and the two sample sizes repalce one sample size. \\n\\nHere..\\n* $\\\\bar{x}_M$ is the mean of our male group sample measurements. \\n* $ \\\\bar{x}_F$ is the mean of female group samples. \\n* $ n_M$ and $n_F$ are the sample number of observations in each group. \\n* $ S^2$ is the sample variance.\\n\\nIt is good to have an understanding of what going on in the background. However, we will use **scipy.stats** to find the t-statistics. \\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"44e9000aefed8ea0125463486cc4a00c17e580e5\"},\"source\":\"#### Compare P-value with $\\\\alpha$\\n> It looks like the p-value is very small compared to our significance level($\\\\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is \\\"**There is a significant difference in the survival rate between the male and female passengers.\\\"**\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"df06b6c8-daf6-4f5b-af51-9c1dfbac7a68\",\"_uuid\":\"34869ce4ce852633b1f4a5cd111b98841982cc19\"},\"source\":\"# Part 5: Feature Engineering\\n<a id=\\\"feature_engineering\\\"></a>\\n***\\nFeature Engineering is exactly what its sounds like. Sometimes we want to create extra features from with in the features that we have, sometimes we want to remove features that are alike. Features engineering is the simple word for doing all those. It is important to remember that we will create new features in such ways that will not cause **multicollinearity(when there is a relationship among independent variables)** to occur. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8c439069-6168-4cda-846f-db4c21265089\",\"_uuid\":\"3ca0785fe824c6ea471b2bcf9600007ed238d450\"},\"source\":\"## name_length\\n***Creating a new feature \\\"name_length\\\" that will take the count of letters of each name***\"},{\"cell_type\":\"code\",\"execution_count\":61,\"metadata\":{\"_cell_guid\":\"d30d71c1-55bc-41c8-8536-9909d9f02538\",\"_kg_hide-input\":true,\"_uuid\":\"cb17c6f59bb2123cbf2cbc9c282b4d70ee283a86\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.817993Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.817477Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.832377Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.831471Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.817745Z\"}},\"outputs\":[],\"source\":\"# Creating a new colomn with a \\ntrain['name_length'] = [len(i) for i in train.Name]\\ntest['name_length'] = [len(i) for i in test.Name]\\n\\ndef name_length_group(size):\\n    a = ''\\n    if (size <=20):\\n        a = 'short'\\n    elif (size <=35):\\n        a = 'medium'\\n    elif (size <=45):\\n        a = 'good'\\n    else:\\n        a = 'long'\\n    return a\\n\\n\\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\\ntest['nLength_group'] = test['name_length'].map(name_length_group)\\n\\n## Here \\\"map\\\" is python's built-in function. \\n## \\\"map\\\" function basically takes a function and \\n## returns an iterable list/tuple or in this case series. \\n## However,\\\"map\\\" can also be used like map(function) e.g. map(name_length_group) \\n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \\n## However, here we don't need to use parameter(\\\"size\\\") for name_length_group because when we \\n## used the map function like \\\".map\\\" with a series before dot, we are basically hinting that series \\n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \\n\\n\\n## cuts the column by given bins based on the range of name_length\\n#group_names = ['short', 'medium', 'good', 'long']\\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"012489c507bf8bfb1ca3db9b0506493cf5595e61\"},\"source\":\"## title\\n**Getting the title of each name as a new feature. **\"},{\"cell_type\":\"code\",\"execution_count\":62,\"metadata\":{\"_cell_guid\":\"ded64d5f-43de-4a9e-b9c5-ec4d2869387a\",\"_kg_hide-input\":true,\"_uuid\":\"9c23229f7d06a1303a04b4a81c927453686ffec9\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.833953Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.833501Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.842414Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.841468Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.83376Z\"}},\"outputs\":[],\"source\":\"## get the title from the name\\ntrain[\\\"title\\\"] = [i.split('.')[0] for i in train.Name]\\ntrain[\\\"title\\\"] = [i.split(',')[1] for i in train.title]\\n## Whenever we split like that, there is a good change that we will end up with while space around our string values. Let's check that. \"},{\"cell_type\":\"code\",\"execution_count\":63,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.84422Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.843818Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.853522Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.852642Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.84407Z\"}},\"outputs\":[],\"source\":\"print(train.title.unique())\"},{\"cell_type\":\"code\",\"execution_count\":64,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.855322Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.854858Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.86306Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.86222Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.855101Z\"}},\"outputs\":[],\"source\":\"## Let's fix that\\ntrain.title = train.title.apply(lambda x: x.strip())\"},{\"cell_type\":\"code\",\"execution_count\":65,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.864826Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.864362Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.872663Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.871817Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.864612Z\"}},\"outputs\":[],\"source\":\"## We can also combile all three lines above for test set here\\ntest['title'] = [i.split('.')[0].split(',')[1].strip() for i in test.Name]\\n\\n## However it is important to be able to write readable code, and the line above is not so readable. \"},{\"cell_type\":\"code\",\"execution_count\":66,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.874489Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.873918Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.896665Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.895832Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.874258Z\"}},\"outputs\":[],\"source\":\"## Let's replace some of the rare values with the keyword 'rare' and other word choice of our own. \\n## train Data\\ntrain[\\\"title\\\"] = [i.replace('Ms', 'Miss') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Mlle', 'Miss') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Mme', 'Mrs') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Dr', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Col', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Major', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Don', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Sir', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Lady', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Capt', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('the Countess', 'rare') for i in train.title]\\ntrain[\\\"title\\\"] = [i.replace('Rev', 'rare') for i in train.title]\\n\\n\\n## Now in programming there is a term called DRY(Don't repeat yourself), whenever we are repeating  \\n## same code over and over again, there should be a light-bulb turning on in our head and make us think\\n## to code in a way that is not repeating or dull. Let's write a function to do exactly what we \\n## did in the code above, only not repeating and more interesting. \"},{\"cell_type\":\"code\",\"execution_count\":67,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.900031Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.899771Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.910036Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.908929Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.899989Z\"}},\"outputs\":[],\"source\":\"## we are writing a function that can help us modify title column\\ndef name_converted(feature):\\n    \\\"\\\"\\\"\\n    This function helps modifying the title column\\n    \\\"\\\"\\\"\\n    \\n    result = ''\\n    if feature in ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col', 'Rev', 'Dona', 'Dr']:\\n        result = 'rare'\\n    elif feature in ['Ms', 'Mlle']:\\n        result = 'Miss'\\n    elif feature == 'Mme':\\n        result = 'Mrs'\\n    else:\\n        result = feature\\n    return result\\n\\ntest.title = test.title.map(name_converted)\\ntrain.title = train.title.map(name_converted)\"},{\"cell_type\":\"code\",\"execution_count\":68,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.912187Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.911644Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.923512Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.922507Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.912136Z\"}},\"outputs\":[],\"source\":\"print(train.title.unique())\\nprint(test.title.unique())\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"42ccf293-04c7-4bea-9570-4cce9227b8af\",\"_uuid\":\"e870c4fc44de4b2395963e583c84d2cae83c004b\"},\"source\":\"## family_size\\n***Creating a new feature called \\\"family_size\\\".*** \"},{\"cell_type\":\"code\",\"execution_count\":69,\"metadata\":{\"_cell_guid\":\"7083a7e7-d1d5-4cc1-ad67-c454b139f5f1\",\"_kg_hide-input\":true,\"_uuid\":\"cdfd54429cb235dd3b73535518950b2e515e54f2\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.925581Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.925033Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.933955Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.933137Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.925315Z\"}},\"outputs\":[],\"source\":\"## Family_size seems like a good feature to create\\ntrain['family_size'] = train.SibSp + train.Parch+1\\ntest['family_size'] = test.SibSp + test.Parch+1\"},{\"cell_type\":\"code\",\"execution_count\":70,\"metadata\":{\"_cell_guid\":\"3d471d07-7735-4aab-8b26-3f26e481dc49\",\"_kg_hide-input\":true,\"_uuid\":\"2e23467af7a2e85fcaa06b52b303daf2e5e44250\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.935971Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.935422Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.942647Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.941882Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.935671Z\"}},\"outputs\":[],\"source\":\"## bin the family size. \\ndef family_group(size):\\n    \\\"\\\"\\\"\\n    This funciton groups(loner, small, large) family based on family size\\n    \\\"\\\"\\\"\\n    \\n    a = ''\\n    if (size <= 1):\\n        a = 'loner'\\n    elif (size <= 4):\\n        a = 'small'\\n    else:\\n        a = 'large'\\n    return a\"},{\"cell_type\":\"code\",\"execution_count\":71,\"metadata\":{\"_cell_guid\":\"82f3cf5a-7e8d-42c3-a06b-56e17e890358\",\"_kg_hide-input\":true,\"_uuid\":\"549239812f919f5348da08db4264632d2b21b587\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.944511Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.94417Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.95416Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.953395Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.944448Z\"}},\"outputs\":[],\"source\":\"## apply the family_group function in family_size\\ntrain['family_group'] = train['family_size'].map(family_group)\\ntest['family_group'] = test['family_size'].map(family_group)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"d827a2d9-8ca5-454a-8323-90c397b25ccf\",\"_uuid\":\"3aa4ad0fac364f8f3c04e240841ee097baa3c871\"},\"source\":\"## is_alone\"},{\"cell_type\":\"code\",\"execution_count\":72,\"metadata\":{\"_cell_guid\":\"298b28d6-75a7-4e49-b1c3-7755f1727327\",\"_kg_hide-input\":true,\"_uuid\":\"45315bb62f69e94e66109e7da06c6c5ade578398\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.956031Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.955569Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.964779Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.963853Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.955855Z\"}},\"outputs\":[],\"source\":\"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"fee91907-4197-46c2-92c1-92474565e9a0\",\"_uuid\":\"0a6032d2746a7cf75e2cc899615d72433572fd6d\"},\"source\":\"## ticket\"},{\"cell_type\":\"code\",\"execution_count\":73,\"metadata\":{\"_cell_guid\":\"352c794d-728d-44de-9160-25da7abe0c06\",\"_kg_hide-input\":true,\"_uuid\":\"5b99e1f7d7757f11e6dd6dbc627f3bd6e2fbd874\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.966936Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.9664Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.97799Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.976969Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.966816Z\"}},\"outputs\":[],\"source\":\"train.Ticket.value_counts().sample(10)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"dd50f2d503d4b951bee458793dde6e23f0e35dc9\"},\"source\":\"I have yet to figureout how to best manage ticket feature. So, any suggestion would be truly appreciated. For now, I will get rid off the ticket feature.\"},{\"cell_type\":\"code\",\"execution_count\":74,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"d23d451982f0cbe44976c2eacafb726d816e9195\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.979613Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.979155Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.989456Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.988913Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.97941Z\"}},\"outputs\":[],\"source\":\"train.drop(['Ticket'], axis=1, inplace=True)\\n\\ntest.drop(['Ticket'], axis=1, inplace=True)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"60cb16dc-9bc3-4ff3-93b8-e3b3d4bcc0c8\",\"_uuid\":\"800052abc32a56c5f5f875bb3652c02e93c6b0a8\"},\"source\":\"## calculated_fare\"},{\"cell_type\":\"code\",\"execution_count\":75,\"metadata\":{\"_cell_guid\":\"adaa30fe-cb0f-4666-bf95-505f1dcce188\",\"_kg_hide-input\":true,\"_uuid\":\"9374a6357551a7551e71731d72f5ceb3144856df\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:15.991841Z\",\"iopub.status.busy\":\"2021-06-26T16:35:15.991313Z\",\"iopub.status.idle\":\"2021-06-26T16:35:15.999545Z\",\"shell.execute_reply\":\"2021-06-26T16:35:15.998734Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:15.991562Z\"}},\"outputs\":[],\"source\":\"## Calculating fare based on family size. \\ntrain['calculated_fare'] = train.Fare/train.family_size\\ntest['calculated_fare'] = test.Fare/test.family_size\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"157cec80a8138c7976b135f093fc52832b82d71e\"},\"source\":\"Some people have travelled in groups like family or friends. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger, therefore calculated fare will be much handy in this situation. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"60579ed1-9978-4d4a-aea0-79c75b6b1376\",\"_uuid\":\"c0e1c25bc6a7717646a5d0d063acae220e496e9e\"},\"source\":\"## fare_group\"},{\"cell_type\":\"code\",\"execution_count\":76,\"metadata\":{\"_cell_guid\":\"8c33b78c-14cb-4cc2-af0f-65079a741570\",\"_kg_hide-input\":true,\"_uuid\":\"35685a6ca28651eab389c4673c21da2ea5ba4187\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:16.001667Z\",\"iopub.status.busy\":\"2021-06-26T16:35:16.001088Z\",\"iopub.status.idle\":\"2021-06-26T16:35:16.012304Z\",\"shell.execute_reply\":\"2021-06-26T16:35:16.011542Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:16.00135Z\"}},\"outputs\":[],\"source\":\"def fare_group(fare):\\n    \\\"\\\"\\\"\\n    This function creates a fare group based on the fare provided\\n    \\\"\\\"\\\"\\n    \\n    a= ''\\n    if fare <= 4:\\n        a = 'Very_low'\\n    elif fare <= 10:\\n        a = 'low'\\n    elif fare <= 20:\\n        a = 'mid'\\n    elif fare <= 45:\\n        a = 'high'\\n    else:\\n        a = \\\"very_high\\\"\\n    return a\\n\\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\\n\\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"5f5072cf-2234-425e-b91d-9609971117a0\",\"_uuid\":\"907614ee16efce8cbcc32b5535648688d23271eb\"},\"source\":\"Fare group was calculated based on <i>calculated_fare</i>. This can further help our cause. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"57a333f5c225ce65ec46a7e8b3c33d78fd70752e\"},\"source\":\"## PassengerId\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"b44cc5b5f6fd4d844b85f689f3a713599915bbce\"},\"source\":\"It seems like <i>PassengerId</i> column only works as an id in this dataset without any significant effect on the dataset. Let's drop it.\"},{\"cell_type\":\"code\",\"execution_count\":77,\"metadata\":{\"_uuid\":\"dadea67801cf5b56a882aa96bb874a4afa0e0bec\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:16.014434Z\",\"iopub.status.busy\":\"2021-06-26T16:35:16.013951Z\",\"iopub.status.idle\":\"2021-06-26T16:35:16.025524Z\",\"shell.execute_reply\":\"2021-06-26T16:35:16.024631Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:16.014266Z\"}},\"outputs\":[],\"source\":\"train.drop(['PassengerId'], axis=1, inplace=True)\\n\\ntest.drop(['PassengerId'], axis=1, inplace=True)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6a494c58-c1cf-44e9-be41-f404626ab299\",\"_uuid\":\"704994b577f803ae51c5c6473a2d96f49bdd12ea\"},\"source\":\"## Creating dummy variables\\n\\nYou might be wondering what is a dummy variable? \\n\\nDummy variable is an important **prepocessing machine learning step**. Often times Categorical variables are an important features, which can be the difference between a good model and a great model. While working with a dataset, having meaningful value for example, \\\"male\\\" or \\\"female\\\" instead of 0's and 1's is more intuitive for us. However, machines do not understand the value of categorical values, for example, in this dataset we have gender male or female, algorithms do not accept categorical variables as input. In order to feed data in a machine learning model, we  \"},{\"cell_type\":\"code\",\"execution_count\":78,\"metadata\":{\"_cell_guid\":\"9243ac8c-be44-46d0-a0ca-ee5f19b89bd4\",\"_kg_hide-input\":true,\"_uuid\":\"7b8db3930fb1bfb91db16686223dfc6d8e77744d\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:16.027132Z\",\"iopub.status.busy\":\"2021-06-26T16:35:16.026701Z\",\"iopub.status.idle\":\"2021-06-26T16:35:16.059319Z\",\"shell.execute_reply\":\"2021-06-26T16:35:16.058745Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:16.027081Z\"}},\"outputs\":[],\"source\":\"\\ntrain = pd.get_dummies(train, columns=['title',\\\"Pclass\\\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\\ntest = pd.get_dummies(test, columns=['title',\\\"Pclass\\\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\\ntest.drop(['Name','family_size',\\\"Fare\\\",'name_length'], axis=1, inplace=True)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"67dc98ce-bedc-456d-bdbb-9684bbd88d66\",\"_uuid\":\"23586743d94d093f76f05a2fd3ca0ae75c0d663c\"},\"source\":\"## age\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"a519858b2df34c499bb53808a5a23592ba7af040\"},\"source\":\"As I promised before, we are going to use Random forest regressor in this section to predict the missing age values. Let's do it\"},{\"cell_type\":\"code\",\"execution_count\":79,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:16.061141Z\",\"iopub.status.busy\":\"2021-06-26T16:35:16.060714Z\",\"iopub.status.idle\":\"2021-06-26T16:35:16.084728Z\",\"shell.execute_reply\":\"2021-06-26T16:35:16.083793Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:16.060961Z\"}},\"outputs\":[],\"source\":\"train.head()\"},{\"cell_type\":\"code\",\"execution_count\":80,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"9597c320c3db4db5e5c28980a28abaae7281bc61\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:16.086463Z\",\"iopub.status.busy\":\"2021-06-26T16:35:16.086001Z\",\"iopub.status.idle\":\"2021-06-26T16:35:16.096908Z\",\"shell.execute_reply\":\"2021-06-26T16:35:16.095838Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:16.086235Z\"}},\"outputs\":[],\"source\":\"## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \\ntrain = pd.concat([train[[\\\"Survived\\\", \\\"Age\\\", \\\"Sex\\\",\\\"SibSp\\\",\\\"Parch\\\"]], train.loc[:,\\\"is_alone\\\":]], axis=1)\\ntest = pd.concat([test[[\\\"Age\\\", \\\"Sex\\\"]], test.loc[:,\\\"SibSp\\\":]], axis=1)\"},{\"cell_type\":\"code\",\"execution_count\":81,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"91662e7b63c2361fdcf3215f130b3895154ad92d\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:16.098683Z\",\"iopub.status.busy\":\"2021-06-26T16:35:16.098263Z\",\"iopub.status.idle\":\"2021-06-26T16:35:22.704889Z\",\"shell.execute_reply\":\"2021-06-26T16:35:22.704165Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:16.098504Z\"}},\"outputs\":[],\"source\":\"## Importing RandomForestRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\n\\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \\ndef completing_age(df):\\n    ## gettting all the features except survived\\n    age_df = df.loc[:,\\\"Age\\\":] \\n    \\n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\\n    \\n    y = temp_train.Age.values ## setting target variables(age) in y \\n    x = temp_train.loc[:, \\\"Sex\\\":].values\\n    \\n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\\n    rfr.fit(x, y)\\n    \\n    predicted_age = rfr.predict(temp_test.loc[:, \\\"Sex\\\":])\\n    \\n    df.loc[df.Age.isnull(), \\\"Age\\\"] = predicted_age\\n    \\n\\n    return df\\n\\n## Implementing the completing_age function in both train and test dataset. \\ncompleting_age(train)\\ncompleting_age(test);\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"8f4891f73fe40cdf20cbcdfce93bda7a4f5ccc5d\"},\"source\":\"Let's take a look at the histogram of the age column. \"},{\"cell_type\":\"code\",\"execution_count\":82,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"8fc55e4670061d46dab3cc6585b3cc71eb996868\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:22.708567Z\",\"iopub.status.busy\":\"2021-06-26T16:35:22.708283Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.194075Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.193419Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:22.708515Z\"}},\"outputs\":[],\"source\":\"## Let's look at the his\\nplt.subplots(figsize = (22,10),)\\nsns.distplot(train.Age, bins = 100, kde = True, rug = False, norm_hist=False);\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"97fcc2a4c7cdc7f998052aed543b86e113499580\"},\"source\":\"## age_group\\nWe can create a new feature by grouping the \\\"Age\\\" column\"},{\"cell_type\":\"code\",\"execution_count\":83,\"metadata\":{\"_cell_guid\":\"3140c968-6755-42ec-aa70-d30c0acede1e\",\"_kg_hide-input\":true,\"_uuid\":\"c3bd77bb4d9d5411aa696a605be127db181d2a67\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.196215Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.195696Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.219708Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.218664Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.195943Z\"}},\"outputs\":[],\"source\":\"## create bins for age\\ndef age_group_fun(age):\\n    \\\"\\\"\\\"\\n    This function creates a bin for age\\n    \\\"\\\"\\\"\\n    a = ''\\n    if age <= 1:\\n        a = 'infant'\\n    elif age <= 4: \\n        a = 'toddler'\\n    elif age <= 13:\\n        a = 'child'\\n    elif age <= 18:\\n        a = 'teenager'\\n    elif age <= 35:\\n        a = 'Young_Adult'\\n    elif age <= 45:\\n        a = 'adult'\\n    elif age <= 55:\\n        a = 'middle_aged'\\n    elif age <= 65:\\n        a = 'senior_citizen'\\n    else:\\n        a = 'old'\\n    return a\\n        \\n## Applying \\\"age_group_fun\\\" function to the \\\"Age\\\" column.\\ntrain['age_group'] = train['Age'].map(age_group_fun)\\ntest['age_group'] = test['Age'].map(age_group_fun)\\n\\n## Creating dummies for \\\"age_group\\\" feature. \\ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<div class=\\\"alert alert-danger\\\">\\n<h1>Need to paraphrase this section</h1>\\n<h2>Feature Selection</h2>\\n<h3>Feature selection is an important part of machine learning models. There are many reasons why we use feature selection.</h3> \\n<ul>\\n    <li>Simple models are easier to interpret. People who acts according to model results have a better understanding of the model.</li>\\n    <li>Shorter training times.</li>\\n    <li>Enhanced generalisation by reducing overfitting. </li>\\n    <li>Easier to implement by software developers> model production.</li>\\n        <ul>\\n            <li>As Data Scientists we need to remember no to creating models with too many variables since it might overwhelm production engineers.</li>\\n    </ul>\\n    <li>Reduced risk of data errors during model use</li>\\n    <li>Data redundancy</li>\\n</ul>\\n</div>\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9de7bf55-edfb-42e0-a235-7fee883001d9\",\"_uuid\":\"8eb07418adf26340ec68fa41401e68d08603f6d4\"},\"source\":\"# Part 6: Pre-Modeling Tasks\\n## 6a. Separating dependent and independent variables\\n<a id=\\\"dependent_independent\\\"></a>\\n***\\nBefore we apply any machine learning models, It is important to separate dependent and independent variables. Our dependent variable or target variable is something that we are trying to find, and our independent variable is the features we use to find the dependent variable. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. To specify them, we need to separate them from each other, and the code below does just that.\\n\\nP.S. In our test dataset, we do not have a dependent variable feature. We are to predict that using machine learning models. \"},{\"cell_type\":\"code\",\"execution_count\":84,\"metadata\":{\"_cell_guid\":\"dcb0934f-8e3f-40b6-859e-abf70b0b074e\",\"_kg_hide-input\":true,\"_uuid\":\"607db6be6dfacc7385e5adcc0feeee28c50c99c5\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.221875Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.221297Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.229845Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.228853Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.221578Z\"}},\"outputs\":[],\"source\":\"# separating our independent and dependent variable\\nX = train.drop(['Survived'], axis = 1)\\ny = train[\\\"Survived\\\"]\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"042502ae-2714-43e2-9e33-6705b1aa781a\",\"_uuid\":\"92001d23ce79265c0f7d2b3d6f67094feeec2ea7\"},\"source\":\"## 6b. Splitting the training data\\n<a id=\\\"split_training_data\\\" ></a>\\n***\\nThere are multiple ways of splitting data. They are...\\n* train_test_split.\\n* cross_validation. \\n\\nWe have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two-thirds of the train data. Once we train our algorithm using 2/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, **X_train**, **X_test**, **y_train**, **y_test**.  \\n* **X_train** and **y_train** first used to train the algorithm. \\n* then, **X_test** is used in that trained algorithms to predict **outcomes. **\\n* Once we get the **outcomes**, we compare it with **y_test**\\n\\nBy comparing the **outcome** of the model with **y_test**, we can determine whether our algorithms are performing well or not. As we compare we use confusion matrix to determine different aspects of model performance.\\n\\nP.S. When we use cross validation it is important to remember not to use **X_train, X_test, y_train and y_test**, rather we will use **X and y**. I will discuss more on that. \"},{\"cell_type\":\"code\",\"execution_count\":85,\"metadata\":{\"_cell_guid\":\"348a5be2-5f4f-4c98-93a3-7352b6060ef4\",\"_kg_hide-input\":true,\"_uuid\":\"41b70e57f8e03da9910c20af89a9fa4a2aaea85b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.231964Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.23135Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.240022Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.239414Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.231633Z\"}},\"outputs\":[],\"source\":\"from sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)\"},{\"cell_type\":\"code\",\"execution_count\":86,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.242734Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.24208Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.250654Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.249893Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.242373Z\"}},\"outputs\":[],\"source\":\"len(X_train)\"},{\"cell_type\":\"code\",\"execution_count\":87,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.260997Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.260779Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.265643Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.264688Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.260954Z\"}},\"outputs\":[],\"source\":\"len(X_test)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"1f920690-2084-498c-a2fa-e618ad2228d8\",\"_uuid\":\"75407683b262fb65fc4afdfca6084d4ddaebe9a9\"},\"source\":\"## 6c. Feature Scaling\\n<a id=\\\"feature_scaling\\\" ></a>\\n***\\nFeature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit. For some machine learning models, it is not a problem. However, for many other ones, its quite a problem. Many machine learning algorithms uses euclidian distances to calculate the distance between two points, it is quite a problem. Let's again look at a the sample of the **train** dataset below.\"},{\"cell_type\":\"code\",\"execution_count\":88,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"d788baa4b88106afe5b30c769a6c85a1d67a5d6c\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.26761Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.267136Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.295264Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.294322Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.267383Z\"}},\"outputs\":[],\"source\":\"train.sample(5)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"3d213fdd45a46ea0cf060adc7d9af58a84a03e21\"},\"source\":\"Here **Age** and **Calculated_fare** is much higher in magnitude compared to others machine learning features. This can create problems as many machine learning models will get confused thinking **Age** and **Calculated_fare** have higher weight than other features. Therefore, we need to do feature scaling to get a better result. \\nThere are multiple ways to do feature scaling. \\n<ul>\\n    <li><b>MinMaxScaler</b>-Scales the data using the max and min values so that it fits between 0 and 1.</li>\\n    <li><b>StandardScaler</b>-Scales the data so that it has mean 0 and variance of 1.</li>\\n    <li><b>RobustScaler</b>-Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.</b>\\n </ul>\\nI will discuss more on that in a different kernel. For now we will use <b>Standard Scaler</b> to feature scale our dataset. \\n\\nP.S. I am showing a sample of both before and after so that you can see how scaling changes the dataset. \"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"2bf3db75976f363c0e922b0b7843716f900e0fd9\"},\"source\":\"<h3><font color=\\\"$5831bc\\\" face=\\\"Comic Sans MS\\\">Before Scaling</font></h3>\"},{\"cell_type\":\"code\",\"execution_count\":89,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"c4011a767b1d846f2866b4573d1d6d116afe8427\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.297022Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.296548Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.319251Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.318338Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.296792Z\"}},\"outputs\":[],\"source\":\"headers = X_train.columns \\n\\nX_train.head()\"},{\"cell_type\":\"code\",\"execution_count\":90,\"metadata\":{\"_cell_guid\":\"5c89c54b-7f5a-4e31-9e8f-58726cef5eab\",\"_kg_hide-input\":true,\"_uuid\":\"182b849ba7f2b311e919cdbf83970b97736e9d98\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.320979Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.320476Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.331478Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.33067Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.320738Z\"}},\"outputs\":[],\"source\":\"# Feature Scaling\\n## We will be using standardscaler to transform\\nfrom sklearn.preprocessing import StandardScaler\\nst_scale = StandardScaler()\\n\\n## transforming \\\"train_x\\\"\\nX_train = st_scale.fit_transform(X_train)\\n## transforming \\\"test_x\\\"\\nX_test = st_scale.transform(X_test)\\n\\n## transforming \\\"The testset\\\"\\n#test = st_scale.transform(test)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"d425ca579370db88e39cdd1811ba3df2b257b36c\"},\"source\":\"<h3><font color=\\\"#5831bc\\\" face=\\\"Comic Sans MS\\\">After Scaling</font></h3>\"},{\"cell_type\":\"code\",\"execution_count\":91,\"metadata\":{\"_kg_hide-input\":true,\"_uuid\":\"fc6f031833ac9e2734aa7b3a2373b667679c6b2f\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.333531Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.333111Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.359161Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.358554Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.333347Z\"}},\"outputs\":[],\"source\":\"pd.DataFrame(X_train, columns=headers).head()\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"You can see how the features have transformed above.\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"0e03e40b-789a-40a0-a095-135f3d1c8f23\",\"_uuid\":\"99e108b83ba88738e42480b053371d60d89151cf\"},\"source\":\"# Part 7: Modeling the Data\\n<a id=\\\"modelingthedata\\\"></a>\\n***\\nIn the previous versions of this kernel, I thought about explaining each model before applying it. However, this process makes this kernel too lengthy to sit and read at one go. Therefore I have decided to break this kernel down and explain each algorithm in a different kernel and add the links here. If you like to review logistic regression, please click [here](https://www.kaggle.com/masumrumi/logistic-regression-with-titanic-dataset). \"},{\"cell_type\":\"code\",\"execution_count\":92,\"metadata\":{\"_cell_guid\":\"0c8b0c41-6738-4689-85b0-b83a16e46ab9\",\"_uuid\":\"09140be1a71e37b441a16951a82747462b767e6e\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.361067Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.360637Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.383762Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.383049Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.360889Z\"}},\"outputs\":[],\"source\":\"# import LogisticRegression model in python. \\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\\n\\n## call on the model object\\nlogreg = LogisticRegression(solver='liblinear',\\n                            penalty= 'l1',random_state = 42\\n                                \\n                            )\\n\\n## fit the model with \\\"train_x\\\" and \\\"train_y\\\"\\nlogreg.fit(X_train,y_train)\\n\\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \\n## we use \\\"X_test\\\" portion of the data(this data was not used to fit the model) to predict model outcome. \\ny_pred = logreg.predict(X_test)\\n\\n## Once predicted we save that outcome in \\\"y_pred\\\" variable.\\n## Then we compare the predicted value( \\\"y_pred\\\") and actual value(\\\"test_y\\\") to see how well our model is performing. \"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<h1><font color=\\\"#5831bc\\\" face=\\\"Comic Sans MS\\\">Evaluating a classification model</font></h1>\\n\\nThere are multiple ways to evaluate a classification model. \\n\\n* Confusion Matrix. \\n* ROC Curve\\n* AUC Curve. \\n\\n\\n## Confusion Matrix\\n<b>Confusion matrix</b>, a table that <b>describes the performance of a classification model</b>. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary/multiple outcome classes by comparing actual and predicted cases. For example, in terms of this dataset, our model is a binary one and we are trying to classify whether the passenger survived or not survived. we have fit the model using **X_train** and **y_train** and predicted the outcome of **X_test** in the variable **y_pred**. So, now we will use a confusion matrix to compare between **y_test** and **y_pred**. Let's do the confusion matrix. \\n\"},{\"cell_type\":\"code\",\"execution_count\":93,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.385843Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.385341Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.399434Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.398674Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.385606Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import classification_report, confusion_matrix\\n# printing confision matrix\\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\\\\n            columns=[\\\"Predicted Not-Survived\\\", \\\"Predicted Survived\\\"],\\\\\\n            index=[\\\"Not-Survived\\\",\\\"Survived\\\"] )\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"Our **y_test** has a total of 294 data points; part of the original train set that we splitted in order to evaluate our model. Each number here represents certain details about our model. If we were to think about this interms of column and raw, we could see that...\\n\\n* the first column is of data points that the machine predicted as not-survived.\\n* the second column is of the statistics that the model predicted as survievd.\\n* In terms of raws, the first raw indexed as \\\"Not-survived\\\" means that the value in that raw are actual statistics of not survived once. \\n* and the \\\"Survived\\\" indexed raw are values that actually survived.\\n\\nNow you can see that the predicted not-survived and predicted survived sort of overlap with actual survived and actual not-survived. After all it is a matrix and we have some terminologies to call these statistics more specifically. Let's see what they are\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<ul style=\\\"list-style-type:square;\\\">\\n    <li><b>True Positive(TP)</b>: values that the model predicted as yes(survived) and is actually yes(survived).</li>\\n    <li><b>True Negative(TN)</b>: values that model predicted as no(not-survived) and is actually no(not-survived)</li>\\n    <li><b>False Positive(or Type I error)</b>: values that model predicted as yes(survived) but actually no(not-survived)</li>\\n    <li><b>False Negative(or Type II error)</b>: values that model predicted as no(not-survived) but actually yes(survived)</li>\\n</ul>\\n\\nFor this dataset, whenever the model is predicting something as yes, it means the model is predicting that the passenger survived and for cases when the model predicting no; it means the passenger did not survive. Let's determine the value of all these terminologies above.\\n<ul style=\\\"list-style-type:square;\\\">\\n    <li><b>True Positive(TP):87</b></li>\\n    <li><b>True Negative(TN):149</b></li>\\n    <li><b>False Positive(FP):28</b></li>\\n    <li><b>False Negative(FN):30</b></li>\\n</ul>\\nFrom these four terminologies, we can compute many other rates that are used to evaluate a binary classifier. \\n\\n\\n#### Accuracy: \\n** Accuracy is the measure of how often the model is correct.** \\n* (TP + TN)/total = (87+149)/294 = .8027\\n\\nWe can also calculate accuracy score using scikit learn. \"},{\"cell_type\":\"code\",\"execution_count\":94,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.400652Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.400403Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.408635Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.40776Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.400604Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import accuracy_score\\naccuracy_score(y_test, y_pred)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"**Misclassification Rate:** Misclassification Rate is the measure of how often the model is wrong**\\n* Misclassification Rate and Accuracy are opposite of each other.\\n* Missclassification is equivalent to 1 minus Accuracy. \\n* Misclassification Rate is also known as \\\"Error Rate\\\".\\n\\n> (FP + FN)/Total = (28+30)/294 = 0.19\\n\\n**True Positive Rate/Recall/Sensitivity:** How often the model predicts yes(survived) when it's actually yes(survived)?\\n> TP/(TP+FN) = 87/(87+30) = 0.7435897435897436\\n\"},{\"cell_type\":\"code\",\"execution_count\":95,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.410491Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.410085Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.418315Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.417549Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.410444Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import recall_score\\nrecall_score(y_test, y_pred)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"\\n**False Positive Rate:** How often the model predicts yes(survived) when it's actually no(not-survived)?\\n> FP/(FP+TN) = 28/(28+149) = 0.15819209039548024\\n\\n**True Negative Rate/Specificity:** How often the model predicts no(not-survived) when it's actually no(not-survived)?\\n* True Negative Rate is equivalent to 1 minus False Positive Rate.\\n\\n> TN/(TN+FP) = 149/(149+28) = 0.8418079096045198\\n\\n**Precision:** How often is it correct when the model predicts yes. \\n> TP/(TP+FP) = 87/(87+28) = 0.7565217391304347\"},{\"cell_type\":\"code\",\"execution_count\":96,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.4204Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.419791Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.429679Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.42864Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.420242Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import precision_score\\nprecision_score(y_test, y_pred)\"},{\"cell_type\":\"code\",\"execution_count\":97,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.431682Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.431234Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.44225Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.441202Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.43147Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import classification_report, balanced_accuracy_score\\nprint(classification_report(y_test, y_pred))\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"we have our confusion matrix. How about we give it a little more character. \"},{\"cell_type\":\"code\",\"execution_count\":98,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.444153Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.443714Z\",\"iopub.status.idle\":\"2021-06-26T16:35:23.873374Z\",\"shell.execute_reply\":\"2021-06-26T16:35:23.869521Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.444104Z\"}},\"outputs\":[],\"source\":\"from sklearn.utils.multiclass import unique_labels\\nfrom sklearn.metrics import confusion_matrix\\n\\n\\ndef plot_confusion_matrix(y_true, y_pred, classes,\\n                          normalize=False,\\n                          title=None,\\n                          cmap=plt.cm.Blues):\\n    \\\"\\\"\\\"\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \\\"\\\"\\\"\\n    if not title:\\n        if normalize:\\n            title = 'Normalized confusion matrix'\\n        else:\\n            title = 'Confusion matrix, without normalization'\\n\\n    # Compute confusion matrix\\n    cm = confusion_matrix(y_true, y_pred)\\n    # Only use the labels that appear in the data\\n    classes = classes[unique_labels(y_true, y_pred)]\\n    if normalize:\\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\\n        print(\\\"Normalized confusion matrix\\\")\\n    else:\\n        print('Confusion matrix, without normalization')\\n\\n    print(cm)\\n\\n    fig, ax = plt.subplots()\\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\\n    ax.figure.colorbar(im, ax=ax)\\n    # We want to show all ticks...\\n    ax.set(xticks=np.arange(cm.shape[1]),\\n           yticks=np.arange(cm.shape[0]),\\n           # ... and label them with the respective list entries\\n           xticklabels=classes, yticklabels=classes,\\n           title=title,\\n           ylabel='True label',\\n           xlabel='Predicted label')\\n\\n    # Rotate the tick labels and set their alignment.\\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\\\"right\\\",\\n             rotation_mode=\\\"anchor\\\")\\n\\n    # Loop over data dimensions and create text annotations.\\n    fmt = '.2f' if normalize else 'd'\\n    thresh = cm.max() / 2.\\n    for i in range(cm.shape[0]):\\n        for j in range(cm.shape[1]):\\n            ax.text(j, i, format(cm[i, j], fmt),\\n                    ha=\\\"center\\\", va=\\\"center\\\",\\n                    color=\\\"white\\\" if cm[i, j] > thresh else \\\"black\\\")\\n    fig.tight_layout()\\n    return ax\\n\\n\\nnp.set_printoptions(precision=2)\\n\\nclass_names = np.array(['not_survived','survived'])\\n\\n# Plot non-normalized confusion matrix\\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\\n                      title='Confusion matrix, without normalization')\\n\\n# Plot normalized confusion matrix\\nplot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\\n                      title='Normalized confusion matrix')\\n\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"e13731cbb9d9040cf6e4088e8660eca66037a8cc\"},\"source\":\"<h1>AUC & ROC Curve</h1>\"},{\"cell_type\":\"code\",\"execution_count\":99,\"metadata\":{\"_uuid\":\"1e71bc7c685b757b6920076527780674d6f619bc\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:23.877891Z\",\"iopub.status.busy\":\"2021-06-26T16:35:23.875713Z\",\"iopub.status.idle\":\"2021-06-26T16:35:24.505751Z\",\"shell.execute_reply\":\"2021-06-26T16:35:24.501314Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:23.87783Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import roc_curve, auc\\n#plt.style.use('seaborn-pastel')\\ny_score = logreg.decision_function(X_test)\\n\\nFPR, TPR, _ = roc_curve(y_test, y_score)\\nROC_AUC = auc(FPR, TPR)\\nprint (ROC_AUC)\\n\\nplt.figure(figsize =[11,9])\\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\\nplt.xlim([0.0,1.0])\\nplt.ylim([0.0,1.05])\\nplt.xlabel('False Positive Rate', fontsize = 18)\\nplt.ylabel('True Positive Rate', fontsize = 18)\\nplt.title('ROC for Titanic survivors', fontsize= 18)\\nplt.show()\"},{\"cell_type\":\"code\",\"execution_count\":100,\"metadata\":{\"_uuid\":\"22f15e384372a1ece2f28cd9eced0c703a79598f\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:24.50731Z\",\"iopub.status.busy\":\"2021-06-26T16:35:24.506981Z\",\"iopub.status.idle\":\"2021-06-26T16:35:24.8481Z\",\"shell.execute_reply\":\"2021-06-26T16:35:24.846974Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:24.507251Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import precision_recall_curve\\n\\ny_score = logreg.decision_function(X_test)\\n\\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\\nPR_AUC = auc(recall, precision)\\n\\nplt.figure(figsize=[11,9])\\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\\nplt.xlabel('Recall', fontsize=18)\\nplt.ylabel('Precision', fontsize=18)\\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\\nplt.legend(loc=\\\"lower right\\\")\\nplt.show()\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"e46b6d4bcb0ef70c06535b58bbe84c8a301ead91\"},\"source\":\"## Using Cross-validation:\\nPros: \\n* Helps reduce variance. \\n* Expends models predictability. \\n\"},{\"cell_type\":\"code\",\"execution_count\":101,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:24.855506Z\",\"iopub.status.busy\":\"2021-06-26T16:35:24.853028Z\",\"iopub.status.idle\":\"2021-06-26T16:35:24.862513Z\",\"shell.execute_reply\":\"2021-06-26T16:35:24.861421Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:24.853368Z\"}},\"outputs\":[],\"source\":\"sc = st_scale\"},{\"cell_type\":\"code\",\"execution_count\":102,\"metadata\":{\"_uuid\":\"17791284c3e88236de2daa112422cde8ddcb0641\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:24.868704Z\",\"iopub.status.busy\":\"2021-06-26T16:35:24.86826Z\",\"iopub.status.idle\":\"2021-06-26T16:35:25.014634Z\",\"shell.execute_reply\":\"2021-06-26T16:35:25.013771Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:24.86853Z\"},\"scrolled\":true},\"outputs\":[],\"source\":\"## Using StratifiedShuffleSplit\\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\\n## Using standard scale for the whole dataset.\\n\\n## saving the feature names for decision tree display\\ncolumn_names = X.columns\\n\\nX = sc.fit_transform(X)\\naccuracies = cross_val_score(LogisticRegression(solver='liblinear'), X,y, cv  = cv)\\nprint (\\\"Cross-Validation accuracy scores:{}\\\".format(accuracies))\\nprint (\\\"Mean Cross-Validation accuracy score: {}\\\".format(round(accuracies.mean(),5)))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"d1f2930c-43ae-4c15-87f7-ccc9214ee0e1\",\"_uuid\":\"b8020ecfe44bebdf7a2b95ec49393e8baac6bcf9\"},\"source\":\"## Grid Search on Logistic Regression\\n* What is grid search? \\n* What are the pros and cons?\\n\\n**Gridsearch** is a simple concept but effective technique in Machine Learning. The word **GridSearch** stands for the fact that we are searching for optimal parameter/parameters over a \\\"grid.\\\" These optimal parameters are also known as **Hyperparameters**. **The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model.**. For example, when we choose to use linear regression, we may decide to add a penalty to the loss function such as Ridge or Lasso. These penalties require specific alpha (the strength of the regularization technique) to set beforehand. The higher the value of alpha, the more penalty is being added. GridSearch finds the optimal value of alpha among a range of values provided by us, and then we go on and use that optimal value to fit the model and get sweet results. It is essential to understand those model parameters are different from models outcomes, for example, **coefficients** or model evaluation metrics such as **accuracy score** or **mean squared error** are model outcomes and different than hyperparameters.\\n\\n#### This part of the kernel is a working progress. Please check back again for future updates.####\"},{\"cell_type\":\"code\",\"execution_count\":103,\"metadata\":{\"_cell_guid\":\"0620523c-b33b-4302-8a1c-4b6759ffa5fa\",\"_uuid\":\"36a379a00a31dd161be1723f65490990294fe13d\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:25.021234Z\",\"iopub.status.busy\":\"2021-06-26T16:35:25.018883Z\",\"iopub.status.idle\":\"2021-06-26T16:35:40.193433Z\",\"shell.execute_reply\":\"2021-06-26T16:35:40.192566Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:25.021181Z\"}},\"outputs\":[],\"source\":\"from sklearn.model_selection import GridSearchCV, StratifiedKFold\\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\\n## remember effective alpha scores are 0<alpha<infinity \\nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\\n## Choosing penalties(Lasso(l1) or Ridge(l2))\\npenalties = ['l1','l2']\\n## Choose a cross validation strategy. \\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\\n\\n## setting param for param_grid in GridSearchCV. \\nparam = {'penalty': penalties, 'C': C_vals}\\n\\nlogreg = LogisticRegression(solver='liblinear')\\n## Calling on GridSearchCV object. \\ngrid = GridSearchCV(estimator=LogisticRegression(), \\n                           param_grid = param,\\n                           scoring = 'accuracy',\\n                            n_jobs =-1,\\n                           cv = cv\\n                          )\\n## Fitting the model\\ngrid.fit(X, y)\"},{\"cell_type\":\"code\",\"execution_count\":104,\"metadata\":{\"_cell_guid\":\"1fa35072-87c4-4f47-86ab-dda03d4b7b15\",\"_uuid\":\"4c6650e39550527b271ddf733dcfe5221bcd5c98\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:40.195216Z\",\"iopub.status.busy\":\"2021-06-26T16:35:40.194925Z\",\"iopub.status.idle\":\"2021-06-26T16:35:40.201259Z\",\"shell.execute_reply\":\"2021-06-26T16:35:40.200225Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:40.19517Z\"}},\"outputs\":[],\"source\":\"## Getting the best of everything. \\nprint (grid.best_score_)\\nprint (grid.best_params_)\\nprint(grid.best_estimator_)\\n\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"dcd2ad782c168abb5cfb5a3d148814e53cb2119c\"},\"source\":\"\\n#### Using the best parameters from the grid-search. \"},{\"cell_type\":\"code\",\"execution_count\":105,\"metadata\":{\"_uuid\":\"ba53f6b3610821dc820936dde7b7803a54d20f5a\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:40.204086Z\",\"iopub.status.busy\":\"2021-06-26T16:35:40.203576Z\",\"iopub.status.idle\":\"2021-06-26T16:35:40.214041Z\",\"shell.execute_reply\":\"2021-06-26T16:35:40.212929Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:40.20393Z\"}},\"outputs\":[],\"source\":\"### Using the best parameters from the grid-search.\\nlogreg_grid = grid.best_estimator_\\nlogreg_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\" #### This part of the kernel is a working progress. Please check back again for future updates.####\\n \\n Resources: \\n * [Confusion Matrix](https://www.youtube.com/watch?v=8Oog7TXHvFY)\\n### Under-fitting & Over-fitting: \\nSo, we have our first model and its score. But, how do we make sure that our model is performing well. Our model may be overfitting or underfitting. In fact, for those of you don't know what overfitting and underfitting is, Let's find out.\\n\\n![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/fittings.jpg)\\n\\nAs you see in the chart above. **Underfitting** is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. On the other hand, **Overfitting** is when the model performs too well on the training data but does poorly in the validation set or test sets.  This situation is also known as having less bias but more variation and perform poorly as well. Ideally, we want to configure a model that performs well not only in the training data but also in the test data. This is where **bias-variance tradeoff** comes in. When we have a model that overfits, meaning less biased and more of variance, we introduce some bias in exchange of having much less variance. One particular tactic for this task is regularization models (Ridge, Lasso, Elastic Net).  These models are built to deal with the bias-variance tradeoff. This [kernel](https://www.kaggle.com/dansbecker/underfitting-and-overfitting) explains this topic well. Also, the following chart gives us a mental picture of where we want our models to be. \\n![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\\n\\nIdeally, we want to pick a sweet spot where the model performs well in training set, validation set, and test set. As the model gets complex, bias decreases, variance increases. However, the most critical part is the error rates. We want our models to be at the bottom of that **U** shape where the error rate is the least. That sweet spot is also known as **Optimum Model Complexity(OMC).**\\n\\nNow that we know what we want in terms of under-fitting and over-fitting, let's talk about how to combat them. \\n\\nHow to combat over-fitting?\\n<ul>\\n    <li>Simplify the model by using less parameters.</li>\\n    <li>Simplify the model by changing the hyperparameters.</li>\\n    <li>Introducing regularization models. </li>\\n    <li>Use more training data. </li>\\n    <li>Gatter more data ( and gather better quality data). </li>\\n    </ul>\\n #### This part of the kernel is a working progress. Please check back again for future updates.####\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"8046e4d9-12db-4b1c-9e9e-31fd5e6543f2\",\"_uuid\":\"26b0ea9184b2c37eabe4e705b1c840956ecc1e10\"},\"source\":\"## 7b. K-Nearest Neighbor classifier(KNN)\\n<a id=\\\"knn\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":106,\"metadata\":{\"_uuid\":\"953bc2c18b5fd93bcd51a42cc04a0539d86d5bac\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:40.216328Z\",\"iopub.status.busy\":\"2021-06-26T16:35:40.215853Z\",\"iopub.status.idle\":\"2021-06-26T16:35:40.416985Z\",\"shell.execute_reply\":\"2021-06-26T16:35:40.416038Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:40.216141Z\"}},\"outputs\":[],\"source\":\"## Importing the model. \\nfrom sklearn.neighbors import KNeighborsClassifier\\n## calling on the model oject. \\nknn = KNeighborsClassifier(metric='minkowski', p=2)\\n## knn classifier works by doing euclidian distance \\n\\n\\n## doing 10 fold staratified-shuffle-split cross validation \\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\\n\\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\\nprint (\\\"Cross-Validation accuracy scores:{}\\\".format(accuracies))\\nprint (\\\"Mean Cross-Validation accuracy score: {}\\\".format(round(accuracies.mean(),3)))\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"6aa75e53129898ccd714370dc55c0ed2830e72f4\"},\"source\":\"#### Manually find the best possible k value for KNN\"},{\"cell_type\":\"code\",\"execution_count\":107,\"metadata\":{\"_uuid\":\"9c0f44165e08f63ae5436180c5a7182e6db5c63f\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:40.418857Z\",\"iopub.status.busy\":\"2021-06-26T16:35:40.418419Z\",\"iopub.status.idle\":\"2021-06-26T16:35:46.541601Z\",\"shell.execute_reply\":\"2021-06-26T16:35:46.540815Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:40.418687Z\"}},\"outputs\":[],\"source\":\"## Search for an optimal value of k for KNN.\\nk_range = range(1,31)\\nk_scores = []\\nfor k in k_range:\\n    knn = KNeighborsClassifier(n_neighbors=k)\\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\\n    k_scores.append(scores.mean())\\nprint(\\\"Accuracy scores are: {}\\\\n\\\".format(k_scores))\\nprint (\\\"Mean accuracy score: {}\\\".format(np.mean(k_scores)))\\n\"},{\"cell_type\":\"code\",\"execution_count\":108,\"metadata\":{\"_uuid\":\"e123680b431ba99d399fa8205c32bcfdc7cabd81\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:46.543234Z\",\"iopub.status.busy\":\"2021-06-26T16:35:46.542789Z\",\"iopub.status.idle\":\"2021-06-26T16:35:46.685143Z\",\"shell.execute_reply\":\"2021-06-26T16:35:46.684141Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:46.543184Z\"}},\"outputs\":[],\"source\":\"from matplotlib import pyplot as plt\\nplt.plot(k_range, k_scores)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"77b5b6e3b7bc925e0b008cd6d531175e5cc44040\"},\"source\":\"### Grid search on KNN classifier\"},{\"cell_type\":\"code\",\"execution_count\":109,\"metadata\":{\"_uuid\":\"507e2a7cdb28a47be45ed247f1343c123a6b592b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:46.687026Z\",\"iopub.status.busy\":\"2021-06-26T16:35:46.686671Z\",\"iopub.status.idle\":\"2021-06-26T16:35:55.465245Z\",\"shell.execute_reply\":\"2021-06-26T16:35:55.464452Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:46.686956Z\"}},\"outputs\":[],\"source\":\"from sklearn.model_selection import GridSearchCV\\n## trying out multiple values for k\\nk_range = range(1,31)\\n## \\nweights_options=['uniform','distance']\\n# \\nparam = {'n_neighbors':k_range, 'weights':weights_options}\\n## Using startifiedShufflesplit. \\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \\ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\\n## Fitting the model. \\ngrid.fit(X,y)\"},{\"cell_type\":\"code\",\"execution_count\":110,\"metadata\":{\"_uuid\":\"c710770daa6cf327dcc28e18b3ed180fabecd49b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:55.466929Z\",\"iopub.status.busy\":\"2021-06-26T16:35:55.466654Z\",\"iopub.status.idle\":\"2021-06-26T16:35:55.475348Z\",\"shell.execute_reply\":\"2021-06-26T16:35:55.474575Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:55.466883Z\"}},\"outputs\":[],\"source\":\"print(grid.best_score_)\\nprint(grid.best_params_)\\nprint(grid.best_estimator_)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"bb06144264d3127c92169aed7c29c2f66ad0ffc4\"},\"source\":\"#### Using best estimator from grid search using KNN. \"},{\"cell_type\":\"code\",\"execution_count\":111,\"metadata\":{\"_uuid\":\"dd1fbf223c4ec9db65dde4924e2827e46029da1a\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:55.477181Z\",\"iopub.status.busy\":\"2021-06-26T16:35:55.476629Z\",\"iopub.status.idle\":\"2021-06-26T16:35:55.555736Z\",\"shell.execute_reply\":\"2021-06-26T16:35:55.554788Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:55.476983Z\"}},\"outputs\":[],\"source\":\"### Using the best parameters from the grid-search.\\nknn_grid= grid.best_estimator_\\nknn_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_uuid\":\"c2ebec8b83f23e3e27d23bdd707852269edd4d24\"},\"source\":\"#### Using RandomizedSearchCV\\nRandomized search is a close cousin of grid search. It doesn't  always provide the best result but its fast. \"},{\"cell_type\":\"code\",\"execution_count\":112,\"metadata\":{\"_uuid\":\"e159b267a57d7519fc0ee8b3d1e95b841d3daf60\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:35:55.557501Z\",\"iopub.status.busy\":\"2021-06-26T16:35:55.557097Z\",\"iopub.status.idle\":\"2021-06-26T16:36:02.332003Z\",\"shell.execute_reply\":\"2021-06-26T16:36:02.331364Z\",\"shell.execute_reply.started\":\"2021-06-26T16:35:55.557338Z\"}},\"outputs\":[],\"source\":\"from sklearn.model_selection import RandomizedSearchCV\\n## trying out multiple values for k\\nk_range = range(1,31)\\n## \\nweights_options=['uniform','distance']\\n# \\nparam = {'n_neighbors':k_range, 'weights':weights_options}\\n## Using startifiedShufflesplit. \\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30)\\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \\n## for RandomizedSearchCV, \\ngrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\\n## Fitting the model. \\ngrid.fit(X,y)\"},{\"cell_type\":\"code\",\"execution_count\":113,\"metadata\":{\"_uuid\":\"c58492525dd18659ef9f9c774ee7601a55e96f36\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:36:02.333632Z\",\"iopub.status.busy\":\"2021-06-26T16:36:02.333341Z\",\"iopub.status.idle\":\"2021-06-26T16:36:02.340211Z\",\"shell.execute_reply\":\"2021-06-26T16:36:02.338113Z\",\"shell.execute_reply.started\":\"2021-06-26T16:36:02.333572Z\"}},\"outputs\":[],\"source\":\"print (grid.best_score_)\\nprint (grid.best_params_)\\nprint(grid.best_estimator_)\"},{\"cell_type\":\"code\",\"execution_count\":114,\"metadata\":{\"_uuid\":\"6fb31588585d50de773ba0db6c378363841a5313\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:36:02.343117Z\",\"iopub.status.busy\":\"2021-06-26T16:36:02.34256Z\",\"iopub.status.idle\":\"2021-06-26T16:36:02.420683Z\",\"shell.execute_reply\":\"2021-06-26T16:36:02.419712Z\",\"shell.execute_reply.started\":\"2021-06-26T16:36:02.342922Z\"}},\"outputs\":[],\"source\":\"### Using the best parameters from the grid-search.\\nknn_ran_grid = grid.best_estimator_\\nknn_ran_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"be0143d6-a7ea-4752-9520-c692f4c3eb8a\",\"_uuid\":\"21e91edd53b6587d5a05036045bc5eea52f056da\"},\"source\":\"## Gaussian Naive Bayes\\n<a id=\\\"gaussian_naive\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":115,\"metadata\":{\"_uuid\":\"8b2435030dbef1303bfc2864d227f5918f359330\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:36:02.422487Z\",\"iopub.status.busy\":\"2021-06-26T16:36:02.421997Z\",\"iopub.status.idle\":\"2021-06-26T16:36:02.433216Z\",\"shell.execute_reply\":\"2021-06-26T16:36:02.43234Z\",\"shell.execute_reply.started\":\"2021-06-26T16:36:02.422237Z\"}},\"outputs\":[],\"source\":\"# Gaussian Naive Bayes\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.metrics import accuracy_score\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X, y)\\ny_pred = gaussian.predict(X_test)\\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\\nprint(gaussian_accy)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"c3e025c5-50f3-4fa1-a385-438d6665199b\",\"_uuid\":\"2a1558118d9e673395246acc4f3c0edb1b1895f0\"},\"source\":\"## Support Vector Machines(SVM)\\n<a id=\\\"svm\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":116,\"metadata\":{\"_uuid\":\"56895672215b0b6365c6aaa10e446216ef635f53\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:36:02.435838Z\",\"iopub.status.busy\":\"2021-06-26T16:36:02.435282Z\",\"iopub.status.idle\":\"2021-06-26T16:37:25.882123Z\",\"shell.execute_reply\":\"2021-06-26T16:37:25.881483Z\",\"shell.execute_reply.started\":\"2021-06-26T16:36:02.435553Z\"}},\"outputs\":[],\"source\":\"from sklearn.svm import SVC\\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \\ngammas = [0.0001,0.001, 0.01, 0.1, 1]\\nparam_grid = {'C': Cs, 'gamma' : gammas}\\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\\ngrid_search.fit(X,y)\"},{\"cell_type\":\"code\",\"execution_count\":117,\"metadata\":{\"_uuid\":\"4108264ea5d18e3d3fa38a30584a032c734d6d49\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:25.8839Z\",\"iopub.status.busy\":\"2021-06-26T16:37:25.883609Z\",\"iopub.status.idle\":\"2021-06-26T16:37:25.890029Z\",\"shell.execute_reply\":\"2021-06-26T16:37:25.889244Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:25.883852Z\"}},\"outputs\":[],\"source\":\"print(grid_search.best_score_)\\nprint(grid_search.best_params_)\\nprint(grid_search.best_estimator_)\"},{\"cell_type\":\"code\",\"execution_count\":118,\"metadata\":{\"_uuid\":\"db18a3b5475f03b21a039e31e4962c43f7caffdc\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:25.892123Z\",\"iopub.status.busy\":\"2021-06-26T16:37:25.891542Z\",\"iopub.status.idle\":\"2021-06-26T16:37:25.934216Z\",\"shell.execute_reply\":\"2021-06-26T16:37:25.933352Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:25.892073Z\"}},\"outputs\":[],\"source\":\"# using the best found hyper paremeters to get the score. \\nsvm_grid = grid_search.best_estimator_\\nsvm_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"## Decision Tree Classifier\\n\\nDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. If you would like to get a detailed understanding of Decision tree classifier, please take a look at [this](https://www.kaggle.com/masumrumi/decision-tree-with-titanic-dataset) kernel. \"},{\"cell_type\":\"code\",\"execution_count\":119,\"metadata\":{\"_cell_guid\":\"38c90de9-d2e9-4341-a378-a854762d8be2\",\"_uuid\":\"18efb62b713591d1512010536ff10d9f6a91ec11\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:25.936111Z\",\"iopub.status.busy\":\"2021-06-26T16:37:25.935654Z\",\"iopub.status.idle\":\"2021-06-26T16:37:57.983942Z\",\"shell.execute_reply\":\"2021-06-26T16:37:57.983035Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:25.935918Z\"}},\"outputs\":[],\"source\":\"from sklearn.tree import DecisionTreeClassifier\\nmax_depth = range(1,30)\\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\\ncriterion=[\\\"entropy\\\", \\\"gini\\\"]\\n\\nparam = {'max_depth':max_depth, \\n         'max_features':max_feature, \\n         'criterion': criterion}\\ngrid = GridSearchCV(DecisionTreeClassifier(), \\n                                param_grid = param, \\n                                 verbose=False, \\n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\\n                                n_jobs = -1)\\ngrid.fit(X, y) \"},{\"cell_type\":\"code\",\"execution_count\":120,\"metadata\":{\"_cell_guid\":\"b2222e4e-f5f2-4601-b95f-506d7811610a\",\"_uuid\":\"b0fb5055e6b4a7fb69ef44f669c4df693ce46212\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:57.988346Z\",\"iopub.status.busy\":\"2021-06-26T16:37:57.988045Z\",\"iopub.status.idle\":\"2021-06-26T16:37:57.994617Z\",\"shell.execute_reply\":\"2021-06-26T16:37:57.993662Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:57.988287Z\"},\"scrolled\":true},\"outputs\":[],\"source\":\"print( grid.best_params_)\\nprint (grid.best_score_)\\nprint (grid.best_estimator_)\"},{\"cell_type\":\"code\",\"execution_count\":121,\"metadata\":{\"_cell_guid\":\"d731079a-31b4-429a-8445-48597bb2639d\",\"_uuid\":\"76c26437d374442826ef140574c5c4880ae1e853\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:57.996876Z\",\"iopub.status.busy\":\"2021-06-26T16:37:57.996238Z\",\"iopub.status.idle\":\"2021-06-26T16:37:58.010892Z\",\"shell.execute_reply\":\"2021-06-26T16:37:58.010194Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:57.996695Z\"}},\"outputs\":[],\"source\":\"dectree_grid = grid.best_estimator_\\n## using the best found hyper paremeters to get the score. \\ndectree_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\" <h4> Let's look at the feature importance from decision tree grid.</h4>\"},{\"cell_type\":\"code\",\"execution_count\":122,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:58.013756Z\",\"iopub.status.busy\":\"2021-06-26T16:37:58.01221Z\",\"iopub.status.idle\":\"2021-06-26T16:37:58.034194Z\",\"shell.execute_reply\":\"2021-06-26T16:37:58.033436Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:58.013683Z\"}},\"outputs\":[],\"source\":\"## feature importance\\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\\n                                   index = column_names,\\n                                    columns=['importance'])\\nfeature_importances.sort_values(by='importance', ascending=False).head(10)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"These are the top 10 features determined by **Decision Tree** helped classifing the fates of many passenger on Titanic on that night.\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"## 7f. Random Forest Classifier\\n<a id=\\\"random_forest\\\"></a>\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"I admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest(RF). RF is an ensemble method (combination of many decision trees) which is where the \\\"forest\\\" part comes in. One crucial details about Random Forest is that while using a forest of decision trees, RF model <b>takes random subsets of the original dataset(bootstrapped)</b> and <b>random subsets of the variables(features/columns)</b>. Using this method, the RF model creates 100's-1000's(the amount can be menually determined) of a wide variety of decision trees. This variety makes the RF model more effective and accurate. We then run each test data point through all of these 100's to 1000's of decision trees or the RF model and take a vote on the output. \\n\\n\"},{\"cell_type\":\"code\",\"execution_count\":123,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:37:58.040453Z\",\"iopub.status.busy\":\"2021-06-26T16:37:58.038063Z\",\"iopub.status.idle\":\"2021-06-26T16:39:53.557817Z\",\"shell.execute_reply\":\"2021-06-26T16:39:53.556973Z\",\"shell.execute_reply.started\":\"2021-06-26T16:37:58.040398Z\"}},\"outputs\":[],\"source\":\"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\\nfrom sklearn.ensemble import RandomForestClassifier\\nn_estimators = [140,145,150,155,160];\\nmax_depth = range(1,10);\\ncriterions = ['gini', 'entropy'];\\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\\n\\n\\nparameters = {'n_estimators':n_estimators,\\n              'max_depth':max_depth,\\n              'criterion': criterions\\n              \\n        }\\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\\n                                 param_grid=parameters,\\n                                 cv=cv,\\n                                 n_jobs = -1)\\ngrid.fit(X,y) \"},{\"cell_type\":\"code\",\"execution_count\":124,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:39:53.559492Z\",\"iopub.status.busy\":\"2021-06-26T16:39:53.559192Z\",\"iopub.status.idle\":\"2021-06-26T16:39:53.567897Z\",\"shell.execute_reply\":\"2021-06-26T16:39:53.56675Z\",\"shell.execute_reply.started\":\"2021-06-26T16:39:53.559434Z\"}},\"outputs\":[],\"source\":\"print (grid.best_score_)\\nprint (grid.best_params_)\\nprint (grid.best_estimator_)\"},{\"cell_type\":\"code\",\"execution_count\":125,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:39:53.570209Z\",\"iopub.status.busy\":\"2021-06-26T16:39:53.56951Z\",\"iopub.status.idle\":\"2021-06-26T16:39:53.600458Z\",\"shell.execute_reply\":\"2021-06-26T16:39:53.599531Z\",\"shell.execute_reply.started\":\"2021-06-26T16:39:53.569928Z\"}},\"outputs\":[],\"source\":\"rf_grid = grid.best_estimator_\\nrf_grid.score(X,y)\"},{\"cell_type\":\"code\",\"execution_count\":126,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:39:53.602628Z\",\"iopub.status.busy\":\"2021-06-26T16:39:53.602028Z\",\"iopub.status.idle\":\"2021-06-26T16:39:53.613347Z\",\"shell.execute_reply\":\"2021-06-26T16:39:53.612229Z\",\"shell.execute_reply.started\":\"2021-06-26T16:39:53.602297Z\"}},\"outputs\":[],\"source\":\"from sklearn.metrics import classification_report\\n# Print classification report for y_test\\nprint(classification_report(y_test, y_pred, labels=rf_grid.classes_))\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"## Feature Importance\"},{\"cell_type\":\"code\",\"execution_count\":127,\"metadata\":{\"_kg_hide-input\":true,\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:39:53.615537Z\",\"iopub.status.busy\":\"2021-06-26T16:39:53.614947Z\",\"iopub.status.idle\":\"2021-06-26T16:39:53.637392Z\",\"shell.execute_reply\":\"2021-06-26T16:39:53.63647Z\",\"shell.execute_reply.started\":\"2021-06-26T16:39:53.615192Z\"}},\"outputs\":[],\"source\":\"## feature importance\\nfeature_importances = pd.DataFrame(rf_grid.feature_importances_,\\n                                   index = column_names,\\n                                    columns=['importance'])\\nfeature_importances.sort_values(by='importance', ascending=False).head(10)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<h3>Why Random Forest?(Pros and Cons)</h3>\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"9c4c43f6-42c4-4cd3-a038-3f0c37f3c767\",\"_uuid\":\"aba2679da04529faf9f9175ab20a66ee71217f92\"},\"source\":\"***\\n<h2>Introducing Ensemble Learning</h2>\\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. \\n\\nThere are two types of ensemple learnings. \\n\\n**Bagging/Averaging Methods**\\n> In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\\n\\n**Boosting Methods**\\n> The other family of ensemble methods are boosting methods, where base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\\n\\n<h4 align=\\\"right\\\">Source:GA</h4>\\n\\nResource: <a href=\\\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\\\">Ensemble methods: bagging, boosting and stacking</a>\\n***\\n## 7g. Bagging Classifier\\n<a id=\\\"bagging\\\"></a>\\n***\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<a href=\\\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\\\">Bagging Classifier</a>(Bootstrap Aggregating) is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Let's do a quick review:\\n* Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. Here is an image to show how bootstrapped dataset works. \\n<img src=\\\"https://uc-r.github.io/public/images/analytics/bootstrap/bootstrap.png\\\" width=\\\"600\\\">\\n<h4 align=\\\"center\\\">Resampling from original dataset to bootstrapped datasets</h4>\\n<h4 align=\\\"right\\\">Source: https://uc-r.github.io</h4>\\n\\n\\n* After running a learning algorithm on each one of the bootstrapped datasets, all models are combined by taking their average. the test data/new data then go through this averaged classifier/combined classifier and predict the output. \\n\\nHere is an image to make it clear on how bagging works, \\n<img src=\\\"https://prachimjoshi.files.wordpress.com/2015/07/screen_shot_2010-12-03_at_5-46-21_pm.png\\\" width=\\\"600\\\">\\n<h4 align=\\\"right\\\">Source: https://prachimjoshi.files.wordpress.com</h4>\\nPlease check out [this](https://www.kaggle.com/masumrumi/bagging-with-titanic-dataset) kernel if you want to find out more about bagging classifier. \"},{\"cell_type\":\"code\",\"execution_count\":128,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:39:53.639198Z\",\"iopub.status.busy\":\"2021-06-26T16:39:53.63871Z\",\"iopub.status.idle\":\"2021-06-26T16:40:17.162923Z\",\"shell.execute_reply\":\"2021-06-26T16:40:17.162277Z\",\"shell.execute_reply.started\":\"2021-06-26T16:39:53.638945Z\"}},\"outputs\":[],\"source\":\"from sklearn.ensemble import BaggingClassifier\\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\\n\\nparameters = {'n_estimators':n_estimators,\\n              \\n        }\\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\\n                                      bootstrap_features=False),\\n                                 param_grid=parameters,\\n                                 cv=cv,\\n                                 n_jobs = -1)\\ngrid.fit(X,y) \"},{\"cell_type\":\"code\",\"execution_count\":129,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:40:17.164621Z\",\"iopub.status.busy\":\"2021-06-26T16:40:17.164322Z\",\"iopub.status.idle\":\"2021-06-26T16:40:17.172911Z\",\"shell.execute_reply\":\"2021-06-26T16:40:17.172302Z\",\"shell.execute_reply.started\":\"2021-06-26T16:40:17.164559Z\"}},\"outputs\":[],\"source\":\"print (grid.best_score_)\\nprint (grid.best_params_)\\nprint (grid.best_estimator_)\"},{\"cell_type\":\"code\",\"execution_count\":130,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:40:17.174968Z\",\"iopub.status.busy\":\"2021-06-26T16:40:17.174466Z\",\"iopub.status.idle\":\"2021-06-26T16:40:17.226122Z\",\"shell.execute_reply\":\"2021-06-26T16:40:17.225161Z\",\"shell.execute_reply.started\":\"2021-06-26T16:40:17.174765Z\"}},\"outputs\":[],\"source\":\"bagging_grid = grid.best_estimator_\\nbagging_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<h3>Why use Bagging? (Pros and cons)</h3>\\nBagging works best with strong and complex models(for example, fully developed decision trees). However, don't let that fool you to thinking that similar to a decision tree, bagging also overfits the model. Instead, bagging reduces overfitting since a lot of the sample training data are repeated and used to create base estimators. With a lot of equally likely training data, bagging is not very susceptible to overfitting with noisy data, therefore reduces variance. However, the downside is that this leads to an increase in bias.\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<h4>Random Forest VS. Bagging Classifier</h4>\\n\\nIf some of you are like me, you may find Random Forest to be similar to Bagging Classifier. However, there is a fundamental difference between these two which is **Random Forests ability to pick subsets of features in each node.** I will elaborate on this in a future update.\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"## 7h. AdaBoost Classifier\\n<a id=\\\"AdaBoost\\\"></a>\\n***\\nAdaBoost is another <b>ensemble model</b> and is quite different than Bagging. Let's point out the core concepts. \\n> AdaBoost combines a lot of \\\"weak learners\\\"(they are also called stump; a tree with only one node and two leaves) to make classifications.\\n\\n> This base model fitting is an iterative process where each stump is chained one after the other; <b>It cannot run in parallel.</b>\\n\\n> <b>Some stumps get more say in the final classifications than others.</b> The models use weights that are assigned to each data point/raw indicating their \\\"importance.\\\" Samples with higher weight have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. \\n\\n> <b>Each stump is made by talking the previous stump's mistakes into account.</b> After each iteration weights gets re-calculated in order to take the errors/misclassifications from the last stump into consideration. \\n\\n> The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. \\n\\nTo illustrate what we have talked about so far let's look at the following visualization. \\n\\n<img src=\\\"https://cdn-images-1.medium.com/max/1600/0*paPv7vXuq4eBHZY7.png\\\">\\n<h5 align=\\\"right\\\"> Source: Diogo(Medium)</h5>\\n\\n\\n\\n\\nLet's dive into each one of the nitty-gritty stuff about AdaBoost:\\n***\\n> <b>First</b>, we determine the best feature to split the dataset using Gini index(basics from decision tree). The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain(the lower the Gini index is, the better unmixed the label is, therefore, better split).\\n***\\n> <b>Secondly</b>, we need to determine how much say a stump will have in the final classification and how we can calculate that.\\n* We learn how much say a stump has in the final classification by calculating how well it classified the samples (aka calculate the total error of the weight).\\n* The <b>Total Error</b> for a stump is the sum of the weights associated with the incorrectly classified samples. For example, lets say, we start a stump with 10 datasets. The first stump will uniformly distribute an weight amoung all the datapoints. Which means each data point will have 1/10 weight. Let's say once the weight is distributed we run the model and find 2 incorrect predicitons. In order to calculate the total erorr we add up all the misclassified weights. Here we get 1/10 + 1/10 = 2/10 or 1/5. This is our total error. We can also think about it\\n\\n\\n$$ \\\\epsilon_t = \\\\frac{\\\\text{misclassifications}_t}{\\\\text{observations}_t} $$\\n\\n\\n* Since the weight is uniformly distributed(all add up to 1) among all data points, the total error will always be between 0(perfect stump) and 1(horrible stump).\\n* We use the total error to determine the amount of say a stump has in the final classification using the following formula\\n \\n\\n$$ \\\\alpha_t = \\\\frac{1}{2}ln \\\\left(\\\\frac{1-\\\\epsilon_t}{\\\\epsilon_t}\\\\right) \\\\text{where } \\\\epsilon_t < 1$$\\n\\n\\nWhere $\\\\epsilon_t$ is the misclassification rate for the current classifier:\\n\\n\\n$$ \\\\epsilon_t = \\\\frac{\\\\text{misclassifications}_t}{\\\\text{observations}_t} $$\\n\\n\\nHere...\\n* $\\\\alpha_t$ = Amount of Say\\n* $\\\\epsilon_t$ = Total error\\n\\n\\n\\nWe can draw a graph to determine the amount of say using the value of total error(0 to 1)\\n\\n<img src=\\\"http://chrisjmccormick.files.wordpress.com/2013/12/adaboost_alphacurve.png\\\">\\n<h5 align=\\\"right\\\"> Source: Chris McCormick</h5>\\n\\n* The blue line tells us the amount of say for <b>Total Error(Error rate)</b> between 0 and 1. \\n* When the stump does a reasonably good job, and the <b>total error</b> is minimal, then the <b>amount of say(Alpha)</b> is relatively large, and the alpha value is positive. \\n* When the stump does an average job(similar to a coin flip/the ratio of getting correct and incorrect ~50%/50%), then the <b>total error</b> is ~0.5. In this case the <b>amount of say</b> is <b>0</b>.\\n* When the error rate is high let's say close to 1, then the <b>amount of say</b> will be negative, which means if the stump outputs a value as \\\"survived\\\" the included weight will turn that value into \\\"not survived.\\\"\\n\\nP.S. If the <b>Total Error</b> is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. \\n \\n ***\\n> <b>Third</b>, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. The pseducode for calculating the new sample weight is as follows. \\n\\n\\n$$ New Sample Weight = Sample Weight + e^{\\\\alpha_t}$$\\n\\nHere the $\\\\alpha_t(AmountOfSay)$ can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples; hinting the next stump to put more emphasize on those. Inversely, we want to decrease the sample weight of the correctly classified samples; hinting the next stump to put less emphasize on those. \\n\\nThe following equation help us to do this calculation. \\n\\n$$ D_{t+1}(i) = D_t(i) e^{-\\\\alpha_t y_i h_t(x_i)} $$\\n\\nHere, \\n* $D_{t+1}(i)$ = New Sample Weight. \\n* $D_t(i)$ = Current Sample weight.\\n* $\\\\alpha_t$ = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and \\n* $y_i h_t(x_i)$ = place holder for 1 if stump correctly classified, -1 if misclassified. \\n\\nFinally, we put together the combined classifier, which is \\n\\n$$ AdaBoost(X) = sign\\\\left(\\\\sum_{t=1}^T\\\\alpha_t h_t(X)\\\\right) $$ \\n\\nHere, \\n\\n$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\\n\\n$T$ is the set of \\\"weak learners\\\"\\n\\n$\\\\alpha_t$ is the contribution weight for weak learner $t$\\n\\n$h_t(X)$ is the prediction of weak learner $t$\\n\\nand $y$ is binary **with values -1 and 1**\\n\\n\\nP.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.\\n\\n<h3>Resources:</h3>\\n<ul>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=LsK-xG1cLYA\\\">Statquest</a></li>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=-DUxtdeCiB4\\\">Principles of Machine Learning | AdaBoost(Video)</a></li>\\n</ul>\"},{\"cell_type\":\"code\",\"execution_count\":131,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:40:17.227822Z\",\"iopub.status.busy\":\"2021-06-26T16:40:17.227396Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.311627Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.311009Z\",\"shell.execute_reply.started\":\"2021-06-26T16:40:17.227656Z\"}},\"outputs\":[],\"source\":\"from sklearn.ensemble import AdaBoostClassifier\\nn_estimators = [100,140,145,150,160, 170,175,180,185];\\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\\nlearning_r = [0.1,1,0.01,0.5]\\n\\nparameters = {'n_estimators':n_estimators,\\n              'learning_rate':learning_r\\n              \\n        }\\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\\n                                     ),\\n                                 param_grid=parameters,\\n                                 cv=cv,\\n                                 n_jobs = -1)\\ngrid.fit(X,y) \"},{\"cell_type\":\"code\",\"execution_count\":132,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.313135Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.31287Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.318909Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.318191Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.313088Z\"}},\"outputs\":[],\"source\":\"print (grid.best_score_)\\nprint (grid.best_params_)\\nprint (grid.best_estimator_)\"},{\"cell_type\":\"code\",\"execution_count\":133,\"metadata\":{\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.320845Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.320267Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.35912Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.358535Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.320797Z\"}},\"outputs\":[],\"source\":\"adaBoost_grid = grid.best_estimator_\\nadaBoost_grid.score(X,y)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"## Pros and cons of boosting\\n\\n---\\n\\n### Pros\\n\\n- Achieves higher performance than bagging when hyper-parameters tuned properly.\\n- Can be used for classification and regression equally well.\\n- Easily handles mixed data types.\\n- Can use \\\"robust\\\" loss functions that make the model resistant to outliers.\\n\\n---\\n\\n### Cons\\n\\n- Difficult and time consuming to properly tune hyper-parameters.\\n- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\\n- More risk of overfitting compared to bagging.\\n\\n<h3>Resources: </h3>\\n<ul>\\n    <li><a href=\\\"http://mccormickml.com/2013/12/13/adaboost-tutorial/\\\">AdaBoost Tutorial-Chris McCormick</a></li>\\n    <li><a href=\\\"http://rob.schapire.net/papers/explaining-adaboost.pdf\\\">Explaining AdaBoost by Robert Schapire(One of the original author of AdaBoost)</a></li>\\n</ul>\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"6ea60e91-544f-49fc-8128-ee190e8292e7\",\"_uuid\":\"860921893a28a1fe9a4ce47f0779f1e7b154ca0a\"},\"source\":\"## 7i. Gradient Boosting Classifier\\n<a id=\\\"gradient_boosting\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":134,\"metadata\":{\"_cell_guid\":\"d32d6df9-b8e7-4637-bacc-2baec08547b8\",\"_uuid\":\"fd788c4f4cde834a1329f325f1f59e3f77c37e42\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.360536Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.360265Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.521396Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.520426Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.360479Z\"},\"scrolled\":true},\"outputs\":[],\"source\":\"# Gradient Boosting Classifier\\nfrom sklearn.ensemble import GradientBoostingClassifier\\n\\ngradient_boost = GradientBoostingClassifier()\\ngradient_boost.fit(X, y)\\ny_pred = gradient_boost.predict(X_test)\\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\\nprint(gradient_accy)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<div class=\\\" alert alert-info\\\">\\n<h3>Resources: </h3>\\n<ul>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=sDv4f4s2SB8\\\">Gradient Descent(StatQuest)</a></li>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=3CC4N4z3GJc\\\">Gradient Boost(Regression Main Ideas)(StatQuest)</a></li>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=3CC4N4z3GJc\\\">Gradient Boost(Regression Calculation)(StatQuest)</a></li>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=jxuNLH5dXCs\\\">Gradient Boost(Classification Main Ideas)(StatQuest)</a></li>\\n    <li><a href=\\\"https://www.youtube.com/watch?v=StWY5QWMXCw\\\">Gradient Boost(Classification Calculation)(StatQuest)</a></li>\\n    <li><a href=\\\"https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\\\">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>\\n</ul>\\n</div>\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"996b8ee8-13ff-461d-8f7b-ac0d7d488cff\",\"_uuid\":\"ee9c7a2ccdf93a90f929b6618105afbe699bd6de\"},\"source\":\"## 7j. XGBClassifier\\n<a id=\\\"XGBClassifier\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":135,\"metadata\":{\"_cell_guid\":\"5d94cc5b-d8b7-40d3-b264-138539daabfa\",\"_uuid\":\"9d96154d2267ea26a6682a73bd1850026eb1303b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.523177Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.522724Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.526955Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.525945Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.522964Z\"}},\"outputs\":[],\"source\":\"# from xgboost import XGBClassifier\\n# XGBClassifier = XGBClassifier()\\n# XGBClassifier.fit(X, y)\\n# y_pred = XGBClassifier.predict(X_test)\\n# XGBClassifier_accy = round(accuracy_score(y_pred, y_test), 3)\\n# print(XGBClassifier_accy)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"a6b4c23c-b42b-4fad-b37d-c84154b3478d\",\"_uuid\":\"3fa68b3d2e835b1a14088102561a2f8d4dac8f5c\"},\"source\":\"## 7k. Extra Trees Classifier\\n<a id=\\\"extra_tree\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":136,\"metadata\":{\"_cell_guid\":\"2e567e01-6b5f-4313-84af-cc378c3b709e\",\"_uuid\":\"c9b958e2488adf6f79401c677087e3250d63ac9b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.528841Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.528382Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.555697Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.554889Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.528664Z\"}},\"outputs\":[],\"source\":\"from sklearn.ensemble import ExtraTreesClassifier\\nExtraTreesClassifier = ExtraTreesClassifier()\\nExtraTreesClassifier.fit(X, y)\\ny_pred = ExtraTreesClassifier.predict(X_test)\\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\\nprint(extraTree_accy)\\n\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"20a66dcc-7f9f-4802-aa6d-58be75e07539\",\"_uuid\":\"c55a54821feda82c75dde28bab7e2cf4445c4cf0\"},\"source\":\"## 7l. Gaussian Process Classifier\\n<a id=\\\"GaussianProcessClassifier\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":137,\"metadata\":{\"_cell_guid\":\"23bd5744-e04d-49bb-9d70-7c2a518f76dd\",\"_uuid\":\"57fc008eea2ce1c0b595f888a82ddeaee6ce2177\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.557268Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.556845Z\",\"iopub.status.idle\":\"2021-06-26T16:41:28.863352Z\",\"shell.execute_reply\":\"2021-06-26T16:41:28.862576Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.557221Z\"}},\"outputs\":[],\"source\":\"from sklearn.gaussian_process import GaussianProcessClassifier\\nGaussianProcessClassifier = GaussianProcessClassifier()\\nGaussianProcessClassifier.fit(X, y)\\ny_pred = GaussianProcessClassifier.predict(X_test)\\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\\nprint(gau_pro_accy)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"ec676e4d-0cbe-43fa-9ff8-92d76030faef\",\"_uuid\":\"6f89f2cb63120a4594c7b0f2883b6872aa444700\"},\"source\":\"## 7m. Voting Classifier\\n<a id=\\\"voting_classifer\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":138,\"metadata\":{\"_cell_guid\":\"ac208dd3-1045-47bb-9512-de5ecb5c81b0\",\"_uuid\":\"821c74bbf404193219eb91fe53755d669f5a14d1\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:28.865063Z\",\"iopub.status.busy\":\"2021-06-26T16:41:28.86463Z\",\"iopub.status.idle\":\"2021-06-26T16:41:30.314425Z\",\"shell.execute_reply\":\"2021-06-26T16:41:30.313671Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:28.865013Z\"}},\"outputs\":[],\"source\":\"from sklearn.ensemble import VotingClassifier\\n\\nvoting_classifier = VotingClassifier(estimators=[\\n    ('lr_grid', logreg_grid),\\n    ('svc', svm_grid),\\n    ('random_forest', rf_grid),\\n    ('gradient_boosting', gradient_boost),\\n    ('decision_tree_grid',dectree_grid),\\n    ('knn_classifier', knn_grid),\\n#     ('XGB_Classifier', XGBClassifier),\\n    ('bagging_classifier', bagging_grid),\\n    ('adaBoost_classifier',adaBoost_grid),\\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\\n    ('gaussian_classifier',gaussian),\\n    ('gaussian_process_classifier', GaussianProcessClassifier)\\n],voting='hard')\\n\\n#voting_classifier = voting_classifier.fit(train_x,train_y)\\nvoting_classifier = voting_classifier.fit(X,y)\"},{\"cell_type\":\"code\",\"execution_count\":139,\"metadata\":{\"_cell_guid\":\"648ac6a6-2437-490a-bf76-1612a71126e8\",\"_uuid\":\"518a02ae91cc91d618e476d1fc643cd3912ee5fb\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:30.316454Z\",\"iopub.status.busy\":\"2021-06-26T16:41:30.316008Z\",\"iopub.status.idle\":\"2021-06-26T16:41:30.42114Z\",\"shell.execute_reply\":\"2021-06-26T16:41:30.420152Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:30.31627Z\"}},\"outputs\":[],\"source\":\"y_pred = voting_classifier.predict(X_test)\\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\\nprint(voting_accy)\"},{\"cell_type\":\"code\",\"execution_count\":140,\"metadata\":{\"_cell_guid\":\"277534eb-7ec8-4359-a2f4-30f7f76611b8\",\"_kg_hide-input\":true,\"_uuid\":\"00a9b98fd4e230db427a63596a2747f05b1654c1\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:30.422908Z\",\"iopub.status.busy\":\"2021-06-26T16:41:30.422475Z\",\"iopub.status.idle\":\"2021-06-26T16:41:30.426856Z\",\"shell.execute_reply\":\"2021-06-26T16:41:30.425882Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:30.422736Z\"}},\"outputs\":[],\"source\":\"#models = pd.DataFrame({\\n#    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n#              'Random Forest', 'Naive Bayes', \\n#              'Decision Tree', 'Gradient Boosting Classifier', 'Voting Classifier', 'XGB Classifier','ExtraTrees Classifier','Bagging Classifier'],\\n#    'Score': [svc_accy, knn_accy, logreg_accy, \\n#              random_accy, gaussian_accy, dectree_accy,\\n#               gradient_accy, voting_accy, XGBClassifier_accy, extraTree_accy, bagging_accy]})\\n#models.sort_values(by='Score', ascending=False)\"},{\"cell_type\":\"markdown\",\"metadata\":{\"_cell_guid\":\"7128f3dd-1d8d-4b8e-afb4-891d8cb9657c\",\"_uuid\":\"7e17482a69dbe99319219a603ea39f8bbde98b87\"},\"source\":\"# Part 8: Submit test predictions\\n<a id=\\\"submit_predictions\\\"></a>\\n***\"},{\"cell_type\":\"code\",\"execution_count\":141,\"metadata\":{\"_uuid\":\"eb0054822f296ba86aa6005b2a5e35fbc1aec88b\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:30.429099Z\",\"iopub.status.busy\":\"2021-06-26T16:41:30.42862Z\",\"iopub.status.idle\":\"2021-06-26T16:41:30.646363Z\",\"shell.execute_reply\":\"2021-06-26T16:41:30.645616Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:30.428903Z\"}},\"outputs\":[],\"source\":\"all_models = [logreg_grid,\\n              knn_grid, \\n              knn_ran_grid,\\n              svm_grid,\\n              dectree_grid,\\n              rf_grid,\\n              bagging_grid,\\n              adaBoost_grid,\\n              voting_classifier]\\n\\nc = {}\\nfor i in all_models:\\n    a = i.predict(X_test)\\n    b = accuracy_score(a, y_test)\\n    c[i] = b\\n    \\n\"},{\"cell_type\":\"code\",\"execution_count\":142,\"metadata\":{\"_cell_guid\":\"51368e53-52e4-41cf-9cc9-af6164c9c6f5\",\"_uuid\":\"b947f168f6655c1c6eadaf53f3485d57c0cd74c7\",\"execution\":{\"iopub.execute_input\":\"2021-06-26T16:41:30.648318Z\",\"iopub.status.busy\":\"2021-06-26T16:41:30.647987Z\",\"iopub.status.idle\":\"2021-06-26T16:41:32.045557Z\",\"shell.execute_reply\":\"2021-06-26T16:41:32.044733Z\",\"shell.execute_reply.started\":\"2021-06-26T16:41:30.648259Z\"}},\"outputs\":[],\"source\":\"test_prediction = (max(c, key=c.get)).predict(test)\\nsubmission = pd.DataFrame({\\n        \\\"PassengerId\\\": passengerid,\\n        \\\"Survived\\\": test_prediction\\n    })\\n\\nsubmission.PassengerId = submission.PassengerId.astype(int)\\nsubmission.Survived = submission.Survived.astype(int)\\n\\nsubmission.to_csv(\\\"titanic1_submission.csv\\\", index=False)\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<div class=\\\"alert alert-info\\\">\\n    <h1>Resources</h1>\\n    <ul>\\n        <li><b>Statistics</b></li>\\n        <ul>\\n            <li><a href=\\\"https://statistics.laerd.com/statistical-guides/measures-of-spread-standard-deviation.php\\\">Types of Standard Deviation</a></li>\\n            <li><a href=\\\"https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen\\\">What Is a t-test? And Why Is It Like Telling a Kid to Clean Up that Mess in the Kitchen?</a></li>\\n            <li><a href=\\\"https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-t-values-and-p-values-in-statistics\\\">What Are T Values and P Values in Statistics?</a></li>\\n            <li><a href=\\\"https://www.youtube.com/watch?v=E4KCfcVwzyw\\\">What is p-value? How we decide on our confidence level.</a></li>\\n        </ul>\\n        <li><b>Writing pythonic code</b></li>\\n        <ul>\\n            <li><a href=\\\"https://www.kaggle.com/rtatman/six-steps-to-more-professional-data-science-code\\\">Six steps to more professional data science code</a></li>\\n            <li><a href=\\\"https://www.kaggle.com/jpmiller/creating-a-good-analytics-report\\\">Creating a Good Analytics Report</a></li>\\n            <li><a href=\\\"https://en.wikipedia.org/wiki/Code_smell\\\">Code Smell</a></li>\\n            <li><a href=\\\"https://www.python.org/dev/peps/pep-0008/\\\">Python style guides</a></li>\\n            <li><a href=\\\"https://gist.github.com/sloria/7001839\\\">The Best of the Best Practices(BOBP) Guide for Python</a></li>\\n            <li><a href=\\\"https://www.python.org/dev/peps/pep-0020/\\\">PEP 20 -- The Zen of Python</a></li>\\n            <li><a href=\\\"https://docs.python-guide.org/\\\">The Hitchiker's Guide to Python</a></li>\\n            <li><a href=\\\"https://realpython.com/tutorials/best-practices/\\\">Python Best Practice Patterns</a></li>\\n            <li><a href=\\\"http://www.nilunder.com/blog/2013/08/03/pythonic-sensibilities/\\\">Pythonic Sensibilities</a></li>\\n        </ul>\\n        <li><b>Why Scikit-Learn?</b></li>\\n        <ul>\\n            <li><a href=\\\"https://www.oreilly.com/content/intro-to-scikit-learn/\\\">Introduction to Scikit-Learn</a></li>\\n            <li><a href=\\\"https://www.oreilly.com/content/six-reasons-why-i-recommend-scikit-learn/\\\">Six reasons why I recommend scikit-learn</a></li>\\n            <li><a href=\\\"https://hub.packtpub.com/learn-scikit-learn/\\\">Why you should learn Scikit-learn</a></li>\\n            <li><a href=\\\"https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\\\">A Deep Dive Into Sklearn Pipelines</a></li>\\n            <li><a href=\\\"https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial\\\">Sklearn pipelines tutorial</a></li>\\n            <li><a href=\\\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\\\">Managing Machine Learning workflows with Sklearn pipelines</a></li>\\n            <li><a href=\\\"https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\\\">A simple example of pipeline in Machine Learning using SKlearn</a></li>\\n        </ul>\\n    </ul>\\n    <h1>Credits</h1>\\n    <ul>\\n        <li>To Brandon Foltz for his <a href=\\\"https://www.youtube.com/channel/UCFrjdcImgcQVyFbK04MBEhA\\\">youtube</a> channel and for being an amazing teacher.</li>\\n        <li>To GA where I started my data science journey.</li>\\n        <li>To Kaggle community for inspiring me over and over again with all the resources I need.</li>\\n        <li>To Udemy Course \\\"Deployment of Machine Learning\\\". I have used and modified some of the code from this course to help making the learning process intuitive.</li>\\n    </ul>\\n</div>\"},{\"cell_type\":\"markdown\",\"metadata\":{},\"source\":\"<div class=\\\"alert alert-info\\\">\\n<h4>If you like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:</h4>\\n    <ul>\\n        <li><a href=\\\"https://www.linkedin.com/in/masumrumi/\\\"><b>LinkedIn</b></a></li>\\n        <li><a href=\\\"https://github.com/masumrumi\\\"><b>Github</b></a></li>\\n        <li><a href=\\\"https://masumrumi.github.io/cv/\\\"><b>masumrumi.github.io/cv/</b></a></li>\\n        <li><a href=\\\"https://www.youtube.com/channel/UC1mPjGyLcZmsMgZ8SJgrfdw\\\"><b>Youtube</b></a></li>\\n    </ul>\\n\\n<p>This kernel will always be a work in progress. I will incorporate new concepts of data science as I comprehend them with each update. If you have any idea/suggestions about this notebook, please let me know. Any feedback about further improvements would be genuinely appreciated.</p>\\n\\n<h1>If you have come this far, Congratulations!!</h1>\\n\\n<h1>If this notebook helped you in any way or you liked it, please upvote and/or leave a comment!! :)</h1></div>\"}],\"metadata\":{\"interpreter\":{\"hash\":\"4ee7c4e4684082ce1f46e630280ae5702684ea5c5635036c275c1be52ef87f64\"},\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"},\"language_info\":{\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"file_extension\":\".py\",\"mimetype\":\"text/x-python\",\"name\":\"python\",\"nbconvert_exporter\":\"python\",\"pygments_lexer\":\"ipython3\",\"version\":\"3.8.8\"}},\"nbformat\":4,\"nbformat_minor\":4}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb b/dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb
--- a/dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/titanic/Masum Rumi/a-statistical-analysis-ml-workflow-of-titanic.ipynb	(date 1658512097885)
@@ -1,1 +1,3292 @@
-{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9c75ca41-8357-479e-8a46-ebdec5f035f3","_uuid":"319ae25236d9fddf1745ea1c4cb365e5dbb00372"},"source":"<img src=\"http://data.freehdw.com/ships-titanic-vehicles-best.jpg\"  Width=\"800\">"},{"cell_type":"markdown","metadata":{"_uuid":"bdce3bc433feb19f6622ab910cfe2123ccd07a1c"},"source":"<a id=\"introduction\" ></a><br>\nThis kernel is for all aspiring data scientists to learn from and to review their knowledge. We will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation. I am super excited to share my first kernel with the Kaggle community. As I go on in this journey and learn new topics, I will incorporate them with each new updates. So, check for them and please <b>leave a comment</b> if you have any suggestions to make this kernel better!! Going back to the topics of this kernel, I will do more in-depth visualizations to explain the data, and the machine learning classifiers will be used to predict passenger survival status.\n\n<div style=\"text-align: left\"> \n    <br>\n    NOTE:\n    <ul>\n        <li>Follow me on <a href=\"https://www.youtube.com/channel/UC1mPjGyLcZmsMgZ8SJgrfdw\"><b>YOUTUBE</b></a> to get the video tutorial for this notebook.\n        <li>If you want to learn more about Advanced Regression models, please check out <a href=\"https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing\">this</a> kernel.</li>\n        <li>If you are reading this on github, I recommend you read this on <a href=\"https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic\">kaggle</a>.</li>\n    </ul>\n</div>"},{"cell_type":"markdown","metadata":{"_cell_guid":"7224a910-ec6b-481d-82f1-90ca6b5d037e","_uuid":"9cd04af82734c5b53aaddc80992e1f499c180611"},"source":"# Kernel Goals\n<a id=\"aboutthiskernel\"></a>\n***\nThere are three primary goals of this kernel.\n- <b>Do a statistical analysis</b> of how some group of people was survived more than others. \n- <b>Do an exploratory data analysis(EDA)</b> of titanic with visualizations and storytelling.  \n- <b>Predict</b>: Use machine learning classification models to predict the chances of passengers survival.\n\nP.S. If you want to learn more about regression models, try this [kernel](https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing/edit/run/9585160). "},{"cell_type":"markdown","metadata":{"_cell_guid":"b3b559a5-dad0-419e-835a-e6babd1042ff","_uuid":"1b1a0b28ad37a349e284d1e6ce6477d11b95e7c9"},"source":"# Part 1: Importing Necessary Libraries and datasets\n***\n<a id=\"import_libraries**\"></a>\n## 1a. Loading libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. "},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"80643cb5-64f3-4180-92a9-2f8e83263ac6","_kg_hide-input":true,"_uuid":"33d54abf387474bce3017f1fc3832493355010c0","tags":[]},"outputs":[],"source":"# Import necessary modules\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline \n# %config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \n\nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\n\n\n\nimport os ## imporing os\nprint(os.listdir(\"../input/\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd41125b-6dd4-41d9-8905-31edc812d18e","_uuid":"82ccd43cc8449346749bf8a35e1acb9a40e3b141"},"source":"## 1b. Loading Datasets\n<a id=\"load_data\"></a>\n***"},{"cell_type":"markdown","metadata":{"_uuid":"30b23f046eef6d19c26e6ad967cef914cf312791"},"source":"After loading the necessary modules, we need to import the datasets. Many of the business problems usually come with a tremendous amount of messy data. We extract those data from many sources. I am hoping to write about that in a different kernel. For now, we are going to work with a less complicated and quite popular machine learning dataset."},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"28722a45-5f11-4629-8814-9ab913e9349a","_kg_hide-input":false,"_uuid":"185b34e70f2efded0c665c6713f79b840ddf0c89","execution":{"iopub.execute_input":"2021-06-26T16:35:04.910168Z","iopub.status.busy":"2021-06-26T16:35:04.909538Z","iopub.status.idle":"2021-06-26T16:35:04.930876Z","shell.execute_reply":"2021-06-26T16:35:04.930259Z","shell.execute_reply.started":"2021-06-26T16:35:04.91012Z"}},"outputs":[],"source":"## Importing the datasets\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")"},{"cell_type":"markdown","metadata":{"_uuid":"d55ae33391486797b979ef1117e8d8401ac1dab4"},"source":"You are probably wondering why two datasets? Also, Why have I named it \"train\" and \"test\"?  To explain that I am going to give you an overall picture of the supervised machine learning process. \n\n\"Machine Learning\" is simply \"Machine\" and \"Learning\". Nothing more and nothing less. In a supervised machine learning process, we are giving machine/computer/models specific inputs or data(text/number/image/audio) to learn from aka we are training the machine to learn certain aspects based on the data and the output. Now, how can we determine that machine is actually learning what we are try to teach? That is where the test set comes to play. We withhold part of the data where we know the output/result of each datapoints, and we use this data to test the trained models.  We then compare the outcomes to determine the performance of the algorithms. If you are a bit confused thats okay. I will explain more as we keep reading. Let's take a look at sample datasets."},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"temp = train.groupby(\"Sex\")['Age'].min().to_frame().reset_index()"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"temp"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"temp = temp.rename(columns={\"Age\": \"min_age\"})"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"temp"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"train.dtypes"},{"cell_type":"markdown","metadata":{"_cell_guid":"c87c72ba-c9b2-48e9-86d8-c711d0795ca0","_uuid":"5759d720798ca115cc5d3d2f75be6961d1455832"},"source":"## 1c. A Glimpse of the Datasets. \n<a id=\"glimpse\"></a>\n***"},{"cell_type":"markdown","metadata":{},"source":"# Train Set"},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:04.932322Z","iopub.status.busy":"2021-06-26T16:35:04.932015Z","iopub.status.idle":"2021-06-26T16:35:04.958015Z","shell.execute_reply":"2021-06-26T16:35:04.957084Z","shell.execute_reply.started":"2021-06-26T16:35:04.932234Z"}},"outputs":[],"source":"%%time\ntrain.sample(5)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ab439b32-e251-489a-89fd-cfcd61b236bf","_uuid":"69b24241db4d4eae9e46711c384d8130f6fa8322"},"source":"# Test Set"},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"0f0649fa-b003-403f-9d7c-d2d14a6cf068","_kg_hide-input":true,"_uuid":"877b2fc905cd60e3f9a525b6fedad9a5c0a671e5","execution":{"iopub.execute_input":"2021-06-26T16:35:04.960536Z","iopub.status.busy":"2021-06-26T16:35:04.960114Z","iopub.status.idle":"2021-06-26T16:35:08.927166Z","shell.execute_reply":"2021-06-26T16:35:08.926315Z","shell.execute_reply.started":"2021-06-26T16:35:04.960357Z"}},"outputs":[],"source":"## Take a look at the overview of the dataset. \n%timeit test.sample(5)"},{"cell_type":"markdown","metadata":{"_uuid":"5f7426639cf97db92e4ca85a13e89c8394f6aa7c"},"source":"This is a sample of train and test dataset. Lets find out a bit more about the train and test dataset. "},{"cell_type":"code","execution_count":12,"metadata":{"_kg_hide-input":true,"_uuid":"1258a94388599a131fe08cd6e05205b15d53df66","execution":{"iopub.execute_input":"2021-06-26T16:35:08.929903Z","iopub.status.busy":"2021-06-26T16:35:08.929444Z","iopub.status.idle":"2021-06-26T16:35:08.945917Z","shell.execute_reply":"2021-06-26T16:35:08.945011Z","shell.execute_reply.started":"2021-06-26T16:35:08.92985Z"}},"outputs":[],"source":"print (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (train.info())\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))\nprint (test.info())"},{"cell_type":"markdown","metadata":{"_cell_guid":"15c64d36-94b3-4798-af86-775f70feb2dd","_uuid":"c72d21139ee6220aee5d8f654561864a5f6499b7"},"source":" ## 1d. About This Dataset\n<a id=\"aboutthisdataset\"></a>\n***\nThe data has split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\n***The training set includes our target variable(dependent variable), passenger survival status***(also known as the ground truth from the Titanic tragedy) along with other independent features like gender, class, fare, and Pclass. \n\nThe test set should be used to see how well our model performs on unseen data. When we say unseen data, we mean that the algorithm or machine learning models have no relation to the test data. We do not want to use any part of the test data in any way to modify our algorithms; Which are the reasons why we clean our test data and train data separately. ***The test set does not provide passengers survival status***. We are going to use our model to predict passenger survival status.\n\nNow let's go through the features and describe a little. There is a couple of different type of variables, They are...\n\n***\n**Categorical:**\n- **Nominal**(variables that have two or more categories, but which do not have an intrinsic order.)\n   > - **Cabin**\n   > - **Embarked**(Port of Embarkation)\n            C(Cherbourg)\n            Q(Queenstown) \n            S(Southampton)\n        \n- **Dichotomous**(Nominal variable with only two categories)\n   > - **Sex**\n            Female\n            Male\n- **Ordinal**(variables that have two or more categories just like nominal variables. Only the categories can also be ordered or ranked.)\n   > - **Pclass** (A proxy for socio-economic status (SES)) \n            1(Upper)\n            2(Middle) \n            3(Lower)\n***\n**Numeric:**\n- **Discrete**\n  >  - **Passenger ID**(Unique identifing # for each passenger)\n  >  - **SibSp**\n  >  - **Parch**\n  >  - **Survived** (Our outcome or dependent variable)\n            0\n            1\n- **Continous**\n>  - **Age**\n>  - **Fare**\n***\n**Text Variable**\n> - **Ticket** (Ticket number for passenger.)\n> - **Name**(  Name of the passenger.) \n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b21d695-c767-48ad-a3c8-abb9bba56e71","_uuid":"53fdd02b149e47bd7168dba94ddff754626b1781"},"source":"## 1e. Tableau Visualization of the Data\n<a id='tableau_visualization'></a>\n***\nI have incorporated a tableau visualization below of the training data. This visualization... \n* is for us to have an overview and play around with the dataset. \n* is done without making any changes(including Null values) to any features of the dataset.\n***\nLet's get a better perspective of the dataset through this visualization.\n"},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"0ca9339e-4d13-4eb6-b28b-4a9e614ca2d0","_kg_hide-input":true,"_uuid":"bc9819aecc9adceb1fa3fe151388fd41f5dcece2","execution":{"iopub.execute_input":"2021-06-26T16:35:08.947896Z","iopub.status.busy":"2021-06-26T16:35:08.947379Z","iopub.status.idle":"2021-06-26T16:35:08.954197Z","shell.execute_reply":"2021-06-26T16:35:08.953477Z","shell.execute_reply.started":"2021-06-26T16:35:08.947651Z"}},"outputs":[],"source":"%%HTML\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Titanic_data_mining&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"},{"cell_type":"markdown","metadata":{"_cell_guid":"2b6ce9bc-8210-433d-ab4b-d8afe93c3810","_uuid":"b46be01bb1ba3ff4f23c72038679542ba3f780de"},"source":"We want to see how the left vertical bar changes when we filter out unique values of certain features. We can use multiple filters to see if there are any correlations among them. For example, if we click on **upper** and **Female** tab, we would see that green color dominates the bar with a ratio of 91:3 survived and non survived female passengers; a 97% survival rate for females. We can reset the filters by clicking anywhere in the whilte space. The age distribution chart on top provides us with some more info such as, what was the age range of those three unlucky females as the red color give away the unsurvived once. If you would like to check out some of my other tableau charts, please click [here.](https://public.tableau.com/profile/masum.rumi#!/)"},{"cell_type":"markdown","metadata":{"_cell_guid":"24dfbb58-4708-42a1-9122-c7e0b96ad0e9","_uuid":"e789474652ddf03c65e7bb8f17f69544b907cecb"},"source":"# Part 2: Overview and Cleaning the Data\n<a id=\"cleaningthedata\"></a>\n***\n## 2a. Overview"},{"cell_type":"markdown","metadata":{"_cell_guid":"359e6e3e-3a27-45aa-b6cf-ec18b8220eae","_uuid":"f0ec8e9300f40427a2a53f9c3e3f92e120ce786b"},"source":"Datasets in the real world are often messy, However, this dataset is almost clean. Lets analyze and see what we have here."},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"bf19c831-fbe0-49b6-8bf8-d7db118f40b1","_kg_hide-input":true,"_uuid":"5a0593fb4564f0284ca7fdf5c006020cb288db95","execution":{"iopub.execute_input":"2021-06-26T16:35:08.956119Z","iopub.status.busy":"2021-06-26T16:35:08.955538Z","iopub.status.idle":"2021-06-26T16:35:08.973222Z","shell.execute_reply":"2021-06-26T16:35:08.972151Z","shell.execute_reply.started":"2021-06-26T16:35:08.956072Z"}},"outputs":[],"source":"## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n## We will drop PassengerID and Ticket since it will be useless for our data. \n#train.drop(['PassengerId'], axis=1, inplace=True)\n#test.drop(['PassengerId'], axis=1, inplace=True)\n\nprint (train.info())\nprint (\"*\"*40)\nprint (test.info())"},{"cell_type":"markdown","metadata":{"_cell_guid":"f9b2f56f-e95c-478d-aa49-3f6cb277830f","_uuid":"b5accab7fc7471fea224dcae81683b9f3c0f617b"},"source":"It looks like, the features have unequal amount of data entries for every column and they have many different types of variables. This can happen for the following reasons...\n* We may have missing values in our features.\n* We may have categorical features. \n* We may have alphanumerical or/and text features. \n"},{"cell_type":"markdown","metadata":{"_cell_guid":"9912539a-12b5-4739-bc2c-e1cecf758dca","_uuid":"6105e90cd8f0e8d49ae188edad65414678a7be23"},"source":"## 2b. Dealing with Missing values\n<a id=\"dealwithnullvalues\"></a>\n***\n**Missing values in *train* dataset.**"},{"cell_type":"code","execution_count":15,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:08.975451Z","iopub.status.busy":"2021-06-26T16:35:08.974927Z","iopub.status.idle":"2021-06-26T16:35:08.98326Z","shell.execute_reply":"2021-06-26T16:35:08.982644Z","shell.execute_reply.started":"2021-06-26T16:35:08.975205Z"}},"outputs":[],"source":"# Let's write a functin to print the total percentage of the missing values.(this can be a good exercise for beginners to try to write simple functions like this.)\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])"},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:08.985334Z","iopub.status.busy":"2021-06-26T16:35:08.984844Z","iopub.status.idle":"2021-06-26T16:35:09.090701Z","shell.execute_reply":"2021-06-26T16:35:09.090116Z","shell.execute_reply.started":"2021-06-26T16:35:08.985168Z"}},"outputs":[],"source":"%timeit -r2 -n10 missing_percentage(train) # setting the number of runs(-r) and/or loops (-n)"},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:09.092256Z","iopub.status.busy":"2021-06-26T16:35:09.09199Z","iopub.status.idle":"2021-06-26T16:35:09.108063Z","shell.execute_reply":"2021-06-26T16:35:09.107054Z","shell.execute_reply.started":"2021-06-26T16:35:09.092212Z"}},"outputs":[],"source":"missing_percentage(train)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e1b2b57-78b9-4021-bc53-a7681b63f97c","_uuid":"197487867c9d099548c7d009c4a80418927be07c"},"source":"**Missing values in *test* set.**"},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:09.110118Z","iopub.status.busy":"2021-06-26T16:35:09.109653Z","iopub.status.idle":"2021-06-26T16:35:09.206762Z","shell.execute_reply":"2021-06-26T16:35:09.205452Z","shell.execute_reply.started":"2021-06-26T16:35:09.10993Z"}},"outputs":[],"source":"%%timeit -r2 -n10 \nmissing_percentage(test)"},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"073ef91b-e401-47a1-9b0a-d08ad710abce","_kg_hide-input":true,"_uuid":"1ec1de271f57c9435ce111261ba08c5d6e34dbcb","execution":{"iopub.execute_input":"2021-06-26T16:35:09.208229Z","iopub.status.busy":"2021-06-26T16:35:09.207968Z","iopub.status.idle":"2021-06-26T16:35:09.221423Z","shell.execute_reply":"2021-06-26T16:35:09.220732Z","shell.execute_reply.started":"2021-06-26T16:35:09.208186Z"}},"outputs":[],"source":"missing_percentage(test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0217a17b-8017-4221-a664-dbbc42f7a5eb","_uuid":"2051377dfc36cbeb9fda78cb02d5bd3a00ee2457"},"source":"We see that in both **train**, and **test** dataset have missing values. Let's make an effort to fill these missing values starting with \"Embarked\" feature. "},{"cell_type":"markdown","metadata":{"_cell_guid":"aaf73f0b-ec84-4da1-b424-0170691c50c8","_uuid":"84d3c45c3a59e16ac2c887d6effe71434b2659ef"},"source":"### Embarked feature\n***"},{"cell_type":"code","execution_count":20,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:09.223175Z","iopub.status.busy":"2021-06-26T16:35:09.222681Z","iopub.status.idle":"2021-06-26T16:35:09.230671Z","shell.execute_reply":"2021-06-26T16:35:09.229793Z","shell.execute_reply.started":"2021-06-26T16:35:09.223128Z"}},"outputs":[],"source":"def percent_value_counts(df, feature):\n    \"\"\"This function takes in a dataframe and a column and finds the percentage of the value_counts\"\"\"\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n    ## creating a df with th\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    ## concating percent and total dataframe\n\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([total, percent], axis = 1)\n    "},{"cell_type":"code","execution_count":21,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:09.236974Z","iopub.status.busy":"2021-06-26T16:35:09.236548Z","iopub.status.idle":"2021-06-26T16:35:09.254321Z","shell.execute_reply":"2021-06-26T16:35:09.253654Z","shell.execute_reply.started":"2021-06-26T16:35:09.236929Z"}},"outputs":[],"source":"percent_value_counts(train, 'Embarked')"},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:09.259474Z","iopub.status.busy":"2021-06-26T16:35:09.259268Z","iopub.status.idle":"2021-06-26T16:35:09.274228Z","shell.execute_reply":"2021-06-26T16:35:09.27333Z","shell.execute_reply.started":"2021-06-26T16:35:09.259433Z"}},"outputs":[],"source":"percent_value_counts(train, 'Embarked')"},{"cell_type":"markdown","metadata":{"_cell_guid":"826ae31d-4bd0-45f6-8c05-8b5d12d41144","_uuid":"174873ebdb2cd6c23777d464103afa26c0183ab2"},"source":"It looks like there are only two null values( ~ 0.22 %) in the Embarked feature, we can replace these with the mode value \"S\". However, let's dig a little deeper. \n\n**Let's see what are those two null values**"},{"cell_type":"code","execution_count":23,"metadata":{"_cell_guid":"000ebdd7-ff57-48d9-91bf-a29ba79f1a1c","_kg_hide-input":true,"_uuid":"6b9cb050e9dae424bb738ba9cdf3c84715887fa3","execution":{"iopub.execute_input":"2021-06-26T16:35:09.276102Z","iopub.status.busy":"2021-06-26T16:35:09.275649Z","iopub.status.idle":"2021-06-26T16:35:09.292037Z","shell.execute_reply":"2021-06-26T16:35:09.291163Z","shell.execute_reply.started":"2021-06-26T16:35:09.275879Z"}},"outputs":[],"source":"train[train.Embarked.isnull()]"},{"cell_type":"markdown","metadata":{"_cell_guid":"306da283-fbd9-45fc-a79e-ac4a3fa7d396","_uuid":"57a4016a0ff673cdf5716310d42d7f142d275132"},"source":"We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values"},{"cell_type":"code","execution_count":24,"metadata":{"_cell_guid":"bf257322-0c9c-4fc5-8790-87d8c94ad28a","_kg_hide-input":true,"_uuid":"ad15052fe6cebe37161c6e01e33a5c083dc2b558","execution":{"iopub.execute_input":"2021-06-26T16:35:09.293919Z","iopub.status.busy":"2021-06-26T16:35:09.293564Z","iopub.status.idle":"2021-06-26T16:35:09.866643Z","shell.execute_reply":"2021-06-26T16:35:09.865701Z","shell.execute_reply.started":"2021-06-26T16:35:09.293817Z"}},"outputs":[],"source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 18)\nax2.set_title('Test Set',  fontsize = 18)\n\n\n# ## Fixing legends\n# leg_1 = ax1.get_legend()\n# leg_1.set_title(\"PClass\")\n# legs = leg_1.texts\n# legs[0].set_text('Upper')\n# legs[1].set_text('Middle')\n# legs[2].set_text('Lower')\n\nfig.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e353881-a7d7-4fbf-bfd3-874479c0a650","_uuid":"c8a7f8c033f571d2fc8986009765ac4a78d3b6a7"},"source":"Here, in both training set and test set, the average fare closest to $80 are in the <b>C</b> Embarked values where pclass is 1. So, let's fill in the missing values as \"C\" "},{"cell_type":"code","execution_count":25,"metadata":{"_cell_guid":"2f5f3c63-d22c-483c-a688-a5ec2a477330","_kg_hide-input":true,"_uuid":"52e51ada5dfeb700bf775c66e9307d6d1e2233de","execution":{"iopub.execute_input":"2021-06-26T16:35:09.868523Z","iopub.status.busy":"2021-06-26T16:35:09.868016Z","iopub.status.idle":"2021-06-26T16:35:09.874135Z","shell.execute_reply":"2021-06-26T16:35:09.873022Z","shell.execute_reply.started":"2021-06-26T16:35:09.868249Z"},"scrolled":true},"outputs":[],"source":"## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"47c17b1e-9486-43da-84ad-f91014225e88","_uuid":"44af808c1563671899ee498c9df12312c294277c"},"source":"### Cabin Feature\n***"},{"cell_type":"code","execution_count":26,"metadata":{"_cell_guid":"e76cd770-b498-4444-b47a-4ac6ae63193b","_kg_hide-input":true,"_uuid":"b809a788784e2fb443457d7ef4ca17a896bf58b4","execution":{"iopub.execute_input":"2021-06-26T16:35:09.876171Z","iopub.status.busy":"2021-06-26T16:35:09.875621Z","iopub.status.idle":"2021-06-26T16:35:09.886193Z","shell.execute_reply":"2021-06-26T16:35:09.885088Z","shell.execute_reply.started":"2021-06-26T16:35:09.875859Z"},"scrolled":true},"outputs":[],"source":"print(\"Train Cabin missing: \" + str(train.Cabin.isnull().sum()/len(train.Cabin)))\nprint(\"Test Cabin missing: \" + str(test.Cabin.isnull().sum()/len(test.Cabin)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"47d450a8-0692-4403-8447-ab09d6dd0b8f","_uuid":"e61d1e4613dd4f51970d504e93ae30c072ca9d98"},"source":"Approximately 77% of Cabin feature is missing in the training data and 78% missing on the test data. \nWe have two choices, \n* we can either get rid of the whole feature, or \n* we can brainstorm a little and find an appropriate way to put them in use. For example, We may say passengers with cabin record had a higher socio-economic-status then others. We may also say passengers with cabin record were more likely to be taken into consideration when loading into the boat.\n\nLet's combine train and test data first and for now, will assign all the null values as **\"N\"**"},{"cell_type":"code","execution_count":27,"metadata":{"_kg_hide-input":true,"_uuid":"8ff7b4f88285bc65d72063d7fdf8a09a5acb62d3","execution":{"iopub.execute_input":"2021-06-26T16:35:09.888377Z","iopub.status.busy":"2021-06-26T16:35:09.88784Z","iopub.status.idle":"2021-06-26T16:35:09.902296Z","shell.execute_reply":"2021-06-26T16:35:09.901697Z","shell.execute_reply.started":"2021-06-26T16:35:09.888114Z"}},"outputs":[],"source":"## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dae4beab-8c5a-4192-a460-e9abc6f14d3e","_uuid":"e2d84eff7cafdd68a471876b65e0ae866151d6d2"},"source":"All the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name. "},{"cell_type":"code","execution_count":28,"metadata":{"_cell_guid":"87995359-8a77-4e38-b8bb-e9b4bdeb17ed","_kg_hide-input":true,"_uuid":"c1e9e06eb7f2a6eeb1a6d69f000217e7de7d5f25","execution":{"iopub.execute_input":"2021-06-26T16:35:09.904181Z","iopub.status.busy":"2021-06-26T16:35:09.903766Z","iopub.status.idle":"2021-06-26T16:35:09.909654Z","shell.execute_reply":"2021-06-26T16:35:09.908573Z","shell.execute_reply.started":"2021-06-26T16:35:09.904014Z"}},"outputs":[],"source":"all_data.Cabin = [i[0] for i in all_data.Cabin]"},{"cell_type":"markdown","metadata":{},"source":"Now let's look at the value counts of the cabin features and see how it looks. "},{"cell_type":"code","execution_count":29,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:09.91156Z","iopub.status.busy":"2021-06-26T16:35:09.911098Z","iopub.status.idle":"2021-06-26T16:35:09.928945Z","shell.execute_reply":"2021-06-26T16:35:09.928025Z","shell.execute_reply.started":"2021-06-26T16:35:09.911398Z"}},"outputs":[],"source":"percent_value_counts(all_data, \"Cabin\")"},{"cell_type":"markdown","metadata":{"_uuid":"5e8cff0316f95162cdc9c2f3da905ad49fc548ca"},"source":"So, We still haven't done any effective work to replace the null values. Let's stop for a second here and think through how we can take advantage of some of the other features here.  \n* We can use the average of the fare column We can use pythons ***groupby*** function to get the mean fare of each cabin letter. "},{"cell_type":"code","execution_count":30,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:09.930774Z","iopub.status.busy":"2021-06-26T16:35:09.930283Z","iopub.status.idle":"2021-06-26T16:35:09.942122Z","shell.execute_reply":"2021-06-26T16:35:09.941067Z","shell.execute_reply.started":"2021-06-26T16:35:09.930532Z"}},"outputs":[],"source":"all_data.groupby(\"Cabin\")['Fare'].mean().sort_values()"},{"cell_type":"markdown","metadata":{"_uuid":"8605664271220cb4a17fa1aca65207681503c9dd"},"source":"Now, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means. "},{"cell_type":"code","execution_count":31,"metadata":{"_kg_hide-input":true,"_uuid":"a466da29f1989fa983147faf9e63d18783468567","execution":{"iopub.execute_input":"2021-06-26T16:35:09.943855Z","iopub.status.busy":"2021-06-26T16:35:09.943364Z","iopub.status.idle":"2021-06-26T16:35:09.952677Z","shell.execute_reply":"2021-06-26T16:35:09.952057Z","shell.execute_reply.started":"2021-06-26T16:35:09.943627Z"}},"outputs":[],"source":"def cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    "},{"cell_type":"markdown","metadata":{"_uuid":"6f56c9950206a5a8f30c39ca207dc47859b8d8a0"},"source":"Let's apply <b>cabin_estimator</b> function in each unknown cabins(cabin with <b>null</b> values). Once that is done we will separate our train and test to continue towards machine learning modeling. "},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:09.95455Z","iopub.status.busy":"2021-06-26T16:35:09.954083Z","iopub.status.idle":"2021-06-26T16:35:09.96302Z","shell.execute_reply":"2021-06-26T16:35:09.962357Z","shell.execute_reply.started":"2021-06-26T16:35:09.95437Z"}},"outputs":[],"source":"with_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]"},{"cell_type":"code","execution_count":33,"metadata":{"_kg_hide-input":true,"_uuid":"1c646b64c6e062656e5f727d5499266f847c4832","execution":{"iopub.execute_input":"2021-06-26T16:35:09.965179Z","iopub.status.busy":"2021-06-26T16:35:09.96464Z","iopub.status.idle":"2021-06-26T16:35:09.981536Z","shell.execute_reply":"2021-06-26T16:35:09.980705Z","shell.execute_reply.started":"2021-06-26T16:35:09.964885Z"}},"outputs":[],"source":"##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers"},{"cell_type":"markdown","metadata":{"_cell_guid":"26d918c2-3c6b-48e8-8e2b-fc4531e8c59e","_uuid":"05a777057d9803235a17d79b72eefe7085ebf2e5"},"source":"### Fare Feature\n***\nIf you have paid attention so far, you know that there is only one missing value in the fare column. Let's have it. "},{"cell_type":"code","execution_count":34,"metadata":{"_cell_guid":"2c75f369-e781-43df-be06-32585b372a0a","_kg_hide-input":true,"_uuid":"020cafd121f2e6cbed89265c993ef3d76566cd6b","execution":{"iopub.execute_input":"2021-06-26T16:35:09.983259Z","iopub.status.busy":"2021-06-26T16:35:09.982793Z","iopub.status.idle":"2021-06-26T16:35:10.000785Z","shell.execute_reply":"2021-06-26T16:35:09.999778Z","shell.execute_reply.started":"2021-06-26T16:35:09.983086Z"}},"outputs":[],"source":"test[test.Fare.isnull()]"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ffece2f-9df0-44e5-80cc-84894a8d0d45","_uuid":"bce23c7620db2cde9bae8efa04b00c78819f0268"},"source":"Here, We can take the average of the **Fare** column to fill in the NaN value. However, for the sake of learning and practicing, we will try something else. We can take the average of the values where**Pclass** is ***3***, **Sex** is ***male*** and **Embarked** is ***S***"},{"cell_type":"code","execution_count":35,"metadata":{"_cell_guid":"e742aa76-b6f8-4882-8bd6-aa10b96f06aa","_kg_hide-input":true,"_uuid":"f1dc8c6c33ba7df075ee608467be2a83dc1764fd","execution":{"iopub.execute_input":"2021-06-26T16:35:10.002749Z","iopub.status.busy":"2021-06-26T16:35:10.002232Z","iopub.status.idle":"2021-06-26T16:35:10.012662Z","shell.execute_reply":"2021-06-26T16:35:10.011431Z","shell.execute_reply.started":"2021-06-26T16:35:10.00248Z"}},"outputs":[],"source":"missing_value = test[(test.Pclass == 3) & \n                     (test.Embarked == \"S\") & \n                     (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ff2fbe3-9858-4aad-9e33-e909d5128879","_uuid":"e04222497a5dfd77ac07dbcacbdc10dc1732da21"},"source":"### Age Feature\n***\nWe know that the feature \"Age\" is the one with most missing values, let's see it in terms of percentage. "},{"cell_type":"code","execution_count":36,"metadata":{"_cell_guid":"8ff25fb3-7a4a-4e06-b48f-a06b8d844917","_kg_hide-input":true,"_uuid":"c356e8e85f53a27e44b5f28936773a289592c5eb","execution":{"iopub.execute_input":"2021-06-26T16:35:10.014347Z","iopub.status.busy":"2021-06-26T16:35:10.014023Z","iopub.status.idle":"2021-06-26T16:35:10.024214Z","shell.execute_reply":"2021-06-26T16:35:10.023404Z","shell.execute_reply.started":"2021-06-26T16:35:10.014284Z"}},"outputs":[],"source":"print (\"Train age missing value: \" + str((train.Age.isnull().sum()/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((test.Age.isnull().sum()/len(test))*100)+str(\"%\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"105d0bf8-dada-4499-8a41-499caf20fa81","_uuid":"8678df259a8f4e7f85f92603f312e1df76a26589"},"source":"We will take a different approach since **~20% data in the Age column is missing** in both train and test dataset. The age variable seems to be promising for determining survival rate. Therefore, It would be unwise to replace the missing values with median, mean or mode. We will use machine learning model Random Forest Regressor to impute missing value instead of Null value. We will keep the age column unchanged for now and work on that in the feature engineering section."},{"cell_type":"markdown","metadata":{"_cell_guid":"81537f22-2c69-45f2-90d3-a2a8790cb2fd","_uuid":"84518982b94e7e811bf3560a3862f06a46f1b530"},"source":"# Part 3. Visualization and Feature Relations\n<a id=\"visualization_and_feature_relations\" ></a>\n***\nBefore we dive into finding relations between independent variables and our dependent variable(survivor), let us create some assumptions about how the relations may turn-out among features.\n\n**Assumptions:**\n- Gender: More female survived than male\n- Pclass: Higher socio-economic status passenger survived more than others. \n- Age: Younger passenger survived more than other passengers. \n- Fare: Passenger with higher fare survived more that other passengers. This can be quite correlated with Pclass. \n\n\nNow, let's see how the features are related to each other by creating some visualizations. "},{"cell_type":"markdown","metadata":{"_cell_guid":"63420775-00e1-4650-a2f3-2ae6eebab23c","_uuid":"ca8bfb1bfe4d1079635a54c8daec3399b8355749"},"source":"## 3a. Gender and Survived\n<a id=\"gender_and_survived\"></a>\n***"},{"cell_type":"code","execution_count":37,"metadata":{"_cell_guid":"78322e76-ccaa-4bb9-9cc2-7a3394ddfe8c","_kg_hide-input":true,"_uuid":"6008755b1522e2a849b6e1ccbb7da57270293ca4","execution":{"iopub.execute_input":"2021-06-26T16:35:10.026025Z","iopub.status.busy":"2021-06-26T16:35:10.025517Z","iopub.status.idle":"2021-06-26T16:35:10.265216Z","shell.execute_reply":"2021-06-26T16:35:10.26434Z","shell.execute_reply.started":"2021-06-26T16:35:10.025965Z"}},"outputs":[],"source":"import seaborn as sns\npal = {'male':\"green\", 'female':\"Pink\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", \n                 y = \"Survived\", \n                 data=train, \n                 palette = pal,\n                 linewidth=5,\n                 order = ['female','male'],\n                 capsize = .05,\n\n                )\n\nplt.title(\"Survived/Non-Survived Passenger Gender Distribution\", fontsize = 25,loc = 'center', pad = 40)\nplt.ylabel(\"% of passenger survived\", fontsize = 15, )\nplt.xlabel(\"Sex\",fontsize = 15);\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa7cb175-3c4d-4367-8b35-d3b43fb7d07d","_uuid":"ef171de53cb343da95d1ba82ebd961b1ff1756c3"},"source":"This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived."},{"cell_type":"code","execution_count":38,"metadata":{"_cell_guid":"6e303476-c1ed-49bb-8b9d-14659dd5739d","_kg_hide-input":true,"_uuid":"163515a4c926323f7288f385795ea7b1ea545d7a","execution":{"iopub.execute_input":"2021-06-26T16:35:10.267021Z","iopub.status.busy":"2021-06-26T16:35:10.266613Z","iopub.status.idle":"2021-06-26T16:35:10.438911Z","shell.execute_reply":"2021-06-26T16:35:10.437974Z","shell.execute_reply.started":"2021-06-26T16:35:10.266858Z"}},"outputs":[],"source":"pal = {1:\"seagreen\", 0:\"gray\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", \n                   hue=\"Survived\",\n                   data = train, \n                   linewidth=4, \n                   palette = pal\n)\n\n## Fixing title, xlabel and ylabel\nplt.title(\"Passenger Gender Distribution - Survived vs Not-survived\", fontsize = 25, pad=40)\nplt.xlabel(\"Sex\", fontsize = 15);\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\n\n## Fixing xticks\n#labels = ['Female', 'Male']\n#plt.xticks(sorted(train.Sex.unique()), labels)\n\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title(\"Survived\")\nlegs = leg.texts\nlegs[0].set_text(\"No\")\nlegs[1].set_text(\"Yes\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0835c20e-f155-4bd7-8032-895d8c8042e6","_uuid":"bf15a586513bdde73dfa2279b739ffca040e71e4"},"source":"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n**Summary**\n***\n- As we suspected, female passengers have survived at a much better rate than male passengers. \n- It seems about right since females and children were the priority. "},{"cell_type":"markdown","metadata":{"_cell_guid":"2daa3614-866c-48d7-a8cb-26ee8126a806","_uuid":"e746a4be3c0ed3d94a7a4366a5bff565c7bc9834"},"source":"## 3b. Pclass and Survived\n<a id=\"pcalss_and_survived\"></a>\n***"},{"cell_type":"code","execution_count":39,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:10.441162Z","iopub.status.busy":"2021-06-26T16:35:10.440668Z","iopub.status.idle":"2021-06-26T16:35:10.62703Z","shell.execute_reply":"2021-06-26T16:35:10.62605Z","shell.execute_reply.started":"2021-06-26T16:35:10.440907Z"}},"outputs":[],"source":"temp = train[['Pclass', 'Survived', 'PassengerId']].groupby(['Pclass', 'Survived']).count().reset_index()\ntemp_df = pd.pivot_table(temp, values = 'PassengerId', index = 'Pclass',columns = 'Survived')\nnames = ['No', 'Yes']\ntemp_df.columns = names\nr = [0,1,2]\ntotals = [i+j for i, j in zip(temp_df['No'], temp_df['Yes'])]\nNo_s = [i / j * 100 for i,j in zip(temp_df['No'], totals)]\nYes_s = [i / j * 100 for i,j in zip(temp_df['Yes'], totals)]\n## Plotting\nplt.subplots(figsize = (15,10))\nbarWidth = 0.60\nnames = ('Upper', 'Middle', 'Lower')\n# Create green Bars\nplt.bar(r, No_s, color='Red', edgecolor='white', width=barWidth)\n# Create orange Bars\nplt.bar(r, Yes_s, bottom=No_s, color='Green', edgecolor='white', width=barWidth)\n\n \n# Custom x axis\nplt.xticks(r, names)\nplt.xlabel(\"Pclass\")\nplt.ylabel('Percentage')\n \n# Show graphic\nplt.show()\n"},{"cell_type":"code","execution_count":40,"metadata":{"_cell_guid":"93a3a621-7be8-4f28-960d-939068944d3f","_kg_hide-input":true,"_uuid":"61543e636b742647f90ea778f30a178a84e50533","execution":{"iopub.execute_input":"2021-06-26T16:35:10.628812Z","iopub.status.busy":"2021-06-26T16:35:10.628387Z","iopub.status.idle":"2021-06-26T16:35:10.938152Z","shell.execute_reply":"2021-06-26T16:35:10.937374Z","shell.execute_reply.started":"2021-06-26T16:35:10.628643Z"}},"outputs":[],"source":"plt.subplots(figsize = (15,10))\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=train, \n            linewidth=6,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 3\n            \n\n           )\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25, pad=40)\nplt.xlabel(\"Socio-Economic class\", fontsize = 15);\nplt.ylabel(\"% of Passenger Survived\", fontsize = 15);\nnames = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, names);"},{"cell_type":"markdown","metadata":{"_cell_guid":"e2c5ce9f-9759-43b6-b286-ec771a5a64c1","_uuid":"6faf3d5f770c23febb20cdc81cc079ed37d59959"},"source":"- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy, while \n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived. \n\n"},{"cell_type":"code","execution_count":41,"metadata":{"_cell_guid":"f6eba487-9c63-4cd8-908a-393e2c277e45","_kg_hide-input":true,"_uuid":"10867e6cb57231ae599406d827ba5e3f13ccb088","execution":{"iopub.execute_input":"2021-06-26T16:35:10.939885Z","iopub.status.busy":"2021-06-26T16:35:10.939421Z","iopub.status.idle":"2021-06-26T16:35:11.16284Z","shell.execute_reply":"2021-06-26T16:35:11.161997Z","shell.execute_reply.started":"2021-06-26T16:35:10.939834Z"}},"outputs":[],"source":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived', \n              )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Passenger Class\", fontsize = 15,labelpad =20)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train.Pclass.unique()), labels);"},{"cell_type":"markdown","metadata":{"_cell_guid":"43ffcf43-2d0c-4033-8112-9edcca3576f1","_uuid":"f397633bae24a35d3fbe87d1ca54023356e065f9"},"source":"This KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \n\n**Summary**\n***\nThe first class passengers had the upper hand during the tragedy. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. "},{"cell_type":"markdown","metadata":{"_cell_guid":"1cb9d740-749b-4700-b9e9-973dbcad6aab","_uuid":"8eeb41d08ce680d51452deeb0ad054b184d67e16"},"source":"## 3c. Fare and Survived\n<a id=\"fare_and_survived\"></a>\n***"},{"cell_type":"code","execution_count":42,"metadata":{"_cell_guid":"cd6eb8a9-10a6-4ab8-aaec-4820df35f4c1","_kg_hide-input":true,"_uuid":"85737078f0e84fe972a5ddb81b29e114fcfb54be","execution":{"iopub.execute_input":"2021-06-26T16:35:11.164195Z","iopub.status.busy":"2021-06-26T16:35:11.163924Z","iopub.status.idle":"2021-06-26T16:35:11.392608Z","shell.execute_reply":"2021-06-26T16:35:11.391811Z","shell.execute_reply.started":"2021-06-26T16:35:11.164152Z"}},"outputs":[],"source":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Fare\", fontsize = 15, labelpad = 20);\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"6073f329-df80-4ab9-b99b-72e6fcdfe0c6","_uuid":"b5eba2b28ea428114d8ffab52feef95484bd76c0"},"source":"This plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check..."},{"cell_type":"code","execution_count":43,"metadata":{"_cell_guid":"bee8b01b-a56a-4762-bde0-4404a1c5ac1a","_kg_hide-input":true,"_uuid":"916ab9dc56a05105afa80127d69deb9fc0095ba2","execution":{"iopub.execute_input":"2021-06-26T16:35:11.394719Z","iopub.status.busy":"2021-06-26T16:35:11.394204Z","iopub.status.idle":"2021-06-26T16:35:11.41273Z","shell.execute_reply":"2021-06-26T16:35:11.411929Z","shell.execute_reply.started":"2021-06-26T16:35:11.394447Z"}},"outputs":[],"source":"train[train.Fare > 280]"},{"cell_type":"markdown","metadata":{"_cell_guid":"3467e2d8-315c-4223-9166-0aca54543cdd","_uuid":"443d93fcfbad82fc611ce88e12556a6325ccd15c"},"source":"As we assumed, it looks like an outlier with a fare of $512. We sure can delete this point. However, we will keep it for now. "},{"cell_type":"markdown","metadata":{"_cell_guid":"95c27d94-fa65-4bf9-a855-8e5dab17704e","_uuid":"64ff8df884805f04692dc601da1ef99527309d54"},"source":"## 3d. Age and Survived\n<a id=\"age_and_survived\"></a>\n***"},{"cell_type":"code","execution_count":44,"metadata":{"_cell_guid":"9eb6733b-7577-4360-8252-e6d97c78b7db","_kg_hide-input":true,"_uuid":"c6a4f46a7ce0e197f72abe293b69100c29a044ca","execution":{"iopub.execute_input":"2021-06-26T16:35:11.41461Z","iopub.status.busy":"2021-06-26T16:35:11.414164Z","iopub.status.idle":"2021-06-26T16:35:11.66643Z","shell.execute_reply":"2021-06-26T16:35:11.665545Z","shell.execute_reply.started":"2021-06-26T16:35:11.414413Z"}},"outputs":[],"source":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 25, pad = 40)\nplt.xlabel(\"Age\", fontsize = 15, labelpad = 20)\nplt.ylabel('Frequency', fontsize = 15, labelpad= 20);"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9aab64c-6170-4c8d-8446-cecdc9804b55","_uuid":"5238df80f5454d29e3793596a21fd0c00cb64a6c"},"source":"There is nothing out of the ordinary about this plot, except the very left part of the distribution. This may hint on the posibility that children and infants were the priority. "},{"cell_type":"markdown","metadata":{"_cell_guid":"077605b2-e9b4-4c45-8c5a-188508165f10","_uuid":"f8245da79c5394f7665d0b5429cb2fe4c4d0b057"},"source":"## 3e. Combined Feature Relations\n<a id='combined_feature_relations'></a>\n***\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. "},{"cell_type":"code","execution_count":45,"metadata":{"_cell_guid":"924e19c4-8d58-404c-9a84-02f096269351","_kg_hide-input":true,"_uuid":"71fc1c9843f789e19a5e8b2929579914d8ecdb3f","execution":{"iopub.execute_input":"2021-06-26T16:35:11.668148Z","iopub.status.busy":"2021-06-26T16:35:11.667828Z","iopub.status.idle":"2021-06-26T16:35:12.368731Z","shell.execute_reply":"2021-06-26T16:35:12.367992Z","shell.execute_reply.started":"2021-06-26T16:35:11.668097Z"}},"outputs":[],"source":"pal = {1:\"seagreen\", 0:\"gray\"}\ng = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Survived\", margin_titles=True, hue = \"Survived\",\n                  palette=pal)\ng = g.map(plt.hist, \"Age\", edgecolor = 'white');\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"089999b4-bc44-49c6-9f86-aaaccabaa224","_uuid":"6aac036e1b235e5b10bc6a153ed226acfce2cfcb"},"source":"Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky"},{"cell_type":"code","execution_count":46,"metadata":{"_cell_guid":"dcc34a91-261d-4929-a4eb-5072fcaf86ce","_kg_hide-input":true,"_uuid":"b2ad776bf0254be1ccf76f46a40db7960aa3db24","execution":{"iopub.execute_input":"2021-06-26T16:35:12.370419Z","iopub.status.busy":"2021-06-26T16:35:12.369999Z","iopub.status.idle":"2021-06-26T16:35:14.029152Z","shell.execute_reply":"2021-06-26T16:35:14.028323Z","shell.execute_reply.started":"2021-06-26T16:35:12.370369Z"}},"outputs":[],"source":"g = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Embarked\", margin_titles=True, hue = \"Survived\",\n                  palette = pal\n                  )\ng = g.map(plt.hist, \"Age\", edgecolor = 'white').add_legend();\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b9b9906c-805d-438b-b72e-a57cc60d5ae8","_uuid":"4070616f2637a720a3cb580264cfaed9235b9020"},"source":"This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. "},{"cell_type":"code","execution_count":47,"metadata":{"_cell_guid":"fd9fe9e2-f7d4-4f83-9ce4-0a22160ef4fe","_kg_hide-input":true,"_uuid":"f4d77506cabc7150466fa5bda64585d15814d48c","execution":{"iopub.execute_input":"2021-06-26T16:35:14.030913Z","iopub.status.busy":"2021-06-26T16:35:14.030621Z","iopub.status.idle":"2021-06-26T16:35:14.493248Z","shell.execute_reply":"2021-06-26T16:35:14.49225Z","shell.execute_reply.started":"2021-06-26T16:35:14.030867Z"}},"outputs":[],"source":"g = sns.FacetGrid(train, size=5,hue=\"Survived\", col =\"Sex\", margin_titles=True,\n                palette=pal,)\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\ng.fig.suptitle(\"Survived by Sex, Fare and Age\", size = 25)\nplt.subplots_adjust(top=0.85)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c309d4b-3e24-406b-bd28-d5055a660f16","_uuid":"90bbc6e6edbf6188170a4de1b38732d009f7afae"},"source":"This facet grid unveils a couple of interesting insights. Let's find out.\n* The grid above clearly demonstrates the three outliers with Fare of over \\$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \\$100. "},{"cell_type":"code","execution_count":48,"metadata":{"_cell_guid":"783403f6-9d3c-4a12-8505-cf321bd1a1ef","_kg_hide-input":true,"_uuid":"75c41c85dc76c9749e5c417e1ed0425eed9c55e0","execution":{"iopub.execute_input":"2021-06-26T16:35:14.495102Z","iopub.status.busy":"2021-06-26T16:35:14.494676Z","iopub.status.idle":"2021-06-26T16:35:14.888836Z","shell.execute_reply":"2021-06-26T16:35:14.887825Z","shell.execute_reply.started":"2021-06-26T16:35:14.494921Z"}},"outputs":[],"source":"## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n## factor plot\nsns.factorplot(x = \"Parch\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title(\"Factorplot of Parents/Children survived\", fontsize = 25)\nplt.subplots_adjust(top=0.85)"},{"cell_type":"markdown","metadata":{"_cell_guid":"33916321-237d-4381-990f-0faa11723c20","_uuid":"263113f38121c9e5f14247f05c262ee218be87f2"},"source":"**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**"},{"cell_type":"code","execution_count":49,"metadata":{"_cell_guid":"f6ed143e-3e02-4e97-a255-73807018f0d1","_kg_hide-input":true,"_uuid":"4ce5a4a6cff3966ac1811ee95f81c81fe4861a51","execution":{"iopub.execute_input":"2021-06-26T16:35:14.890716Z","iopub.status.busy":"2021-06-26T16:35:14.890276Z","iopub.status.idle":"2021-06-26T16:35:15.244771Z","shell.execute_reply":"2021-06-26T16:35:15.243687Z","shell.execute_reply.started":"2021-06-26T16:35:14.890522Z"}},"outputs":[],"source":"sns.factorplot(x =  \"SibSp\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title('Factorplot of Sibilings/Spouses survived', fontsize = 25)\nplt.subplots_adjust(top=0.85)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ee5b61b4-67d3-46b4-847d-4b5b85a8c791","_uuid":"c7a045b78e6b5f45ad891cf0515a6a4b2534d2ff"},"source":"**While, passenger who traveled in small groups with sibilings/spouses had better changes of survivint than other passengers.**"},{"cell_type":"code","execution_count":50,"metadata":{"_cell_guid":"50a0920d-556b-4439-a67f-384ce793d094","_kg_hide-input":true,"_uuid":"dfe723c71d4d29f599701d806ca97cd01a60142f","execution":{"iopub.execute_input":"2021-06-26T16:35:15.246815Z","iopub.status.busy":"2021-06-26T16:35:15.246286Z","iopub.status.idle":"2021-06-26T16:35:15.256239Z","shell.execute_reply":"2021-06-26T16:35:15.255215Z","shell.execute_reply.started":"2021-06-26T16:35:15.246539Z"}},"outputs":[],"source":"# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"003a7779-5966-45f8-a711-67e67234a654","_uuid":"402cd49464156ead61d5dd5698ffeb00eb71d0d3"},"source":"# Part 4: Statistical Overview\n<a id=\"statisticaloverview\"></a>\n***"},{"cell_type":"markdown","metadata":{"_cell_guid":"91eba73b-f744-478b-bd6b-13da6cff000b","_uuid":"3e8b752c8963a76a86c8b1db80783c644090bdfa"},"source":"![title](https://cdn-images-1.medium.com/max/400/1*hFJ-LI7IXcWpxSLtaC0dfg.png)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b7e6e77-50bf-469f-b92b-73056224bc61","_uuid":"797aa171f2e13ea965cb9a352fcfd2001e119747"},"source":"**Train info**"},{"cell_type":"code","execution_count":51,"metadata":{"_cell_guid":"ad856ee6-b1ec-445d-92b0-cd6a83d58301","_kg_hide-input":true,"_uuid":"35fc657641cc24aff89ade7d83d8b92e472dc3e6","execution":{"iopub.execute_input":"2021-06-26T16:35:15.258807Z","iopub.status.busy":"2021-06-26T16:35:15.258212Z","iopub.status.idle":"2021-06-26T16:35:15.297787Z","shell.execute_reply":"2021-06-26T16:35:15.297008Z","shell.execute_reply.started":"2021-06-26T16:35:15.258485Z"}},"outputs":[],"source":"train.describe()"},{"cell_type":"code","execution_count":52,"metadata":{"_cell_guid":"327c6775-9ba4-4d65-8c97-304cc9512e6a","_kg_hide-input":true,"_uuid":"2f9f5fb7bade4d82d7b5a564a8ac91123b4921d2","execution":{"iopub.execute_input":"2021-06-26T16:35:15.299834Z","iopub.status.busy":"2021-06-26T16:35:15.299339Z","iopub.status.idle":"2021-06-26T16:35:15.331049Z","shell.execute_reply":"2021-06-26T16:35:15.330129Z","shell.execute_reply.started":"2021-06-26T16:35:15.29957Z"}},"outputs":[],"source":"train.describe(include =['O'])"},{"cell_type":"code","execution_count":53,"metadata":{"_cell_guid":"5b817552-ecb8-4f6e-9950-6697d4c44d1f","_kg_hide-input":true,"_uuid":"c88dcae6209f02226f2e772b42616b5650d108f4","execution":{"iopub.execute_input":"2021-06-26T16:35:15.33335Z","iopub.status.busy":"2021-06-26T16:35:15.332779Z","iopub.status.idle":"2021-06-26T16:35:15.352959Z","shell.execute_reply":"2021-06-26T16:35:15.35204Z","shell.execute_reply.started":"2021-06-26T16:35:15.333034Z"}},"outputs":[],"source":"# Overview(Survived vs non survied)\nsurvived_summary = train.groupby(\"Survived\")\nsurvived_summary.mean().reset_index()"},{"cell_type":"code","execution_count":54,"metadata":{"_cell_guid":"502dd0d2-a51a-47da-904c-66c9840a1b74","_kg_hide-input":true,"_uuid":"65f9a660b942a8f92db94fe8fc41ccfa76a354cd","execution":{"iopub.execute_input":"2021-06-26T16:35:15.354847Z","iopub.status.busy":"2021-06-26T16:35:15.354359Z","iopub.status.idle":"2021-06-26T16:35:15.371166Z","shell.execute_reply":"2021-06-26T16:35:15.37032Z","shell.execute_reply.started":"2021-06-26T16:35:15.354612Z"}},"outputs":[],"source":"survived_summary = train.groupby(\"Sex\")\nsurvived_summary.mean().reset_index()"},{"cell_type":"code","execution_count":55,"metadata":{"_cell_guid":"68cb2dac-6295-44d6-8aa0-5cddb53dd72c","_kg_hide-input":true,"_uuid":"e49170e6e56329f68aba07a36389883ee1bee5ca","execution":{"iopub.execute_input":"2021-06-26T16:35:15.373155Z","iopub.status.busy":"2021-06-26T16:35:15.372618Z","iopub.status.idle":"2021-06-26T16:35:15.392055Z","shell.execute_reply":"2021-06-26T16:35:15.391506Z","shell.execute_reply.started":"2021-06-26T16:35:15.373092Z"}},"outputs":[],"source":"survived_summary = train.groupby(\"Pclass\")\nsurvived_summary.mean().reset_index()"},{"cell_type":"markdown","metadata":{"_cell_guid":"89ba2894-b129-4709-913d-f8cb35815925","_uuid":"e310c182f3541069329efcdd37373235fb144567"},"source":"I have gathered a small summary from the statistical overview above. Let's see what they are...\n- This train data set has 891 raw and 9 columns. \n- only 38% passenger survived during that tragedy.\n- ~74% female passenger survived, while only ~19% male passenger survived. \n- ~63% first class passengers survived, while only 24% lower class passenger survived.\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"5454218c-0a45-4a89-96fe-83d89b588183","_uuid":"d00b4e471e863f766c4aad7b88e1e6d9e57d6423"},"source":"## 4a. Correlation Matrix and Heatmap\n<a id=\"heatmap\"></a>\n***\n### Correlations"},{"cell_type":"code","execution_count":56,"metadata":{"_cell_guid":"d0acfa7a-6f3e-4783-925d-6e443a9a5baa","_kg_hide-input":true,"_uuid":"c4057023aa30d3ce1befae168c00f3cb8491804b","execution":{"iopub.execute_input":"2021-06-26T16:35:15.393816Z","iopub.status.busy":"2021-06-26T16:35:15.393405Z","iopub.status.idle":"2021-06-26T16:35:15.406432Z","shell.execute_reply":"2021-06-26T16:35:15.405713Z","shell.execute_reply.started":"2021-06-26T16:35:15.393654Z"}},"outputs":[],"source":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))"},{"cell_type":"markdown","metadata":{"_cell_guid":"92a69940-78f8-4139-a9a7-24ccf5f6afe7","_uuid":"211c5e2e817f4b10e64a28f5f8ce1d7eec9761fc"},"source":"** Sex is the most important correlated feature with *Survived(dependent variable)* feature followed by Pclass.** "},{"cell_type":"code","execution_count":57,"metadata":{"_cell_guid":"3e9fdd2e-f081-48ad-9c0f-afa475b15dfe","_kg_hide-input":true,"_uuid":"c3212c222341c250aacee47c43b1a023b9b65857","execution":{"iopub.execute_input":"2021-06-26T16:35:15.408424Z","iopub.status.busy":"2021-06-26T16:35:15.407893Z","iopub.status.idle":"2021-06-26T16:35:15.421826Z","shell.execute_reply":"2021-06-26T16:35:15.42092Z","shell.execute_reply.started":"2021-06-26T16:35:15.408231Z"}},"outputs":[],"source":"## get the most important variables. \ncorr = train.corr()**2\ncorr.Survived.sort_values(ascending=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f5f257ef-88b1-4302-ad41-d90892fbe4e9","_uuid":"1837acd3898d4787c9011e353dfc4dc15fd1abb2"},"source":"\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** "},{"cell_type":"code","execution_count":58,"metadata":{"_cell_guid":"eee23849-a390-4d16-a8df-d29c6f575413","_kg_hide-input":true,"_uuid":"285660c315b854497fe00847d051ceac5c9ec298","execution":{"iopub.execute_input":"2021-06-26T16:35:15.423924Z","iopub.status.busy":"2021-06-26T16:35:15.423431Z","iopub.status.idle":"2021-06-26T16:35:15.769867Z","shell.execute_reply":"2021-06-26T16:35:15.768898Z","shell.execute_reply.started":"2021-06-26T16:35:15.423681Z"}},"outputs":[],"source":"## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nimport numpy as np\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.set_style('whitegrid')\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n            linewidths=.9, \n            linecolor='white',\n            fmt='.2g',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e0b5ceb-fde5-40a7-b33b-b44e8f04189a","_uuid":"41e2bc2eff5699b14a0f47d5bd2e428ee5bec3b8"},"source":"#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. "},{"cell_type":"markdown","metadata":{"_cell_guid":"85faf680-5f78-414f-87b9-b72ef6d6ffc2","_uuid":"18c908fdbe16ae939827ec12a4ce028094a8a587"},"source":"## 4b. Statistical Test for Correlation\n<a id=\"statistical_test\"></a>\n***\n\nStatistical tests are the scientific way to prove the validation of theories. In any case, when we look at the data, we seem to have an intuitive understanding of where data is leading us. However, when we do statistical tests, we get a scientific or mathematical perspective of how significant these results are. Let's apply some of these methods and see how we are doing with our predictions.\n\n###  Hypothesis Testing Outline\n\nA hypothesis test compares the mean of a control group and experimental group and tries to find out whether the two sample means are different from each other and if they are different, how significant that difference is.\n \nA **hypothesis test** usually consists of multiple parts: \n\n1. Formulate a well-developed research problem or question: The hypothesis test usually starts with a concrete and well-developed researched problem. We need to ask the right question that can be answered using statistical analysis. \n2. **The null hypothesis($H_0$) and Alternating hypothesis($H_1$)**:\n> * The **null hypothesis($H_0$)** is something that is assumed to be true. It is the status quo. In a null hypothesis, the observations are the result of pure chance. When we set out to experiment, we form the null hypothesis by saying that there is no difference between the means of the control group and the experimental group.\n> *  An **Alternative hypothesis($H_A$)** is a claim and the opposite of the null hypothesis.  It is going against the status quo. In an alternative theory, the observations show a real effect combined with a component of chance variation.\n    \n3. Determine the **test statistic**: test statistic can be used to assess the truth of the null hypothesis. Depending on the standard deviation we either use t-statistics or z-statistics. In addition to that, we want to identify whether the test is a one-tailed test or two-tailed test. [This](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article explains it pretty well. [This](https://stattrek.com/hypothesis-test/hypothesis-testing.aspx) article is pretty good as well. \n\n4. Specify a **Significance level** and **Confidence Interval**: The significance level($\\alpha$) is the probability of rejecting a null hypothesis when it is true. In other words, we are ***comfortable/confident*** with rejecting the null hypothesis a significant amount of times even though it is true. This considerable amount is our Significant level. In addition to that, Significance level is one minus our Confidence interval. For example, if we say, our significance level is 5%, then our confidence interval would be (1 - 0.05) = 0.95 or 95%. \n\n5. Compute the **T-Statistics/Z-Statistics**: Computing the t-statistics follows a simple equation. This equation slightly differs depending on one sample test or two sample test  \n\n6. Compute the **P-value**: P-value is the probability that a test statistic at least as significant as the one observed would be obtained assuming that the null hypothesis is correct. The p-value is known to be unintuitive, and even many professors are known to explain it wrong. I think this [video](https://www.youtube.com/watch?v=E4KCfcVwzyw) explains the p-value well. **The smaller the P-value, the stronger the evidence against the null hypothesis.**\n\n7. **Describe the result and compare the p-value with the significance value($\\alpha$)**: If p<=$\\alpha$, then the observed effect is statistically significant, the null hypothesis is ruled out, and the alternative hypothesis is valid. However if the p> $\\alpha$, we say that, we fail to reject the null hypothesis. Even though this sentence is grammatically wrong, it is logically right. We never accept the null hypothesis just because we are doing the statistical test with sample data points.\n\nWe will follow each of these steps above to do your hypothesis testing below.\n\nP.S. Khan Academy has a set of videos that I think are intuative and helped me understand conceptually. \n\n***"},{"cell_type":"markdown","metadata":{"_uuid":"f3b49278bd1b8eff8fe1b14c1506d73cf53bd859"},"source":"### Hypothesis testing for Titanic\n#### Formulating a well developed researched question: \nRegarding this dataset, we can formulate the null hypothesis and alternative hypothesis by asking the following questions. \n> * **Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive?**. \n> * **Is there a substantial difference in the survival rate between the male and female passengers?**\n\n\n#### The Null Hypothesis and The Alternative Hypothesis:\nWe can formulate our hypothesis by asking questions differently. However, it is essential to understand what our end goal is. Here our dependent variable or target variable is **Survived**. Therefore, we say\n\n> ** Null Hypothesis($H_0$):** There is no difference in the survival rate between the male and female passengers. or the mean difference between male and female passenger in the survival rate is zero.  \n>  ** Alternative Hypothesis($H_A$):** There is a difference in the survival rate between the male and female passengers. or the mean difference in the survival rate between male and female is not zero.\n\n\nOnc thing we can do is try to set up the Null and Alternative Hypothesis in such way that, when we do our t-test, we can choose to do one tailed test. According to [this](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article, one-tailed tests are more powerful than two-tailed test. In addition to that, [this](https://www.youtube.com/watch?v=5NcMFlrnYp8&list=PLIeGtxpvyG-LrjxQ60pxZaimkaKKs0zGF) video is also quite helpful understanding these topics. with this in mind we can update/modify our null and alternative hypothesis. Let's see how we can rewrite this..\n\n> **Null Hypothesis(H0):** male mean is greater or equal to female mean.\n\n> **Alternative Hypothesis(H1):** male mean is less than female mean. \n\n#### Determine the test statistics:\n> This will be a two-tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. \n> Since we do not know the standard deviation($\\sigma$) and n is small, we will use the t-distribution. \n\n#### Specify the significance level:\n> Specifying a significance level is an important step of the hypothesis test. It is an ultimate balance between type 1 error and type 2 error. We will discuss more in-depth about those in another lesson. For now, we have decided to make our significance level($\\alpha$) = 0.05. So, our confidence interval or non-rejection region would be (1 - $\\alpha$)=(1-0.05) = 95%. \n\n#### Computing T-statistics and P-value:\nLet's take a random sample and see the difference."},{"cell_type":"code","execution_count":59,"metadata":{"_kg_hide-input":true,"_uuid":"abd034cffc591bf1ef2b4a8ed3e5a65eb133d61e","execution":{"iopub.execute_input":"2021-06-26T16:35:15.771771Z","iopub.status.busy":"2021-06-26T16:35:15.771345Z","iopub.status.idle":"2021-06-26T16:35:15.783362Z","shell.execute_reply":"2021-06-26T16:35:15.782301Z","shell.execute_reply.started":"2021-06-26T16:35:15.771603Z"}},"outputs":[],"source":"male_mean = train[train['Sex'] == 1].Survived.mean()\n\nfemale_mean = train[train['Sex'] == 0].Survived.mean()\nprint (\"Male survival mean: \" + str(male_mean))\nprint (\"female survival mean: \" + str(female_mean))\n\nprint (\"The mean difference between male and female survival rate: \" + str(female_mean - male_mean))"},{"cell_type":"markdown","metadata":{"_uuid":"0c1c27af262ba094ff1fd02867b1a41d5369720f"},"source":"Now, we have to understand that those two means are not  **the population mean ($\\bar{\\mu}$)**.  *The population mean is a statistical term statistician uses to indicate the actual average of the entire group. The group can be any gathering of multiple numbers such as animal, human, plants, money, stocks.* For example, To find the age population mean of Bulgaria; we will have to account for every single person's age and take their age. Which is almost impossible and if we were to go that route; there is no point of doing statistics in the first place. Therefore we approach this problem using sample sets. The idea of using sample set is that; if we take multiple samples of the same population and take the mean of them and put them in a distribution; eventually the distribution start to look more like a **normal distribution**. The more samples we take and the more sample means will be added and, the closer the normal distribution will reach towards population mean. This is where **Central limit theory** comes from. We will go more in depth of this topic later on. \n\nGoing back to our dataset, like we are saying these means above are part of the whole story. We were given part of the data to train our machine learning models, and the other part of the data was held back for testing. Therefore, It is impossible for us at this point to know the population means of survival for male and females. Situation like this calls for a statistical approach. We will use the sampling distribution approach to do the test. let's take 50 random sample of male and female from our train data."},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.785359Z","iopub.status.busy":"2021-06-26T16:35:15.784861Z","iopub.status.idle":"2021-06-26T16:35:15.815921Z","shell.execute_reply":"2021-06-26T16:35:15.815302Z","shell.execute_reply.started":"2021-06-26T16:35:15.785103Z"}},"outputs":[],"source":"# separating male and female dataframe. \nimport random\nmale = train[train['Sex'] == 1]\nfemale = train[train['Sex'] == 0]\n\n## empty list for storing mean sample\nm_mean_samples = []\nf_mean_samples = []\n\nfor i in range(50):\n    m_mean_samples.append(np.mean(random.sample(list(male['Survived']),50,)))\n    f_mean_samples.append(np.mean(random.sample(list(female['Survived']),50,)))\n    \n\n# Print them out\nprint (f\"Male mean sample mean: {round(np.mean(m_mean_samples),2)}\")\nprint (f\"Male mean sample mean: {round(np.mean(f_mean_samples),2)}\")\nprint (f\"Difference between male and female mean sample mean: {round(np.mean(f_mean_samples) - np.mean(m_mean_samples),2)}\")"},{"cell_type":"markdown","metadata":{},"source":"H0: male mean is greater or equal to female mean<br>\nH1: male mean is less than female mean. "},{"cell_type":"markdown","metadata":{"_uuid":"706d89356793f306d807c3fb277963e07181915c"},"source":"According to the samples our male samples ($\\bar{x}_m$) and female samples($\\bar{x}_f$) mean measured difference is ~ 0.55(statistically this is called the point estimate of the male population mean and female population mean). keeping in mind that...\n* We randomly select 50 people to be in the male group and 50 people to be in the female group. \n* We know our sample is selected from a broader population(trainning set). \n* We know we could have totally ended up with a different random sample of males and females.\n***\nWith all three points above in mind, how confident are we that, the measured difference is real or statistically significant? we can perform a **t-test** to evaluate that. When we perform a **t-test** we are usually trying to find out **an evidence of significant difference between population mean with hypothesized mean(1 sample t-test) or in our case difference between two population means(2 sample t-test).** \n\n\n\nThe **t-statistics** is the measure of a degree to which our groups differ standardized by the variance of our measurements. In order words, it is basically the measure of signal over noise. Let us describe the previous sentence a bit more for clarification. I am going to use [this post](http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen) as reference to describe the t-statistics here. \n\n\n#### Calculating the t-statistics\n# $$t = \\frac{\\bar{x}-\\mu}{\\frac{S} {\\sqrt{n}} }$$\n\nHere..\n* $\\bar{x}$ is the sample mean. \n* $\\mu$ is the hypothesized mean. \n* S is the standard deviation. \n* n is the sample size. \n\n\n1. Now, the denominator of this fraction $(\\bar{x}-\\mu)$ is basically the strength of the signal. where we calculate the difference between hypothesized mean and sample mean. If the mean difference is higher, then the signal is stronger. \n\nthe numerator of this fraction ** ${S}/ {\\sqrt{n}}$ ** calculates the amount of variation or noise of the data set. Here S is standard deviation, which tells us how much variation is there in the data. n is the sample size. \n\nSo, according to the explanation above, the t-value or t-statistics is basically measures the strength of the signal(the difference) to the amount of noise(the variation) in the data and that is how we calculate the t-value in one sample t-test. However, in order to calculate between two sample population mean or in our case we will use the follow equation. \n\n# $$t = \\frac{\\bar{x}_M - \\bar{x}_F}{\\sqrt {s^2 (\\frac{1}{n_M} + \\frac{1}{n_F})}}$$\n\nThis equation may seem too complex, however, the idea behind these two are similar. Both of them have the concept of signal/noise. The only difference is that we replace our hypothesis mean with another sample mean and the two sample sizes repalce one sample size. \n\nHere..\n* $\\bar{x}_M$ is the mean of our male group sample measurements. \n* $ \\bar{x}_F$ is the mean of female group samples. \n* $ n_M$ and $n_F$ are the sample number of observations in each group. \n* $ S^2$ is the sample variance.\n\nIt is good to have an understanding of what going on in the background. However, we will use **scipy.stats** to find the t-statistics. \n"},{"cell_type":"markdown","metadata":{"_uuid":"44e9000aefed8ea0125463486cc4a00c17e580e5"},"source":"#### Compare P-value with $\\alpha$\n> It looks like the p-value is very small compared to our significance level($\\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is \"**There is a significant difference in the survival rate between the male and female passengers.\"**"},{"cell_type":"markdown","metadata":{"_cell_guid":"df06b6c8-daf6-4f5b-af51-9c1dfbac7a68","_uuid":"34869ce4ce852633b1f4a5cd111b98841982cc19"},"source":"# Part 5: Feature Engineering\n<a id=\"feature_engineering\"></a>\n***\nFeature Engineering is exactly what its sounds like. Sometimes we want to create extra features from with in the features that we have, sometimes we want to remove features that are alike. Features engineering is the simple word for doing all those. It is important to remember that we will create new features in such ways that will not cause **multicollinearity(when there is a relationship among independent variables)** to occur. "},{"cell_type":"markdown","metadata":{"_cell_guid":"8c439069-6168-4cda-846f-db4c21265089","_uuid":"3ca0785fe824c6ea471b2bcf9600007ed238d450"},"source":"## name_length\n***Creating a new feature \"name_length\" that will take the count of letters of each name***"},{"cell_type":"code","execution_count":61,"metadata":{"_cell_guid":"d30d71c1-55bc-41c8-8536-9909d9f02538","_kg_hide-input":true,"_uuid":"cb17c6f59bb2123cbf2cbc9c282b4d70ee283a86","execution":{"iopub.execute_input":"2021-06-26T16:35:15.817993Z","iopub.status.busy":"2021-06-26T16:35:15.817477Z","iopub.status.idle":"2021-06-26T16:35:15.832377Z","shell.execute_reply":"2021-06-26T16:35:15.831471Z","shell.execute_reply.started":"2021-06-26T16:35:15.817745Z"}},"outputs":[],"source":"# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)"},{"cell_type":"markdown","metadata":{"_uuid":"012489c507bf8bfb1ca3db9b0506493cf5595e61"},"source":"## title\n**Getting the title of each name as a new feature. **"},{"cell_type":"code","execution_count":62,"metadata":{"_cell_guid":"ded64d5f-43de-4a9e-b9c5-ec4d2869387a","_kg_hide-input":true,"_uuid":"9c23229f7d06a1303a04b4a81c927453686ffec9","execution":{"iopub.execute_input":"2021-06-26T16:35:15.833953Z","iopub.status.busy":"2021-06-26T16:35:15.833501Z","iopub.status.idle":"2021-06-26T16:35:15.842414Z","shell.execute_reply":"2021-06-26T16:35:15.841468Z","shell.execute_reply.started":"2021-06-26T16:35:15.83376Z"}},"outputs":[],"source":"## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\n## Whenever we split like that, there is a good change that we will end up with while space around our string values. Let's check that. "},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.84422Z","iopub.status.busy":"2021-06-26T16:35:15.843818Z","iopub.status.idle":"2021-06-26T16:35:15.853522Z","shell.execute_reply":"2021-06-26T16:35:15.852642Z","shell.execute_reply.started":"2021-06-26T16:35:15.84407Z"}},"outputs":[],"source":"print(train.title.unique())"},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.855322Z","iopub.status.busy":"2021-06-26T16:35:15.854858Z","iopub.status.idle":"2021-06-26T16:35:15.86306Z","shell.execute_reply":"2021-06-26T16:35:15.86222Z","shell.execute_reply.started":"2021-06-26T16:35:15.855101Z"}},"outputs":[],"source":"## Let's fix that\ntrain.title = train.title.apply(lambda x: x.strip())"},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.864826Z","iopub.status.busy":"2021-06-26T16:35:15.864362Z","iopub.status.idle":"2021-06-26T16:35:15.872663Z","shell.execute_reply":"2021-06-26T16:35:15.871817Z","shell.execute_reply.started":"2021-06-26T16:35:15.864612Z"}},"outputs":[],"source":"## We can also combile all three lines above for test set here\ntest['title'] = [i.split('.')[0].split(',')[1].strip() for i in test.Name]\n\n## However it is important to be able to write readable code, and the line above is not so readable. "},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.874489Z","iopub.status.busy":"2021-06-26T16:35:15.873918Z","iopub.status.idle":"2021-06-26T16:35:15.896665Z","shell.execute_reply":"2021-06-26T16:35:15.895832Z","shell.execute_reply.started":"2021-06-26T16:35:15.874258Z"}},"outputs":[],"source":"## Let's replace some of the rare values with the keyword 'rare' and other word choice of our own. \n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n## Now in programming there is a term called DRY(Don't repeat yourself), whenever we are repeating  \n## same code over and over again, there should be a light-bulb turning on in our head and make us think\n## to code in a way that is not repeating or dull. Let's write a function to do exactly what we \n## did in the code above, only not repeating and more interesting. "},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.900031Z","iopub.status.busy":"2021-06-26T16:35:15.899771Z","iopub.status.idle":"2021-06-26T16:35:15.910036Z","shell.execute_reply":"2021-06-26T16:35:15.908929Z","shell.execute_reply.started":"2021-06-26T16:35:15.899989Z"}},"outputs":[],"source":"## we are writing a function that can help us modify title column\ndef name_converted(feature):\n    \"\"\"\n    This function helps modifying the title column\n    \"\"\"\n    \n    result = ''\n    if feature in ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col', 'Rev', 'Dona', 'Dr']:\n        result = 'rare'\n    elif feature in ['Ms', 'Mlle']:\n        result = 'Miss'\n    elif feature == 'Mme':\n        result = 'Mrs'\n    else:\n        result = feature\n    return result\n\ntest.title = test.title.map(name_converted)\ntrain.title = train.title.map(name_converted)"},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:15.912187Z","iopub.status.busy":"2021-06-26T16:35:15.911644Z","iopub.status.idle":"2021-06-26T16:35:15.923512Z","shell.execute_reply":"2021-06-26T16:35:15.922507Z","shell.execute_reply.started":"2021-06-26T16:35:15.912136Z"}},"outputs":[],"source":"print(train.title.unique())\nprint(test.title.unique())"},{"cell_type":"markdown","metadata":{"_cell_guid":"42ccf293-04c7-4bea-9570-4cce9227b8af","_uuid":"e870c4fc44de4b2395963e583c84d2cae83c004b"},"source":"## family_size\n***Creating a new feature called \"family_size\".*** "},{"cell_type":"code","execution_count":69,"metadata":{"_cell_guid":"7083a7e7-d1d5-4cc1-ad67-c454b139f5f1","_kg_hide-input":true,"_uuid":"cdfd54429cb235dd3b73535518950b2e515e54f2","execution":{"iopub.execute_input":"2021-06-26T16:35:15.925581Z","iopub.status.busy":"2021-06-26T16:35:15.925033Z","iopub.status.idle":"2021-06-26T16:35:15.933955Z","shell.execute_reply":"2021-06-26T16:35:15.933137Z","shell.execute_reply.started":"2021-06-26T16:35:15.925315Z"}},"outputs":[],"source":"## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1"},{"cell_type":"code","execution_count":70,"metadata":{"_cell_guid":"3d471d07-7735-4aab-8b26-3f26e481dc49","_kg_hide-input":true,"_uuid":"2e23467af7a2e85fcaa06b52b303daf2e5e44250","execution":{"iopub.execute_input":"2021-06-26T16:35:15.935971Z","iopub.status.busy":"2021-06-26T16:35:15.935422Z","iopub.status.idle":"2021-06-26T16:35:15.942647Z","shell.execute_reply":"2021-06-26T16:35:15.941882Z","shell.execute_reply.started":"2021-06-26T16:35:15.935671Z"}},"outputs":[],"source":"## bin the family size. \ndef family_group(size):\n    \"\"\"\n    This funciton groups(loner, small, large) family based on family size\n    \"\"\"\n    \n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a"},{"cell_type":"code","execution_count":71,"metadata":{"_cell_guid":"82f3cf5a-7e8d-42c3-a06b-56e17e890358","_kg_hide-input":true,"_uuid":"549239812f919f5348da08db4264632d2b21b587","execution":{"iopub.execute_input":"2021-06-26T16:35:15.944511Z","iopub.status.busy":"2021-06-26T16:35:15.94417Z","iopub.status.idle":"2021-06-26T16:35:15.95416Z","shell.execute_reply":"2021-06-26T16:35:15.953395Z","shell.execute_reply.started":"2021-06-26T16:35:15.944448Z"}},"outputs":[],"source":"## apply the family_group function in family_size\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d827a2d9-8ca5-454a-8323-90c397b25ccf","_uuid":"3aa4ad0fac364f8f3c04e240841ee097baa3c871"},"source":"## is_alone"},{"cell_type":"code","execution_count":72,"metadata":{"_cell_guid":"298b28d6-75a7-4e49-b1c3-7755f1727327","_kg_hide-input":true,"_uuid":"45315bb62f69e94e66109e7da06c6c5ade578398","execution":{"iopub.execute_input":"2021-06-26T16:35:15.956031Z","iopub.status.busy":"2021-06-26T16:35:15.955569Z","iopub.status.idle":"2021-06-26T16:35:15.964779Z","shell.execute_reply":"2021-06-26T16:35:15.963853Z","shell.execute_reply.started":"2021-06-26T16:35:15.955855Z"}},"outputs":[],"source":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]"},{"cell_type":"markdown","metadata":{"_cell_guid":"fee91907-4197-46c2-92c1-92474565e9a0","_uuid":"0a6032d2746a7cf75e2cc899615d72433572fd6d"},"source":"## ticket"},{"cell_type":"code","execution_count":73,"metadata":{"_cell_guid":"352c794d-728d-44de-9160-25da7abe0c06","_kg_hide-input":true,"_uuid":"5b99e1f7d7757f11e6dd6dbc627f3bd6e2fbd874","execution":{"iopub.execute_input":"2021-06-26T16:35:15.966936Z","iopub.status.busy":"2021-06-26T16:35:15.9664Z","iopub.status.idle":"2021-06-26T16:35:15.97799Z","shell.execute_reply":"2021-06-26T16:35:15.976969Z","shell.execute_reply.started":"2021-06-26T16:35:15.966816Z"}},"outputs":[],"source":"train.Ticket.value_counts().sample(10)"},{"cell_type":"markdown","metadata":{"_uuid":"dd50f2d503d4b951bee458793dde6e23f0e35dc9"},"source":"I have yet to figureout how to best manage ticket feature. So, any suggestion would be truly appreciated. For now, I will get rid off the ticket feature."},{"cell_type":"code","execution_count":74,"metadata":{"_kg_hide-input":true,"_uuid":"d23d451982f0cbe44976c2eacafb726d816e9195","execution":{"iopub.execute_input":"2021-06-26T16:35:15.979613Z","iopub.status.busy":"2021-06-26T16:35:15.979155Z","iopub.status.idle":"2021-06-26T16:35:15.989456Z","shell.execute_reply":"2021-06-26T16:35:15.988913Z","shell.execute_reply.started":"2021-06-26T16:35:15.97941Z"}},"outputs":[],"source":"train.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"60cb16dc-9bc3-4ff3-93b8-e3b3d4bcc0c8","_uuid":"800052abc32a56c5f5f875bb3652c02e93c6b0a8"},"source":"## calculated_fare"},{"cell_type":"code","execution_count":75,"metadata":{"_cell_guid":"adaa30fe-cb0f-4666-bf95-505f1dcce188","_kg_hide-input":true,"_uuid":"9374a6357551a7551e71731d72f5ceb3144856df","execution":{"iopub.execute_input":"2021-06-26T16:35:15.991841Z","iopub.status.busy":"2021-06-26T16:35:15.991313Z","iopub.status.idle":"2021-06-26T16:35:15.999545Z","shell.execute_reply":"2021-06-26T16:35:15.998734Z","shell.execute_reply.started":"2021-06-26T16:35:15.991562Z"}},"outputs":[],"source":"## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare/train.family_size\ntest['calculated_fare'] = test.Fare/test.family_size"},{"cell_type":"markdown","metadata":{"_uuid":"157cec80a8138c7976b135f093fc52832b82d71e"},"source":"Some people have travelled in groups like family or friends. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger, therefore calculated fare will be much handy in this situation. "},{"cell_type":"markdown","metadata":{"_cell_guid":"60579ed1-9978-4d4a-aea0-79c75b6b1376","_uuid":"c0e1c25bc6a7717646a5d0d063acae220e496e9e"},"source":"## fare_group"},{"cell_type":"code","execution_count":76,"metadata":{"_cell_guid":"8c33b78c-14cb-4cc2-af0f-65079a741570","_kg_hide-input":true,"_uuid":"35685a6ca28651eab389c4673c21da2ea5ba4187","execution":{"iopub.execute_input":"2021-06-26T16:35:16.001667Z","iopub.status.busy":"2021-06-26T16:35:16.001088Z","iopub.status.idle":"2021-06-26T16:35:16.012304Z","shell.execute_reply":"2021-06-26T16:35:16.011542Z","shell.execute_reply.started":"2021-06-26T16:35:16.00135Z"}},"outputs":[],"source":"def fare_group(fare):\n    \"\"\"\n    This function creates a fare group based on the fare provided\n    \"\"\"\n    \n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f5072cf-2234-425e-b91d-9609971117a0","_uuid":"907614ee16efce8cbcc32b5535648688d23271eb"},"source":"Fare group was calculated based on <i>calculated_fare</i>. This can further help our cause. "},{"cell_type":"markdown","metadata":{"_uuid":"57a333f5c225ce65ec46a7e8b3c33d78fd70752e"},"source":"## PassengerId"},{"cell_type":"markdown","metadata":{"_uuid":"b44cc5b5f6fd4d844b85f689f3a713599915bbce"},"source":"It seems like <i>PassengerId</i> column only works as an id in this dataset without any significant effect on the dataset. Let's drop it."},{"cell_type":"code","execution_count":77,"metadata":{"_uuid":"dadea67801cf5b56a882aa96bb874a4afa0e0bec","execution":{"iopub.execute_input":"2021-06-26T16:35:16.014434Z","iopub.status.busy":"2021-06-26T16:35:16.013951Z","iopub.status.idle":"2021-06-26T16:35:16.025524Z","shell.execute_reply":"2021-06-26T16:35:16.024631Z","shell.execute_reply.started":"2021-06-26T16:35:16.014266Z"}},"outputs":[],"source":"train.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a494c58-c1cf-44e9-be41-f404626ab299","_uuid":"704994b577f803ae51c5c6473a2d96f49bdd12ea"},"source":"## Creating dummy variables\n\nYou might be wondering what is a dummy variable? \n\nDummy variable is an important **prepocessing machine learning step**. Often times Categorical variables are an important features, which can be the difference between a good model and a great model. While working with a dataset, having meaningful value for example, \"male\" or \"female\" instead of 0's and 1's is more intuitive for us. However, machines do not understand the value of categorical values, for example, in this dataset we have gender male or female, algorithms do not accept categorical variables as input. In order to feed data in a machine learning model, we  "},{"cell_type":"code","execution_count":78,"metadata":{"_cell_guid":"9243ac8c-be44-46d0-a0ca-ee5f19b89bd4","_kg_hide-input":true,"_uuid":"7b8db3930fb1bfb91db16686223dfc6d8e77744d","execution":{"iopub.execute_input":"2021-06-26T16:35:16.027132Z","iopub.status.busy":"2021-06-26T16:35:16.026701Z","iopub.status.idle":"2021-06-26T16:35:16.059319Z","shell.execute_reply":"2021-06-26T16:35:16.058745Z","shell.execute_reply.started":"2021-06-26T16:35:16.027081Z"}},"outputs":[],"source":"\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"67dc98ce-bedc-456d-bdbb-9684bbd88d66","_uuid":"23586743d94d093f76f05a2fd3ca0ae75c0d663c"},"source":"## age"},{"cell_type":"markdown","metadata":{"_uuid":"a519858b2df34c499bb53808a5a23592ba7af040"},"source":"As I promised before, we are going to use Random forest regressor in this section to predict the missing age values. Let's do it"},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:16.061141Z","iopub.status.busy":"2021-06-26T16:35:16.060714Z","iopub.status.idle":"2021-06-26T16:35:16.084728Z","shell.execute_reply":"2021-06-26T16:35:16.083793Z","shell.execute_reply.started":"2021-06-26T16:35:16.060961Z"}},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":80,"metadata":{"_kg_hide-input":true,"_uuid":"9597c320c3db4db5e5c28980a28abaae7281bc61","execution":{"iopub.execute_input":"2021-06-26T16:35:16.086463Z","iopub.status.busy":"2021-06-26T16:35:16.086001Z","iopub.status.idle":"2021-06-26T16:35:16.096908Z","shell.execute_reply":"2021-06-26T16:35:16.095838Z","shell.execute_reply.started":"2021-06-26T16:35:16.086235Z"}},"outputs":[],"source":"## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)"},{"cell_type":"code","execution_count":81,"metadata":{"_kg_hide-input":true,"_uuid":"91662e7b63c2361fdcf3215f130b3895154ad92d","execution":{"iopub.execute_input":"2021-06-26T16:35:16.098683Z","iopub.status.busy":"2021-06-26T16:35:16.098263Z","iopub.status.idle":"2021-06-26T16:35:22.704889Z","shell.execute_reply":"2021-06-26T16:35:22.704165Z","shell.execute_reply.started":"2021-06-26T16:35:16.098504Z"}},"outputs":[],"source":"## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);"},{"cell_type":"markdown","metadata":{"_uuid":"8f4891f73fe40cdf20cbcdfce93bda7a4f5ccc5d"},"source":"Let's take a look at the histogram of the age column. "},{"cell_type":"code","execution_count":82,"metadata":{"_kg_hide-input":true,"_uuid":"8fc55e4670061d46dab3cc6585b3cc71eb996868","execution":{"iopub.execute_input":"2021-06-26T16:35:22.708567Z","iopub.status.busy":"2021-06-26T16:35:22.708283Z","iopub.status.idle":"2021-06-26T16:35:23.194075Z","shell.execute_reply":"2021-06-26T16:35:23.193419Z","shell.execute_reply.started":"2021-06-26T16:35:22.708515Z"}},"outputs":[],"source":"## Let's look at the his\nplt.subplots(figsize = (22,10),)\nsns.distplot(train.Age, bins = 100, kde = True, rug = False, norm_hist=False);"},{"cell_type":"markdown","metadata":{"_uuid":"97fcc2a4c7cdc7f998052aed543b86e113499580"},"source":"## age_group\nWe can create a new feature by grouping the \"Age\" column"},{"cell_type":"code","execution_count":83,"metadata":{"_cell_guid":"3140c968-6755-42ec-aa70-d30c0acede1e","_kg_hide-input":true,"_uuid":"c3bd77bb4d9d5411aa696a605be127db181d2a67","execution":{"iopub.execute_input":"2021-06-26T16:35:23.196215Z","iopub.status.busy":"2021-06-26T16:35:23.195696Z","iopub.status.idle":"2021-06-26T16:35:23.219708Z","shell.execute_reply":"2021-06-26T16:35:23.218664Z","shell.execute_reply.started":"2021-06-26T16:35:23.195943Z"}},"outputs":[],"source":"## create bins for age\ndef age_group_fun(age):\n    \"\"\"\n    This function creates a bin for age\n    \"\"\"\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);"},{"cell_type":"markdown","metadata":{},"source":"<div class=\"alert alert-danger\">\n<h1>Need to paraphrase this section</h1>\n<h2>Feature Selection</h2>\n<h3>Feature selection is an important part of machine learning models. There are many reasons why we use feature selection.</h3> \n<ul>\n    <li>Simple models are easier to interpret. People who acts according to model results have a better understanding of the model.</li>\n    <li>Shorter training times.</li>\n    <li>Enhanced generalisation by reducing overfitting. </li>\n    <li>Easier to implement by software developers> model production.</li>\n        <ul>\n            <li>As Data Scientists we need to remember no to creating models with too many variables since it might overwhelm production engineers.</li>\n    </ul>\n    <li>Reduced risk of data errors during model use</li>\n    <li>Data redundancy</li>\n</ul>\n</div>"},{"cell_type":"markdown","metadata":{"_cell_guid":"9de7bf55-edfb-42e0-a235-7fee883001d9","_uuid":"8eb07418adf26340ec68fa41401e68d08603f6d4"},"source":"# Part 6: Pre-Modeling Tasks\n## 6a. Separating dependent and independent variables\n<a id=\"dependent_independent\"></a>\n***\nBefore we apply any machine learning models, It is important to separate dependent and independent variables. Our dependent variable or target variable is something that we are trying to find, and our independent variable is the features we use to find the dependent variable. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. To specify them, we need to separate them from each other, and the code below does just that.\n\nP.S. In our test dataset, we do not have a dependent variable feature. We are to predict that using machine learning models. "},{"cell_type":"code","execution_count":84,"metadata":{"_cell_guid":"dcb0934f-8e3f-40b6-859e-abf70b0b074e","_kg_hide-input":true,"_uuid":"607db6be6dfacc7385e5adcc0feeee28c50c99c5","execution":{"iopub.execute_input":"2021-06-26T16:35:23.221875Z","iopub.status.busy":"2021-06-26T16:35:23.221297Z","iopub.status.idle":"2021-06-26T16:35:23.229845Z","shell.execute_reply":"2021-06-26T16:35:23.228853Z","shell.execute_reply.started":"2021-06-26T16:35:23.221578Z"}},"outputs":[],"source":"# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]"},{"cell_type":"markdown","metadata":{"_cell_guid":"042502ae-2714-43e2-9e33-6705b1aa781a","_uuid":"92001d23ce79265c0f7d2b3d6f67094feeec2ea7"},"source":"## 6b. Splitting the training data\n<a id=\"split_training_data\" ></a>\n***\nThere are multiple ways of splitting data. They are...\n* train_test_split.\n* cross_validation. \n\nWe have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two-thirds of the train data. Once we train our algorithm using 2/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, **X_train**, **X_test**, **y_train**, **y_test**.  \n* **X_train** and **y_train** first used to train the algorithm. \n* then, **X_test** is used in that trained algorithms to predict **outcomes. **\n* Once we get the **outcomes**, we compare it with **y_test**\n\nBy comparing the **outcome** of the model with **y_test**, we can determine whether our algorithms are performing well or not. As we compare we use confusion matrix to determine different aspects of model performance.\n\nP.S. When we use cross validation it is important to remember not to use **X_train, X_test, y_train and y_test**, rather we will use **X and y**. I will discuss more on that. "},{"cell_type":"code","execution_count":85,"metadata":{"_cell_guid":"348a5be2-5f4f-4c98-93a3-7352b6060ef4","_kg_hide-input":true,"_uuid":"41b70e57f8e03da9910c20af89a9fa4a2aaea85b","execution":{"iopub.execute_input":"2021-06-26T16:35:23.231964Z","iopub.status.busy":"2021-06-26T16:35:23.23135Z","iopub.status.idle":"2021-06-26T16:35:23.240022Z","shell.execute_reply":"2021-06-26T16:35:23.239414Z","shell.execute_reply.started":"2021-06-26T16:35:23.231633Z"}},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)"},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.242734Z","iopub.status.busy":"2021-06-26T16:35:23.24208Z","iopub.status.idle":"2021-06-26T16:35:23.250654Z","shell.execute_reply":"2021-06-26T16:35:23.249893Z","shell.execute_reply.started":"2021-06-26T16:35:23.242373Z"}},"outputs":[],"source":"len(X_train)"},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.260997Z","iopub.status.busy":"2021-06-26T16:35:23.260779Z","iopub.status.idle":"2021-06-26T16:35:23.265643Z","shell.execute_reply":"2021-06-26T16:35:23.264688Z","shell.execute_reply.started":"2021-06-26T16:35:23.260954Z"}},"outputs":[],"source":"len(X_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1f920690-2084-498c-a2fa-e618ad2228d8","_uuid":"75407683b262fb65fc4afdfca6084d4ddaebe9a9"},"source":"## 6c. Feature Scaling\n<a id=\"feature_scaling\" ></a>\n***\nFeature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit. For some machine learning models, it is not a problem. However, for many other ones, its quite a problem. Many machine learning algorithms uses euclidian distances to calculate the distance between two points, it is quite a problem. Let's again look at a the sample of the **train** dataset below."},{"cell_type":"code","execution_count":88,"metadata":{"_kg_hide-input":true,"_uuid":"d788baa4b88106afe5b30c769a6c85a1d67a5d6c","execution":{"iopub.execute_input":"2021-06-26T16:35:23.26761Z","iopub.status.busy":"2021-06-26T16:35:23.267136Z","iopub.status.idle":"2021-06-26T16:35:23.295264Z","shell.execute_reply":"2021-06-26T16:35:23.294322Z","shell.execute_reply.started":"2021-06-26T16:35:23.267383Z"}},"outputs":[],"source":"train.sample(5)"},{"cell_type":"markdown","metadata":{"_uuid":"3d213fdd45a46ea0cf060adc7d9af58a84a03e21"},"source":"Here **Age** and **Calculated_fare** is much higher in magnitude compared to others machine learning features. This can create problems as many machine learning models will get confused thinking **Age** and **Calculated_fare** have higher weight than other features. Therefore, we need to do feature scaling to get a better result. \nThere are multiple ways to do feature scaling. \n<ul>\n    <li><b>MinMaxScaler</b>-Scales the data using the max and min values so that it fits between 0 and 1.</li>\n    <li><b>StandardScaler</b>-Scales the data so that it has mean 0 and variance of 1.</li>\n    <li><b>RobustScaler</b>-Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.</b>\n </ul>\nI will discuss more on that in a different kernel. For now we will use <b>Standard Scaler</b> to feature scale our dataset. \n\nP.S. I am showing a sample of both before and after so that you can see how scaling changes the dataset. "},{"cell_type":"markdown","metadata":{"_uuid":"2bf3db75976f363c0e922b0b7843716f900e0fd9"},"source":"<h3><font color=\"$5831bc\" face=\"Comic Sans MS\">Before Scaling</font></h3>"},{"cell_type":"code","execution_count":89,"metadata":{"_kg_hide-input":true,"_uuid":"c4011a767b1d846f2866b4573d1d6d116afe8427","execution":{"iopub.execute_input":"2021-06-26T16:35:23.297022Z","iopub.status.busy":"2021-06-26T16:35:23.296548Z","iopub.status.idle":"2021-06-26T16:35:23.319251Z","shell.execute_reply":"2021-06-26T16:35:23.318338Z","shell.execute_reply.started":"2021-06-26T16:35:23.296792Z"}},"outputs":[],"source":"headers = X_train.columns \n\nX_train.head()"},{"cell_type":"code","execution_count":90,"metadata":{"_cell_guid":"5c89c54b-7f5a-4e31-9e8f-58726cef5eab","_kg_hide-input":true,"_uuid":"182b849ba7f2b311e919cdbf83970b97736e9d98","execution":{"iopub.execute_input":"2021-06-26T16:35:23.320979Z","iopub.status.busy":"2021-06-26T16:35:23.320476Z","iopub.status.idle":"2021-06-26T16:35:23.331478Z","shell.execute_reply":"2021-06-26T16:35:23.33067Z","shell.execute_reply.started":"2021-06-26T16:35:23.320738Z"}},"outputs":[],"source":"# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nst_scale = StandardScaler()\n\n## transforming \"train_x\"\nX_train = st_scale.fit_transform(X_train)\n## transforming \"test_x\"\nX_test = st_scale.transform(X_test)\n\n## transforming \"The testset\"\n#test = st_scale.transform(test)"},{"cell_type":"markdown","metadata":{"_uuid":"d425ca579370db88e39cdd1811ba3df2b257b36c"},"source":"<h3><font color=\"#5831bc\" face=\"Comic Sans MS\">After Scaling</font></h3>"},{"cell_type":"code","execution_count":91,"metadata":{"_kg_hide-input":true,"_uuid":"fc6f031833ac9e2734aa7b3a2373b667679c6b2f","execution":{"iopub.execute_input":"2021-06-26T16:35:23.333531Z","iopub.status.busy":"2021-06-26T16:35:23.333111Z","iopub.status.idle":"2021-06-26T16:35:23.359161Z","shell.execute_reply":"2021-06-26T16:35:23.358554Z","shell.execute_reply.started":"2021-06-26T16:35:23.333347Z"}},"outputs":[],"source":"pd.DataFrame(X_train, columns=headers).head()"},{"cell_type":"markdown","metadata":{},"source":"You can see how the features have transformed above."},{"cell_type":"markdown","metadata":{"_cell_guid":"0e03e40b-789a-40a0-a095-135f3d1c8f23","_uuid":"99e108b83ba88738e42480b053371d60d89151cf"},"source":"# Part 7: Modeling the Data\n<a id=\"modelingthedata\"></a>\n***\nIn the previous versions of this kernel, I thought about explaining each model before applying it. However, this process makes this kernel too lengthy to sit and read at one go. Therefore I have decided to break this kernel down and explain each algorithm in a different kernel and add the links here. If you like to review logistic regression, please click [here](https://www.kaggle.com/masumrumi/logistic-regression-with-titanic-dataset). "},{"cell_type":"code","execution_count":92,"metadata":{"_cell_guid":"0c8b0c41-6738-4689-85b0-b83a16e46ab9","_uuid":"09140be1a71e37b441a16951a82747462b767e6e","execution":{"iopub.execute_input":"2021-06-26T16:35:23.361067Z","iopub.status.busy":"2021-06-26T16:35:23.360637Z","iopub.status.idle":"2021-06-26T16:35:23.383762Z","shell.execute_reply":"2021-06-26T16:35:23.383049Z","shell.execute_reply.started":"2021-06-26T16:35:23.360889Z"}},"outputs":[],"source":"# import LogisticRegression model in python. \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression(solver='liblinear',\n                            penalty= 'l1',random_state = 42\n                                \n                            )\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(X_train,y_train)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"X_test\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(X_test)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. "},{"cell_type":"markdown","metadata":{},"source":"<h1><font color=\"#5831bc\" face=\"Comic Sans MS\">Evaluating a classification model</font></h1>\n\nThere are multiple ways to evaluate a classification model. \n\n* Confusion Matrix. \n* ROC Curve\n* AUC Curve. \n\n\n## Confusion Matrix\n<b>Confusion matrix</b>, a table that <b>describes the performance of a classification model</b>. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary/multiple outcome classes by comparing actual and predicted cases. For example, in terms of this dataset, our model is a binary one and we are trying to classify whether the passenger survived or not survived. we have fit the model using **X_train** and **y_train** and predicted the outcome of **X_test** in the variable **y_pred**. So, now we will use a confusion matrix to compare between **y_test** and **y_pred**. Let's do the confusion matrix. \n"},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.385843Z","iopub.status.busy":"2021-06-26T16:35:23.385341Z","iopub.status.idle":"2021-06-26T16:35:23.399434Z","shell.execute_reply":"2021-06-26T16:35:23.398674Z","shell.execute_reply.started":"2021-06-26T16:35:23.385606Z"}},"outputs":[],"source":"from sklearn.metrics import classification_report, confusion_matrix\n# printing confision matrix\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )"},{"cell_type":"markdown","metadata":{},"source":"Our **y_test** has a total of 294 data points; part of the original train set that we splitted in order to evaluate our model. Each number here represents certain details about our model. If we were to think about this interms of column and raw, we could see that...\n\n* the first column is of data points that the machine predicted as not-survived.\n* the second column is of the statistics that the model predicted as survievd.\n* In terms of raws, the first raw indexed as \"Not-survived\" means that the value in that raw are actual statistics of not survived once. \n* and the \"Survived\" indexed raw are values that actually survived.\n\nNow you can see that the predicted not-survived and predicted survived sort of overlap with actual survived and actual not-survived. After all it is a matrix and we have some terminologies to call these statistics more specifically. Let's see what they are"},{"cell_type":"markdown","metadata":{},"source":"<ul style=\"list-style-type:square;\">\n    <li><b>True Positive(TP)</b>: values that the model predicted as yes(survived) and is actually yes(survived).</li>\n    <li><b>True Negative(TN)</b>: values that model predicted as no(not-survived) and is actually no(not-survived)</li>\n    <li><b>False Positive(or Type I error)</b>: values that model predicted as yes(survived) but actually no(not-survived)</li>\n    <li><b>False Negative(or Type II error)</b>: values that model predicted as no(not-survived) but actually yes(survived)</li>\n</ul>\n\nFor this dataset, whenever the model is predicting something as yes, it means the model is predicting that the passenger survived and for cases when the model predicting no; it means the passenger did not survive. Let's determine the value of all these terminologies above.\n<ul style=\"list-style-type:square;\">\n    <li><b>True Positive(TP):87</b></li>\n    <li><b>True Negative(TN):149</b></li>\n    <li><b>False Positive(FP):28</b></li>\n    <li><b>False Negative(FN):30</b></li>\n</ul>\nFrom these four terminologies, we can compute many other rates that are used to evaluate a binary classifier. \n\n\n#### Accuracy: \n** Accuracy is the measure of how often the model is correct.** \n* (TP + TN)/total = (87+149)/294 = .8027\n\nWe can also calculate accuracy score using scikit learn. "},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.400652Z","iopub.status.busy":"2021-06-26T16:35:23.400403Z","iopub.status.idle":"2021-06-26T16:35:23.408635Z","shell.execute_reply":"2021-06-26T16:35:23.40776Z","shell.execute_reply.started":"2021-06-26T16:35:23.400604Z"}},"outputs":[],"source":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)"},{"cell_type":"markdown","metadata":{},"source":"**Misclassification Rate:** Misclassification Rate is the measure of how often the model is wrong**\n* Misclassification Rate and Accuracy are opposite of each other.\n* Missclassification is equivalent to 1 minus Accuracy. \n* Misclassification Rate is also known as \"Error Rate\".\n\n> (FP + FN)/Total = (28+30)/294 = 0.19\n\n**True Positive Rate/Recall/Sensitivity:** How often the model predicts yes(survived) when it's actually yes(survived)?\n> TP/(TP+FN) = 87/(87+30) = 0.7435897435897436\n"},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.410491Z","iopub.status.busy":"2021-06-26T16:35:23.410085Z","iopub.status.idle":"2021-06-26T16:35:23.418315Z","shell.execute_reply":"2021-06-26T16:35:23.417549Z","shell.execute_reply.started":"2021-06-26T16:35:23.410444Z"}},"outputs":[],"source":"from sklearn.metrics import recall_score\nrecall_score(y_test, y_pred)"},{"cell_type":"markdown","metadata":{},"source":"\n**False Positive Rate:** How often the model predicts yes(survived) when it's actually no(not-survived)?\n> FP/(FP+TN) = 28/(28+149) = 0.15819209039548024\n\n**True Negative Rate/Specificity:** How often the model predicts no(not-survived) when it's actually no(not-survived)?\n* True Negative Rate is equivalent to 1 minus False Positive Rate.\n\n> TN/(TN+FP) = 149/(149+28) = 0.8418079096045198\n\n**Precision:** How often is it correct when the model predicts yes. \n> TP/(TP+FP) = 87/(87+28) = 0.7565217391304347"},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.4204Z","iopub.status.busy":"2021-06-26T16:35:23.419791Z","iopub.status.idle":"2021-06-26T16:35:23.429679Z","shell.execute_reply":"2021-06-26T16:35:23.42864Z","shell.execute_reply.started":"2021-06-26T16:35:23.420242Z"}},"outputs":[],"source":"from sklearn.metrics import precision_score\nprecision_score(y_test, y_pred)"},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:23.431682Z","iopub.status.busy":"2021-06-26T16:35:23.431234Z","iopub.status.idle":"2021-06-26T16:35:23.44225Z","shell.execute_reply":"2021-06-26T16:35:23.441202Z","shell.execute_reply.started":"2021-06-26T16:35:23.43147Z"}},"outputs":[],"source":"from sklearn.metrics import classification_report, balanced_accuracy_score\nprint(classification_report(y_test, y_pred))"},{"cell_type":"markdown","metadata":{},"source":"we have our confusion matrix. How about we give it a little more character. "},{"cell_type":"code","execution_count":98,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:35:23.444153Z","iopub.status.busy":"2021-06-26T16:35:23.443714Z","iopub.status.idle":"2021-06-26T16:35:23.873374Z","shell.execute_reply":"2021-06-26T16:35:23.869521Z","shell.execute_reply.started":"2021-06-26T16:35:23.444104Z"}},"outputs":[],"source":"from sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\nclass_names = np.array(['not_survived','survived'])\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"e13731cbb9d9040cf6e4088e8660eca66037a8cc"},"source":"<h1>AUC & ROC Curve</h1>"},{"cell_type":"code","execution_count":99,"metadata":{"_uuid":"1e71bc7c685b757b6920076527780674d6f619bc","execution":{"iopub.execute_input":"2021-06-26T16:35:23.877891Z","iopub.status.busy":"2021-06-26T16:35:23.875713Z","iopub.status.idle":"2021-06-26T16:35:24.505751Z","shell.execute_reply":"2021-06-26T16:35:24.501314Z","shell.execute_reply.started":"2021-06-26T16:35:23.87783Z"}},"outputs":[],"source":"from sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()"},{"cell_type":"code","execution_count":100,"metadata":{"_uuid":"22f15e384372a1ece2f28cd9eced0c703a79598f","execution":{"iopub.execute_input":"2021-06-26T16:35:24.50731Z","iopub.status.busy":"2021-06-26T16:35:24.506981Z","iopub.status.idle":"2021-06-26T16:35:24.8481Z","shell.execute_reply":"2021-06-26T16:35:24.846974Z","shell.execute_reply.started":"2021-06-26T16:35:24.507251Z"}},"outputs":[],"source":"from sklearn.metrics import precision_recall_curve\n\ny_score = logreg.decision_function(X_test)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"e46b6d4bcb0ef70c06535b58bbe84c8a301ead91"},"source":"## Using Cross-validation:\nPros: \n* Helps reduce variance. \n* Expends models predictability. \n"},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:35:24.855506Z","iopub.status.busy":"2021-06-26T16:35:24.853028Z","iopub.status.idle":"2021-06-26T16:35:24.862513Z","shell.execute_reply":"2021-06-26T16:35:24.861421Z","shell.execute_reply.started":"2021-06-26T16:35:24.853368Z"}},"outputs":[],"source":"sc = st_scale"},{"cell_type":"code","execution_count":102,"metadata":{"_uuid":"17791284c3e88236de2daa112422cde8ddcb0641","execution":{"iopub.execute_input":"2021-06-26T16:35:24.868704Z","iopub.status.busy":"2021-06-26T16:35:24.86826Z","iopub.status.idle":"2021-06-26T16:35:25.014634Z","shell.execute_reply":"2021-06-26T16:35:25.013771Z","shell.execute_reply.started":"2021-06-26T16:35:24.86853Z"},"scrolled":true},"outputs":[],"source":"## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)\naccuracies = cross_val_score(LogisticRegression(solver='liblinear'), X,y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1f2930c-43ae-4c15-87f7-ccc9214ee0e1","_uuid":"b8020ecfe44bebdf7a2b95ec49393e8baac6bcf9"},"source":"## Grid Search on Logistic Regression\n* What is grid search? \n* What are the pros and cons?\n\n**Gridsearch** is a simple concept but effective technique in Machine Learning. The word **GridSearch** stands for the fact that we are searching for optimal parameter/parameters over a \"grid.\" These optimal parameters are also known as **Hyperparameters**. **The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model.**. For example, when we choose to use linear regression, we may decide to add a penalty to the loss function such as Ridge or Lasso. These penalties require specific alpha (the strength of the regularization technique) to set beforehand. The higher the value of alpha, the more penalty is being added. GridSearch finds the optimal value of alpha among a range of values provided by us, and then we go on and use that optimal value to fit the model and get sweet results. It is essential to understand those model parameters are different from models outcomes, for example, **coefficients** or model evaluation metrics such as **accuracy score** or **mean squared error** are model outcomes and different than hyperparameters.\n\n#### This part of the kernel is a working progress. Please check back again for future updates.####"},{"cell_type":"code","execution_count":103,"metadata":{"_cell_guid":"0620523c-b33b-4302-8a1c-4b6759ffa5fa","_uuid":"36a379a00a31dd161be1723f65490990294fe13d","execution":{"iopub.execute_input":"2021-06-26T16:35:25.021234Z","iopub.status.busy":"2021-06-26T16:35:25.018883Z","iopub.status.idle":"2021-06-26T16:35:40.193433Z","shell.execute_reply":"2021-06-26T16:35:40.192566Z","shell.execute_reply.started":"2021-06-26T16:35:25.021181Z"}},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\n## remember effective alpha scores are 0<alpha<infinity \nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n## Choose a cross validation strategy. \ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)"},{"cell_type":"code","execution_count":104,"metadata":{"_cell_guid":"1fa35072-87c4-4f47-86ab-dda03d4b7b15","_uuid":"4c6650e39550527b271ddf733dcfe5221bcd5c98","execution":{"iopub.execute_input":"2021-06-26T16:35:40.195216Z","iopub.status.busy":"2021-06-26T16:35:40.194925Z","iopub.status.idle":"2021-06-26T16:35:40.201259Z","shell.execute_reply":"2021-06-26T16:35:40.200225Z","shell.execute_reply.started":"2021-06-26T16:35:40.19517Z"}},"outputs":[],"source":"## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"dcd2ad782c168abb5cfb5a3d148814e53cb2119c"},"source":"\n#### Using the best parameters from the grid-search. "},{"cell_type":"code","execution_count":105,"metadata":{"_uuid":"ba53f6b3610821dc820936dde7b7803a54d20f5a","execution":{"iopub.execute_input":"2021-06-26T16:35:40.204086Z","iopub.status.busy":"2021-06-26T16:35:40.203576Z","iopub.status.idle":"2021-06-26T16:35:40.214041Z","shell.execute_reply":"2021-06-26T16:35:40.212929Z","shell.execute_reply.started":"2021-06-26T16:35:40.20393Z"}},"outputs":[],"source":"### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)"},{"cell_type":"markdown","metadata":{},"source":" #### This part of the kernel is a working progress. Please check back again for future updates.####\n \n Resources: \n * [Confusion Matrix](https://www.youtube.com/watch?v=8Oog7TXHvFY)\n### Under-fitting & Over-fitting: \nSo, we have our first model and its score. But, how do we make sure that our model is performing well. Our model may be overfitting or underfitting. In fact, for those of you don't know what overfitting and underfitting is, Let's find out.\n\n![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/fittings.jpg)\n\nAs you see in the chart above. **Underfitting** is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. On the other hand, **Overfitting** is when the model performs too well on the training data but does poorly in the validation set or test sets.  This situation is also known as having less bias but more variation and perform poorly as well. Ideally, we want to configure a model that performs well not only in the training data but also in the test data. This is where **bias-variance tradeoff** comes in. When we have a model that overfits, meaning less biased and more of variance, we introduce some bias in exchange of having much less variance. One particular tactic for this task is regularization models (Ridge, Lasso, Elastic Net).  These models are built to deal with the bias-variance tradeoff. This [kernel](https://www.kaggle.com/dansbecker/underfitting-and-overfitting) explains this topic well. Also, the following chart gives us a mental picture of where we want our models to be. \n![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n\nIdeally, we want to pick a sweet spot where the model performs well in training set, validation set, and test set. As the model gets complex, bias decreases, variance increases. However, the most critical part is the error rates. We want our models to be at the bottom of that **U** shape where the error rate is the least. That sweet spot is also known as **Optimum Model Complexity(OMC).**\n\nNow that we know what we want in terms of under-fitting and over-fitting, let's talk about how to combat them. \n\nHow to combat over-fitting?\n<ul>\n    <li>Simplify the model by using less parameters.</li>\n    <li>Simplify the model by changing the hyperparameters.</li>\n    <li>Introducing regularization models. </li>\n    <li>Use more training data. </li>\n    <li>Gatter more data ( and gather better quality data). </li>\n    </ul>\n #### This part of the kernel is a working progress. Please check back again for future updates.####"},{"cell_type":"markdown","metadata":{"_cell_guid":"8046e4d9-12db-4b1c-9e9e-31fd5e6543f2","_uuid":"26b0ea9184b2c37eabe4e705b1c840956ecc1e10"},"source":"## 7b. K-Nearest Neighbor classifier(KNN)\n<a id=\"knn\"></a>\n***"},{"cell_type":"code","execution_count":106,"metadata":{"_uuid":"953bc2c18b5fd93bcd51a42cc04a0539d86d5bac","execution":{"iopub.execute_input":"2021-06-26T16:35:40.216328Z","iopub.status.busy":"2021-06-26T16:35:40.215853Z","iopub.status.idle":"2021-06-26T16:35:40.416985Z","shell.execute_reply":"2021-06-26T16:35:40.416038Z","shell.execute_reply.started":"2021-06-26T16:35:40.216141Z"}},"outputs":[],"source":"## Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\n## calling on the model oject. \nknn = KNeighborsClassifier(metric='minkowski', p=2)\n## knn classifier works by doing euclidian distance \n\n\n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3)))"},{"cell_type":"markdown","metadata":{"_uuid":"6aa75e53129898ccd714370dc55c0ed2830e72f4"},"source":"#### Manually find the best possible k value for KNN"},{"cell_type":"code","execution_count":107,"metadata":{"_uuid":"9c0f44165e08f63ae5436180c5a7182e6db5c63f","execution":{"iopub.execute_input":"2021-06-26T16:35:40.418857Z","iopub.status.busy":"2021-06-26T16:35:40.418419Z","iopub.status.idle":"2021-06-26T16:35:46.541601Z","shell.execute_reply":"2021-06-26T16:35:46.540815Z","shell.execute_reply.started":"2021-06-26T16:35:40.418687Z"}},"outputs":[],"source":"## Search for an optimal value of k for KNN.\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))\n"},{"cell_type":"code","execution_count":108,"metadata":{"_uuid":"e123680b431ba99d399fa8205c32bcfdc7cabd81","execution":{"iopub.execute_input":"2021-06-26T16:35:46.543234Z","iopub.status.busy":"2021-06-26T16:35:46.542789Z","iopub.status.idle":"2021-06-26T16:35:46.685143Z","shell.execute_reply":"2021-06-26T16:35:46.684141Z","shell.execute_reply.started":"2021-06-26T16:35:46.543184Z"}},"outputs":[],"source":"from matplotlib import pyplot as plt\nplt.plot(k_range, k_scores)"},{"cell_type":"markdown","metadata":{"_uuid":"77b5b6e3b7bc925e0b008cd6d531175e5cc44040"},"source":"### Grid search on KNN classifier"},{"cell_type":"code","execution_count":109,"metadata":{"_uuid":"507e2a7cdb28a47be45ed247f1343c123a6b592b","execution":{"iopub.execute_input":"2021-06-26T16:35:46.687026Z","iopub.status.busy":"2021-06-26T16:35:46.686671Z","iopub.status.idle":"2021-06-26T16:35:55.465245Z","shell.execute_reply":"2021-06-26T16:35:55.464452Z","shell.execute_reply.started":"2021-06-26T16:35:46.686956Z"}},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n## Fitting the model. \ngrid.fit(X,y)"},{"cell_type":"code","execution_count":110,"metadata":{"_uuid":"c710770daa6cf327dcc28e18b3ed180fabecd49b","execution":{"iopub.execute_input":"2021-06-26T16:35:55.466929Z","iopub.status.busy":"2021-06-26T16:35:55.466654Z","iopub.status.idle":"2021-06-26T16:35:55.475348Z","shell.execute_reply":"2021-06-26T16:35:55.474575Z","shell.execute_reply.started":"2021-06-26T16:35:55.466883Z"}},"outputs":[],"source":"print(grid.best_score_)\nprint(grid.best_params_)\nprint(grid.best_estimator_)"},{"cell_type":"markdown","metadata":{"_uuid":"bb06144264d3127c92169aed7c29c2f66ad0ffc4"},"source":"#### Using best estimator from grid search using KNN. "},{"cell_type":"code","execution_count":111,"metadata":{"_uuid":"dd1fbf223c4ec9db65dde4924e2827e46029da1a","execution":{"iopub.execute_input":"2021-06-26T16:35:55.477181Z","iopub.status.busy":"2021-06-26T16:35:55.476629Z","iopub.status.idle":"2021-06-26T16:35:55.555736Z","shell.execute_reply":"2021-06-26T16:35:55.554788Z","shell.execute_reply.started":"2021-06-26T16:35:55.476983Z"}},"outputs":[],"source":"### Using the best parameters from the grid-search.\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)"},{"cell_type":"markdown","metadata":{"_uuid":"c2ebec8b83f23e3e27d23bdd707852269edd4d24"},"source":"#### Using RandomizedSearchCV\nRandomized search is a close cousin of grid search. It doesn't  always provide the best result but its fast. "},{"cell_type":"code","execution_count":112,"metadata":{"_uuid":"e159b267a57d7519fc0ee8b3d1e95b841d3daf60","execution":{"iopub.execute_input":"2021-06-26T16:35:55.557501Z","iopub.status.busy":"2021-06-26T16:35:55.557097Z","iopub.status.idle":"2021-06-26T16:36:02.332003Z","shell.execute_reply":"2021-06-26T16:36:02.331364Z","shell.execute_reply.started":"2021-06-26T16:35:55.557338Z"}},"outputs":[],"source":"from sklearn.model_selection import RandomizedSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \n## for RandomizedSearchCV, \ngrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n## Fitting the model. \ngrid.fit(X,y)"},{"cell_type":"code","execution_count":113,"metadata":{"_uuid":"c58492525dd18659ef9f9c774ee7601a55e96f36","execution":{"iopub.execute_input":"2021-06-26T16:36:02.333632Z","iopub.status.busy":"2021-06-26T16:36:02.333341Z","iopub.status.idle":"2021-06-26T16:36:02.340211Z","shell.execute_reply":"2021-06-26T16:36:02.338113Z","shell.execute_reply.started":"2021-06-26T16:36:02.333572Z"}},"outputs":[],"source":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)"},{"cell_type":"code","execution_count":114,"metadata":{"_uuid":"6fb31588585d50de773ba0db6c378363841a5313","execution":{"iopub.execute_input":"2021-06-26T16:36:02.343117Z","iopub.status.busy":"2021-06-26T16:36:02.34256Z","iopub.status.idle":"2021-06-26T16:36:02.420683Z","shell.execute_reply":"2021-06-26T16:36:02.419712Z","shell.execute_reply.started":"2021-06-26T16:36:02.342922Z"}},"outputs":[],"source":"### Using the best parameters from the grid-search.\nknn_ran_grid = grid.best_estimator_\nknn_ran_grid.score(X,y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"be0143d6-a7ea-4752-9520-c692f4c3eb8a","_uuid":"21e91edd53b6587d5a05036045bc5eea52f056da"},"source":"## Gaussian Naive Bayes\n<a id=\"gaussian_naive\"></a>\n***"},{"cell_type":"code","execution_count":115,"metadata":{"_uuid":"8b2435030dbef1303bfc2864d227f5918f359330","execution":{"iopub.execute_input":"2021-06-26T16:36:02.422487Z","iopub.status.busy":"2021-06-26T16:36:02.421997Z","iopub.status.idle":"2021-06-26T16:36:02.433216Z","shell.execute_reply":"2021-06-26T16:36:02.43234Z","shell.execute_reply.started":"2021-06-26T16:36:02.422237Z"}},"outputs":[],"source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c3e025c5-50f3-4fa1-a385-438d6665199b","_uuid":"2a1558118d9e673395246acc4f3c0edb1b1895f0"},"source":"## Support Vector Machines(SVM)\n<a id=\"svm\"></a>\n***"},{"cell_type":"code","execution_count":116,"metadata":{"_uuid":"56895672215b0b6365c6aaa10e446216ef635f53","execution":{"iopub.execute_input":"2021-06-26T16:36:02.435838Z","iopub.status.busy":"2021-06-26T16:36:02.435282Z","iopub.status.idle":"2021-06-26T16:37:25.882123Z","shell.execute_reply":"2021-06-26T16:37:25.881483Z","shell.execute_reply.started":"2021-06-26T16:36:02.435553Z"}},"outputs":[],"source":"from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)"},{"cell_type":"code","execution_count":117,"metadata":{"_uuid":"4108264ea5d18e3d3fa38a30584a032c734d6d49","execution":{"iopub.execute_input":"2021-06-26T16:37:25.8839Z","iopub.status.busy":"2021-06-26T16:37:25.883609Z","iopub.status.idle":"2021-06-26T16:37:25.890029Z","shell.execute_reply":"2021-06-26T16:37:25.889244Z","shell.execute_reply.started":"2021-06-26T16:37:25.883852Z"}},"outputs":[],"source":"print(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)"},{"cell_type":"code","execution_count":118,"metadata":{"_uuid":"db18a3b5475f03b21a039e31e4962c43f7caffdc","execution":{"iopub.execute_input":"2021-06-26T16:37:25.892123Z","iopub.status.busy":"2021-06-26T16:37:25.891542Z","iopub.status.idle":"2021-06-26T16:37:25.934216Z","shell.execute_reply":"2021-06-26T16:37:25.933352Z","shell.execute_reply.started":"2021-06-26T16:37:25.892073Z"}},"outputs":[],"source":"# using the best found hyper paremeters to get the score. \nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)"},{"cell_type":"markdown","metadata":{},"source":"## Decision Tree Classifier\n\nDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. If you would like to get a detailed understanding of Decision tree classifier, please take a look at [this](https://www.kaggle.com/masumrumi/decision-tree-with-titanic-dataset) kernel. "},{"cell_type":"code","execution_count":119,"metadata":{"_cell_guid":"38c90de9-d2e9-4341-a378-a854762d8be2","_uuid":"18efb62b713591d1512010536ff10d9f6a91ec11","execution":{"iopub.execute_input":"2021-06-26T16:37:25.936111Z","iopub.status.busy":"2021-06-26T16:37:25.935654Z","iopub.status.idle":"2021-06-26T16:37:57.983942Z","shell.execute_reply":"2021-06-26T16:37:57.983035Z","shell.execute_reply.started":"2021-06-26T16:37:25.935918Z"}},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) "},{"cell_type":"code","execution_count":120,"metadata":{"_cell_guid":"b2222e4e-f5f2-4601-b95f-506d7811610a","_uuid":"b0fb5055e6b4a7fb69ef44f669c4df693ce46212","execution":{"iopub.execute_input":"2021-06-26T16:37:57.988346Z","iopub.status.busy":"2021-06-26T16:37:57.988045Z","iopub.status.idle":"2021-06-26T16:37:57.994617Z","shell.execute_reply":"2021-06-26T16:37:57.993662Z","shell.execute_reply.started":"2021-06-26T16:37:57.988287Z"},"scrolled":true},"outputs":[],"source":"print( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)"},{"cell_type":"code","execution_count":121,"metadata":{"_cell_guid":"d731079a-31b4-429a-8445-48597bb2639d","_uuid":"76c26437d374442826ef140574c5c4880ae1e853","execution":{"iopub.execute_input":"2021-06-26T16:37:57.996876Z","iopub.status.busy":"2021-06-26T16:37:57.996238Z","iopub.status.idle":"2021-06-26T16:37:58.010892Z","shell.execute_reply":"2021-06-26T16:37:58.010194Z","shell.execute_reply.started":"2021-06-26T16:37:57.996695Z"}},"outputs":[],"source":"dectree_grid = grid.best_estimator_\n## using the best found hyper paremeters to get the score. \ndectree_grid.score(X,y)"},{"cell_type":"markdown","metadata":{},"source":" <h4> Let's look at the feature importance from decision tree grid.</h4>"},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:37:58.013756Z","iopub.status.busy":"2021-06-26T16:37:58.01221Z","iopub.status.idle":"2021-06-26T16:37:58.034194Z","shell.execute_reply":"2021-06-26T16:37:58.033436Z","shell.execute_reply.started":"2021-06-26T16:37:58.013683Z"}},"outputs":[],"source":"## feature importance\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)"},{"cell_type":"markdown","metadata":{},"source":"These are the top 10 features determined by **Decision Tree** helped classifing the fates of many passenger on Titanic on that night."},{"cell_type":"markdown","metadata":{},"source":"## 7f. Random Forest Classifier\n<a id=\"random_forest\"></a>"},{"cell_type":"markdown","metadata":{},"source":"I admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest(RF). RF is an ensemble method (combination of many decision trees) which is where the \"forest\" part comes in. One crucial details about Random Forest is that while using a forest of decision trees, RF model <b>takes random subsets of the original dataset(bootstrapped)</b> and <b>random subsets of the variables(features/columns)</b>. Using this method, the RF model creates 100's-1000's(the amount can be menually determined) of a wide variety of decision trees. This variety makes the RF model more effective and accurate. We then run each test data point through all of these 100's to 1000's of decision trees or the RF model and take a vote on the output. \n\n"},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:37:58.040453Z","iopub.status.busy":"2021-06-26T16:37:58.038063Z","iopub.status.idle":"2021-06-26T16:39:53.557817Z","shell.execute_reply":"2021-06-26T16:39:53.556973Z","shell.execute_reply.started":"2021-06-26T16:37:58.040398Z"}},"outputs":[],"source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) "},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:39:53.559492Z","iopub.status.busy":"2021-06-26T16:39:53.559192Z","iopub.status.idle":"2021-06-26T16:39:53.567897Z","shell.execute_reply":"2021-06-26T16:39:53.56675Z","shell.execute_reply.started":"2021-06-26T16:39:53.559434Z"}},"outputs":[],"source":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)"},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:39:53.570209Z","iopub.status.busy":"2021-06-26T16:39:53.56951Z","iopub.status.idle":"2021-06-26T16:39:53.600458Z","shell.execute_reply":"2021-06-26T16:39:53.599531Z","shell.execute_reply.started":"2021-06-26T16:39:53.569928Z"}},"outputs":[],"source":"rf_grid = grid.best_estimator_\nrf_grid.score(X,y)"},{"cell_type":"code","execution_count":126,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:39:53.602628Z","iopub.status.busy":"2021-06-26T16:39:53.602028Z","iopub.status.idle":"2021-06-26T16:39:53.613347Z","shell.execute_reply":"2021-06-26T16:39:53.612229Z","shell.execute_reply.started":"2021-06-26T16:39:53.602297Z"}},"outputs":[],"source":"from sklearn.metrics import classification_report\n# Print classification report for y_test\nprint(classification_report(y_test, y_pred, labels=rf_grid.classes_))"},{"cell_type":"markdown","metadata":{},"source":"## Feature Importance"},{"cell_type":"code","execution_count":127,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-06-26T16:39:53.615537Z","iopub.status.busy":"2021-06-26T16:39:53.614947Z","iopub.status.idle":"2021-06-26T16:39:53.637392Z","shell.execute_reply":"2021-06-26T16:39:53.63647Z","shell.execute_reply.started":"2021-06-26T16:39:53.615192Z"}},"outputs":[],"source":"## feature importance\nfeature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)"},{"cell_type":"markdown","metadata":{},"source":"<h3>Why Random Forest?(Pros and Cons)</h3>"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c4c43f6-42c4-4cd3-a038-3f0c37f3c767","_uuid":"aba2679da04529faf9f9175ab20a66ee71217f92"},"source":"***\n<h2>Introducing Ensemble Learning</h2>\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. \n\nThere are two types of ensemple learnings. \n\n**Bagging/Averaging Methods**\n> In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\n**Boosting Methods**\n> The other family of ensemble methods are boosting methods, where base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\n<h4 align=\"right\">Source:GA</h4>\n\nResource: <a href=\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\">Ensemble methods: bagging, boosting and stacking</a>\n***\n## 7g. Bagging Classifier\n<a id=\"bagging\"></a>\n***"},{"cell_type":"markdown","metadata":{},"source":"<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">Bagging Classifier</a>(Bootstrap Aggregating) is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Let's do a quick review:\n* Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. Here is an image to show how bootstrapped dataset works. \n<img src=\"https://uc-r.github.io/public/images/analytics/bootstrap/bootstrap.png\" width=\"600\">\n<h4 align=\"center\">Resampling from original dataset to bootstrapped datasets</h4>\n<h4 align=\"right\">Source: https://uc-r.github.io</h4>\n\n\n* After running a learning algorithm on each one of the bootstrapped datasets, all models are combined by taking their average. the test data/new data then go through this averaged classifier/combined classifier and predict the output. \n\nHere is an image to make it clear on how bagging works, \n<img src=\"https://prachimjoshi.files.wordpress.com/2015/07/screen_shot_2010-12-03_at_5-46-21_pm.png\" width=\"600\">\n<h4 align=\"right\">Source: https://prachimjoshi.files.wordpress.com</h4>\nPlease check out [this](https://www.kaggle.com/masumrumi/bagging-with-titanic-dataset) kernel if you want to find out more about bagging classifier. "},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:39:53.639198Z","iopub.status.busy":"2021-06-26T16:39:53.63871Z","iopub.status.idle":"2021-06-26T16:40:17.162923Z","shell.execute_reply":"2021-06-26T16:40:17.162277Z","shell.execute_reply.started":"2021-06-26T16:39:53.638945Z"}},"outputs":[],"source":"from sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) "},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:40:17.164621Z","iopub.status.busy":"2021-06-26T16:40:17.164322Z","iopub.status.idle":"2021-06-26T16:40:17.172911Z","shell.execute_reply":"2021-06-26T16:40:17.172302Z","shell.execute_reply.started":"2021-06-26T16:40:17.164559Z"}},"outputs":[],"source":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)"},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:40:17.174968Z","iopub.status.busy":"2021-06-26T16:40:17.174466Z","iopub.status.idle":"2021-06-26T16:40:17.226122Z","shell.execute_reply":"2021-06-26T16:40:17.225161Z","shell.execute_reply.started":"2021-06-26T16:40:17.174765Z"}},"outputs":[],"source":"bagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)"},{"cell_type":"markdown","metadata":{},"source":"<h3>Why use Bagging? (Pros and cons)</h3>\nBagging works best with strong and complex models(for example, fully developed decision trees). However, don't let that fool you to thinking that similar to a decision tree, bagging also overfits the model. Instead, bagging reduces overfitting since a lot of the sample training data are repeated and used to create base estimators. With a lot of equally likely training data, bagging is not very susceptible to overfitting with noisy data, therefore reduces variance. However, the downside is that this leads to an increase in bias."},{"cell_type":"markdown","metadata":{},"source":"<h4>Random Forest VS. Bagging Classifier</h4>\n\nIf some of you are like me, you may find Random Forest to be similar to Bagging Classifier. However, there is a fundamental difference between these two which is **Random Forests ability to pick subsets of features in each node.** I will elaborate on this in a future update."},{"cell_type":"markdown","metadata":{},"source":"## 7h. AdaBoost Classifier\n<a id=\"AdaBoost\"></a>\n***\nAdaBoost is another <b>ensemble model</b> and is quite different than Bagging. Let's point out the core concepts. \n> AdaBoost combines a lot of \"weak learners\"(they are also called stump; a tree with only one node and two leaves) to make classifications.\n\n> This base model fitting is an iterative process where each stump is chained one after the other; <b>It cannot run in parallel.</b>\n\n> <b>Some stumps get more say in the final classifications than others.</b> The models use weights that are assigned to each data point/raw indicating their \"importance.\" Samples with higher weight have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. \n\n> <b>Each stump is made by talking the previous stump's mistakes into account.</b> After each iteration weights gets re-calculated in order to take the errors/misclassifications from the last stump into consideration. \n\n> The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. \n\nTo illustrate what we have talked about so far let's look at the following visualization. \n\n<img src=\"https://cdn-images-1.medium.com/max/1600/0*paPv7vXuq4eBHZY7.png\">\n<h5 align=\"right\"> Source: Diogo(Medium)</h5>\n\n\n\n\nLet's dive into each one of the nitty-gritty stuff about AdaBoost:\n***\n> <b>First</b>, we determine the best feature to split the dataset using Gini index(basics from decision tree). The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain(the lower the Gini index is, the better unmixed the label is, therefore, better split).\n***\n> <b>Secondly</b>, we need to determine how much say a stump will have in the final classification and how we can calculate that.\n* We learn how much say a stump has in the final classification by calculating how well it classified the samples (aka calculate the total error of the weight).\n* The <b>Total Error</b> for a stump is the sum of the weights associated with the incorrectly classified samples. For example, lets say, we start a stump with 10 datasets. The first stump will uniformly distribute an weight amoung all the datapoints. Which means each data point will have 1/10 weight. Let's say once the weight is distributed we run the model and find 2 incorrect predicitons. In order to calculate the total erorr we add up all the misclassified weights. Here we get 1/10 + 1/10 = 2/10 or 1/5. This is our total error. We can also think about it\n\n\n$$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n\n\n* Since the weight is uniformly distributed(all add up to 1) among all data points, the total error will always be between 0(perfect stump) and 1(horrible stump).\n* We use the total error to determine the amount of say a stump has in the final classification using the following formula\n \n\n$$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n\n\nWhere $\\epsilon_t$ is the misclassification rate for the current classifier:\n\n\n$$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n\n\nHere...\n* $\\alpha_t$ = Amount of Say\n* $\\epsilon_t$ = Total error\n\n\n\nWe can draw a graph to determine the amount of say using the value of total error(0 to 1)\n\n<img src=\"http://chrisjmccormick.files.wordpress.com/2013/12/adaboost_alphacurve.png\">\n<h5 align=\"right\"> Source: Chris McCormick</h5>\n\n* The blue line tells us the amount of say for <b>Total Error(Error rate)</b> between 0 and 1. \n* When the stump does a reasonably good job, and the <b>total error</b> is minimal, then the <b>amount of say(Alpha)</b> is relatively large, and the alpha value is positive. \n* When the stump does an average job(similar to a coin flip/the ratio of getting correct and incorrect ~50%/50%), then the <b>total error</b> is ~0.5. In this case the <b>amount of say</b> is <b>0</b>.\n* When the error rate is high let's say close to 1, then the <b>amount of say</b> will be negative, which means if the stump outputs a value as \"survived\" the included weight will turn that value into \"not survived.\"\n\nP.S. If the <b>Total Error</b> is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. \n \n ***\n> <b>Third</b>, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. The pseducode for calculating the new sample weight is as follows. \n\n\n$$ New Sample Weight = Sample Weight + e^{\\alpha_t}$$\n\nHere the $\\alpha_t(AmountOfSay)$ can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples; hinting the next stump to put more emphasize on those. Inversely, we want to decrease the sample weight of the correctly classified samples; hinting the next stump to put less emphasize on those. \n\nThe following equation help us to do this calculation. \n\n$$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n\nHere, \n* $D_{t+1}(i)$ = New Sample Weight. \n* $D_t(i)$ = Current Sample weight.\n* $\\alpha_t$ = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and \n* $y_i h_t(x_i)$ = place holder for 1 if stump correctly classified, -1 if misclassified. \n\nFinally, we put together the combined classifier, which is \n\n$$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$ \n\nHere, \n\n$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n\n$T$ is the set of \"weak learners\"\n\n$\\alpha_t$ is the contribution weight for weak learner $t$\n\n$h_t(X)$ is the prediction of weak learner $t$\n\nand $y$ is binary **with values -1 and 1**\n\n\nP.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.\n\n<h3>Resources:</h3>\n<ul>\n    <li><a href=\"https://www.youtube.com/watch?v=LsK-xG1cLYA\">Statquest</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=-DUxtdeCiB4\">Principles of Machine Learning | AdaBoost(Video)</a></li>\n</ul>"},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:40:17.227822Z","iopub.status.busy":"2021-06-26T16:40:17.227396Z","iopub.status.idle":"2021-06-26T16:41:28.311627Z","shell.execute_reply":"2021-06-26T16:41:28.311009Z","shell.execute_reply.started":"2021-06-26T16:40:17.227656Z"}},"outputs":[],"source":"from sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) "},{"cell_type":"code","execution_count":132,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:41:28.313135Z","iopub.status.busy":"2021-06-26T16:41:28.31287Z","iopub.status.idle":"2021-06-26T16:41:28.318909Z","shell.execute_reply":"2021-06-26T16:41:28.318191Z","shell.execute_reply.started":"2021-06-26T16:41:28.313088Z"}},"outputs":[],"source":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)"},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2021-06-26T16:41:28.320845Z","iopub.status.busy":"2021-06-26T16:41:28.320267Z","iopub.status.idle":"2021-06-26T16:41:28.35912Z","shell.execute_reply":"2021-06-26T16:41:28.358535Z","shell.execute_reply.started":"2021-06-26T16:41:28.320797Z"}},"outputs":[],"source":"adaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)"},{"cell_type":"markdown","metadata":{},"source":"## Pros and cons of boosting\n\n---\n\n### Pros\n\n- Achieves higher performance than bagging when hyper-parameters tuned properly.\n- Can be used for classification and regression equally well.\n- Easily handles mixed data types.\n- Can use \"robust\" loss functions that make the model resistant to outliers.\n\n---\n\n### Cons\n\n- Difficult and time consuming to properly tune hyper-parameters.\n- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n- More risk of overfitting compared to bagging.\n\n<h3>Resources: </h3>\n<ul>\n    <li><a href=\"http://mccormickml.com/2013/12/13/adaboost-tutorial/\">AdaBoost Tutorial-Chris McCormick</a></li>\n    <li><a href=\"http://rob.schapire.net/papers/explaining-adaboost.pdf\">Explaining AdaBoost by Robert Schapire(One of the original author of AdaBoost)</a></li>\n</ul>"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ea60e91-544f-49fc-8128-ee190e8292e7","_uuid":"860921893a28a1fe9a4ce47f0779f1e7b154ca0a"},"source":"## 7i. Gradient Boosting Classifier\n<a id=\"gradient_boosting\"></a>\n***"},{"cell_type":"code","execution_count":134,"metadata":{"_cell_guid":"d32d6df9-b8e7-4637-bacc-2baec08547b8","_uuid":"fd788c4f4cde834a1329f325f1f59e3f77c37e42","execution":{"iopub.execute_input":"2021-06-26T16:41:28.360536Z","iopub.status.busy":"2021-06-26T16:41:28.360265Z","iopub.status.idle":"2021-06-26T16:41:28.521396Z","shell.execute_reply":"2021-06-26T16:41:28.520426Z","shell.execute_reply.started":"2021-06-26T16:41:28.360479Z"},"scrolled":true},"outputs":[],"source":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X, y)\ny_pred = gradient_boost.predict(X_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)"},{"cell_type":"markdown","metadata":{},"source":"<div class=\" alert alert-info\">\n<h3>Resources: </h3>\n<ul>\n    <li><a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\">Gradient Descent(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Main Ideas)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Calculation)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=jxuNLH5dXCs\">Gradient Boost(Classification Main Ideas)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=StWY5QWMXCw\">Gradient Boost(Classification Calculation)(StatQuest)</a></li>\n    <li><a href=\"https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>\n</ul>\n</div>"},{"cell_type":"markdown","metadata":{"_cell_guid":"996b8ee8-13ff-461d-8f7b-ac0d7d488cff","_uuid":"ee9c7a2ccdf93a90f929b6618105afbe699bd6de"},"source":"## 7j. XGBClassifier\n<a id=\"XGBClassifier\"></a>\n***"},{"cell_type":"code","execution_count":135,"metadata":{"_cell_guid":"5d94cc5b-d8b7-40d3-b264-138539daabfa","_uuid":"9d96154d2267ea26a6682a73bd1850026eb1303b","execution":{"iopub.execute_input":"2021-06-26T16:41:28.523177Z","iopub.status.busy":"2021-06-26T16:41:28.522724Z","iopub.status.idle":"2021-06-26T16:41:28.526955Z","shell.execute_reply":"2021-06-26T16:41:28.525945Z","shell.execute_reply.started":"2021-06-26T16:41:28.522964Z"}},"outputs":[],"source":"# from xgboost import XGBClassifier\n# XGBClassifier = XGBClassifier()\n# XGBClassifier.fit(X, y)\n# y_pred = XGBClassifier.predict(X_test)\n# XGBClassifier_accy = round(accuracy_score(y_pred, y_test), 3)\n# print(XGBClassifier_accy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a6b4c23c-b42b-4fad-b37d-c84154b3478d","_uuid":"3fa68b3d2e835b1a14088102561a2f8d4dac8f5c"},"source":"## 7k. Extra Trees Classifier\n<a id=\"extra_tree\"></a>\n***"},{"cell_type":"code","execution_count":136,"metadata":{"_cell_guid":"2e567e01-6b5f-4313-84af-cc378c3b709e","_uuid":"c9b958e2488adf6f79401c677087e3250d63ac9b","execution":{"iopub.execute_input":"2021-06-26T16:41:28.528841Z","iopub.status.busy":"2021-06-26T16:41:28.528382Z","iopub.status.idle":"2021-06-26T16:41:28.555697Z","shell.execute_reply":"2021-06-26T16:41:28.554889Z","shell.execute_reply.started":"2021-06-26T16:41:28.528664Z"}},"outputs":[],"source":"from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"20a66dcc-7f9f-4802-aa6d-58be75e07539","_uuid":"c55a54821feda82c75dde28bab7e2cf4445c4cf0"},"source":"## 7l. Gaussian Process Classifier\n<a id=\"GaussianProcessClassifier\"></a>\n***"},{"cell_type":"code","execution_count":137,"metadata":{"_cell_guid":"23bd5744-e04d-49bb-9d70-7c2a518f76dd","_uuid":"57fc008eea2ce1c0b595f888a82ddeaee6ce2177","execution":{"iopub.execute_input":"2021-06-26T16:41:28.557268Z","iopub.status.busy":"2021-06-26T16:41:28.556845Z","iopub.status.idle":"2021-06-26T16:41:28.863352Z","shell.execute_reply":"2021-06-26T16:41:28.862576Z","shell.execute_reply.started":"2021-06-26T16:41:28.557221Z"}},"outputs":[],"source":"from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gau_pro_accy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ec676e4d-0cbe-43fa-9ff8-92d76030faef","_uuid":"6f89f2cb63120a4594c7b0f2883b6872aa444700"},"source":"## 7m. Voting Classifier\n<a id=\"voting_classifer\"></a>\n***"},{"cell_type":"code","execution_count":138,"metadata":{"_cell_guid":"ac208dd3-1045-47bb-9512-de5ecb5c81b0","_uuid":"821c74bbf404193219eb91fe53755d669f5a14d1","execution":{"iopub.execute_input":"2021-06-26T16:41:28.865063Z","iopub.status.busy":"2021-06-26T16:41:28.86463Z","iopub.status.idle":"2021-06-26T16:41:30.314425Z","shell.execute_reply":"2021-06-26T16:41:30.313671Z","shell.execute_reply.started":"2021-06-26T16:41:28.865013Z"}},"outputs":[],"source":"from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_classifier', knn_grid),\n#     ('XGB_Classifier', XGBClassifier),\n    ('bagging_classifier', bagging_grid),\n    ('adaBoost_classifier',adaBoost_grid),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_classifier',gaussian),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)"},{"cell_type":"code","execution_count":139,"metadata":{"_cell_guid":"648ac6a6-2437-490a-bf76-1612a71126e8","_uuid":"518a02ae91cc91d618e476d1fc643cd3912ee5fb","execution":{"iopub.execute_input":"2021-06-26T16:41:30.316454Z","iopub.status.busy":"2021-06-26T16:41:30.316008Z","iopub.status.idle":"2021-06-26T16:41:30.42114Z","shell.execute_reply":"2021-06-26T16:41:30.420152Z","shell.execute_reply.started":"2021-06-26T16:41:30.31627Z"}},"outputs":[],"source":"y_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)"},{"cell_type":"code","execution_count":140,"metadata":{"_cell_guid":"277534eb-7ec8-4359-a2f4-30f7f76611b8","_kg_hide-input":true,"_uuid":"00a9b98fd4e230db427a63596a2747f05b1654c1","execution":{"iopub.execute_input":"2021-06-26T16:41:30.422908Z","iopub.status.busy":"2021-06-26T16:41:30.422475Z","iopub.status.idle":"2021-06-26T16:41:30.426856Z","shell.execute_reply":"2021-06-26T16:41:30.425882Z","shell.execute_reply.started":"2021-06-26T16:41:30.422736Z"}},"outputs":[],"source":"#models = pd.DataFrame({\n#    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n#              'Random Forest', 'Naive Bayes', \n#              'Decision Tree', 'Gradient Boosting Classifier', 'Voting Classifier', 'XGB Classifier','ExtraTrees Classifier','Bagging Classifier'],\n#    'Score': [svc_accy, knn_accy, logreg_accy, \n#              random_accy, gaussian_accy, dectree_accy,\n#               gradient_accy, voting_accy, XGBClassifier_accy, extraTree_accy, bagging_accy]})\n#models.sort_values(by='Score', ascending=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7128f3dd-1d8d-4b8e-afb4-891d8cb9657c","_uuid":"7e17482a69dbe99319219a603ea39f8bbde98b87"},"source":"# Part 8: Submit test predictions\n<a id=\"submit_predictions\"></a>\n***"},{"cell_type":"code","execution_count":141,"metadata":{"_uuid":"eb0054822f296ba86aa6005b2a5e35fbc1aec88b","execution":{"iopub.execute_input":"2021-06-26T16:41:30.429099Z","iopub.status.busy":"2021-06-26T16:41:30.42862Z","iopub.status.idle":"2021-06-26T16:41:30.646363Z","shell.execute_reply":"2021-06-26T16:41:30.645616Z","shell.execute_reply.started":"2021-06-26T16:41:30.428903Z"}},"outputs":[],"source":"all_models = [logreg_grid,\n              knn_grid, \n              knn_ran_grid,\n              svm_grid,\n              dectree_grid,\n              rf_grid,\n              bagging_grid,\n              adaBoost_grid,\n              voting_classifier]\n\nc = {}\nfor i in all_models:\n    a = i.predict(X_test)\n    b = accuracy_score(a, y_test)\n    c[i] = b\n    \n"},{"cell_type":"code","execution_count":142,"metadata":{"_cell_guid":"51368e53-52e4-41cf-9cc9-af6164c9c6f5","_uuid":"b947f168f6655c1c6eadaf53f3485d57c0cd74c7","execution":{"iopub.execute_input":"2021-06-26T16:41:30.648318Z","iopub.status.busy":"2021-06-26T16:41:30.647987Z","iopub.status.idle":"2021-06-26T16:41:32.045557Z","shell.execute_reply":"2021-06-26T16:41:32.044733Z","shell.execute_reply.started":"2021-06-26T16:41:30.648259Z"}},"outputs":[],"source":"test_prediction = (max(c, key=c.get)).predict(test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction\n    })\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)"},{"cell_type":"markdown","metadata":{},"source":"<div class=\"alert alert-info\">\n    <h1>Resources</h1>\n    <ul>\n        <li><b>Statistics</b></li>\n        <ul>\n            <li><a href=\"https://statistics.laerd.com/statistical-guides/measures-of-spread-standard-deviation.php\">Types of Standard Deviation</a></li>\n            <li><a href=\"https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen\">What Is a t-test? And Why Is It Like Telling a Kid to Clean Up that Mess in the Kitchen?</a></li>\n            <li><a href=\"https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-t-values-and-p-values-in-statistics\">What Are T Values and P Values in Statistics?</a></li>\n            <li><a href=\"https://www.youtube.com/watch?v=E4KCfcVwzyw\">What is p-value? How we decide on our confidence level.</a></li>\n        </ul>\n        <li><b>Writing pythonic code</b></li>\n        <ul>\n            <li><a href=\"https://www.kaggle.com/rtatman/six-steps-to-more-professional-data-science-code\">Six steps to more professional data science code</a></li>\n            <li><a href=\"https://www.kaggle.com/jpmiller/creating-a-good-analytics-report\">Creating a Good Analytics Report</a></li>\n            <li><a href=\"https://en.wikipedia.org/wiki/Code_smell\">Code Smell</a></li>\n            <li><a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style guides</a></li>\n            <li><a href=\"https://gist.github.com/sloria/7001839\">The Best of the Best Practices(BOBP) Guide for Python</a></li>\n            <li><a href=\"https://www.python.org/dev/peps/pep-0020/\">PEP 20 -- The Zen of Python</a></li>\n            <li><a href=\"https://docs.python-guide.org/\">The Hitchiker's Guide to Python</a></li>\n            <li><a href=\"https://realpython.com/tutorials/best-practices/\">Python Best Practice Patterns</a></li>\n            <li><a href=\"http://www.nilunder.com/blog/2013/08/03/pythonic-sensibilities/\">Pythonic Sensibilities</a></li>\n        </ul>\n        <li><b>Why Scikit-Learn?</b></li>\n        <ul>\n            <li><a href=\"https://www.oreilly.com/content/intro-to-scikit-learn/\">Introduction to Scikit-Learn</a></li>\n            <li><a href=\"https://www.oreilly.com/content/six-reasons-why-i-recommend-scikit-learn/\">Six reasons why I recommend scikit-learn</a></li>\n            <li><a href=\"https://hub.packtpub.com/learn-scikit-learn/\">Why you should learn Scikit-learn</a></li>\n            <li><a href=\"https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\">A Deep Dive Into Sklearn Pipelines</a></li>\n            <li><a href=\"https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial\">Sklearn pipelines tutorial</a></li>\n            <li><a href=\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\">Managing Machine Learning workflows with Sklearn pipelines</a></li>\n            <li><a href=\"https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\">A simple example of pipeline in Machine Learning using SKlearn</a></li>\n        </ul>\n    </ul>\n    <h1>Credits</h1>\n    <ul>\n        <li>To Brandon Foltz for his <a href=\"https://www.youtube.com/channel/UCFrjdcImgcQVyFbK04MBEhA\">youtube</a> channel and for being an amazing teacher.</li>\n        <li>To GA where I started my data science journey.</li>\n        <li>To Kaggle community for inspiring me over and over again with all the resources I need.</li>\n        <li>To Udemy Course \"Deployment of Machine Learning\". I have used and modified some of the code from this course to help making the learning process intuitive.</li>\n    </ul>\n</div>"},{"cell_type":"markdown","metadata":{},"source":"<div class=\"alert alert-info\">\n<h4>If you like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:</h4>\n    <ul>\n        <li><a href=\"https://www.linkedin.com/in/masumrumi/\"><b>LinkedIn</b></a></li>\n        <li><a href=\"https://github.com/masumrumi\"><b>Github</b></a></li>\n        <li><a href=\"https://masumrumi.github.io/cv/\"><b>masumrumi.github.io/cv/</b></a></li>\n        <li><a href=\"https://www.youtube.com/channel/UC1mPjGyLcZmsMgZ8SJgrfdw\"><b>Youtube</b></a></li>\n    </ul>\n\n<p>This kernel will always be a work in progress. I will incorporate new concepts of data science as I comprehend them with each update. If you have any idea/suggestions about this notebook, please let me know. Any feedback about further improvements would be genuinely appreciated.</p>\n\n<h1>If you have come this far, Congratulations!!</h1>\n\n<h1>If this notebook helped you in any way or you liked it, please upvote and/or leave a comment!! :)</h1></div>"}],"metadata":{"interpreter":{"hash":"4ee7c4e4684082ce1f46e630280ae5702684ea5c5635036c275c1be52ef87f64"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "9c75ca41-8357-479e-8a46-ebdec5f035f3",
+    "_uuid": "319ae25236d9fddf1745ea1c4cb365e5dbb00372"
+   },
+   "source": "<img src=\"http://data.freehdw.com/ships-titanic-vehicles-best.jpg\"  Width=\"800\">"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "bdce3bc433feb19f6622ab910cfe2123ccd07a1c"
+   },
+   "source": "<a id=\"introduction\" ></a><br>\nThis kernel is for all aspiring data scientists to learn from and to review their knowledge. We will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation. I am super excited to share my first kernel with the Kaggle community. As I go on in this journey and learn new topics, I will incorporate them with each new updates. So, check for them and please <b>leave a comment</b> if you have any suggestions to make this kernel better!! Going back to the topics of this kernel, I will do more in-depth visualizations to explain the data, and the machine learning classifiers will be used to predict passenger survival status.\n\n<div style=\"text-align: left\"> \n    <br>\n    NOTE:\n    <ul>\n        <li>Follow me on <a href=\"https://www.youtube.com/channel/UC1mPjGyLcZmsMgZ8SJgrfdw\"><b>YOUTUBE</b></a> to get the video tutorial for this notebook.\n        <li>If you want to learn more about Advanced Regression models, please check out <a href=\"https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing\">this</a> kernel.</li>\n        <li>If you are reading this on github, I recommend you read this on <a href=\"https://www.kaggle.com/masumrumi/a-statistical-analysis-ml-workflow-of-titanic\">kaggle</a>.</li>\n    </ul>\n</div>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "7224a910-ec6b-481d-82f1-90ca6b5d037e",
+    "_uuid": "9cd04af82734c5b53aaddc80992e1f499c180611"
+   },
+   "source": "# Kernel Goals\n<a id=\"aboutthiskernel\"></a>\n***\nThere are three primary goals of this kernel.\n- <b>Do a statistical analysis</b> of how some group of people was survived more than others. \n- <b>Do an exploratory data analysis(EDA)</b> of titanic with visualizations and storytelling.  \n- <b>Predict</b>: Use machine learning classification models to predict the chances of passengers survival.\n\nP.S. If you want to learn more about regression models, try this [kernel](https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing/edit/run/9585160). "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "b3b559a5-dad0-419e-835a-e6babd1042ff",
+    "_uuid": "1b1a0b28ad37a349e284d1e6ce6477d11b95e7c9"
+   },
+   "source": "# Part 1: Importing Necessary Libraries and datasets\n***\n<a id=\"import_libraries**\"></a>\n## 1a. Loading libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {
+    "_cell_guid": "80643cb5-64f3-4180-92a9-2f8e83263ac6",
+    "_kg_hide-input": true,
+    "_uuid": "33d54abf387474bce3017f1fc3832493355010c0",
+    "tags": []
+   },
+   "outputs": [],
+   "source": "# Import necessary modules\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline \n# %config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \n\nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\n\n\n\nimport os ## imporing os\nprint(os.listdir(\"../input/\"))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "bd41125b-6dd4-41d9-8905-31edc812d18e",
+    "_uuid": "82ccd43cc8449346749bf8a35e1acb9a40e3b141"
+   },
+   "source": "## 1b. Loading Datasets\n<a id=\"load_data\"></a>\n***"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "30b23f046eef6d19c26e6ad967cef914cf312791"
+   },
+   "source": "After loading the necessary modules, we need to import the datasets. Many of the business problems usually come with a tremendous amount of messy data. We extract those data from many sources. I am hoping to write about that in a different kernel. For now, we are going to work with a less complicated and quite popular machine learning dataset."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {
+    "_cell_guid": "28722a45-5f11-4629-8814-9ab913e9349a",
+    "_kg_hide-input": false,
+    "_uuid": "185b34e70f2efded0c665c6713f79b840ddf0c89",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:04.910168Z",
+     "iopub.status.busy": "2021-06-26T16:35:04.909538Z",
+     "iopub.status.idle": "2021-06-26T16:35:04.930876Z",
+     "shell.execute_reply": "2021-06-26T16:35:04.930259Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:04.91012Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Importing the datasets\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "d55ae33391486797b979ef1117e8d8401ac1dab4"
+   },
+   "source": "You are probably wondering why two datasets? Also, Why have I named it \"train\" and \"test\"?  To explain that I am going to give you an overall picture of the supervised machine learning process. \n\n\"Machine Learning\" is simply \"Machine\" and \"Learning\". Nothing more and nothing less. In a supervised machine learning process, we are giving machine/computer/models specific inputs or data(text/number/image/audio) to learn from aka we are training the machine to learn certain aspects based on the data and the output. Now, how can we determine that machine is actually learning what we are try to teach? That is where the test set comes to play. We withhold part of the data where we know the output/result of each datapoints, and we use this data to test the trained models.  We then compare the outcomes to determine the performance of the algorithms. If you are a bit confused thats okay. I will explain more as we keep reading. Let's take a look at sample datasets."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [],
+   "source": "train.head()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": "temp = train.groupby(\"Sex\")['Age'].min().to_frame().reset_index()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": "temp"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [],
+   "source": "temp = temp.rename(columns={\"Age\": \"min_age\"})"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [],
+   "source": "temp"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [],
+   "source": "train.head()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {},
+   "outputs": [],
+   "source": "train.dtypes"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "c87c72ba-c9b2-48e9-86d8-c711d0795ca0",
+    "_uuid": "5759d720798ca115cc5d3d2f75be6961d1455832"
+   },
+   "source": "## 1c. A Glimpse of the Datasets. \n<a id=\"glimpse\"></a>\n***"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "# Train Set"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:04.932322Z",
+     "iopub.status.busy": "2021-06-26T16:35:04.932015Z",
+     "iopub.status.idle": "2021-06-26T16:35:04.958015Z",
+     "shell.execute_reply": "2021-06-26T16:35:04.957084Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:04.932234Z"
+    }
+   },
+   "outputs": [],
+   "source": "%%time\ntrain.sample(5)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "ab439b32-e251-489a-89fd-cfcd61b236bf",
+    "_uuid": "69b24241db4d4eae9e46711c384d8130f6fa8322"
+   },
+   "source": "# Test Set"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {
+    "_cell_guid": "0f0649fa-b003-403f-9d7c-d2d14a6cf068",
+    "_kg_hide-input": true,
+    "_uuid": "877b2fc905cd60e3f9a525b6fedad9a5c0a671e5",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:04.960536Z",
+     "iopub.status.busy": "2021-06-26T16:35:04.960114Z",
+     "iopub.status.idle": "2021-06-26T16:35:08.927166Z",
+     "shell.execute_reply": "2021-06-26T16:35:08.926315Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:04.960357Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Take a look at the overview of the dataset. \n%timeit test.sample(5)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "5f7426639cf97db92e4ca85a13e89c8394f6aa7c"
+   },
+   "source": "This is a sample of train and test dataset. Lets find out a bit more about the train and test dataset. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "1258a94388599a131fe08cd6e05205b15d53df66",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:08.929903Z",
+     "iopub.status.busy": "2021-06-26T16:35:08.929444Z",
+     "iopub.status.idle": "2021-06-26T16:35:08.945917Z",
+     "shell.execute_reply": "2021-06-26T16:35:08.945011Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:08.92985Z"
+    }
+   },
+   "outputs": [],
+   "source": "print (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (train.info())\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))\nprint (test.info())"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "15c64d36-94b3-4798-af86-775f70feb2dd",
+    "_uuid": "c72d21139ee6220aee5d8f654561864a5f6499b7"
+   },
+   "source": " ## 1d. About This Dataset\n<a id=\"aboutthisdataset\"></a>\n***\nThe data has split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\n***The training set includes our target variable(dependent variable), passenger survival status***(also known as the ground truth from the Titanic tragedy) along with other independent features like gender, class, fare, and Pclass. \n\nThe test set should be used to see how well our model performs on unseen data. When we say unseen data, we mean that the algorithm or machine learning models have no relation to the test data. We do not want to use any part of the test data in any way to modify our algorithms; Which are the reasons why we clean our test data and train data separately. ***The test set does not provide passengers survival status***. We are going to use our model to predict passenger survival status.\n\nNow let's go through the features and describe a little. There is a couple of different type of variables, They are...\n\n***\n**Categorical:**\n- **Nominal**(variables that have two or more categories, but which do not have an intrinsic order.)\n   > - **Cabin**\n   > - **Embarked**(Port of Embarkation)\n            C(Cherbourg)\n            Q(Queenstown) \n            S(Southampton)\n        \n- **Dichotomous**(Nominal variable with only two categories)\n   > - **Sex**\n            Female\n            Male\n- **Ordinal**(variables that have two or more categories just like nominal variables. Only the categories can also be ordered or ranked.)\n   > - **Pclass** (A proxy for socio-economic status (SES)) \n            1(Upper)\n            2(Middle) \n            3(Lower)\n***\n**Numeric:**\n- **Discrete**\n  >  - **Passenger ID**(Unique identifing # for each passenger)\n  >  - **SibSp**\n  >  - **Parch**\n  >  - **Survived** (Our outcome or dependent variable)\n            0\n            1\n- **Continous**\n>  - **Age**\n>  - **Fare**\n***\n**Text Variable**\n> - **Ticket** (Ticket number for passenger.)\n> - **Name**(  Name of the passenger.) \n\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "7b21d695-c767-48ad-a3c8-abb9bba56e71",
+    "_uuid": "53fdd02b149e47bd7168dba94ddff754626b1781"
+   },
+   "source": "## 1e. Tableau Visualization of the Data\n<a id='tableau_visualization'></a>\n***\nI have incorporated a tableau visualization below of the training data. This visualization... \n* is for us to have an overview and play around with the dataset. \n* is done without making any changes(including Null values) to any features of the dataset.\n***\nLet's get a better perspective of the dataset through this visualization.\n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 13,
+   "metadata": {
+    "_cell_guid": "0ca9339e-4d13-4eb6-b28b-4a9e614ca2d0",
+    "_kg_hide-input": true,
+    "_uuid": "bc9819aecc9adceb1fa3fe151388fd41f5dcece2",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:08.947896Z",
+     "iopub.status.busy": "2021-06-26T16:35:08.947379Z",
+     "iopub.status.idle": "2021-06-26T16:35:08.954197Z",
+     "shell.execute_reply": "2021-06-26T16:35:08.953477Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:08.947651Z"
+    }
+   },
+   "outputs": [],
+   "source": "%%HTML\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Titanic_data_mining&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "2b6ce9bc-8210-433d-ab4b-d8afe93c3810",
+    "_uuid": "b46be01bb1ba3ff4f23c72038679542ba3f780de"
+   },
+   "source": "We want to see how the left vertical bar changes when we filter out unique values of certain features. We can use multiple filters to see if there are any correlations among them. For example, if we click on **upper** and **Female** tab, we would see that green color dominates the bar with a ratio of 91:3 survived and non survived female passengers; a 97% survival rate for females. We can reset the filters by clicking anywhere in the whilte space. The age distribution chart on top provides us with some more info such as, what was the age range of those three unlucky females as the red color give away the unsurvived once. If you would like to check out some of my other tableau charts, please click [here.](https://public.tableau.com/profile/masum.rumi#!/)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "24dfbb58-4708-42a1-9122-c7e0b96ad0e9",
+    "_uuid": "e789474652ddf03c65e7bb8f17f69544b907cecb"
+   },
+   "source": "# Part 2: Overview and Cleaning the Data\n<a id=\"cleaningthedata\"></a>\n***\n## 2a. Overview"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "359e6e3e-3a27-45aa-b6cf-ec18b8220eae",
+    "_uuid": "f0ec8e9300f40427a2a53f9c3e3f92e120ce786b"
+   },
+   "source": "Datasets in the real world are often messy, However, this dataset is almost clean. Lets analyze and see what we have here."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 14,
+   "metadata": {
+    "_cell_guid": "bf19c831-fbe0-49b6-8bf8-d7db118f40b1",
+    "_kg_hide-input": true,
+    "_uuid": "5a0593fb4564f0284ca7fdf5c006020cb288db95",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:08.956119Z",
+     "iopub.status.busy": "2021-06-26T16:35:08.955538Z",
+     "iopub.status.idle": "2021-06-26T16:35:08.973222Z",
+     "shell.execute_reply": "2021-06-26T16:35:08.972151Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:08.956072Z"
+    }
+   },
+   "outputs": [],
+   "source": "## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n## We will drop PassengerID and Ticket since it will be useless for our data. \n#train.drop(['PassengerId'], axis=1, inplace=True)\n#test.drop(['PassengerId'], axis=1, inplace=True)\n\nprint (train.info())\nprint (\"*\"*40)\nprint (test.info())"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "f9b2f56f-e95c-478d-aa49-3f6cb277830f",
+    "_uuid": "b5accab7fc7471fea224dcae81683b9f3c0f617b"
+   },
+   "source": "It looks like, the features have unequal amount of data entries for every column and they have many different types of variables. This can happen for the following reasons...\n* We may have missing values in our features.\n* We may have categorical features. \n* We may have alphanumerical or/and text features. \n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "9912539a-12b5-4739-bc2c-e1cecf758dca",
+    "_uuid": "6105e90cd8f0e8d49ae188edad65414678a7be23"
+   },
+   "source": "## 2b. Dealing with Missing values\n<a id=\"dealwithnullvalues\"></a>\n***\n**Missing values in *train* dataset.**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:08.975451Z",
+     "iopub.status.busy": "2021-06-26T16:35:08.974927Z",
+     "iopub.status.idle": "2021-06-26T16:35:08.98326Z",
+     "shell.execute_reply": "2021-06-26T16:35:08.982644Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:08.975205Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Let's write a functin to print the total percentage of the missing values.(this can be a good exercise for beginners to try to write simple functions like this.)\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:08.985334Z",
+     "iopub.status.busy": "2021-06-26T16:35:08.984844Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.090701Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.090116Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:08.985168Z"
+    }
+   },
+   "outputs": [],
+   "source": "%timeit -r2 -n10 missing_percentage(train) # setting the number of runs(-r) and/or loops (-n)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.092256Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.09199Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.108063Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.107054Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.092212Z"
+    }
+   },
+   "outputs": [],
+   "source": "missing_percentage(train)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "6e1b2b57-78b9-4021-bc53-a7681b63f97c",
+    "_uuid": "197487867c9d099548c7d009c4a80418927be07c"
+   },
+   "source": "**Missing values in *test* set.**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 18,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.110118Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.109653Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.206762Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.205452Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.10993Z"
+    }
+   },
+   "outputs": [],
+   "source": "%%timeit -r2 -n10 \nmissing_percentage(test)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 19,
+   "metadata": {
+    "_cell_guid": "073ef91b-e401-47a1-9b0a-d08ad710abce",
+    "_kg_hide-input": true,
+    "_uuid": "1ec1de271f57c9435ce111261ba08c5d6e34dbcb",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.208229Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.207968Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.221423Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.220732Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.208186Z"
+    }
+   },
+   "outputs": [],
+   "source": "missing_percentage(test)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "0217a17b-8017-4221-a664-dbbc42f7a5eb",
+    "_uuid": "2051377dfc36cbeb9fda78cb02d5bd3a00ee2457"
+   },
+   "source": "We see that in both **train**, and **test** dataset have missing values. Let's make an effort to fill these missing values starting with \"Embarked\" feature. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "aaf73f0b-ec84-4da1-b424-0170691c50c8",
+    "_uuid": "84d3c45c3a59e16ac2c887d6effe71434b2659ef"
+   },
+   "source": "### Embarked feature\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.223175Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.222681Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.230671Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.229793Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.223128Z"
+    }
+   },
+   "outputs": [],
+   "source": "def percent_value_counts(df, feature):\n    \"\"\"This function takes in a dataframe and a column and finds the percentage of the value_counts\"\"\"\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n    ## creating a df with th\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    ## concating percent and total dataframe\n\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([total, percent], axis = 1)\n    "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 21,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.236974Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.236548Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.254321Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.253654Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.236929Z"
+    }
+   },
+   "outputs": [],
+   "source": "percent_value_counts(train, 'Embarked')"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 22,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.259474Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.259268Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.274228Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.27333Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.259433Z"
+    }
+   },
+   "outputs": [],
+   "source": "percent_value_counts(train, 'Embarked')"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "826ae31d-4bd0-45f6-8c05-8b5d12d41144",
+    "_uuid": "174873ebdb2cd6c23777d464103afa26c0183ab2"
+   },
+   "source": "It looks like there are only two null values( ~ 0.22 %) in the Embarked feature, we can replace these with the mode value \"S\". However, let's dig a little deeper. \n\n**Let's see what are those two null values**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 23,
+   "metadata": {
+    "_cell_guid": "000ebdd7-ff57-48d9-91bf-a29ba79f1a1c",
+    "_kg_hide-input": true,
+    "_uuid": "6b9cb050e9dae424bb738ba9cdf3c84715887fa3",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.276102Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.275649Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.292037Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.291163Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.275879Z"
+    }
+   },
+   "outputs": [],
+   "source": "train[train.Embarked.isnull()]"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "306da283-fbd9-45fc-a79e-ac4a3fa7d396",
+    "_uuid": "57a4016a0ff673cdf5716310d42d7f142d275132"
+   },
+   "source": "We may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 24,
+   "metadata": {
+    "_cell_guid": "bf257322-0c9c-4fc5-8790-87d8c94ad28a",
+    "_kg_hide-input": true,
+    "_uuid": "ad15052fe6cebe37161c6e01e33a5c083dc2b558",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.293919Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.293564Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.866643Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.865701Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.293817Z"
+    }
+   },
+   "outputs": [],
+   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 18)\nax2.set_title('Test Set',  fontsize = 18)\n\n\n# ## Fixing legends\n# leg_1 = ax1.get_legend()\n# leg_1.set_title(\"PClass\")\n# legs = leg_1.texts\n# legs[0].set_text('Upper')\n# legs[1].set_text('Middle')\n# legs[2].set_text('Lower')\n\nfig.show()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "0e353881-a7d7-4fbf-bfd3-874479c0a650",
+    "_uuid": "c8a7f8c033f571d2fc8986009765ac4a78d3b6a7"
+   },
+   "source": "Here, in both training set and test set, the average fare closest to $80 are in the <b>C</b> Embarked values where pclass is 1. So, let's fill in the missing values as \"C\" "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 25,
+   "metadata": {
+    "_cell_guid": "2f5f3c63-d22c-483c-a688-a5ec2a477330",
+    "_kg_hide-input": true,
+    "_uuid": "52e51ada5dfeb700bf775c66e9307d6d1e2233de",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.868523Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.868016Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.874135Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.873022Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.868249Z"
+    },
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": "## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "47c17b1e-9486-43da-84ad-f91014225e88",
+    "_uuid": "44af808c1563671899ee498c9df12312c294277c"
+   },
+   "source": "### Cabin Feature\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 26,
+   "metadata": {
+    "_cell_guid": "e76cd770-b498-4444-b47a-4ac6ae63193b",
+    "_kg_hide-input": true,
+    "_uuid": "b809a788784e2fb443457d7ef4ca17a896bf58b4",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.876171Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.875621Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.886193Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.885088Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.875859Z"
+    },
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": "print(\"Train Cabin missing: \" + str(train.Cabin.isnull().sum()/len(train.Cabin)))\nprint(\"Test Cabin missing: \" + str(test.Cabin.isnull().sum()/len(test.Cabin)))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "47d450a8-0692-4403-8447-ab09d6dd0b8f",
+    "_uuid": "e61d1e4613dd4f51970d504e93ae30c072ca9d98"
+   },
+   "source": "Approximately 77% of Cabin feature is missing in the training data and 78% missing on the test data. \nWe have two choices, \n* we can either get rid of the whole feature, or \n* we can brainstorm a little and find an appropriate way to put them in use. For example, We may say passengers with cabin record had a higher socio-economic-status then others. We may also say passengers with cabin record were more likely to be taken into consideration when loading into the boat.\n\nLet's combine train and test data first and for now, will assign all the null values as **\"N\"**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 27,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "8ff7b4f88285bc65d72063d7fdf8a09a5acb62d3",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.888377Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.88784Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.902296Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.901697Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.888114Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "dae4beab-8c5a-4192-a460-e9abc6f14d3e",
+    "_uuid": "e2d84eff7cafdd68a471876b65e0ae866151d6d2"
+   },
+   "source": "All the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 28,
+   "metadata": {
+    "_cell_guid": "87995359-8a77-4e38-b8bb-e9b4bdeb17ed",
+    "_kg_hide-input": true,
+    "_uuid": "c1e9e06eb7f2a6eeb1a6d69f000217e7de7d5f25",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.904181Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.903766Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.909654Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.908573Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.904014Z"
+    }
+   },
+   "outputs": [],
+   "source": "all_data.Cabin = [i[0] for i in all_data.Cabin]"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "Now let's look at the value counts of the cabin features and see how it looks. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 29,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.91156Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.911098Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.928945Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.928025Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.911398Z"
+    }
+   },
+   "outputs": [],
+   "source": "percent_value_counts(all_data, \"Cabin\")"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "5e8cff0316f95162cdc9c2f3da905ad49fc548ca"
+   },
+   "source": "So, We still haven't done any effective work to replace the null values. Let's stop for a second here and think through how we can take advantage of some of the other features here.  \n* We can use the average of the fare column We can use pythons ***groupby*** function to get the mean fare of each cabin letter. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 30,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.930774Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.930283Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.942122Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.941067Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.930532Z"
+    }
+   },
+   "outputs": [],
+   "source": "all_data.groupby(\"Cabin\")['Fare'].mean().sort_values()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "8605664271220cb4a17fa1aca65207681503c9dd"
+   },
+   "source": "Now, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 31,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "a466da29f1989fa983147faf9e63d18783468567",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.943855Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.943364Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.952677Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.952057Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.943627Z"
+    }
+   },
+   "outputs": [],
+   "source": "def cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "6f56c9950206a5a8f30c39ca207dc47859b8d8a0"
+   },
+   "source": "Let's apply <b>cabin_estimator</b> function in each unknown cabins(cabin with <b>null</b> values). Once that is done we will separate our train and test to continue towards machine learning modeling. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 32,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.95455Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.954083Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.96302Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.962357Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.95437Z"
+    }
+   },
+   "outputs": [],
+   "source": "with_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 33,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "1c646b64c6e062656e5f727d5499266f847c4832",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.965179Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.96464Z",
+     "iopub.status.idle": "2021-06-26T16:35:09.981536Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.980705Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.964885Z"
+    }
+   },
+   "outputs": [],
+   "source": "##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "26d918c2-3c6b-48e8-8e2b-fc4531e8c59e",
+    "_uuid": "05a777057d9803235a17d79b72eefe7085ebf2e5"
+   },
+   "source": "### Fare Feature\n***\nIf you have paid attention so far, you know that there is only one missing value in the fare column. Let's have it. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 34,
+   "metadata": {
+    "_cell_guid": "2c75f369-e781-43df-be06-32585b372a0a",
+    "_kg_hide-input": true,
+    "_uuid": "020cafd121f2e6cbed89265c993ef3d76566cd6b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:09.983259Z",
+     "iopub.status.busy": "2021-06-26T16:35:09.982793Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.000785Z",
+     "shell.execute_reply": "2021-06-26T16:35:09.999778Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:09.983086Z"
+    }
+   },
+   "outputs": [],
+   "source": "test[test.Fare.isnull()]"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "0ffece2f-9df0-44e5-80cc-84894a8d0d45",
+    "_uuid": "bce23c7620db2cde9bae8efa04b00c78819f0268"
+   },
+   "source": "Here, We can take the average of the **Fare** column to fill in the NaN value. However, for the sake of learning and practicing, we will try something else. We can take the average of the values where**Pclass** is ***3***, **Sex** is ***male*** and **Embarked** is ***S***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 35,
+   "metadata": {
+    "_cell_guid": "e742aa76-b6f8-4882-8bd6-aa10b96f06aa",
+    "_kg_hide-input": true,
+    "_uuid": "f1dc8c6c33ba7df075ee608467be2a83dc1764fd",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.002749Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.002232Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.012662Z",
+     "shell.execute_reply": "2021-06-26T16:35:10.011431Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.00248Z"
+    }
+   },
+   "outputs": [],
+   "source": "missing_value = test[(test.Pclass == 3) & \n                     (test.Embarked == \"S\") & \n                     (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "3ff2fbe3-9858-4aad-9e33-e909d5128879",
+    "_uuid": "e04222497a5dfd77ac07dbcacbdc10dc1732da21"
+   },
+   "source": "### Age Feature\n***\nWe know that the feature \"Age\" is the one with most missing values, let's see it in terms of percentage. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "metadata": {
+    "_cell_guid": "8ff25fb3-7a4a-4e06-b48f-a06b8d844917",
+    "_kg_hide-input": true,
+    "_uuid": "c356e8e85f53a27e44b5f28936773a289592c5eb",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.014347Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.014023Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.024214Z",
+     "shell.execute_reply": "2021-06-26T16:35:10.023404Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.014284Z"
+    }
+   },
+   "outputs": [],
+   "source": "print (\"Train age missing value: \" + str((train.Age.isnull().sum()/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((test.Age.isnull().sum()/len(test))*100)+str(\"%\"))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "105d0bf8-dada-4499-8a41-499caf20fa81",
+    "_uuid": "8678df259a8f4e7f85f92603f312e1df76a26589"
+   },
+   "source": "We will take a different approach since **~20% data in the Age column is missing** in both train and test dataset. The age variable seems to be promising for determining survival rate. Therefore, It would be unwise to replace the missing values with median, mean or mode. We will use machine learning model Random Forest Regressor to impute missing value instead of Null value. We will keep the age column unchanged for now and work on that in the feature engineering section."
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "81537f22-2c69-45f2-90d3-a2a8790cb2fd",
+    "_uuid": "84518982b94e7e811bf3560a3862f06a46f1b530"
+   },
+   "source": "# Part 3. Visualization and Feature Relations\n<a id=\"visualization_and_feature_relations\" ></a>\n***\nBefore we dive into finding relations between independent variables and our dependent variable(survivor), let us create some assumptions about how the relations may turn-out among features.\n\n**Assumptions:**\n- Gender: More female survived than male\n- Pclass: Higher socio-economic status passenger survived more than others. \n- Age: Younger passenger survived more than other passengers. \n- Fare: Passenger with higher fare survived more that other passengers. This can be quite correlated with Pclass. \n\n\nNow, let's see how the features are related to each other by creating some visualizations. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "63420775-00e1-4650-a2f3-2ae6eebab23c",
+    "_uuid": "ca8bfb1bfe4d1079635a54c8daec3399b8355749"
+   },
+   "source": "## 3a. Gender and Survived\n<a id=\"gender_and_survived\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {
+    "_cell_guid": "78322e76-ccaa-4bb9-9cc2-7a3394ddfe8c",
+    "_kg_hide-input": true,
+    "_uuid": "6008755b1522e2a849b6e1ccbb7da57270293ca4",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.026025Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.025517Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.265216Z",
+     "shell.execute_reply": "2021-06-26T16:35:10.26434Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.025965Z"
+    }
+   },
+   "outputs": [],
+   "source": "import seaborn as sns\npal = {'male':\"green\", 'female':\"Pink\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", \n                 y = \"Survived\", \n                 data=train, \n                 palette = pal,\n                 linewidth=5,\n                 order = ['female','male'],\n                 capsize = .05,\n\n                )\n\nplt.title(\"Survived/Non-Survived Passenger Gender Distribution\", fontsize = 25,loc = 'center', pad = 40)\nplt.ylabel(\"% of passenger survived\", fontsize = 15, )\nplt.xlabel(\"Sex\",fontsize = 15);\n\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "fa7cb175-3c4d-4367-8b35-d3b43fb7d07d",
+    "_uuid": "ef171de53cb343da95d1ba82ebd961b1ff1756c3"
+   },
+   "source": "This bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 38,
+   "metadata": {
+    "_cell_guid": "6e303476-c1ed-49bb-8b9d-14659dd5739d",
+    "_kg_hide-input": true,
+    "_uuid": "163515a4c926323f7288f385795ea7b1ea545d7a",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.267021Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.266613Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.438911Z",
+     "shell.execute_reply": "2021-06-26T16:35:10.437974Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.266858Z"
+    }
+   },
+   "outputs": [],
+   "source": "pal = {1:\"seagreen\", 0:\"gray\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", \n                   hue=\"Survived\",\n                   data = train, \n                   linewidth=4, \n                   palette = pal\n)\n\n## Fixing title, xlabel and ylabel\nplt.title(\"Passenger Gender Distribution - Survived vs Not-survived\", fontsize = 25, pad=40)\nplt.xlabel(\"Sex\", fontsize = 15);\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\n\n## Fixing xticks\n#labels = ['Female', 'Male']\n#plt.xticks(sorted(train.Sex.unique()), labels)\n\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title(\"Survived\")\nlegs = leg.texts\nlegs[0].set_text(\"No\")\nlegs[1].set_text(\"Yes\")\nplt.show()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "0835c20e-f155-4bd7-8032-895d8c8042e6",
+    "_uuid": "bf15a586513bdde73dfa2279b739ffca040e71e4"
+   },
+   "source": "This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n**Summary**\n***\n- As we suspected, female passengers have survived at a much better rate than male passengers. \n- It seems about right since females and children were the priority. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "2daa3614-866c-48d7-a8cb-26ee8126a806",
+    "_uuid": "e746a4be3c0ed3d94a7a4366a5bff565c7bc9834"
+   },
+   "source": "## 3b. Pclass and Survived\n<a id=\"pcalss_and_survived\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 39,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.441162Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.440668Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.62703Z",
+     "shell.execute_reply": "2021-06-26T16:35:10.62605Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.440907Z"
+    }
+   },
+   "outputs": [],
+   "source": "temp = train[['Pclass', 'Survived', 'PassengerId']].groupby(['Pclass', 'Survived']).count().reset_index()\ntemp_df = pd.pivot_table(temp, values = 'PassengerId', index = 'Pclass',columns = 'Survived')\nnames = ['No', 'Yes']\ntemp_df.columns = names\nr = [0,1,2]\ntotals = [i+j for i, j in zip(temp_df['No'], temp_df['Yes'])]\nNo_s = [i / j * 100 for i,j in zip(temp_df['No'], totals)]\nYes_s = [i / j * 100 for i,j in zip(temp_df['Yes'], totals)]\n## Plotting\nplt.subplots(figsize = (15,10))\nbarWidth = 0.60\nnames = ('Upper', 'Middle', 'Lower')\n# Create green Bars\nplt.bar(r, No_s, color='Red', edgecolor='white', width=barWidth)\n# Create orange Bars\nplt.bar(r, Yes_s, bottom=No_s, color='Green', edgecolor='white', width=barWidth)\n\n \n# Custom x axis\nplt.xticks(r, names)\nplt.xlabel(\"Pclass\")\nplt.ylabel('Percentage')\n \n# Show graphic\nplt.show()\n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 40,
+   "metadata": {
+    "_cell_guid": "93a3a621-7be8-4f28-960d-939068944d3f",
+    "_kg_hide-input": true,
+    "_uuid": "61543e636b742647f90ea778f30a178a84e50533",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.628812Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.628387Z",
+     "iopub.status.idle": "2021-06-26T16:35:10.938152Z",
+     "shell.execute_reply": "2021-06-26T16:35:10.937374Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.628643Z"
+    }
+   },
+   "outputs": [],
+   "source": "plt.subplots(figsize = (15,10))\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=train, \n            linewidth=6,\n            capsize = .05,\n            errcolor='blue',\n            errwidth = 3\n            \n\n           )\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25, pad=40)\nplt.xlabel(\"Socio-Economic class\", fontsize = 15);\nplt.ylabel(\"% of Passenger Survived\", fontsize = 15);\nnames = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, names);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "e2c5ce9f-9759-43b6-b286-ec771a5a64c1",
+    "_uuid": "6faf3d5f770c23febb20cdc81cc079ed37d59959"
+   },
+   "source": "- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy, while \n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived. \n\n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 41,
+   "metadata": {
+    "_cell_guid": "f6eba487-9c63-4cd8-908a-393e2c277e45",
+    "_kg_hide-input": true,
+    "_uuid": "10867e6cb57231ae599406d827ba5e3f13ccb088",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:10.939885Z",
+     "iopub.status.busy": "2021-06-26T16:35:10.939421Z",
+     "iopub.status.idle": "2021-06-26T16:35:11.16284Z",
+     "shell.execute_reply": "2021-06-26T16:35:11.161997Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:10.939834Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived', \n              )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Passenger Class\", fontsize = 15,labelpad =20)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train.Pclass.unique()), labels);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "43ffcf43-2d0c-4033-8112-9edcca3576f1",
+    "_uuid": "f397633bae24a35d3fbe87d1ca54023356e065f9"
+   },
+   "source": "This KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \n\n**Summary**\n***\nThe first class passengers had the upper hand during the tragedy. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "1cb9d740-749b-4700-b9e9-973dbcad6aab",
+    "_uuid": "8eeb41d08ce680d51452deeb0ad054b184d67e16"
+   },
+   "source": "## 3c. Fare and Survived\n<a id=\"fare_and_survived\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 42,
+   "metadata": {
+    "_cell_guid": "cd6eb8a9-10a6-4ab8-aaec-4820df35f4c1",
+    "_kg_hide-input": true,
+    "_uuid": "85737078f0e84fe972a5ddb81b29e114fcfb54be",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:11.164195Z",
+     "iopub.status.busy": "2021-06-26T16:35:11.163924Z",
+     "iopub.status.idle": "2021-06-26T16:35:11.392608Z",
+     "shell.execute_reply": "2021-06-26T16:35:11.391811Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:11.164152Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Fare\", fontsize = 15, labelpad = 20);\n\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "6073f329-df80-4ab9-b99b-72e6fcdfe0c6",
+    "_uuid": "b5eba2b28ea428114d8ffab52feef95484bd76c0"
+   },
+   "source": "This plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check..."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 43,
+   "metadata": {
+    "_cell_guid": "bee8b01b-a56a-4762-bde0-4404a1c5ac1a",
+    "_kg_hide-input": true,
+    "_uuid": "916ab9dc56a05105afa80127d69deb9fc0095ba2",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:11.394719Z",
+     "iopub.status.busy": "2021-06-26T16:35:11.394204Z",
+     "iopub.status.idle": "2021-06-26T16:35:11.41273Z",
+     "shell.execute_reply": "2021-06-26T16:35:11.411929Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:11.394447Z"
+    }
+   },
+   "outputs": [],
+   "source": "train[train.Fare > 280]"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "3467e2d8-315c-4223-9166-0aca54543cdd",
+    "_uuid": "443d93fcfbad82fc611ce88e12556a6325ccd15c"
+   },
+   "source": "As we assumed, it looks like an outlier with a fare of $512. We sure can delete this point. However, we will keep it for now. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "95c27d94-fa65-4bf9-a855-8e5dab17704e",
+    "_uuid": "64ff8df884805f04692dc601da1ef99527309d54"
+   },
+   "source": "## 3d. Age and Survived\n<a id=\"age_and_survived\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 44,
+   "metadata": {
+    "_cell_guid": "9eb6733b-7577-4360-8252-e6d97c78b7db",
+    "_kg_hide-input": true,
+    "_uuid": "c6a4f46a7ce0e197f72abe293b69100c29a044ca",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:11.41461Z",
+     "iopub.status.busy": "2021-06-26T16:35:11.414164Z",
+     "iopub.status.idle": "2021-06-26T16:35:11.66643Z",
+     "shell.execute_reply": "2021-06-26T16:35:11.665545Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:11.414413Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 25, pad = 40)\nplt.xlabel(\"Age\", fontsize = 15, labelpad = 20)\nplt.ylabel('Frequency', fontsize = 15, labelpad= 20);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "a9aab64c-6170-4c8d-8446-cecdc9804b55",
+    "_uuid": "5238df80f5454d29e3793596a21fd0c00cb64a6c"
+   },
+   "source": "There is nothing out of the ordinary about this plot, except the very left part of the distribution. This may hint on the posibility that children and infants were the priority. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "077605b2-e9b4-4c45-8c5a-188508165f10",
+    "_uuid": "f8245da79c5394f7665d0b5429cb2fe4c4d0b057"
+   },
+   "source": "## 3e. Combined Feature Relations\n<a id='combined_feature_relations'></a>\n***\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 45,
+   "metadata": {
+    "_cell_guid": "924e19c4-8d58-404c-9a84-02f096269351",
+    "_kg_hide-input": true,
+    "_uuid": "71fc1c9843f789e19a5e8b2929579914d8ecdb3f",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:11.668148Z",
+     "iopub.status.busy": "2021-06-26T16:35:11.667828Z",
+     "iopub.status.idle": "2021-06-26T16:35:12.368731Z",
+     "shell.execute_reply": "2021-06-26T16:35:12.367992Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:11.668097Z"
+    }
+   },
+   "outputs": [],
+   "source": "pal = {1:\"seagreen\", 0:\"gray\"}\ng = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Survived\", margin_titles=True, hue = \"Survived\",\n                  palette=pal)\ng = g.map(plt.hist, \"Age\", edgecolor = 'white');\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "089999b4-bc44-49c6-9f86-aaaccabaa224",
+    "_uuid": "6aac036e1b235e5b10bc6a153ed226acfce2cfcb"
+   },
+   "source": "Facetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during the tragedy. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 46,
+   "metadata": {
+    "_cell_guid": "dcc34a91-261d-4929-a4eb-5072fcaf86ce",
+    "_kg_hide-input": true,
+    "_uuid": "b2ad776bf0254be1ccf76f46a40db7960aa3db24",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:12.370419Z",
+     "iopub.status.busy": "2021-06-26T16:35:12.369999Z",
+     "iopub.status.idle": "2021-06-26T16:35:14.029152Z",
+     "shell.execute_reply": "2021-06-26T16:35:14.028323Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:12.370369Z"
+    }
+   },
+   "outputs": [],
+   "source": "g = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Embarked\", margin_titles=True, hue = \"Survived\",\n                  palette = pal\n                  )\ng = g.map(plt.hist, \"Age\", edgecolor = 'white').add_legend();\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "b9b9906c-805d-438b-b72e-a57cc60d5ae8",
+    "_uuid": "4070616f2637a720a3cb580264cfaed9235b9020"
+   },
+   "source": "This is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 47,
+   "metadata": {
+    "_cell_guid": "fd9fe9e2-f7d4-4f83-9ce4-0a22160ef4fe",
+    "_kg_hide-input": true,
+    "_uuid": "f4d77506cabc7150466fa5bda64585d15814d48c",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:14.030913Z",
+     "iopub.status.busy": "2021-06-26T16:35:14.030621Z",
+     "iopub.status.idle": "2021-06-26T16:35:14.493248Z",
+     "shell.execute_reply": "2021-06-26T16:35:14.49225Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:14.030867Z"
+    }
+   },
+   "outputs": [],
+   "source": "g = sns.FacetGrid(train, size=5,hue=\"Survived\", col =\"Sex\", margin_titles=True,\n                palette=pal,)\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\ng.fig.suptitle(\"Survived by Sex, Fare and Age\", size = 25)\nplt.subplots_adjust(top=0.85)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "1c309d4b-3e24-406b-bd28-d5055a660f16",
+    "_uuid": "90bbc6e6edbf6188170a4de1b38732d009f7afae"
+   },
+   "source": "This facet grid unveils a couple of interesting insights. Let's find out.\n* The grid above clearly demonstrates the three outliers with Fare of over \\$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \\$100. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 48,
+   "metadata": {
+    "_cell_guid": "783403f6-9d3c-4a12-8505-cf321bd1a1ef",
+    "_kg_hide-input": true,
+    "_uuid": "75c41c85dc76c9749e5c417e1ed0425eed9c55e0",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:14.495102Z",
+     "iopub.status.busy": "2021-06-26T16:35:14.494676Z",
+     "iopub.status.idle": "2021-06-26T16:35:14.888836Z",
+     "shell.execute_reply": "2021-06-26T16:35:14.887825Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:14.494921Z"
+    }
+   },
+   "outputs": [],
+   "source": "## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n## factor plot\nsns.factorplot(x = \"Parch\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title(\"Factorplot of Parents/Children survived\", fontsize = 25)\nplt.subplots_adjust(top=0.85)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "33916321-237d-4381-990f-0faa11723c20",
+    "_uuid": "263113f38121c9e5f14247f05c262ee218be87f2"
+   },
+   "source": "**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 49,
+   "metadata": {
+    "_cell_guid": "f6ed143e-3e02-4e97-a255-73807018f0d1",
+    "_kg_hide-input": true,
+    "_uuid": "4ce5a4a6cff3966ac1811ee95f81c81fe4861a51",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:14.890716Z",
+     "iopub.status.busy": "2021-06-26T16:35:14.890276Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.244771Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.243687Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:14.890522Z"
+    }
+   },
+   "outputs": [],
+   "source": "sns.factorplot(x =  \"SibSp\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title('Factorplot of Sibilings/Spouses survived', fontsize = 25)\nplt.subplots_adjust(top=0.85)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "ee5b61b4-67d3-46b4-847d-4b5b85a8c791",
+    "_uuid": "c7a045b78e6b5f45ad891cf0515a6a4b2534d2ff"
+   },
+   "source": "**While, passenger who traveled in small groups with sibilings/spouses had better changes of survivint than other passengers.**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 50,
+   "metadata": {
+    "_cell_guid": "50a0920d-556b-4439-a67f-384ce793d094",
+    "_kg_hide-input": true,
+    "_uuid": "dfe723c71d4d29f599701d806ca97cd01a60142f",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.246815Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.246286Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.256239Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.255215Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.246539Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "003a7779-5966-45f8-a711-67e67234a654",
+    "_uuid": "402cd49464156ead61d5dd5698ffeb00eb71d0d3"
+   },
+   "source": "# Part 4: Statistical Overview\n<a id=\"statisticaloverview\"></a>\n***"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "91eba73b-f744-478b-bd6b-13da6cff000b",
+    "_uuid": "3e8b752c8963a76a86c8b1db80783c644090bdfa"
+   },
+   "source": "![title](https://cdn-images-1.medium.com/max/400/1*hFJ-LI7IXcWpxSLtaC0dfg.png)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "7b7e6e77-50bf-469f-b92b-73056224bc61",
+    "_uuid": "797aa171f2e13ea965cb9a352fcfd2001e119747"
+   },
+   "source": "**Train info**"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 51,
+   "metadata": {
+    "_cell_guid": "ad856ee6-b1ec-445d-92b0-cd6a83d58301",
+    "_kg_hide-input": true,
+    "_uuid": "35fc657641cc24aff89ade7d83d8b92e472dc3e6",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.258807Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.258212Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.297787Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.297008Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.258485Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.describe()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 52,
+   "metadata": {
+    "_cell_guid": "327c6775-9ba4-4d65-8c97-304cc9512e6a",
+    "_kg_hide-input": true,
+    "_uuid": "2f9f5fb7bade4d82d7b5a564a8ac91123b4921d2",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.299834Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.299339Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.331049Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.330129Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.29957Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.describe(include =['O'])"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 53,
+   "metadata": {
+    "_cell_guid": "5b817552-ecb8-4f6e-9950-6697d4c44d1f",
+    "_kg_hide-input": true,
+    "_uuid": "c88dcae6209f02226f2e772b42616b5650d108f4",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.33335Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.332779Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.352959Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.35204Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.333034Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Overview(Survived vs non survied)\nsurvived_summary = train.groupby(\"Survived\")\nsurvived_summary.mean().reset_index()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 54,
+   "metadata": {
+    "_cell_guid": "502dd0d2-a51a-47da-904c-66c9840a1b74",
+    "_kg_hide-input": true,
+    "_uuid": "65f9a660b942a8f92db94fe8fc41ccfa76a354cd",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.354847Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.354359Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.371166Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.37032Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.354612Z"
+    }
+   },
+   "outputs": [],
+   "source": "survived_summary = train.groupby(\"Sex\")\nsurvived_summary.mean().reset_index()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 55,
+   "metadata": {
+    "_cell_guid": "68cb2dac-6295-44d6-8aa0-5cddb53dd72c",
+    "_kg_hide-input": true,
+    "_uuid": "e49170e6e56329f68aba07a36389883ee1bee5ca",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.373155Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.372618Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.392055Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.391506Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.373092Z"
+    }
+   },
+   "outputs": [],
+   "source": "survived_summary = train.groupby(\"Pclass\")\nsurvived_summary.mean().reset_index()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "89ba2894-b129-4709-913d-f8cb35815925",
+    "_uuid": "e310c182f3541069329efcdd37373235fb144567"
+   },
+   "source": "I have gathered a small summary from the statistical overview above. Let's see what they are...\n- This train data set has 891 raw and 9 columns. \n- only 38% passenger survived during that tragedy.\n- ~74% female passenger survived, while only ~19% male passenger survived. \n- ~63% first class passengers survived, while only 24% lower class passenger survived.\n\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "5454218c-0a45-4a89-96fe-83d89b588183",
+    "_uuid": "d00b4e471e863f766c4aad7b88e1e6d9e57d6423"
+   },
+   "source": "## 4a. Correlation Matrix and Heatmap\n<a id=\"heatmap\"></a>\n***\n### Correlations"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 56,
+   "metadata": {
+    "_cell_guid": "d0acfa7a-6f3e-4783-925d-6e443a9a5baa",
+    "_kg_hide-input": true,
+    "_uuid": "c4057023aa30d3ce1befae168c00f3cb8491804b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.393816Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.393405Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.406432Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.405713Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.393654Z"
+    }
+   },
+   "outputs": [],
+   "source": "pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "92a69940-78f8-4139-a9a7-24ccf5f6afe7",
+    "_uuid": "211c5e2e817f4b10e64a28f5f8ce1d7eec9761fc"
+   },
+   "source": "** Sex is the most important correlated feature with *Survived(dependent variable)* feature followed by Pclass.** "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 57,
+   "metadata": {
+    "_cell_guid": "3e9fdd2e-f081-48ad-9c0f-afa475b15dfe",
+    "_kg_hide-input": true,
+    "_uuid": "c3212c222341c250aacee47c43b1a023b9b65857",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.408424Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.407893Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.421826Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.42092Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.408231Z"
+    }
+   },
+   "outputs": [],
+   "source": "## get the most important variables. \ncorr = train.corr()**2\ncorr.Survived.sort_values(ascending=False)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "f5f257ef-88b1-4302-ad41-d90892fbe4e9",
+    "_uuid": "1837acd3898d4787c9011e353dfc4dc15fd1abb2"
+   },
+   "source": "\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 58,
+   "metadata": {
+    "_cell_guid": "eee23849-a390-4d16-a8df-d29c6f575413",
+    "_kg_hide-input": true,
+    "_uuid": "285660c315b854497fe00847d051ceac5c9ec298",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.423924Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.423431Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.769867Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.768898Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.423681Z"
+    }
+   },
+   "outputs": [],
+   "source": "## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nimport numpy as np\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.set_style('whitegrid')\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n            linewidths=.9, \n            linecolor='white',\n            fmt='.2g',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "0e0b5ceb-fde5-40a7-b33b-b44e8f04189a",
+    "_uuid": "41e2bc2eff5699b14a0f47d5bd2e428ee5bec3b8"
+   },
+   "source": "#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "85faf680-5f78-414f-87b9-b72ef6d6ffc2",
+    "_uuid": "18c908fdbe16ae939827ec12a4ce028094a8a587"
+   },
+   "source": "## 4b. Statistical Test for Correlation\n<a id=\"statistical_test\"></a>\n***\n\nStatistical tests are the scientific way to prove the validation of theories. In any case, when we look at the data, we seem to have an intuitive understanding of where data is leading us. However, when we do statistical tests, we get a scientific or mathematical perspective of how significant these results are. Let's apply some of these methods and see how we are doing with our predictions.\n\n###  Hypothesis Testing Outline\n\nA hypothesis test compares the mean of a control group and experimental group and tries to find out whether the two sample means are different from each other and if they are different, how significant that difference is.\n \nA **hypothesis test** usually consists of multiple parts: \n\n1. Formulate a well-developed research problem or question: The hypothesis test usually starts with a concrete and well-developed researched problem. We need to ask the right question that can be answered using statistical analysis. \n2. **The null hypothesis($H_0$) and Alternating hypothesis($H_1$)**:\n> * The **null hypothesis($H_0$)** is something that is assumed to be true. It is the status quo. In a null hypothesis, the observations are the result of pure chance. When we set out to experiment, we form the null hypothesis by saying that there is no difference between the means of the control group and the experimental group.\n> *  An **Alternative hypothesis($H_A$)** is a claim and the opposite of the null hypothesis.  It is going against the status quo. In an alternative theory, the observations show a real effect combined with a component of chance variation.\n    \n3. Determine the **test statistic**: test statistic can be used to assess the truth of the null hypothesis. Depending on the standard deviation we either use t-statistics or z-statistics. In addition to that, we want to identify whether the test is a one-tailed test or two-tailed test. [This](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article explains it pretty well. [This](https://stattrek.com/hypothesis-test/hypothesis-testing.aspx) article is pretty good as well. \n\n4. Specify a **Significance level** and **Confidence Interval**: The significance level($\\alpha$) is the probability of rejecting a null hypothesis when it is true. In other words, we are ***comfortable/confident*** with rejecting the null hypothesis a significant amount of times even though it is true. This considerable amount is our Significant level. In addition to that, Significance level is one minus our Confidence interval. For example, if we say, our significance level is 5%, then our confidence interval would be (1 - 0.05) = 0.95 or 95%. \n\n5. Compute the **T-Statistics/Z-Statistics**: Computing the t-statistics follows a simple equation. This equation slightly differs depending on one sample test or two sample test  \n\n6. Compute the **P-value**: P-value is the probability that a test statistic at least as significant as the one observed would be obtained assuming that the null hypothesis is correct. The p-value is known to be unintuitive, and even many professors are known to explain it wrong. I think this [video](https://www.youtube.com/watch?v=E4KCfcVwzyw) explains the p-value well. **The smaller the P-value, the stronger the evidence against the null hypothesis.**\n\n7. **Describe the result and compare the p-value with the significance value($\\alpha$)**: If p<=$\\alpha$, then the observed effect is statistically significant, the null hypothesis is ruled out, and the alternative hypothesis is valid. However if the p> $\\alpha$, we say that, we fail to reject the null hypothesis. Even though this sentence is grammatically wrong, it is logically right. We never accept the null hypothesis just because we are doing the statistical test with sample data points.\n\nWe will follow each of these steps above to do your hypothesis testing below.\n\nP.S. Khan Academy has a set of videos that I think are intuative and helped me understand conceptually. \n\n***"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "f3b49278bd1b8eff8fe1b14c1506d73cf53bd859"
+   },
+   "source": "### Hypothesis testing for Titanic\n#### Formulating a well developed researched question: \nRegarding this dataset, we can formulate the null hypothesis and alternative hypothesis by asking the following questions. \n> * **Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive?**. \n> * **Is there a substantial difference in the survival rate between the male and female passengers?**\n\n\n#### The Null Hypothesis and The Alternative Hypothesis:\nWe can formulate our hypothesis by asking questions differently. However, it is essential to understand what our end goal is. Here our dependent variable or target variable is **Survived**. Therefore, we say\n\n> ** Null Hypothesis($H_0$):** There is no difference in the survival rate between the male and female passengers. or the mean difference between male and female passenger in the survival rate is zero.  \n>  ** Alternative Hypothesis($H_A$):** There is a difference in the survival rate between the male and female passengers. or the mean difference in the survival rate between male and female is not zero.\n\n\nOnc thing we can do is try to set up the Null and Alternative Hypothesis in such way that, when we do our t-test, we can choose to do one tailed test. According to [this](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article, one-tailed tests are more powerful than two-tailed test. In addition to that, [this](https://www.youtube.com/watch?v=5NcMFlrnYp8&list=PLIeGtxpvyG-LrjxQ60pxZaimkaKKs0zGF) video is also quite helpful understanding these topics. with this in mind we can update/modify our null and alternative hypothesis. Let's see how we can rewrite this..\n\n> **Null Hypothesis(H0):** male mean is greater or equal to female mean.\n\n> **Alternative Hypothesis(H1):** male mean is less than female mean. \n\n#### Determine the test statistics:\n> This will be a two-tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. \n> Since we do not know the standard deviation($\\sigma$) and n is small, we will use the t-distribution. \n\n#### Specify the significance level:\n> Specifying a significance level is an important step of the hypothesis test. It is an ultimate balance between type 1 error and type 2 error. We will discuss more in-depth about those in another lesson. For now, we have decided to make our significance level($\\alpha$) = 0.05. So, our confidence interval or non-rejection region would be (1 - $\\alpha$)=(1-0.05) = 95%. \n\n#### Computing T-statistics and P-value:\nLet's take a random sample and see the difference."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 59,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "abd034cffc591bf1ef2b4a8ed3e5a65eb133d61e",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.771771Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.771345Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.783362Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.782301Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.771603Z"
+    }
+   },
+   "outputs": [],
+   "source": "male_mean = train[train['Sex'] == 1].Survived.mean()\n\nfemale_mean = train[train['Sex'] == 0].Survived.mean()\nprint (\"Male survival mean: \" + str(male_mean))\nprint (\"female survival mean: \" + str(female_mean))\n\nprint (\"The mean difference between male and female survival rate: \" + str(female_mean - male_mean))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "0c1c27af262ba094ff1fd02867b1a41d5369720f"
+   },
+   "source": "Now, we have to understand that those two means are not  **the population mean ($\\bar{\\mu}$)**.  *The population mean is a statistical term statistician uses to indicate the actual average of the entire group. The group can be any gathering of multiple numbers such as animal, human, plants, money, stocks.* For example, To find the age population mean of Bulgaria; we will have to account for every single person's age and take their age. Which is almost impossible and if we were to go that route; there is no point of doing statistics in the first place. Therefore we approach this problem using sample sets. The idea of using sample set is that; if we take multiple samples of the same population and take the mean of them and put them in a distribution; eventually the distribution start to look more like a **normal distribution**. The more samples we take and the more sample means will be added and, the closer the normal distribution will reach towards population mean. This is where **Central limit theory** comes from. We will go more in depth of this topic later on. \n\nGoing back to our dataset, like we are saying these means above are part of the whole story. We were given part of the data to train our machine learning models, and the other part of the data was held back for testing. Therefore, It is impossible for us at this point to know the population means of survival for male and females. Situation like this calls for a statistical approach. We will use the sampling distribution approach to do the test. let's take 50 random sample of male and female from our train data."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 60,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.785359Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.784861Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.815921Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.815302Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.785103Z"
+    }
+   },
+   "outputs": [],
+   "source": "# separating male and female dataframe. \nimport random\nmale = train[train['Sex'] == 1]\nfemale = train[train['Sex'] == 0]\n\n## empty list for storing mean sample\nm_mean_samples = []\nf_mean_samples = []\n\nfor i in range(50):\n    m_mean_samples.append(np.mean(random.sample(list(male['Survived']),50,)))\n    f_mean_samples.append(np.mean(random.sample(list(female['Survived']),50,)))\n    \n\n# Print them out\nprint (f\"Male mean sample mean: {round(np.mean(m_mean_samples),2)}\")\nprint (f\"Male mean sample mean: {round(np.mean(f_mean_samples),2)}\")\nprint (f\"Difference between male and female mean sample mean: {round(np.mean(f_mean_samples) - np.mean(m_mean_samples),2)}\")"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "H0: male mean is greater or equal to female mean<br>\nH1: male mean is less than female mean. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "706d89356793f306d807c3fb277963e07181915c"
+   },
+   "source": "According to the samples our male samples ($\\bar{x}_m$) and female samples($\\bar{x}_f$) mean measured difference is ~ 0.55(statistically this is called the point estimate of the male population mean and female population mean). keeping in mind that...\n* We randomly select 50 people to be in the male group and 50 people to be in the female group. \n* We know our sample is selected from a broader population(trainning set). \n* We know we could have totally ended up with a different random sample of males and females.\n***\nWith all three points above in mind, how confident are we that, the measured difference is real or statistically significant? we can perform a **t-test** to evaluate that. When we perform a **t-test** we are usually trying to find out **an evidence of significant difference between population mean with hypothesized mean(1 sample t-test) or in our case difference between two population means(2 sample t-test).** \n\n\n\nThe **t-statistics** is the measure of a degree to which our groups differ standardized by the variance of our measurements. In order words, it is basically the measure of signal over noise. Let us describe the previous sentence a bit more for clarification. I am going to use [this post](http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen) as reference to describe the t-statistics here. \n\n\n#### Calculating the t-statistics\n# $$t = \\frac{\\bar{x}-\\mu}{\\frac{S} {\\sqrt{n}} }$$\n\nHere..\n* $\\bar{x}$ is the sample mean. \n* $\\mu$ is the hypothesized mean. \n* S is the standard deviation. \n* n is the sample size. \n\n\n1. Now, the denominator of this fraction $(\\bar{x}-\\mu)$ is basically the strength of the signal. where we calculate the difference between hypothesized mean and sample mean. If the mean difference is higher, then the signal is stronger. \n\nthe numerator of this fraction ** ${S}/ {\\sqrt{n}}$ ** calculates the amount of variation or noise of the data set. Here S is standard deviation, which tells us how much variation is there in the data. n is the sample size. \n\nSo, according to the explanation above, the t-value or t-statistics is basically measures the strength of the signal(the difference) to the amount of noise(the variation) in the data and that is how we calculate the t-value in one sample t-test. However, in order to calculate between two sample population mean or in our case we will use the follow equation. \n\n# $$t = \\frac{\\bar{x}_M - \\bar{x}_F}{\\sqrt {s^2 (\\frac{1}{n_M} + \\frac{1}{n_F})}}$$\n\nThis equation may seem too complex, however, the idea behind these two are similar. Both of them have the concept of signal/noise. The only difference is that we replace our hypothesis mean with another sample mean and the two sample sizes repalce one sample size. \n\nHere..\n* $\\bar{x}_M$ is the mean of our male group sample measurements. \n* $ \\bar{x}_F$ is the mean of female group samples. \n* $ n_M$ and $n_F$ are the sample number of observations in each group. \n* $ S^2$ is the sample variance.\n\nIt is good to have an understanding of what going on in the background. However, we will use **scipy.stats** to find the t-statistics. \n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "44e9000aefed8ea0125463486cc4a00c17e580e5"
+   },
+   "source": "#### Compare P-value with $\\alpha$\n> It looks like the p-value is very small compared to our significance level($\\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is \"**There is a significant difference in the survival rate between the male and female passengers.\"**"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "df06b6c8-daf6-4f5b-af51-9c1dfbac7a68",
+    "_uuid": "34869ce4ce852633b1f4a5cd111b98841982cc19"
+   },
+   "source": "# Part 5: Feature Engineering\n<a id=\"feature_engineering\"></a>\n***\nFeature Engineering is exactly what its sounds like. Sometimes we want to create extra features from with in the features that we have, sometimes we want to remove features that are alike. Features engineering is the simple word for doing all those. It is important to remember that we will create new features in such ways that will not cause **multicollinearity(when there is a relationship among independent variables)** to occur. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "8c439069-6168-4cda-846f-db4c21265089",
+    "_uuid": "3ca0785fe824c6ea471b2bcf9600007ed238d450"
+   },
+   "source": "## name_length\n***Creating a new feature \"name_length\" that will take the count of letters of each name***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 61,
+   "metadata": {
+    "_cell_guid": "d30d71c1-55bc-41c8-8536-9909d9f02538",
+    "_kg_hide-input": true,
+    "_uuid": "cb17c6f59bb2123cbf2cbc9c282b4d70ee283a86",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.817993Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.817477Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.832377Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.831471Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.817745Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "012489c507bf8bfb1ca3db9b0506493cf5595e61"
+   },
+   "source": "## title\n**Getting the title of each name as a new feature. **"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 62,
+   "metadata": {
+    "_cell_guid": "ded64d5f-43de-4a9e-b9c5-ec4d2869387a",
+    "_kg_hide-input": true,
+    "_uuid": "9c23229f7d06a1303a04b4a81c927453686ffec9",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.833953Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.833501Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.842414Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.841468Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.83376Z"
+    }
+   },
+   "outputs": [],
+   "source": "## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\n## Whenever we split like that, there is a good change that we will end up with while space around our string values. Let's check that. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 63,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.84422Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.843818Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.853522Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.852642Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.84407Z"
+    }
+   },
+   "outputs": [],
+   "source": "print(train.title.unique())"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 64,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.855322Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.854858Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.86306Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.86222Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.855101Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Let's fix that\ntrain.title = train.title.apply(lambda x: x.strip())"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 65,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.864826Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.864362Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.872663Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.871817Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.864612Z"
+    }
+   },
+   "outputs": [],
+   "source": "## We can also combile all three lines above for test set here\ntest['title'] = [i.split('.')[0].split(',')[1].strip() for i in test.Name]\n\n## However it is important to be able to write readable code, and the line above is not so readable. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 66,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.874489Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.873918Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.896665Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.895832Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.874258Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Let's replace some of the rare values with the keyword 'rare' and other word choice of our own. \n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n## Now in programming there is a term called DRY(Don't repeat yourself), whenever we are repeating  \n## same code over and over again, there should be a light-bulb turning on in our head and make us think\n## to code in a way that is not repeating or dull. Let's write a function to do exactly what we \n## did in the code above, only not repeating and more interesting. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 67,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.900031Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.899771Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.910036Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.908929Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.899989Z"
+    }
+   },
+   "outputs": [],
+   "source": "## we are writing a function that can help us modify title column\ndef name_converted(feature):\n    \"\"\"\n    This function helps modifying the title column\n    \"\"\"\n    \n    result = ''\n    if feature in ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col', 'Rev', 'Dona', 'Dr']:\n        result = 'rare'\n    elif feature in ['Ms', 'Mlle']:\n        result = 'Miss'\n    elif feature == 'Mme':\n        result = 'Mrs'\n    else:\n        result = feature\n    return result\n\ntest.title = test.title.map(name_converted)\ntrain.title = train.title.map(name_converted)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 68,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.912187Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.911644Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.923512Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.922507Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.912136Z"
+    }
+   },
+   "outputs": [],
+   "source": "print(train.title.unique())\nprint(test.title.unique())"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "42ccf293-04c7-4bea-9570-4cce9227b8af",
+    "_uuid": "e870c4fc44de4b2395963e583c84d2cae83c004b"
+   },
+   "source": "## family_size\n***Creating a new feature called \"family_size\".*** "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 69,
+   "metadata": {
+    "_cell_guid": "7083a7e7-d1d5-4cc1-ad67-c454b139f5f1",
+    "_kg_hide-input": true,
+    "_uuid": "cdfd54429cb235dd3b73535518950b2e515e54f2",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.925581Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.925033Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.933955Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.933137Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.925315Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 70,
+   "metadata": {
+    "_cell_guid": "3d471d07-7735-4aab-8b26-3f26e481dc49",
+    "_kg_hide-input": true,
+    "_uuid": "2e23467af7a2e85fcaa06b52b303daf2e5e44250",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.935971Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.935422Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.942647Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.941882Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.935671Z"
+    }
+   },
+   "outputs": [],
+   "source": "## bin the family size. \ndef family_group(size):\n    \"\"\"\n    This funciton groups(loner, small, large) family based on family size\n    \"\"\"\n    \n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 71,
+   "metadata": {
+    "_cell_guid": "82f3cf5a-7e8d-42c3-a06b-56e17e890358",
+    "_kg_hide-input": true,
+    "_uuid": "549239812f919f5348da08db4264632d2b21b587",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.944511Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.94417Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.95416Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.953395Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.944448Z"
+    }
+   },
+   "outputs": [],
+   "source": "## apply the family_group function in family_size\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "d827a2d9-8ca5-454a-8323-90c397b25ccf",
+    "_uuid": "3aa4ad0fac364f8f3c04e240841ee097baa3c871"
+   },
+   "source": "## is_alone"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 72,
+   "metadata": {
+    "_cell_guid": "298b28d6-75a7-4e49-b1c3-7755f1727327",
+    "_kg_hide-input": true,
+    "_uuid": "45315bb62f69e94e66109e7da06c6c5ade578398",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.956031Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.955569Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.964779Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.963853Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.955855Z"
+    }
+   },
+   "outputs": [],
+   "source": "train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "fee91907-4197-46c2-92c1-92474565e9a0",
+    "_uuid": "0a6032d2746a7cf75e2cc899615d72433572fd6d"
+   },
+   "source": "## ticket"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 73,
+   "metadata": {
+    "_cell_guid": "352c794d-728d-44de-9160-25da7abe0c06",
+    "_kg_hide-input": true,
+    "_uuid": "5b99e1f7d7757f11e6dd6dbc627f3bd6e2fbd874",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.966936Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.9664Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.97799Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.976969Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.966816Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.Ticket.value_counts().sample(10)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "dd50f2d503d4b951bee458793dde6e23f0e35dc9"
+   },
+   "source": "I have yet to figureout how to best manage ticket feature. So, any suggestion would be truly appreciated. For now, I will get rid off the ticket feature."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 74,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "d23d451982f0cbe44976c2eacafb726d816e9195",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.979613Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.979155Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.989456Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.988913Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.97941Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "60cb16dc-9bc3-4ff3-93b8-e3b3d4bcc0c8",
+    "_uuid": "800052abc32a56c5f5f875bb3652c02e93c6b0a8"
+   },
+   "source": "## calculated_fare"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 75,
+   "metadata": {
+    "_cell_guid": "adaa30fe-cb0f-4666-bf95-505f1dcce188",
+    "_kg_hide-input": true,
+    "_uuid": "9374a6357551a7551e71731d72f5ceb3144856df",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:15.991841Z",
+     "iopub.status.busy": "2021-06-26T16:35:15.991313Z",
+     "iopub.status.idle": "2021-06-26T16:35:15.999545Z",
+     "shell.execute_reply": "2021-06-26T16:35:15.998734Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:15.991562Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare/train.family_size\ntest['calculated_fare'] = test.Fare/test.family_size"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "157cec80a8138c7976b135f093fc52832b82d71e"
+   },
+   "source": "Some people have travelled in groups like family or friends. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger, therefore calculated fare will be much handy in this situation. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "60579ed1-9978-4d4a-aea0-79c75b6b1376",
+    "_uuid": "c0e1c25bc6a7717646a5d0d063acae220e496e9e"
+   },
+   "source": "## fare_group"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 76,
+   "metadata": {
+    "_cell_guid": "8c33b78c-14cb-4cc2-af0f-65079a741570",
+    "_kg_hide-input": true,
+    "_uuid": "35685a6ca28651eab389c4673c21da2ea5ba4187",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:16.001667Z",
+     "iopub.status.busy": "2021-06-26T16:35:16.001088Z",
+     "iopub.status.idle": "2021-06-26T16:35:16.012304Z",
+     "shell.execute_reply": "2021-06-26T16:35:16.011542Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:16.00135Z"
+    }
+   },
+   "outputs": [],
+   "source": "def fare_group(fare):\n    \"\"\"\n    This function creates a fare group based on the fare provided\n    \"\"\"\n    \n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "5f5072cf-2234-425e-b91d-9609971117a0",
+    "_uuid": "907614ee16efce8cbcc32b5535648688d23271eb"
+   },
+   "source": "Fare group was calculated based on <i>calculated_fare</i>. This can further help our cause. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "57a333f5c225ce65ec46a7e8b3c33d78fd70752e"
+   },
+   "source": "## PassengerId"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "b44cc5b5f6fd4d844b85f689f3a713599915bbce"
+   },
+   "source": "It seems like <i>PassengerId</i> column only works as an id in this dataset without any significant effect on the dataset. Let's drop it."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 77,
+   "metadata": {
+    "_uuid": "dadea67801cf5b56a882aa96bb874a4afa0e0bec",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:16.014434Z",
+     "iopub.status.busy": "2021-06-26T16:35:16.013951Z",
+     "iopub.status.idle": "2021-06-26T16:35:16.025524Z",
+     "shell.execute_reply": "2021-06-26T16:35:16.024631Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:16.014266Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "6a494c58-c1cf-44e9-be41-f404626ab299",
+    "_uuid": "704994b577f803ae51c5c6473a2d96f49bdd12ea"
+   },
+   "source": "## Creating dummy variables\n\nYou might be wondering what is a dummy variable? \n\nDummy variable is an important **prepocessing machine learning step**. Often times Categorical variables are an important features, which can be the difference between a good model and a great model. While working with a dataset, having meaningful value for example, \"male\" or \"female\" instead of 0's and 1's is more intuitive for us. However, machines do not understand the value of categorical values, for example, in this dataset we have gender male or female, algorithms do not accept categorical variables as input. In order to feed data in a machine learning model, we  "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 78,
+   "metadata": {
+    "_cell_guid": "9243ac8c-be44-46d0-a0ca-ee5f19b89bd4",
+    "_kg_hide-input": true,
+    "_uuid": "7b8db3930fb1bfb91db16686223dfc6d8e77744d",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:16.027132Z",
+     "iopub.status.busy": "2021-06-26T16:35:16.026701Z",
+     "iopub.status.idle": "2021-06-26T16:35:16.059319Z",
+     "shell.execute_reply": "2021-06-26T16:35:16.058745Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:16.027081Z"
+    }
+   },
+   "outputs": [],
+   "source": "\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "67dc98ce-bedc-456d-bdbb-9684bbd88d66",
+    "_uuid": "23586743d94d093f76f05a2fd3ca0ae75c0d663c"
+   },
+   "source": "## age"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "a519858b2df34c499bb53808a5a23592ba7af040"
+   },
+   "source": "As I promised before, we are going to use Random forest regressor in this section to predict the missing age values. Let's do it"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 79,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:16.061141Z",
+     "iopub.status.busy": "2021-06-26T16:35:16.060714Z",
+     "iopub.status.idle": "2021-06-26T16:35:16.084728Z",
+     "shell.execute_reply": "2021-06-26T16:35:16.083793Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:16.060961Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.head()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 80,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "9597c320c3db4db5e5c28980a28abaae7281bc61",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:16.086463Z",
+     "iopub.status.busy": "2021-06-26T16:35:16.086001Z",
+     "iopub.status.idle": "2021-06-26T16:35:16.096908Z",
+     "shell.execute_reply": "2021-06-26T16:35:16.095838Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:16.086235Z"
+    }
+   },
+   "outputs": [],
+   "source": "## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 81,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "91662e7b63c2361fdcf3215f130b3895154ad92d",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:16.098683Z",
+     "iopub.status.busy": "2021-06-26T16:35:16.098263Z",
+     "iopub.status.idle": "2021-06-26T16:35:22.704889Z",
+     "shell.execute_reply": "2021-06-26T16:35:22.704165Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:16.098504Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "8f4891f73fe40cdf20cbcdfce93bda7a4f5ccc5d"
+   },
+   "source": "Let's take a look at the histogram of the age column. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 82,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "8fc55e4670061d46dab3cc6585b3cc71eb996868",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:22.708567Z",
+     "iopub.status.busy": "2021-06-26T16:35:22.708283Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.194075Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.193419Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:22.708515Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Let's look at the his\nplt.subplots(figsize = (22,10),)\nsns.distplot(train.Age, bins = 100, kde = True, rug = False, norm_hist=False);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "97fcc2a4c7cdc7f998052aed543b86e113499580"
+   },
+   "source": "## age_group\nWe can create a new feature by grouping the \"Age\" column"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 83,
+   "metadata": {
+    "_cell_guid": "3140c968-6755-42ec-aa70-d30c0acede1e",
+    "_kg_hide-input": true,
+    "_uuid": "c3bd77bb4d9d5411aa696a605be127db181d2a67",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.196215Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.195696Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.219708Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.218664Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.195943Z"
+    }
+   },
+   "outputs": [],
+   "source": "## create bins for age\ndef age_group_fun(age):\n    \"\"\"\n    This function creates a bin for age\n    \"\"\"\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<div class=\"alert alert-danger\">\n<h1>Need to paraphrase this section</h1>\n<h2>Feature Selection</h2>\n<h3>Feature selection is an important part of machine learning models. There are many reasons why we use feature selection.</h3> \n<ul>\n    <li>Simple models are easier to interpret. People who acts according to model results have a better understanding of the model.</li>\n    <li>Shorter training times.</li>\n    <li>Enhanced generalisation by reducing overfitting. </li>\n    <li>Easier to implement by software developers> model production.</li>\n        <ul>\n            <li>As Data Scientists we need to remember no to creating models with too many variables since it might overwhelm production engineers.</li>\n    </ul>\n    <li>Reduced risk of data errors during model use</li>\n    <li>Data redundancy</li>\n</ul>\n</div>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "9de7bf55-edfb-42e0-a235-7fee883001d9",
+    "_uuid": "8eb07418adf26340ec68fa41401e68d08603f6d4"
+   },
+   "source": "# Part 6: Pre-Modeling Tasks\n## 6a. Separating dependent and independent variables\n<a id=\"dependent_independent\"></a>\n***\nBefore we apply any machine learning models, It is important to separate dependent and independent variables. Our dependent variable or target variable is something that we are trying to find, and our independent variable is the features we use to find the dependent variable. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. To specify them, we need to separate them from each other, and the code below does just that.\n\nP.S. In our test dataset, we do not have a dependent variable feature. We are to predict that using machine learning models. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 84,
+   "metadata": {
+    "_cell_guid": "dcb0934f-8e3f-40b6-859e-abf70b0b074e",
+    "_kg_hide-input": true,
+    "_uuid": "607db6be6dfacc7385e5adcc0feeee28c50c99c5",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.221875Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.221297Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.229845Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.228853Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.221578Z"
+    }
+   },
+   "outputs": [],
+   "source": "# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "042502ae-2714-43e2-9e33-6705b1aa781a",
+    "_uuid": "92001d23ce79265c0f7d2b3d6f67094feeec2ea7"
+   },
+   "source": "## 6b. Splitting the training data\n<a id=\"split_training_data\" ></a>\n***\nThere are multiple ways of splitting data. They are...\n* train_test_split.\n* cross_validation. \n\nWe have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two-thirds of the train data. Once we train our algorithm using 2/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, **X_train**, **X_test**, **y_train**, **y_test**.  \n* **X_train** and **y_train** first used to train the algorithm. \n* then, **X_test** is used in that trained algorithms to predict **outcomes. **\n* Once we get the **outcomes**, we compare it with **y_test**\n\nBy comparing the **outcome** of the model with **y_test**, we can determine whether our algorithms are performing well or not. As we compare we use confusion matrix to determine different aspects of model performance.\n\nP.S. When we use cross validation it is important to remember not to use **X_train, X_test, y_train and y_test**, rather we will use **X and y**. I will discuss more on that. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 85,
+   "metadata": {
+    "_cell_guid": "348a5be2-5f4f-4c98-93a3-7352b6060ef4",
+    "_kg_hide-input": true,
+    "_uuid": "41b70e57f8e03da9910c20af89a9fa4a2aaea85b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.231964Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.23135Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.240022Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.239414Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.231633Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 86,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.242734Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.24208Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.250654Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.249893Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.242373Z"
+    }
+   },
+   "outputs": [],
+   "source": "len(X_train)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 87,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.260997Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.260779Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.265643Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.264688Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.260954Z"
+    }
+   },
+   "outputs": [],
+   "source": "len(X_test)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "1f920690-2084-498c-a2fa-e618ad2228d8",
+    "_uuid": "75407683b262fb65fc4afdfca6084d4ddaebe9a9"
+   },
+   "source": "## 6c. Feature Scaling\n<a id=\"feature_scaling\" ></a>\n***\nFeature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit. For some machine learning models, it is not a problem. However, for many other ones, its quite a problem. Many machine learning algorithms uses euclidian distances to calculate the distance between two points, it is quite a problem. Let's again look at a the sample of the **train** dataset below."
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 88,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "d788baa4b88106afe5b30c769a6c85a1d67a5d6c",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.26761Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.267136Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.295264Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.294322Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.267383Z"
+    }
+   },
+   "outputs": [],
+   "source": "train.sample(5)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "3d213fdd45a46ea0cf060adc7d9af58a84a03e21"
+   },
+   "source": "Here **Age** and **Calculated_fare** is much higher in magnitude compared to others machine learning features. This can create problems as many machine learning models will get confused thinking **Age** and **Calculated_fare** have higher weight than other features. Therefore, we need to do feature scaling to get a better result. \nThere are multiple ways to do feature scaling. \n<ul>\n    <li><b>MinMaxScaler</b>-Scales the data using the max and min values so that it fits between 0 and 1.</li>\n    <li><b>StandardScaler</b>-Scales the data so that it has mean 0 and variance of 1.</li>\n    <li><b>RobustScaler</b>-Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.</b>\n </ul>\nI will discuss more on that in a different kernel. For now we will use <b>Standard Scaler</b> to feature scale our dataset. \n\nP.S. I am showing a sample of both before and after so that you can see how scaling changes the dataset. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "2bf3db75976f363c0e922b0b7843716f900e0fd9"
+   },
+   "source": "<h3><font color=\"$5831bc\" face=\"Comic Sans MS\">Before Scaling</font></h3>"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 89,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "c4011a767b1d846f2866b4573d1d6d116afe8427",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.297022Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.296548Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.319251Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.318338Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.296792Z"
+    }
+   },
+   "outputs": [],
+   "source": "headers = X_train.columns \n\nX_train.head()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 90,
+   "metadata": {
+    "_cell_guid": "5c89c54b-7f5a-4e31-9e8f-58726cef5eab",
+    "_kg_hide-input": true,
+    "_uuid": "182b849ba7f2b311e919cdbf83970b97736e9d98",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.320979Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.320476Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.331478Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.33067Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.320738Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nst_scale = StandardScaler()\n\n## transforming \"train_x\"\nX_train = st_scale.fit_transform(X_train)\n## transforming \"test_x\"\nX_test = st_scale.transform(X_test)\n\n## transforming \"The testset\"\n#test = st_scale.transform(test)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "d425ca579370db88e39cdd1811ba3df2b257b36c"
+   },
+   "source": "<h3><font color=\"#5831bc\" face=\"Comic Sans MS\">After Scaling</font></h3>"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 91,
+   "metadata": {
+    "_kg_hide-input": true,
+    "_uuid": "fc6f031833ac9e2734aa7b3a2373b667679c6b2f",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.333531Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.333111Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.359161Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.358554Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.333347Z"
+    }
+   },
+   "outputs": [],
+   "source": "pd.DataFrame(X_train, columns=headers).head()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "You can see how the features have transformed above."
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "0e03e40b-789a-40a0-a095-135f3d1c8f23",
+    "_uuid": "99e108b83ba88738e42480b053371d60d89151cf"
+   },
+   "source": "# Part 7: Modeling the Data\n<a id=\"modelingthedata\"></a>\n***\nIn the previous versions of this kernel, I thought about explaining each model before applying it. However, this process makes this kernel too lengthy to sit and read at one go. Therefore I have decided to break this kernel down and explain each algorithm in a different kernel and add the links here. If you like to review logistic regression, please click [here](https://www.kaggle.com/masumrumi/logistic-regression-with-titanic-dataset). "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 92,
+   "metadata": {
+    "_cell_guid": "0c8b0c41-6738-4689-85b0-b83a16e46ab9",
+    "_uuid": "09140be1a71e37b441a16951a82747462b767e6e",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.361067Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.360637Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.383762Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.383049Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.360889Z"
+    }
+   },
+   "outputs": [],
+   "source": "# import LogisticRegression model in python. \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression(solver='liblinear',\n                            penalty= 'l1',random_state = 42\n                                \n                            )\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(X_train,y_train)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"X_test\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(X_test)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. "
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<h1><font color=\"#5831bc\" face=\"Comic Sans MS\">Evaluating a classification model</font></h1>\n\nThere are multiple ways to evaluate a classification model. \n\n* Confusion Matrix. \n* ROC Curve\n* AUC Curve. \n\n\n## Confusion Matrix\n<b>Confusion matrix</b>, a table that <b>describes the performance of a classification model</b>. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary/multiple outcome classes by comparing actual and predicted cases. For example, in terms of this dataset, our model is a binary one and we are trying to classify whether the passenger survived or not survived. we have fit the model using **X_train** and **y_train** and predicted the outcome of **X_test** in the variable **y_pred**. So, now we will use a confusion matrix to compare between **y_test** and **y_pred**. Let's do the confusion matrix. \n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 93,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.385843Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.385341Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.399434Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.398674Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.385606Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import classification_report, confusion_matrix\n# printing confision matrix\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "Our **y_test** has a total of 294 data points; part of the original train set that we splitted in order to evaluate our model. Each number here represents certain details about our model. If we were to think about this interms of column and raw, we could see that...\n\n* the first column is of data points that the machine predicted as not-survived.\n* the second column is of the statistics that the model predicted as survievd.\n* In terms of raws, the first raw indexed as \"Not-survived\" means that the value in that raw are actual statistics of not survived once. \n* and the \"Survived\" indexed raw are values that actually survived.\n\nNow you can see that the predicted not-survived and predicted survived sort of overlap with actual survived and actual not-survived. After all it is a matrix and we have some terminologies to call these statistics more specifically. Let's see what they are"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<ul style=\"list-style-type:square;\">\n    <li><b>True Positive(TP)</b>: values that the model predicted as yes(survived) and is actually yes(survived).</li>\n    <li><b>True Negative(TN)</b>: values that model predicted as no(not-survived) and is actually no(not-survived)</li>\n    <li><b>False Positive(or Type I error)</b>: values that model predicted as yes(survived) but actually no(not-survived)</li>\n    <li><b>False Negative(or Type II error)</b>: values that model predicted as no(not-survived) but actually yes(survived)</li>\n</ul>\n\nFor this dataset, whenever the model is predicting something as yes, it means the model is predicting that the passenger survived and for cases when the model predicting no; it means the passenger did not survive. Let's determine the value of all these terminologies above.\n<ul style=\"list-style-type:square;\">\n    <li><b>True Positive(TP):87</b></li>\n    <li><b>True Negative(TN):149</b></li>\n    <li><b>False Positive(FP):28</b></li>\n    <li><b>False Negative(FN):30</b></li>\n</ul>\nFrom these four terminologies, we can compute many other rates that are used to evaluate a binary classifier. \n\n\n#### Accuracy: \n** Accuracy is the measure of how often the model is correct.** \n* (TP + TN)/total = (87+149)/294 = .8027\n\nWe can also calculate accuracy score using scikit learn. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 94,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.400652Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.400403Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.408635Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.40776Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.400604Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "**Misclassification Rate:** Misclassification Rate is the measure of how often the model is wrong**\n* Misclassification Rate and Accuracy are opposite of each other.\n* Missclassification is equivalent to 1 minus Accuracy. \n* Misclassification Rate is also known as \"Error Rate\".\n\n> (FP + FN)/Total = (28+30)/294 = 0.19\n\n**True Positive Rate/Recall/Sensitivity:** How often the model predicts yes(survived) when it's actually yes(survived)?\n> TP/(TP+FN) = 87/(87+30) = 0.7435897435897436\n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 95,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.410491Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.410085Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.418315Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.417549Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.410444Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import recall_score\nrecall_score(y_test, y_pred)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "\n**False Positive Rate:** How often the model predicts yes(survived) when it's actually no(not-survived)?\n> FP/(FP+TN) = 28/(28+149) = 0.15819209039548024\n\n**True Negative Rate/Specificity:** How often the model predicts no(not-survived) when it's actually no(not-survived)?\n* True Negative Rate is equivalent to 1 minus False Positive Rate.\n\n> TN/(TN+FP) = 149/(149+28) = 0.8418079096045198\n\n**Precision:** How often is it correct when the model predicts yes. \n> TP/(TP+FP) = 87/(87+28) = 0.7565217391304347"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 96,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.4204Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.419791Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.429679Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.42864Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.420242Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import precision_score\nprecision_score(y_test, y_pred)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 97,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.431682Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.431234Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.44225Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.441202Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.43147Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import classification_report, balanced_accuracy_score\nprint(classification_report(y_test, y_pred))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "we have our confusion matrix. How about we give it a little more character. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 98,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.444153Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.443714Z",
+     "iopub.status.idle": "2021-06-26T16:35:23.873374Z",
+     "shell.execute_reply": "2021-06-26T16:35:23.869521Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.444104Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\nclass_names = np.array(['not_survived','survived'])\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "e13731cbb9d9040cf6e4088e8660eca66037a8cc"
+   },
+   "source": "<h1>AUC & ROC Curve</h1>"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 99,
+   "metadata": {
+    "_uuid": "1e71bc7c685b757b6920076527780674d6f619bc",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:23.877891Z",
+     "iopub.status.busy": "2021-06-26T16:35:23.875713Z",
+     "iopub.status.idle": "2021-06-26T16:35:24.505751Z",
+     "shell.execute_reply": "2021-06-26T16:35:24.501314Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:23.87783Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 100,
+   "metadata": {
+    "_uuid": "22f15e384372a1ece2f28cd9eced0c703a79598f",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:24.50731Z",
+     "iopub.status.busy": "2021-06-26T16:35:24.506981Z",
+     "iopub.status.idle": "2021-06-26T16:35:24.8481Z",
+     "shell.execute_reply": "2021-06-26T16:35:24.846974Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:24.507251Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import precision_recall_curve\n\ny_score = logreg.decision_function(X_test)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "e46b6d4bcb0ef70c06535b58bbe84c8a301ead91"
+   },
+   "source": "## Using Cross-validation:\nPros: \n* Helps reduce variance. \n* Expends models predictability. \n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 101,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:24.855506Z",
+     "iopub.status.busy": "2021-06-26T16:35:24.853028Z",
+     "iopub.status.idle": "2021-06-26T16:35:24.862513Z",
+     "shell.execute_reply": "2021-06-26T16:35:24.861421Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:24.853368Z"
+    }
+   },
+   "outputs": [],
+   "source": "sc = st_scale"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 102,
+   "metadata": {
+    "_uuid": "17791284c3e88236de2daa112422cde8ddcb0641",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:24.868704Z",
+     "iopub.status.busy": "2021-06-26T16:35:24.86826Z",
+     "iopub.status.idle": "2021-06-26T16:35:25.014634Z",
+     "shell.execute_reply": "2021-06-26T16:35:25.013771Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:24.86853Z"
+    },
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": "## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)\naccuracies = cross_val_score(LogisticRegression(solver='liblinear'), X,y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "d1f2930c-43ae-4c15-87f7-ccc9214ee0e1",
+    "_uuid": "b8020ecfe44bebdf7a2b95ec49393e8baac6bcf9"
+   },
+   "source": "## Grid Search on Logistic Regression\n* What is grid search? \n* What are the pros and cons?\n\n**Gridsearch** is a simple concept but effective technique in Machine Learning. The word **GridSearch** stands for the fact that we are searching for optimal parameter/parameters over a \"grid.\" These optimal parameters are also known as **Hyperparameters**. **The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model.**. For example, when we choose to use linear regression, we may decide to add a penalty to the loss function such as Ridge or Lasso. These penalties require specific alpha (the strength of the regularization technique) to set beforehand. The higher the value of alpha, the more penalty is being added. GridSearch finds the optimal value of alpha among a range of values provided by us, and then we go on and use that optimal value to fit the model and get sweet results. It is essential to understand those model parameters are different from models outcomes, for example, **coefficients** or model evaluation metrics such as **accuracy score** or **mean squared error** are model outcomes and different than hyperparameters.\n\n#### This part of the kernel is a working progress. Please check back again for future updates.####"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 103,
+   "metadata": {
+    "_cell_guid": "0620523c-b33b-4302-8a1c-4b6759ffa5fa",
+    "_uuid": "36a379a00a31dd161be1723f65490990294fe13d",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:25.021234Z",
+     "iopub.status.busy": "2021-06-26T16:35:25.018883Z",
+     "iopub.status.idle": "2021-06-26T16:35:40.193433Z",
+     "shell.execute_reply": "2021-06-26T16:35:40.192566Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:25.021181Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\n## remember effective alpha scores are 0<alpha<infinity \nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n## Choose a cross validation strategy. \ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 104,
+   "metadata": {
+    "_cell_guid": "1fa35072-87c4-4f47-86ab-dda03d4b7b15",
+    "_uuid": "4c6650e39550527b271ddf733dcfe5221bcd5c98",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:40.195216Z",
+     "iopub.status.busy": "2021-06-26T16:35:40.194925Z",
+     "iopub.status.idle": "2021-06-26T16:35:40.201259Z",
+     "shell.execute_reply": "2021-06-26T16:35:40.200225Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:40.19517Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "dcd2ad782c168abb5cfb5a3d148814e53cb2119c"
+   },
+   "source": "\n#### Using the best parameters from the grid-search. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 105,
+   "metadata": {
+    "_uuid": "ba53f6b3610821dc820936dde7b7803a54d20f5a",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:40.204086Z",
+     "iopub.status.busy": "2021-06-26T16:35:40.203576Z",
+     "iopub.status.idle": "2021-06-26T16:35:40.214041Z",
+     "shell.execute_reply": "2021-06-26T16:35:40.212929Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:40.20393Z"
+    }
+   },
+   "outputs": [],
+   "source": "### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": " #### This part of the kernel is a working progress. Please check back again for future updates.####\n \n Resources: \n * [Confusion Matrix](https://www.youtube.com/watch?v=8Oog7TXHvFY)\n### Under-fitting & Over-fitting: \nSo, we have our first model and its score. But, how do we make sure that our model is performing well. Our model may be overfitting or underfitting. In fact, for those of you don't know what overfitting and underfitting is, Let's find out.\n\n![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/fittings.jpg)\n\nAs you see in the chart above. **Underfitting** is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. On the other hand, **Overfitting** is when the model performs too well on the training data but does poorly in the validation set or test sets.  This situation is also known as having less bias but more variation and perform poorly as well. Ideally, we want to configure a model that performs well not only in the training data but also in the test data. This is where **bias-variance tradeoff** comes in. When we have a model that overfits, meaning less biased and more of variance, we introduce some bias in exchange of having much less variance. One particular tactic for this task is regularization models (Ridge, Lasso, Elastic Net).  These models are built to deal with the bias-variance tradeoff. This [kernel](https://www.kaggle.com/dansbecker/underfitting-and-overfitting) explains this topic well. Also, the following chart gives us a mental picture of where we want our models to be. \n![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n\nIdeally, we want to pick a sweet spot where the model performs well in training set, validation set, and test set. As the model gets complex, bias decreases, variance increases. However, the most critical part is the error rates. We want our models to be at the bottom of that **U** shape where the error rate is the least. That sweet spot is also known as **Optimum Model Complexity(OMC).**\n\nNow that we know what we want in terms of under-fitting and over-fitting, let's talk about how to combat them. \n\nHow to combat over-fitting?\n<ul>\n    <li>Simplify the model by using less parameters.</li>\n    <li>Simplify the model by changing the hyperparameters.</li>\n    <li>Introducing regularization models. </li>\n    <li>Use more training data. </li>\n    <li>Gatter more data ( and gather better quality data). </li>\n    </ul>\n #### This part of the kernel is a working progress. Please check back again for future updates.####"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "8046e4d9-12db-4b1c-9e9e-31fd5e6543f2",
+    "_uuid": "26b0ea9184b2c37eabe4e705b1c840956ecc1e10"
+   },
+   "source": "## 7b. K-Nearest Neighbor classifier(KNN)\n<a id=\"knn\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 106,
+   "metadata": {
+    "_uuid": "953bc2c18b5fd93bcd51a42cc04a0539d86d5bac",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:40.216328Z",
+     "iopub.status.busy": "2021-06-26T16:35:40.215853Z",
+     "iopub.status.idle": "2021-06-26T16:35:40.416985Z",
+     "shell.execute_reply": "2021-06-26T16:35:40.416038Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:40.216141Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\n## calling on the model oject. \nknn = KNeighborsClassifier(metric='minkowski', p=2)\n## knn classifier works by doing euclidian distance \n\n\n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3)))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "6aa75e53129898ccd714370dc55c0ed2830e72f4"
+   },
+   "source": "#### Manually find the best possible k value for KNN"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 107,
+   "metadata": {
+    "_uuid": "9c0f44165e08f63ae5436180c5a7182e6db5c63f",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:40.418857Z",
+     "iopub.status.busy": "2021-06-26T16:35:40.418419Z",
+     "iopub.status.idle": "2021-06-26T16:35:46.541601Z",
+     "shell.execute_reply": "2021-06-26T16:35:46.540815Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:40.418687Z"
+    }
+   },
+   "outputs": [],
+   "source": "## Search for an optimal value of k for KNN.\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))\n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 108,
+   "metadata": {
+    "_uuid": "e123680b431ba99d399fa8205c32bcfdc7cabd81",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:46.543234Z",
+     "iopub.status.busy": "2021-06-26T16:35:46.542789Z",
+     "iopub.status.idle": "2021-06-26T16:35:46.685143Z",
+     "shell.execute_reply": "2021-06-26T16:35:46.684141Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:46.543184Z"
+    }
+   },
+   "outputs": [],
+   "source": "from matplotlib import pyplot as plt\nplt.plot(k_range, k_scores)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "77b5b6e3b7bc925e0b008cd6d531175e5cc44040"
+   },
+   "source": "### Grid search on KNN classifier"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 109,
+   "metadata": {
+    "_uuid": "507e2a7cdb28a47be45ed247f1343c123a6b592b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:46.687026Z",
+     "iopub.status.busy": "2021-06-26T16:35:46.686671Z",
+     "iopub.status.idle": "2021-06-26T16:35:55.465245Z",
+     "shell.execute_reply": "2021-06-26T16:35:55.464452Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:46.686956Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.model_selection import GridSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n## Fitting the model. \ngrid.fit(X,y)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 110,
+   "metadata": {
+    "_uuid": "c710770daa6cf327dcc28e18b3ed180fabecd49b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:55.466929Z",
+     "iopub.status.busy": "2021-06-26T16:35:55.466654Z",
+     "iopub.status.idle": "2021-06-26T16:35:55.475348Z",
+     "shell.execute_reply": "2021-06-26T16:35:55.474575Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:55.466883Z"
+    }
+   },
+   "outputs": [],
+   "source": "print(grid.best_score_)\nprint(grid.best_params_)\nprint(grid.best_estimator_)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "bb06144264d3127c92169aed7c29c2f66ad0ffc4"
+   },
+   "source": "#### Using best estimator from grid search using KNN. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 111,
+   "metadata": {
+    "_uuid": "dd1fbf223c4ec9db65dde4924e2827e46029da1a",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:55.477181Z",
+     "iopub.status.busy": "2021-06-26T16:35:55.476629Z",
+     "iopub.status.idle": "2021-06-26T16:35:55.555736Z",
+     "shell.execute_reply": "2021-06-26T16:35:55.554788Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:55.476983Z"
+    }
+   },
+   "outputs": [],
+   "source": "### Using the best parameters from the grid-search.\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_uuid": "c2ebec8b83f23e3e27d23bdd707852269edd4d24"
+   },
+   "source": "#### Using RandomizedSearchCV\nRandomized search is a close cousin of grid search. It doesn't  always provide the best result but its fast. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 112,
+   "metadata": {
+    "_uuid": "e159b267a57d7519fc0ee8b3d1e95b841d3daf60",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:35:55.557501Z",
+     "iopub.status.busy": "2021-06-26T16:35:55.557097Z",
+     "iopub.status.idle": "2021-06-26T16:36:02.332003Z",
+     "shell.execute_reply": "2021-06-26T16:36:02.331364Z",
+     "shell.execute_reply.started": "2021-06-26T16:35:55.557338Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.model_selection import RandomizedSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \n## for RandomizedSearchCV, \ngrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n## Fitting the model. \ngrid.fit(X,y)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 113,
+   "metadata": {
+    "_uuid": "c58492525dd18659ef9f9c774ee7601a55e96f36",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:36:02.333632Z",
+     "iopub.status.busy": "2021-06-26T16:36:02.333341Z",
+     "iopub.status.idle": "2021-06-26T16:36:02.340211Z",
+     "shell.execute_reply": "2021-06-26T16:36:02.338113Z",
+     "shell.execute_reply.started": "2021-06-26T16:36:02.333572Z"
+    }
+   },
+   "outputs": [],
+   "source": "print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 114,
+   "metadata": {
+    "_uuid": "6fb31588585d50de773ba0db6c378363841a5313",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:36:02.343117Z",
+     "iopub.status.busy": "2021-06-26T16:36:02.34256Z",
+     "iopub.status.idle": "2021-06-26T16:36:02.420683Z",
+     "shell.execute_reply": "2021-06-26T16:36:02.419712Z",
+     "shell.execute_reply.started": "2021-06-26T16:36:02.342922Z"
+    }
+   },
+   "outputs": [],
+   "source": "### Using the best parameters from the grid-search.\nknn_ran_grid = grid.best_estimator_\nknn_ran_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "be0143d6-a7ea-4752-9520-c692f4c3eb8a",
+    "_uuid": "21e91edd53b6587d5a05036045bc5eea52f056da"
+   },
+   "source": "## Gaussian Naive Bayes\n<a id=\"gaussian_naive\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 115,
+   "metadata": {
+    "_uuid": "8b2435030dbef1303bfc2864d227f5918f359330",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:36:02.422487Z",
+     "iopub.status.busy": "2021-06-26T16:36:02.421997Z",
+     "iopub.status.idle": "2021-06-26T16:36:02.433216Z",
+     "shell.execute_reply": "2021-06-26T16:36:02.43234Z",
+     "shell.execute_reply.started": "2021-06-26T16:36:02.422237Z"
+    }
+   },
+   "outputs": [],
+   "source": "# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "c3e025c5-50f3-4fa1-a385-438d6665199b",
+    "_uuid": "2a1558118d9e673395246acc4f3c0edb1b1895f0"
+   },
+   "source": "## Support Vector Machines(SVM)\n<a id=\"svm\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 116,
+   "metadata": {
+    "_uuid": "56895672215b0b6365c6aaa10e446216ef635f53",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:36:02.435838Z",
+     "iopub.status.busy": "2021-06-26T16:36:02.435282Z",
+     "iopub.status.idle": "2021-06-26T16:37:25.882123Z",
+     "shell.execute_reply": "2021-06-26T16:37:25.881483Z",
+     "shell.execute_reply.started": "2021-06-26T16:36:02.435553Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 117,
+   "metadata": {
+    "_uuid": "4108264ea5d18e3d3fa38a30584a032c734d6d49",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:25.8839Z",
+     "iopub.status.busy": "2021-06-26T16:37:25.883609Z",
+     "iopub.status.idle": "2021-06-26T16:37:25.890029Z",
+     "shell.execute_reply": "2021-06-26T16:37:25.889244Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:25.883852Z"
+    }
+   },
+   "outputs": [],
+   "source": "print(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 118,
+   "metadata": {
+    "_uuid": "db18a3b5475f03b21a039e31e4962c43f7caffdc",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:25.892123Z",
+     "iopub.status.busy": "2021-06-26T16:37:25.891542Z",
+     "iopub.status.idle": "2021-06-26T16:37:25.934216Z",
+     "shell.execute_reply": "2021-06-26T16:37:25.933352Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:25.892073Z"
+    }
+   },
+   "outputs": [],
+   "source": "# using the best found hyper paremeters to get the score. \nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "## Decision Tree Classifier\n\nDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. If you would like to get a detailed understanding of Decision tree classifier, please take a look at [this](https://www.kaggle.com/masumrumi/decision-tree-with-titanic-dataset) kernel. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 119,
+   "metadata": {
+    "_cell_guid": "38c90de9-d2e9-4341-a378-a854762d8be2",
+    "_uuid": "18efb62b713591d1512010536ff10d9f6a91ec11",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:25.936111Z",
+     "iopub.status.busy": "2021-06-26T16:37:25.935654Z",
+     "iopub.status.idle": "2021-06-26T16:37:57.983942Z",
+     "shell.execute_reply": "2021-06-26T16:37:57.983035Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:25.935918Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 120,
+   "metadata": {
+    "_cell_guid": "b2222e4e-f5f2-4601-b95f-506d7811610a",
+    "_uuid": "b0fb5055e6b4a7fb69ef44f669c4df693ce46212",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:57.988346Z",
+     "iopub.status.busy": "2021-06-26T16:37:57.988045Z",
+     "iopub.status.idle": "2021-06-26T16:37:57.994617Z",
+     "shell.execute_reply": "2021-06-26T16:37:57.993662Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:57.988287Z"
+    },
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": "print( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 121,
+   "metadata": {
+    "_cell_guid": "d731079a-31b4-429a-8445-48597bb2639d",
+    "_uuid": "76c26437d374442826ef140574c5c4880ae1e853",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:57.996876Z",
+     "iopub.status.busy": "2021-06-26T16:37:57.996238Z",
+     "iopub.status.idle": "2021-06-26T16:37:58.010892Z",
+     "shell.execute_reply": "2021-06-26T16:37:58.010194Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:57.996695Z"
+    }
+   },
+   "outputs": [],
+   "source": "dectree_grid = grid.best_estimator_\n## using the best found hyper paremeters to get the score. \ndectree_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": " <h4> Let's look at the feature importance from decision tree grid.</h4>"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 122,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:58.013756Z",
+     "iopub.status.busy": "2021-06-26T16:37:58.01221Z",
+     "iopub.status.idle": "2021-06-26T16:37:58.034194Z",
+     "shell.execute_reply": "2021-06-26T16:37:58.033436Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:58.013683Z"
+    }
+   },
+   "outputs": [],
+   "source": "## feature importance\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "These are the top 10 features determined by **Decision Tree** helped classifing the fates of many passenger on Titanic on that night."
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "## 7f. Random Forest Classifier\n<a id=\"random_forest\"></a>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "I admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest(RF). RF is an ensemble method (combination of many decision trees) which is where the \"forest\" part comes in. One crucial details about Random Forest is that while using a forest of decision trees, RF model <b>takes random subsets of the original dataset(bootstrapped)</b> and <b>random subsets of the variables(features/columns)</b>. Using this method, the RF model creates 100's-1000's(the amount can be menually determined) of a wide variety of decision trees. This variety makes the RF model more effective and accurate. We then run each test data point through all of these 100's to 1000's of decision trees or the RF model and take a vote on the output. \n\n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 123,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:37:58.040453Z",
+     "iopub.status.busy": "2021-06-26T16:37:58.038063Z",
+     "iopub.status.idle": "2021-06-26T16:39:53.557817Z",
+     "shell.execute_reply": "2021-06-26T16:39:53.556973Z",
+     "shell.execute_reply.started": "2021-06-26T16:37:58.040398Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 124,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:39:53.559492Z",
+     "iopub.status.busy": "2021-06-26T16:39:53.559192Z",
+     "iopub.status.idle": "2021-06-26T16:39:53.567897Z",
+     "shell.execute_reply": "2021-06-26T16:39:53.56675Z",
+     "shell.execute_reply.started": "2021-06-26T16:39:53.559434Z"
+    }
+   },
+   "outputs": [],
+   "source": "print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 125,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:39:53.570209Z",
+     "iopub.status.busy": "2021-06-26T16:39:53.56951Z",
+     "iopub.status.idle": "2021-06-26T16:39:53.600458Z",
+     "shell.execute_reply": "2021-06-26T16:39:53.599531Z",
+     "shell.execute_reply.started": "2021-06-26T16:39:53.569928Z"
+    }
+   },
+   "outputs": [],
+   "source": "rf_grid = grid.best_estimator_\nrf_grid.score(X,y)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 126,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:39:53.602628Z",
+     "iopub.status.busy": "2021-06-26T16:39:53.602028Z",
+     "iopub.status.idle": "2021-06-26T16:39:53.613347Z",
+     "shell.execute_reply": "2021-06-26T16:39:53.612229Z",
+     "shell.execute_reply.started": "2021-06-26T16:39:53.602297Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.metrics import classification_report\n# Print classification report for y_test\nprint(classification_report(y_test, y_pred, labels=rf_grid.classes_))"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "## Feature Importance"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 127,
+   "metadata": {
+    "_kg_hide-input": true,
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:39:53.615537Z",
+     "iopub.status.busy": "2021-06-26T16:39:53.614947Z",
+     "iopub.status.idle": "2021-06-26T16:39:53.637392Z",
+     "shell.execute_reply": "2021-06-26T16:39:53.63647Z",
+     "shell.execute_reply.started": "2021-06-26T16:39:53.615192Z"
+    }
+   },
+   "outputs": [],
+   "source": "## feature importance\nfeature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<h3>Why Random Forest?(Pros and Cons)</h3>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "9c4c43f6-42c4-4cd3-a038-3f0c37f3c767",
+    "_uuid": "aba2679da04529faf9f9175ab20a66ee71217f92"
+   },
+   "source": "***\n<h2>Introducing Ensemble Learning</h2>\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. \n\nThere are two types of ensemple learnings. \n\n**Bagging/Averaging Methods**\n> In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\n**Boosting Methods**\n> The other family of ensemble methods are boosting methods, where base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\n<h4 align=\"right\">Source:GA</h4>\n\nResource: <a href=\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\">Ensemble methods: bagging, boosting and stacking</a>\n***\n## 7g. Bagging Classifier\n<a id=\"bagging\"></a>\n***"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">Bagging Classifier</a>(Bootstrap Aggregating) is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Let's do a quick review:\n* Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. Here is an image to show how bootstrapped dataset works. \n<img src=\"https://uc-r.github.io/public/images/analytics/bootstrap/bootstrap.png\" width=\"600\">\n<h4 align=\"center\">Resampling from original dataset to bootstrapped datasets</h4>\n<h4 align=\"right\">Source: https://uc-r.github.io</h4>\n\n\n* After running a learning algorithm on each one of the bootstrapped datasets, all models are combined by taking their average. the test data/new data then go through this averaged classifier/combined classifier and predict the output. \n\nHere is an image to make it clear on how bagging works, \n<img src=\"https://prachimjoshi.files.wordpress.com/2015/07/screen_shot_2010-12-03_at_5-46-21_pm.png\" width=\"600\">\n<h4 align=\"right\">Source: https://prachimjoshi.files.wordpress.com</h4>\nPlease check out [this](https://www.kaggle.com/masumrumi/bagging-with-titanic-dataset) kernel if you want to find out more about bagging classifier. "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 128,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:39:53.639198Z",
+     "iopub.status.busy": "2021-06-26T16:39:53.63871Z",
+     "iopub.status.idle": "2021-06-26T16:40:17.162923Z",
+     "shell.execute_reply": "2021-06-26T16:40:17.162277Z",
+     "shell.execute_reply.started": "2021-06-26T16:39:53.638945Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 129,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:40:17.164621Z",
+     "iopub.status.busy": "2021-06-26T16:40:17.164322Z",
+     "iopub.status.idle": "2021-06-26T16:40:17.172911Z",
+     "shell.execute_reply": "2021-06-26T16:40:17.172302Z",
+     "shell.execute_reply.started": "2021-06-26T16:40:17.164559Z"
+    }
+   },
+   "outputs": [],
+   "source": "print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 130,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:40:17.174968Z",
+     "iopub.status.busy": "2021-06-26T16:40:17.174466Z",
+     "iopub.status.idle": "2021-06-26T16:40:17.226122Z",
+     "shell.execute_reply": "2021-06-26T16:40:17.225161Z",
+     "shell.execute_reply.started": "2021-06-26T16:40:17.174765Z"
+    }
+   },
+   "outputs": [],
+   "source": "bagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<h3>Why use Bagging? (Pros and cons)</h3>\nBagging works best with strong and complex models(for example, fully developed decision trees). However, don't let that fool you to thinking that similar to a decision tree, bagging also overfits the model. Instead, bagging reduces overfitting since a lot of the sample training data are repeated and used to create base estimators. With a lot of equally likely training data, bagging is not very susceptible to overfitting with noisy data, therefore reduces variance. However, the downside is that this leads to an increase in bias."
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<h4>Random Forest VS. Bagging Classifier</h4>\n\nIf some of you are like me, you may find Random Forest to be similar to Bagging Classifier. However, there is a fundamental difference between these two which is **Random Forests ability to pick subsets of features in each node.** I will elaborate on this in a future update."
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "## 7h. AdaBoost Classifier\n<a id=\"AdaBoost\"></a>\n***\nAdaBoost is another <b>ensemble model</b> and is quite different than Bagging. Let's point out the core concepts. \n> AdaBoost combines a lot of \"weak learners\"(they are also called stump; a tree with only one node and two leaves) to make classifications.\n\n> This base model fitting is an iterative process where each stump is chained one after the other; <b>It cannot run in parallel.</b>\n\n> <b>Some stumps get more say in the final classifications than others.</b> The models use weights that are assigned to each data point/raw indicating their \"importance.\" Samples with higher weight have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. \n\n> <b>Each stump is made by talking the previous stump's mistakes into account.</b> After each iteration weights gets re-calculated in order to take the errors/misclassifications from the last stump into consideration. \n\n> The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. \n\nTo illustrate what we have talked about so far let's look at the following visualization. \n\n<img src=\"https://cdn-images-1.medium.com/max/1600/0*paPv7vXuq4eBHZY7.png\">\n<h5 align=\"right\"> Source: Diogo(Medium)</h5>\n\n\n\n\nLet's dive into each one of the nitty-gritty stuff about AdaBoost:\n***\n> <b>First</b>, we determine the best feature to split the dataset using Gini index(basics from decision tree). The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain(the lower the Gini index is, the better unmixed the label is, therefore, better split).\n***\n> <b>Secondly</b>, we need to determine how much say a stump will have in the final classification and how we can calculate that.\n* We learn how much say a stump has in the final classification by calculating how well it classified the samples (aka calculate the total error of the weight).\n* The <b>Total Error</b> for a stump is the sum of the weights associated with the incorrectly classified samples. For example, lets say, we start a stump with 10 datasets. The first stump will uniformly distribute an weight amoung all the datapoints. Which means each data point will have 1/10 weight. Let's say once the weight is distributed we run the model and find 2 incorrect predicitons. In order to calculate the total erorr we add up all the misclassified weights. Here we get 1/10 + 1/10 = 2/10 or 1/5. This is our total error. We can also think about it\n\n\n$$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n\n\n* Since the weight is uniformly distributed(all add up to 1) among all data points, the total error will always be between 0(perfect stump) and 1(horrible stump).\n* We use the total error to determine the amount of say a stump has in the final classification using the following formula\n \n\n$$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n\n\nWhere $\\epsilon_t$ is the misclassification rate for the current classifier:\n\n\n$$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n\n\nHere...\n* $\\alpha_t$ = Amount of Say\n* $\\epsilon_t$ = Total error\n\n\n\nWe can draw a graph to determine the amount of say using the value of total error(0 to 1)\n\n<img src=\"http://chrisjmccormick.files.wordpress.com/2013/12/adaboost_alphacurve.png\">\n<h5 align=\"right\"> Source: Chris McCormick</h5>\n\n* The blue line tells us the amount of say for <b>Total Error(Error rate)</b> between 0 and 1. \n* When the stump does a reasonably good job, and the <b>total error</b> is minimal, then the <b>amount of say(Alpha)</b> is relatively large, and the alpha value is positive. \n* When the stump does an average job(similar to a coin flip/the ratio of getting correct and incorrect ~50%/50%), then the <b>total error</b> is ~0.5. In this case the <b>amount of say</b> is <b>0</b>.\n* When the error rate is high let's say close to 1, then the <b>amount of say</b> will be negative, which means if the stump outputs a value as \"survived\" the included weight will turn that value into \"not survived.\"\n\nP.S. If the <b>Total Error</b> is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. \n \n ***\n> <b>Third</b>, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. The pseducode for calculating the new sample weight is as follows. \n\n\n$$ New Sample Weight = Sample Weight + e^{\\alpha_t}$$\n\nHere the $\\alpha_t(AmountOfSay)$ can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples; hinting the next stump to put more emphasize on those. Inversely, we want to decrease the sample weight of the correctly classified samples; hinting the next stump to put less emphasize on those. \n\nThe following equation help us to do this calculation. \n\n$$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n\nHere, \n* $D_{t+1}(i)$ = New Sample Weight. \n* $D_t(i)$ = Current Sample weight.\n* $\\alpha_t$ = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and \n* $y_i h_t(x_i)$ = place holder for 1 if stump correctly classified, -1 if misclassified. \n\nFinally, we put together the combined classifier, which is \n\n$$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$ \n\nHere, \n\n$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n\n$T$ is the set of \"weak learners\"\n\n$\\alpha_t$ is the contribution weight for weak learner $t$\n\n$h_t(X)$ is the prediction of weak learner $t$\n\nand $y$ is binary **with values -1 and 1**\n\n\nP.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.\n\n<h3>Resources:</h3>\n<ul>\n    <li><a href=\"https://www.youtube.com/watch?v=LsK-xG1cLYA\">Statquest</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=-DUxtdeCiB4\">Principles of Machine Learning | AdaBoost(Video)</a></li>\n</ul>"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 131,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:40:17.227822Z",
+     "iopub.status.busy": "2021-06-26T16:40:17.227396Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.311627Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.311009Z",
+     "shell.execute_reply.started": "2021-06-26T16:40:17.227656Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) "
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 132,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.313135Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.31287Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.318909Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.318191Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.313088Z"
+    }
+   },
+   "outputs": [],
+   "source": "print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 133,
+   "metadata": {
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.320845Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.320267Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.35912Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.358535Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.320797Z"
+    }
+   },
+   "outputs": [],
+   "source": "adaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "## Pros and cons of boosting\n\n---\n\n### Pros\n\n- Achieves higher performance than bagging when hyper-parameters tuned properly.\n- Can be used for classification and regression equally well.\n- Easily handles mixed data types.\n- Can use \"robust\" loss functions that make the model resistant to outliers.\n\n---\n\n### Cons\n\n- Difficult and time consuming to properly tune hyper-parameters.\n- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n- More risk of overfitting compared to bagging.\n\n<h3>Resources: </h3>\n<ul>\n    <li><a href=\"http://mccormickml.com/2013/12/13/adaboost-tutorial/\">AdaBoost Tutorial-Chris McCormick</a></li>\n    <li><a href=\"http://rob.schapire.net/papers/explaining-adaboost.pdf\">Explaining AdaBoost by Robert Schapire(One of the original author of AdaBoost)</a></li>\n</ul>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "6ea60e91-544f-49fc-8128-ee190e8292e7",
+    "_uuid": "860921893a28a1fe9a4ce47f0779f1e7b154ca0a"
+   },
+   "source": "## 7i. Gradient Boosting Classifier\n<a id=\"gradient_boosting\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 134,
+   "metadata": {
+    "_cell_guid": "d32d6df9-b8e7-4637-bacc-2baec08547b8",
+    "_uuid": "fd788c4f4cde834a1329f325f1f59e3f77c37e42",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.360536Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.360265Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.521396Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.520426Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.360479Z"
+    },
+    "scrolled": true
+   },
+   "outputs": [],
+   "source": "# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X, y)\ny_pred = gradient_boost.predict(X_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<div class=\" alert alert-info\">\n<h3>Resources: </h3>\n<ul>\n    <li><a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\">Gradient Descent(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Main Ideas)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Calculation)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=jxuNLH5dXCs\">Gradient Boost(Classification Main Ideas)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=StWY5QWMXCw\">Gradient Boost(Classification Calculation)(StatQuest)</a></li>\n    <li><a href=\"https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>\n</ul>\n</div>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "996b8ee8-13ff-461d-8f7b-ac0d7d488cff",
+    "_uuid": "ee9c7a2ccdf93a90f929b6618105afbe699bd6de"
+   },
+   "source": "## 7j. XGBClassifier\n<a id=\"XGBClassifier\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 135,
+   "metadata": {
+    "_cell_guid": "5d94cc5b-d8b7-40d3-b264-138539daabfa",
+    "_uuid": "9d96154d2267ea26a6682a73bd1850026eb1303b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.523177Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.522724Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.526955Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.525945Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.522964Z"
+    }
+   },
+   "outputs": [],
+   "source": "# from xgboost import XGBClassifier\n# XGBClassifier = XGBClassifier()\n# XGBClassifier.fit(X, y)\n# y_pred = XGBClassifier.predict(X_test)\n# XGBClassifier_accy = round(accuracy_score(y_pred, y_test), 3)\n# print(XGBClassifier_accy)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "a6b4c23c-b42b-4fad-b37d-c84154b3478d",
+    "_uuid": "3fa68b3d2e835b1a14088102561a2f8d4dac8f5c"
+   },
+   "source": "## 7k. Extra Trees Classifier\n<a id=\"extra_tree\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 136,
+   "metadata": {
+    "_cell_guid": "2e567e01-6b5f-4313-84af-cc378c3b709e",
+    "_uuid": "c9b958e2488adf6f79401c677087e3250d63ac9b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.528841Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.528382Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.555697Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.554889Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.528664Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)\n"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "20a66dcc-7f9f-4802-aa6d-58be75e07539",
+    "_uuid": "c55a54821feda82c75dde28bab7e2cf4445c4cf0"
+   },
+   "source": "## 7l. Gaussian Process Classifier\n<a id=\"GaussianProcessClassifier\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 137,
+   "metadata": {
+    "_cell_guid": "23bd5744-e04d-49bb-9d70-7c2a518f76dd",
+    "_uuid": "57fc008eea2ce1c0b595f888a82ddeaee6ce2177",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.557268Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.556845Z",
+     "iopub.status.idle": "2021-06-26T16:41:28.863352Z",
+     "shell.execute_reply": "2021-06-26T16:41:28.862576Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.557221Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gau_pro_accy)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "ec676e4d-0cbe-43fa-9ff8-92d76030faef",
+    "_uuid": "6f89f2cb63120a4594c7b0f2883b6872aa444700"
+   },
+   "source": "## 7m. Voting Classifier\n<a id=\"voting_classifer\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 138,
+   "metadata": {
+    "_cell_guid": "ac208dd3-1045-47bb-9512-de5ecb5c81b0",
+    "_uuid": "821c74bbf404193219eb91fe53755d669f5a14d1",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:28.865063Z",
+     "iopub.status.busy": "2021-06-26T16:41:28.86463Z",
+     "iopub.status.idle": "2021-06-26T16:41:30.314425Z",
+     "shell.execute_reply": "2021-06-26T16:41:30.313671Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:28.865013Z"
+    }
+   },
+   "outputs": [],
+   "source": "from sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_classifier', knn_grid),\n#     ('XGB_Classifier', XGBClassifier),\n    ('bagging_classifier', bagging_grid),\n    ('adaBoost_classifier',adaBoost_grid),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_classifier',gaussian),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 139,
+   "metadata": {
+    "_cell_guid": "648ac6a6-2437-490a-bf76-1612a71126e8",
+    "_uuid": "518a02ae91cc91d618e476d1fc643cd3912ee5fb",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:30.316454Z",
+     "iopub.status.busy": "2021-06-26T16:41:30.316008Z",
+     "iopub.status.idle": "2021-06-26T16:41:30.42114Z",
+     "shell.execute_reply": "2021-06-26T16:41:30.420152Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:30.31627Z"
+    }
+   },
+   "outputs": [],
+   "source": "y_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 140,
+   "metadata": {
+    "_cell_guid": "277534eb-7ec8-4359-a2f4-30f7f76611b8",
+    "_kg_hide-input": true,
+    "_uuid": "00a9b98fd4e230db427a63596a2747f05b1654c1",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:30.422908Z",
+     "iopub.status.busy": "2021-06-26T16:41:30.422475Z",
+     "iopub.status.idle": "2021-06-26T16:41:30.426856Z",
+     "shell.execute_reply": "2021-06-26T16:41:30.425882Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:30.422736Z"
+    }
+   },
+   "outputs": [],
+   "source": "#models = pd.DataFrame({\n#    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n#              'Random Forest', 'Naive Bayes', \n#              'Decision Tree', 'Gradient Boosting Classifier', 'Voting Classifier', 'XGB Classifier','ExtraTrees Classifier','Bagging Classifier'],\n#    'Score': [svc_accy, knn_accy, logreg_accy, \n#              random_accy, gaussian_accy, dectree_accy,\n#               gradient_accy, voting_accy, XGBClassifier_accy, extraTree_accy, bagging_accy]})\n#models.sort_values(by='Score', ascending=False)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {
+    "_cell_guid": "7128f3dd-1d8d-4b8e-afb4-891d8cb9657c",
+    "_uuid": "7e17482a69dbe99319219a603ea39f8bbde98b87"
+   },
+   "source": "# Part 8: Submit test predictions\n<a id=\"submit_predictions\"></a>\n***"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 141,
+   "metadata": {
+    "_uuid": "eb0054822f296ba86aa6005b2a5e35fbc1aec88b",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:30.429099Z",
+     "iopub.status.busy": "2021-06-26T16:41:30.42862Z",
+     "iopub.status.idle": "2021-06-26T16:41:30.646363Z",
+     "shell.execute_reply": "2021-06-26T16:41:30.645616Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:30.428903Z"
+    }
+   },
+   "outputs": [],
+   "source": "all_models = [logreg_grid,\n              knn_grid, \n              knn_ran_grid,\n              svm_grid,\n              dectree_grid,\n              rf_grid,\n              bagging_grid,\n              adaBoost_grid,\n              voting_classifier]\n\nc = {}\nfor i in all_models:\n    a = i.predict(X_test)\n    b = accuracy_score(a, y_test)\n    c[i] = b\n    \n"
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 142,
+   "metadata": {
+    "_cell_guid": "51368e53-52e4-41cf-9cc9-af6164c9c6f5",
+    "_uuid": "b947f168f6655c1c6eadaf53f3485d57c0cd74c7",
+    "execution": {
+     "iopub.execute_input": "2021-06-26T16:41:30.648318Z",
+     "iopub.status.busy": "2021-06-26T16:41:30.647987Z",
+     "iopub.status.idle": "2021-06-26T16:41:32.045557Z",
+     "shell.execute_reply": "2021-06-26T16:41:32.044733Z",
+     "shell.execute_reply.started": "2021-06-26T16:41:30.648259Z"
+    }
+   },
+   "outputs": [],
+   "source": "test_prediction = (max(c, key=c.get)).predict(test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction\n    })\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<div class=\"alert alert-info\">\n    <h1>Resources</h1>\n    <ul>\n        <li><b>Statistics</b></li>\n        <ul>\n            <li><a href=\"https://statistics.laerd.com/statistical-guides/measures-of-spread-standard-deviation.php\">Types of Standard Deviation</a></li>\n            <li><a href=\"https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen\">What Is a t-test? And Why Is It Like Telling a Kid to Clean Up that Mess in the Kitchen?</a></li>\n            <li><a href=\"https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-t-values-and-p-values-in-statistics\">What Are T Values and P Values in Statistics?</a></li>\n            <li><a href=\"https://www.youtube.com/watch?v=E4KCfcVwzyw\">What is p-value? How we decide on our confidence level.</a></li>\n        </ul>\n        <li><b>Writing pythonic code</b></li>\n        <ul>\n            <li><a href=\"https://www.kaggle.com/rtatman/six-steps-to-more-professional-data-science-code\">Six steps to more professional data science code</a></li>\n            <li><a href=\"https://www.kaggle.com/jpmiller/creating-a-good-analytics-report\">Creating a Good Analytics Report</a></li>\n            <li><a href=\"https://en.wikipedia.org/wiki/Code_smell\">Code Smell</a></li>\n            <li><a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style guides</a></li>\n            <li><a href=\"https://gist.github.com/sloria/7001839\">The Best of the Best Practices(BOBP) Guide for Python</a></li>\n            <li><a href=\"https://www.python.org/dev/peps/pep-0020/\">PEP 20 -- The Zen of Python</a></li>\n            <li><a href=\"https://docs.python-guide.org/\">The Hitchiker's Guide to Python</a></li>\n            <li><a href=\"https://realpython.com/tutorials/best-practices/\">Python Best Practice Patterns</a></li>\n            <li><a href=\"http://www.nilunder.com/blog/2013/08/03/pythonic-sensibilities/\">Pythonic Sensibilities</a></li>\n        </ul>\n        <li><b>Why Scikit-Learn?</b></li>\n        <ul>\n            <li><a href=\"https://www.oreilly.com/content/intro-to-scikit-learn/\">Introduction to Scikit-Learn</a></li>\n            <li><a href=\"https://www.oreilly.com/content/six-reasons-why-i-recommend-scikit-learn/\">Six reasons why I recommend scikit-learn</a></li>\n            <li><a href=\"https://hub.packtpub.com/learn-scikit-learn/\">Why you should learn Scikit-learn</a></li>\n            <li><a href=\"https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\">A Deep Dive Into Sklearn Pipelines</a></li>\n            <li><a href=\"https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial\">Sklearn pipelines tutorial</a></li>\n            <li><a href=\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\">Managing Machine Learning workflows with Sklearn pipelines</a></li>\n            <li><a href=\"https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\">A simple example of pipeline in Machine Learning using SKlearn</a></li>\n        </ul>\n    </ul>\n    <h1>Credits</h1>\n    <ul>\n        <li>To Brandon Foltz for his <a href=\"https://www.youtube.com/channel/UCFrjdcImgcQVyFbK04MBEhA\">youtube</a> channel and for being an amazing teacher.</li>\n        <li>To GA where I started my data science journey.</li>\n        <li>To Kaggle community for inspiring me over and over again with all the resources I need.</li>\n        <li>To Udemy Course \"Deployment of Machine Learning\". I have used and modified some of the code from this course to help making the learning process intuitive.</li>\n    </ul>\n</div>"
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": "<div class=\"alert alert-info\">\n<h4>If you like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on:</h4>\n    <ul>\n        <li><a href=\"https://www.linkedin.com/in/masumrumi/\"><b>LinkedIn</b></a></li>\n        <li><a href=\"https://github.com/masumrumi\"><b>Github</b></a></li>\n        <li><a href=\"https://masumrumi.github.io/cv/\"><b>masumrumi.github.io/cv/</b></a></li>\n        <li><a href=\"https://www.youtube.com/channel/UC1mPjGyLcZmsMgZ8SJgrfdw\"><b>Youtube</b></a></li>\n    </ul>\n\n<p>This kernel will always be a work in progress. I will incorporate new concepts of data science as I comprehend them with each update. If you have any idea/suggestions about this notebook, please let me know. Any feedback about further improvements would be genuinely appreciated.</p>\n\n<h1>If you have come this far, Congratulations!!</h1>\n\n<h1>If this notebook helped you in any way or you liked it, please upvote and/or leave a comment!! :)</h1></div>"
+  }
+ ],
+ "metadata": {
+  "interpreter": {
+   "hash": "4ee7c4e4684082ce1f46e630280ae5702684ea5c5635036c275c1be52ef87f64"
+  },
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.8.8"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
\ No newline at end of file
Index: dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"This notebook gives a simple combination of literal matching and Named Entity Recognition using BERT (base model from huggingface).\\n\\nThe training phase of the BERT model was done in another kernel: Pytorch BERT for Named Entity Recognition.\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"MAX_SAMPLE = None # set a small number for experimentation, set None for production.\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Install packages\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Import\"},{\"metadata\":{\"_uuid\":\"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\"_cell_guid\":\"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"import os\\nimport re\\nimport json\\nimport time\\nimport datetime\\nimport random\\nimport glob\\nimport importlib\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom tqdm import tqdm\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nrandom.seed(123)\\nnp.random.seed(456)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Load data\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\\ntrain = pd.read_csv(train_path)\\ntrain = train[:MAX_SAMPLE]\\n\\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\\npapers = {}\\nfor paper_id in train['Id'].unique():\\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\\n        paper = json.load(f)\\n        papers[paper_id] = paper\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\\nsample_submission = pd.read_csv(sample_submission_path)\\n\\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\\nfor paper_id in sample_submission['Id']:\\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\\n        paper = json.load(f)\\n        papers[paper_id] = paper\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Literal matching\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Create a knowledge bank\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"all_labels = set()\\n\\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\\n    all_labels.add(str(label_1).lower())\\n    all_labels.add(str(label_2).lower())\\n    all_labels.add(str(label_3).lower())\\n    \\nprint(f'No. different labels: {len(all_labels)}')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Matching on test data\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def clean_text(txt):\\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\\n\\ndef totally_clean_text(txt):\\n    txt = clean_text(txt)\\n    txt = re.sub(' +', ' ', txt)\\n    return txt\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"literal_preds = []\\n\\nfor paper_id in sample_submission['Id']:\\n    paper = papers[paper_id]\\n    text_1 = '. '.join(section['text'] for section in paper).lower()\\n    text_2 = totally_clean_text(text_1)\\n    \\n    labels = set()\\n    for label in all_labels:\\n        if label in text_1 or label in text_2:\\n            labels.add(clean_text(label))\\n    \\n    literal_preds.append('|'.join(labels))\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"literal_preds[:5]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"markdown\",\"source\":\"# Bert prediction\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Paths and Hyperparameters\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"MAX_LENGTH = 64 # max no. words for each sentence.\\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\\n\\nPREDICT_BATCH = 64000 \\n\\nPRETRAINED_PATH = '../input/coleridge-bert-models/output'\\nTEST_INPUT_SAVE_PATH = './input_data'\\nTEST_NER_DATA_FILE = 'test_ner_input.json'\\nTRAIN_PATH = '../input/coleridge-bert-models/train_ner.json'\\nVAL_PATH = '../input/coleridge-bert-models/train_ner.json'\\n\\nPREDICTION_SAVE_PATH = './pred'\\nPREDICTION_FILE = 'test_predictions.txt'\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Transform data to NER format\"},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"Group by publication, training labels should have the same form as expected output.\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"train = train.groupby('Id').agg({\\n    'pub_title': 'first',\\n    'dataset_title': '|'.join,\\n    'dataset_label': '|'.join,\\n    'cleaned_label': '|'.join\\n}).reset_index()\\n\\nprint(f'No. grouped training rows: {len(train)}')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def clean_training_text(txt):\\n    \\\"\\\"\\\"\\n    similar to the default clean_text function but without lowercasing.\\n    \\\"\\\"\\\"\\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\\n\\ndef shorten_sentences(sentences):\\n    short_sentences = []\\n    for sentence in sentences:\\n        words = sentence.split()\\n        if len(words) > MAX_LENGTH:\\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\\n        else:\\n            short_sentences.append(sentence)\\n    return short_sentences\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"test_rows = [] # test data in NER format\\npaper_length = [] # store the number of sentences each paper has\\n\\nfor paper_id in sample_submission['Id']:\\n    # load paper\\n    paper = papers[paper_id]\\n    \\n    # extract sentences\\n    sentences = [clean_training_text(sentence) for section in paper \\n                 for sentence in section['text'].split('.')\\n                ]\\n    sentences = shorten_sentences(sentences) # make sentences short\\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\\n        \\n    # collect all sentences in json\\n    for sentence in sentences:\\n        sentence_words = sentence.split()\\n        dummy_tags = ['O']*len(sentence_words)\\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\\n    \\n    # track which sentence belongs to which data point\\n    paper_length.append(len(sentences))\\n    \\nprint(f'total number of sentences: {len(test_rows)}')\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Do predict and collect results\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"os.environ[\\\"MODEL_PATH\\\"] = f\\\"{PRETRAINED_PATH}\\\"\\nos.environ[\\\"TRAIN_FILE\\\"] = f\\\"{TRAIN_PATH}\\\"\\nos.environ[\\\"VALIDATION_FILE\\\"] = f\\\"{VAL_PATH}\\\"\\nos.environ[\\\"TEST_FILE\\\"] = f\\\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\\\"\\nos.environ[\\\"OUTPUT_DIR\\\"] = f\\\"{PREDICTION_SAVE_PATH}\\\"\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# copy my_seqeval.py to the working directory because the input directory is non-writable\\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\\n\\n# make necessart directories and files\\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def bert_predict():\\n    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\\\\n    --model_name_or_path \\\"$MODEL_PATH\\\" \\\\\\n    --train_file \\\"$TRAIN_FILE\\\" \\\\\\n    --validation_file \\\"$VALIDATION_FILE\\\" \\\\\\n    --test_file \\\"$TEST_FILE\\\" \\\\\\n    --output_dir \\\"$OUTPUT_DIR\\\" \\\\\\n    --report_to 'none' \\\\\\n    --seed 123 \\\\\\n    --do_predict\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"bert_outputs = []\\n\\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\\n    # write data rows to input file\\n    with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\\n            json.dump(row, f)\\n            f.write('\\\\n')\\n    \\n    # remove output dir\\n    !rm -r \\\"$OUTPUT_DIR\\\"\\n    \\n    # do predict\\n    bert_predict()\\n    \\n    # read predictions\\n    with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\\n        this_preds = f.read().split('\\\\n')[:-1]\\n        bert_outputs += [pred.split() for pred in this_preds]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Restore Dataset labels from predictions\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"# get test sentences\\ntest_sentences = [row['tokens'] for row in test_rows]\\n\\ndel test_rows\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"bert_dataset_labels = [] # store all dataset labels for each publication\\n\\nfor length in paper_length:\\n    labels = set()\\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\\n        curr_phrase = ''\\n        for word, tag in zip(sentence, pred):\\n            if tag == 'B': # start a new phrase\\n                if curr_phrase:\\n                    labels.add(curr_phrase)\\n                    curr_phrase = ''\\n                curr_phrase = word\\n            elif tag == 'I' and curr_phrase: # continue the phrase\\n                curr_phrase += ' ' + word\\n            else: # end last phrase (if any)\\n                if curr_phrase:\\n                    labels.add(curr_phrase)\\n                    curr_phrase = ''\\n        # check if the label is the suffix of the sentence\\n        if curr_phrase:\\n            labels.add(curr_phrase)\\n            curr_phrase = ''\\n    \\n    # record dataset labels for this publication\\n    bert_dataset_labels.append(labels)\\n    \\n    del test_sentences[:length], bert_outputs[:length]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"bert_dataset_labels[:5]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"### Filter based on Jaccard score and clean\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def jaccard_similarity(s1, s2):\\n    l1 = s1.split(\\\" \\\")\\n    l2 = s2.split(\\\" \\\")    \\n    intersection = len(list(set(l1).intersection(l2)))\\n    union = (len(l1) + len(l2)) - intersection\\n    return float(intersection) / union\\n\\nfiltered_bert_labels = []\\n\\nfor labels in bert_dataset_labels:\\n    filtered = []\\n    \\n    for label in sorted(labels, key=len):\\n        label = clean_text(label)\\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\\n            filtered.append(label)\\n    \\n    filtered_bert_labels.append('|'.join(filtered))\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"filtered_bert_labels[:5]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Aggregate final predictions and write submission file\"},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"final_predictions = []\\nfor literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\\n    if literal_match:\\n        final_predictions.append(literal_match)\\n    else:\\n        final_predictions.append(bert_pred)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"sample_submission['PredictionString'] = final_predictions\\nsample_submission.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"sample_submission.to_csv(f'submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat\":4,\"nbformat_minor\":4}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb b/dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb
--- a/dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/coleridgeinitiative-show-us-the-data/Tung M Phung/coleridge-matching-bert-ner.ipynb	(date 1658512097893)
@@ -1,1 +1,329 @@
-{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook gives a simple combination of literal matching and Named Entity Recognition using BERT (base model from huggingface).\n\nThe training phase of the BERT model was done in another kernel: Pytorch BERT for Named Entity Recognition."},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SAMPLE = None # set a small number for experimentation, set None for production.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Install packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\n\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Literal matching"},{"metadata":{},"cell_type":"markdown","source":"### Create a knowledge bank"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_labels = set()\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matching on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"literal_preds = []\n\nfor paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"literal_preds[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Bert prediction"},{"metadata":{},"cell_type":"markdown","source":"### Paths and Hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\nPRETRAINED_PATH = '../input/coleridge-bert-models/output'\nTEST_INPUT_SAVE_PATH = './input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '../input/coleridge-bert-models/train_ner.json'\nVAL_PATH = '../input/coleridge-bert-models/train_ner.json'\n\nPREDICTION_SAVE_PATH = './pred'\nPREDICTION_FILE = 'test_predictions.txt'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transform data to NER format"},{"metadata":{},"cell_type":"markdown","source":"Group by publication, training labels should have the same form as expected output."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do predict and collect results"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_predict():\n    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_outputs = []\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Restore Dataset labels from predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n\ndel test_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_dataset_labels = [] # store all dataset labels for each publication\n\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_dataset_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter based on Jaccard score and clean"},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\nfiltered_bert_labels = []\n\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.append('|'.join(filtered))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_bert_labels[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Aggregate final predictions and write submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = []\nfor literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n    if literal_match:\n        final_predictions.append(literal_match)\n    else:\n        final_predictions.append(bert_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['PredictionString'] = final_predictions\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(f'submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "This notebook gives a simple combination of literal matching and Named Entity Recognition using BERT (base model from huggingface).\n\nThe training phase of the BERT model was done in another kernel: Pytorch BERT for Named Entity Recognition."
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "MAX_SAMPLE = None # set a small number for experimentation, set None for production.",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Install packages"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Import"
+  },
+  {
+   "metadata": {
+    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
+    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Load data"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\n\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "sample_submission_path = '../input/coleridgeinitiative-show-us-the-data/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '../input/coleridgeinitiative-show-us-the-data/test'\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Literal matching"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Create a knowledge bank"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "all_labels = set()\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Matching on test data"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "literal_preds = []\n\nfor paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "literal_preds[:5]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "markdown",
+   "source": "# Bert prediction"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Paths and Hyperparameters"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\nPRETRAINED_PATH = '../input/coleridge-bert-models/output'\nTEST_INPUT_SAVE_PATH = './input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '../input/coleridge-bert-models/train_ner.json'\nVAL_PATH = '../input/coleridge-bert-models/train_ner.json'\n\nPREDICTION_SAVE_PATH = './pred'\nPREDICTION_FILE = 'test_predictions.txt'",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Transform data to NER format"
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "Group by publication, training labels should have the same form as expected output."
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Do predict and collect results"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def bert_predict():\n    !python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "bert_outputs = []\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Restore Dataset labels from predictions"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n\ndel test_rows",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "bert_dataset_labels = [] # store all dataset labels for each publication\n\nfor length in paper_length:\n    labels = set()\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n    \n    # record dataset labels for this publication\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "bert_dataset_labels[:5]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "### Filter based on Jaccard score and clean"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) / union\n\nfiltered_bert_labels = []\n\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.append('|'.join(filtered))",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "filtered_bert_labels[:5]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Aggregate final predictions and write submission file"
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "final_predictions = []\nfor literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n    if literal_match:\n        final_predictions.append(literal_match)\n    else:\n        final_predictions.append(bert_pred)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "sample_submission['PredictionString'] = final_predictions\nsample_submission.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "sample_submission.to_csv(f'submission.csv', index=False)",
+   "execution_count": null,
+   "outputs": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
\ No newline at end of file
Index: dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat_minor\":4,\"nbformat\":4,\"cells\":[{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-info\\\">  \\n    <h3><strong>\uD83D\uDC68\u200D\uD83D\uDCBB Getting Started with LearnPlatform COVID-19 Impact on Digital Learning\\nUse digital learning data to analyze the impact of COVID-19 on student lea</strong></h3>\\n    <i></i>\\n</div>\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"# <center><img src=\\\"https://akm-img-a-in.tosshub.com/businesstoday/images/story/201712/data-analytics_660_120717022837.jpg\\\"></center>\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-success\\\">  \\n \\n<hr>\\n<b>Source of Data: </b> \\n<hr> \\n <a href=\\\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/datahttps://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\\\">https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data</a>\\n   \\n</div>\",\"metadata\":{\"papermill\":{\"duration\":0.020638,\"end_time\":\"2020-12-02T21:20:36.560772\",\"exception\":false,\"start_time\":\"2020-12-02T21:20:36.540134\",\"status\":\"completed\"},\"tags\":[]}},{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-info\\\">  \\n \\n<hr>\\n<b>Data Description: </b> \\n<hr> \\n    <p>We include three basic sets of files to help you get started. The engagement data are based on LearnPlatforms Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district. The product file includes information about the characteristics of the top 372 products with most users in 2020. The district file includes information about the characteristics of school districts, including data from National Center for Education Statistics (NCES), The Federal Communications Commission (FCC), and Edunomics Lab.</p>\\n <a href=\\\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\\\">https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data</a>\\n   \\n</div>\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-danger\\\">  \\n<h2><center><strong>Importing Python Libraries \uD83D\uDCD5 \uD83D\uDCD7 \uD83D\uDCD8 \uD83D\uDCD9</strong></center></h2>\\n        \\n</div>\",\"metadata\":{\"papermill\":{\"duration\":0.065466,\"end_time\":\"2020-11-30T07:38:51.578836\",\"exception\":false,\"start_time\":\"2020-11-30T07:38:51.51337\",\"status\":\"completed\"},\"tags\":[]}},{\"cell_type\":\"markdown\",\"source\":\"- Libraries are important and we call them to perform the different actions on our data and for training the models.\\n- Its a first step to load the library to perform the specific task\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"import pandas as pd\\nimport numpy as np  \\nimport seaborn as sns \\npal = sns.color_palette()\\nfrom wordcloud import WordCloud\\nimport plotly.express as px\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nfrom sklearn import preprocessing\\nimport glob\\nimport plotly.offline as py\\nimport plotly.graph_objs as go\\nimport plotly.tools as tls\\nfrom keras.models import Sequential\\nfrom tensorflow.keras.layers import Activation, Dense, Dropout\",\"metadata\":{\"id\":\"-E4TvIVLMa73\",\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:01.792658Z\",\"iopub.execute_input\":\"2021-08-02T22:05:01.793408Z\",\"iopub.status.idle\":\"2021-08-02T22:05:05.188735Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:01.793297Z\",\"shell.execute_reply\":\"2021-08-02T22:05:05.1879Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"<h4>How we can install the libraries in python?</h4>\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"<h4>To install the python library is very easy</h4>\\n- pip install name_of_library \\n<h5> Like if you wanted to install tensorflow? </h5>\\n- pip install tensforflow\",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-danger\\\">  \\n<h2><center><strong>Loading the data \uD83D\uDCC1 \uD83D\uDCC2</strong></center></h2>\\n        \\n</div>\",\"metadata\":{\"papermill\":{\"duration\":0.065466,\"end_time\":\"2020-11-30T07:38:51.578836\",\"exception\":false,\"start_time\":\"2020-11-30T07:38:51.51337\",\"status\":\"completed\"},\"tags\":[]}},{\"cell_type\":\"code\",\"source\":\"districts_data=pd.read_csv(\\\"../input/learnplatform-covid19-impact-on-digital-learning/districts_info.csv\\\")\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:07.497469Z\",\"iopub.execute_input\":\"2021-08-02T22:05:07.498066Z\",\"iopub.status.idle\":\"2021-08-02T22:05:07.510311Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:07.498031Z\",\"shell.execute_reply\":\"2021-08-02T22:05:07.509154Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-danger\\\">  \\n<h2><center><strong>Exploratory data analysis \uD83D\uDD0E \uD83D\uDCCA</strong></center></h2>\\n        \\n</div>\",\"metadata\":{\"papermill\":{\"duration\":0.065466,\"end_time\":\"2020-11-30T07:38:51.578836\",\"exception\":false,\"start_time\":\"2020-11-30T07:38:51.51337\",\"status\":\"completed\"},\"tags\":[]}},{\"cell_type\":\"markdown\",\"source\":\"#### Five top records of data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"districts_data.head()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:07.932198Z\",\"iopub.execute_input\":\"2021-08-02T22:05:07.932577Z\",\"iopub.status.idle\":\"2021-08-02T22:05:07.956583Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:07.932546Z\",\"shell.execute_reply\":\"2021-08-02T22:05:07.95568Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Five last records of data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"districts_data.tail()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:08.265237Z\",\"iopub.execute_input\":\"2021-08-02T22:05:08.265793Z\",\"iopub.status.idle\":\"2021-08-02T22:05:08.280731Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:08.265745Z\",\"shell.execute_reply\":\"2021-08-02T22:05:08.279699Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Coloumns/features in data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"districts_data.columns\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:08.691185Z\",\"iopub.execute_input\":\"2021-08-02T22:05:08.69155Z\",\"iopub.status.idle\":\"2021-08-02T22:05:08.698283Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:08.69152Z\",\"shell.execute_reply\":\"2021-08-02T22:05:08.697226Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Length of data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"print('lenght of data is', len(districts_data))\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:09.099852Z\",\"iopub.execute_input\":\"2021-08-02T22:05:09.100255Z\",\"iopub.status.idle\":\"2021-08-02T22:05:09.105603Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:09.10022Z\",\"shell.execute_reply\":\"2021-08-02T22:05:09.10456Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Shape of data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"districts_data.shape\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:09.474064Z\",\"iopub.execute_input\":\"2021-08-02T22:05:09.474641Z\",\"iopub.status.idle\":\"2021-08-02T22:05:09.481442Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:09.474605Z\",\"shell.execute_reply\":\"2021-08-02T22:05:09.480442Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Data information\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"districts_data.info()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:09.826782Z\",\"iopub.execute_input\":\"2021-08-02T22:05:09.827404Z\",\"iopub.status.idle\":\"2021-08-02T22:05:09.844905Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:09.827356Z\",\"shell.execute_reply\":\"2021-08-02T22:05:09.843552Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Data types of all coloumns\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"districts_data.dtypes\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:10.275442Z\",\"iopub.execute_input\":\"2021-08-02T22:05:10.275798Z\",\"iopub.status.idle\":\"2021-08-02T22:05:10.284211Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:10.275768Z\",\"shell.execute_reply\":\"2021-08-02T22:05:10.282936Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Checking Null values / missing values\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"np.sum(districts_data.isnull().any(axis=1))\",\"metadata\":{\"scrolled\":true,\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:10.650892Z\",\"iopub.execute_input\":\"2021-08-02T22:05:10.651266Z\",\"iopub.status.idle\":\"2021-08-02T22:05:10.660402Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:10.651233Z\",\"shell.execute_reply\":\"2021-08-02T22:05:10.659392Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"#### Rows and columns in the dataset\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"print('Count of columns in the data is:  ', len(districts_data.columns))\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:10.926482Z\",\"iopub.execute_input\":\"2021-08-02T22:05:10.926859Z\",\"iopub.status.idle\":\"2021-08-02T22:05:10.93196Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:10.926827Z\",\"shell.execute_reply\":\"2021-08-02T22:05:10.93092Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"print('Count of rows in the data is:  ', len(districts_data))\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:11.128309Z\",\"iopub.execute_input\":\"2021-08-02T22:05:11.12898Z\",\"iopub.status.idle\":\"2021-08-02T22:05:11.1344Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:11.128944Z\",\"shell.execute_reply\":\"2021-08-02T22:05:11.133398Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Deleting the duplicate rows\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"current=len(districts_data)\\nprint('Rows of data before Delecting ', current)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:11.523298Z\",\"iopub.execute_input\":\"2021-08-02T22:05:11.523699Z\",\"iopub.status.idle\":\"2021-08-02T22:05:11.528882Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:11.523662Z\",\"shell.execute_reply\":\"2021-08-02T22:05:11.528087Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"districts_data=districts_data.drop_duplicates()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:11.750441Z\",\"iopub.execute_input\":\"2021-08-02T22:05:11.75102Z\",\"iopub.status.idle\":\"2021-08-02T22:05:11.757892Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:11.750968Z\",\"shell.execute_reply\":\"2021-08-02T22:05:11.757012Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"now=len(districts_data)\\nprint('Rows of data before Delecting ', now)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:12.019946Z\",\"iopub.execute_input\":\"2021-08-02T22:05:12.020339Z\",\"iopub.status.idle\":\"2021-08-02T22:05:12.026657Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:12.020301Z\",\"shell.execute_reply\":\"2021-08-02T22:05:12.025441Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"diff=current-now\\nprint('Duplicated rows deleted ', diff)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:12.167608Z\",\"iopub.execute_input\":\"2021-08-02T22:05:12.167974Z\",\"iopub.status.idle\":\"2021-08-02T22:05:12.176338Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:12.167944Z\",\"shell.execute_reply\":\"2021-08-02T22:05:12.175089Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"<div class=\\\"alert alert-block alert-danger\\\">  \\n<h1><center><strong>Data Visualization \uD83D\uDCDD</strong></center></h1>\\n\\n\\n   \\n        \\n</div>\",\"metadata\":{\"papermill\":{\"duration\":0.065466,\"end_time\":\"2020-11-30T07:38:51.578836\",\"exception\":false,\"start_time\":\"2020-11-30T07:38:51.51337\",\"status\":\"completed\"},\"tags\":[]}},{\"cell_type\":\"markdown\",\"source\":\"## Univariate Analysis \",\"metadata\":{}},{\"cell_type\":\"markdown\",\"source\":\"Distribution of state\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"plt.figure(figsize=(12,10))\\nsns.countplot(districts_data.state)\\nplt.xticks(rotation=90)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:12.935975Z\",\"iopub.execute_input\":\"2021-08-02T22:05:12.936584Z\",\"iopub.status.idle\":\"2021-08-02T22:05:13.353888Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:12.936549Z\",\"shell.execute_reply\":\"2021-08-02T22:05:13.353057Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"districts_data[\\\"state\\\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:13.3558Z\",\"iopub.execute_input\":\"2021-08-02T22:05:13.356265Z\",\"iopub.status.idle\":\"2021-08-02T22:05:13.825823Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:13.356219Z\",\"shell.execute_reply\":\"2021-08-02T22:05:13.824655Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"Distribution of locale\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"plt.figure(figsize=(12,10))\\nsns.countplot(districts_data.locale)\\nplt.xticks(rotation=90)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:13.827517Z\",\"iopub.execute_input\":\"2021-08-02T22:05:13.827823Z\",\"iopub.status.idle\":\"2021-08-02T22:05:14.012078Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:13.827795Z\",\"shell.execute_reply\":\"2021-08-02T22:05:14.01082Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"districts_data[\\\"locale\\\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:14.013818Z\",\"iopub.execute_input\":\"2021-08-02T22:05:14.014288Z\",\"iopub.status.idle\":\"2021-08-02T22:05:14.302337Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:14.014242Z\",\"shell.execute_reply\":\"2021-08-02T22:05:14.3012Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"Distribution of pct_black/hispanic\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"sns.countplot(data= districts_data, x = \\\"pct_black/hispanic\\\")\\nplt.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:14.304616Z\",\"iopub.execute_input\":\"2021-08-02T22:05:14.305191Z\",\"iopub.status.idle\":\"2021-08-02T22:05:14.469063Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:14.305138Z\",\"shell.execute_reply\":\"2021-08-02T22:05:14.467913Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"pct_free/reduced\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"sns.countplot(data= districts_data, x = \\\"pct_free/reduced\\\")\\nplt.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:14.567292Z\",\"iopub.execute_input\":\"2021-08-02T22:05:14.567649Z\",\"iopub.status.idle\":\"2021-08-02T22:05:14.714231Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:14.567619Z\",\"shell.execute_reply\":\"2021-08-02T22:05:14.713398Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"county_connections_ratio\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"sns.countplot(data= districts_data, x = \\\"county_connections_ratio\\\")\\nplt.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:14.935861Z\",\"iopub.execute_input\":\"2021-08-02T22:05:14.936238Z\",\"iopub.status.idle\":\"2021-08-02T22:05:15.087982Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:14.936205Z\",\"shell.execute_reply\":\"2021-08-02T22:05:15.086945Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"plt.figure(figsize=(12,10))\\nsns.countplot(districts_data.pp_total_raw)\\nplt.xticks(rotation=90)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:15.159193Z\",\"iopub.execute_input\":\"2021-08-02T22:05:15.159543Z\",\"iopub.status.idle\":\"2021-08-02T22:05:15.443286Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:15.159515Z\",\"shell.execute_reply\":\"2021-08-02T22:05:15.442039Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"## Loading the Products data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"products_data = pd.read_csv(\\\"../input/learnplatform-covid19-impact-on-digital-learning/products_info.csv\\\")\\nproducts_data\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:15.652715Z\",\"iopub.execute_input\":\"2021-08-02T22:05:15.65308Z\",\"iopub.status.idle\":\"2021-08-02T22:05:15.678822Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:15.653048Z\",\"shell.execute_reply\":\"2021-08-02T22:05:15.677793Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Distribution of Sector(s) in the District Information Data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"ds = products_data['Sector(s)'].value_counts().reset_index()\\nds.columns = [\\n    'Sector(s)', \\n    'percent'\\n]\\nds['percent'] /= len(products_data)\\n\\nfig = px.pie(\\n    ds, \\n    names='Sector(s)', \\n    values='percent',\\n    color_discrete_sequence=px.colors.sequential.Mint,\\n    title='Distribution of Sector(s) in the District Information Data:', \\n    width=700,\\n    height=500\\n)\\nfig.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:16.034641Z\",\"iopub.execute_input\":\"2021-08-02T22:05:16.03502Z\",\"iopub.status.idle\":\"2021-08-02T22:05:16.307441Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:16.034989Z\",\"shell.execute_reply\":\"2021-08-02T22:05:16.30635Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### districts state wordcloud\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"cloud = WordCloud(width=1440, height=1080).generate(\\\" \\\".join(districts_data['state'].astype(str)))\\nplt.figure(figsize=(15, 10))\\nplt.imshow(cloud)\\nplt.axis('off')\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:16.486466Z\",\"iopub.execute_input\":\"2021-08-02T22:05:16.486811Z\",\"iopub.status.idle\":\"2021-08-02T22:05:17.566233Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:16.486782Z\",\"shell.execute_reply\":\"2021-08-02T22:05:17.563955Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Occurrence of states in the District Information Data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"ds = districts_data['state'].value_counts().reset_index()\\nds.columns = [\\n    'state', \\n    'percent'\\n]\\nds['percent'] /= len(districts_data)\\n\\nfig = px.pie(\\n    ds, \\n    names='state', \\n    values='percent',\\n    color_discrete_sequence=px.colors.sequential.Mint,\\n    title='Occurrence of states in the District Information Data:', \\n    width=700,\\n    height=500\\n)\\nfig.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:17.56817Z\",\"iopub.execute_input\":\"2021-08-02T22:05:17.568629Z\",\"iopub.status.idle\":\"2021-08-02T22:05:17.630353Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:17.568583Z\",\"shell.execute_reply\":\"2021-08-02T22:05:17.62937Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Occurrence of Locale in the District Information Data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"ds = districts_data['locale'].value_counts().reset_index()\\nds.columns = [\\n    'locale', \\n    'percent'\\n]\\nds['percent'] /= len(districts_data)\\n\\nfig = px.pie(\\n    ds, \\n    names='locale', \\n    values='percent',\\n    color_discrete_sequence=px.colors.sequential.Mint,\\n    title='Occurrence of Locale in the District Information Data:', \\n    width=700,\\n    height=500\\n)\\nfig.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:18.229385Z\",\"iopub.execute_input\":\"2021-08-02T22:05:18.229778Z\",\"iopub.status.idle\":\"2021-08-02T22:05:18.289692Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:18.229733Z\",\"shell.execute_reply\":\"2021-08-02T22:05:18.288628Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"# Loading All Engagement Files\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"CSV_files=pd.DataFrame()\\naddress = glob.glob('../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/*.csv')\\ncount=0\\nfor i in address:\\n    with open(i, \\\"rb\\\") as data_of_files:\\n        data=pd.read_csv(data_of_files)\\n        CSV_files=pd.concat([CSV_files,data], axis=0)\\n        count=count+1\\n        if count==233:\\n            break  \\nCSV_files\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:05:18.291184Z\",\"iopub.execute_input\":\"2021-08-02T22:05:18.291508Z\",\"iopub.status.idle\":\"2021-08-02T22:07:27.465127Z\",\"shell.execute_reply.started\":\"2021-08-02T22:05:18.291479Z\",\"shell.execute_reply\":\"2021-08-02T22:07:27.461207Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Numeric features distrubution \",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"CSV_files.hist(figsize=(20,20),bins = 20, color=\\\"#107009AA\\\")\\nplt.title(\\\"Numeric Features Distribution\\\")\\nplt.show()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:07:27.467551Z\",\"iopub.execute_input\":\"2021-08-02T22:07:27.467876Z\",\"iopub.status.idle\":\"2021-08-02T22:07:30.685567Z\",\"shell.execute_reply.started\":\"2021-08-02T22:07:27.467843Z\",\"shell.execute_reply\":\"2021-08-02T22:07:30.684387Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Bivariate Analysis\\n\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"colormap = plt.cm.RdBu\\nplt.figure(figsize=(14,12))\\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\\nsns.heatmap(CSV_files.corr(),linewidths=0.1,vmax=1.0, \\n            square=True, cmap=colormap, linecolor='white', annot=True)\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:07:30.687396Z\",\"iopub.execute_input\":\"2021-08-02T22:07:30.687725Z\",\"iopub.status.idle\":\"2021-08-02T22:07:32.289986Z\",\"shell.execute_reply.started\":\"2021-08-02T22:07:30.687692Z\",\"shell.execute_reply\":\"2021-08-02T22:07:32.288881Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### Missing value Treatment\\n\\n\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"CSV_files.isnull().sum()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:07:32.291361Z\",\"iopub.execute_input\":\"2021-08-02T22:07:32.291673Z\",\"iopub.status.idle\":\"2021-08-02T22:07:34.609477Z\",\"shell.execute_reply.started\":\"2021-08-02T22:07:32.291644Z\",\"shell.execute_reply\":\"2021-08-02T22:07:34.608366Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"### lets calculate the total missing values in the each column\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"data_total = CSV_files.isnull().sum()\\ndata_percent = ((CSV_files.isnull().sum()/CSV_files.shape[0])*100).round(2)\\nmissing_data = pd.concat([data_total, data_percent],\\n                                axis=1, \\n                                keys=['Data_Total', 'Data_Percent %'],\\n                                sort = True)\\nmissing_data.style.bar(color = ['gold'])\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:07:34.611935Z\",\"iopub.execute_input\":\"2021-08-02T22:07:34.612405Z\",\"iopub.status.idle\":\"2021-08-02T22:07:39.286994Z\",\"shell.execute_reply.started\":\"2021-08-02T22:07:34.612334Z\",\"shell.execute_reply\":\"2021-08-02T22:07:39.285997Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"## Combining the data\",\"metadata\":{}},{\"cell_type\":\"code\",\"source\":\"print(products_data[\\\"LP ID\\\"].nunique())\\nprint(CSV_files[\\\"lp_id\\\"].nunique())\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:07:39.288361Z\",\"iopub.execute_input\":\"2021-08-02T22:07:39.288908Z\",\"iopub.status.idle\":\"2021-08-02T22:07:39.769592Z\",\"shell.execute_reply.started\":\"2021-08-02T22:07:39.288865Z\",\"shell.execute_reply\":\"2021-08-02T22:07:39.76835Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"code\",\"source\":\"products_engagement_data = pd.merge(products_data, CSV_files, left_on='LP ID', right_on='lp_id')\\nproducts_engagement_data.head()\",\"metadata\":{\"execution\":{\"iopub.status.busy\":\"2021-08-02T22:07:39.771934Z\",\"iopub.execute_input\":\"2021-08-02T22:07:39.772314Z\",\"iopub.status.idle\":\"2021-08-02T22:07:51.471418Z\",\"shell.execute_reply.started\":\"2021-08-02T22:07:39.772279Z\",\"shell.execute_reply\":\"2021-08-02T22:07:51.470272Z\"},\"trusted\":true},\"execution_count\":null,\"outputs\":[]},{\"cell_type\":\"markdown\",\"source\":\"## More code is coming Soon. Please upvote if you like the work and you will get notifications with additions. \\n\\n<center><img src=\\\"https://thumbs.gfycat.com/AshamedWeightyDachshund-max-1mb.gif\\\"></center>\",\"metadata\":{}}]}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb b/dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb
--- a/dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/learnplatform-covid19-impact-on-digital-learning/Muhammad Imran Zaman/covid-19-impact-on-digital-learning.ipynb	(date 1658512097901)
@@ -1,1 +1,844 @@
-{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n    <h3><strong> Getting Started with LearnPlatform COVID-19 Impact on Digital Learning\nUse digital learning data to analyze the impact of COVID-19 on student lea</strong></h3>\n    <i></i>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# <center><img src=\"https://akm-img-a-in.tosshub.com/businesstoday/images/story/201712/data-analytics_660_120717022837.jpg\"></center>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n \n<hr>\n<b>Source of Data: </b> \n<hr> \n <a href=\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/datahttps://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\">https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data</a>\n   \n</div>","metadata":{"papermill":{"duration":0.020638,"end_time":"2020-12-02T21:20:36.560772","exception":false,"start_time":"2020-12-02T21:20:36.540134","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">  \n \n<hr>\n<b>Data Description: </b> \n<hr> \n    <p>We include three basic sets of files to help you get started. The engagement data are based on LearnPlatforms Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district. The product file includes information about the characteristics of the top 372 products with most users in 2020. The district file includes information about the characteristics of school districts, including data from National Center for Education Statistics (NCES), The Federal Communications Commission (FCC), and Edunomics Lab.</p>\n <a href=\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\">https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data</a>\n   \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Importing Python Libraries    </strong></center></h2>\n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"- Libraries are important and we call them to perform the different actions on our data and for training the models.\n- Its a first step to load the library to perform the specific task","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np  \nimport seaborn as sns \npal = sns.color_palette()\nfrom wordcloud import WordCloud\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nimport glob\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout","metadata":{"id":"-E4TvIVLMa73","scrolled":true,"execution":{"iopub.status.busy":"2021-08-02T22:05:01.792658Z","iopub.execute_input":"2021-08-02T22:05:01.793408Z","iopub.status.idle":"2021-08-02T22:05:05.188735Z","shell.execute_reply.started":"2021-08-02T22:05:01.793297Z","shell.execute_reply":"2021-08-02T22:05:05.1879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>How we can install the libraries in python?</h4>","metadata":{}},{"cell_type":"markdown","source":"<h4>To install the python library is very easy</h4>\n- pip install name_of_library \n<h5> Like if you wanted to install tensorflow? </h5>\n- pip install tensforflow","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Loading the data  </strong></center></h2>\n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"code","source":"districts_data=pd.read_csv(\"../input/learnplatform-covid19-impact-on-digital-learning/districts_info.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:07.497469Z","iopub.execute_input":"2021-08-02T22:05:07.498066Z","iopub.status.idle":"2021-08-02T22:05:07.510311Z","shell.execute_reply.started":"2021-08-02T22:05:07.498031Z","shell.execute_reply":"2021-08-02T22:05:07.509154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Exploratory data analysis  </strong></center></h2>\n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Five top records of data","metadata":{}},{"cell_type":"code","source":"districts_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:07.932198Z","iopub.execute_input":"2021-08-02T22:05:07.932577Z","iopub.status.idle":"2021-08-02T22:05:07.956583Z","shell.execute_reply.started":"2021-08-02T22:05:07.932546Z","shell.execute_reply":"2021-08-02T22:05:07.95568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Five last records of data","metadata":{}},{"cell_type":"code","source":"districts_data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:08.265237Z","iopub.execute_input":"2021-08-02T22:05:08.265793Z","iopub.status.idle":"2021-08-02T22:05:08.280731Z","shell.execute_reply.started":"2021-08-02T22:05:08.265745Z","shell.execute_reply":"2021-08-02T22:05:08.279699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Coloumns/features in data","metadata":{}},{"cell_type":"code","source":"districts_data.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:08.691185Z","iopub.execute_input":"2021-08-02T22:05:08.69155Z","iopub.status.idle":"2021-08-02T22:05:08.698283Z","shell.execute_reply.started":"2021-08-02T22:05:08.69152Z","shell.execute_reply":"2021-08-02T22:05:08.697226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Length of data","metadata":{}},{"cell_type":"code","source":"print('lenght of data is', len(districts_data))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:09.099852Z","iopub.execute_input":"2021-08-02T22:05:09.100255Z","iopub.status.idle":"2021-08-02T22:05:09.105603Z","shell.execute_reply.started":"2021-08-02T22:05:09.10022Z","shell.execute_reply":"2021-08-02T22:05:09.10456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shape of data","metadata":{}},{"cell_type":"code","source":"districts_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:09.474064Z","iopub.execute_input":"2021-08-02T22:05:09.474641Z","iopub.status.idle":"2021-08-02T22:05:09.481442Z","shell.execute_reply.started":"2021-08-02T22:05:09.474605Z","shell.execute_reply":"2021-08-02T22:05:09.480442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data information","metadata":{}},{"cell_type":"code","source":"districts_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:09.826782Z","iopub.execute_input":"2021-08-02T22:05:09.827404Z","iopub.status.idle":"2021-08-02T22:05:09.844905Z","shell.execute_reply.started":"2021-08-02T22:05:09.827356Z","shell.execute_reply":"2021-08-02T22:05:09.843552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data types of all coloumns","metadata":{}},{"cell_type":"code","source":"districts_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:10.275442Z","iopub.execute_input":"2021-08-02T22:05:10.275798Z","iopub.status.idle":"2021-08-02T22:05:10.284211Z","shell.execute_reply.started":"2021-08-02T22:05:10.275768Z","shell.execute_reply":"2021-08-02T22:05:10.282936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Null values / missing values","metadata":{}},{"cell_type":"code","source":"np.sum(districts_data.isnull().any(axis=1))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-02T22:05:10.650892Z","iopub.execute_input":"2021-08-02T22:05:10.651266Z","iopub.status.idle":"2021-08-02T22:05:10.660402Z","shell.execute_reply.started":"2021-08-02T22:05:10.651233Z","shell.execute_reply":"2021-08-02T22:05:10.659392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Rows and columns in the dataset","metadata":{}},{"cell_type":"code","source":"print('Count of columns in the data is:  ', len(districts_data.columns))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:10.926482Z","iopub.execute_input":"2021-08-02T22:05:10.926859Z","iopub.status.idle":"2021-08-02T22:05:10.93196Z","shell.execute_reply.started":"2021-08-02T22:05:10.926827Z","shell.execute_reply":"2021-08-02T22:05:10.93092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Count of rows in the data is:  ', len(districts_data))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:11.128309Z","iopub.execute_input":"2021-08-02T22:05:11.12898Z","iopub.status.idle":"2021-08-02T22:05:11.1344Z","shell.execute_reply.started":"2021-08-02T22:05:11.128944Z","shell.execute_reply":"2021-08-02T22:05:11.133398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deleting the duplicate rows","metadata":{}},{"cell_type":"code","source":"current=len(districts_data)\nprint('Rows of data before Delecting ', current)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:11.523298Z","iopub.execute_input":"2021-08-02T22:05:11.523699Z","iopub.status.idle":"2021-08-02T22:05:11.528882Z","shell.execute_reply.started":"2021-08-02T22:05:11.523662Z","shell.execute_reply":"2021-08-02T22:05:11.528087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"districts_data=districts_data.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:11.750441Z","iopub.execute_input":"2021-08-02T22:05:11.75102Z","iopub.status.idle":"2021-08-02T22:05:11.757892Z","shell.execute_reply.started":"2021-08-02T22:05:11.750968Z","shell.execute_reply":"2021-08-02T22:05:11.757012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"now=len(districts_data)\nprint('Rows of data before Delecting ', now)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:12.019946Z","iopub.execute_input":"2021-08-02T22:05:12.020339Z","iopub.status.idle":"2021-08-02T22:05:12.026657Z","shell.execute_reply.started":"2021-08-02T22:05:12.020301Z","shell.execute_reply":"2021-08-02T22:05:12.025441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff=current-now\nprint('Duplicated rows deleted ', diff)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:12.167608Z","iopub.execute_input":"2021-08-02T22:05:12.167974Z","iopub.status.idle":"2021-08-02T22:05:12.176338Z","shell.execute_reply.started":"2021-08-02T22:05:12.167944Z","shell.execute_reply":"2021-08-02T22:05:12.175089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Data Visualization </strong></center></h1>\n\n\n   \n        \n</div>","metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Univariate Analysis ","metadata":{}},{"cell_type":"markdown","source":"Distribution of state","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(districts_data.state)\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:12.935975Z","iopub.execute_input":"2021-08-02T22:05:12.936584Z","iopub.status.idle":"2021-08-02T22:05:13.353888Z","shell.execute_reply.started":"2021-08-02T22:05:12.936549Z","shell.execute_reply":"2021-08-02T22:05:13.353057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"districts_data[\"state\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:13.3558Z","iopub.execute_input":"2021-08-02T22:05:13.356265Z","iopub.status.idle":"2021-08-02T22:05:13.825823Z","shell.execute_reply.started":"2021-08-02T22:05:13.356219Z","shell.execute_reply":"2021-08-02T22:05:13.824655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of locale","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(districts_data.locale)\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:13.827517Z","iopub.execute_input":"2021-08-02T22:05:13.827823Z","iopub.status.idle":"2021-08-02T22:05:14.012078Z","shell.execute_reply.started":"2021-08-02T22:05:13.827795Z","shell.execute_reply":"2021-08-02T22:05:14.01082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"districts_data[\"locale\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:14.013818Z","iopub.execute_input":"2021-08-02T22:05:14.014288Z","iopub.status.idle":"2021-08-02T22:05:14.302337Z","shell.execute_reply.started":"2021-08-02T22:05:14.014242Z","shell.execute_reply":"2021-08-02T22:05:14.3012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of pct_black/hispanic","metadata":{}},{"cell_type":"code","source":"sns.countplot(data= districts_data, x = \"pct_black/hispanic\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:14.304616Z","iopub.execute_input":"2021-08-02T22:05:14.305191Z","iopub.status.idle":"2021-08-02T22:05:14.469063Z","shell.execute_reply.started":"2021-08-02T22:05:14.305138Z","shell.execute_reply":"2021-08-02T22:05:14.467913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pct_free/reduced","metadata":{}},{"cell_type":"code","source":"sns.countplot(data= districts_data, x = \"pct_free/reduced\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:14.567292Z","iopub.execute_input":"2021-08-02T22:05:14.567649Z","iopub.status.idle":"2021-08-02T22:05:14.714231Z","shell.execute_reply.started":"2021-08-02T22:05:14.567619Z","shell.execute_reply":"2021-08-02T22:05:14.713398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"county_connections_ratio","metadata":{}},{"cell_type":"code","source":"sns.countplot(data= districts_data, x = \"county_connections_ratio\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:14.935861Z","iopub.execute_input":"2021-08-02T22:05:14.936238Z","iopub.status.idle":"2021-08-02T22:05:15.087982Z","shell.execute_reply.started":"2021-08-02T22:05:14.936205Z","shell.execute_reply":"2021-08-02T22:05:15.086945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(districts_data.pp_total_raw)\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:15.159193Z","iopub.execute_input":"2021-08-02T22:05:15.159543Z","iopub.status.idle":"2021-08-02T22:05:15.443286Z","shell.execute_reply.started":"2021-08-02T22:05:15.159515Z","shell.execute_reply":"2021-08-02T22:05:15.442039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Products data","metadata":{}},{"cell_type":"code","source":"products_data = pd.read_csv(\"../input/learnplatform-covid19-impact-on-digital-learning/products_info.csv\")\nproducts_data","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:15.652715Z","iopub.execute_input":"2021-08-02T22:05:15.65308Z","iopub.status.idle":"2021-08-02T22:05:15.678822Z","shell.execute_reply.started":"2021-08-02T22:05:15.653048Z","shell.execute_reply":"2021-08-02T22:05:15.677793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of Sector(s) in the District Information Data","metadata":{}},{"cell_type":"code","source":"ds = products_data['Sector(s)'].value_counts().reset_index()\nds.columns = [\n    'Sector(s)', \n    'percent'\n]\nds['percent'] /= len(products_data)\n\nfig = px.pie(\n    ds, \n    names='Sector(s)', \n    values='percent',\n    color_discrete_sequence=px.colors.sequential.Mint,\n    title='Distribution of Sector(s) in the District Information Data:', \n    width=700,\n    height=500\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:16.034641Z","iopub.execute_input":"2021-08-02T22:05:16.03502Z","iopub.status.idle":"2021-08-02T22:05:16.307441Z","shell.execute_reply.started":"2021-08-02T22:05:16.034989Z","shell.execute_reply":"2021-08-02T22:05:16.30635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### districts state wordcloud","metadata":{}},{"cell_type":"code","source":"cloud = WordCloud(width=1440, height=1080).generate(\" \".join(districts_data['state'].astype(str)))\nplt.figure(figsize=(15, 10))\nplt.imshow(cloud)\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:16.486466Z","iopub.execute_input":"2021-08-02T22:05:16.486811Z","iopub.status.idle":"2021-08-02T22:05:17.566233Z","shell.execute_reply.started":"2021-08-02T22:05:16.486782Z","shell.execute_reply":"2021-08-02T22:05:17.563955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Occurrence of states in the District Information Data","metadata":{}},{"cell_type":"code","source":"ds = districts_data['state'].value_counts().reset_index()\nds.columns = [\n    'state', \n    'percent'\n]\nds['percent'] /= len(districts_data)\n\nfig = px.pie(\n    ds, \n    names='state', \n    values='percent',\n    color_discrete_sequence=px.colors.sequential.Mint,\n    title='Occurrence of states in the District Information Data:', \n    width=700,\n    height=500\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:17.56817Z","iopub.execute_input":"2021-08-02T22:05:17.568629Z","iopub.status.idle":"2021-08-02T22:05:17.630353Z","shell.execute_reply.started":"2021-08-02T22:05:17.568583Z","shell.execute_reply":"2021-08-02T22:05:17.62937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Occurrence of Locale in the District Information Data","metadata":{}},{"cell_type":"code","source":"ds = districts_data['locale'].value_counts().reset_index()\nds.columns = [\n    'locale', \n    'percent'\n]\nds['percent'] /= len(districts_data)\n\nfig = px.pie(\n    ds, \n    names='locale', \n    values='percent',\n    color_discrete_sequence=px.colors.sequential.Mint,\n    title='Occurrence of Locale in the District Information Data:', \n    width=700,\n    height=500\n)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:18.229385Z","iopub.execute_input":"2021-08-02T22:05:18.229778Z","iopub.status.idle":"2021-08-02T22:05:18.289692Z","shell.execute_reply.started":"2021-08-02T22:05:18.229733Z","shell.execute_reply":"2021-08-02T22:05:18.288628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading All Engagement Files","metadata":{}},{"cell_type":"code","source":"CSV_files=pd.DataFrame()\naddress = glob.glob('../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/*.csv')\ncount=0\nfor i in address:\n    with open(i, \"rb\") as data_of_files:\n        data=pd.read_csv(data_of_files)\n        CSV_files=pd.concat([CSV_files,data], axis=0)\n        count=count+1\n        if count==233:\n            break  \nCSV_files","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:05:18.291184Z","iopub.execute_input":"2021-08-02T22:05:18.291508Z","iopub.status.idle":"2021-08-02T22:07:27.465127Z","shell.execute_reply.started":"2021-08-02T22:05:18.291479Z","shell.execute_reply":"2021-08-02T22:07:27.461207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numeric features distrubution ","metadata":{}},{"cell_type":"code","source":"CSV_files.hist(figsize=(20,20),bins = 20, color=\"#107009AA\")\nplt.title(\"Numeric Features Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:07:27.467551Z","iopub.execute_input":"2021-08-02T22:07:27.467876Z","iopub.status.idle":"2021-08-02T22:07:30.685567Z","shell.execute_reply.started":"2021-08-02T22:07:27.467843Z","shell.execute_reply":"2021-08-02T22:07:30.684387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bivariate Analysis\n","metadata":{}},{"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(CSV_files.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:07:30.687396Z","iopub.execute_input":"2021-08-02T22:07:30.687725Z","iopub.status.idle":"2021-08-02T22:07:32.289986Z","shell.execute_reply.started":"2021-08-02T22:07:30.687692Z","shell.execute_reply":"2021-08-02T22:07:32.288881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing value Treatment\n\n","metadata":{}},{"cell_type":"code","source":"CSV_files.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:07:32.291361Z","iopub.execute_input":"2021-08-02T22:07:32.291673Z","iopub.status.idle":"2021-08-02T22:07:34.609477Z","shell.execute_reply.started":"2021-08-02T22:07:32.291644Z","shell.execute_reply":"2021-08-02T22:07:34.608366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lets calculate the total missing values in the each column","metadata":{}},{"cell_type":"code","source":"data_total = CSV_files.isnull().sum()\ndata_percent = ((CSV_files.isnull().sum()/CSV_files.shape[0])*100).round(2)\nmissing_data = pd.concat([data_total, data_percent],\n                                axis=1, \n                                keys=['Data_Total', 'Data_Percent %'],\n                                sort = True)\nmissing_data.style.bar(color = ['gold'])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:07:34.611935Z","iopub.execute_input":"2021-08-02T22:07:34.612405Z","iopub.status.idle":"2021-08-02T22:07:39.286994Z","shell.execute_reply.started":"2021-08-02T22:07:34.612334Z","shell.execute_reply":"2021-08-02T22:07:39.285997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combining the data","metadata":{}},{"cell_type":"code","source":"print(products_data[\"LP ID\"].nunique())\nprint(CSV_files[\"lp_id\"].nunique())","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:07:39.288361Z","iopub.execute_input":"2021-08-02T22:07:39.288908Z","iopub.status.idle":"2021-08-02T22:07:39.769592Z","shell.execute_reply.started":"2021-08-02T22:07:39.288865Z","shell.execute_reply":"2021-08-02T22:07:39.76835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"products_engagement_data = pd.merge(products_data, CSV_files, left_on='LP ID', right_on='lp_id')\nproducts_engagement_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T22:07:39.771934Z","iopub.execute_input":"2021-08-02T22:07:39.772314Z","iopub.status.idle":"2021-08-02T22:07:51.471418Z","shell.execute_reply.started":"2021-08-02T22:07:39.772279Z","shell.execute_reply":"2021-08-02T22:07:51.470272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## More code is coming Soon. Please upvote if you like the work and you will get notifications with additions. \n\n<center><img src=\"https://thumbs.gfycat.com/AshamedWeightyDachshund-max-1mb.gif\"></center>","metadata":{}}]}
\ No newline at end of file
+{
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat_minor": 4,
+ "nbformat": 4,
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-info\">  \n    <h3><strong> Getting Started with LearnPlatform COVID-19 Impact on Digital Learning\nUse digital learning data to analyze the impact of COVID-19 on student lea</strong></h3>\n    <i></i>\n</div>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# <center><img src=\"https://akm-img-a-in.tosshub.com/businesstoday/images/story/201712/data-analytics_660_120717022837.jpg\"></center>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-success\">  \n \n<hr>\n<b>Source of Data: </b> \n<hr> \n <a href=\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/datahttps://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\">https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data</a>\n   \n</div>",
+   "metadata": {
+    "papermill": {
+     "duration": 0.020638,
+     "end_time": "2020-12-02T21:20:36.560772",
+     "exception": false,
+     "start_time": "2020-12-02T21:20:36.540134",
+     "status": "completed"
+    },
+    "tags": []
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-info\">  \n \n<hr>\n<b>Data Description: </b> \n<hr> \n    <p>We include three basic sets of files to help you get started. The engagement data are based on LearnPlatforms Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district. The product file includes information about the characteristics of the top 372 products with most users in 2020. The district file includes information about the characteristics of school districts, including data from National Center for Education Statistics (NCES), The Federal Communications Commission (FCC), and Edunomics Lab.</p>\n <a href=\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\">https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data</a>\n   \n</div>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Importing Python Libraries    </strong></center></h2>\n        \n</div>",
+   "metadata": {
+    "papermill": {
+     "duration": 0.065466,
+     "end_time": "2020-11-30T07:38:51.578836",
+     "exception": false,
+     "start_time": "2020-11-30T07:38:51.51337",
+     "status": "completed"
+    },
+    "tags": []
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "- Libraries are important and we call them to perform the different actions on our data and for training the models.\n- Its a first step to load the library to perform the specific task",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "import pandas as pd\nimport numpy as np  \nimport seaborn as sns \npal = sns.color_palette()\nfrom wordcloud import WordCloud\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nimport glob\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout",
+   "metadata": {
+    "id": "-E4TvIVLMa73",
+    "scrolled": true,
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:01.792658Z",
+     "iopub.execute_input": "2021-08-02T22:05:01.793408Z",
+     "iopub.status.idle": "2021-08-02T22:05:05.188735Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:01.793297Z",
+     "shell.execute_reply": "2021-08-02T22:05:05.1879Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h4>How we can install the libraries in python?</h4>",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<h4>To install the python library is very easy</h4>\n- pip install name_of_library \n<h5> Like if you wanted to install tensorflow? </h5>\n- pip install tensforflow",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Loading the data  </strong></center></h2>\n        \n</div>",
+   "metadata": {
+    "papermill": {
+     "duration": 0.065466,
+     "end_time": "2020-11-30T07:38:51.578836",
+     "exception": false,
+     "start_time": "2020-11-30T07:38:51.51337",
+     "status": "completed"
+    },
+    "tags": []
+   }
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data=pd.read_csv(\"../input/learnplatform-covid19-impact-on-digital-learning/districts_info.csv\")",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:07.497469Z",
+     "iopub.execute_input": "2021-08-02T22:05:07.498066Z",
+     "iopub.status.idle": "2021-08-02T22:05:07.510311Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:07.498031Z",
+     "shell.execute_reply": "2021-08-02T22:05:07.509154Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-danger\">  \n<h2><center><strong>Exploratory data analysis  </strong></center></h2>\n        \n</div>",
+   "metadata": {
+    "papermill": {
+     "duration": 0.065466,
+     "end_time": "2020-11-30T07:38:51.578836",
+     "exception": false,
+     "start_time": "2020-11-30T07:38:51.51337",
+     "status": "completed"
+    },
+    "tags": []
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Five top records of data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data.head()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:07.932198Z",
+     "iopub.execute_input": "2021-08-02T22:05:07.932577Z",
+     "iopub.status.idle": "2021-08-02T22:05:07.956583Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:07.932546Z",
+     "shell.execute_reply": "2021-08-02T22:05:07.95568Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Five last records of data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data.tail()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:08.265237Z",
+     "iopub.execute_input": "2021-08-02T22:05:08.265793Z",
+     "iopub.status.idle": "2021-08-02T22:05:08.280731Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:08.265745Z",
+     "shell.execute_reply": "2021-08-02T22:05:08.279699Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Coloumns/features in data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data.columns",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:08.691185Z",
+     "iopub.execute_input": "2021-08-02T22:05:08.69155Z",
+     "iopub.status.idle": "2021-08-02T22:05:08.698283Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:08.69152Z",
+     "shell.execute_reply": "2021-08-02T22:05:08.697226Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Length of data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "print('lenght of data is', len(districts_data))",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:09.099852Z",
+     "iopub.execute_input": "2021-08-02T22:05:09.100255Z",
+     "iopub.status.idle": "2021-08-02T22:05:09.105603Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:09.10022Z",
+     "shell.execute_reply": "2021-08-02T22:05:09.10456Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Shape of data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data.shape",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:09.474064Z",
+     "iopub.execute_input": "2021-08-02T22:05:09.474641Z",
+     "iopub.status.idle": "2021-08-02T22:05:09.481442Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:09.474605Z",
+     "shell.execute_reply": "2021-08-02T22:05:09.480442Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Data information",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data.info()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:09.826782Z",
+     "iopub.execute_input": "2021-08-02T22:05:09.827404Z",
+     "iopub.status.idle": "2021-08-02T22:05:09.844905Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:09.827356Z",
+     "shell.execute_reply": "2021-08-02T22:05:09.843552Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Data types of all coloumns",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data.dtypes",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:10.275442Z",
+     "iopub.execute_input": "2021-08-02T22:05:10.275798Z",
+     "iopub.status.idle": "2021-08-02T22:05:10.284211Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:10.275768Z",
+     "shell.execute_reply": "2021-08-02T22:05:10.282936Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Checking Null values / missing values",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "np.sum(districts_data.isnull().any(axis=1))",
+   "metadata": {
+    "scrolled": true,
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:10.650892Z",
+     "iopub.execute_input": "2021-08-02T22:05:10.651266Z",
+     "iopub.status.idle": "2021-08-02T22:05:10.660402Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:10.651233Z",
+     "shell.execute_reply": "2021-08-02T22:05:10.659392Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "#### Rows and columns in the dataset",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "print('Count of columns in the data is:  ', len(districts_data.columns))",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:10.926482Z",
+     "iopub.execute_input": "2021-08-02T22:05:10.926859Z",
+     "iopub.status.idle": "2021-08-02T22:05:10.93196Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:10.926827Z",
+     "shell.execute_reply": "2021-08-02T22:05:10.93092Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "print('Count of rows in the data is:  ', len(districts_data))",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:11.128309Z",
+     "iopub.execute_input": "2021-08-02T22:05:11.12898Z",
+     "iopub.status.idle": "2021-08-02T22:05:11.1344Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:11.128944Z",
+     "shell.execute_reply": "2021-08-02T22:05:11.133398Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Deleting the duplicate rows",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "current=len(districts_data)\nprint('Rows of data before Delecting ', current)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:11.523298Z",
+     "iopub.execute_input": "2021-08-02T22:05:11.523699Z",
+     "iopub.status.idle": "2021-08-02T22:05:11.528882Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:11.523662Z",
+     "shell.execute_reply": "2021-08-02T22:05:11.528087Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data=districts_data.drop_duplicates()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:11.750441Z",
+     "iopub.execute_input": "2021-08-02T22:05:11.75102Z",
+     "iopub.status.idle": "2021-08-02T22:05:11.757892Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:11.750968Z",
+     "shell.execute_reply": "2021-08-02T22:05:11.757012Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "now=len(districts_data)\nprint('Rows of data before Delecting ', now)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:12.019946Z",
+     "iopub.execute_input": "2021-08-02T22:05:12.020339Z",
+     "iopub.status.idle": "2021-08-02T22:05:12.026657Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:12.020301Z",
+     "shell.execute_reply": "2021-08-02T22:05:12.025441Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "diff=current-now\nprint('Duplicated rows deleted ', diff)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:12.167608Z",
+     "iopub.execute_input": "2021-08-02T22:05:12.167974Z",
+     "iopub.status.idle": "2021-08-02T22:05:12.176338Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:12.167944Z",
+     "shell.execute_reply": "2021-08-02T22:05:12.175089Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "<div class=\"alert alert-block alert-danger\">  \n<h1><center><strong>Data Visualization </strong></center></h1>\n\n\n   \n        \n</div>",
+   "metadata": {
+    "papermill": {
+     "duration": 0.065466,
+     "end_time": "2020-11-30T07:38:51.578836",
+     "exception": false,
+     "start_time": "2020-11-30T07:38:51.51337",
+     "status": "completed"
+    },
+    "tags": []
+   }
+  },
+  {
+   "cell_type": "markdown",
+   "source": "## Univariate Analysis ",
+   "metadata": {}
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Distribution of state",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "plt.figure(figsize=(12,10))\nsns.countplot(districts_data.state)\nplt.xticks(rotation=90)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:12.935975Z",
+     "iopub.execute_input": "2021-08-02T22:05:12.936584Z",
+     "iopub.status.idle": "2021-08-02T22:05:13.353888Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:12.936549Z",
+     "shell.execute_reply": "2021-08-02T22:05:13.353057Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data[\"state\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:13.3558Z",
+     "iopub.execute_input": "2021-08-02T22:05:13.356265Z",
+     "iopub.status.idle": "2021-08-02T22:05:13.825823Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:13.356219Z",
+     "shell.execute_reply": "2021-08-02T22:05:13.824655Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Distribution of locale",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "plt.figure(figsize=(12,10))\nsns.countplot(districts_data.locale)\nplt.xticks(rotation=90)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:13.827517Z",
+     "iopub.execute_input": "2021-08-02T22:05:13.827823Z",
+     "iopub.status.idle": "2021-08-02T22:05:14.012078Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:13.827795Z",
+     "shell.execute_reply": "2021-08-02T22:05:14.01082Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "districts_data[\"locale\"].value_counts().head(10).plot(kind = 'pie', autopct='%1.1f%%', figsize=(10, 10), startangle=0).legend()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:14.013818Z",
+     "iopub.execute_input": "2021-08-02T22:05:14.014288Z",
+     "iopub.status.idle": "2021-08-02T22:05:14.302337Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:14.014242Z",
+     "shell.execute_reply": "2021-08-02T22:05:14.3012Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "Distribution of pct_black/hispanic",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "sns.countplot(data= districts_data, x = \"pct_black/hispanic\")\nplt.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:14.304616Z",
+     "iopub.execute_input": "2021-08-02T22:05:14.305191Z",
+     "iopub.status.idle": "2021-08-02T22:05:14.469063Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:14.305138Z",
+     "shell.execute_reply": "2021-08-02T22:05:14.467913Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "pct_free/reduced",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "sns.countplot(data= districts_data, x = \"pct_free/reduced\")\nplt.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:14.567292Z",
+     "iopub.execute_input": "2021-08-02T22:05:14.567649Z",
+     "iopub.status.idle": "2021-08-02T22:05:14.714231Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:14.567619Z",
+     "shell.execute_reply": "2021-08-02T22:05:14.713398Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "county_connections_ratio",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "sns.countplot(data= districts_data, x = \"county_connections_ratio\")\nplt.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:14.935861Z",
+     "iopub.execute_input": "2021-08-02T22:05:14.936238Z",
+     "iopub.status.idle": "2021-08-02T22:05:15.087982Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:14.936205Z",
+     "shell.execute_reply": "2021-08-02T22:05:15.086945Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "plt.figure(figsize=(12,10))\nsns.countplot(districts_data.pp_total_raw)\nplt.xticks(rotation=90)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:15.159193Z",
+     "iopub.execute_input": "2021-08-02T22:05:15.159543Z",
+     "iopub.status.idle": "2021-08-02T22:05:15.443286Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:15.159515Z",
+     "shell.execute_reply": "2021-08-02T22:05:15.442039Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "## Loading the Products data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "products_data = pd.read_csv(\"../input/learnplatform-covid19-impact-on-digital-learning/products_info.csv\")\nproducts_data",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:15.652715Z",
+     "iopub.execute_input": "2021-08-02T22:05:15.65308Z",
+     "iopub.status.idle": "2021-08-02T22:05:15.678822Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:15.653048Z",
+     "shell.execute_reply": "2021-08-02T22:05:15.677793Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Distribution of Sector(s) in the District Information Data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "ds = products_data['Sector(s)'].value_counts().reset_index()\nds.columns = [\n    'Sector(s)', \n    'percent'\n]\nds['percent'] /= len(products_data)\n\nfig = px.pie(\n    ds, \n    names='Sector(s)', \n    values='percent',\n    color_discrete_sequence=px.colors.sequential.Mint,\n    title='Distribution of Sector(s) in the District Information Data:', \n    width=700,\n    height=500\n)\nfig.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:16.034641Z",
+     "iopub.execute_input": "2021-08-02T22:05:16.03502Z",
+     "iopub.status.idle": "2021-08-02T22:05:16.307441Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:16.034989Z",
+     "shell.execute_reply": "2021-08-02T22:05:16.30635Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### districts state wordcloud",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "cloud = WordCloud(width=1440, height=1080).generate(\" \".join(districts_data['state'].astype(str)))\nplt.figure(figsize=(15, 10))\nplt.imshow(cloud)\nplt.axis('off')",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:16.486466Z",
+     "iopub.execute_input": "2021-08-02T22:05:16.486811Z",
+     "iopub.status.idle": "2021-08-02T22:05:17.566233Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:16.486782Z",
+     "shell.execute_reply": "2021-08-02T22:05:17.563955Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Occurrence of states in the District Information Data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "ds = districts_data['state'].value_counts().reset_index()\nds.columns = [\n    'state', \n    'percent'\n]\nds['percent'] /= len(districts_data)\n\nfig = px.pie(\n    ds, \n    names='state', \n    values='percent',\n    color_discrete_sequence=px.colors.sequential.Mint,\n    title='Occurrence of states in the District Information Data:', \n    width=700,\n    height=500\n)\nfig.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:17.56817Z",
+     "iopub.execute_input": "2021-08-02T22:05:17.568629Z",
+     "iopub.status.idle": "2021-08-02T22:05:17.630353Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:17.568583Z",
+     "shell.execute_reply": "2021-08-02T22:05:17.62937Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Occurrence of Locale in the District Information Data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "ds = districts_data['locale'].value_counts().reset_index()\nds.columns = [\n    'locale', \n    'percent'\n]\nds['percent'] /= len(districts_data)\n\nfig = px.pie(\n    ds, \n    names='locale', \n    values='percent',\n    color_discrete_sequence=px.colors.sequential.Mint,\n    title='Occurrence of Locale in the District Information Data:', \n    width=700,\n    height=500\n)\nfig.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:18.229385Z",
+     "iopub.execute_input": "2021-08-02T22:05:18.229778Z",
+     "iopub.status.idle": "2021-08-02T22:05:18.289692Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:18.229733Z",
+     "shell.execute_reply": "2021-08-02T22:05:18.288628Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "# Loading All Engagement Files",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "CSV_files=pd.DataFrame()\naddress = glob.glob('../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/*.csv')\ncount=0\nfor i in address:\n    with open(i, \"rb\") as data_of_files:\n        data=pd.read_csv(data_of_files)\n        CSV_files=pd.concat([CSV_files,data], axis=0)\n        count=count+1\n        if count==233:\n            break  \nCSV_files",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:05:18.291184Z",
+     "iopub.execute_input": "2021-08-02T22:05:18.291508Z",
+     "iopub.status.idle": "2021-08-02T22:07:27.465127Z",
+     "shell.execute_reply.started": "2021-08-02T22:05:18.291479Z",
+     "shell.execute_reply": "2021-08-02T22:07:27.461207Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Numeric features distrubution ",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "CSV_files.hist(figsize=(20,20),bins = 20, color=\"#107009AA\")\nplt.title(\"Numeric Features Distribution\")\nplt.show()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:07:27.467551Z",
+     "iopub.execute_input": "2021-08-02T22:07:27.467876Z",
+     "iopub.status.idle": "2021-08-02T22:07:30.685567Z",
+     "shell.execute_reply.started": "2021-08-02T22:07:27.467843Z",
+     "shell.execute_reply": "2021-08-02T22:07:30.684387Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Bivariate Analysis\n",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(CSV_files.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:07:30.687396Z",
+     "iopub.execute_input": "2021-08-02T22:07:30.687725Z",
+     "iopub.status.idle": "2021-08-02T22:07:32.289986Z",
+     "shell.execute_reply.started": "2021-08-02T22:07:30.687692Z",
+     "shell.execute_reply": "2021-08-02T22:07:32.288881Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### Missing value Treatment\n\n",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "CSV_files.isnull().sum()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:07:32.291361Z",
+     "iopub.execute_input": "2021-08-02T22:07:32.291673Z",
+     "iopub.status.idle": "2021-08-02T22:07:34.609477Z",
+     "shell.execute_reply.started": "2021-08-02T22:07:32.291644Z",
+     "shell.execute_reply": "2021-08-02T22:07:34.608366Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "### lets calculate the total missing values in the each column",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "data_total = CSV_files.isnull().sum()\ndata_percent = ((CSV_files.isnull().sum()/CSV_files.shape[0])*100).round(2)\nmissing_data = pd.concat([data_total, data_percent],\n                                axis=1, \n                                keys=['Data_Total', 'Data_Percent %'],\n                                sort = True)\nmissing_data.style.bar(color = ['gold'])",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:07:34.611935Z",
+     "iopub.execute_input": "2021-08-02T22:07:34.612405Z",
+     "iopub.status.idle": "2021-08-02T22:07:39.286994Z",
+     "shell.execute_reply.started": "2021-08-02T22:07:34.612334Z",
+     "shell.execute_reply": "2021-08-02T22:07:39.285997Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "## Combining the data",
+   "metadata": {}
+  },
+  {
+   "cell_type": "code",
+   "source": "print(products_data[\"LP ID\"].nunique())\nprint(CSV_files[\"lp_id\"].nunique())",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:07:39.288361Z",
+     "iopub.execute_input": "2021-08-02T22:07:39.288908Z",
+     "iopub.status.idle": "2021-08-02T22:07:39.769592Z",
+     "shell.execute_reply.started": "2021-08-02T22:07:39.288865Z",
+     "shell.execute_reply": "2021-08-02T22:07:39.76835Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "code",
+   "source": "products_engagement_data = pd.merge(products_data, CSV_files, left_on='LP ID', right_on='lp_id')\nproducts_engagement_data.head()",
+   "metadata": {
+    "execution": {
+     "iopub.status.busy": "2021-08-02T22:07:39.771934Z",
+     "iopub.execute_input": "2021-08-02T22:07:39.772314Z",
+     "iopub.status.idle": "2021-08-02T22:07:51.471418Z",
+     "shell.execute_reply.started": "2021-08-02T22:07:39.772279Z",
+     "shell.execute_reply": "2021-08-02T22:07:51.470272Z"
+    },
+    "trusted": true
+   },
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "cell_type": "markdown",
+   "source": "## More code is coming Soon. Please upvote if you like the work and you will get notifications with additions. \n\n<center><img src=\"https://thumbs.gfycat.com/AshamedWeightyDachshund-max-1mb.gif\"></center>",
+   "metadata": {}
+  }
+ ]
+}
\ No newline at end of file
Index: dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# YOLOv5 Pseudo Labeling\\n\\nAccording to the results of [this notebook](https://www.kaggle.com/nvnnghia/fasterrcnn-pseudo-labeling) FaterRCNN seems to work well with Pseudo Labeling.\\nIn this notebook I am going to test Pseudo labeling technique on Yolov5.\",\"execution_count\":null},{\"metadata\":{\"_uuid\":\"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\"_cell_guid\":\"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\nimport numpy as np # linear algebra\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\nimport os\\nfrom tqdm.auto import tqdm\\nimport shutil as sh\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Getting yolov5 repo \",\"execution_count\":null},{\"metadata\":{\"_uuid\":\"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a\",\"_cell_guid\":\"79c7e3d0-c299-4dcb-8224-4455121ee9b0\",\"trusted\":true,\"_kg_hide-input\":true},\"cell_type\":\"code\",\"source\":\"#!git clone https://github.com/ultralytics/yolov5\\n#!mv yolov5/* ./\\n\\n!cp -r ../input/yolov5train/* .\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"!pip install --no-deps '../input/weightedboxesfusion/' > /dev/null\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Convert train data to yolov5 format\\nBased on [this notebook](https://www.kaggle.com/orkatz2/yolov5-train)\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def convertTrainLabel():\\n    df = pd.read_csv('../input/global-wheat-detection/train.csv')\\n    bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\\n    for i, column in enumerate(['x', 'y', 'w', 'h']):\\n        df[column] = bboxs[:,i]\\n    df.drop(columns=['bbox'], inplace=True)\\n    df['x_center'] = df['x'] + df['w']/2\\n    df['y_center'] = df['y'] + df['h']/2\\n    df['classes'] = 0\\n    from tqdm.auto import tqdm\\n    import shutil as sh\\n    df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\\n    \\n    index = list(set(df.image_id))\\n    \\n    source = 'train'\\n    if True:\\n        for fold in [0]:\\n            val_index = index[len(index)*fold//5:len(index)*(fold+1)//5]\\n            for name,mini in tqdm(df.groupby('image_id')):\\n                if name in val_index:\\n                    path2save = 'val2017/'\\n                else:\\n                    path2save = 'train2017/'\\n                if not os.path.exists('convertor/fold{}/labels/'.format(fold)+path2save):\\n                    os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\\n                with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\\\".txt\\\", 'w+') as f:\\n                    row = mini[['classes','x_center','y_center','w','h']].astype(float).values\\n                    row = row/1024\\n                    row = row.astype(str)\\n                    for j in range(len(row)):\\n                        text = ' '.join(row[j])\\n                        f.write(text)\\n                        f.write(\\\"\\\\n\\\")\\n                if not os.path.exists('convertor/fold{}/images/{}'.format(fold,path2save)):\\n                    os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\\n                sh.copy(\\\"../input/global-wheat-detection/{}/{}.jpg\\\".format(source,name),'convertor/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))\\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Some useful functions\\nTTA, WBF, etc\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from ensemble_boxes import *\\ndef run_wbf(boxes, scores, image_size=1023, iou_thr=0.5, skip_box_thr=0.7, weights=None):\\n    #boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\\n    #scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\\n    labels = [np.zeros(score.shape[0]) for score in scores]\\n    boxes = [box/(image_size) for box in boxes]\\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\\n    #boxes, scores, labels = nms(boxes, scores, labels, weights=[1,1,1,1,1], iou_thr=0.5)\\n    boxes = boxes*(image_size)\\n    return boxes, scores, labels\\n\\ndef TTAImage(image, index):\\n    image1 = image.copy()\\n    if index==0: \\n        rotated_image = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\\n        return rotated_image\\n    elif index==1:\\n        rotated_image2 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\\n        rotated_image2 = cv2.rotate(rotated_image2, cv2.ROTATE_90_CLOCKWISE)\\n        return rotated_image2\\n    elif index==2:\\n        rotated_image3 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\\n        return rotated_image3\\n    elif index == 3:\\n        return image1\\n    \\ndef rotBoxes90(boxes, im_w, im_h):\\n    ret_boxes =[]\\n    for box in boxes:\\n        x1, y1, x2, y2 = box\\n        x1, y1, x2, y2 = x1-im_w//2, im_h//2 - y1, x2-im_w//2, im_h//2 - y2\\n        x1, y1, x2, y2 = y1, -x1, y2, -x2\\n        x1, y1, x2, y2 = int(x1+im_w//2), int(im_h//2 - y1), int(x2+im_w//2), int(im_h//2 - y2)\\n        x1a, y1a, x2a, y2a = min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)\\n        ret_boxes.append([x1a, y1a, x2a, y2a])\\n    return np.array(ret_boxes)\\n\\ndef detect1Image(im0, imgsz, model, device, conf_thres, iou_thres):\\n    img = letterbox(im0, new_shape=imgsz)[0]\\n    # Convert\\n    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\\n    img = np.ascontiguousarray(img)\\n\\n\\n    img = torch.from_numpy(img).to(device)\\n    img =  img.float()  # uint8 to fp16/32\\n    img /= 255.0   \\n    if img.ndimension() == 3:\\n        img = img.unsqueeze(0)\\n\\n    # Inference\\n    pred = model(img, augment=False)[0]\\n\\n    # Apply NMS\\n    pred = non_max_suppression(pred, conf_thres, iou_thres)\\n\\n    boxes = []\\n    scores = []\\n    for i, det in enumerate(pred):  # detections per image\\n        # save_path = 'draw/' + image_id + '.jpg'\\n        if det is not None and len(det):\\n            # Rescale boxes from img_size to im0 size\\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\\n\\n            # Write results\\n            for *xyxy, conf, cls in det:\\n                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\\n                scores.append(conf)\\n\\n    return np.array(boxes), np.array(scores) \",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Make pseudo labels for Yolov5\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"from utils.datasets import *\\nfrom utils.utils import *\\n\\ndef makePseudolabel():\\n    source = '../input/global-wheat-detection/test/'\\n    weights = '../input/yolov5/bestv4.pt'\\n    imgsz = 1024\\n    conf_thres = 0.5\\n    iou_thres = 0.6\\n    is_TTA = True\\n    \\n    imagenames =  os.listdir(source)\\n    \\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\\n\\n    # Load model\\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\\n    model.to(device).eval()\\n    \\n    dataset = LoadImages(source, img_size=imgsz)\\n\\n    path2save = 'train2017/'\\n    if not os.path.exists('convertor/fold0/labels/'+path2save):\\n        os.makedirs('convertor/fold0/labels/'+path2save)\\n    if not os.path.exists('convertor/fold0/images/{}'.format(path2save)):\\n        os.makedirs('convertor/fold0/images/{}'.format(path2save))\\n            \\n    for name in imagenames:\\n        image_id = name.split('.')[0]\\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\\n        if im01.shape[0]!=1024 or im01.shape[1]!=1024:\\n            continue\\n        assert im01 is not None, 'Image Not Found '\\n        # Padded resize\\n        im_w, im_h = im01.shape[:2]\\n        if is_TTA:\\n            enboxes = []\\n            enscores = []\\n            for i in range(4):\\n                im0 = TTAImage(im01, i)\\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\\n                for _ in range(3-i):\\n                    boxes = rotBoxes90(boxes, im_w, im_h)\\n                    \\n                enboxes.append(boxes)\\n                enscores.append(scores) \\n\\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.43)\\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\\n        else:\\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\\n\\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\\n        \\n        boxes = boxes[scores >= 0.05].astype(np.int32)\\n        scores = scores[scores >=float(0.05)]\\n        \\n        lineo = ''\\n        for box in boxes:\\n            x1, y1, w, h = box\\n            xc, yc, w, h = (x1+w/2)/1024, (y1+h/2)/1024, w/1024, h/1024\\n            lineo += '0 %f %f %f %f\\\\n'%(xc, yc, w, h)\\n            \\n        fileo = open('convertor/fold0/labels/'+path2save+image_id+\\\".txt\\\", 'w+')\\n        fileo.write(lineo)\\n        fileo.close()\\n        sh.copy(\\\"../input/global-wheat-detection/test/{}.jpg\\\".format(image_id),'convertor/fold0/images/{}/{}.jpg'.format(path2save,image_id))\\n            \\n\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"\\nconvertTrainLabel()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"makePseudolabel()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"!ls\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Retrain yolov5 with pseudo data\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"if len(os.listdir('../input/global-wheat-detection/test/'))<11:\\n    pass\\n    #!python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml  --weights ../input/yolov5/bestv4.pt   \\nelse:\\n    !python train.py --img 1024 --batch 4 --epochs 10 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml --weights ../input/yolov5/bestv4.pt\\n    \\n    \\n!rm -rf convertor\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def format_prediction_string(boxes, scores):\\n    pred_strings = []\\n    for j in zip(scores, boxes):\\n        pred_strings.append(\\\"{0:.4f} {1} {2} {3} {4}\\\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\\n\\n    return \\\" \\\".join(pred_strings)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{},\"cell_type\":\"markdown\",\"source\":\"# Final prediction\",\"execution_count\":null},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"def detect():\\n    source = '../input/global-wheat-detection/test/'\\n    weights = 'weights/best.pt'\\n    if not os.path.exists(weights):\\n        weights = '../input/yolov5/bestv4.pt'\\n    imgsz = 1024\\n    conf_thres = 0.5\\n    iou_thres = 0.6\\n    is_TTA = True\\n    \\n    imagenames =  os.listdir(source)\\n    \\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\\n\\n    # Load model\\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\\n    model.to(device).eval()\\n    \\n    dataset = LoadImages(source, img_size=imgsz)\\n\\n    results = []\\n    fig, ax = plt.subplots(5, 2, figsize=(30, 70))\\n    count = 0\\n    # img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\\n    #for path, img, im0s, _ in dataset:\\n    for name in imagenames:\\n        image_id = name.split('.')[0]\\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\\n        assert im01 is not None, 'Image Not Found '\\n        # Padded resize\\n        im_w, im_h = im01.shape[:2]\\n        if is_TTA:\\n            enboxes = []\\n            enscores = []\\n            for i in range(4):\\n                im0 = TTAImage(im01, i)\\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\\n                for _ in range(3-i):\\n                    boxes = rotBoxes90(boxes, im_w, im_h)\\n                    \\n                if 1: #i<3:\\n                    enboxes.append(boxes)\\n                    enscores.append(scores) \\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\\n            enboxes.append(boxes)\\n            enscores.append(scores)\\n\\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.5)\\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\\n        else:\\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\\n\\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\\n        \\n        boxes = boxes[scores >= 0.05].astype(np.int32)\\n        scores = scores[scores >=float(0.05)]\\n        if count<10:\\n            #sample = image.permute(1,2,0).cpu().numpy()\\n            for box, score in zip(boxes,scores):\\n                cv2.rectangle(im0,\\n                              (box[0], box[1]),\\n                              (box[2]+box[0], box[3]+box[1]),\\n                              (220, 0, 0), 2)\\n                cv2.putText(im0, '%.2f'%(score), (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX ,  \\n                   0.5, (255,255,255), 2, cv2.LINE_AA)\\n            ax[count%5][count//5].imshow(im0)\\n            count+=1\\n            \\n        result = {\\n            'image_id': image_id,\\n            'PredictionString': format_prediction_string(boxes, scores)\\n        }\\n\\n        results.append(result)\\n    return results\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"trusted\":true},\"cell_type\":\"code\",\"source\":\"results = detect()\\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\\ntest_df.to_csv('submission.csv', index=False)\\ntest_df.head()\",\"execution_count\":null,\"outputs\":[]}],\"metadata\":{\"kernelspec\":{\"language\":\"python\",\"display_name\":\"Python 3\",\"name\":\"python3\"},\"language_info\":{\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"version\":\"3.6.4\",\"file_extension\":\".py\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"name\":\"python\",\"mimetype\":\"text/x-python\"}},\"nbformat\":4,\"nbformat_minor\":4}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb b/dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb
--- a/dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/global-wheat-detection/nvnn/yolov5-pseudo-labeling.ipynb	(date 1658512097905)
@@ -1,1 +1,189 @@
-{"cells":[{"metadata":{},"cell_type":"markdown","source":"# YOLOv5 Pseudo Labeling\n\nAccording to the results of [this notebook](https://www.kaggle.com/nvnnghia/fasterrcnn-pseudo-labeling) FaterRCNN seems to work well with Pseudo Labeling.\nIn this notebook I am going to test Pseudo labeling technique on Yolov5.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tqdm.auto import tqdm\nimport shutil as sh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting yolov5 repo ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#!git clone https://github.com/ultralytics/yolov5\n#!mv yolov5/* ./\n\n!cp -r ../input/yolov5train/* .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --no-deps '../input/weightedboxesfusion/' > /dev/null","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert train data to yolov5 format\nBased on [this notebook](https://www.kaggle.com/orkatz2/yolov5-train)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def convertTrainLabel():\n    df = pd.read_csv('../input/global-wheat-detection/train.csv')\n    bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n    for i, column in enumerate(['x', 'y', 'w', 'h']):\n        df[column] = bboxs[:,i]\n    df.drop(columns=['bbox'], inplace=True)\n    df['x_center'] = df['x'] + df['w']/2\n    df['y_center'] = df['y'] + df['h']/2\n    df['classes'] = 0\n    from tqdm.auto import tqdm\n    import shutil as sh\n    df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\n    \n    index = list(set(df.image_id))\n    \n    source = 'train'\n    if True:\n        for fold in [0]:\n            val_index = index[len(index)*fold//5:len(index)*(fold+1)//5]\n            for name,mini in tqdm(df.groupby('image_id')):\n                if name in val_index:\n                    path2save = 'val2017/'\n                else:\n                    path2save = 'train2017/'\n                if not os.path.exists('convertor/fold{}/labels/'.format(fold)+path2save):\n                    os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\n                with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                    row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n                    row = row/1024\n                    row = row.astype(str)\n                    for j in range(len(row)):\n                        text = ' '.join(row[j])\n                        f.write(text)\n                        f.write(\"\\n\")\n                if not os.path.exists('convertor/fold{}/images/{}'.format(fold,path2save)):\n                    os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\n                sh.copy(\"../input/global-wheat-detection/{}/{}.jpg\".format(source,name),'convertor/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some useful functions\nTTA, WBF, etc","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from ensemble_boxes import *\ndef run_wbf(boxes, scores, image_size=1023, iou_thr=0.5, skip_box_thr=0.7, weights=None):\n    #boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n    #scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n    labels = [np.zeros(score.shape[0]) for score in scores]\n    boxes = [box/(image_size) for box in boxes]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    #boxes, scores, labels = nms(boxes, scores, labels, weights=[1,1,1,1,1], iou_thr=0.5)\n    boxes = boxes*(image_size)\n    return boxes, scores, labels\n\ndef TTAImage(image, index):\n    image1 = image.copy()\n    if index==0: \n        rotated_image = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image\n    elif index==1:\n        rotated_image2 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image2 = cv2.rotate(rotated_image2, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image2\n    elif index==2:\n        rotated_image3 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image3\n    elif index == 3:\n        return image1\n    \ndef rotBoxes90(boxes, im_w, im_h):\n    ret_boxes =[]\n    for box in boxes:\n        x1, y1, x2, y2 = box\n        x1, y1, x2, y2 = x1-im_w//2, im_h//2 - y1, x2-im_w//2, im_h//2 - y2\n        x1, y1, x2, y2 = y1, -x1, y2, -x2\n        x1, y1, x2, y2 = int(x1+im_w//2), int(im_h//2 - y1), int(x2+im_w//2), int(im_h//2 - y2)\n        x1a, y1a, x2a, y2a = min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)\n        ret_boxes.append([x1a, y1a, x2a, y2a])\n    return np.array(ret_boxes)\n\ndef detect1Image(im0, imgsz, model, device, conf_thres, iou_thres):\n    img = letterbox(im0, new_shape=imgsz)[0]\n    # Convert\n    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n    img = np.ascontiguousarray(img)\n\n\n    img = torch.from_numpy(img).to(device)\n    img =  img.float()  # uint8 to fp16/32\n    img /= 255.0   \n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = model(img, augment=False)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres, iou_thres)\n\n    boxes = []\n    scores = []\n    for i, det in enumerate(pred):  # detections per image\n        # save_path = 'draw/' + image_id + '.jpg'\n        if det is not None and len(det):\n            # Rescale boxes from img_size to im0 size\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n            # Write results\n            for *xyxy, conf, cls in det:\n                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\n                scores.append(conf)\n\n    return np.array(boxes), np.array(scores) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make pseudo labels for Yolov5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from utils.datasets import *\nfrom utils.utils import *\n\ndef makePseudolabel():\n    source = '../input/global-wheat-detection/test/'\n    weights = '../input/yolov5/bestv4.pt'\n    imgsz = 1024\n    conf_thres = 0.5\n    iou_thres = 0.6\n    is_TTA = True\n    \n    imagenames =  os.listdir(source)\n    \n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # Load model\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n    model.to(device).eval()\n    \n    dataset = LoadImages(source, img_size=imgsz)\n\n    path2save = 'train2017/'\n    if not os.path.exists('convertor/fold0/labels/'+path2save):\n        os.makedirs('convertor/fold0/labels/'+path2save)\n    if not os.path.exists('convertor/fold0/images/{}'.format(path2save)):\n        os.makedirs('convertor/fold0/images/{}'.format(path2save))\n            \n    for name in imagenames:\n        image_id = name.split('.')[0]\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\n        if im01.shape[0]!=1024 or im01.shape[1]!=1024:\n            continue\n        assert im01 is not None, 'Image Not Found '\n        # Padded resize\n        im_w, im_h = im01.shape[:2]\n        if is_TTA:\n            enboxes = []\n            enscores = []\n            for i in range(4):\n                im0 = TTAImage(im01, i)\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\n                for _ in range(3-i):\n                    boxes = rotBoxes90(boxes, im_w, im_h)\n                    \n                enboxes.append(boxes)\n                enscores.append(scores) \n\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.43)\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\n        else:\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        boxes = boxes[scores >= 0.05].astype(np.int32)\n        scores = scores[scores >=float(0.05)]\n        \n        lineo = ''\n        for box in boxes:\n            x1, y1, w, h = box\n            xc, yc, w, h = (x1+w/2)/1024, (y1+h/2)/1024, w/1024, h/1024\n            lineo += '0 %f %f %f %f\\n'%(xc, yc, w, h)\n            \n        fileo = open('convertor/fold0/labels/'+path2save+image_id+\".txt\", 'w+')\n        fileo.write(lineo)\n        fileo.close()\n        sh.copy(\"../input/global-wheat-detection/test/{}.jpg\".format(image_id),'convertor/fold0/images/{}/{}.jpg'.format(path2save,image_id))\n            \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nconvertTrainLabel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"makePseudolabel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Retrain yolov5 with pseudo data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if len(os.listdir('../input/global-wheat-detection/test/'))<11:\n    pass\n    #!python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml  --weights ../input/yolov5/bestv4.pt   \nelse:\n    !python train.py --img 1024 --batch 4 --epochs 10 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml --weights ../input/yolov5/bestv4.pt\n    \n    \n!rm -rf convertor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect():\n    source = '../input/global-wheat-detection/test/'\n    weights = 'weights/best.pt'\n    if not os.path.exists(weights):\n        weights = '../input/yolov5/bestv4.pt'\n    imgsz = 1024\n    conf_thres = 0.5\n    iou_thres = 0.6\n    is_TTA = True\n    \n    imagenames =  os.listdir(source)\n    \n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # Load model\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n    model.to(device).eval()\n    \n    dataset = LoadImages(source, img_size=imgsz)\n\n    results = []\n    fig, ax = plt.subplots(5, 2, figsize=(30, 70))\n    count = 0\n    # img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    #for path, img, im0s, _ in dataset:\n    for name in imagenames:\n        image_id = name.split('.')[0]\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\n        assert im01 is not None, 'Image Not Found '\n        # Padded resize\n        im_w, im_h = im01.shape[:2]\n        if is_TTA:\n            enboxes = []\n            enscores = []\n            for i in range(4):\n                im0 = TTAImage(im01, i)\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\n                for _ in range(3-i):\n                    boxes = rotBoxes90(boxes, im_w, im_h)\n                    \n                if 1: #i<3:\n                    enboxes.append(boxes)\n                    enscores.append(scores) \n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n            enboxes.append(boxes)\n            enscores.append(scores)\n\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.5)\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\n        else:\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        boxes = boxes[scores >= 0.05].astype(np.int32)\n        scores = scores[scores >=float(0.05)]\n        if count<10:\n            #sample = image.permute(1,2,0).cpu().numpy()\n            for box, score in zip(boxes,scores):\n                cv2.rectangle(im0,\n                              (box[0], box[1]),\n                              (box[2]+box[0], box[3]+box[1]),\n                              (220, 0, 0), 2)\n                cv2.putText(im0, '%.2f'%(score), (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX ,  \n                   0.5, (255,255,255), 2, cv2.LINE_AA)\n            ax[count%5][count//5].imshow(im0)\n            count+=1\n            \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        results.append(result)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = detect()\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# YOLOv5 Pseudo Labeling\n\nAccording to the results of [this notebook](https://www.kaggle.com/nvnnghia/fasterrcnn-pseudo-labeling) FaterRCNN seems to work well with Pseudo Labeling.\nIn this notebook I am going to test Pseudo labeling technique on Yolov5.",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
+    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom tqdm.auto import tqdm\nimport shutil as sh",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Getting yolov5 repo ",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
+    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
+    "trusted": true,
+    "_kg_hide-input": true
+   },
+   "cell_type": "code",
+   "source": "#!git clone https://github.com/ultralytics/yolov5\n#!mv yolov5/* ./\n\n!cp -r ../input/yolov5train/* .",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "!pip install --no-deps '../input/weightedboxesfusion/' > /dev/null",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Convert train data to yolov5 format\nBased on [this notebook](https://www.kaggle.com/orkatz2/yolov5-train)",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def convertTrainLabel():\n    df = pd.read_csv('../input/global-wheat-detection/train.csv')\n    bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n    for i, column in enumerate(['x', 'y', 'w', 'h']):\n        df[column] = bboxs[:,i]\n    df.drop(columns=['bbox'], inplace=True)\n    df['x_center'] = df['x'] + df['w']/2\n    df['y_center'] = df['y'] + df['h']/2\n    df['classes'] = 0\n    from tqdm.auto import tqdm\n    import shutil as sh\n    df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\n    \n    index = list(set(df.image_id))\n    \n    source = 'train'\n    if True:\n        for fold in [0]:\n            val_index = index[len(index)*fold//5:len(index)*(fold+1)//5]\n            for name,mini in tqdm(df.groupby('image_id')):\n                if name in val_index:\n                    path2save = 'val2017/'\n                else:\n                    path2save = 'train2017/'\n                if not os.path.exists('convertor/fold{}/labels/'.format(fold)+path2save):\n                    os.makedirs('convertor/fold{}/labels/'.format(fold)+path2save)\n                with open('convertor/fold{}/labels/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n                    row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n                    row = row/1024\n                    row = row.astype(str)\n                    for j in range(len(row)):\n                        text = ' '.join(row[j])\n                        f.write(text)\n                        f.write(\"\\n\")\n                if not os.path.exists('convertor/fold{}/images/{}'.format(fold,path2save)):\n                    os.makedirs('convertor/fold{}/images/{}'.format(fold,path2save))\n                sh.copy(\"../input/global-wheat-detection/{}/{}.jpg\".format(source,name),'convertor/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))\n",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Some useful functions\nTTA, WBF, etc",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "from ensemble_boxes import *\ndef run_wbf(boxes, scores, image_size=1023, iou_thr=0.5, skip_box_thr=0.7, weights=None):\n    #boxes = [prediction[image_index]['boxes'].data.cpu().numpy()/(image_size-1) for prediction in predictions]\n    #scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n    labels = [np.zeros(score.shape[0]) for score in scores]\n    boxes = [box/(image_size) for box in boxes]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    #boxes, scores, labels = nms(boxes, scores, labels, weights=[1,1,1,1,1], iou_thr=0.5)\n    boxes = boxes*(image_size)\n    return boxes, scores, labels\n\ndef TTAImage(image, index):\n    image1 = image.copy()\n    if index==0: \n        rotated_image = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image\n    elif index==1:\n        rotated_image2 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image2 = cv2.rotate(rotated_image2, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image2\n    elif index==2:\n        rotated_image3 = cv2.rotate(image1, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n        rotated_image3 = cv2.rotate(rotated_image3, cv2.ROTATE_90_CLOCKWISE)\n        return rotated_image3\n    elif index == 3:\n        return image1\n    \ndef rotBoxes90(boxes, im_w, im_h):\n    ret_boxes =[]\n    for box in boxes:\n        x1, y1, x2, y2 = box\n        x1, y1, x2, y2 = x1-im_w//2, im_h//2 - y1, x2-im_w//2, im_h//2 - y2\n        x1, y1, x2, y2 = y1, -x1, y2, -x2\n        x1, y1, x2, y2 = int(x1+im_w//2), int(im_h//2 - y1), int(x2+im_w//2), int(im_h//2 - y2)\n        x1a, y1a, x2a, y2a = min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)\n        ret_boxes.append([x1a, y1a, x2a, y2a])\n    return np.array(ret_boxes)\n\ndef detect1Image(im0, imgsz, model, device, conf_thres, iou_thres):\n    img = letterbox(im0, new_shape=imgsz)[0]\n    # Convert\n    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n    img = np.ascontiguousarray(img)\n\n\n    img = torch.from_numpy(img).to(device)\n    img =  img.float()  # uint8 to fp16/32\n    img /= 255.0   \n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = model(img, augment=False)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres, iou_thres)\n\n    boxes = []\n    scores = []\n    for i, det in enumerate(pred):  # detections per image\n        # save_path = 'draw/' + image_id + '.jpg'\n        if det is not None and len(det):\n            # Rescale boxes from img_size to im0 size\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n\n            # Write results\n            for *xyxy, conf, cls in det:\n                boxes.append([int(xyxy[0]), int(xyxy[1]), int(xyxy[2]), int(xyxy[3])])\n                scores.append(conf)\n\n    return np.array(boxes), np.array(scores) ",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Make pseudo labels for Yolov5",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "from utils.datasets import *\nfrom utils.utils import *\n\ndef makePseudolabel():\n    source = '../input/global-wheat-detection/test/'\n    weights = '../input/yolov5/bestv4.pt'\n    imgsz = 1024\n    conf_thres = 0.5\n    iou_thres = 0.6\n    is_TTA = True\n    \n    imagenames =  os.listdir(source)\n    \n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # Load model\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n    model.to(device).eval()\n    \n    dataset = LoadImages(source, img_size=imgsz)\n\n    path2save = 'train2017/'\n    if not os.path.exists('convertor/fold0/labels/'+path2save):\n        os.makedirs('convertor/fold0/labels/'+path2save)\n    if not os.path.exists('convertor/fold0/images/{}'.format(path2save)):\n        os.makedirs('convertor/fold0/images/{}'.format(path2save))\n            \n    for name in imagenames:\n        image_id = name.split('.')[0]\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\n        if im01.shape[0]!=1024 or im01.shape[1]!=1024:\n            continue\n        assert im01 is not None, 'Image Not Found '\n        # Padded resize\n        im_w, im_h = im01.shape[:2]\n        if is_TTA:\n            enboxes = []\n            enscores = []\n            for i in range(4):\n                im0 = TTAImage(im01, i)\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\n                for _ in range(3-i):\n                    boxes = rotBoxes90(boxes, im_w, im_h)\n                    \n                enboxes.append(boxes)\n                enscores.append(scores) \n\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.43)\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\n        else:\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        boxes = boxes[scores >= 0.05].astype(np.int32)\n        scores = scores[scores >=float(0.05)]\n        \n        lineo = ''\n        for box in boxes:\n            x1, y1, w, h = box\n            xc, yc, w, h = (x1+w/2)/1024, (y1+h/2)/1024, w/1024, h/1024\n            lineo += '0 %f %f %f %f\\n'%(xc, yc, w, h)\n            \n        fileo = open('convertor/fold0/labels/'+path2save+image_id+\".txt\", 'w+')\n        fileo.write(lineo)\n        fileo.close()\n        sh.copy(\"../input/global-wheat-detection/test/{}.jpg\".format(image_id),'convertor/fold0/images/{}/{}.jpg'.format(path2save,image_id))\n            \n",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "\nconvertTrainLabel()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "makePseudolabel()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "!ls",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Retrain yolov5 with pseudo data",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "if len(os.listdir('../input/global-wheat-detection/test/'))<11:\n    pass\n    #!python train.py --img 1024 --batch 4 --epochs 1 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml  --weights ../input/yolov5/bestv4.pt   \nelse:\n    !python train.py --img 1024 --batch 4 --epochs 10 --data ../input/configyolo5/wheat0.yaml --cfg ../input/yolov5/v5/v5/models/yolov5x.yaml --weights ../input/yolov5/bestv4.pt\n    \n    \n!rm -rf convertor",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {},
+   "cell_type": "markdown",
+   "source": "# Final prediction",
+   "execution_count": null
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "def detect():\n    source = '../input/global-wheat-detection/test/'\n    weights = 'weights/best.pt'\n    if not os.path.exists(weights):\n        weights = '../input/yolov5/bestv4.pt'\n    imgsz = 1024\n    conf_thres = 0.5\n    iou_thres = 0.6\n    is_TTA = True\n    \n    imagenames =  os.listdir(source)\n    \n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # Load model\n    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n    model.to(device).eval()\n    \n    dataset = LoadImages(source, img_size=imgsz)\n\n    results = []\n    fig, ax = plt.subplots(5, 2, figsize=(30, 70))\n    count = 0\n    # img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n    #for path, img, im0s, _ in dataset:\n    for name in imagenames:\n        image_id = name.split('.')[0]\n        im01 = cv2.imread('%s/%s.jpg'%(source,image_id))  # BGR\n        assert im01 is not None, 'Image Not Found '\n        # Padded resize\n        im_w, im_h = im01.shape[:2]\n        if is_TTA:\n            enboxes = []\n            enscores = []\n            for i in range(4):\n                im0 = TTAImage(im01, i)\n                boxes, scores = detect1Image(im0, imgsz, model, device, conf_thres, iou_thres)\n                for _ in range(3-i):\n                    boxes = rotBoxes90(boxes, im_w, im_h)\n                    \n                if 1: #i<3:\n                    enboxes.append(boxes)\n                    enscores.append(scores) \n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n            enboxes.append(boxes)\n            enscores.append(scores)\n\n            boxes, scores, labels = run_wbf(enboxes, enscores, image_size = im_w, iou_thr=0.6, skip_box_thr=0.5)\n            boxes = boxes.astype(np.int32).clip(min=0, max=im_w)\n        else:\n            boxes, scores = detect1Image(im01, imgsz, model, device, conf_thres, iou_thres)\n\n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        \n        boxes = boxes[scores >= 0.05].astype(np.int32)\n        scores = scores[scores >=float(0.05)]\n        if count<10:\n            #sample = image.permute(1,2,0).cpu().numpy()\n            for box, score in zip(boxes,scores):\n                cv2.rectangle(im0,\n                              (box[0], box[1]),\n                              (box[2]+box[0], box[3]+box[1]),\n                              (220, 0, 0), 2)\n                cv2.putText(im0, '%.2f'%(score), (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX ,  \n                   0.5, (255,255,255), 2, cv2.LINE_AA)\n            ax[count%5][count//5].imshow(im0)\n            count+=1\n            \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        results.append(result)\n    return results",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "trusted": true
+   },
+   "cell_type": "code",
+   "source": "results = detect()\ntest_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.to_csv('submission.csv', index=False)\ntest_df.head()",
+   "execution_count": null,
+   "outputs": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "language": "python",
+   "display_name": "Python 3",
+   "name": "python3"
+  },
+  "language_info": {
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "version": "3.6.4",
+   "file_extension": ".py",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "name": "python",
+   "mimetype": "text/x-python"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
\ No newline at end of file
Index: dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\"cells\":[{\"metadata\":{\"_uuid\":\"fed5696c67bf55a553d6d04313a77e8c617cad99\",\"_cell_guid\":\"ea25cdf7-bdbc-3cf1-0737-bc51675e3374\"},\"cell_type\":\"markdown\",\"source\":\"# Titanic Data Science Solutions\\n\\n\\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \\n\\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\\n\\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\\n\\n## Workflow stages\\n\\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\\n\\n1. Question or problem definition.\\n2. Acquire training and testing data.\\n3. Wrangle, prepare, cleanse the data.\\n4. Analyze, identify patterns, and explore the data.\\n5. Model, predict and solve the problem.\\n6. Visualize, report, and present the problem solving steps and final solution.\\n7. Supply or submit the results.\\n\\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\\n\\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\\n\\n\\n## Question and problem definition\\n\\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\\n\\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\\n\\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\\n\\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\\n\\n## Workflow goals\\n\\nThe data science solutions workflow solves for seven major goals.\\n\\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\\n\\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\\n\\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\\n\\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\\n\\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\\n\\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\\n\\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals.\"},{\"metadata\":{\"_uuid\":\"960f8b1937dc4915ce1eb0f82614b1985c4321a4\",\"_cell_guid\":\"56a3be4e-76ef-20c6-25e8-da16147cf6d7\"},\"cell_type\":\"markdown\",\"source\":\"## Refactor Release 2017-Jan-29\\n\\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\\n\\n### User comments\\n\\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\\n\\n### Porting issues\\n\\n- Specify plot dimensions, bring legend into plot.\\n\\n\\n### Best practices\\n\\n- Performing feature correlation analysis early in the project.\\n- Using multiple plots instead of overlays for readability.\"},{\"metadata\":{\"_uuid\":\"847a9b3972a6be2d2f3346ff01fea976d92ecdb6\",\"_cell_guid\":\"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# data analysis and wrangling\\nimport pandas as pd\\nimport numpy as np\\nimport random as rnd\\n\\n# visualization\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\n# machine learning\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.svm import SVC, LinearSVC\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.linear_model import SGDClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2d307b99ee3d19da3c1cddf509ed179c21dec94a\",\"_cell_guid\":\"6b5dc743-15b1-aac6-405e-081def6ecca1\"},\"cell_type\":\"markdown\",\"source\":\"## Acquire data\\n\\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.\"},{\"metadata\":{\"_uuid\":\"13f38775c12ad6f914254a08f0d1ef948a2bd453\",\"_cell_guid\":\"e7319668-86fe-8adc-438d-0eef3fd0a982\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = pd.read_csv('../input/train.csv')\\ntest_df = pd.read_csv('../input/test.csv')\\ncombine = [train_df, test_df]\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"79282222056237a52bbbb1dbd831f057f1c23d69\",\"_cell_guid\":\"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by describing data\\n\\nPandas also helps describe the datasets answering following questions early in our project.\\n\\n**Which features are available in the dataset?**\\n\\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data).\"},{\"metadata\":{\"_uuid\":\"ef106f38a00e162a80c523778af6dcc778ccc1c2\",\"_cell_guid\":\"ce473d29-8d19-76b8-24a4-48c217286e42\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(train_df.columns.values)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1d7acf42af29a63bc038f14eded24e8b8146f541\",\"_cell_guid\":\"cd19a6f6-347f-be19-607b-dca950590b37\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are categorical?**\\n\\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\\n\\n**Which features are numerical?**\\n\\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\\n\\n- Continous: Age, Fare. Discrete: SibSp, Parch.\"},{\"metadata\":{\"_uuid\":\"e068cd3a0465b65a0930a100cb348b9146d5fd2f\",\"_cell_guid\":\"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# preview the data\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c34fa51a38336d97d5f6a184908cca37daebd584\",\"_cell_guid\":\"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46\"},\"cell_type\":\"markdown\",\"source\":\"**Which features are mixed data types?**\\n\\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\\n\\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\\n\\n**Which features may contain errors or typos?**\\n\\nThis is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\\n\\n- Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\"},{\"metadata\":{\"_uuid\":\"3488e80f309d29f5b68bbcfaba8d78da84f4fb7d\",\"_cell_guid\":\"f6e761c2-e2ff-d300-164c-af257083bb46\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df.tail()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"699c52b7a8d076ccd5ea5bc5d606313c558a6e8e\",\"_cell_guid\":\"8bfe9610-689a-29b2-26ee-f67cd4719079\"},\"cell_type\":\"markdown\",\"source\":\"**Which features contain blank, null or empty values?**\\n\\nThese will require correcting.\\n\\n- Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\\n- Cabin > Age are incomplete in case of test dataset.\\n\\n**What are the data types for various features?**\\n\\nHelping us during converting goal.\\n\\n- Seven features are integer or floats. Six in case of test dataset.\\n- Five features are strings (object).\"},{\"metadata\":{\"_uuid\":\"817e1cf0ca1cb96c7a28bb81192d92261a8bf427\",\"_cell_guid\":\"9b805f69-665a-2b2e-f31d-50d87d52865d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df.info()\\nprint('_'*40)\\ntest_df.info()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2b7c205bf25979e3242762bfebb0e3eb2fd63010\",\"_cell_guid\":\"859102e1-10df-d451-2649-2d4571e5f082\"},\"cell_type\":\"markdown\",\"source\":\"**What is the distribution of numerical feature values across the samples?**\\n\\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\\n\\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\\n- Survived is a categorical feature with 0 or 1 values.\\n- Around 38% samples survived representative of the actual survival rate at 32%.\\n- Most passengers (> 75%) did not travel with parents or children.\\n- Nearly 30% of the passengers had siblings and/or spouse aboard.\\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\\n- Few elderly passengers (<1%) within age range 65-80.\"},{\"metadata\":{\"_uuid\":\"380251a1c1e0b89147d321968dc739b6cc0eecf2\",\"_cell_guid\":\"58e387fe-86e4-e068-8307-70e37fe3f37b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df.describe()\\n# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\\n# Review Parch distribution using `percentiles=[.75, .8]`\\n# SibSp distribution `[.68, .69]`\\n# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"33bbd1709db622978c0c5879e7c5532d4734ade0\",\"_cell_guid\":\"5462bc60-258c-76bf-0a73-9adc00a2f493\"},\"cell_type\":\"markdown\",\"source\":\"**What is the distribution of categorical features?**\\n\\n- Names are unique across the dataset (count=unique=891)\\n- Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\\n- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\\n- Embarked takes three possible values. S port used by most passengers (top=S)\\n- Ticket feature has high ratio (22%) of duplicate values (unique=681).\"},{\"metadata\":{\"_uuid\":\"daa8663f577f9c1a478496cf14fe363570457191\",\"_cell_guid\":\"8066b378-1964-92e8-1352-dcac934c6af3\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df.describe(include=['O'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"c1d35ebd89a0cf7d7b409470bbb9ecaffd2a9680\",\"_cell_guid\":\"2cb22b88-937d-6f14-8b06-ea3361357889\"},\"cell_type\":\"markdown\",\"source\":\"### Assumtions based on data analysis\\n\\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\\n\\n**Correlating.**\\n\\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\\n\\n**Completing.**\\n\\n1. We may want to complete Age feature as it is definitely correlated to survival.\\n2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\\n\\n**Correcting.**\\n\\n1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\\n2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\\n3. PassengerId may be dropped from training dataset as it does not contribute to survival.\\n4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\\n\\n**Creating.**\\n\\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\\n2. We may want to engineer the Name feature to extract Title as a new feature.\\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\\n4. We may also want to create a Fare range feature if it helps our analysis.\\n\\n**Classifying.**\\n\\nWe may also add to our assumptions based on the problem description noted earlier.\\n\\n1. Women (Sex=female) were more likely to have survived.\\n2. Children (Age<?) were more likely to have survived. \\n3. The upper-class passengers (Pclass=1) were more likely to have survived.\"},{\"metadata\":{\"_uuid\":\"946ee6ca01a3e4eecfa373ca00f88042b683e2ad\",\"_cell_guid\":\"6db63a30-1d86-266e-2799-dded03c45816\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by pivoting features\\n\\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\\n\\n- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\\n- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\\n- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).\"},{\"metadata\":{\"_uuid\":\"97a845528ce9f76e85055a4bb9e97c27091f6aa1\",\"_cell_guid\":\"0964832a-a4be-2d6f-a89e-63526389cee9\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"00a2f2bca094c5984e6a232c730c8b232e7e20bb\",\"_cell_guid\":\"68908ba6-bfe9-5b31-cfde-6987fc0fbe9a\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df[[\\\"Sex\\\", \\\"Survived\\\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"a8f7a16c54417dcd86fc48aeef0c4b240d47d71b\",\"_cell_guid\":\"01c06927-c5a6-342a-5aa8-2e486ec3fd7c\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df[[\\\"SibSp\\\", \\\"Survived\\\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"5d953a6779b00b7f3794757dec8744a03162c8fd\",\"_cell_guid\":\"e686f98b-a8c9-68f8-36a4-d4598638bbd5\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df[[\\\"Parch\\\", \\\"Survived\\\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"5c6204d01f5a9040cf0bb7c678686ae48daa201f\",\"_cell_guid\":\"0d43550e-9eff-3859-3568-8856570eff76\"},\"cell_type\":\"markdown\",\"source\":\"## Analyze by visualizing data\\n\\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\\n\\n### Correlating numerical features\\n\\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\\n\\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\\n\\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\\n\\n**Observations.**\\n\\n- Infants (Age <=4) had high survival rate.\\n- Oldest passengers (Age = 80) survived.\\n- Large number of 15-25 year olds did not survive.\\n- Most passengers are in 15-35 age range.\\n\\n**Decisions.**\\n\\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\\n\\n- We should consider Age (our assumption classifying #2) in our model training.\\n- Complete the Age feature for null values (completing #1).\\n- We should band age groups (creating #3).\"},{\"metadata\":{\"_uuid\":\"d3a1fa63e9dd4f8a810086530a6363c94b36d030\",\"_cell_guid\":\"50294eac-263a-af78-cb7e-3778eb9ad41f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"g = sns.FacetGrid(train_df, col='Survived')\\ng.map(plt.hist, 'Age', bins=20)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"892259f68c2ecf64fd258965cff1ecfe77dd73a9\",\"_cell_guid\":\"87096158-4017-9213-7225-a19aea67a800\"},\"cell_type\":\"markdown\",\"source\":\"### Correlating numerical and ordinal features\\n\\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\\n\\n**Observations.**\\n\\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\\n- Pclass varies in terms of Age distribution of passengers.\\n\\n**Decisions.**\\n\\n- Consider Pclass for model training.\"},{\"metadata\":{\"_uuid\":\"4f5bcfa97c8a72f8b413c786954f3a68e135e05a\",\"_cell_guid\":\"916fdc6b-0190-9267-1ea9-907a3d87330d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\\ngrid.add_legend();\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"892ab7ee88b1b1c5f1ac987884fa31e111bb0507\",\"_cell_guid\":\"36f5a7c0-c55c-f76f-fdf8-945a32a68cb0\"},\"cell_type\":\"markdown\",\"source\":\"### Correlating categorical features\\n\\nNow we can correlate categorical features with our solution goal.\\n\\n**Observations.**\\n\\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\\n\\n**Decisions.**\\n\\n- Add Sex feature to model training.\\n- Complete and add Embarked feature to model training.\"},{\"metadata\":{\"_uuid\":\"c0e1f01b3f58e8f31b938b0e5eb1733132edc8ad\",\"_cell_guid\":\"db57aabd-0e26-9ff9-9ebd-56d401cdf6e8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# grid = sns.FacetGrid(train_df, col='Embarked')\\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\\ngrid.add_legend()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"fd824f937dcb80edd4117a2927cc0d7f99d934b8\",\"_cell_guid\":\"6b3f73f4-4600-c1ce-34e0-bd7d9eeb074a\"},\"cell_type\":\"markdown\",\"source\":\"### Correlating categorical and numerical features\\n\\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\\n\\n**Observations.**\\n\\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\\n\\n**Decisions.**\\n\\n- Consider banding Fare feature.\"},{\"metadata\":{\"_uuid\":\"c8fd535ac1bc90127369027c2101dbc939db118e\",\"_cell_guid\":\"a21f66ac-c30d-f429-cc64-1da5460d16a9\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\\ngrid.add_legend()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"73a9111a8dc2a6b8b6c78ef628b6cae2a63fc33f\",\"_cell_guid\":\"cfac6291-33cc-506e-e548-6cad9408623d\"},\"cell_type\":\"markdown\",\"source\":\"## Wrangle data\\n\\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\\n\\n### Correcting by dropping features\\n\\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\\n\\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\\n\\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.\"},{\"metadata\":{\"_uuid\":\"e328d9882affedcfc4c167aa5bb1ac132547558c\",\"_cell_guid\":\"da057efe-88f0-bf49-917b-bb2fec418ed9\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"print(\\\"Before\\\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\\n\\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\\ncombine = [train_df, test_df]\\n\\n\\\"After\\\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"21d5c47ee69f8fbef967f6f41d736b5d4eb6596f\",\"_cell_guid\":\"6b3a1216-64b6-7fe2-50bc-e89cc964a41c\"},\"cell_type\":\"markdown\",\"source\":\"### Creating new feature extracting from existing\\n\\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\\n\\nIn the following code we extract Title feature using regular expressions. The RegEx pattern `(\\\\w+\\\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\\n\\n**Observations.**\\n\\nWhen we plot Title, Age, and Survived, we note the following observations.\\n\\n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\\n- Survival among Title Age bands varies slightly.\\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\\n\\n**Decision.**\\n\\n- We decide to retain the new Title feature for model training.\"},{\"metadata\":{\"_uuid\":\"c916644bd151f3dc8fca900f656d415b4c55e2bc\",\"_cell_guid\":\"df7f0cd4-992c-4a79-fb19-bf6f0c024d4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\\\.', expand=False)\\n\\npd.crosstab(train_df['Title'], train_df['Sex'])\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"f766d512ea5bfe60b5eb7a816f482f2ab688fd2f\",\"_cell_guid\":\"908c08a6-3395-19a5-0cd7-13341054012a\"},\"cell_type\":\"markdown\",\"source\":\"We can replace many titles with a more common name or classify them as `Rare`.\"},{\"metadata\":{\"_uuid\":\"b8cd938fba61fb4e226c77521b012f4bb8aa01d0\",\"_cell_guid\":\"553f56d7-002a-ee63-21a4-c0efad10cfe9\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\\\\n \\t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\\n\\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\\n    \\ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"de245fe76474d46995a5acc31b905b8aaa5893f6\",\"_cell_guid\":\"6d46be9a-812a-f334-73b9-56ed912c9eca\"},\"cell_type\":\"markdown\",\"source\":\"We can convert the categorical titles to ordinal.\"},{\"metadata\":{\"_uuid\":\"e805ad52f0514497b67c3726104ba46d361eb92c\",\"_cell_guid\":\"67444ebc-4d11-bac1-74a6-059133b6e2e8\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"title_mapping = {\\\"Mr\\\": 1, \\\"Miss\\\": 2, \\\"Mrs\\\": 3, \\\"Master\\\": 4, \\\"Rare\\\": 5}\\nfor dataset in combine:\\n    dataset['Title'] = dataset['Title'].map(title_mapping)\\n    dataset['Title'] = dataset['Title'].fillna(0)\\n\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"5fefaa1b37c537dda164c87a757fe705a99815d9\",\"_cell_guid\":\"f27bb974-a3d7-07a1-f7e4-876f6da87e62\"},\"cell_type\":\"markdown\",\"source\":\"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.\"},{\"metadata\":{\"_uuid\":\"1da299cf2ffd399fd5b37d74fb40665d16ba5347\",\"_cell_guid\":\"9d61dded-5ff0-5018-7580-aecb4ea17506\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\\ntest_df = test_df.drop(['Name'], axis=1)\\ncombine = [train_df, test_df]\\ntrain_df.shape, test_df.shape\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"a1ac66c79b279d94860e66996d3d8dba801a6d9a\",\"_cell_guid\":\"2c8e84bb-196d-bd4a-4df9-f5213561b5d3\"},\"cell_type\":\"markdown\",\"source\":\"### Converting a categorical feature\\n\\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\\n\\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.\"},{\"metadata\":{\"_uuid\":\"840498eaee7baaca228499b0a5652da9d4edaf37\",\"_cell_guid\":\"c20c1df2-157c-e5a0-3e24-15a828095c96\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\\n\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"6da8bfe6c832f4bd2aa1312bdd6b8b4af48a012e\",\"_cell_guid\":\"d72cb29e-5034-1597-b459-83a9640d3d3a\"},\"cell_type\":\"markdown\",\"source\":\"### Completing a numerical continuous feature\\n\\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\\n\\nWe can consider three methods to complete a numerical continuous feature.\\n\\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\\n\\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\\n\\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\\n\\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.\"},{\"metadata\":{\"_uuid\":\"345038c8dd1bac9a9bc5e2cfee13fcc1f833eee0\",\"_cell_guid\":\"c311c43d-6554-3b52-8ef8-533ca08b2f68\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\\ngrid.add_legend()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"6b22ac53d95c7979d5f4580bd5fd29d27155c347\",\"_cell_guid\":\"a4f166f9-f5f9-1819-66c3-d89dd5b0d8ff\"},\"cell_type\":\"markdown\",\"source\":\"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.\"},{\"metadata\":{\"_uuid\":\"24a0971daa4cbc3aa700bae42e68c17ce9f3a6e2\",\"_cell_guid\":\"9299523c-dcf1-fb00-e52f-e2fb860a3920\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"guess_ages = np.zeros((2,3))\\nguess_ages\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8acd90569767b544f055d573bbbb8f6012853385\",\"_cell_guid\":\"ec9fed37-16b1-5518-4fa8-0a7f579dbc82\"},\"cell_type\":\"markdown\",\"source\":\"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\"},{\"metadata\":{\"_uuid\":\"31198f0ad0dbbb74290ebe135abffa994b8f58f3\",\"_cell_guid\":\"a4015dfa-a0ab-65bc-0cbe-efecf1eb2569\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    for i in range(0, 2):\\n        for j in range(0, 3):\\n            guess_df = dataset[(dataset['Sex'] == i) & \\\\\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\\n\\n            # age_mean = guess_df.mean()\\n            # age_std = guess_df.std()\\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\\n\\n            age_guess = guess_df.median()\\n\\n            # Convert random age float to nearest .5 age\\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\\n            \\n    for i in range(0, 2):\\n        for j in range(0, 3):\\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\\\\n                    'Age'] = guess_ages[i,j]\\n\\n    dataset['Age'] = dataset['Age'].astype(int)\\n\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"e7c52b44b703f28e4b6f4ddba67ab65f40274550\",\"_cell_guid\":\"dbe0a8bf-40bc-c581-e10e-76f07b3b71d4\"},\"cell_type\":\"markdown\",\"source\":\"Let us create Age bands and determine correlations with Survived.\"},{\"metadata\":{\"_uuid\":\"5c8b4cbb302f439ef0d6278dcfbdafd952675353\",\"_cell_guid\":\"725d1c84-6323-9d70-5812-baf9994d3aa1\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"856392dd415ac14ab74a885a37d068fc7a58f3a5\",\"_cell_guid\":\"ba4be3a0-e524-9c57-fbec-c8ecc5cde5c6\"},\"cell_type\":\"markdown\",\"source\":\"Let us replace Age with ordinals based on these bands.\"},{\"metadata\":{\"_uuid\":\"ee13831345f389db407c178f66c19cc8331445b0\",\"_cell_guid\":\"797b986d-2c45-a9ee-e5b5-088de817c8b2\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:    \\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\\n    dataset.loc[ dataset['Age'] > 64, 'Age']\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8e3fbc95e0fd6600e28347567416d3f0d77a24cc\",\"_cell_guid\":\"004568b6-dd9a-ff89-43d5-13d4e9370b1d\"},\"cell_type\":\"markdown\",\"source\":\"We can not remove the AgeBand feature.\"},{\"metadata\":{\"_uuid\":\"1ea01ccc4a24e8951556d97c990aa0136da19721\",\"_cell_guid\":\"875e55d4-51b0-5061-b72c-8a23946133a3\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = train_df.drop(['AgeBand'], axis=1)\\ncombine = [train_df, test_df]\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"e3d4a2040c053fbd0486c8cfc4fec3224bd3ebb3\",\"_cell_guid\":\"1c237b76-d7ac-098f-0156-480a838a64a9\"},\"cell_type\":\"markdown\",\"source\":\"### Create new feature combining existing features\\n\\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.\"},{\"metadata\":{\"_uuid\":\"33d1236ce4a8ab888b9fac2d5af1c78d174b32c7\",\"_cell_guid\":\"7e6c04ed-cfaa-3139-4378-574fd095d6ba\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\\n\\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"67f8e4474cd1ecf4261c153ce8b40ea23cf659e4\",\"_cell_guid\":\"842188e6-acf8-2476-ccec-9e3451e4fa86\"},\"cell_type\":\"markdown\",\"source\":\"We can create another feature called IsAlone.\"},{\"metadata\":{\"_uuid\":\"3b8db81cc3513b088c6bcd9cd1938156fe77992f\",\"_cell_guid\":\"5c778c69-a9ae-1b6b-44fe-a0898d07be7a\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['IsAlone'] = 0\\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\\n\\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3da4204b2c78faa54a94bbad78a8aa85fbf90c87\",\"_cell_guid\":\"e6b87c09-e7b2-f098-5b04-4360080d26bc\"},\"cell_type\":\"markdown\",\"source\":\"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.\"},{\"metadata\":{\"_uuid\":\"1e3479690ef7cd8ee10538d4f39d7117246887f0\",\"_cell_guid\":\"74ee56a6-7357-f3bc-b605-6c41f8aa6566\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\\ncombine = [train_df, test_df]\\n\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"71b800ed96407eba05220f76a1288366a22ec887\",\"_cell_guid\":\"f890b730-b1fe-919e-fb07-352fbd7edd44\"},\"cell_type\":\"markdown\",\"source\":\"We can also create an artificial feature combining Pclass and Age.\"},{\"metadata\":{\"_uuid\":\"aac2c5340c06210a8b0199e15461e9049fbf2cff\",\"_cell_guid\":\"305402aa-1ea1-c245-c367-056eef8fe453\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\\n\\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"8264cc5676db8cd3e0b3e3f078cbaa74fd585a3c\",\"_cell_guid\":\"13292c1b-020d-d9aa-525c-941331bb996a\"},\"cell_type\":\"markdown\",\"source\":\"### Completing a categorical feature\\n\\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.\"},{\"metadata\":{\"_uuid\":\"1e3f8af166f60a1b3125a6b046eff5fff02d63cf\",\"_cell_guid\":\"bf351113-9b7f-ef56-7211-e8dd00665b18\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"freq_port = train_df.Embarked.dropna().mode()[0]\\nfreq_port\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"d85b5575fb45f25749298641f6a0a38803e1ff22\",\"_cell_guid\":\"51c21fcc-f066-cd80-18c8-3d140be6cbae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\\n    \\ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"d8830e997995145314328b6218b5606df04499b0\",\"_cell_guid\":\"f6acf7b2-0db3-e583-de50-7e14b495de34\"},\"cell_type\":\"markdown\",\"source\":\"### Converting categorical feature to numeric\\n\\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.\"},{\"metadata\":{\"_uuid\":\"e480a1ef145de0b023821134896391d568a6f4f9\",\"_cell_guid\":\"89a91d76-2cc0-9bbb-c5c5-3c9ecae33c66\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\\n\\ntrain_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"d79834ebc4ab9d48ed404584711475dbf8611b91\",\"_cell_guid\":\"e3dfc817-e1c1-a274-a111-62c1c814cecf\"},\"cell_type\":\"markdown\",\"source\":\"### Quick completing and converting a numeric feature\\n\\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\\n\\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\\n\\nWe may also want round off the fare to two decimals as it represents currency.\"},{\"metadata\":{\"_uuid\":\"aacb62f3526072a84795a178bd59222378bab180\",\"_cell_guid\":\"3600cb86-cf5f-d87b-1b33-638dc8db1564\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\\ntest_df.head()\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3466d98e83899d8b38a36ede794c68c5656f48e6\",\"_cell_guid\":\"4b816bc7-d1fb-c02b-ed1d-ee34b819497d\"},\"cell_type\":\"markdown\",\"source\":\"We can not create FareBand.\"},{\"metadata\":{\"_uuid\":\"b9a78f6b4c72520d4ad99d2c89c84c591216098d\",\"_cell_guid\":\"0e9018b1-ced5-9999-8ce1-258a0952cbf2\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"89400fba71af02d09ff07adf399fb36ac4913db6\",\"_cell_guid\":\"d65901a5-3684-6869-e904-5f1a7cce8a6d\"},\"cell_type\":\"markdown\",\"source\":\"Convert the Fare feature to ordinal values based on the FareBand.\"},{\"metadata\":{\"_uuid\":\"640f305061ec4221a45ba250f8d54bb391035a57\",\"_cell_guid\":\"385f217a-4e00-76dc-1570-1de4eec0c29c\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"for dataset in combine:\\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\\n    dataset['Fare'] = dataset['Fare'].astype(int)\\n\\ntrain_df = train_df.drop(['FareBand'], axis=1)\\ncombine = [train_df, test_df]\\n    \\ntrain_df.head(10)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"531994ed95a3002d1759ceb74d9396db706a41e2\",\"_cell_guid\":\"27272bb9-3c64-4f9a-4a3b-54f02e1c8289\"},\"cell_type\":\"markdown\",\"source\":\"And the test dataset.\"},{\"metadata\":{\"_uuid\":\"8453cecad81fcc44de3f4e4e4c3ce6afa977740d\",\"_cell_guid\":\"d2334d33-4fe5-964d-beac-6aa620066e15\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"test_df.head(10)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"a55f20dd6654610ff2d66c1bf3e4c6c73dcef9e5\",\"_cell_guid\":\"69783c08-c8cc-a6ca-2a9a-5e75581c6d31\"},\"cell_type\":\"markdown\",\"source\":\"## Model, predict and solve\\n\\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\\n\\n- Logistic Regression\\n- KNN or k-Nearest Neighbors\\n- Support Vector Machines\\n- Naive Bayes classifier\\n- Decision Tree\\n- Random Forrest\\n- Perceptron\\n- Artificial neural network\\n- RVM or Relevance Vector Machine\"},{\"metadata\":{\"_uuid\":\"04d2235855f40cffd81f76b977a500fceaae87ad\",\"_cell_guid\":\"0acf54f9-6cf5-24b5-72d9-29b30052823a\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"X_train = train_df.drop(\\\"Survived\\\", axis=1)\\nY_train = train_df[\\\"Survived\\\"]\\nX_test  = test_df.drop(\\\"PassengerId\\\", axis=1).copy()\\nX_train.shape, Y_train.shape, X_test.shape\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"782903c09ec9ee4b6f3e03f7c8b5a62c00461deb\",\"_cell_guid\":\"579bc004-926a-bcfe-e9bb-c8df83356876\"},\"cell_type\":\"markdown\",\"source\":\"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\\n\\nNote the confidence score generated by the model based on our training dataset.\"},{\"metadata\":{\"_uuid\":\"a649b9c53f4c7b40694f60f5c8dc14ec5ef519ec\",\"_cell_guid\":\"0edd9322-db0b-9c37-172d-a3a4f8dec229\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Logistic Regression\\n\\nlogreg = LogisticRegression()\\nlogreg.fit(X_train, Y_train)\\nY_pred = logreg.predict(X_test)\\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\\nacc_log\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"180e27c96c821656a84889f73986c6ddfff51ed3\",\"_cell_guid\":\"3af439ae-1f04-9236-cdc2-ec8170a0d4ee\"},\"cell_type\":\"markdown\",\"source\":\"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\\n\\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\\n\\n- Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\\n- Inversely as Pclass increases, probability of Survived=1 decreases the most.\\n- This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\\n- So is Title as second highest positive correlation.\"},{\"metadata\":{\"_uuid\":\"6e6f58053fae405fc93d312fc999f3904e708dbe\",\"_cell_guid\":\"e545d5aa-4767-7a41-5799-a4c5e529ce72\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"coeff_df = pd.DataFrame(train_df.columns.delete(0))\\ncoeff_df.columns = ['Feature']\\ncoeff_df[\\\"Correlation\\\"] = pd.Series(logreg.coef_[0])\\n\\ncoeff_df.sort_values(by='Correlation', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"ccba9ac0a9c3c648ef9bc778977ab99066ab3945\",\"_cell_guid\":\"ac041064-1693-8584-156b-66674117e4d0\"},\"cell_type\":\"markdown\",\"source\":\"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).\\n\\nNote that the model generates a confidence score which is higher than Logistics Regression model.\"},{\"metadata\":{\"_uuid\":\"60039d5377da49f1aa9ac4a924331328bd69add1\",\"_cell_guid\":\"7a63bf04-a410-9c81-5310-bdef7963298f\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Support Vector Machines\\n\\nsvc = SVC()\\nsvc.fit(X_train, Y_train)\\nY_pred = svc.predict(X_test)\\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\\nacc_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"bb3ed027c45664148b61e3aa5e2ca8111aac8793\",\"_cell_guid\":\"172a6286-d495-5ac4-1a9c-5b77b74ca6d2\"},\"cell_type\":\"markdown\",\"source\":\"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\\n\\nKNN confidence score is better than Logistics Regression but worse than SVM.\"},{\"metadata\":{\"_uuid\":\"54d86cd45703d459d452f89572771deaa8877999\",\"_cell_guid\":\"ca14ae53-f05e-eb73-201c-064d7c3ed610\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"knn = KNeighborsClassifier(n_neighbors = 3)\\nknn.fit(X_train, Y_train)\\nY_pred = knn.predict(X_test)\\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\\nacc_knn\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1535f18113f851e480cd53e0c612dc05835690f3\",\"_cell_guid\":\"810f723d-2313-8dfd-e3e2-26673b9caa90\"},\"cell_type\":\"markdown\",\"source\":\"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\\n\\nThe model generated confidence score is the lowest among the models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"723c835c29e8727bc9bad4b564731f2ca98025d0\",\"_cell_guid\":\"50378071-7043-ed8d-a782-70c947520dae\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Gaussian Naive Bayes\\n\\ngaussian = GaussianNB()\\ngaussian.fit(X_train, Y_train)\\nY_pred = gaussian.predict(X_test)\\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\\nacc_gaussian\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"df148bf93e11c9ec2c97162d5c0c0605b75d9334\",\"_cell_guid\":\"1e286e19-b714-385a-fcfa-8cf5ec19956a\"},\"cell_type\":\"markdown\",\"source\":\"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).\"},{\"metadata\":{\"_uuid\":\"c19d08949f9c3a26931e28adedc848b4deaa8ab6\",\"_cell_guid\":\"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Perceptron\\n\\nperceptron = Perceptron()\\nperceptron.fit(X_train, Y_train)\\nY_pred = perceptron.predict(X_test)\\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\\nacc_perceptron\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"52ea4f44dd626448dd2199cb284b592670b1394b\",\"_cell_guid\":\"a4d56857-9432-55bb-14c0-52ebeb64d198\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Linear SVC\\n\\nlinear_svc = LinearSVC()\\nlinear_svc.fit(X_train, Y_train)\\nY_pred = linear_svc.predict(X_test)\\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\\nacc_linear_svc\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"3a016c1f24da59c85648204302d61ea15920e740\",\"_cell_guid\":\"dc98ed72-3aeb-861f-804d-b6e3d178bf4b\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Stochastic Gradient Descent\\n\\nsgd = SGDClassifier()\\nsgd.fit(X_train, Y_train)\\nY_pred = sgd.predict(X_test)\\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\\nacc_sgd\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"1c70e99920ae34adce03aaef38d61e2b83ff6a9c\",\"_cell_guid\":\"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7\"},\"cell_type\":\"markdown\",\"source\":\"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\\n\\nThe model confidence score is the highest among models evaluated so far.\"},{\"metadata\":{\"_uuid\":\"1f94308b23b934123c03067e84027b507b989e52\",\"_cell_guid\":\"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Decision Tree\\n\\ndecision_tree = DecisionTreeClassifier()\\ndecision_tree.fit(X_train, Y_train)\\nY_pred = decision_tree.predict(X_test)\\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\\nacc_decision_tree\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"24f4e46f202a858076be91752170cad52aa9aefa\",\"_cell_guid\":\"85693668-0cd5-4319-7768-eddb62d2b7d0\"},\"cell_type\":\"markdown\",\"source\":\"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\\n\\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\"},{\"metadata\":{\"_uuid\":\"483c647d2759a2703d20785a44f51b6dee47d0db\",\"_cell_guid\":\"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"# Random Forest\\n\\nrandom_forest = RandomForestClassifier(n_estimators=100)\\nrandom_forest.fit(X_train, Y_train)\\nY_pred = random_forest.predict(X_test)\\nrandom_forest.score(X_train, Y_train)\\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\\nacc_random_forest\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"2c1428d022430ea594af983a433757e11b47c50c\",\"_cell_guid\":\"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d\"},\"cell_type\":\"markdown\",\"source\":\"### Model evaluation\\n\\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. \"},{\"metadata\":{\"_uuid\":\"06a52babe50e0dd837b553c78fc73872168e1c7d\",\"_cell_guid\":\"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"models = pd.DataFrame({\\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \\n              'Random Forest', 'Naive Bayes', 'Perceptron', \\n              'Stochastic Gradient Decent', 'Linear SVC', \\n              'Decision Tree'],\\n    'Score': [acc_svc, acc_knn, acc_log, \\n              acc_random_forest, acc_gaussian, acc_perceptron, \\n              acc_sgd, acc_linear_svc, acc_decision_tree]})\\nmodels.sort_values(by='Score', ascending=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"82b31ea933b3026bd038a8370d651efdcdb3e4d7\",\"_cell_guid\":\"28854d36-051f-3ef0-5535-fa5ba6a9bef7\",\"collapsed\":true,\"trusted\":false},\"cell_type\":\"code\",\"source\":\"submission = pd.DataFrame({\\n        \\\"PassengerId\\\": test_df[\\\"PassengerId\\\"],\\n        \\\"Survived\\\": Y_pred\\n    })\\n# submission.to_csv('../output/submission.csv', index=False)\",\"execution_count\":null,\"outputs\":[]},{\"metadata\":{\"_uuid\":\"0523a03b329df58c33ed672e5fb6cd2c9af1cae3\",\"_cell_guid\":\"fcfc8d9f-e955-cf70-5843-1fb764c54699\"},\"cell_type\":\"markdown\",\"source\":\"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.\"},{\"metadata\":{\"_uuid\":\"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce\",\"_cell_guid\":\"aeec9210-f9d8-cd7c-c4cf-a87376d5f693\"},\"cell_type\":\"markdown\",\"source\":\"## References\\n\\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\\n\\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)\"}],\"metadata\":{\"_is_fork\":false,\"language_info\":{\"name\":\"python\",\"version\":\"3.6.6\",\"mimetype\":\"text/x-python\",\"codemirror_mode\":{\"name\":\"ipython\",\"version\":3},\"pygments_lexer\":\"ipython3\",\"nbconvert_exporter\":\"python\",\"file_extension\":\".py\"},\"_change_revision\":0,\"kernelspec\":{\"display_name\":\"Python 3\",\"language\":\"python\",\"name\":\"python3\"}},\"nbformat\":4,\"nbformat_minor\":1}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb b/dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb
--- a/dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb	(revision 0401533a452952fa84d98fc1fb1dd2220951e7c5)
+++ b/dataset/titanic/Manav Sehgal/titanic-data-science-solutions.ipynb	(date 1658512097909)
@@ -1,1 +1,1043 @@
-{"cells":[{"metadata":{"_uuid":"fed5696c67bf55a553d6d04313a77e8c617cad99","_cell_guid":"ea25cdf7-bdbc-3cf1-0737-bc51675e3374"},"cell_type":"markdown","source":"# Titanic Data Science Solutions\n\n\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \n\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\n\n## Workflow stages\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\n\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\n\n\n## Question and problem definition\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals."},{"metadata":{"_uuid":"960f8b1937dc4915ce1eb0f82614b1985c4321a4","_cell_guid":"56a3be4e-76ef-20c6-25e8-da16147cf6d7"},"cell_type":"markdown","source":"## Refactor Release 2017-Jan-29\n\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\n\n### User comments\n\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\n\n### Porting issues\n\n- Specify plot dimensions, bring legend into plot.\n\n\n### Best practices\n\n- Performing feature correlation analysis early in the project.\n- Using multiple plots instead of overlays for readability."},{"metadata":{"_uuid":"847a9b3972a6be2d2f3346ff01fea976d92ecdb6","_cell_guid":"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8","collapsed":true,"trusted":false},"cell_type":"code","source":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d307b99ee3d19da3c1cddf509ed179c21dec94a","_cell_guid":"6b5dc743-15b1-aac6-405e-081def6ecca1"},"cell_type":"markdown","source":"## Acquire data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together."},{"metadata":{"_uuid":"13f38775c12ad6f914254a08f0d1ef948a2bd453","_cell_guid":"e7319668-86fe-8adc-438d-0eef3fd0a982","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79282222056237a52bbbb1dbd831f057f1c23d69","_cell_guid":"3d6188f3-dc82-8ae6-dabd-83e28fcbf10d"},"cell_type":"markdown","source":"## Analyze by describing data\n\nPandas also helps describe the datasets answering following questions early in our project.\n\n**Which features are available in the dataset?**\n\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data)."},{"metadata":{"_uuid":"ef106f38a00e162a80c523778af6dcc778ccc1c2","_cell_guid":"ce473d29-8d19-76b8-24a4-48c217286e42","collapsed":true,"trusted":false},"cell_type":"code","source":"print(train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d7acf42af29a63bc038f14eded24e8b8146f541","_cell_guid":"cd19a6f6-347f-be19-607b-dca950590b37"},"cell_type":"markdown","source":"**Which features are categorical?**\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n\n**Which features are numerical?**\n\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\n- Continous: Age, Fare. Discrete: SibSp, Parch."},{"metadata":{"_uuid":"e068cd3a0465b65a0930a100cb348b9146d5fd2f","_cell_guid":"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f","collapsed":true,"trusted":false},"cell_type":"code","source":"# preview the data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c34fa51a38336d97d5f6a184908cca37daebd584","_cell_guid":"97f4e6f8-2fea-46c4-e4e8-b69062ee3d46"},"cell_type":"markdown","source":"**Which features are mixed data types?**\n\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\n\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n\n**Which features may contain errors or typos?**\n\nThis is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\n\n- Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names."},{"metadata":{"_uuid":"3488e80f309d29f5b68bbcfaba8d78da84f4fb7d","_cell_guid":"f6e761c2-e2ff-d300-164c-af257083bb46","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"699c52b7a8d076ccd5ea5bc5d606313c558a6e8e","_cell_guid":"8bfe9610-689a-29b2-26ee-f67cd4719079"},"cell_type":"markdown","source":"**Which features contain blank, null or empty values?**\n\nThese will require correcting.\n\n- Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n- Cabin > Age are incomplete in case of test dataset.\n\n**What are the data types for various features?**\n\nHelping us during converting goal.\n\n- Seven features are integer or floats. Six in case of test dataset.\n- Five features are strings (object)."},{"metadata":{"_uuid":"817e1cf0ca1cb96c7a28bb81192d92261a8bf427","_cell_guid":"9b805f69-665a-2b2e-f31d-50d87d52865d","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df.info()\nprint('_'*40)\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b7c205bf25979e3242762bfebb0e3eb2fd63010","_cell_guid":"859102e1-10df-d451-2649-2d4571e5f082"},"cell_type":"markdown","source":"**What is the distribution of numerical feature values across the samples?**\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers (> 75%) did not travel with parents or children.\n- Nearly 30% of the passengers had siblings and/or spouse aboard.\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\n- Few elderly passengers (<1%) within age range 65-80."},{"metadata":{"_uuid":"380251a1c1e0b89147d321968dc739b6cc0eecf2","_cell_guid":"58e387fe-86e4-e068-8307-70e37fe3f37b","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df.describe()\n# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n# Review Parch distribution using `percentiles=[.75, .8]`\n# SibSp distribution `[.68, .69]`\n# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33bbd1709db622978c0c5879e7c5532d4734ade0","_cell_guid":"5462bc60-258c-76bf-0a73-9adc00a2f493"},"cell_type":"markdown","source":"**What is the distribution of categorical features?**\n\n- Names are unique across the dataset (count=unique=891)\n- Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\n- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n- Embarked takes three possible values. S port used by most passengers (top=S)\n- Ticket feature has high ratio (22%) of duplicate values (unique=681)."},{"metadata":{"_uuid":"daa8663f577f9c1a478496cf14fe363570457191","_cell_guid":"8066b378-1964-92e8-1352-dcac934c6af3","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1d35ebd89a0cf7d7b409470bbb9ecaffd2a9680","_cell_guid":"2cb22b88-937d-6f14-8b06-ea3361357889"},"cell_type":"markdown","source":"### Assumtions based on data analysis\n\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating.**\n\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n**Completing.**\n\n1. We may want to complete Age feature as it is definitely correlated to survival.\n2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\n**Correcting.**\n\n1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n3. PassengerId may be dropped from training dataset as it does not contribute to survival.\n4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\n**Creating.**\n\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n2. We may want to engineer the Name feature to extract Title as a new feature.\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n4. We may also want to create a Fare range feature if it helps our analysis.\n\n**Classifying.**\n\nWe may also add to our assumptions based on the problem description noted earlier.\n\n1. Women (Sex=female) were more likely to have survived.\n2. Children (Age<?) were more likely to have survived. \n3. The upper-class passengers (Pclass=1) were more likely to have survived."},{"metadata":{"_uuid":"946ee6ca01a3e4eecfa373ca00f88042b683e2ad","_cell_guid":"6db63a30-1d86-266e-2799-dded03c45816"},"cell_type":"markdown","source":"## Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1)."},{"metadata":{"_uuid":"97a845528ce9f76e85055a4bb9e97c27091f6aa1","_cell_guid":"0964832a-a4be-2d6f-a89e-63526389cee9","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00a2f2bca094c5984e6a232c730c8b232e7e20bb","_cell_guid":"68908ba6-bfe9-5b31-cfde-6987fc0fbe9a","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8f7a16c54417dcd86fc48aeef0c4b240d47d71b","_cell_guid":"01c06927-c5a6-342a-5aa8-2e486ec3fd7c","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d953a6779b00b7f3794757dec8744a03162c8fd","_cell_guid":"e686f98b-a8c9-68f8-36a4-d4598638bbd5","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c6204d01f5a9040cf0bb7c678686ae48daa201f","_cell_guid":"0d43550e-9eff-3859-3568-8856570eff76"},"cell_type":"markdown","source":"## Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n### Correlating numerical features\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values (completing #1).\n- We should band age groups (creating #3)."},{"metadata":{"_uuid":"d3a1fa63e9dd4f8a810086530a6363c94b36d030","_cell_guid":"50294eac-263a-af78-cb7e-3778eb9ad41f","collapsed":true,"trusted":false},"cell_type":"code","source":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"892259f68c2ecf64fd258965cff1ecfe77dd73a9","_cell_guid":"87096158-4017-9213-7225-a19aea67a800"},"cell_type":"markdown","source":"### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."},{"metadata":{"_uuid":"4f5bcfa97c8a72f8b413c786954f3a68e135e05a","_cell_guid":"916fdc6b-0190-9267-1ea9-907a3d87330d","collapsed":true,"trusted":false},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"892ab7ee88b1b1c5f1ac987884fa31e111bb0507","_cell_guid":"36f5a7c0-c55c-f76f-fdf8-945a32a68cb0"},"cell_type":"markdown","source":"### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."},{"metadata":{"_uuid":"c0e1f01b3f58e8f31b938b0e5eb1733132edc8ad","_cell_guid":"db57aabd-0e26-9ff9-9ebd-56d401cdf6e8","collapsed":true,"trusted":false},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd824f937dcb80edd4117a2927cc0d7f99d934b8","_cell_guid":"6b3f73f4-4600-c1ce-34e0-bd7d9eeb074a"},"cell_type":"markdown","source":"### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."},{"metadata":{"_uuid":"c8fd535ac1bc90127369027c2101dbc939db118e","_cell_guid":"a21f66ac-c30d-f429-cc64-1da5460d16a9","collapsed":true,"trusted":false},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73a9111a8dc2a6b8b6c78ef628b6cae2a63fc33f","_cell_guid":"cfac6291-33cc-506e-e548-6cad9408623d"},"cell_type":"markdown","source":"## Wrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n### Correcting by dropping features\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent."},{"metadata":{"_uuid":"e328d9882affedcfc4c167aa5bb1ac132547558c","_cell_guid":"da057efe-88f0-bf49-917b-bb2fec418ed9","collapsed":true,"trusted":false},"cell_type":"code","source":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d5c47ee69f8fbef967f6f41d736b5d4eb6596f","_cell_guid":"6b3a1216-64b6-7fe2-50bc-e89cc964a41c"},"cell_type":"markdown","source":"### Creating new feature extracting from existing\n\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern `(\\w+\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\n\n**Observations.**\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n**Decision.**\n\n- We decide to retain the new Title feature for model training."},{"metadata":{"_uuid":"c916644bd151f3dc8fca900f656d415b4c55e2bc","_cell_guid":"df7f0cd4-992c-4a79-fb19-bf6f0c024d4b","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f766d512ea5bfe60b5eb7a816f482f2ab688fd2f","_cell_guid":"908c08a6-3395-19a5-0cd7-13341054012a"},"cell_type":"markdown","source":"We can replace many titles with a more common name or classify them as `Rare`."},{"metadata":{"_uuid":"b8cd938fba61fb4e226c77521b012f4bb8aa01d0","_cell_guid":"553f56d7-002a-ee63-21a4-c0efad10cfe9","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de245fe76474d46995a5acc31b905b8aaa5893f6","_cell_guid":"6d46be9a-812a-f334-73b9-56ed912c9eca"},"cell_type":"markdown","source":"We can convert the categorical titles to ordinal."},{"metadata":{"_uuid":"e805ad52f0514497b67c3726104ba46d361eb92c","_cell_guid":"67444ebc-4d11-bac1-74a6-059133b6e2e8","collapsed":true,"trusted":false},"cell_type":"code","source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fefaa1b37c537dda164c87a757fe705a99815d9","_cell_guid":"f27bb974-a3d7-07a1-f7e4-876f6da87e62"},"cell_type":"markdown","source":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset."},{"metadata":{"_uuid":"1da299cf2ffd399fd5b37d74fb40665d16ba5347","_cell_guid":"9d61dded-5ff0-5018-7580-aecb4ea17506","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1ac66c79b279d94860e66996d3d8dba801a6d9a","_cell_guid":"2c8e84bb-196d-bd4a-4df9-f5213561b5d3"},"cell_type":"markdown","source":"### Converting a categorical feature\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0."},{"metadata":{"_uuid":"840498eaee7baaca228499b0a5652da9d4edaf37","_cell_guid":"c20c1df2-157c-e5a0-3e24-15a828095c96","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6da8bfe6c832f4bd2aa1312bdd6b8b4af48a012e","_cell_guid":"d72cb29e-5034-1597-b459-83a9640d3d3a"},"cell_type":"markdown","source":"### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2."},{"metadata":{"_uuid":"345038c8dd1bac9a9bc5e2cfee13fcc1f833eee0","_cell_guid":"c311c43d-6554-3b52-8ef8-533ca08b2f68","collapsed":true,"trusted":false},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b22ac53d95c7979d5f4580bd5fd29d27155c347","_cell_guid":"a4f166f9-f5f9-1819-66c3-d89dd5b0d8ff"},"cell_type":"markdown","source":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations."},{"metadata":{"_uuid":"24a0971daa4cbc3aa700bae42e68c17ce9f3a6e2","_cell_guid":"9299523c-dcf1-fb00-e52f-e2fb860a3920","collapsed":true,"trusted":false},"cell_type":"code","source":"guess_ages = np.zeros((2,3))\nguess_ages","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8acd90569767b544f055d573bbbb8f6012853385","_cell_guid":"ec9fed37-16b1-5518-4fa8-0a7f579dbc82"},"cell_type":"markdown","source":"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations."},{"metadata":{"_uuid":"31198f0ad0dbbb74290ebe135abffa994b8f58f3","_cell_guid":"a4015dfa-a0ab-65bc-0cbe-efecf1eb2569","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7c52b44b703f28e4b6f4ddba67ab65f40274550","_cell_guid":"dbe0a8bf-40bc-c581-e10e-76f07b3b71d4"},"cell_type":"markdown","source":"Let us create Age bands and determine correlations with Survived."},{"metadata":{"_uuid":"5c8b4cbb302f439ef0d6278dcfbdafd952675353","_cell_guid":"725d1c84-6323-9d70-5812-baf9994d3aa1","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"856392dd415ac14ab74a885a37d068fc7a58f3a5","_cell_guid":"ba4be3a0-e524-9c57-fbec-c8ecc5cde5c6"},"cell_type":"markdown","source":"Let us replace Age with ordinals based on these bands."},{"metadata":{"_uuid":"ee13831345f389db407c178f66c19cc8331445b0","_cell_guid":"797b986d-2c45-a9ee-e5b5-088de817c8b2","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e3fbc95e0fd6600e28347567416d3f0d77a24cc","_cell_guid":"004568b6-dd9a-ff89-43d5-13d4e9370b1d"},"cell_type":"markdown","source":"We can not remove the AgeBand feature."},{"metadata":{"_uuid":"1ea01ccc4a24e8951556d97c990aa0136da19721","_cell_guid":"875e55d4-51b0-5061-b72c-8a23946133a3","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3d4a2040c053fbd0486c8cfc4fec3224bd3ebb3","_cell_guid":"1c237b76-d7ac-098f-0156-480a838a64a9"},"cell_type":"markdown","source":"### Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets."},{"metadata":{"_uuid":"33d1236ce4a8ab888b9fac2d5af1c78d174b32c7","_cell_guid":"7e6c04ed-cfaa-3139-4378-574fd095d6ba","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67f8e4474cd1ecf4261c153ce8b40ea23cf659e4","_cell_guid":"842188e6-acf8-2476-ccec-9e3451e4fa86"},"cell_type":"markdown","source":"We can create another feature called IsAlone."},{"metadata":{"_uuid":"3b8db81cc3513b088c6bcd9cd1938156fe77992f","_cell_guid":"5c778c69-a9ae-1b6b-44fe-a0898d07be7a","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3da4204b2c78faa54a94bbad78a8aa85fbf90c87","_cell_guid":"e6b87c09-e7b2-f098-5b04-4360080d26bc"},"cell_type":"markdown","source":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone."},{"metadata":{"_uuid":"1e3479690ef7cd8ee10538d4f39d7117246887f0","_cell_guid":"74ee56a6-7357-f3bc-b605-6c41f8aa6566","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71b800ed96407eba05220f76a1288366a22ec887","_cell_guid":"f890b730-b1fe-919e-fb07-352fbd7edd44"},"cell_type":"markdown","source":"We can also create an artificial feature combining Pclass and Age."},{"metadata":{"_uuid":"aac2c5340c06210a8b0199e15461e9049fbf2cff","_cell_guid":"305402aa-1ea1-c245-c367-056eef8fe453","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8264cc5676db8cd3e0b3e3f078cbaa74fd585a3c","_cell_guid":"13292c1b-020d-d9aa-525c-941331bb996a"},"cell_type":"markdown","source":"### Completing a categorical feature\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance."},{"metadata":{"_uuid":"1e3f8af166f60a1b3125a6b046eff5fff02d63cf","_cell_guid":"bf351113-9b7f-ef56-7211-e8dd00665b18","collapsed":true,"trusted":false},"cell_type":"code","source":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d85b5575fb45f25749298641f6a0a38803e1ff22","_cell_guid":"51c21fcc-f066-cd80-18c8-3d140be6cbae","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8830e997995145314328b6218b5606df04499b0","_cell_guid":"f6acf7b2-0db3-e583-de50-7e14b495de34"},"cell_type":"markdown","source":"### Converting categorical feature to numeric\n\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature."},{"metadata":{"_uuid":"e480a1ef145de0b023821134896391d568a6f4f9","_cell_guid":"89a91d76-2cc0-9bbb-c5c5-3c9ecae33c66","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d79834ebc4ab9d48ed404584711475dbf8611b91","_cell_guid":"e3dfc817-e1c1-a274-a111-62c1c814cecf"},"cell_type":"markdown","source":"### Quick completing and converting a numeric feature\n\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency."},{"metadata":{"_uuid":"aacb62f3526072a84795a178bd59222378bab180","_cell_guid":"3600cb86-cf5f-d87b-1b33-638dc8db1564","collapsed":true,"trusted":false},"cell_type":"code","source":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3466d98e83899d8b38a36ede794c68c5656f48e6","_cell_guid":"4b816bc7-d1fb-c02b-ed1d-ee34b819497d"},"cell_type":"markdown","source":"We can not create FareBand."},{"metadata":{"_uuid":"b9a78f6b4c72520d4ad99d2c89c84c591216098d","_cell_guid":"0e9018b1-ced5-9999-8ce1-258a0952cbf2","collapsed":true,"trusted":false},"cell_type":"code","source":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89400fba71af02d09ff07adf399fb36ac4913db6","_cell_guid":"d65901a5-3684-6869-e904-5f1a7cce8a6d"},"cell_type":"markdown","source":"Convert the Fare feature to ordinal values based on the FareBand."},{"metadata":{"_uuid":"640f305061ec4221a45ba250f8d54bb391035a57","_cell_guid":"385f217a-4e00-76dc-1570-1de4eec0c29c","collapsed":true,"trusted":false},"cell_type":"code","source":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"531994ed95a3002d1759ceb74d9396db706a41e2","_cell_guid":"27272bb9-3c64-4f9a-4a3b-54f02e1c8289"},"cell_type":"markdown","source":"And the test dataset."},{"metadata":{"_uuid":"8453cecad81fcc44de3f4e4e4c3ce6afa977740d","_cell_guid":"d2334d33-4fe5-964d-beac-6aa620066e15","collapsed":true,"trusted":false},"cell_type":"code","source":"test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a55f20dd6654610ff2d66c1bf3e4c6c73dcef9e5","_cell_guid":"69783c08-c8cc-a6ca-2a9a-5e75581c6d31"},"cell_type":"markdown","source":"## Model, predict and solve\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine"},{"metadata":{"_uuid":"04d2235855f40cffd81f76b977a500fceaae87ad","_cell_guid":"0acf54f9-6cf5-24b5-72d9-29b30052823a","collapsed":true,"trusted":false},"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"782903c09ec9ee4b6f3e03f7c8b5a62c00461deb","_cell_guid":"579bc004-926a-bcfe-e9bb-c8df83356876"},"cell_type":"markdown","source":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\n\nNote the confidence score generated by the model based on our training dataset."},{"metadata":{"_uuid":"a649b9c53f4c7b40694f60f5c8dc14ec5ef519ec","_cell_guid":"0edd9322-db0b-9c37-172d-a3a4f8dec229","collapsed":true,"trusted":false},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"180e27c96c821656a84889f73986c6ddfff51ed3","_cell_guid":"3af439ae-1f04-9236-cdc2-ec8170a0d4ee"},"cell_type":"markdown","source":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n- Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n- Inversely as Pclass increases, probability of Survived=1 decreases the most.\n- This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n- So is Title as second highest positive correlation."},{"metadata":{"_uuid":"6e6f58053fae405fc93d312fc999f3904e708dbe","_cell_guid":"e545d5aa-4767-7a41-5799-a4c5e529ce72","collapsed":true,"trusted":false},"cell_type":"code","source":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccba9ac0a9c3c648ef9bc778977ab99066ab3945","_cell_guid":"ac041064-1693-8584-156b-66674117e4d0"},"cell_type":"markdown","source":"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).\n\nNote that the model generates a confidence score which is higher than Logistics Regression model."},{"metadata":{"_uuid":"60039d5377da49f1aa9ac4a924331328bd69add1","_cell_guid":"7a63bf04-a410-9c81-5310-bdef7963298f","collapsed":true,"trusted":false},"cell_type":"code","source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb3ed027c45664148b61e3aa5e2ca8111aac8793","_cell_guid":"172a6286-d495-5ac4-1a9c-5b77b74ca6d2"},"cell_type":"markdown","source":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\n\nKNN confidence score is better than Logistics Regression but worse than SVM."},{"metadata":{"_uuid":"54d86cd45703d459d452f89572771deaa8877999","_cell_guid":"ca14ae53-f05e-eb73-201c-064d7c3ed610","collapsed":true,"trusted":false},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1535f18113f851e480cd53e0c612dc05835690f3","_cell_guid":"810f723d-2313-8dfd-e3e2-26673b9caa90"},"cell_type":"markdown","source":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\n\nThe model generated confidence score is the lowest among the models evaluated so far."},{"metadata":{"_uuid":"723c835c29e8727bc9bad4b564731f2ca98025d0","_cell_guid":"50378071-7043-ed8d-a782-70c947520dae","collapsed":true,"trusted":false},"cell_type":"code","source":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df148bf93e11c9ec2c97162d5c0c0605b75d9334","_cell_guid":"1e286e19-b714-385a-fcfa-8cf5ec19956a"},"cell_type":"markdown","source":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron)."},{"metadata":{"_uuid":"c19d08949f9c3a26931e28adedc848b4deaa8ab6","_cell_guid":"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d","collapsed":true,"trusted":false},"cell_type":"code","source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ea4f44dd626448dd2199cb284b592670b1394b","_cell_guid":"a4d56857-9432-55bb-14c0-52ebeb64d198","collapsed":true,"trusted":false},"cell_type":"code","source":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a016c1f24da59c85648204302d61ea15920e740","_cell_guid":"dc98ed72-3aeb-861f-804d-b6e3d178bf4b","collapsed":true,"trusted":false},"cell_type":"code","source":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c70e99920ae34adce03aaef38d61e2b83ff6a9c","_cell_guid":"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7"},"cell_type":"markdown","source":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\n\nThe model confidence score is the highest among models evaluated so far."},{"metadata":{"_uuid":"1f94308b23b934123c03067e84027b507b989e52","_cell_guid":"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0","collapsed":true,"trusted":false},"cell_type":"code","source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24f4e46f202a858076be91752170cad52aa9aefa","_cell_guid":"85693668-0cd5-4319-7768-eddb62d2b7d0"},"cell_type":"markdown","source":"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\n\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results."},{"metadata":{"_uuid":"483c647d2759a2703d20785a44f51b6dee47d0db","_cell_guid":"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567","collapsed":true,"trusted":false},"cell_type":"code","source":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c1428d022430ea594af983a433757e11b47c50c","_cell_guid":"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d"},"cell_type":"markdown","source":"### Model evaluation\n\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. "},{"metadata":{"_uuid":"06a52babe50e0dd837b553c78fc73872168e1c7d","_cell_guid":"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6","collapsed":true,"trusted":false},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82b31ea933b3026bd038a8370d651efdcdb3e4d7","_cell_guid":"28854d36-051f-3ef0-5535-fa5ba6a9bef7","collapsed":true,"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n# submission.to_csv('../output/submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0523a03b329df58c33ed672e5fb6cd2c9af1cae3","_cell_guid":"fcfc8d9f-e955-cf70-5843-1fb764c54699"},"cell_type":"markdown","source":"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome."},{"metadata":{"_uuid":"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce","_cell_guid":"aeec9210-f9d8-cd7c-c4cf-a87376d5f693"},"cell_type":"markdown","source":"## References\n\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)"}],"metadata":{"_is_fork":false,"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"_change_revision":0,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}
\ No newline at end of file
+{
+ "cells": [
+  {
+   "metadata": {
+    "_uuid": "fed5696c67bf55a553d6d04313a77e8c617cad99",
+    "_cell_guid": "ea25cdf7-bdbc-3cf1-0737-bc51675e3374"
+   },
+   "cell_type": "markdown",
+   "source": "# Titanic Data Science Solutions\n\n\n### This notebook is a companion to the book [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312). \n\nThe notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.\n\nThere are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.\n\n## Workflow stages\n\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\nThe workflow indicates general sequence of how each stage may follow the other. However there are use cases with exceptions.\n\n- We may combine mulitple workflow stages. We may analyze by visualizing data.\n- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\n- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\n- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\n\n\n## Question and problem definition\n\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is [described here at Kaggle](https://www.kaggle.com/c/titanic).\n\n> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\nWe may also want to develop some early understanding about the domain of our problem. This is described on the [Kaggle competition description page here](https://www.kaggle.com/c/titanic). Here are the highlights to note.\n\n- On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n- One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n- Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## Workflow goals\n\nThe data science solutions workflow solves for seven major goals.\n\n**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n\n**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n\n**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n\n**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n\n**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n\n**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n\n**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals."
+  },
+  {
+   "metadata": {
+    "_uuid": "960f8b1937dc4915ce1eb0f82614b1985c4321a4",
+    "_cell_guid": "56a3be4e-76ef-20c6-25e8-da16147cf6d7"
+   },
+   "cell_type": "markdown",
+   "source": "## Refactor Release 2017-Jan-29\n\nWe are significantly refactoring the notebook based on (a) comments received by readers, (b) issues in porting notebook from Jupyter kernel (2.7) to Kaggle kernel (3.5), and (c) review of few more best practice kernels.\n\n### User comments\n\n- Combine training and test data for certain operations like converting titles across dataset to numerical values. (thanks @Sharan Naribole)\n- Correct observation - nearly 30% of the passengers had siblings and/or spouses aboard. (thanks @Reinhard)\n- Correctly interpreting logistic regresssion coefficients. (thanks @Reinhard)\n\n### Porting issues\n\n- Specify plot dimensions, bring legend into plot.\n\n\n### Best practices\n\n- Performing feature correlation analysis early in the project.\n- Using multiple plots instead of overlays for readability."
+  },
+  {
+   "metadata": {
+    "_uuid": "847a9b3972a6be2d2f3346ff01fea976d92ecdb6",
+    "_cell_guid": "5767a33c-8f18-4034-e52d-bf7a8f7d8ab8",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "2d307b99ee3d19da3c1cddf509ed179c21dec94a",
+    "_cell_guid": "6b5dc743-15b1-aac6-405e-081def6ecca1"
+   },
+   "cell_type": "markdown",
+   "source": "## Acquire data\n\nThe Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together."
+  },
+  {
+   "metadata": {
+    "_uuid": "13f38775c12ad6f914254a08f0d1ef948a2bd453",
+    "_cell_guid": "e7319668-86fe-8adc-438d-0eef3fd0a982",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine = [train_df, test_df]",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "79282222056237a52bbbb1dbd831f057f1c23d69",
+    "_cell_guid": "3d6188f3-dc82-8ae6-dabd-83e28fcbf10d"
+   },
+   "cell_type": "markdown",
+   "source": "## Analyze by describing data\n\nPandas also helps describe the datasets answering following questions early in our project.\n\n**Which features are available in the dataset?**\n\nNoting the feature names for directly manipulating or analyzing these. These feature names are described on the [Kaggle data page here](https://www.kaggle.com/c/titanic/data)."
+  },
+  {
+   "metadata": {
+    "_uuid": "ef106f38a00e162a80c523778af6dcc778ccc1c2",
+    "_cell_guid": "ce473d29-8d19-76b8-24a4-48c217286e42",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "print(train_df.columns.values)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "1d7acf42af29a63bc038f14eded24e8b8146f541",
+    "_cell_guid": "cd19a6f6-347f-be19-607b-dca950590b37"
+   },
+   "cell_type": "markdown",
+   "source": "**Which features are categorical?**\n\nThese values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\n- Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n\n**Which features are numerical?**\n\nWhich features are numerical? These values change from sample to sample. Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\n- Continous: Age, Fare. Discrete: SibSp, Parch."
+  },
+  {
+   "metadata": {
+    "_uuid": "e068cd3a0465b65a0930a100cb348b9146d5fd2f",
+    "_cell_guid": "8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# preview the data\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "c34fa51a38336d97d5f6a184908cca37daebd584",
+    "_cell_guid": "97f4e6f8-2fea-46c4-e4e8-b69062ee3d46"
+   },
+   "cell_type": "markdown",
+   "source": "**Which features are mixed data types?**\n\nNumerical, alphanumeric data within same feature. These are candidates for correcting goal.\n\n- Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n\n**Which features may contain errors or typos?**\n\nThis is harder to review for a large dataset, however reviewing a few samples from a smaller dataset may just tell us outright, which features may require correcting.\n\n- Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names."
+  },
+  {
+   "metadata": {
+    "_uuid": "3488e80f309d29f5b68bbcfaba8d78da84f4fb7d",
+    "_cell_guid": "f6e761c2-e2ff-d300-164c-af257083bb46",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df.tail()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "699c52b7a8d076ccd5ea5bc5d606313c558a6e8e",
+    "_cell_guid": "8bfe9610-689a-29b2-26ee-f67cd4719079"
+   },
+   "cell_type": "markdown",
+   "source": "**Which features contain blank, null or empty values?**\n\nThese will require correcting.\n\n- Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n- Cabin > Age are incomplete in case of test dataset.\n\n**What are the data types for various features?**\n\nHelping us during converting goal.\n\n- Seven features are integer or floats. Six in case of test dataset.\n- Five features are strings (object)."
+  },
+  {
+   "metadata": {
+    "_uuid": "817e1cf0ca1cb96c7a28bb81192d92261a8bf427",
+    "_cell_guid": "9b805f69-665a-2b2e-f31d-50d87d52865d",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df.info()\nprint('_'*40)\ntest_df.info()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "2b7c205bf25979e3242762bfebb0e3eb2fd63010",
+    "_cell_guid": "859102e1-10df-d451-2649-2d4571e5f082"
+   },
+   "cell_type": "markdown",
+   "source": "**What is the distribution of numerical feature values across the samples?**\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers (> 75%) did not travel with parents or children.\n- Nearly 30% of the passengers had siblings and/or spouse aboard.\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\n- Few elderly passengers (<1%) within age range 65-80."
+  },
+  {
+   "metadata": {
+    "_uuid": "380251a1c1e0b89147d321968dc739b6cc0eecf2",
+    "_cell_guid": "58e387fe-86e4-e068-8307-70e37fe3f37b",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df.describe()\n# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n# Review Parch distribution using `percentiles=[.75, .8]`\n# SibSp distribution `[.68, .69]`\n# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "33bbd1709db622978c0c5879e7c5532d4734ade0",
+    "_cell_guid": "5462bc60-258c-76bf-0a73-9adc00a2f493"
+   },
+   "cell_type": "markdown",
+   "source": "**What is the distribution of categorical features?**\n\n- Names are unique across the dataset (count=unique=891)\n- Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\n- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n- Embarked takes three possible values. S port used by most passengers (top=S)\n- Ticket feature has high ratio (22%) of duplicate values (unique=681)."
+  },
+  {
+   "metadata": {
+    "_uuid": "daa8663f577f9c1a478496cf14fe363570457191",
+    "_cell_guid": "8066b378-1964-92e8-1352-dcac934c6af3",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df.describe(include=['O'])",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "c1d35ebd89a0cf7d7b409470bbb9ecaffd2a9680",
+    "_cell_guid": "2cb22b88-937d-6f14-8b06-ea3361357889"
+   },
+   "cell_type": "markdown",
+   "source": "### Assumtions based on data analysis\n\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating.**\n\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n**Completing.**\n\n1. We may want to complete Age feature as it is definitely correlated to survival.\n2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\n**Correcting.**\n\n1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n3. PassengerId may be dropped from training dataset as it does not contribute to survival.\n4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\n**Creating.**\n\n1. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n2. We may want to engineer the Name feature to extract Title as a new feature.\n3. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n4. We may also want to create a Fare range feature if it helps our analysis.\n\n**Classifying.**\n\nWe may also add to our assumptions based on the problem description noted earlier.\n\n1. Women (Sex=female) were more likely to have survived.\n2. Children (Age<?) were more likely to have survived. \n3. The upper-class passengers (Pclass=1) were more likely to have survived."
+  },
+  {
+   "metadata": {
+    "_uuid": "946ee6ca01a3e4eecfa373ca00f88042b683e2ad",
+    "_cell_guid": "6db63a30-1d86-266e-2799-dded03c45816"
+   },
+   "cell_type": "markdown",
+   "source": "## Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\n- **Pclass** We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\n- **Sex** We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\n- **SibSp and Parch** These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1)."
+  },
+  {
+   "metadata": {
+    "_uuid": "97a845528ce9f76e85055a4bb9e97c27091f6aa1",
+    "_cell_guid": "0964832a-a4be-2d6f-a89e-63526389cee9",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "00a2f2bca094c5984e6a232c730c8b232e7e20bb",
+    "_cell_guid": "68908ba6-bfe9-5b31-cfde-6987fc0fbe9a",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "a8f7a16c54417dcd86fc48aeef0c4b240d47d71b",
+    "_cell_guid": "01c06927-c5a6-342a-5aa8-2e486ec3fd7c",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "5d953a6779b00b7f3794757dec8744a03162c8fd",
+    "_cell_guid": "e686f98b-a8c9-68f8-36a4-d4598638bbd5",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "5c6204d01f5a9040cf0bb7c678686ae48daa201f",
+    "_cell_guid": "0d43550e-9eff-3859-3568-8856570eff76"
+   },
+   "cell_type": "markdown",
+   "source": "## Analyze by visualizing data\n\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.\n\n### Correlating numerical features\n\nLet us start by understanding correlations between numerical features and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age (our assumption classifying #2) in our model training.\n- Complete the Age feature for null values (completing #1).\n- We should band age groups (creating #3)."
+  },
+  {
+   "metadata": {
+    "_uuid": "d3a1fa63e9dd4f8a810086530a6363c94b36d030",
+    "_cell_guid": "50294eac-263a-af78-cb7e-3778eb9ad41f",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "892259f68c2ecf64fd258965cff1ecfe77dd73a9",
+    "_cell_guid": "87096158-4017-9213-7225-a19aea67a800"
+   },
+   "cell_type": "markdown",
+   "source": "### Correlating numerical and ordinal features\n\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.\n\n**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n- Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."
+  },
+  {
+   "metadata": {
+    "_uuid": "4f5bcfa97c8a72f8b413c786954f3a68e135e05a",
+    "_cell_guid": "916fdc6b-0190-9267-1ea9-907a3d87330d",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "892ab7ee88b1b1c5f1ac987884fa31e111bb0507",
+    "_cell_guid": "36f5a7c0-c55c-f76f-fdf8-945a32a68cb0"
+   },
+   "cell_type": "markdown",
+   "source": "### Correlating categorical features\n\nNow we can correlate categorical features with our solution goal.\n\n**Observations.**\n\n- Female passengers had much better survival rate than males. Confirms classifying (#1).\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."
+  },
+  {
+   "metadata": {
+    "_uuid": "c0e1f01b3f58e8f31b938b0e5eb1733132edc8ad",
+    "_cell_guid": "db57aabd-0e26-9ff9-9ebd-56d401cdf6e8",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "fd824f937dcb80edd4117a2927cc0d7f99d934b8",
+    "_cell_guid": "6b3f73f4-4600-c1ce-34e0-bd7d9eeb074a"
+   },
+   "cell_type": "markdown",
+   "source": "### Correlating categorical and numerical features\n\nWe may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).\n\n**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."
+  },
+  {
+   "metadata": {
+    "_uuid": "c8fd535ac1bc90127369027c2101dbc939db118e",
+    "_cell_guid": "a21f66ac-c30d-f429-cc64-1da5460d16a9",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "73a9111a8dc2a6b8b6c78ef628b6cae2a63fc33f",
+    "_cell_guid": "cfac6291-33cc-506e-e548-6cad9408623d"
+   },
+   "cell_type": "markdown",
+   "source": "## Wrangle data\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.\n\n### Correcting by dropping features\n\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent."
+  },
+  {
+   "metadata": {
+    "_uuid": "e328d9882affedcfc4c167aa5bb1ac132547558c",
+    "_cell_guid": "da057efe-88f0-bf49-917b-bb2fec418ed9",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "21d5c47ee69f8fbef967f6f41d736b5d4eb6596f",
+    "_cell_guid": "6b3a1216-64b6-7fe2-50bc-e89cc964a41c"
+   },
+   "cell_type": "markdown",
+   "source": "### Creating new feature extracting from existing\n\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern `(\\w+\\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.\n\n**Observations.**\n\nWhen we plot Title, Age, and Survived, we note the following observations.\n\n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).\n\n**Decision.**\n\n- We decide to retain the new Title feature for model training."
+  },
+  {
+   "metadata": {
+    "_uuid": "c916644bd151f3dc8fca900f656d415b4c55e2bc",
+    "_cell_guid": "df7f0cd4-992c-4a79-fb19-bf6f0c024d4b",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "f766d512ea5bfe60b5eb7a816f482f2ab688fd2f",
+    "_cell_guid": "908c08a6-3395-19a5-0cd7-13341054012a"
+   },
+   "cell_type": "markdown",
+   "source": "We can replace many titles with a more common name or classify them as `Rare`."
+  },
+  {
+   "metadata": {
+    "_uuid": "b8cd938fba61fb4e226c77521b012f4bb8aa01d0",
+    "_cell_guid": "553f56d7-002a-ee63-21a4-c0efad10cfe9",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "de245fe76474d46995a5acc31b905b8aaa5893f6",
+    "_cell_guid": "6d46be9a-812a-f334-73b9-56ed912c9eca"
+   },
+   "cell_type": "markdown",
+   "source": "We can convert the categorical titles to ordinal."
+  },
+  {
+   "metadata": {
+    "_uuid": "e805ad52f0514497b67c3726104ba46d361eb92c",
+    "_cell_guid": "67444ebc-4d11-bac1-74a6-059133b6e2e8",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "5fefaa1b37c537dda164c87a757fe705a99815d9",
+    "_cell_guid": "f27bb974-a3d7-07a1-f7e4-876f6da87e62"
+   },
+   "cell_type": "markdown",
+   "source": "Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset."
+  },
+  {
+   "metadata": {
+    "_uuid": "1da299cf2ffd399fd5b37d74fb40665d16ba5347",
+    "_cell_guid": "9d61dded-5ff0-5018-7580-aecb4ea17506",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "a1ac66c79b279d94860e66996d3d8dba801a6d9a",
+    "_cell_guid": "2c8e84bb-196d-bd4a-4df9-f5213561b5d3"
+   },
+   "cell_type": "markdown",
+   "source": "### Converting a categorical feature\n\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0."
+  },
+  {
+   "metadata": {
+    "_uuid": "840498eaee7baaca228499b0a5652da9d4edaf37",
+    "_cell_guid": "c20c1df2-157c-e5a0-3e24-15a828095c96",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "6da8bfe6c832f4bd2aa1312bdd6b8b4af48a012e",
+    "_cell_guid": "d72cb29e-5034-1597-b459-83a9640d3d3a"
+   },
+   "cell_type": "markdown",
+   "source": "### Completing a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2."
+  },
+  {
+   "metadata": {
+    "_uuid": "345038c8dd1bac9a9bc5e2cfee13fcc1f833eee0",
+    "_cell_guid": "c311c43d-6554-3b52-8ef8-533ca08b2f68",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "6b22ac53d95c7979d5f4580bd5fd29d27155c347",
+    "_cell_guid": "a4f166f9-f5f9-1819-66c3-d89dd5b0d8ff"
+   },
+   "cell_type": "markdown",
+   "source": "Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations."
+  },
+  {
+   "metadata": {
+    "_uuid": "24a0971daa4cbc3aa700bae42e68c17ce9f3a6e2",
+    "_cell_guid": "9299523c-dcf1-fb00-e52f-e2fb860a3920",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "guess_ages = np.zeros((2,3))\nguess_ages",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "8acd90569767b544f055d573bbbb8f6012853385",
+    "_cell_guid": "ec9fed37-16b1-5518-4fa8-0a7f579dbc82"
+   },
+   "cell_type": "markdown",
+   "source": "Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations."
+  },
+  {
+   "metadata": {
+    "_uuid": "31198f0ad0dbbb74290ebe135abffa994b8f58f3",
+    "_cell_guid": "a4015dfa-a0ab-65bc-0cbe-efecf1eb2569",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "e7c52b44b703f28e4b6f4ddba67ab65f40274550",
+    "_cell_guid": "dbe0a8bf-40bc-c581-e10e-76f07b3b71d4"
+   },
+   "cell_type": "markdown",
+   "source": "Let us create Age bands and determine correlations with Survived."
+  },
+  {
+   "metadata": {
+    "_uuid": "5c8b4cbb302f439ef0d6278dcfbdafd952675353",
+    "_cell_guid": "725d1c84-6323-9d70-5812-baf9994d3aa1",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "856392dd415ac14ab74a885a37d068fc7a58f3a5",
+    "_cell_guid": "ba4be3a0-e524-9c57-fbec-c8ecc5cde5c6"
+   },
+   "cell_type": "markdown",
+   "source": "Let us replace Age with ordinals based on these bands."
+  },
+  {
+   "metadata": {
+    "_uuid": "ee13831345f389db407c178f66c19cc8331445b0",
+    "_cell_guid": "797b986d-2c45-a9ee-e5b5-088de817c8b2",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "8e3fbc95e0fd6600e28347567416d3f0d77a24cc",
+    "_cell_guid": "004568b6-dd9a-ff89-43d5-13d4e9370b1d"
+   },
+   "cell_type": "markdown",
+   "source": "We can not remove the AgeBand feature."
+  },
+  {
+   "metadata": {
+    "_uuid": "1ea01ccc4a24e8951556d97c990aa0136da19721",
+    "_cell_guid": "875e55d4-51b0-5061-b72c-8a23946133a3",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "e3d4a2040c053fbd0486c8cfc4fec3224bd3ebb3",
+    "_cell_guid": "1c237b76-d7ac-098f-0156-480a838a64a9"
+   },
+   "cell_type": "markdown",
+   "source": "### Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets."
+  },
+  {
+   "metadata": {
+    "_uuid": "33d1236ce4a8ab888b9fac2d5af1c78d174b32c7",
+    "_cell_guid": "7e6c04ed-cfaa-3139-4378-574fd095d6ba",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "67f8e4474cd1ecf4261c153ce8b40ea23cf659e4",
+    "_cell_guid": "842188e6-acf8-2476-ccec-9e3451e4fa86"
+   },
+   "cell_type": "markdown",
+   "source": "We can create another feature called IsAlone."
+  },
+  {
+   "metadata": {
+    "_uuid": "3b8db81cc3513b088c6bcd9cd1938156fe77992f",
+    "_cell_guid": "5c778c69-a9ae-1b6b-44fe-a0898d07be7a",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "3da4204b2c78faa54a94bbad78a8aa85fbf90c87",
+    "_cell_guid": "e6b87c09-e7b2-f098-5b04-4360080d26bc"
+   },
+   "cell_type": "markdown",
+   "source": "Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone."
+  },
+  {
+   "metadata": {
+    "_uuid": "1e3479690ef7cd8ee10538d4f39d7117246887f0",
+    "_cell_guid": "74ee56a6-7357-f3bc-b605-6c41f8aa6566",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "71b800ed96407eba05220f76a1288366a22ec887",
+    "_cell_guid": "f890b730-b1fe-919e-fb07-352fbd7edd44"
+   },
+   "cell_type": "markdown",
+   "source": "We can also create an artificial feature combining Pclass and Age."
+  },
+  {
+   "metadata": {
+    "_uuid": "aac2c5340c06210a8b0199e15461e9049fbf2cff",
+    "_cell_guid": "305402aa-1ea1-c245-c367-056eef8fe453",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "8264cc5676db8cd3e0b3e3f078cbaa74fd585a3c",
+    "_cell_guid": "13292c1b-020d-d9aa-525c-941331bb996a"
+   },
+   "cell_type": "markdown",
+   "source": "### Completing a categorical feature\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance."
+  },
+  {
+   "metadata": {
+    "_uuid": "1e3f8af166f60a1b3125a6b046eff5fff02d63cf",
+    "_cell_guid": "bf351113-9b7f-ef56-7211-e8dd00665b18",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "d85b5575fb45f25749298641f6a0a38803e1ff22",
+    "_cell_guid": "51c21fcc-f066-cd80-18c8-3d140be6cbae",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "d8830e997995145314328b6218b5606df04499b0",
+    "_cell_guid": "f6acf7b2-0db3-e583-de50-7e14b495de34"
+   },
+   "cell_type": "markdown",
+   "source": "### Converting categorical feature to numeric\n\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature."
+  },
+  {
+   "metadata": {
+    "_uuid": "e480a1ef145de0b023821134896391d568a6f4f9",
+    "_cell_guid": "89a91d76-2cc0-9bbb-c5c5-3c9ecae33c66",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "d79834ebc4ab9d48ed404584711475dbf8611b91",
+    "_cell_guid": "e3dfc817-e1c1-a274-a111-62c1c814cecf"
+   },
+   "cell_type": "markdown",
+   "source": "### Quick completing and converting a numeric feature\n\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency."
+  },
+  {
+   "metadata": {
+    "_uuid": "aacb62f3526072a84795a178bd59222378bab180",
+    "_cell_guid": "3600cb86-cf5f-d87b-1b33-638dc8db1564",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "3466d98e83899d8b38a36ede794c68c5656f48e6",
+    "_cell_guid": "4b816bc7-d1fb-c02b-ed1d-ee34b819497d"
+   },
+   "cell_type": "markdown",
+   "source": "We can not create FareBand."
+  },
+  {
+   "metadata": {
+    "_uuid": "b9a78f6b4c72520d4ad99d2c89c84c591216098d",
+    "_cell_guid": "0e9018b1-ced5-9999-8ce1-258a0952cbf2",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "89400fba71af02d09ff07adf399fb36ac4913db6",
+    "_cell_guid": "d65901a5-3684-6869-e904-5f1a7cce8a6d"
+   },
+   "cell_type": "markdown",
+   "source": "Convert the Fare feature to ordinal values based on the FareBand."
+  },
+  {
+   "metadata": {
+    "_uuid": "640f305061ec4221a45ba250f8d54bb391035a57",
+    "_cell_guid": "385f217a-4e00-76dc-1570-1de4eec0c29c",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "531994ed95a3002d1759ceb74d9396db706a41e2",
+    "_cell_guid": "27272bb9-3c64-4f9a-4a3b-54f02e1c8289"
+   },
+   "cell_type": "markdown",
+   "source": "And the test dataset."
+  },
+  {
+   "metadata": {
+    "_uuid": "8453cecad81fcc44de3f4e4e4c3ce6afa977740d",
+    "_cell_guid": "d2334d33-4fe5-964d-beac-6aa620066e15",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "test_df.head(10)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "a55f20dd6654610ff2d66c1bf3e4c6c73dcef9e5",
+    "_cell_guid": "69783c08-c8cc-a6ca-2a9a-5e75581c6d31"
+   },
+   "cell_type": "markdown",
+   "source": "## Model, predict and solve\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine"
+  },
+  {
+   "metadata": {
+    "_uuid": "04d2235855f40cffd81f76b977a500fceaae87ad",
+    "_cell_guid": "0acf54f9-6cf5-24b5-72d9-29b30052823a",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "782903c09ec9ee4b6f3e03f7c8b5a62c00461deb",
+    "_cell_guid": "579bc004-926a-bcfe-e9bb-c8df83356876"
+   },
+   "cell_type": "markdown",
+   "source": "Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\n\nNote the confidence score generated by the model based on our training dataset."
+  },
+  {
+   "metadata": {
+    "_uuid": "a649b9c53f4c7b40694f60f5c8dc14ec5ef519ec",
+    "_cell_guid": "0edd9322-db0b-9c37-172d-a3a4f8dec229",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "180e27c96c821656a84889f73986c6ddfff51ed3",
+    "_cell_guid": "3af439ae-1f04-9236-cdc2-ec8170a0d4ee"
+   },
+   "cell_type": "markdown",
+   "source": "We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n- Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n- Inversely as Pclass increases, probability of Survived=1 decreases the most.\n- This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n- So is Title as second highest positive correlation."
+  },
+  {
+   "metadata": {
+    "_uuid": "6e6f58053fae405fc93d312fc999f3904e708dbe",
+    "_cell_guid": "e545d5aa-4767-7a41-5799-a4c5e529ce72",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "ccba9ac0a9c3c648ef9bc778977ab99066ab3945",
+    "_cell_guid": "ac041064-1693-8584-156b-66674117e4d0"
+   },
+   "cell_type": "markdown",
+   "source": "Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).\n\nNote that the model generates a confidence score which is higher than Logistics Regression model."
+  },
+  {
+   "metadata": {
+    "_uuid": "60039d5377da49f1aa9ac4a924331328bd69add1",
+    "_cell_guid": "7a63bf04-a410-9c81-5310-bdef7963298f",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "bb3ed027c45664148b61e3aa5e2ca8111aac8793",
+    "_cell_guid": "172a6286-d495-5ac4-1a9c-5b77b74ca6d2"
+   },
+   "cell_type": "markdown",
+   "source": "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\n\nKNN confidence score is better than Logistics Regression but worse than SVM."
+  },
+  {
+   "metadata": {
+    "_uuid": "54d86cd45703d459d452f89572771deaa8877999",
+    "_cell_guid": "ca14ae53-f05e-eb73-201c-064d7c3ed610",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "1535f18113f851e480cd53e0c612dc05835690f3",
+    "_cell_guid": "810f723d-2313-8dfd-e3e2-26673b9caa90"
+   },
+   "cell_type": "markdown",
+   "source": "In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).\n\nThe model generated confidence score is the lowest among the models evaluated so far."
+  },
+  {
+   "metadata": {
+    "_uuid": "723c835c29e8727bc9bad4b564731f2ca98025d0",
+    "_cell_guid": "50378071-7043-ed8d-a782-70c947520dae",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "df148bf93e11c9ec2c97162d5c0c0605b75d9334",
+    "_cell_guid": "1e286e19-b714-385a-fcfa-8cf5ec19956a"
+   },
+   "cell_type": "markdown",
+   "source": "The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron)."
+  },
+  {
+   "metadata": {
+    "_uuid": "c19d08949f9c3a26931e28adedc848b4deaa8ab6",
+    "_cell_guid": "ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "52ea4f44dd626448dd2199cb284b592670b1394b",
+    "_cell_guid": "a4d56857-9432-55bb-14c0-52ebeb64d198",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "3a016c1f24da59c85648204302d61ea15920e740",
+    "_cell_guid": "dc98ed72-3aeb-861f-804d-b6e3d178bf4b",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "1c70e99920ae34adce03aaef38d61e2b83ff6a9c",
+    "_cell_guid": "bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7"
+   },
+   "cell_type": "markdown",
+   "source": "This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\n\nThe model confidence score is the highest among models evaluated so far."
+  },
+  {
+   "metadata": {
+    "_uuid": "1f94308b23b934123c03067e84027b507b989e52",
+    "_cell_guid": "dd85f2b7-ace2-0306-b4ec-79c68cd3fea0",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "24f4e46f202a858076be91752170cad52aa9aefa",
+    "_cell_guid": "85693668-0cd5-4319-7768-eddb62d2b7d0"
+   },
+   "cell_type": "markdown",
+   "source": "The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\n\nThe model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results."
+  },
+  {
+   "metadata": {
+    "_uuid": "483c647d2759a2703d20785a44f51b6dee47d0db",
+    "_cell_guid": "f0694a8e-b618-8ed9-6f0d-8c6fba2c4567",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "2c1428d022430ea594af983a433757e11b47c50c",
+    "_cell_guid": "f6c9eef8-83dd-581c-2d8e-ce932fe3a44d"
+   },
+   "cell_type": "markdown",
+   "source": "### Model evaluation\n\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. "
+  },
+  {
+   "metadata": {
+    "_uuid": "06a52babe50e0dd837b553c78fc73872168e1c7d",
+    "_cell_guid": "1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "82b31ea933b3026bd038a8370d651efdcdb3e4d7",
+    "_cell_guid": "28854d36-051f-3ef0-5535-fa5ba6a9bef7",
+    "collapsed": true,
+    "trusted": false
+   },
+   "cell_type": "code",
+   "source": "submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n# submission.to_csv('../output/submission.csv', index=False)",
+   "execution_count": null,
+   "outputs": []
+  },
+  {
+   "metadata": {
+    "_uuid": "0523a03b329df58c33ed672e5fb6cd2c9af1cae3",
+    "_cell_guid": "fcfc8d9f-e955-cf70-5843-1fb764c54699"
+   },
+   "cell_type": "markdown",
+   "source": "Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome."
+  },
+  {
+   "metadata": {
+    "_uuid": "cdae56d6adbfb15ff9c491c645ae46e2c91d75ce",
+    "_cell_guid": "aeec9210-f9d8-cd7c-c4cf-a87376d5f693"
+   },
+   "cell_type": "markdown",
+   "source": "## References\n\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)"
+  }
+ ],
+ "metadata": {
+  "_is_fork": false,
+  "language_info": {
+   "name": "python",
+   "version": "3.6.6",
+   "mimetype": "text/x-python",
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "pygments_lexer": "ipython3",
+   "nbconvert_exporter": "python",
+   "file_extension": ".py"
+  },
+  "_change_revision": 0,
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 1
+}
\ No newline at end of file
Index: dataset/titanic/Masum Rumi/test.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/dataset/titanic/Masum Rumi/test.ipynb b/dataset/titanic/Masum Rumi/test.ipynb
new file mode 100644
--- /dev/null	(date 1658512086385)
+++ b/dataset/titanic/Masum Rumi/test.ipynb	(date 1658512086385)
@@ -0,0 +1,73 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "source": [
+    "# Gaussian Naive Bayes\n",
+    "from sklearn.metrics import accuracy_score\n",
+    "from sklearn.linear_model import LinearRegression\n",
+    "\n",
+    "param = True\n",
+    "gaussian = LinearRegression(fit_interceptbool=param,\n",
+    "                            normalizebool =False)\n",
+    "gaussian.fit(X, y)\n",
+    "y_pred = gaussian.predict(X_test)\n",
+    "gaussian_accy = round(accuracy_score(y_pred, y_test), 3)\n",
+    "print(gaussian_accy)"
+   ],
+   "metadata": {
+    "collapsed": false,
+    "pycharm": {
+     "name": "#%% md\n"
+    }
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "outputs": [],
+   "source": [
+    "layer = tf.keras.layers.Activation('relu')"
+   ],
+   "metadata": {
+    "collapsed": false,
+    "pycharm": {
+     "name": "#%%\n"
+    }
+   }
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "outputs": [],
+   "source": [],
+   "metadata": {
+    "collapsed": false,
+    "pycharm": {
+     "name": "#%%\n"
+    }
+   }
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 2
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython2",
+   "version": "2.7.6"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 0
+}
\ No newline at end of file
Index: res/.~lock.API-dictionary.xlsx#
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/res/.~lock.API-dictionary.xlsx# b/res/.~lock.API-dictionary.xlsx#
new file mode 100644
--- /dev/null	(date 1658512086389)
+++ b/res/.~lock.API-dictionary.xlsx#	(date 1658512086389)
@@ -0,0 +1,1 @@
+,sajed,sajed,22.07.2022 13:03,file:///home/sajed/.config/libreoffice/4;
\ No newline at end of file
